<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>PPSN_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ppsn---101">PPSN - 101</h2>
<ul>
<li><details>
<summary>
(2024). Discovering rotation symmetric self-dual bent functions with
evolutionary algorithms. <em>PPSN</em>, 429–445. (<a
href="https://doi.org/10.1007/978-3-031-70085-9_27">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bent Boolean functions are interesting mathematical objects with diverse real-world applications. Besides looking at the whole class of bent functions, one could also consider subclasses like rotation symmetric bent functions or (anti)-self-dual bent functions. Such classes are naturally smaller, making it (potentially) easier to enumerate functions inside the class with a computer investigation. This work considers a novel problem of evolving rotation symmetric (anti)-self-dual bent functions. We consider two solution encodings and a number of evolutionary algorithms. We successfully find rotation symmetric (anti)-self-dual functions for several Boolean function sizes, which are the first known examples of such functions. We hope this work will open a new research direction that will result in finding more such functions for larger dimensions, as well as algebraic constructions that will be valid for infinite Boolean function sizes.},
  archive   = {C_PPSN},
  author    = {Carlet, Claude and Ðurasevic, Marko and Jakobovic, Domagoj and Picek, Stjepan},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70085-9_27},
  month     = {9},
  pages     = {429-445},
  title     = {Discovering rotation symmetric self-dual bent functions with evolutionary algorithms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Using evolutionary algorithms for the search of 16-variable
weight-wise perfectly balanced boolean functions with high
non-linearity. <em>PPSN</em>, 416–428. (<a
href="https://doi.org/10.1007/978-3-031-70085-9_26">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {New methods to construct adequate Boolean functions for use in cryptography have had to evolve in accordance with the requirements of continually proposed cryptosystems. Among the desired properties in Boolean functions are balancedness, high non-linearity, algebraic immunity, and resilience, all of which contribute to making the cryptosystem more resistant to various attacks. In 2016, the FLIP steam-cipher was proposed, which requires weight-wise perfectly balanced Boolean functions. As the field of cryptography evolves, so does the need for new methods of constructing Boolean functions. Our research contributes to this ongoing exploration. Evolutionary algorithms are among the explored approaches. However, much of the work has focused on 8-variable functions. In this investigation, previous work done on 8-variable functions is revisited and applied to the search for 16-variable WPB Boolean functions. The investigation yielded promising results, finding 16-variable WPB Boolean functions with high general non-linearity and weight-wise non-linearities that surpassed previous results. This marks a significant advancement in the exploration of EAs’ potential in cryptography.},
  archive   = {C_PPSN},
  author    = {Mandujano, Sara and Lara, Adriana and Ku Cauich, Juan Carlos},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70085-9_26},
  month     = {9},
  pages     = {416-428},
  title     = {Using evolutionary algorithms for the search of 16-variable weight-wise perfectly balanced boolean functions with high non-linearity},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evolution-based feature selection for predicting dissolved
oxygen concentrations in lakes. <em>PPSN</em>, 398–415. (<a
href="https://doi.org/10.1007/978-3-031-70085-9_25">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate prediction of dissolved oxygen (DO) concentrations in lakes requires a comprehensive study of phenological patterns across ecosystems, highlighting the need for precise selection of interactions amongst external factors and internal physical-chemical-biological variables. This paper presents the Multi-population Cognitive Evolutionary Search (MCES), a novel evolutionary algorithm for complex feature interaction selection problems. MCES allows models within every population to evolve adaptively, selecting relevant feature interactions for different lake types and tasks. Evaluated on diverse lakes in the Midwestern USA, MCES not only consistently produces accurate predictions with few observed labels but also, through gene maps of models, reveals sophisticated phenological patterns of different lake types, embodying the innovative concept of “AI from nature, for nature”.},
  archive   = {C_PPSN},
  author    = {Yu, Runlong and Ladwig, Robert and Xu, Xiang and Zhu, Peijun and Hanson, Paul C. and Xie, Yiqun and Jia, Xiaowei},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70085-9_25},
  month     = {9},
  pages     = {398-415},
  title     = {Evolution-based feature selection for predicting dissolved oxygen concentrations in lakes},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EvoVec: Evolutionary image vectorization with adaptive curve
number and color gradients. <em>PPSN</em>, 383–397. (<a
href="https://doi.org/10.1007/978-3-031-70085-9_24">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vector and raster graphics are the two main types of 2D images used in computer graphics. Raster graphics are images consisting of pixels (dots); vector images are created using mathematical objects such as lines, curves, and shapes. The main advantage of vector graphics is that they can be scaled without loss of quality, which is useful for advertising, design, frontend development, and other fields of application. At the moment, the issue of vectorization (conversion from raster to vector graphics) has not been fully resolved. There are two main approaches: deterministic algorithms and machine learning-based algorithms. Both of these types are not able to work with a color gradient and have other disadvantages, such as artifacts for deterministic algorithms, and extremely long working time and predefined curve number for machine learning-based algorithms. To solve the problems of existing solutions, we propose an evolutionary algorithm for image vectorization. Its main idea is to iteratively improve vector images using mutations and crossover. The proposed algorithm does not require any necessary parameters other than the original image and can process color gradients. The results of comparison with existing solutions show that our algorithm qualitatively and quickly vectorize images. Particularly, our approach outperforms others in terms of pixel-by-pixel MSE by $$15\%$$ . The implementation is publicly available ( https://github.com/EgorBa/EvoVec-Evolutionary-Image-Vectorization ).},
  archive   = {C_PPSN},
  author    = {Bazhenov, Egor and Jarsky, Ivan and Efimova, Valeria and Muravyov, Sergey},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70085-9_24},
  month     = {9},
  pages     = {383-397},
  title     = {EvoVec: Evolutionary image vectorization with adaptive curve number and color gradients},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the multi-objective optimization of wind farm cable
layouts with regard to cost and robustness. <em>PPSN</em>, 367–382. (<a
href="https://doi.org/10.1007/978-3-031-70085-9_23">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Offshore wind farms (OWFs) have emerged as a vital component in the transition to renewable energy, especially for countries like the United Kingdom with abundant shallow coastal waters suitable for wind energy exploitation. As net-zero emissions targets propel investments in renewables, OWFs present unique engineering challenges, particularly in the design of cost-effective and efficient infrastructural networks such as layout and electrical system optimization. Diverging from the previous approaches in electrical system optimization for OWFs, this paper introduces network robustness as a pivotal metric in design evaluations, differing from traditional reliability evaluation focused studies. By designing approximate solutions to the capacitated minimum spanning tree (CMST) using an approach grounded in a radial space partitioning strategy, the application of the Non-dominated Sorting Genetic Algorithm II (NSGA-II), and a bespoke domain-specific mutation operator, we present a multi-objective exploration of the cost-robustness trade-off. To demonstrate the effectiveness of our approach and its ability to offer decision makers valuable insight on cable layout designs, we apply it to a real-world case study that considers the Anholt OWF. The obtained results indicate the ability of our approach to discover sets of high-quality solutions, underscoring its potential to enhance the strategic development of robust and economically viable OWF networks.},
  archive   = {C_PPSN},
  author    = {Christie, Lee A. and Sahin, Atakan and Ogunsemi, Akinola and Zăvoianu, Alexandru-Ciprian and McCall, John A. W.},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70085-9_23},
  month     = {9},
  pages     = {367-382},
  title     = {On the multi-objective optimization of wind farm cable layouts with regard to cost and robustness},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attacker-defender strategy optimization using
multi-objective competitive co-evolution. <em>PPSN</em>, 351–366. (<a
href="https://doi.org/10.1007/978-3-031-70085-9_22">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Attacker-defender strategy optimization deals with optimizing and deciding on different tactics used by two independent entities working in tandem. Unlike in standard optimization problems, a complete solution of the entire two-agent problem consists of strategies of both agents and evaluation of a solution requires precise information of both strategies. For this reason, a co-evolutionary optimization framework is proposed in this paper to keep two co-evolving populations interacting with each other in tandem to reach their optimal strategies. While co-evolutionary algorithms have been proposed in the past, multi-objective co-evolutionary problems make the optimization task more complex, resulting in a set of Pareto-optimal strategies for each entity. In this paper, we apply a multi-objective competitive co-evolutionary optimization algorithm to a real-world wargame strategy optimization problem. The proposed co-evolutionary algorithm is used to find trade-off sets of competitive wargame strategies for both entities and a novel post-optimization decision-making procedure is also proposed to choose preferred strategies for each entity in tandem, leading to a stable or a cycle of sequential strategies. To the best of our knowledge, this paper marks one of the first-ever applications of multi-objective, competitive, co-evolutionary optimization approaches to a real-world wargame scenario, revealing their impact and importance in practice.},
  archive   = {C_PPSN},
  author    = {Guha, Ritam and Mckendrick, Ryan and Feest, Bradley and Deb, Kalyanmoy},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70085-9_22},
  month     = {9},
  pages     = {351-366},
  title     = {Attacker-defender strategy optimization using multi-objective competitive co-evolution},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Satellite resource scheduling: Compaction strategies for
genetic algorithm schedulers. <em>PPSN</em>, 335–350. (<a
href="https://doi.org/10.1007/978-3-031-70085-9_21">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The United States Naval Research Laboratory is currently using permutation-based genetic algorithms for large-scale satellite resource scheduling. This is a real-world, deployed application. The permutations must be mapped to a Gantt chart representing the final schedule. How this mapping is done can have a significant impact on the ability of the search algorithm to discover high-quality solutions. We present new work that uses compaction strategies in combination with genetic algorithms to construct less fragmented schedules. A schedule with “fewer holes” should also translate into better resource utilization. We show that this is indeed the case. This work is impactful because this strategy can be used to improve all genetic algorithm schedulers .},
  archive   = {C_PPSN},
  author    = {Whitley, Darrell and de Carvalho, Ozeas Quevedo and Roberts, Mark and Shetty, Vivint and Jampathom, Piyabutra},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70085-9_21},
  month     = {9},
  pages     = {335-350},
  title     = {Satellite resource scheduling: Compaction strategies for genetic algorithm schedulers},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving continuous monte carlo tree search for identifying
parameters in hybrid gene regulatory networks. <em>PPSN</em>, 319–334.
(<a href="https://doi.org/10.1007/978-3-031-70085-9_20">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Monte-Carlo Tree Search (MCTS) is largely responsible for the improvement not only of many computer games, including Go and General Game Playing (GPP), but also of real-world continuous Markov decision process problems. MCTS initially uses the Upper Confidence bounds applied to Trees (UCT), but the Rapid Action Value Estimation (RAVE) heuristic has rapidly taken over in the discrete and continuous domains. Recently, generalized RAVE (GRAVE) outperformed such heuristics in the discrete domain. This paper is concerned with extending the GRAVE heuristic to continuous action and state spaces (cGRAVE). To enhance its performance, we suggest an action decomposition strategy to break down multidimensional actions into multiple unidimensional actions, and we propose a selective policy based on constraints that bias the playouts and select promising actions in the search tree. The approach is experimentally validated on a real-world biological problem: the goal is to identify the continuous parameters of gene regulatory networks (GRNs).},
  archive   = {C_PPSN},
  author    = {Michelucci, Romain and Pallez, Denis and Cazenave, Tristan and Comet, Jean-Paul},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70085-9_20},
  month     = {9},
  pages     = {319-334},
  title     = {Improving continuous monte carlo tree search for identifying parameters in hybrid gene regulatory networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pareto landscape: Visualising the landscape of
multi-objective optimisation problems. <em>PPSN</em>, 299–315. (<a
href="https://doi.org/10.1007/978-3-031-70085-9_19">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fitness landscape is a valuable framework to understand optimisation problems. In single-objective optimisation, by displaying fitness landscape in a 3D space with the “height” representing the fitness (objective function value) of solutions, one can easily comprehend a variety of problem characteristics (optimality, multi-modality, level of ruggedness, etc.) and spatial features of the search space (basin, ridge, funnel, etc.). However, such straightforward visualisation cannot be directly extended to the multi-objective optimisation case in which a solution corresponds to a vector of values on multiple objective functions. In this paper, we make an attempt to address this issue. Instead of objective function values, we use the Pareto dominance relation to stratify solutions, introducing a method we term Pareto landscape for visualising multi-objective problem landscape. We compare Pareto landscape with well-established fitness landscape visualisation methods, including cost landscape, gradient field heatmap and PLOT, and show that Pareto landscape can capture problem characteristics that the other methods cannot do. Lastly, we present the Pareto landscapes of commonly used benchmark problems (ZDT, DTLZ, WFG and BBOB) in the domain, and discuss their features and characteristics.},
  archive   = {C_PPSN},
  author    = {Liang, Zimin and Cui, Zhiji and Li, Miqing},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70085-9_19},
  month     = {9},
  pages     = {299-315},
  title     = {Pareto landscape: Visualising the landscape of multi-objective optimisation problems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reliability of indicator-based comparison results of
evolutionary multi-objective algorithms. <em>PPSN</em>, 285–298. (<a
href="https://doi.org/10.1007/978-3-031-70085-9_18">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In evolutionary multi-objective optimization (EMO), performance indicators are often used to measure the quality of non-dominated solution sets obtained by EMO algorithms. However, the reliability of the performance indicators has not been well studied. In this paper, we compare the quality of non-dominated solution sets using four performance indicators: hypervolume (HV), inverted generational distance (IGD), inverted generational distance+ (IGD+), and additive epsilon ( $$\epsilon _{+}$$ ). Our experimental results show that different performance indicators produce similar results when they are applied to commonly-used benchmark test problems such as DTLZ1 and DTLZ2. However, for real-world problems, we obtained significantly different comparison results from these indicators. Even when we use the same HV indicator, we obtain significantly different results depending on the reference point specifications. These observations suggest the importance of the choice of an indicator for performance comparison of EMO algorithms on real-world problems. When the HV indicator is used, the choice of a reference point is also important. Moreover, our observations suggest the necessity of using multiple indicators (including the HV indicator with multiple reference points) to obtain reliable performance comparison results.},
  archive   = {C_PPSN},
  author    = {Pang, Lie Meng and Ishibuchi, Hisao and Nan, Yang and Gong, Cheng},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70085-9_18},
  month     = {9},
  pages     = {285-298},
  title     = {Reliability of indicator-based comparison results of evolutionary multi-objective algorithms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable quantum approximate optimiser for pseudo-boolean
multi-objective optimisation. <em>PPSN</em>, 268–284. (<a
href="https://doi.org/10.1007/978-3-031-70085-9_17">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Quantum computation uses quantum mechanical principles to reach beyond-classical computational power. This has endless applications, especially in optimisation-problems’ solving. Most of today’s quantum optimisers, more specifically, Quantum Approximate Optimisation Algorithm (QAOA), were originally designed to solve single-objective problems, although real-life scenarios include generally dealing with multiple objectives. Very preliminary literature with design/implementation limitations has been done in this sense. This makes dealing with such limitations and expanding the QAOA applicability to multi-objective optimisation an important step towards advancing quantum computation. To do so, this work presents a decomposition-based Multi-Objective QAOA (MO-QAOA) able to solve multi-objective problems. The proposal’s design explores QAOA’s features considering the error-prone and limited nature of today’s quantum computers as well as the costly quantum simulation. This work’s contributions stand in designing both, (I) sequential and parallel MO-QAOA, based on (II) weighted-sum and Tchebycheff scalarisation, by (III) exploring the QAOA’s parameters’ transference. The validation has been done using 2, 3 and 4-objectives problems of several sizes/complexities/types, using up to 2000 slaves/jobs running quantum computer simulators, as well as three real IBM 127-qubits’ quantum computers. The results show up to 89% execution-time decrease, which supports the applicability/reliability of the proposal in today’s time-constrained and error-prone quantum computers.},
  archive   = {C_PPSN},
  author    = {Dahi, Zakaria Abdelmoiz and Chicano, Francisco and Luque, Gabriel and Derbel, Bilel and Alba, Enrique},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70085-9_17},
  month     = {9},
  pages     = {268-284},
  title     = {Scalable quantum approximate optimiser for pseudo-boolean multi-objective optimisation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reaching pareto front shape invariance with a continuous
multi-objective ant colony optimization algorithm. <em>PPSN</em>,
252–267. (<a
href="https://doi.org/10.1007/978-3-031-70085-9_16">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generating Pareto Front Approximations with good convergence, uniformity, and spread regardless of the geometry of the Pareto Front remains as an open problem. Many Multi-Objective Evolutionary Algorithms (MOEAs) have been proposed for this aim achieving remarkable results. However, the utilization of Swarm Intelligence algorithms such as Multi-Objective Ant Colony Optimization Algorithms (MOACOs) has been scarcely studied. In this paper, we propose a Geometric-Invariant $$\text {MOACO}_\mathbb {R}$$ ( $$\text {GI-MOACO}_\mathbb {R}$$ ) designed to tackle multi-objective optimization problems with a continuous decision space. According to our experimental results, $$\text {GI-MOACO}_\mathbb {R}$$ outperforms the existing MOACOs for continuous search spaces and it is competitive with respect to state-of-the-art MOEAs on several test suites with regular and irregular Pareto Front geometries. To the best of the author’s knowledge, $$\text {GI-MOACO}_\mathbb {R}$$ is the first Pareto-Front-Shape invariant MOACO.},
  archive   = {C_PPSN},
  author    = {Tamayo, Rodolfo Humberto and Falcón-Cardona, Jesús Guillermo and Coello Coello, Carlos A.},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70085-9_16},
  month     = {9},
  pages     = {252-267},
  title     = {Reaching pareto front shape invariance with a continuous multi-objective ant colony optimization algorithm},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Biased pareto optimization for subset selection with dynamic
cost constraints. <em>PPSN</em>, 236–251. (<a
href="https://doi.org/10.1007/978-3-031-70085-9_15">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Subset selection with cost constraints aims to select a subset from a ground set to maximize a monotone objective function without exceeding a given budget, which has various applications such as influence maximization and maximum coverage. In real-world scenarios, the budget, representing available resources, may change over time, which requires that algorithms must adapt quickly to new budgets. However, in this dynamic environment, previous algorithms either lack theoretical guarantees or require a long running time. The state-of-the-art algorithm, POMC, is a Pareto optimization approach designed for static problems, lacking consideration for dynamic problems. In this paper, we propose BPODC, enhancing POMC with biased selection and warm-up strategies tailored for dynamic environments. We focus on the ability of BPODC to leverage existing computational results while adapting to budget changes. We prove that BPODC can maintain the best known $$(\alpha _f/2)(1-e^{-\alpha _f})$$ -approximation guarantee when the budget changes. Experiments on influence maximization and maximum coverage show that BPODC adapts more effectively and rapidly to budget changes, with a running time that is less than that of the static greedy algorithm.},
  archive   = {C_PPSN},
  author    = {Liu, Dan-Xuan and Qian, Chao},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70085-9_15},
  month     = {9},
  pages     = {236-251},
  title     = {Biased pareto optimization for subset selection with dynamic cost constraints},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Influence maximization in hypergraphs using multi-objective
evolutionary algorithms. <em>PPSN</em>, 217–235. (<a
href="https://doi.org/10.1007/978-3-031-70085-9_14">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Influence Maximization (IM) problem is a well-known NP-hard combinatorial problem over graphs whose goal is to find the seed set of nodes in a network that spreads influence at most. Among the various methods for solving the IM problem, evolutionary algorithms (EAs) have been shown to be particularly effective. While the literature on the topic is particularly ample, only a few attempts have been made at solving the IM problem over higher-order networks, namely extensions of standard graphs that can capture interactions that involve more than two nodes. Hypergraphs are a valuable tool for modeling complex interaction networks in various domains; however, they require rethinking of several graph-based problems, including IM. In this work, we propose a multi-objective EA for the IM problem over hypergraphs, aiming at minimizing the seed set size while maximizing influence. Smart initialization and hypergraph-aware mutation operators are utilized to facilitate algorithm convergence. While the existing methods rely on greedy or heuristic methods, to our best knowledge this is the first attempt at applying EAs to this problem. Our results over nine real-world datasets and three propagation models, compared with five baseline algorithms, reveal that our method achieves in most cases state-of-the-art results in terms of hypervolume and solution diversity.},
  archive   = {C_PPSN},
  author    = {Genetti, Stefano and Ribaga, Eros and Cunegatti, Elia and Lotito, Quintino F. and Iacca, Giovanni},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70085-9_14},
  month     = {9},
  pages     = {217-235},
  title     = {Influence maximization in hypergraphs using multi-objective evolutionary algorithms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinvestigating the r2 indicator: Achieving pareto
compliance by integration. <em>PPSN</em>, 202–216. (<a
href="https://doi.org/10.1007/978-3-031-70085-9_13">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In multi-objective optimization, set-based quality indicators are a cornerstone of benchmarking and performance assessment. They capture the quality of a set of trade-off solutions by reducing it to a scalar number. One of the most commonly used set-based metrics is the R2 indicator, which describes the expected utility of a solution set to a decision-maker under a distribution of utility functions. Typically, this indicator is applied by discretizing this distribution of utility functions, yielding a weakly Pareto-compliant indicator. In consequence, adding a nondominated or dominating solution to a solution set may – but does not have to – improve the indicator’s value. In this paper, we reinvestigate the R2 indicator under the premise that we have a continuous, uniform distribution of (Tchebycheff) utility functions. We analyze its properties in detail, demonstrating that this continuous variant is indeed Pareto-compliant – that is, any beneficial solution will improve the metric’s value. Additionally, we provide an efficient computational procedure to compute this metric for bi-objective problems in $$\mathcal O (N \log N)$$ . As a result, this work contributes to the state-of-the-art Pareto-compliant unary performance metrics, such as the hypervolume indicator, offering an efficient and promising alternative.},
  archive   = {C_PPSN},
  author    = {Schäpermeier, Lennart and Kerschke, Pascal},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70085-9_13},
  month     = {9},
  pages     = {202-216},
  title     = {Reinvestigating the r2 indicator: Achieving pareto compliance by integration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An unbounded archive-based inverse model in evolutionary
multi-objective optimization. <em>PPSN</em>, 186–201. (<a
href="https://doi.org/10.1007/978-3-031-70085-9_12">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inverse model (IM) is a method for tailoring solutions to decision-makers based on their preferences. Existing approaches are often trained by the final solution set (as training data) obtained from a multi-objective evolutionary algorithm (MOEA). The final solution set obtained by MOEA usually has a limited number of samples. However, model training will perform poorly when there are few samples in the final solution set. To further improve the performance of the model, we propose an unbounded archive-based inverse model (UAIM) to enhance the quality of the trained inverse model. We first create an unbounded archive to collect all non-dominated solutions during the execution of MOEA. Unlike IM, UAIM is trained using all solutions in the archive. Moreover, for a decision maker’s preference, an alternative solution from the archive is considered if the suggested solution is inferior to the alternative solution in the archive. UAIM thus may provide more reliable suggested solutions for decision-makers. To better evaluate algorithms, we propose two indicators that can measure the matching degree between the suggested solution and the decision maker’s preference. We demonstrate that the proposed UAIM is superior to IM on ten problems.},
  archive   = {C_PPSN},
  author    = {Ye, Rongguang and Chen, Longcan and Zhang, Jinyuan and Ishibuchi, Hisao},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70085-9_12},
  month     = {9},
  pages     = {186-201},
  title     = {An unbounded archive-based inverse model in evolutionary multi-objective optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-objective random bit climbers with weighted
permutation on large scale binary MNK-landscapes. <em>PPSN</em>,
169–185. (<a
href="https://doi.org/10.1007/978-3-031-70085-9_11">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-Objective Evolutionary Algorithms have proven to be very effective when solving Multi-Objective Optimization Problems. However, their performance decreases significantly when solving large scale problems, which can have hundreds or thousands of variables. Although several algorithms have been proposed to tackle this problem in the recent years, most of them are designed for continuous problems, and only a few focus on binary ones. In this paper, we propose a modification to multi-objective random one-bit climbers that achieves better performance in large scale binary problems by learning the trend of the values of the decision variables from previously found solutions and applying that information to decide which ones to focus on when executing the bit climb. We present the implemented algorithm, compare its performance to other well known evolutionary algorithms and study some of its properties.},
  archive   = {C_PPSN},
  author    = {Ide, Felipe Honjo and Aguirre, Hernan and Tanaka, Kiyoshi},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70085-9_11},
  month     = {9},
  pages     = {169-185},
  title     = {Multi-objective random bit climbers with weighted permutation on large scale binary MNK-landscapes},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Near-tight runtime guarantees for many-objective
evolutionary algorithms. <em>PPSN</em>, 153–168. (<a
href="https://doi.org/10.1007/978-3-031-70085-9_10">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite significant progress in the field of mathematical runtime analysis of multi-objective evolutionary algorithms (MOEAs), the performance of MOEAs on discrete many-objective problems is little understood. In particular, the few existing bounds for the SEMO, global SEMO, and SMS-EMOA algorithms on classic benchmarks are all roughly quadratic in the size of the Pareto front. In this work, we prove near-tight runtime guarantees for these three algorithms on the four most common benchmark problems OneMinMax, CountingOnesCountingZeros, LeadingOnesTrailingZeros, and OneJumpZeroJump, and this for arbitrary numbers of objectives. Our bounds depend only linearly on the Pareto front size, showing that these MOEAs on these benchmarks cope much better with many objectives than what previous works suggested. Our bounds are tight apart from small polynomial factors in the number of objectives and length of bitstrings. This is the first time that such tight bounds are proven for many-objective uses of these MOEAs. While it is known that such results cannot hold for the NSGA-II, we do show that our bounds, via a recent structural result, transfer to the NSGA-III algorithm.},
  archive   = {C_PPSN},
  author    = {Wietheger, Simon and Doerr, Benjamin},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70085-9_10},
  month     = {9},
  pages     = {153-168},
  title     = {Near-tight runtime guarantees for many-objective evolutionary algorithms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian forward-inverse transfer for multiobjective
optimization. <em>PPSN</em>, 135–152. (<a
href="https://doi.org/10.1007/978-3-031-70085-9_9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an evolutionary optimizer incorporating knowledge transfer through forward and inverse surrogate models for solving multiobjective problems, within a stringent computational budget. Forward knowledge transfer is employed to fully exploit solution-evaluation datasets from related tasks by building Bayesian forward multitask surrogate models that map points from decision to objective space. Inverse knowledge transfer via Bayesian inverse multitask models makes possible the creation of high-quality solution populations in decision space by mapping back from preferred points in objective space. In contrast to prior work, the proposed method can improve the overall convergence performance to multiple Pareto sets by fully exploiting information available for diverse multiobjective problems. Empirical studies conducted on benchmark and real-world multitask multiobjective optimization problems demonstrate the faster convergence rate and enhanced inverse modeling accuracy of our algorithm compared to state-of-the-art algorithms.},
  archive   = {C_PPSN},
  author    = {Wei, Tingyang and Liu, Jiao and Gupta, Abhishek and Tan, Puay Siew and Ong, Yew-Soon},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70085-9_9},
  month     = {9},
  pages     = {135-152},
  title     = {Bayesian forward-inverse transfer for multiobjective optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evolutionary multi-objective diversity optimization.
<em>PPSN</em>, 117–134. (<a
href="https://doi.org/10.1007/978-3-031-70085-9_8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Creating diverse sets of high-quality solutions has become an important problem in recent years. Previous works on diverse solutions problems consider solutions’ objective quality and diversity where one is regarded as the optimization goal and the other as the constraint. In this paper, we treat this problem as a bi-objective optimization problem, which is to obtain a range of quality-diversity trade-offs. To address this problem, we frame the evolutionary process as evolving a population of populations, and present a suitable general implementation scheme that is compatible with existing evolutionary multi-objective search methods. We realize the scheme in NSGA-II and SPEA2, and test the methods on various instances of maximum coverage, maximum cut and minimum vertex cover problems. The resulting non-dominated populations exhibit rich qualitative features, giving insights into the optimization instances and the quality-diversity trade-offs they induce.},
  archive   = {C_PPSN},
  author    = {Do, Anh Viet and Guo, Mingyu and Neumann, Aneta and Neumann, Frank},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70085-9_8},
  month     = {9},
  pages     = {117-134},
  title     = {Evolutionary multi-objective diversity optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Innovization for route planning applied to an uber movement
speeds dataset for berlin. <em>PPSN</em>, 100–116. (<a
href="https://doi.org/10.1007/978-3-031-70085-9_7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-objective route planning is a prominent but computationally expensive optimisation problem of everyday life. Reusing knowledge from similar route planning problems could enhance the performance and the sustainability of routing algorithms. The goal of this paper is to adapt the concept of innovization to route planning and in this way extract knowledge from Pareto-optimal solutions. As part of the adaptation, we design a multi-objective evolutionary algorithm for routing and introduce a novel local search for routing problems called Perimeter Mutation Local Search. We evaluate our proposed approach on multi-objective time-dependent routing problems to see what knowledge can be gained and whether this knowledge can improve a multiobjective evolutionary algorithm. Our results show that we can extract knowledge using the introduced innovization for route planning. This knowledge is used to improve a multiobjective evolutionary algorithm by reducing computational effort. With only about 40 % of previously necessary function evaluations, we manage to produce similar optimisation results. This is particularly beneficial for mobile applications with limited available computational resources.},
  archive   = {C_PPSN},
  author    = {Röper, Eva and Weise, Jens and Steup, Christoph and Mostaghim, Sanaz},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70085-9_7},
  month     = {9},
  pages     = {100-116},
  title     = {Innovization for route planning applied to an uber movement speeds dataset for berlin},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Solution-based knowledge discovery for multi-objective
optimization. <em>PPSN</em>, 83–99. (<a
href="https://doi.org/10.1007/978-3-031-70085-9_6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the combinatorial optimization field, Knowledge Discovery (KD) mechanisms (e.g., data mining, neural networks) have received increasing interest over the years. KD mechanisms are based upon two main procedures, being the extraction of knowledge from solutions, and the injection of such knowledge into solutions. However, in a multi-objective (MO) context, the simultaneous optimization of many conflicting objectives can lead to the learning of contradictory knowledge. We propose to develop a Solution-based KD (SKD) mechanism suited to MO optimization. It is integrated within two existing metaheuristics: the Iterated MO Local Search (IMOLS) and the MO Evolutionary Algorithm based on Decomposition (MOEA/D). As a case study, we consider a bi-objective Vehicle Routing Problem with Time Windows (bVRPTW), to define accordingly the problem-dependent knowledge of the SKD mechanism. Our experiments show that using the KD mechanism we propose increases the performance of both IMOLS and MOEA/D algorithms.},
  archive   = {C_PPSN},
  author    = {Legrand, Clément and Cattaruzza, Diego and Jourdan, Laetitia and Kessaci, Marie-Eléonore},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70085-9_6},
  month     = {9},
  pages     = {83-99},
  title     = {Solution-based knowledge discovery for multi-objective optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Many-objective cover problem: Discovering few solutions to
cover many objectives. <em>PPSN</em>, 68–82. (<a
href="https://doi.org/10.1007/978-3-031-70085-9_5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many-objective optimization (MaO) is a basic issue in various research areas. Although Pareto optimality is a common criterion for MaO, it may bring many troubles when facing a huge number (e.g., up to 100) of objectives. This paper provides a new perspective on MaO by introducing a many-objective cover problem (MaCP). Given m objectives, MaCP aims to find a solution set with size k ( $$1 &lt; k \ll m$$ ) to cover all objectives (i.e., each objective can be approximately optimized by at least one solution in this set). We prove the NP-hard property of MaCP and develop a clustering-based swarm optimizer (CluSO) with a convergence guarantee to tackle MaCP. Then, we propose a decoupling many-objective test suite (DC-MaTS) with practical significance and use it to evaluate CluSO. Extensive experimental results on various test problems with up to 100 objectives demonstrate both the efficiency and effectiveness of CluSO, while also illustrating that MaCP is a feasible perspective on MaO.},
  archive   = {C_PPSN},
  author    = {Liu, Yilu and Lu, Chengyu and Lin, Xi and Zhang, Qingfu},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70085-9_5},
  month     = {9},
  pages     = {68-82},
  title     = {Many-objective cover problem: Discovering few solutions to cover many objectives},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Three objectives degrade the convergence ability of
dominance-based multi-objective evolutionary algorithms. <em>PPSN</em>,
52–67. (<a href="https://doi.org/10.1007/978-3-031-70085-9_4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the evolutionary multi-objective optimization (EMO) community, it is well known that the convergence ability of dominance-based multi-objective evolutionary algorithms (MOEAs) is severely deteriorated on many-objective problems with more than three objectives. In this paper, we clearly demonstrate that the convergence ability of NSGA-II deteriorates even in the case of three objectives. Our experimental results on multi-objective knapsack and traveling salesman problems with 2–6 objectives show that NSGA-II starts to deteriorate the quality of the current population after a number of generations even when it is applied to three-objective problems. Surprisingly, NSGA-III also shows a similar performance deterioration. We analyze the search behavior of NSGA-II, NSGA-III, three versions of MOEA/D, and SMS-EMOA. Then, we explain the reason for the performance deterioration of NSGA-II and NSGA-III, which exists in the environmental selection mechanism of each algorithm. Another interesting observation is that NSGA-II has the best or second best performance (next to MOEA/D with the weighted sum) among the examined algorithms on many-objective problems in early generations before it starts to show performance deterioration.},
  archive   = {C_PPSN},
  author    = {Gong, Cheng and Pang, Lie Meng and Zhang, Qingfu and Ishibuchi, Hisao},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70085-9_4},
  month     = {9},
  pages     = {52-67},
  title     = {Three objectives degrade the convergence ability of dominance-based multi-objective evolutionary algorithms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LTR-HSS: A learning-to-rank based framework for hypervolume
subset selection. <em>PPSN</em>, 36–51. (<a
href="https://doi.org/10.1007/978-3-031-70085-9_3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hypervolume subset selection (HSS) plays an important role in various aspects of the field of evolutionary multi-objective optimization, such as environmental selection and post-processing for decision-making. The goal of these problems is to find the optimal subset that maximizes the hypervolume from a given candidate solution set. Many methods have been developed to solve or approximately solve different types of HSS problems. However, existing approaches cannot effectively solve HSS problems with a large number of objectives within a short computation time. This drawback directly limits their applicability as a component for developing new EMO algorithms. In this paper, we propose a novel learning-to-rank based framework, named LTR-HSS, for solving the challenging HSS problems with a large number of objectives. The experimental results show that, compared to other state-of-the-art HSS methods, our proposed LTR-HSS requires a shorter computation time to solve HSS problems with large numbers of objectives while achieving superior or competitive hypervolume performance. This demonstrates the potential of our method to be integrated into algorithms for many-objective optimization.},
  archive   = {C_PPSN},
  author    = {Gong, Cheng and Guo, Ping and Shu, Tianye and Zhang, Qingfu and Ishibuchi, Hisao},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70085-9_3},
  month     = {9},
  pages     = {36-51},
  title     = {LTR-HSS: A learning-to-rank based framework for hypervolume subset selection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hypervolume gradient subspace approximation. <em>PPSN</em>,
20–35. (<a href="https://doi.org/10.1007/978-3-031-70085-9_2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-objective evolutionary algorithms (MOEAs) are powerful optimizers that are capable of solving black-box multi-objective optimization problems. Due to their stochastic nature, local search methods, including directed search algorithms, have been proposed to guide search directions in the decision variable space. In particular, recent studies have shown that the inclusion of local hypervolume-based gradient methods can lead to better convergence rates. In this paper, a set-based method of estimating hypervolume gradients without additional function evaluations or Jacobian information is proposed and integrated with SMS-EMOA to form a steady-state MOEA. The proposed algorithm is compared to some widely-used MOEAs on two- and three-objective benchmark suites, outperforming all other algorithms on all 6/6 two-objective problems and 12/17 three-objective problems.},
  archive   = {C_PPSN},
  author    = {Zhang, Kenneth and Rodriguez-Fernandez, Angel E. and Shang, Ke and Ishibuchi, Hisao and Schütze, Oliver},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70085-9_2},
  month     = {9},
  pages     = {20-35},
  title     = {Hypervolume gradient subspace approximation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Selection strategy based on proper pareto optimality in
evolutionary multi-objective optimization. <em>PPSN</em>, 3–19. (<a
href="https://doi.org/10.1007/978-3-031-70085-9_1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {On the multi-objective optimization problems (MOP), the dominance-resistant solution (DRS) refers to the solution that has inferior objective values but is difficult to dominate by other solutions. Prior studies have affirmed that DRSs are prevalent across MOPs and difficult to eliminate, leading to substantial performance deterioration in many multi-objective evolutionary algorithms (MOEAs). In this paper, we propose a metric inspired by proper Pareto optimality and then develop a selection strategy based on this metric (SPP) to mitigate the negative impact of DRSs. Furthermore, we implement SPP on multi-objective evolutionary algorithm based on decomposition (MOEA/D) and call the new algorithm MOEA/D-SPP. Specifically, the algorithm employs the penalty-based boundary intersection method to scalarize the MOP. Subsequently, SPP is integrated into the environmental selection. The strategy measures and sorts a set of solutions such that DRSs can be identified and removed. Finally, weight vectors are adjusted, thereby enhancing the population diversity. In experimental studies, MOEA/D-SPP outperforms five state-of-the-art MOEAs on DRS-MOPs, demonstrating the promising application of SPP.},
  archive   = {C_PPSN},
  author    = {Li, Kai and Lin, Kangnian and Zheng, Ruihao and Wang, Zhenkun},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70085-9_1},
  month     = {9},
  pages     = {3-19},
  title     = {Selection strategy based on proper pareto optimality in evolutionary multi-objective optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring proprioceptive feedback in the evolution of
modular robots. <em>PPSN</em>, 405–418. (<a
href="https://doi.org/10.1007/978-3-031-70071-2_25">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate an evolvable robot system where the body provides proprioceptive sensory signals to the controller (brain) about the positions of the joints. The key aspect we consider is whether all joints should be sensed or if sensing fewer joints would be better. We research this matter based on a test suite of twenty-two robots with various shapes and sizes and implement a system where the controller and the sensory signal system evolve together. Experiments with this system show that the evolved solutions use signals only from a fraction of the joints (25–51%) and perform better than the baseline, where all signals are used. This effect was observed across the majority of the test suite.},
  archive   = {C_PPSN},
  author    = {Hosseinkhani Kargar, Babak and Miras, Karine and Eiben, A. E.},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70071-2_25},
  month     = {9},
  pages     = {405-418},
  title     = {Exploring proprioceptive feedback in the evolution of modular robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A variable-length fuzzy set representation for learning
fuzzy-classifier systems. <em>PPSN</em>, 386–402. (<a
href="https://doi.org/10.1007/978-3-031-70071-2_24">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a novel Learning Fuzzy-Classifier System (LFCS) that incorporates variable-length fuzzy sets in rule antecedents to enhance classification accuracy and mitigate overfitting in real-world data scenarios. Traditional LFCSs utilize fixed-length fuzzy sets, which can limit their performance, especially when the rule set size is restricted in high-dimensional input space. The proposed algorithm, Fuzzy-UCSv (i.e., the Fuzzy-UCS classifier system with a variable-length fuzzy set representation), addresses these limitations by allowing the number of fuzzy sets per dimension in rule-antecedents to vary. Fuzzy-UCSv aims to tackle two primary challenges identified in LFCS: the unnecessary optimization of membership functions for irrelevant features and the difficulty in forming optimal classification boundaries with a single membership function per feature. By optimizing the number of membership functions for each rule using an evolutionary algorithm, Fuzzy-UCSv acquires rules that ignore non-contributing features and effectively cover complex input spaces, significantly improving test accuracy without increasing the risk of overfitting. Experimental results demonstrate that Fuzzy-UCSv outperforms conventional Fuzzy-UCS and other machine learning techniques in terms of test accuracy.},
  archive   = {C_PPSN},
  author    = {Shiraishi, Hiroki and Ye, Rongguang and Ishibuchi, Hisao and Nakata, Masaya},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70071-2_24},
  month     = {9},
  pages     = {386-402},
  title     = {A variable-length fuzzy set representation for learning fuzzy-classifier systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pareto-informed multi-objective neural architecture search.
<em>PPSN</em>, 369–385. (<a
href="https://doi.org/10.1007/978-3-031-70071-2_23">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Aiming at the auto-design of powerful neural architectures with a requirement of compromising multiple objectives, this paper introduces a novel approach called Pareto-informed Multi-objective Neural Architecture Search (PiMO-NAS), which employs a solution generator influenced by Tchebycheff decomposition to explore the objective space of multi-objective NAS. Our methodology initiates with a transformation of discrete search space into continuous form, followed by iterative solution optimization, in which Gaussian Process (GP) surrogate models are utilized to establish a mapping from decision space to objective space. Subsequently, a solution generator, directed by preference vectors from the objective space, is designed to generate decision vectors to map the weights from the objective space back to the decision space. This solution generator is further optimized based on the gradients derived from the GP models. To ensure diversity in the solution pool, the solution generator synthesizes new candidate solutions guided by preference vectors generated by a well-designed adaptive sampler. In order to verify the performance of the proposed PiMO-NAS, a series of experiments were conducted within two typical NAS search spaces (i.e., the Once-For-All(OFA) and AutoFormer based ones, covering both convolutional neural networks and vision transformers), and more than 30 state-of-the-art NAS methods and models were employed for performance comparisons. Experimental results showcase that our approach can outperform most peers in terms of search time and solution quality, and has fantastic ability to efficiently discover high-performing neural architectures. In the OFA-based search spaces, compared with the MSuNAS, the proposed PiMO-NAS was able to achieve similar performance in two-thirds of the number of iterations, thereby saving about 24% of the search time. In the AutoFormer-based search space, we successfully approached a strong baseline formed by a single-objective evolutionary algorithm with restricted parameter quantities, approximating the entire Pareto front in a comparable timeframe.},
  archive   = {C_PPSN},
  author    = {Luo, Ganyuan and Li, Hao and Chen, Zefeng and Zhou, Yuren},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70071-2_23},
  month     = {9},
  pages     = {369-385},
  title     = {Pareto-informed multi-objective neural architecture search},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning discretized bayesian networks with GOMEA.
<em>PPSN</em>, 352–368. (<a
href="https://doi.org/10.1007/978-3-031-70071-2_22">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bayesian networks model relationships between random variables under uncertainty and can be used to predict the likelihood of events and outcomes while incorporating observed evidence. From an eXplainable AI (XAI) perspective, such models are interesting as they tend to be compact. Moreover, captured relations can be directly inspected by domain experts. In practice, data is often real-valued. Unless assumptions of normality can be made, discretization is often required. The optimal discretization, however, depends on the relations modelled between the variables. This complicates learning Bayesian networks from data. For this reason, most literature focuses on learning conditional dependencies between sets of variables, called structure learning. In this work, we extend an existing state-of-the-art structure learning approach based on the Gene-pool Optimal Mixing Evolutionary Algorithm (GOMEA) to jointly learn variable discretizations. The proposed Discretizing Bayesian Network GOMEA (DBN-GOMEA) obtains similar or better results than the current state-of-the-art when tasked to retrieve randomly generated ground-truth networks. Moreover, leveraging a key strength of evolutionary algorithms, we can straightforwardly perform DBN learning multi-objectively. We show how this enables incorporating expert knowledge in a uniquely insightful fashion, finding multiple DBNs that trade-off complexity, accuracy, and the difference with a pre-determined expert network.},
  archive   = {C_PPSN},
  author    = {Ha, Damy M. F. and Alderliesten, Tanja and Bosman, Peter A. N.},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70071-2_22},
  month     = {9},
  pages     = {352-368},
  title     = {Learning discretized bayesian networks with GOMEA},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatic brain tumor segmentation using convolutional
neural networks: U-net framework with PSO-tuned hyperparameters.
<em>PPSN</em>, 333–351. (<a
href="https://doi.org/10.1007/978-3-031-70071-2_21">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate segmentation of brain tumors from magnetic resonance imaging (MRI) data is imperative for precise diagnosis and treatment planning. Manual segmentation, while accurate, is labor-intensive and subject to human error. In this study, we propose an innovative approach leveraging a modified convolutional neural network (CNN) architecture, U-Net, optimized using Particle Swarm Optimization (PSO) to tackle this challenge. Our method achieves significantly improved segmentation accuracy through pre-training hyperparameter tuning, particularly adjusting learning rates and dropout rates with PSO. Compared to existing methods, we observe enhancements of up to 4 p.p. in the Dice Similarity Coefficient (DSC) and 2 p.p. in the Jaccard Index (JI). Using skip connections and dropout layers in CNN-U-Net enables the effective capture of intricate features while mitigating overfitting, resulting in robust segmentation performance. Experimental results showcase the superiority of our approach across different tumor classes, including Meningioma, Glioma, and Pituitary, as well as overall, with maximum DSC and JI values of 94.14% and 89.02%, respectively. Comparative analysis against established techniques underscores the reliability and robustness of our proposed method. By demonstrating the efficacy of deep learning coupled with metaheuristic optimization in medical image segmentation, our study contributes to advancing the field’s understanding and applications. This research lays a foundation for future automated brain tumor segmentation developments, with implications for clinical practice and patient care.},
  archive   = {C_PPSN},
  author    = {Saifullah, Shoffan and Dreżewski, Rafał},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70071-2_21},
  month     = {9},
  pages     = {333-351},
  title     = {Automatic brain tumor segmentation using convolutional neural networks: U-net framework with PSO-tuned hyperparameters},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Population-based algorithms built on weighted automata.
<em>PPSN</em>, 315–332. (<a
href="https://doi.org/10.1007/978-3-031-70071-2_20">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many algorithms in natural computing and computational biology are population-based: genetic algorithms evolve candidate solutions for optimization problems; artificial immune systems and learning classifier systems maintain populations of rules. Using such algorithms at very large population sizes (e.g., millions or billions) is computationally expensive. Here, we develop a methodology for implementing population-based models using weighted finite state machines (WFSMs) with exact rational weights. For populations that can be represented as weighted sets of strings, WFSMs can reduce memory use and runtime of population-based algorithms by orders of magnitude. We demonstrate the generality of our approach by constructing an immune-inspired anomaly detector for string data and an evolutionary algorithm that solves Boolean satisfiability problems. The WFSM approach allows repurposing of advanced algorithms developed for natural language processing, and should be applicable to other population-based algorithms such as learning classifier systems.},
  archive   = {C_PPSN},
  author    = {Schröder, Gijs and Wortel, Inge and Textor, Johannes},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70071-2_20},
  month     = {9},
  pages     = {315-332},
  title     = {Population-based algorithms built on weighted automata},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A first running time analysis of the strength pareto
evolutionary algorithm 2 (SPEA2). <em>PPSN</em>, 295–312. (<a
href="https://doi.org/10.1007/978-3-031-70071-2_19">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Evolutionary algorithms (EAs) have emerged as a predominant approach for addressing multi-objective optimization problems. However, the theoretical foundation of multi-objective EAs (MOEAs), particularly the fundamental aspects like running time analysis, remains largely underexplored. Existing theoretical studies mainly focus on basic MOEAs, with little attention given to practical MOEAs. In this paper, we present a running time analysis of strength Pareto evolutionary algorithm 2 (SPEA2) for the first time. Specifically, we prove that the expected running time of SPEA2 for solving three commonly used multi-objective problems, i.e., mOneMinMax, mLeadingOnesTrailingZeroes, and m-OneJumpZeroJump, is $$O(\mu n\cdot \min \{m\log n, n\})$$ , $$O(\mu n^2)$$ , and $$O(\mu n^k \cdot \min \{mn, 3^{m/2}\})$$ , respectively. Here m denotes the number of objectives, and the population size $$\mu $$ is required to be at least $$(2n/m+1)^{m/2}$$ , $$(2n/m+1)^{m-1}$$ and $$(2n/m-2k+3)^{m/2}$$ , respectively. The proofs are accomplished through general theorems which are also applicable for analyzing the expected running time of other MOEAs on these problems, and thus can be helpful for future theoretical analysis of MOEAs.},
  archive   = {C_PPSN},
  author    = {Ren, Shengjie and Bian, Chao and Li, Miqing and Qian, Chao},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70071-2_19},
  month     = {9},
  pages     = {295-312},
  title     = {A first running time analysis of the strength pareto evolutionary algorithm 2 (SPEA2)},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). When does the time-linkage property help optimization by
evolutionary algorithms? <em>PPSN</em>, 280–294. (<a
href="https://doi.org/10.1007/978-3-031-70071-2_18">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent theoretical works show that the time-linkage property challenges evolutionary algorithms to optimize. Here we consider three positive circumstances and give the first runtime analyses to show that the time-linkage property can also help the optimization of evolutionary algorithms. The problem is easier to optimize if the time-linkage property changes the optimal function value to an easy-to-reach one. We construct a time-linkage variant of the $$\textsc {Cliff}_{d}$$ problem with this feature and prove that conditional on an event that happens with $$\varOmega (1)$$ probability, the $$(1 + 1)$$ EA reaches the optimum in expected $$O(n \ln n)$$ iterations. It is much better than the expected runtime of $$\varTheta (n^d)$$ for the original $$\textsc {Cliff}_{d}$$ . If the time-linkage property does not change the optimal function value but enlarges the optimal solution set, the problem is also possible to be easier to optimize. We construct another time-linkage variant of the $$\textsc {Cliff}_{d}$$ problem with this feature, and also prove an expected runtime of $$O(n\ln n)$$ (conditional on an event happening with $$\varOmega (1)$$ probability), compared with the expected runtime of $$\varOmega (n^{d-2})$$ for the corresponding problem without the time-linkage property. Even if the time-linkage property neither changes the optimal function value nor the optimal solution set, it is still possible to ease this problem if the intermediate solution, from which the optimum is easier to reach, is more prone to be maintained. We construct a time-linkage variant of the Jump problem, and proved that the expected runtime is reduced from $$O(n^k)$$ to $$O(n^{k-1})$$ . Our experiments also verify the above theoretical findings.},
  archive   = {C_PPSN},
  author    = {Li, Mingfeng and Zheng, Weijie and Xie, Wen and Sun, Ao and Yao, Xin},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70071-2_18},
  month     = {9},
  pages     = {280-294},
  title     = {When does the time-linkage property help optimization by evolutionary algorithms?},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Runtime analysis for state-of-the-art multi-objective
evolutionary algorithms on the subset selection problem. <em>PPSN</em>,
264–279. (<a
href="https://doi.org/10.1007/978-3-031-70071-2_17">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the last few years, the mathematical runtime analysis of randomized search heuristics has made a huge step forward by developing the methods to analyze the most prominent multi-objective evolutionary algorithms (MOEAs) as opposed to previously only simplistic algorithms. These results confirmed that many previous results extend to state-of-the-art MOEAs, but also showed that algorithms like the NSGA-II can have unexpected difficulties on problems easily solved by simple MOEAs. We continue this line of research by analyzing how the NSGA-II and the SMS-EMOA (also with a recently proposed stochastic population update) solve the NP-hard subset selection problem. For these two state-of-the-art algorithms, we prove performance guarantees that agree with those previously shown for the POSS algorithm, a variant of the simplistic GSEMO, namely that they compute $$(1-e^{-\gamma })$$ -approximate solutions in expected time $$O(k^2n)$$ . Our experiments confirm these findings. This work is the first runtime analysis of state-of-the-art MOEAs for the subset selection problem, and also the first runtime analysis of SMS-EMOA on a combinatorial problem.},
  archive   = {C_PPSN},
  author    = {Deng, Renzhong and Zheng, Weijie and Li, Mingfeng and Liu, Jie and Doerr, Benjamin},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70071-2_17},
  month     = {9},
  pages     = {264-279},
  title     = {Runtime analysis for state-of-the-art multi-objective evolutionary algorithms on the subset selection problem},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Level-based theorems for runtime analysis of
multi-objective evolutionary algorithms. <em>PPSN</em>, 246–263. (<a
href="https://doi.org/10.1007/978-3-031-70071-2_16">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Runtime analysis of multi-objective evolutionary algorithms (MOEAs) is a rapidly emerging field in which recent breakthroughs studied state-of-the-art MOEAs like NSGA-II and NSGA-III. These analyses typically bound the expected time to cover the Pareto front by analysing (1) the expected time to find a first Pareto-optimal search point and (2) the expected time to cover the whole Pareto front from there. We support this development by providing a powerful general tool for bounding the expected time to reach a first Pareto-optimal search point. It is based on the well-known fitness-level method, a simple and versatile yet powerful analysis method, adapted to multiple objectives. The benefits are to simplify runtime analyses by removing repetitive arguments used across many runtime analyses, thus allowing for shorter and simpler proofs, and to make runtime analysis of MOEAs more accessible to other researchers. Our level-based theorems further provide additional results on stochastic domination and tail bounds in addition to bounds on expected hitting times. We identify sufficient conditions for NSGA-II and NSGA-III to reach the Pareto front, which may pave the way for runtime analyses of state-of-the-art MOEAs approximating the Pareto front with population sizes smaller than the Pareto front.},
  archive   = {C_PPSN},
  author    = {Dang, Duc-Cuong and Opris, Andre and Sudholt, Dirk},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70071-2_16},
  month     = {9},
  pages     = {246-263},
  title     = {Level-based theorems for runtime analysis of multi-objective evolutionary algorithms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). On the equivalence between stochastic tournament and
power-law ranking selection and how to implement them efficiently.
<em>PPSN</em>, 230–245. (<a
href="https://doi.org/10.1007/978-3-031-70071-2_15">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tournament selection is a popular parent selection mechanism in evolutionary algorithms. Bian and Qian (PPSN 2022) proved that choosing the tournament size uniformly at random, called stochastic tournament selection, in combination with crossover significantly improves the performance of NSGA-II on some benchmark functions. We show that this selection mechanism is asymptotically equivalent to the power-law ranking selection proposed in Covantes Osuna et al. (Theor. Comput. Sci. 832, 2020) with the exponent of 2. Thus asymptotic runtime bounds proven for one operator also hold when one operator is replaced with the other. We also investigate how to implement these operators efficiently for NSGA-II on the problems considered in the previous papers. We propose to implement the stochastic tournament with a pre-computed selection distribution to save on random numbers. Experiments on high dimensional problems demonstrate the superiority of this method compared to the standard implementation. Overall, the power-law ranking selection is the most efficient selection mechanism for the studied problems. Remarkably, we also find that the way ties are broken between equally fit solutions can make the difference between the best and the worst approach, especially when crossover is involved.},
  archive   = {C_PPSN},
  author    = {Dang, Duc-Cuong and Opris, Andre and Sudholt, Dirk},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70071-2_15},
  month     = {9},
  pages     = {230-245},
  title     = {On the equivalence between stochastic tournament and power-law ranking selection and how to implement them efficiently},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ranking diversity benefits coevolutionary algorithms on an
intransitive game. <em>PPSN</em>, 213–229. (<a
href="https://doi.org/10.1007/978-3-031-70071-2_14">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Competitive coevolutionary algorithms (CoEAs) often encounter so-called coevolutionary pathologies particularly cycling behavior, which becomes more pronounced for games where there is no clear hierarchy of superiority among the possible strategies (intransitive games). In order to avoid these pathologies and ensure an efficient optimisation, it has been suggested that it is critical to choose a good evaluation environment (set of solutions used for evaluation). In this paper, we use runtime analysis to increase our understanding of the essential characteristics that the evaluation environments should possess to ensure efficient runtime on the intransitive problem class $$\textsc {Bilinear}_{\alpha ,\beta }$$ . For this problem class, we observe that it is beneficial to maintain a high diversity of rankings in the evaluation environment, that is, a set of individuals used for evaluation which are diverse in how they rank opponents. We propose and analyse two mechanisms that implement this idea. In the first approach, we ensure diversity of rankings through an archive. In the second approach, we introduce a CoEA without an archive, but with a ranking diversity mechanism. Both approaches optimise $$\textsc {Bilinear}_{\alpha ,\beta }$$ in expected polynomial time.},
  archive   = {C_PPSN},
  author    = {Fajardo, Mario Alejandro Hevia and Lehre, Per Kristian},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70071-2_14},
  month     = {9},
  pages     = {213-229},
  title     = {Ranking diversity benefits coevolutionary algorithms on an intransitive game},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Proven runtime guarantees for how the MOEA/d: Computes the
pareto front from the subproblem solutions. <em>PPSN</em>, 197–212. (<a
href="https://doi.org/10.1007/978-3-031-70071-2_13">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The decomposition-based multi-objective evolutionary algorithm (MOEA/D) does not directly optimize a given multi-objective function f, but instead optimizes $$N + 1$$ single-objective subproblems of f in a co-evolutionary manner. It maintains an archive of all non-dominated solutions found and outputs it as approximation to the Pareto front. Once the MOEA/D found all optima of the subproblems (the g-optima), it may still miss Pareto optima of f. The algorithm is then tasked to find the remaining Pareto optima directly by mutating the g-optima. In this work, we analyze for the first time how the MOEA/D with only standard mutation operators computes the whole Pareto front of the OneMinMax benchmark when the g-optima are a strict subset of the Pareto front. For standard bit mutation, we prove an expected runtime of $$O(n N \log n + n^{n/(2N)} N \log n)$$ function evaluations. Especially for the second, more interesting phase when the algorithm start with all g-optima, we prove an $$\varOmega (n^{(1/2)(n/N + 1)} \sqrt{N} 2^{-n/N})$$ expected runtime. This runtime is super-polynomial if $$N = o(n)$$ , since this leaves large gaps between the g-optima, which require costly mutations to cover. For power-law mutation with exponent $$\beta \in (1, 2)$$ , we prove an expected runtime of $$O\left( n N \log n + n^{\beta } \log n\right) $$ function evaluations. The $$O\left( n^{\beta } \log n\right) $$ term stems from the second phase of starting with all g-optima, and it is independent of the number of subproblems N. This leads to a huge speedup compared to the lower bound for standard bit mutation. In general, our overall bound for power-law suggests that the MOEA/D performs best for $$N = O(n^{\beta - 1})$$ , resulting in an $$O(n^\beta \log n)$$ bound. In contrast to standard bit mutation, smaller values of N are better for power-law mutation, as it is capable of easily creating missing solutions.},
  archive   = {C_PPSN},
  author    = {Doerr, Benjamin and Krejca, Martin S. and Weeks, Noé},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70071-2_13},
  month     = {9},
  pages     = {197-212},
  title     = {Proven runtime guarantees for how the MOEA/D: Computes the pareto front from the subproblem solutions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Local optima in diversity optimization: Non-trivial
offspring population is essential. <em>PPSN</em>, 181–196. (<a
href="https://doi.org/10.1007/978-3-031-70071-2_12">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The main goal of diversity optimization is to find a diverse set of solutions which satisfy some lower bound on their fitness. Evolutionary algorithms (EAs) are often used for such tasks, since they are naturally designed to optimize populations of solutions. This approach to diversity optimization, called EDO, has been previously studied from theoretical perspective, but most studies considered only EAs with a trivial offspring population such as the $$(\mu + 1)$$ EA. In this paper we give an example instance of a k-vertex cover problem, which highlights a critical difference of the diversity optimization from the regular single-objective optimization, namely that there might be a locally optimal population from which we can escape only by replacing at least two individuals at once, which the $$(\mu + 1)$$ algorithms cannot do. We also show that the $$(\mu + \lambda )$$ EA with $$\lambda \ge \mu $$ can effectively find a diverse population on k-vertex cover, if using a mutation operator inspired by Branson and Sutton (TCS 2023). To avoid the problem of subset selection which arises in the $$(\mu + \lambda )$$ EA when it optimizes diversity, we also propose the $$(1_\mu + 1_\mu )$$ EA $$_D$$ , which is an analogue of the $$(1 + 1)$$ EA for populations, and which is also efficient at optimizing diversity on the k-vertex cover problem.},
  archive   = {C_PPSN},
  author    = {Antipov, Denis and Neumann, Aneta and Neumann, Frank},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70071-2_12},
  month     = {9},
  pages     = {181-196},
  title     = {Local optima in diversity optimization: Non-trivial offspring population is essential},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Archive-based single-objective evolutionary algorithms for
submodular optimization. <em>PPSN</em>, 166–180. (<a
href="https://doi.org/10.1007/978-3-031-70071-2_11">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Constrained submodular optimization problems play a key role in the area of combinatorial optimization as they capture many NP-hard optimization problems. So far, Pareto optimization approaches using multi-objective formulations have been shown to be successful to tackle these problems while single-objective formulations lead to difficulties for algorithms such as the $$(1+1)$$ -EA due to the presence of local optima. We introduce for the first time single-objective algorithms that are provably successful for different classes of constrained submodular maximization problems. Our algorithms are variants of the $$(1+\lambda )$$ -EA and $$(1+1)$$ -EA and increase the feasible region of the search space incrementally in order to deal with the considered submodular problems.},
  archive   = {C_PPSN},
  author    = {Neumann, Frank and Rudolph, Günter},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70071-2_11},
  month     = {9},
  pages     = {166-180},
  title     = {Archive-based single-objective evolutionary algorithms for submodular optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analysis of evolutionary diversity optimisation for the
maximum matching problem. <em>PPSN</em>, 149–165. (<a
href="https://doi.org/10.1007/978-3-031-70071-2_10">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper delves into the enhancement of solution diversity in evolutionary algorithms (EAs) for the maximum matching problem, with a particular focus on complete bipartite graphs and paths. We utilize binary string encoding for matchings and employ Hamming distance as the metric for measuring diversity, aiming to maximize it. Central to our research is the $$(\mu +1)$$ -EA $$_D$$ and 2P-EA $$_D$$ , applied for diversity optimization, which we rigorously analyze both theoretically and empirically. For complete bipartite graphs, our runtime analysis demonstrates that, for reasonably small $$\mu $$ , the $$(\mu +1)$$ -EA $$_D$$ achieves maximal diversity with an expected runtime of $$O(\mu ^2 m^4\log (m))$$ for the big gap case (where the population size $$\mu $$ is less than the difference in the sizes of the bipartite partitions) and $$O(\mu ^2 m^2\log (m))$$ otherwise. For paths we give an upper bound of $$O(\mu ^3m^3)$$ . Additionally, for the 2P-EA $$_D$$ we give stronger performance bounds of $$O(\mu ^2 m^2\log (m))$$ for the big gap case, $$O(\mu ^2 n^2\log (n))$$ otherwise, and $$O(\mu ^3 m^2)$$ for paths. Here $$n$$ is the total number of vertices and $$m$$ the number of edges. Our empirical studies, examining the scaling behavior with respect to m and $$\mu $$ , complement these theoretical insights and suggest potential for further refinement of the runtime bounds.},
  archive   = {C_PPSN},
  author    = {Gadea Harder, Jonathan and Neumann, Aneta and Neumann, Frank},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70071-2_10},
  month     = {9},
  pages     = {149-165},
  title     = {Analysis of evolutionary diversity optimisation for the maximum matching problem},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evolving populations of solved subgraphs with crossover and
constraint repair. <em>PPSN</em>, 133–148. (<a
href="https://doi.org/10.1007/978-3-031-70071-2_9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a population-based approach to solving parameterized graph problems for which the goal is to identify a small set of vertices subject to a feasibility criterion. The idea is to evolve a population of individuals where each individual corresponds to an optimal solution to a subgraph of the original problem. The crossover operation then combines both solutions and subgraphs with the hope to generate an optimal solution for a slightly larger graph. In order to correctly combine solutions and subgraphs, we propose a new crossover operator called generalized allelic crossover which generalizes uniform crossover by associating a probability at each locus depending on the combined alleles of the parents. We prove for graphs with n vertices and m edges, the approach solves the k-vertex cover problem in expected time $$O{\left( 4^k m + m^4 \log n \right) }$$ using a simple RLS-style mutation. This bound can be improved to $$O{\left( 4^k m + m^2 n k \log n\right) }$$ by using standard mutation constrained to the vertices of the graph.},
  archive   = {C_PPSN},
  author    = {Lee, Jiwon and Sutton, Andrew M.},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70071-2_9},
  month     = {9},
  pages     = {133-148},
  title     = {Evolving populations of solved subgraphs with crossover and constraint repair},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Overcoming binary adversarial optimisation with competitive
coevolution. <em>PPSN</em>, 117–132. (<a
href="https://doi.org/10.1007/978-3-031-70071-2_8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Co-evolutionary algorithms (CoEAs), which pair candidate designs with test cases, are frequently used in adversarial optimisation, particularly for binary test-based problems where designs and tests yield binary outcomes. The effectiveness of designs is determined by their performance against tests, and the value of tests is based on their ability to identify failing designs, often leading to more sophisticated tests and improved designs. However, CoEAs can exhibit complex, sometimes pathological behaviours like disengagement. Through runtime analysis, we aim to rigorously analyse whether CoEAs can efficiently solve test-based adversarial optimisation problems in an expected polynomial runtime. This paper carries out the first rigorous runtime analysis of $$(1,\lambda )$$ -CoEA for binary test-based adversarial optimisation problems. In particular, we introduce a binary test-based benchmark problem called Diagonal problem and initiate the first runtime analysis of competitive CoEA on this problem. The mathematical analysis shows that the $$(1,\lambda )$$ -CoEA can efficiently find an $$\varepsilon $$ approximation to the optimal solution of the Diagonal problem, i.e. in expected polynomial runtime assuming sufficiently low mutation rates and large offspring population size. On the other hand, the standard $$(1,\lambda )$$ -EA fails to find an $$\varepsilon $$ approximation to the optimal solution of the Diagonal problem in polynomial runtime. This illustrates the potential of coevolution for solving binary adversarial optimisation problems.},
  archive   = {C_PPSN},
  author    = {Lehre, Per Kristian and Lin, Shishen},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70071-2_8},
  month     = {9},
  pages     = {117-132},
  title     = {Overcoming binary adversarial optimisation with competitive coevolution},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How population diversity influences the efficiency of
crossover. <em>PPSN</em>, 102–116. (<a
href="https://doi.org/10.1007/978-3-031-70071-2_7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Our theoretical understanding of crossover is limited by our ability to analyze how population diversity evolves. In this study, we provide one of the first rigorous analyses of population diversity and optimization time in a setting where large diversity and large population sizes are required to speed up progress. We give a formal and general criterion which amount of diversity is necessary and sufficient to speed up the $$(\mu +1)$$ Genetic Algorithm on LeadingOnes. We show that the naturally evolving diversity falls short of giving a substantial speed-up for any $$\mu =O(\sqrt{n}/\log ^2 n)$$ . On the other hand, we show that even for $$\mu =2$$ , if we simply break ties in favor of diversity then this increases diversity so much that optimization is accelerated by a constant factor.3(Proofs in this submission are mostly omitted due to the page limit. A full version with detailed proofs can be found in the arXiv version of this article [2], but reviewers are not required to consult that version or to check correctness of those proofs.)},
  archive   = {C_PPSN},
  author    = {Cerf, Sacha and Lengler, Johannes},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70071-2_7},
  month     = {9},
  pages     = {102-116},
  title     = {How population diversity influences the efficiency of crossover},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Greedy versus curious parent selection for multi-objective
evolutionary algorithms. <em>PPSN</em>, 86–101. (<a
href="https://doi.org/10.1007/978-3-031-70071-2_6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {From the literature we know that simple evolutionary multi-objective algorithms can optimize the classic two-objective test functions OneMinMax and CountingOnesCountingZeroes in $$O(n^2\log n)$$ expected time. We extend this result to any pair of generalized OneMax functions and show that, if the optima of the two functions are d apart, then (G)SEMO has an expected optimization time of $$O(dn \log (n) )$$ . In an attempt to achieve better optimization times, some algorithms consider parent selection. We show that parent selection based on the curiosity-based novelty search can improve the optimization time to $$O(n^2)$$ on OneMinMax. By contrast, we show that greedy parent selection schemes can be trapped with an incomplete Pareto front for superpolynomial time. Finally, we provide experimental results on the two-objective optimization of linear functions.},
  archive   = {C_PPSN},
  author    = {Antipov, Denis and Kötzing, Timo and Radhakrishnan, Aishwarya},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70071-2_6},
  month     = {9},
  pages     = {86-101},
  title     = {Greedy versus curious parent selection for multi-objective evolutionary algorithms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Faster optimization through genetic drift. <em>PPSN</em>,
70–85. (<a href="https://doi.org/10.1007/978-3-031-70071-2_5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The compact Genetic Algorithm (cGA), parameterized by its hypothetical population size K, offers a low-memory alternative to evolving a large offspring population of solutions. It evolves a probability distribution, biasing it towards promising samples. For the classical benchmark OneMax, the cGA has two different modes of operation: a conservative one with small step sizes $$\varTheta (1/(\sqrt{n}\log n))$$ , which is slow but prevents genetic drift, and an aggressive one with large step sizes $$\varTheta (1/\log n)$$ , in which genetic drift leads to wrong decisions, but those are corrected efficiently. On OneMax, an easy hill-climbing problem, both modes lead to optimization times of $$\varTheta (n\log n)$$ and are thus equally efficient. In this paper we study how both regimes change when we replace OneMax by the harder hill-climbing problem Dynamic BinVal. It turns out that the aggressive mode is not affected and still yields quasi-linear runtime $$O(n{{\,\mathrm{\textrm{polylog}}\,}}n)$$ . However, the conservative mode becomes substantially slower, yielding a runtime of $$\varOmega (n^2)$$ , since genetic drift can only be avoided with smaller step sizes of O(1/n). We complement our theoretical results with simulations.},
  archive   = {C_PPSN},
  author    = {Florescu, Cella and Kaufmann, Marc and Lengler, Johannes and Schaller, Ulysse},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70071-2_5},
  month     = {9},
  pages     = {70-85},
  title     = {Faster optimization through genetic drift},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Runtime analysis of a multi-valued compact genetic algorithm
on generalized OneMax. <em>PPSN</em>, 53–69. (<a
href="https://doi.org/10.1007/978-3-031-70071-2_4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A class of metaheuristic techniques called estimation-of-distribution algorithms (EDAs) is employed in optimization as a more sophisticated substitute for traditional strategies like evolutionary algorithms. EDAs generally drive the search for the optimum by creating probabilistic models of potential candidate solutions through repeated sampling and selection from the underlying search space. Most theoretical research on EDAs has focused on pseudo-Boolean optimization. Jedidia et al. (GECCO 2023) introduced a framework for EDAs for optimizing problems involving multi-valued decision variables. In addition, they conduct a mathematical runtime analysis of a multi-valued UMDA on the r-valued LeadingOnes function. Using their framework, here we focus on the multi-valued compact genetic algorithm ( $$r$$ -cGA) and provide a first runtime analysis of a generalized OneMax function. To prove our results, we investigate the effect of genetic drift and progress of the probabilistic model towards the optimum. After finding the right algorithm parameters, we prove that the $$r$$ -cGA solves this r-valued OneMax problem efficiently. We establish that the runtime bound is $$\text {O}(r^2 n \log ^2 r \log ^3 n)$$ with high probability.},
  archive   = {C_PPSN},
  author    = {Adak, Sumit and Witt, Carsten},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70071-2_4},
  month     = {9},
  pages     = {53-69},
  title     = {Runtime analysis of a multi-valued compact genetic algorithm on generalized OneMax},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sliding window 3-objective pareto optimization for problems
with chance constraints. <em>PPSN</em>, 36–52. (<a
href="https://doi.org/10.1007/978-3-031-70071-2_3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Constrained single-objective problems have been frequently tackled by evolutionary multi-objective algorithms where the constraint is relaxed into an additional objective. Recently, it has been shown that Pareto optimization approaches using bi-objective models can be significantly sped up using sliding windows [16]. In this paper, we extend the sliding window approach to 3-objective formulations for tackling chance constrained problems. On the theoretical side, we show that our new sliding window approach improves previous runtime bounds obtained in [15] while maintaining the same approximation guarantees. Our experimental investigations for the chance constrained dominating set problem show that our new sliding window approach allows one to solve much larger instances in a much more efficient way than the 3-objective approach presented in [15].},
  archive   = {C_PPSN},
  author    = {Neumann, Frank and Witt, Carsten},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70071-2_3},
  month     = {9},
  pages     = {36-52},
  title     = {Sliding window 3-objective pareto optimization for problems with chance constraints},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Runtime analysis of evolutionary diversity optimization on a
tri-objective version of the (LeadingOnes, TrailingZeros) problem.
<em>PPSN</em>, 19–35. (<a
href="https://doi.org/10.1007/978-3-031-70071-2_2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Diversity optimization is a class of optimization problems in which we aim to find a diverse set of good solutions. One of the frequently used approaches to solve such problems is to use evolutionary algorithms which evolve a desired diverse population. This approach is called evolutionary diversity optimization (EDO). In this paper, we analyse EDO on a 3-objective function LOTZ $$_k$$ , which is a modification of the 2-objective benchmark function (LeadingOnes, TrailingZeros). We prove that the GSEMO computes a set of all Pareto-optimal solutions in $$O(kn^3)$$ expected iterations. We also analyze the runtime of the GSEMO $$_D$$ (a modification of the GSEMO for diversity optimization) until it finds a population with the best possible diversity for two different diversity measures, the total imbalance and the sorted imbalances vector. For the first measure we show that the GSEMO $$_D$$ optimizes it asymptotically faster than it finds a Pareto-optimal population, in $$O(kn^2\log (n))$$ expected iterations, and for the second measure we show an upper bound of $$O(k^2n^3\log (n))$$ expected iterations. We complement our theoretical analysis with an empirical study, which shows a very similar behavior for both diversity measures that is close to the theoretical predictions.},
  archive   = {C_PPSN},
  author    = {Antipov, Denis and Neumann, Aneta and Neumann, Frank and Sutton, Andrew M.},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70071-2_2},
  month     = {9},
  pages     = {19-35},
  title     = {Runtime analysis of evolutionary diversity optimization on a tri-objective version of the (LeadingOnes, TrailingZeros) problem},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-adjusting evolutionary algorithms are slow on a class
of multimodal landscapes. <em>PPSN</em>, 3–18. (<a
href="https://doi.org/10.1007/978-3-031-70071-2_1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The one-fifth rule and its generalizations are a classical parameter control mechanism in discrete domains. They have also been transferred to control the offspring population size of the $$(1 , \lambda )$$ -EA. This has been shown to work very well for hill-climbing, and combined with a restart mechanism it was recently shown by Hevia Fajardo and Sudholt to improve performance on the multi-modal problem Cliff drastically. In this work we show that the positive results do not extend to other types of local optima. On the distorted OneMax benchmark, the self-adjusting $$(1 , \lambda )$$ -EA is slowed down just as elitist algorithms because self-adaptation prevents the algorithm from escaping from local optima. This makes the self-adaptive algorithm considerably worse than good static parameter choices, which do allow to escape from local optima efficiently. We show this theoretically and complement the result with empirical runtime results.},
  archive   = {C_PPSN},
  author    = {Lengler, Johannes and Sturm, Konstantin},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70071-2_1},
  month     = {9},
  pages     = {3-18},
  title     = {Self-adjusting evolutionary algorithms are slow on a class of multimodal landscapes},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A surrogate-assisted partial optimization for expensive
constrained optimization problems. <em>PPSN</em>, 391–407. (<a
href="https://doi.org/10.1007/978-3-031-70068-2_24">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Surrogate-assisted evolutionary algorithms (SAEAs) are gradually gaining attention as a method for solving expensive optimization problems with inequality constraints. Most SAEAs construct a surrogate model for each objective/constraint function and then aggregate approximation functions of constraints to estimate the feasibility of unevaluated solutions. However, because of the aggregation, the differences in the scales among constraints are ignored. Constraints with smaller scales do not benefit from constraint handling techniques as much as larger constraints, while the effects of handling constraints with larger scales scatter to the other many constraints. This results in an inefficient constraint optimization. Accordingly, this work proposes a new SAEA that partially optimizes each objective/constraint, namely surrogate-assisted partial optimization (SAPO). Solutions with better values of objective/constraint are selected from the evaluated solutions as the parent solutions and a focused objective/constraint is independently optimized using surrogate models one by one. Experimental results reveal the superiority of SAPO compared to the state-of-the-art SAEAs on a single-objective optimization problem suite with inequality constraints under an expensive optimization scenario.},
  archive   = {C_PPSN},
  author    = {Nishihara, Kei and Nakata, Masaya},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70068-2_24},
  month     = {9},
  pages     = {391-407},
  title     = {A surrogate-assisted partial optimization for expensive constrained optimization problems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evolve cost-aware acquisition functions using large language
models. <em>PPSN</em>, 374–390. (<a
href="https://doi.org/10.1007/978-3-031-70068-2_23">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many real-world optimization scenarios involve expensive evaluation with unknown and heterogeneous costs. Cost-aware Bayesian optimization stands out as a prominent solution in addressing these challenges. To approach the global optimum within a limited budget in a cost-efficient manner, the design of cost-aware acquisition functions (AFs) becomes a crucial step. However, traditional manual design paradigm typically requires extensive domain knowledge and involves a labor-intensive trial-and-error process. This paper introduces EvolCAF, a novel framework that integrates large language models (LLMs) with evolutionary computation (EC) to automatically design cost-aware AFs. Leveraging the crossover and mutation in the algorithmic space, EvolCAF offers a novel design paradigm, significantly reduces the reliance on domain expertise and model training. The designed cost-aware AF maximizes the utilization of available information from historical data, surrogate models and budget details. It introduces novel ideas not previously explored in the existing literature on acquisition function design, allowing for clear interpretations to provide insights into its behavior and decision-making process. In comparison to the well-known EIpu and EI-cool methods designed by human experts, our approach showcases remarkable efficiency and generalization across various tasks, including 12 synthetic problems and 3 real-world hyperparameter tuning test sets.},
  archive   = {C_PPSN},
  author    = {Yao, Yiming and Liu, Fei and Cheng, Ji and Zhang, Qingfu},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70068-2_23},
  month     = {9},
  pages     = {374-390},
  title     = {Evolve cost-aware acquisition functions using large language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Re-examining supervised dimension reduction for
high-dimensional bayesian optimization. <em>PPSN</em>, 356–373. (<a
href="https://doi.org/10.1007/978-3-031-70068-2_22">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bayesian optimization (BO) has been broadly applied to optimize expensive-to-evaluate black-box functions, but it is still challenging to scale BO to high dimensions while retaining sample efficiency. A solution in the existing literature is to assume that there exists a lower-dimensional structure for objective functions and learn the lower-dimensional embedding via supervised dimension reduction. For example, BO based on Sliced Inverse Regression (SIR) directly uses SIR to discover the intrinsic lower-dimensional structure of the objective function. However, the assumption of SIR leads to a mismatch in BO, and maximizing a high-dimensional acquisition function also leads to its poor performance. To reduce the mismatch between dimension reduction methods and BO, we introduce Kernel Dimension Reduction (KDR) and manifold KDR to BO. Furthermore, to improve the performance of acquisition functions, we construct a constrained low-dimensional acquisition function, where the constraint is constructed by the inverse mapping from the central subspace back to the original space using a batch of Gaussian Process models. We verify empirically that tackling these two issues improves the performance of methods based on supervised dimension reduction on a wide range of problems.},
  archive   = {C_PPSN},
  author    = {Chen, Quanlin and Huo, Jing and Chen, Yiyu and Ding, Tianyu and Gao, Yang and Li, Dong and He, Xu},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70068-2_22},
  month     = {9},
  pages     = {356-373},
  title     = {Re-examining supervised dimension reduction for high-dimensional bayesian optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An adaptive approach to bayesian optimization with setup
switching costs. <em>PPSN</em>, 340–355. (<a
href="https://doi.org/10.1007/978-3-031-70068-2_21">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Black-box optimization methods typically assume that evaluations of the black-box objective function are equally costly to evaluate. We investigate here a resource-constrained setting where changes to certain decision variables of the search space incur a higher switching cost, e.g., due to expensive changes to the experimental setup. In this scenario, there is a trade-off between fixing the values of those costly variables or accepting this additional cost to explore more of the search space. We adapt two process-constrained batch algorithms to this sequential problem formulation, and propose two new methods—one cost-aware and one cost-ignorant. We validate and compare the algorithms using a set of 7 scalable test functions with different switching-cost settings. Our proposed cost-aware parameter-free algorithm yields comparable results to tuned process-constrained algorithms in all settings we considered, suggesting some degree of robustness to varying landscape features and cost trade-offs. This method starts to outperform the other algorithms with increasing switching cost. Our work expands on other recent Bayesian Optimization studies in resource-constrained settings that consider a batch setting only. Although the contributions of this work are relevant to the general class of resource-constrained problems, they are particularly relevant to problems where adaptability to varying resource availability is of high importance.},
  archive   = {C_PPSN},
  author    = {Pricopie, Stefan and Allmendinger, Richard and López-Ibáñez, Manuel and Fare, Clyde and Benatan, Matt and Knowles, Joshua},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70068-2_21},
  month     = {9},
  pages     = {340-355},
  title     = {An adaptive approach to bayesian optimization with setup switching costs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Balancing between time budgets and costs in
surrogate-assisted evolutionary algorithms. <em>PPSN</em>, 322–339. (<a
href="https://doi.org/10.1007/978-3-031-70068-2_20">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For many real-world multi-objective optimisation problems, function evaluations are computationally expensive, resulting in a limited budget of function evaluations that can be performed in practice. To tackle such expensive problems, multi-objective surrogate-assisted evolutionary algorithms (SAEAs) have been introduced. Often, the performance of these EAs is measured after a fixed number of function evaluations (typically several hundreds) and complex surrogate models are found to be the best to use. However, when selecting an SAEA for a real-world problem, the surrogate building time, surrogate evaluation time, function evaluation time, and available optimisation time budget should be considered simultaneously. To gain insight into the performance of various surrogate models under different conditions, we evaluate an EA with and without four surrogate models (both complex and simple) for a range of optimisation time budgets and function evaluation times while considering the surrogate building and surrogate evaluation times. We use 55 bbob-biobj benchmark problems as well as a real-world problem where the fitness function involves a biomechanical simulation. Our results, on both types of problems, indicate that a larger hypervolume can be obtained with SAEAs when a function evaluation takes longer than 0.384 s (on the hardware we used). While we confirm that state-of-the-art complex surrogate models are mostly the best choice if up to several hundred function evaluations can be performed, we also observe that simple surrogate models can still outperform non-surrogate-assisted EAs if several thousand function evaluations can be performed.},
  archive   = {C_PPSN},
  author    = {Rodriguez, Cedric J. and Bosman, Peter A. N. and Alderliesten, Tanja},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70068-2_20},
  month     = {9},
  pages     = {322-339},
  title     = {Balancing between time budgets and costs in surrogate-assisted evolutionary algorithms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Performance comparison of surrogate-assisted evolutionary
algorithms on computational fluid dynamics problems. <em>PPSN</em>,
303–321. (<a
href="https://doi.org/10.1007/978-3-031-70068-2_19">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Surrogate-assisted evolutionary algorithms (SAEAs) are recently among the most widely studied methods for their capability to solve expensive real-world optimization problems. However, the development of new methods and benchmarking with other techniques still relies almost exclusively on artificially created problems. In this paper, we use two real-world computational fluid dynamics problems to compare the performance of eleven state-of-the-art single-objective SAEAs. We analyze the performance by investigating the quality and robustness of the obtained solutions and the convergence properties of the selected methods. Our findings suggest that the more recently published methods, as well as the techniques that utilize differential evolution as one of their optimization mechanisms, perform significantly better than the other considered methods.},
  archive   = {C_PPSN},
  author    = {Kůdela, Jakub and Dobrovský, Ladislav},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70068-2_19},
  month     = {9},
  pages     = {303-321},
  title     = {Performance comparison of surrogate-assisted evolutionary algorithms on computational fluid dynamics problems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LB+IC-CMA-ES: Two simple modifications of CMA-ES to handle
mixed-integer problems. <em>PPSN</em>, 284–299. (<a
href="https://doi.org/10.1007/978-3-031-70068-2_18">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present LB+IC-CMA-ES, a variant of CMA-ES that handles mixed-integer problems. The algorithm uses two simple mechanisms to handle integer variables: (i) a lower bound (LB) on the variance of integer variables and (ii) integer centering (IC) of variables to their domain middle depending on their value. After presenting the algorithm, we evaluate the different variants ensuing from these modifications on the BBOB mixed-integer testbed and compare the performance with the recently introduced CMA-ES with margin.},
  archive   = {C_PPSN},
  author    = {Marty, Tristan and Hansen, Nikolaus and Auger, Anne and Semet, Yann and Héron, Sébastien},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70068-2_18},
  month     = {9},
  pages     = {284-299},
  title     = {LB+IC-CMA-ES: Two simple modifications of CMA-ES to handle mixed-integer problems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Avoiding redundant restarts in multimodal global
optimization. <em>PPSN</em>, 268–283. (<a
href="https://doi.org/10.1007/978-3-031-70068-2_17">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Naïve restarts of global optimization solvers when operating on multimodal search landscapes may resemble the Coupon’s Collector Problem, with a potential to waste significant function evaluations budget on revisiting the same basins of attractions. In this paper, we assess the degree to which such “duplicate restarts” occur on standard multimodal benchmark functions, which defines the redundancy potential of each particular landscape. We then propose a repelling mechanism to avoid such wasted restarts with the CMA-ES and investigate its efficacy on test cases with high redundancy potential compared to the standard restart mechanism.},
  archive   = {C_PPSN},
  author    = {de Nobel, Jacob and Vermetten, Diederick and Kononova, Anna V. and Shir, Ofer M. and Bäck, Thomas},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70068-2_17},
  month     = {9},
  pages     = {268-283},
  title     = {Avoiding redundant restarts in multimodal global optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Natural gradient interpretation of rank-one update in
CMA-ES. <em>PPSN</em>, 252–267. (<a
href="https://doi.org/10.1007/978-3-031-70068-2_16">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The covariance matrix adaptation evolution strategy (CMA-ES) is a stochastic search algorithm using a multivariate normal distribution for continuous black-box optimization. In addition to strong empirical results, part of the CMA-ES can be described by a stochastic natural gradient method and can be derived from information geometric optimization (IGO) framework. However, there are some components of the CMA-ES, such as the rank-one update, for which the theoretical understanding is limited. While the rank-one update makes the covariance matrix to increase the likelihood of generating a solution in the direction of the evolution path, this idea has been difficult to formulate and interpret as a natural gradient method unlike the rank- $$\mu $$ update. In this work, we provide a new interpretation of the rank-one update in the CMA-ES from the perspective of the natural gradient with prior distribution. First, we propose maximum a posteriori IGO (MAP-IGO), which is the IGO framework extended to incorporate a prior distribution. Then, we derive the rank-one update from the MAP-IGO by setting the prior distribution based on the idea that the promising mean vector should exist in the direction of the evolution path. Moreover, the newly derived rank-one update is extensible, where an additional term appears in the update for the mean vector. We empirically investigate the properties of the additional term using various benchmark functions.},
  archive   = {C_PPSN},
  author    = {Hamano, Ryoki and Shirakawa, Shinichi and Nomura, Masahiro},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70068-2_16},
  month     = {9},
  pages     = {252-267},
  title     = {Natural gradient interpretation of rank-one update in CMA-ES},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CMA-ES for discrete and mixed-variable optimization on sets
of points. <em>PPSN</em>, 236–251. (<a
href="https://doi.org/10.1007/978-3-031-70068-2_15">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Discrete and mixed-variable optimization problems have appeared in several real-world applications. Most of the research on mixed-variable optimization considers a mixture of integer and continuous variables, and several integer handlings have been developed to inherit the optimization performance of the continuous optimization methods to mixed-integer optimization. In some applications, acceptable solutions are given by selecting possible points in the disjoint subspaces. This paper focuses on the optimization on sets of points and proposes an optimization method by extending the covariance matrix adaptation evolution strategy (CMA-ES), termed the CMA-ES on sets of points (CMA-ES-SoP). The CMA-ES-SoP incorporates margin correction that maintains the generation probability of neighboring points to prevent premature convergence to a specific non-optimal point, which is an effective integer-handling technique for CMA-ES. In addition, because margin correction with a fixed margin value tends to increase the marginal probabilities for a portion of neighboring points more than necessary, the CMA-ES-SoP updates the target margin value adaptively to make the average of the marginal probabilities close to a predefined target probability. Numerical simulations demonstrated that the CMA-ES-SoP successfully optimized the optimization problems on sets of points, whereas the naive CMA-ES failed to optimize them due to premature convergence.},
  archive   = {C_PPSN},
  author    = {Uchida, Kento and Hamano, Ryoki and Nomura, Masahiro and Saito, Shota and Shirakawa, Shinichi},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70068-2_15},
  month     = {9},
  pages     = {236-251},
  title     = {CMA-ES for discrete and mixed-variable optimization on sets of points},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A potential function for a variable-metric evolution
strategy. <em>PPSN</em>, 221–235. (<a
href="https://doi.org/10.1007/978-3-031-70068-2_14">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper works towards an analysis of a variable-metric evolution strategy by means of drift analysis. Drift analysis has been effective for proving convergence and analyzing the runtime of a simple (1+1)-ES. We make a first step towards including covariance matrix adaptation (CMA). To this end, we develop a novel class of potential functions for the (1+1)-CMA-ES optimizing two-dimensional convex quadratic functions. We leverage invariances to efficiently sample a representative space of states. We use simulations to gain an empirical estimate of the expected minimal drift induced by the candidate potential function and to tune potential function parameters. Our results indicate that the tuned potential function is negative and uniformly bounded away from zero, which yields linear convergence.},
  archive   = {C_PPSN},
  author    = {Frank, Stephan and Glasmachers, Tobias},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70068-2_14},
  month     = {9},
  pages     = {221-235},
  title     = {A potential function for a variable-metric evolution strategy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Warm starting of CMA-ES for contextual optimization
problems. <em>PPSN</em>, 205–220. (<a
href="https://doi.org/10.1007/978-3-031-70068-2_13">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Several practical applications of evolutionary computation possess objective functions that receive the design variables and externally given parameters. Such problems are termed contextual optimization problems. These problems require finding the optimal solutions corresponding to the given context vectors. Existing contextual optimization methods train a policy model to predict the optimal solution from context vectors. However, the performance of such models is limited by their representation ability. By contrast, warm starting methods have been used to initialize evolutionary algorithms on a given problem using the optimization results on similar problems. Because warm starting methods do not consider the context vectors, their performances can be improved on contextual optimization problems. Herein, we propose a covariance matrix adaptation evolution strategy with contextual warm starting (CMA-ES-CWS) to efficiently optimize the contextual optimization problem with a given context vector. The CMA-ES-CWS utilizes the optimization results of past context vectors to train the multivariate Gaussian process regression. Subsequently, the CMA-ES-CWS performs warm starting for a given context vector by initializing the search distribution using posterior distribution of the Gaussian process regression. The results of the numerical simulation suggest that CMA-ES-CWS outperforms the existing contextual optimization and warm starting methods.},
  archive   = {C_PPSN},
  author    = {Sekino, Yuta and Uchida, Kento and Shirakawa, Shinichi},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70068-2_13},
  month     = {9},
  pages     = {205-220},
  title     = {Warm starting of CMA-ES for contextual optimization problems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Understanding the importance of evolutionary search in
automated heuristic design with large language models. <em>PPSN</em>,
185–202. (<a
href="https://doi.org/10.1007/978-3-031-70068-2_12">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automated heuristic design (AHD) has gained considerable attention for its potential to automate the development of effective heuristics. The recent advent of large language models (LLMs) has paved a new avenue for AHD, with initial efforts focusing on framing AHD as an evolutionary program search (EPS) problem. However, inconsistent benchmark settings, inadequate baselines, and a lack of detailed component analysis have left the necessity of integrating LLMs with search strategies and the true progress achieved by existing LLM-based EPS methods to be inadequately justified. This work seeks to fulfill these research queries by conducting a large-scale benchmark comprising four LLM-based EPS methods and four AHD problems across nine LLMs and five independent runs. Our extensive experiments yield meaningful insights, providing empirical grounding for the importance of evolutionary search in LLM-based AHD approaches, while also contributing to the advancement of future EPS algorithmic development. To foster accessibility and reproducibility, we have fully open-sourced our benchmark and corresponding results.},
  archive   = {C_PPSN},
  author    = {Zhang, Rui and Liu, Fei and Lin, Xi and Wang, Zhenkun and Lu, Zhichao and Zhang, Qingfu},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70068-2_12},
  month     = {9},
  pages     = {185-202},
  title     = {Understanding the importance of evolutionary search in automated heuristic design with large language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IMOPSE: A comprehensive open source library for single- and
multi-objective metaheuristic optimization. <em>PPSN</em>, 170–184. (<a
href="https://doi.org/10.1007/978-3-031-70068-2_11">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Intelligent Multi-Objective Problem Solving Environment (iMOPSE) is a robust open-source C++ library designed to tackle NP-hard optimization problems. It hosts a suite of multi-objective optimization algorithms, including state-of-the-art NSGA-II, MOEA/D, SPEA2, or NTGA2, complemented by a set of single-objective optimization metaheuristics such as Genetic Algorithms, Differential Evolution, Ant Colony Optimization, Tabu Search, Simulated Annealing, and Particle Swarm Optimization. One of iMOPSE’s notable strengths lies in its ability to handle classical NP-hard problems with constraints, ranging from the Traveling Salesman and Traveling Thief to Capacitated Vehicle Routing and Multi-Skill Resource-Constrained Project Scheduling Problems. Its flexible encoding mechanism adeptly manages different problems and facilitates the utilization of specialized operators. Moreover, iMOPSE offers pre-configured problem instances and method setups, along with a suite of tools for data collection, visualization, and analysis, bolstering its efficacy for rigorous research and optimization result interpretation. iMOPSE also provides extensive customization options, enabling researchers to explore and research various optimization methods and scenarios effectively. Its user-friendly interface streamlines setup procedures through intuitive input parameters and configuration files, ensuring accessibility across Windows and Unix-based operating systems. Together, these features position iMOPSE as a comprehensive solution for addressing real-world optimization challenges.},
  archive   = {C_PPSN},
  author    = {Gmyrek, Konrad and Myszkowski, Paweł B. and Antkiewicz, Michał and Olech, Łukasz P.},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70068-2_11},
  month     = {9},
  pages     = {170-184},
  title     = {IMOPSE: A comprehensive open source library for single- and multi-objective metaheuristic optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybridizing target- and SHAP-encoded features for algorithm
selection in mixed-variable black-box optimization. <em>PPSN</em>,
154–169. (<a
href="https://doi.org/10.1007/978-3-031-70068-2_10">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Exploratory landscape analysis (ELA) is a well-established tool to characterize optimization problems via numerical features. ELA is used for problem comprehension, algorithm design, and applications such as automated algorithm selection and configuration. Until recently, however, ELA was limited to search spaces with either continuous or discrete variables, neglecting problems with mixed variable types. This gap was addressed in a recent study that uses an approach based on target-encoding to compute exploratory landscape features for mixed-variable problems. In this work, we investigate an alternative encoding scheme based on SHAP values. While these features do not lead to better results in the algorithm selection setting considered in previous work, the two different encoding mechanisms exhibit complementary performance. Combining both feature sets into a hybrid approach outperforms each encoding mechanism individually. Finally, we experiment with two different ways of meta-selecting between the two feature sets. Both approaches are capable of taking advantage of the performance complementarity of the models trained on target-encoded and SHAP-encoded feature sets, respectively.},
  archive   = {C_PPSN},
  author    = {Dietrich, Konstantin and Prager, Raphael Patrick and Doerr, Carola and Trautmann, Heike},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70068-2_10},
  month     = {9},
  pages     = {154-169},
  title     = {Hybridizing target- and SHAP-encoded features for algorithm selection in mixed-variable black-box optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learned features vs. Classical ELA on affine BBOB functions.
<em>PPSN</em>, 137–153. (<a
href="https://doi.org/10.1007/978-3-031-70068-2_9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automated algorithm selection has proven to be effective to improve optimization performance by using machine learning to select the best-performing algorithm for the particular problem being solved. However, doing so requires the ability to describe the landscape of optimization problems using numerical features, which is a difficult task. In this work, we analyze the synergies and complementarity of recently proposed feature sets TransOpt and Deep ELA, which are based on deep-learning, and compare them to the commonly used classical ELA features. We analyze the correlation between the feature sets as well as how well one set can predict the other. We show that while the feature sets contain some shared information, each also contains important unique information. Further, we compare and benchmark the different feature sets for the task of automated algorithm selection on the recently proposed affine black-box optimization problems. We find that while classical ELA is the best-performing feature set by itself, using selected features from a combination of all three feature sets provides superior performance, and all three sets individually substantially outperform the single best solver.},
  archive   = {C_PPSN},
  author    = {Seiler, Moritz and Škvorc, Urban and Cenikj, Gjorgjina and Doerr, Carola and Trautmann, Heike},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70068-2_9},
  month     = {9},
  pages     = {137-153},
  title     = {Learned features vs. classical ELA on affine BBOB functions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating the robustness of deep-learning
algorithm-selection models by evolving adversarial instances.
<em>PPSN</em>, 121–136. (<a
href="https://doi.org/10.1007/978-3-031-70068-2_8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep neural networks (DNN) are increasingly being used to perform algorithm-selection in combinatorial optimisation domains, particularly as they accommodate input representations which avoid designing and calculating features. Mounting evidence from domains that use images as input shows that deep convolutional networks are vulnerable to adversarial samples, in which a small perturbation of an instance can cause the DNN to misclassify. However, it remains unknown as to whether deep recurrent networks (DRN) which have recently been shown promise as algorithm-selectors in the bin-packing domain are equally vulnerable. We use an evolutionary algorithm (EA) to find perturbations of instances from two existing benchmarks for online bin packing that cause trained DRNs to misclassify: adversarial samples are successfully generated from up to $$56\%$$ of the original instances depending on the dataset. Analysis of the new misclassified instances sheds light on the ‘fragility’ of some training instances, i.e. instances where it is trivial to find a small perturbation that results in a misclassification and the factors that influence this. Finally, the method generates a large number of new instances misclassified with a wide variation in confidence, providing a rich new source of training data to create more robust models.},
  archive   = {C_PPSN},
  author    = {Hart, Emma and Renau, Quentin and Sim, Kevin and Alissa, Mohamad},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70068-2_8},
  month     = {9},
  pages     = {121-136},
  title     = {Evaluating the robustness of deep-learning algorithm-selection models by evolving adversarial instances},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature encapsulation by stages in the regression domain
using grammatical evolution. <em>PPSN</em>, 105–120. (<a
href="https://doi.org/10.1007/978-3-031-70068-2_7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Feature Encapsulation by Stages (FES) is a recently proposed mechanism that can be implemented in any Evolutionary Computation (EC) metaheuristic. Encapsulation occurs via input space expansion in several stages by adding the best individual so far as an additional input. FES has been shown to perform well in training Boolean problems. This paper extends FES to the regression domain. Grammatical Evolution (GE), a branch of Genetic Programming (GP), supports the implementation of the FES approach by enabling the investigation of performance across various search guides expressed in the grammar. We conduct experiments on both synthetic and real-world symbolic regression problems, including multi-target issues. Additionally, we study several FES-based approaches utilising the best selection process for each problem, choosing between tournament, $$\epsilon $$ -Lexicase, and $$\epsilon \hbox {-}\textrm{Lexi}^2$$ . Statistical tests on unseen subsets’ results show that FES outperforms the standard baseline in all problems. Furthermore, we analyse individual complexity across generations, showing that populations utilising FES consist of simpler individuals, thereby reducing computational costs.},
  archive   = {C_PPSN},
  author    = {Reyes Fernández de Bulnes, Darian and de Lima, Allan and Galván, Edgar and Ryan, Conor},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70068-2_7},
  month     = {9},
  pages     = {105-120},
  title     = {Feature encapsulation by stages in the regression domain using grammatical evolution},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Landscape-aware automated algorithm configuration using
multi-output mixed regression and classification. <em>PPSN</em>, 87–104.
(<a href="https://doi.org/10.1007/978-3-031-70068-2_6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In landscape-aware algorithm selection problem, the effectiveness of feature-based predictive models strongly depends on the representativeness of training data for practical applications. In this work, we investigate the potential of randomly generated functions (RGF) for the model training, which cover a much more diverse set of optimization problem classes compared to the widely-used black-box optimization benchmarking (BBOB) suite. Correspondingly, we focus on automated algorithm configuration (AAC), that is, selecting the best suited algorithm and fine-tuning its hyperparameters based on the landscape features of problem instances. Precisely, we analyze the performance of dense neural network (NN) models in handling the multi-output mixed regression and classification tasks using different training data sets, such as RGF and many-affine BBOB (MA-BBOB) functions. Based on our results on the BBOB functions in 5d and 20d, near optimal configurations can be identified using the proposed approach, which can most of the time outperform the off-the-shelf default configuration considered by practitioners with limited knowledge about AAC. Furthermore, the predicted configurations are competitive against the single best solver in many cases. Overall, configurations with better performance can be best identified by using NN models trained on a combination of RGF and MA-BBOB functions.},
  archive   = {C_PPSN},
  author    = {Long, Fu Xing and Frenzel, Moritz and Krause, Peter and Gitterle, Markus and Bäck, Thomas and Stein, Niki van},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70068-2_6},
  month     = {9},
  pages     = {87-104},
  title     = {Landscape-aware automated algorithm configuration using multi-output mixed regression and classification},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identifying easy instances to improve efficiency of ML
pipelines for algorithm-selection. <em>PPSN</em>, 70–86. (<a
href="https://doi.org/10.1007/978-3-031-70068-2_5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Algorithm-selection (AS) methods are essential in order to obtain the best performance from a portfolio of solvers over large sets of instances. However, many AS methods rely on an analysis phase, e.g. where features are computed by sampling solutions and used as input in a machine-learning model. For AS to be efficient, it is therefore important that this analysis phase is not computationally expensive. We propose a method for identifying easy instances which can be solved quickly using a generalist solver without any need for algorithm-selection. This saves computational budget associated with feature-computation which can then be used elsewhere in an AS pipeline, e.g., enabling additional function evaluations on hard problems. Experiments on the BBOB dataset in two settings (batch and streaming) show that identifying easy instances results in substantial savings in function evaluations. Re-allocating the saved budget to hard problems provides gains in performance compared to both the virtual best solver (VBS) computed with the original budget, the single best solver (SBS) and a trained algorithm-selector.},
  archive   = {C_PPSN},
  author    = {Renau, Quentin and Hart, Emma},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70068-2_5},
  month     = {9},
  pages     = {70-86},
  title     = {Identifying easy instances to improve efficiency of ML pipelines for algorithm-selection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Emergence of specialised collective behaviors in evolving
heterogeneous swarms. <em>PPSN</em>, 53–69. (<a
href="https://doi.org/10.1007/978-3-031-70068-2_4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Natural groups of animals, such as swarms of social insects, exhibit astonishing degrees of task specialization, useful for solving complex tasks and for survival. This is supported by phenotypic plasticity: individuals sharing the same genotype that is expressed differently for different classes of individuals, each specializing in one task. In this work, we evolve a swarm of simulated robots with phenotypic plasticity to study the emergence of specialized collective behavior during an emergent perception task. Phenotypic plasticity is realized in the form of heterogeneity of behavior by dividing the genotype into two components, with a different neural network controller associated to each component. The whole genotype, which expresses the behavior of the whole group through the two components, is subject to evolution with a single fitness function. We analyze the obtained behaviors and use the insights provided by these results to design an online regulatory mechanism. Our experiments show four main findings: 1) Heterogeneity improves both robustness and scalability; 2) The sub-groups evolve distinct emergent behaviors. 3) The effectiveness of the whole swarm depends on the interaction between the two sub-groups, leading to a more robust performance than with singular sub-group behavior. 4) The online regulatory mechanism improves overall performance and scalability.},
  archive   = {C_PPSN},
  author    = {van Diggelen, Fuda and de Carlo, Matteo and Cambier, Nicolas and Ferrante, Eliseo and Eiben, Guszti},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70068-2_4},
  month     = {9},
  pages     = {53-69},
  title     = {Emergence of specialised collective behaviors in evolving heterogeneous swarms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A deep dive into effects of structural bias on CMA-ES
performance along affine trajectories. <em>PPSN</em>, 36–50. (<a
href="https://doi.org/10.1007/978-3-031-70068-2_3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To guide the design of better iterative optimisation heuristics, it is imperative to understand how inherent structural biases within algorithm components affect the performance on a wide variety of search landscapes. This study explores the impact of structural bias in the modular Covariance Matrix Adaptation Evolution Strategy (modCMA), focusing on the roles of various modulars within the algorithm. Through an extensive investigation involving $$435\,456$$ configurations of modCMA, we identified key modules that significantly influence structural bias of various classes. Our analysis utilized the Deep-BIAS toolbox for structural bias detection and classification, complemented by SHAP analysis for quantifying module contributions. The performance of these configurations was tested on a sequence of affine-recombined functions, maintaining fixed optimum locations while gradually varying the landscape features. Our results demonstrate an interplay between module-induced structural bias and algorithm performance across different landscape characteristics.},
  archive   = {C_PPSN},
  author    = {van Stein, Niki and Thomson, Sarah L. and Kononova, Anna V.},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70068-2_3},
  month     = {9},
  pages     = {36-50},
  title     = {A deep dive into effects of structural bias on CMA-ES performance along affine trajectories},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Empirical analysis of the dynamic binary value problem with
IOHprofiler. <em>PPSN</em>, 20–35. (<a
href="https://doi.org/10.1007/978-3-031-70068-2_2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Optimization problems in dynamic environments have recently been the source of several theoretical studies. One of these problems is the monotonic Dynamic Binary Value problem, which theoretically has high discriminatory power between different Genetic Algorithms. Given this theoretical foundation, we integrate several versions of this problem into the IOHprofiler benchmarking framework. Using this integration, we perform several large-scale benchmarking experiments to both recreate theoretical results on moderate dimensional problems and investigate aspects of GA’s performance which have not yet been studied theoretically. Our results highlight some of the many synergies between theory and benchmarking and offer a platform through which further research into dynamic optimization problems can be performed.},
  archive   = {C_PPSN},
  author    = {Vermetten, Diederick and Lengler, Johannes and Rusin, Dimitri and Bäck, Thomas and Doerr, Carola},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70068-2_2},
  month     = {9},
  pages     = {20-35},
  title     = {Empirical analysis of the dynamic binary value problem with IOHprofiler},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Aggregated partial hypervolumes - an overall indicator for
performance evaluation of multimodal multiobjective optimization
methods. <em>PPSN</em>, 3–19. (<a
href="https://doi.org/10.1007/978-3-031-70068-2_1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multimodal multiobjective optimization (MMMOO) can be perceived as the combination of multiobjective optimization (MOO) and multimodal optimization (MMO). The performance of an MMMOO method should be thus assessed from both perspectives, leading to the prevalence of dual-metric indicators in the existing literature. This study first analyzes the ideal outcome of MMMOO for informed decision-making to determine the prerequisites of a theoretically and practically sound performance indicator. Then, it critically evaluates existing indicators, especially those that intend to measure success from the MMO perspective. Subsequently, it introduces Aggregated Partial Hypervolumes (APHVs) as a novel overall parametric performance indicator that not only addresses the drawbacks of existing ones but can also reflect the relative importance of MMO for the decision-maker. Finally, a few descriptive MMMOO examples are studied to verify that the optimal population according to APHVs matches our understanding of the ideal outcome of MMMOO, taking into account the relative importance of both the MMO and the MOO perspectives.},
  archive   = {C_PPSN},
  author    = {Ahrari, Ali and Sarker, Ruhul and Coello, Carlos A. Coello},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70068-2_1},
  month     = {9},
  pages     = {3-19},
  title     = {Aggregated partial hypervolumes - an overall indicator for performance evaluation of multimodal multiobjective optimization methods},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Regularized feature selection landscapes: An empirical study
of multimodality. <em>PPSN</em>, 409–426. (<a
href="https://doi.org/10.1007/978-3-031-70055-2_25">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The processing of features in data is among the key topics in machine learning. While a broad range of heuristics for feature processing, including feature selection, have been developed and experimented with, less research has been concerned with the underlying fitness landscape. In this paper, we perform a fitness landscape analysis of feature selection, using local optima networks and other methods. We focus on the impact of regularization, an important element of many machine learning methods. Our study using ten datasets and learning of decision trees confirms and adds to previous findings that feature selection landscapes are highly multimodal. It is the first study to focus on the impact of regularization on the landscape induced by feature selection. In the ten datasets studied, we find a high degree of multimodality when there is no regularization. With increasing regularization, the degree of multimodality generally drops off but remains substantial.},
  archive   = {C_PPSN},
  author    = {Sánchez-Díaz, Xavier F. C. and Masson, Corentin and Mengshoel, Ole Jakob},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70055-2_25},
  month     = {9},
  pages     = {409-426},
  title     = {Regularized feature selection landscapes: An empirical study of multimodality},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Over sampling local optima: Selection and sampling bias in
hybrid genetic algorithms. <em>PPSN</em>, 393–408. (<a
href="https://doi.org/10.1007/978-3-031-70055-2_24">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Partition Crossover induces lattices over subsets of local optima in the search spaces of classic combinatorial problems such as MAX-SAT and the Traveling Salesman Problem. This paper explores the interaction between Partition Crossover, the lattices that are produced, and various algorithmic decisions. First, we prove that hard selection such as “truncation selection” will make it more difficult to find opportunities to successfully apply Partition Crossover. This suggests that less aggressive forms of selection could be more productive. Second, we consider hybrid genetic algorithms (GAs) that only recombine solutions that are local optima. We prove that hybrid GAs have an inherent bias that makes them more likely to sample other local optima. These two results can inform the design of more effective hybrid evolutionary algorithms.},
  archive   = {C_PPSN},
  author    = {Whitley, Darrell and Ochoa, Gabriela and Chicano, Francisco},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70055-2_24},
  month     = {9},
  pages     = {393-408},
  title     = {Over sampling local optima: Selection and sampling bias in hybrid genetic algorithms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Entropy, search trajectories, and explainability for
frequency fitness assignment. <em>PPSN</em>, 377–392. (<a
href="https://doi.org/10.1007/978-3-031-70055-2_23">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Local optima are a menace that can trap optimisation processes. Frequency fitness assignment (FFA) is an concept aiming to overcome this problem. It steers the search towards solutions with rare fitness instead of high-quality fitness. FFA-based algorithms have shown promise in the literature, but their behaviour is not well understood. We take a first step in this direction by seeking to explain FFA behaviour and performance for the first time. In particular, we attempt to understand the difference in how FFA-based algorithms navigate the space when compared with a standard objective-guided algorithm which incorporates diversification: simulated annealing (SA). As a testbed for these investigations, a set of quadratic assignment problem (QAP) benchmark instances designed to be difficult for metaheuristics is used. A statistical analysis of trajectory behaviours for FFA-based algorithms is conducted. Additionally, we consider and compare the fitness distributions encountered by each algorithm, as well their respective proficiency on the problems. The findings help to explain FFA performance behaviours, and show that FFA explores more widely and consistently than SA. It is hoped that the explanatory approach adopted in this study serves as an example and inspires further similar investigations into how—and why—FFA-assisted optimisation works.},
  archive   = {C_PPSN},
  author    = {Thomson, Sarah L. and Ochoa, Gabriela and van den Berg, Daan and Liang, Tianyu and Weise, Thomas},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70055-2_23},
  month     = {9},
  pages     = {377-392},
  title     = {Entropy, search trajectories, and explainability for frequency fitness assignment},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contrasting the landscapes of feature selection under
different machine learning models. <em>PPSN</em>, 360–376. (<a
href="https://doi.org/10.1007/978-3-031-70055-2_22">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Feature selection plays a crucial role in improving the performance of machine learning (ML) models for various prediction tasks and in explaining their recommendations. Feature selection can be defined as an optimization problem whose evaluation function calls on an ML algorithm—a method known as the wrapper approach. While a thorough understanding of the landscape of the feature selection problem might help guide the development of efficient evolutionary algorithms and algorithm selection technologies, only a couple of previous studies have explored this problem’s landscape. In addition, only k-nearest neighbors classification is typically used as an ML model. This paper investigates how the choice of an ML model influences the search difficulty of the feature selection problem. Specifically, we examine the feature selection problem with 14 classification datasets and 6 ML models by means of landscape analysis and local optima networks, and we relate them to the performance of three feature selection algorithms. Our findings have important implications for feature selection problems and algorithms.},
  archive   = {C_PPSN},
  author    = {Liefooghe, Arnaud and Tanabe, Ryoji and Verel, Sébastien},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70055-2_22},
  month     = {9},
  pages     = {360-376},
  title     = {Contrasting the landscapes of feature selection under different machine learning models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Funnels in multi-objective fitness landscapes.
<em>PPSN</em>, 343–359. (<a
href="https://doi.org/10.1007/978-3-031-70055-2_21">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Funnels are related to the big-valley hypothesis in combinatorial fitness landscapes. It suggests that local optima are not randomly distributed but are instead clustered around the global optimum, forming a coarse-grained global structure. Multi-funnel structures emerge when more than one cluster of local optima is present, some surrounding sub-optimal solutions. These multi-funnel landscapes can be challenging to search, as the optimisation process may get trapped in a sub-optimal funnel. We propose a characterisation of funnels in multi-objective combinatorial landscapes based on the solution ranks using non-dominated sorting, and a variation of the recent graph model of multi-objective landscapes: the compressed Pareto local optimal solution network (C-PLOS-net). Using a set of $$\rho $$ mnk-landscapes, we construct and visualise monotonic C-PLOS-nets, and introduce a set of metrics to characterise the landscapes’ funnel structure. The proposed metrics are found to capture the landscape global structure, to correlate with benchmark parameters, and to explain the performance of well-established multi-objective local search and evolutionary algorithms.},
  archive   = {C_PPSN},
  author    = {Ochoa, Gabriela and Liefooghe, Arnaud and Verel, Sébastien},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70055-2_21},
  month     = {9},
  pages     = {343-359},
  title     = {Funnels in multi-objective fitness landscapes},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing the computational efficiency of genetic
programming through alternative floating-point primitives.
<em>PPSN</em>, 322–339. (<a
href="https://doi.org/10.1007/978-3-031-70055-2_20">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Can evolution operate effectively with noisy floating-point function primitives? In this paper, we are motivated by recent work that aims to accelerate genetic programming (GP) through specialized hardware and field-programmable gate arrays (FPGAs), for which it has been shown that additional performance and power/energy benefits could likely be achieved with floating-point function primitives that trade off enhanced computational efficiency for increased error. Although GP is known to be robust in filtering out certain forms of noise (e.g., within input data), it is not immediately clear that less-accurate function primitives would be viable for GP, since GP formulates arbitrary compositions of its primitives, which could potentially compound error to a prohibitive level. In addition, when introducing more complex forms of computation, such as function differentiation and local optimization techniques, it is not readily apparent that using rougher primitive implementations would be tenable. Here, we address both situations by employing the state-of-the-art CPU-based Operon tool on a diverse set of 15 regression problems, and we show that tree-based GP is capable of evolving very similar (and sometimes better) results with alternative high-performance approximations of standard function primitives, while often also allowing for faster CPU runtimes. Most importantly, in the context of specialized hardware, we conclude that our proposed techniques can likely allow for significant speedups over general-purpose computing platforms, as well as improved power/energy efficiency.},
  archive   = {C_PPSN},
  author    = {Crary, Christopher and Burlacu, Bogdan and Banzhaf, Wolfgang},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70055-2_20},
  month     = {9},
  pages     = {322-339},
  title     = {Enhancing the computational efficiency of genetic programming through alternative floating-point primitives},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multimodal adaptive graph evolution for program synthesis.
<em>PPSN</em>, 306–321. (<a
href="https://doi.org/10.1007/978-3-031-70055-2_19">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Program synthesis constitutes a category of problems where the objective is to automatically produce computer programs that meet specified criteria. Among Genetic Programming algorithms, Cartesian Genetic Programming has been successfully used for a variety of function synthesis problems, such as circuit design, pattern analysis, and game playing. These problems are designed to work only on a single data type, for example, boolean values or entire images. Cartesian Genetic Programming cannot directly be applied to problems with multiple data types, which poses a great limitation, as more realistic programs should be able to deal with different data types. Mixed-Type Cartesian Genetic Programming is the only current extension of Cartesian Genetic Programming which allows for processing different data types. In this work, we present and study Multimodal Adaptive Graph Evolution, a multi-chromosome generalization of Cartesian Genetic Programming that groups functions by return type and constrains graph mutation based on node’s type coherence. We compare Multimodal Adaptive Graph Evolution to Mixed-Type Cartesian Genetic Programming on the Program Synthesis Benchmark Suite, showing that the representation and mutation constraints of Multimodal Adaptive Graph Evolution aid in the search of multimodal functions. Using Search Trajectory Networks, we find that Multimodal Adaptive Graph Evolution converges faster to a local or global minimum compared to Mixed-Type Cartesian Genetic Programming and explores the solution space more effectively by creating candidate solutions with lower semantic redundancy.},
  archive   = {C_PPSN},
  author    = {De La Torre, Camilo and Lavinas, Yuri and Cortacero, Kevin and Luga, Hervé and Wilson, Dennis G. and Cussat-Blanc, Sylvain},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70055-2_19},
  month     = {9},
  pages     = {306-321},
  title     = {Multimodal adaptive graph evolution for program synthesis},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decision tree based wrappers for hearing loss.
<em>PPSN</em>, 290–305. (<a
href="https://doi.org/10.1007/978-3-031-70055-2_18">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Audiology entities are using Machine Learning (ML) models to guide their screening towards people at risk. Feature Engineering (FE) focuses on optimizing data for ML models, with evolutionary methods being effective in feature selection and construction tasks. This work aims to benchmark an evolutionary FE wrapper, using models based on decision trees as proxies. The FEDORA framework is applied to a Hearing Loss (HL) dataset, being able to reduce data dimensionality and statistically maintain baseline performance. Compared to traditional methods, FEDORA demonstrates superior performance, with a maximum balanced accuracy of 76.2%, using 57 features. The framework also generated an individual that achieved 72.8% balanced accuracy using a single feature.},
  archive   = {C_PPSN},
  author    = {Rabuge, Miguel and Lourenço, Nuno},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70055-2_18},
  month     = {9},
  pages     = {290-305},
  title     = {Decision tree based wrappers for hearing loss},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The inefficiency of genetic programming for symbolic
regression. <em>PPSN</em>, 273–289. (<a
href="https://doi.org/10.1007/978-3-031-70055-2_17">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We analyse the search behaviour of genetic programming (GP) for symbolic regression (SR) in search spaces that are small enough to allow exhaustive enumeration, and use an improved exhaustive symbolic regression algorithm to generate the set of semantically unique expression structures, which is orders of magnitude smaller than the original SR search space. The efficiency of GP and a hypothetical random search in this set of unique expressions is compared, whereby the efficiency is quantified via the number of function evaluations performed until a given error threshold is reached, and the percentage of unique expressions evaluated during the search after simplification to a canonical form. The results for two real-world datasets with a single input variable show that GP in such limited search space explores only a small fraction of the search space, and evaluates semantically equivalent expressions repeatedly. GP has a smaller success probability than the idealised random search for such small search spaces.},
  archive   = {C_PPSN},
  author    = {Kronberger, Gabriel and Olivetti de Franca, Fabricio and Desmond, Harry and Bartlett, Deaglan J. and Kammerer, Lukas},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70055-2_17},
  month     = {9},
  pages     = {273-289},
  title     = {The inefficiency of genetic programming for symbolic regression},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive sampling of biomedical images with cartesian
genetic programming. <em>PPSN</em>, 256–272. (<a
href="https://doi.org/10.1007/978-3-031-70055-2_16">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this contribution we study how to effectively evolve programs tailored for biomedical image segmentation by using an Active Learning approach in Cartesian Genetic Programming (CGP). Active Learning allows to dynamically select training data by identifying the most informative next image to add to the training set. We study how different metrics for selecting images under active learning impact the searchability of CGP. Our results show that datasets built during evolution with active learning improve the performance of Cartesian GP substantially. In addition, we found that the choice of the particular metric used for selecting which images to add heavily impacts convergence speed. Our work shows that the right choice of the image selection metric positively impacts the effectiveness of the evolutionary algorithm.},
  archive   = {C_PPSN},
  author    = {Lavinas, Yuri and Haut, Nathan and Punch, William and Banzhaf, Wolfgang and Cussat-Blanc, Sylvain},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70055-2_16},
  month     = {9},
  pages     = {256-272},
  title     = {Adaptive sampling of biomedical images with cartesian genetic programming},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simultaneous model-based evolution of constants and
expression structure in GP-GOMEA for symbolic regression. <em>PPSN</em>,
238–255. (<a
href="https://doi.org/10.1007/978-3-031-70055-2_15">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Genetic programming (GP) approaches are among the state-of-the-art for symbolic regression, the task of constructing symbolic expressions that fit well with data. To find highly accurate symbolic expressions, both the expression structure and any contained real-valued constants, are important. GP-GOMEA, a modern model-based evolutionary algorithm, is one of the leading algorithms for finding accurate, yet compact expressions. Yet, GP-GOMEA does not perform dedicated constant optimization, but rather uses ephemeral random constants. Hence, the accuracy of GP-GOMEA may well still be improved upon by the incorporation of a constant optimization mechanism. Existing research into mixed discrete-continuous optimization with EAs has shown that a simultaneous and well-integrated approach to optimizing both discrete and continuous parts, leads to the best results on a variety of problems, especially when there are interactions between these parts. In this paper, we therefore propose a novel approach where constants in expressions are optimized at the same time as the expression structure by merging the real-valued variant of GOMEA with GP-GOMEA. The proposed approach is compared to other forms of handling constants in GP-GOMEA, and in the context of other commonly used techniques such as linear scaling, restarts, and constant tuning after GP optimization. Our results indicate that our novel approach generally performs best and confirms the importance of simultaneous constant optimization during evolution.},
  archive   = {C_PPSN},
  author    = {Koch, Johannes and Alderliesten, Tanja and Bosman, Peter A. N.},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70055-2_15},
  month     = {9},
  pages     = {238-255},
  title     = {Simultaneous model-based evolution of constants and expression structure in GP-GOMEA for symbolic regression},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Symbol graph genetic programming for symbolic regression.
<em>PPSN</em>, 221–237. (<a
href="https://doi.org/10.1007/978-3-031-70055-2_14">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper tackles the challenge of symbolic regression (SR) with a vast mathematical expression space, where the primary difficulty lies in accurately identifying subspaces that are more likely to contain the correct mathematical expressions. Establishing the NP-hard nature of the SR problem, this study introduces a novel approach named Symbol Graph Genetic Programming (SGGP) (Code is available at https://github.com/SymbolGraph/sggp ). SGGP begins by constructing a symbol graph to represent the mathematical expression space effectively. It then employs the generalized Pareto distribution based on semantic similarity to assess the likelihood that each edge (subspace) in this graph will yield superior individuals. Guided by these probabilistic evaluations, SGGP strategically samples new individuals in its quest to discover accurate mathematical expressions. Comparative experiments conducted across three different benchmark types demonstrate that SGGP outperforms 21 existing baseline SR methods, achieving greater accuracy and conciseness in the mathematical expressions it generates.},
  archive   = {C_PPSN},
  author    = {Song, Jinglu and Lu, Qiang and Tian, Bozhou and Zhang, Jingwen and Luo, Jake and Wang, Zhiguang},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70055-2_14},
  month     = {9},
  pages     = {221-237},
  title     = {Symbol graph genetic programming for symbolic regression},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). P-mixup: Improving generalization performance of
evolutionary feature construction with pessimistic vicinal risk
minimization. <em>PPSN</em>, 201–220. (<a
href="https://doi.org/10.1007/978-3-031-70055-2_13">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Genetic programming (GP)-based feature construction has achieved great success as an automated machine learning technique to improve learning performance. The key challenge in GP-based feature construction is that it is easy to overfit the training data. In supervised learning, unseen data usually lie in the vicinity of the training data and behave similar to the training data. However, a rugged model may make significantly different predictions, thus resulting in poor generalization performance. Here, we propose pessimistic vicinal risk minimization method to control overfitting in GP-based feature construction. The idea is to minimize the worst-case loss on vicinal examples of training instances, where vicinal examples are synthesized using an instance-wise mixing method. The experimental results on 58 datasets demonstrate that GP with the proposed overfitting control method clearly outperforms standard GP and seven other overfitting control methods for GP, validating the superiority of using pessimistic vicinal risk minimization to control overfitting in GP for feature construction.},
  archive   = {C_PPSN},
  author    = {Zhang, Hengzhe and Chen, Qi and Xue, Bing and Banzhaf, Wolfgang and Zhang, Mengjie},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70055-2_13},
  month     = {9},
  pages     = {201-220},
  title     = {P-mixup: Improving generalization performance of evolutionary feature construction with pessimistic vicinal risk minimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving the performance of relocation rules for the
container relocation problem with the rollout algorithm. <em>PPSN</em>,
184–200. (<a
href="https://doi.org/10.1007/978-3-031-70055-2_12">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Container relocation problems represent a significant challenge in maritime ports and terminals. To address this challenge, there is a growing demand for innovative and efficient solution methods. While exact and metaheuristic methods often yield superior results, they require a substantial time to reach good solutions. On the other hand, relocation rules (RRs) represent simple yet efficient constructive heuristics. Nevertheless, RRs suffer from two main issues, they are difficult to design for different problem variants and their performance is quite limited. To tackle the first issue, genetic programming is commonly used to automatically generate RRs. However, regarding the second issue, there is no single approach by which their performance can be improved. In this study, we investigate the application of the rollout algorithm in combination with manually and automatically generated RRs to improve their performance. The idea of using the rollout algorithm is to balance between an exhaustive and heuristic search, where RRs are used to determine the most appropriate decision in each step of the rollout algorithm. The results demonstrate that with the use of the rollout algorithm it is possible to significantly improve the performance of RRs, albeit with increased execution time. Nevertheless, even in this case, the method can still solve all the considered problems within seconds, underscoring its effectiveness.},
  archive   = {C_PPSN},
  author    = {Đurasević, Marko and Đumić, Mateja and Gil-Gala, Francisco Javier and Frid, Nikolina and Jakobović, Domagoj},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70055-2_12},
  month     = {9},
  pages     = {184-200},
  title     = {Improving the performance of relocation rules for the container relocation problem with the rollout algorithm},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unit-aware genetic programming for the development of
empirical equations. <em>PPSN</em>, 168–183. (<a
href="https://doi.org/10.1007/978-3-031-70055-2_11">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When developing empirical equations, domain experts require these to be accurate and adhere to physical laws. Often, constants with unknown units need to be discovered alongside the equations. Traditional unit-aware genetic programming (GP) approaches cannot be used when unknown constants with undetermined units are included. This paper presents a method for dimensional analysis that propagates unknown units as “jokers” and returns the magnitude of unit violations. We propose three methods, namely evolutive culling, a repair mechanism, and a multi-objective approach, to integrate the dimensional analysis in the GP algorithm. Experiments on datasets with ground truth demonstrate comparable performance of evolutive culling and the multi-objective approach to a baseline without dimensional analysis. Extensive analysis of the results on datasets without ground truth reveals that the unit-aware algorithms make only low sacrifices in accuracy, while producing unit-adherent solutions.},
  archive   = {C_PPSN},
  author    = {Reuter, Julia and Martinek, Viktor and Herzog, Roland and Mostaghim, Sanaz},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70055-2_11},
  month     = {9},
  pages     = {168-183},
  title     = {Unit-aware genetic programming for the development of empirical equations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Positional bias does not influence cartesian genetic
programming with crossover. <em>PPSN</em>, 151–167. (<a
href="https://doi.org/10.1007/978-3-031-70055-2_10">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The recombination operator plays an important role in many evolutionary algorithms. However, in Cartesian Genetic Programming (CGP), which is part of the aforementioned category, the usefulness of crossover is contested. In this work, we investigate whether CGP’s positional bias actually influences the usefulness of the crossover operator negatively. This bias describes a skewed distribution of CGP’s active and inactive nodes, which might lead to destructive behaviours of standard recombination operators. We try to answer our hypothesis by employing one standard CGP implementation and one without the effects of positional bias. Both versions are combined with one of four standard crossover operators, or with no crossover operator. Additionally, two different selection methods are used to configure a CGP variant. We then analyse their performance and convergence behaviour on eight benchmarks taken from the Boolean and symbolic regression domain. By using Bayesian inference, we are able to rank them, and we found that positional bias does not influence CGP with crossover. Furthermore, we argue that the current research on CGP with standard crossover operators is incomplete, and CGP with recombination might not negatively impact its evolutionary search process. On the contrary, using CGP with crossover improves its performance.},
  archive   = {C_PPSN},
  author    = {Cui, Henning and Heider, Michael and Hähner, Jörg},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70055-2_10},
  month     = {9},
  pages     = {151-167},
  title     = {Positional bias does not influence cartesian genetic programming with crossover},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Knowledge-guided optimization for complex vehicle routing
with 3D loading constraints. <em>PPSN</em>, 133–148. (<a
href="https://doi.org/10.1007/978-3-031-70055-2_9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The split delivery vehicle routing problem with three-dimensional loading constraints (3L-SDVRP) intertwines complex routing and packing challenges. The current study addresses 3L-SDVRP using intelligent optimization algorithms, which iteratively evolve towards optimal solutions. A pivotal aspect of these algorithms is search operators that determine the search direction and the search step size. Effective operators significantly improve algorithmic performance. Traditional operators like swap, shift, and 2-opt fall short in complex scenarios like 3L-SDVRP, mainly due to their limited capacity to leverage domain knowledge. Additionally, the search step size is crucial: smaller steps enhance fine-grained search (exploitation), while larger steps facilitate exploring new areas (exploration). However, optimally balancing these step sizes remains an unresolved issue in 3L-SDVRP. To address this, we introduce an adaptive knowledge-guided insertion (AKI) operator. This innovative operator uses node distribution characteristics for adaptive node insertion, enhancing search abilities through domain knowledge integration and larger step sizes. Integrating AKI with the local search framework, we develop an adaptive knowledge-guided search (AKS) algorithm, which effectively balances exploitation and exploration by combining traditional neighbourhood operators for detailed searches with the AKI operator for broader exploration. Our experiments demonstrate that the AKS algorithm significantly outperforms the state-of-the-art method in solving various 3L-SDVRP instances.},
  archive   = {C_PPSN},
  author    = {Zhang, Han and Li, Qing and Yao, Xin},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70055-2_9},
  month     = {9},
  pages     = {133-148},
  title     = {Knowledge-guided optimization for complex vehicle routing with 3D loading constraints},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-objective evolutionary approaches for the knapsack
problem with stochastic profits. <em>PPSN</em>, 116–132. (<a
href="https://doi.org/10.1007/978-3-031-70055-2_8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Uncertainties in real-world problems impose a challenge in finding reliable solutions. If mishandled, they can lead to suboptimal or infeasible solutions. Chance constraints are a natural way to capture uncertain problem parameters. They model probabilistic constraints involving the stochastic parameters and an upper bound of probability that mimics the confidence level of the solution. We focus on the knapsack problem with stochastic profits to guarantee a certain level of confidence in the profit of the solutions. We present a bi-objective fitness formulation that uses expected profit and standard deviation to capture the chance constraints. This formulation enables optimising the problem independent of a specific confidence level. We evaluate the proposed fitness formulation using well-known evolutionary algorithms GSEMO, NSGA-II and MOEA/D. Moreover, we introduce a filtering method that refines the interim populations based on the confidence levels of its solutions. We evaluate this method by applying it along with GSEMO to improve the quality of its population during optimisation. We conduct extensive experiments to show the effectiveness of these approaches using several benchmarks and present a detailed analysis of the results.},
  archive   = {C_PPSN},
  author    = {Perera, Kokila Kasuni and Neumann, Frank and Neumann, Aneta},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70055-2_8},
  month     = {9},
  pages     = {116-132},
  title     = {Multi-objective evolutionary approaches for the knapsack problem with stochastic profits},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dancing to the state of the art? <em>PPSN</em>, 100–115. (<a
href="https://doi.org/10.1007/978-3-031-70055-2_7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Solving the Traveling Salesperson Problem (TSP) remains a persistent challenge, despite its fundamental role in numerous generalized applications in modern contexts. Heuristic solvers address the demand for finding high-quality solutions efficiently. Among these solvers, the Lin-Kernighan-Helsgaun (LKH) heuristic stands out, as it complements the performance of genetic algorithms across a diverse range of problem instances. However, frequent timeouts on challenging instances hinder the practical applicability of the solver. Within this work, we investigate a previously overlooked factor contributing to many timeouts: The use of a fixed candidate set based on a tree structure. Our investigations reveal that candidate sets based on Hamiltonian circuits contain more optimal edges. We thus propose to integrate this promising initialization strategy, in the form of POPMUSIC, within an efficient restart version of LKH. As confirmed by our experimental studies, this refined TSP heuristic is much more efficient – causing fewer timeouts and improving the performance (in terms of penalized average runtime) by an order of magnitude – and thereby challenges the state of the art in TSP solving.},
  archive   = {C_PPSN},
  author    = {Heins, Jonathan and Schäpermeier, Lennart and Kerschke, Pascal and Whitley, Darrell},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70055-2_7},
  month     = {9},
  pages     = {100-115},
  title     = {Dancing to the state of the art?},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning a prior for monte carlo search by replaying
solutions to combinatorial problems. <em>PPSN</em>, 85–99. (<a
href="https://doi.org/10.1007/978-3-031-70055-2_6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Monte Carlo Search gives excellent results in multiple difficult combinatorial problems. Using a prior to perform non uniform playouts during the search improves a lot the results compared to uniform playouts. Handmade heuristics tailored to the combinatorial problem are often used as priors. We propose a method to automatically compute a prior. It uses statistics on solved problems. It is a simple and general method that incurs no computational cost at playout time and that brings large performance gains. The method is applied to three difficult combinatorial problems: Latin Square Completion, Kakuro, and Inverse RNA Folding.},
  archive   = {C_PPSN},
  author    = {Cazenave, Tristan},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70055-2_6},
  month     = {9},
  pages     = {85-99},
  title     = {Learning a prior for monte carlo search by replaying solutions to combinatorial problems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ant colony optimization for the dynamic electric vehicle
routing problem. <em>PPSN</em>, 68–84. (<a
href="https://doi.org/10.1007/978-3-031-70055-2_5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traffic congestion significantly affects the efficiency of electric vehicles (EVs), especially during extended periods of low-speed conditions, which, in this context, will violate battery capacity of the vehicle. This study addresses the dynamic electric vehicle routing problem (DEVRP), focusing on minimizing the impact of traffic congestion. Using the proven adaptation capabilities and behaviors of ant colonies, we applied the ant colony optimization (ACO) approach to improve vehicle performance under dynamic traffic conditions. Specifically, our experimental findings, on a set of benchmark generated test cases, demonstrate the effectiveness of transferring knowledge from previously optimized environments rather than optimizing from ground up. The advantage of ACO in DEVRP highlights the importance of adaptive-learning, knowledge-based, and decision-making in optimizing EV routes, presenting a promising path for future research in intelligent transportation systems.},
  archive   = {C_PPSN},
  author    = {Anastasiadou, Maria N. and Mavrovouniotis, Michalis and Hadjimitsis, Diofantos},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70055-2_5},
  month     = {9},
  pages     = {68-84},
  title     = {Ant colony optimization for the dynamic electric vehicle routing problem},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalizing and unifying gray-box combinatorial
optimization operators. <em>PPSN</em>, 52–67. (<a
href="https://doi.org/10.1007/978-3-031-70055-2_4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Gray-box optimization leverages the information available about the mathematical structure of an optimization problem to design efficient search operators. Efficient hill climbers and crossover operators have been proposed in the domain of pseudo-Boolean optimization and also in some permutation problems. However, there is no general rule on how to design these efficient operators in different representation domains. This paper proposes a general framework that encompasses all known gray-box operators for combinatorial optimization problems. The framework is general enough to shed light on the design of new efficient operators for new problems and representation domains. We also unify the proofs of efficiency for gray-box hill climbers and crossovers and show that the mathematical property explaining the speed-up of gray-box crossover operators, also explains the efficient identification of improving moves in gray-box hill climbers. We illustrate the power of the new framework by proposing an efficient hill climber and crossover for two related permutation problems: the Linear Ordering Problem and the Single Machine Total Weighted Tardiness Problem.},
  archive   = {C_PPSN},
  author    = {Chicano, Francisco and Whitley, Darrell and Ochoa, Gabriela and Tinós, Renato},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70055-2_4},
  month     = {9},
  pages     = {52-67},
  title     = {Generalizing and unifying gray-box combinatorial optimization operators},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GPGLS: Genetic programming guided local search for
large-scale vehicle routing problems. <em>PPSN</em>, 36–51. (<a
href="https://doi.org/10.1007/978-3-031-70055-2_3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Vehicle Routing Problem (VRP) is a classical combinatorial optimization problem. In this paper, we focus on Large-Scale VRP (LSVRP), which contains more than 200 customers. In particular, the Knowledge Guided Local Search (KGLS) has shown highly competitive performance for LSVRP, due to the strength of GLS for jumping out of local optima and improved utility functions of GLS. The newly discovered good or effective utility function used by KGLS suggests that the default utility function used in the traditional GLS is by no means the optimal. However, manually designing better utility function for GLS is very time-consuming and can involve much trial-and-error. To address this issue, we proposed to use Genetic Programming (GP) to automatically design utility functions for GLS. We developed a GP training framework in which an individual stands for a possible utility function for GLS. To evaluate a GP individual, GLS runs on the training instances, where the GP individual is used as the utility function to identify the edges to penalize. We also designed a set of terminals to capture a wide range of possible factors for the utility function. The results on the commonly used X dataset demonstrates that GP successfully evolved significantly better GLS algorithms than the competitive KGLS on a majority of the large-scale X instances. The further analysis also shows the effectiveness of the newly learned GLS utility functions that take into account new factors which are not been considered by GLS and KGLS.},
  archive   = {C_PPSN},
  author    = {Liu, Saining and Cavalcanti Costa, Joao Guilherme and Mei, Yi and Zhang, Mengjie},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70055-2_3},
  month     = {9},
  pages     = {36-51},
  title     = {GPGLS: Genetic programming guided local search for large-scale vehicle routing problems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sliding window bi-objective evolutionary algorithms for
optimizing chance-constrained monotone submodular functions.
<em>PPSN</em>, 20–35. (<a
href="https://doi.org/10.1007/978-3-031-70055-2_2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Variants of the GSEMO algorithm using multi-objective formulations have been successfully analyzed and applied to optimize chance-constrained submodular functions. However, due to the effect of the increasing population size of the GSEMO algorithm considered in these studies from the algorithms, the approach becomes ineffective if the number of trade-offs obtained grows quickly during the optimization run. In this paper, we apply the sliding-selection approach introduced in [21] to the optimization of chance-constrained monotone submodular functions. We theoretically analyze the resulting SW-GSEMO algorithm which successfully limits the population size as a key factor that impacts the runtime and show that this allows it to obtain better runtime guarantees than the best ones currently known for the GSEMO. In our experimental study, we compare the performance of the SW-GSEMO to the GSEMO and NSGA-II on the maximum coverage problem under the chance constraint and show that the SW-GSEMO outperforms the other two approaches in most cases. In order to get additional insights into the optimization behavior of SW-GSEMO, we visualize the selection behavior of SW-GSEMO during its optimization process and show it beats other algorithms to obtain the highest quality of solution in variable instances.},
  archive   = {C_PPSN},
  author    = {Yan, Xiankun and Neumann, Aneta and Neumann, Frank},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70055-2_2},
  month     = {9},
  pages     = {20-35},
  title     = {Sliding window bi-objective evolutionary algorithms for optimizing chance-constrained monotone submodular functions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the design of diploid memetic algorithms for solving the
multidimensional multi-way number partitioning problem. <em>PPSN</em>,
3–19. (<a href="https://doi.org/10.1007/978-3-031-70055-2_1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we investigate the ability and the performance of a diploid memetic algorithm (DMA) to solve the multidimensional multi-way number partitioning problem (MDMWNPP). Given a multiset consisting of a number of vectors of fixed dimension, the MDMWNPP searches for a partition of the vectors into a given number of subsets with the property that the sums of the elements in each subset are equal or almost equal for all the coordinates of the vectors. We design an enhanced genetic algorithm using diploidy to maintain diversity of the population for solving the MDMWNPP. The resulted diploid genetic algorithm (DGA) is hybridized by incorporating a local search procedure to guide the search towards the most promising search regions of the solution space, obtaining a diploid memetic algorithm. We report preliminary computational results on a set of standard benchmark instances from the literature to assess the performance of our developed DMA. The achieved computational results show that our novel solution approach compares favorably against the existing state-of-the-art algorithms. These findings were confirmed by the performed statistical evaluation. Finally, we conduct ablation studies on key algorithmic components to confirm their novelty and effectiveness.},
  archive   = {C_PPSN},
  author    = {Petrovan, Adrian and Pop, Petrică C. and Sabo, Cosmin},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-70055-2_1},
  month     = {9},
  pages     = {3-19},
  title     = {On the design of diploid memetic algorithms for solving the multidimensional multi-way number partitioning problem},
  year      = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
