<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ICRA_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="icra---1760">ICRA - 1760</h2>
<ul>
<li><details>
<summary>
(2024). Generative modeling of residuals for real-time
risk-sensitive safety with discrete-time control barrier functions.
<em>ICRA</em>, i–viii. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A key source of brittleness for robotic systems is the presence of model uncertainty and external disturbances. Most existing approaches to robust control either seek to bound the worst-case disturbance (which results in conservative behavior), or to learn a deterministic dynamics model (which is unable to capture uncertain dynamics or disturbances). This work proposes a different approach: training a state-conditioned generative model to represent the distribution of error residuals between the nominal dynamics and the actual system. In particular we introduce the Online Risk-Informed Optimization controller (ORIO), which uses Discrete-Time Control Barrier Functions, combined with a learned, generative disturbance model, to ensure the safety of the system up to some level of risk. We demonstrate our approach in simulations and hardware, and show that our method can learn a disturbance model that is accurate enough to enable risk-sensitive control of a quadrotor flying aggressively with an unmodelled slung load. We use a conditional variational autoencoder (CVAE) to learn a state-conditioned dynamics residual distribution, and find that the resulting controller can run at 100Hz on an embedded computer and exhibits less conservative behavior while retaining theoretical safety properties.},
  archive   = {C_ICRA},
  author    = {Ryan K. Cosner and Igor Sadalski and Jana K. Woo and Preston Culbertson and Aaron D. Ames},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611355},
  month     = {5},
  pages     = {i-viii},
  title     = {Generative modeling of residuals for real-time risk-sensitive safety with discrete-time control barrier functions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards geometric motion planning for high-dimensional
systems: Gait-based coordinate optimization and local metrics.
<em>ICRA</em>, 18494–18500. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Geometric motion planning offers effective and interpretable gait analysis and optimization tools for locomoting systems. However, due to the curse of dimensionality in coordinate optimization, a key component of geometric motion planning, it is almost infeasible to apply current geometric motion planning to high-dimensional systems. In this paper, we propose a gait-based coordinate optimization method that overcomes the curse of dimensionality. We also identify a unified geometric representation of locomotion by generalizing various nonholonomic constraints into local metrics. By combining these two approaches, we take a step towards geometric motion planning for high-dimensional systems. We test our method in two classes of high-dimensional systems - low Reynolds number swimmers and free-falling Cassie - with up to 11-dimensional shape variables. The resulting optimal gait in the high-dimensional system shows better efficiency compared to that of the reduced-order model. Furthermore, we provide a geometric optimality interpretation of the optimal gait.},
  archive   = {C_ICRA},
  author    = {Yanhao Yang and Capprin Bass and Ross L. Hatton},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610063},
  month     = {5},
  pages     = {18494-18500},
  title     = {Towards geometric motion planning for high-dimensional systems: Gait-based coordinate optimization and local metrics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Floating-base manipulation on zero-perturbation manifolds.
<em>ICRA</em>, 18487–18493. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To achieve high-dexterity motion planning on floating-base systems, the base dynamics induced by arm motions must be treated carefully. In general, it is a significant challenge to establish a fixed-base frame during tasking due to forces and torques on the base that arise directly from arm motions (e.g. arm drag in low Reynolds environments and arm momentum in high Reynolds environments). While thrusters can in theory be used to regulate the vehicle pose, it is often insufficient to establish a stable pose for precise tasking, whether the cause be due to underactuation, modeling inaccuracy, suboptimal control parameters, or insufficient power. We propose a solution that asks the thrusters to do less high bandwidth perturbation correction by planning arm motions that induce zero perturbation on the base. We are able to cast our motion planner as a nonholonomic rapidly-exploring random tree (RRT) by representing the floating-base dynamics as pfaffian constraints on joint velocity. These constraints guide the manipulators to move on zero-perturbation manifolds (which inhabit a subspace of the tangent space of the internal configuration space). To invoke this representation (termed a perturbation map) we assume the body velocity (perturbation) of the base to be a joint-defined linear mapping of joint velocity and describe situations where this assumption is realistic (including underwater, aerial, and orbital environments). The core insight of this work is that when perturbation of the floating-base has affine structure with respect to joint velocity, it provides the system a class of kinematic reduction that permits the use of sample-based motion planners (specifically a nonholonomic RRT). We show that this allows rapid, exploration-geared motion planning for high degree of freedom systems in obstacle rich environments, even on floating-base systems with nontrivial dynamics.},
  archive   = {C_ICRA},
  author    = {Brian A. Bittner and Jason Reid and Kevin C. Wolfe},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611217},
  month     = {5},
  pages     = {18487-18493},
  title     = {Floating-base manipulation on zero-perturbation manifolds},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LORIS: A lightweight free-climbing robot for extreme terrain
exploration. <em>ICRA</em>, 18480–18486. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Climbing robots can investigate scientifically valuable sites that conventional rovers cannot access due to steep terrain features. Robots equipped with microspine grippers are particularly well-suited to ascending rocky cliff faces, but most existing designs are either large and slow or limited to relatively flat surfaces such as walls. We present a novel free-climbing robot to bridge this gap through innovations in gripper design and force control. Fully passive grippers and wrist joints allow secure grasping while reducing mass and complexity. Forces are distributed among the robot’s grippers using an optimization-based control strategy to minimize the risk of unexpected detachment. The robot prototype has demonstrated vertical climbing on both flat cinder block walls and uneven rock surfaces in full Earth gravity.},
  archive   = {C_ICRA},
  author    = {Paul Nadan and Spencer Backus and Aaron M. Johnson},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611653},
  month     = {5},
  pages     = {18480-18486},
  title     = {LORIS: A lightweight free-climbing robot for extreme terrain exploration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and implementation of a robotic testbench for
analyzing pincer grip execution in human specimen hands. <em>ICRA</em>,
18465–18471. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study presents an innovative test rig engineered to explore the kinematic and viscoelastic characteristics of human specimen hands. The rig features eight force-controlled motors linked to muscle tendons, enabling precise stimulation of hand specimens. Hand movements are monitored through an optical tracking system, while a force-torque sensor quantifies the resultant fingertip loads. Employing this setup, we successfully demonstrated a pincer grip using a cadaver hand and measured both muscle forces and grip strength. Our results reveal a nonlinear relationship between tendon forces and grip strength, which can be modeled by an exponential fit. This investigation serves as a nexus between biomechanical and robotics-focused research, providing critical insights for the advancement of robotic hand actuation and therapeutic interventions.},
  archive   = {C_ICRA},
  author    = {Nikolas Wilhelm and Claudio Glowalla and Sami Haddadin and Julian Schote and Hannes Höppner and Patrick van der Smagt and Maximilian Karl and Rainer Burgkart},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610715},
  month     = {5},
  pages     = {18465-18471},
  title     = {Design and implementation of a robotic testbench for analyzing pincer grip execution in human specimen hands},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Probabilistic 3D multi-object cooperative tracking for
autonomous driving via differentiable multi-sensor kalman filter.
<em>ICRA</em>, 18458–18464. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current state-of-the-art autonomous driving vehicles mainly rely on each individual sensor system to perform perception tasks. Such a framework’s reliability could be limited by occlusion or sensor failure. To address this issue, more recent research proposes using vehicle-to-vehicle (V2V) communication to share perception information with others. However, most relevant works focus only on cooperative detection and leave cooperative tracking an underexplored research field. A few recent datasets, such as V2V4Real, provide 3D multi-object cooperative tracking benchmarks. However, their proposed methods mainly use cooperative detection results as input to a standard single-sensor Kalman Filter-based tracking algorithm. In their approach, the measurement uncertainty of different sensors from different connected autonomous vehicles (CAVs) may not be properly estimated to utilize the theoretical optimality property of Kalman Filter-based tracking algorithms. In this paper, we propose a novel 3D multi-object cooperative tracking algorithm for autonomous driving via a differentiable multi-sensor Kalman Filter. Our algorithm learns to estimate measurement uncertainty for each detection that can better utilize the theoretical property of Kalman Filter-based tracking methods. The experiment results show that our algorithm improves the tracking accuracy by 17% with only 0.037x communication costs compared with the state-of-the-art method in V2V4Real. Our code and videos are available at the URL and the URL.},
  archive   = {C_ICRA},
  author    = {Hsu-Kuang Chiu and Chien-Yi Wang and Min-Hung Chen and Stephen F. Smith},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610487},
  month     = {5},
  pages     = {18458-18464},
  title     = {Probabilistic 3D multi-object cooperative tracking for autonomous driving via differentiable multi-sensor kalman filter},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards visibility estimation and noise-distribution-based
defogging for LiDAR in autonomous driving. <em>ICRA</em>, 18443–18449.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Point clouds play a crucial role in robots and intelligent vehicles. Noise caused by fog droplets seriously degrades the quality of point clouds. Previous researches have shown that the extent of degradation is correlated with visibility. The fog attenuation coefficient is associated with visibility. In light of this background, this paper proposes a noise-distribution-based defogging method for point clouds. Our approach hinges on the estimation of the fog attenuation coefficient, facilitated by road-based prior knowledge. Subsequently, our method integrates the fog-induced noise distribution inferred from the LiDAR imaging model with the spatially non-uniform distribution of point clouds caused by LiDAR structure. The fused results are input to a statistical filter based on the relative sparsity of noise to achieve defogging. This paper is one of the early works focusing on point cloud defogging. Its core insight lies in the estimation of the attenuation coefficient and the employment of fog-induced noise distribution for defogging. Experiments demonstrate that our method can accurately mitigate the impact of fog and meanwhile enhance the performance of 3D object detection network.},
  archive   = {C_ICRA},
  author    = {Jie Zhan and Yucong Duan and Junfeng Ding and Xuzhong Hu and Xiao Huang and Jie Ma},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610699},
  month     = {5},
  pages     = {18443-18449},
  title     = {Towards visibility estimation and noise-distribution-based defogging for LiDAR in autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). QUEST: Query stream for practical cooperative perception.
<em>ICRA</em>, 18436–18442. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cooperative perception can effectively enhance individual perception performance by providing additional viewpoint and expanding the sensing field. Existing cooperation paradigms are either interpretable (result cooperation) or flexible (feature cooperation). In this paper, we propose the concept of query cooperation to enable interpretable instance-level flexible feature interaction. To specifically explain the concept, we propose a cooperative perception framework, termed QUEST, which let query stream flow among agents. The cross-agent queries are interacted via fusion for co-aware instances and complementation for individual unaware instances. Taking camera-based vehicle-infrastructure perception as a typical practical application scene, the experimental results on the real-world dataset, DAIR-V2X-Seq, demonstrate the effectiveness of QUEST and further reveal the advantage of the query cooperation paradigm on transmission flexibility and robustness to packet dropout. We hope our work can further facilitate the cross-agent representation interaction for better cooperative perception in practice.},
  archive   = {C_ICRA},
  author    = {Siqi Fan and Haibao Yu and Wenxian Yang and Jirui Yuan and Zaiqing Nie},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610214},
  month     = {5},
  pages     = {18436-18442},
  title     = {QUEST: Query stream for practical cooperative perception},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards motion forecasting with real-world perception
inputs: Are end-to-end approaches competitive? <em>ICRA</em>,
18428–18435. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion forecasting is crucial in enabling autonomous vehicles to anticipate the future trajectories of surrounding agents. To do so, it requires solving mapping, detection, tracking, and then forecasting problems, in a multi-step pipeline. In this complex system, advances in conventional forecasting methods have been made using curated data, i.e., with the assumption of perfect maps, detection, and tracking. This paradigm, however, ignores any errors from upstream modules. Meanwhile, an emerging end-to-end paradigm, that tightly integrates the perception and forecasting architectures into joint training, promises to solve this issue. However, the evaluation protocols between the two methods were so far incompatible and their comparison was not possible. In fact, conventional forecasting methods are usually not trained nor tested in real-world pipelines (e.g., with upstream detection, tracking, and mapping modules). In this work, we aim to bring forecasting models closer to the real-world deployment. First, we propose a unified evaluation pipeline for forecasting methods with real-world perception inputs, allowing us to compare conventional and end-to-end methods for the first time. Second, our in-depth study uncovers a substantial performance gap when transitioning from curated to perception-based data. In particular, we show that this gap (1) stems not only from differences in precision but also from the nature of imperfect inputs provided by perception modules, and that (2) is not trivially reduced by simply finetuning on perception outputs. Based on extensive experiments, we provide recommendations for critical areas that require improvement and guidance towards more robust motion forecasting in the real world. The evaluation library for benchmarking models under standardized and practical conditions is provided: https://github.com/valeoai/MFEval.},
  archive   = {C_ICRA},
  author    = {Yihong Xu and Loïck Chambon and Éloi Zablocki and Mickaël Chen and Alexandre Alahi and Matthieu Cord and Patrick Pérez},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610201},
  month     = {5},
  pages     = {18428-18435},
  title     = {Towards motion forecasting with real-world perception inputs: Are end-to-end approaches competitive?},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AdvGPS: Adversarial GPS for multi-agent perception attack.
<em>ICRA</em>, 18421–18427. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The multi-agent perception system collects visual data from sensors located on various agents and leverages their relative poses determined by GPS signals to effectively fuse information, mitigating the limitations of single-agent sensing, such as occlusion. However, the precision of GPS signals can be influenced by a range of factors, including wireless transmission and obstructions like buildings. Given the pivotal role of GPS signals in perception fusion and the potential for various interference, it becomes imperative to investigate whether specific GPS signals can easily mislead the multi-agent perception system. To address this concern, we frame the task as an adversarial attack challenge and introduce ADVGPS, a method capable of generating adversarial GPS signals which are also stealthy for individual agents within the system, significantly reducing object detection accuracy. To enhance the success rates of these attacks in a black-box scenario, we introduce three types of statistically sensitive natural discrepancies: appearance-based discrepancy, distribution-based discrepancy, and task-aware discrepancy. Our extensive experiments on the OPV2V dataset demonstrate that these attacks substantially undermine the performance of state-of-the-art methods, showcasing remarkable transferability across different point cloud based 3D detection systems. This alarming revelation underscores the pressing need to address security implications within multi-agent perception systems, thereby underscoring a critical area of research. The code is available at https://github.com/jinlong17/AdvGPS.},
  archive   = {C_ICRA},
  author    = {Jinlong Li and Baolu Li and Xinyu Liu and Jianwu Fang and Felix Juefei-Xu and Qing Guo and Hongkai Yu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610012},
  month     = {5},
  pages     = {18421-18427},
  title     = {AdvGPS: Adversarial GPS for multi-agent perception attack},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Breaking data silos: Cross-domain learning for multi-agent
perception from independent private sources. <em>ICRA</em>, 18414–18420.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The diverse agents in multi-agent perception systems may be from different companies. Each company might use the identical classic neural network architecture based encoder for feature extraction. However, the data source to train the various agents is independent and private in each company, leading to the Distribution Gap of different private data for training distinct agents in multi-agent perception system. The data silos by the above Distribution Gap could result in a significant performance decline in multi-agent perception. In this paper, we thoroughly examine the impact of the distribution gap on existing multi-agent perception systems. To break the data silos, we introduce the Feature Distribution-aware Aggregation (FDA) framework for cross-domain learning to mitigate the above Distribution Gap in multi-agent perception. FDA comprises two key components: Learnable Feature Compensation Module and Distribution-aware Statistical Consistency Module, both aimed at enhancing intermediate features to minimize the distribution gap among multi-agent features. Intensive experiments on the public OPV2V and V2XSet datasets underscore FDA’s effectiveness in point cloud-based 3D object detection, presenting it as an invaluable augmentation to existing multi-agent perception systems. The code is available at https://github.com/jinlong17/BDS-V2V.},
  archive   = {C_ICRA},
  author    = {Jinlong Li and Baolu Li and Xinyu Liu and Runsheng Xu and Jiaqi Ma and Hongkai Yu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610591},
  month     = {5},
  pages     = {18414-18420},
  title     = {Breaking data silos: Cross-domain learning for multi-agent perception from independent private sources},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MonoOcc: Digging into monocular semantic occupancy
prediction. <em>ICRA</em>, 18398–18405. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Monocular Semantic Occupancy Prediction aims to infer the complete 3D geometry and semantic information of scenes from only 2D images. It has garnered significant attention, particularly due to its potential to enhance the 3D perception of autonomous vehicles. However, existing methods rely on a complex cascaded framework with relatively limited information to restore 3D scenes, including a dependency on supervision solely on the whole network’s output, single-frame input, and the utilization of a small backbone. These challenges, in turn, hinder the optimization of the framework and yield inferior prediction results, particularly concerning smaller and long-tailed objects. To address these issues, we propose MonoOcc. In particular, we (i) improve the monocular occupancy prediction framework by proposing an auxiliary semantic loss as supervision to the shallow layers of the framework and an image-conditioned cross-attention module to refine voxel features with visual clues, and (ii) employ a distillation module that transfers temporal information and richer knowledge from a larger image backbone to the monocular semantic occupancy prediction framework with low cost of hardware. With these advantages, our method yields state-of-the-art performance on the camera-based SemanticKITTI Scene Completion benchmark. Codes and models can be accessed at https://github.com/ucaszyp/MonoOcc.},
  archive   = {C_ICRA},
  author    = {Yupeng Zheng and Xiang Li and Pengfei Li and Yuhang Zheng and Bu Jin and Chengliang Zhong and Xiaoxiao Long and Hao Zhao and Qichao Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611261},
  month     = {5},
  pages     = {18398-18405},
  title     = {MonoOcc: Digging into monocular semantic occupancy prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RGBD-based image goal navigation with pose drift: A
topo-metric graph based approach. <em>ICRA</em>, 18391–18397. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image-goal navigation in unknown environments with sensor error is of considerable difficulty for autonomous robots. In this paper, we propose a drift-resisting topo-metric graph to map the environment and localize the robot using only relative poses. The error-sharing mechanism under this representation effectively reduces the impact of accumulated drifts commonly encountered in navigation tasks. A Reinforcement Learning based policy was proposed for sub-goal selection on this topo-metric graph, which improves navigation efficiency by handling task-driven features taking both image correlation and topological layout into account. We adopt a modular system design with this map representation and graph policy, leaving the low-level motion planning problems to classical controllers for better stability and generalizability. Experimental results demonstrate that our method can achieve robust navigation performance in a variety of unknown environments and even 50% higher success rate over existing methods in complex environments with odometry drift.},
  archive   = {C_ICRA},
  author    = {Shuhao Ye and Yuxiang Cui and Hao Sha and Sha Lu and Yu Zhang and Rong Xiong and Yue Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610727},
  month     = {5},
  pages     = {18391-18397},
  title     = {RGBD-based image goal navigation with pose drift: A topo-metric graph based approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gas-source efficiently active searching in unfamiliar
environments. <em>ICRA</em>, 18384–18390. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Searching Gas Source actively and efficiently in unknown hazard environments is an important but challenging issue. Using mobile robots to autonomously search and navigate to gas source location provides a promising way. Existing methods are mostly based on the modularization framework which investigates the gas-source search and robot navigation tasks independently, leading to a decoupled approach that results in higher collision risks and lower navigation efficiency. Moreover, existing robot navigation techniques grapple with the intricacies of navigating through unknown environments. To tackle these complexities, we introduce an integrated framework that merges gas source localization with robot navigation. This unified structure, underpinned by an end-to-end learning approach, resolves the inherent conflicts between gas exploration and collision avoidance. Our approach aggregates the local observations (raw 3D-LiDAR data) and the expert guidance information (gas distribution), and directly generates navigation actions by implementing the reinforcement learning with a novel reward function based on region dynamic guidances, thus effectively addressing the challenges of active gas source searching in unknown environments. Simulation results underscore the adaptability of our method to diverse unknown environments, along with its superior gas source searching capabilities compared to conventional approaches. Finally, we conduct real-world experiments to demonstrate our feasibility.},
  archive   = {C_ICRA},
  author    = {Yu Zhai and Yanzi Miao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610097},
  month     = {5},
  pages     = {18384-18390},
  title     = {Gas-source efficiently active searching in unfamiliar environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-modal registration using adaptive modeling in
infrastructure-based vehicle localization*. <em>ICRA</em>, 18377–18383.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Infrastructure-based vehicle localization, in comparison to single-agent approaches, offers several advantages including reduced system cost, extended perception range, enhanced data fusion capabilities, and energy savings. Many conventional approaches impose limitations on the types of objects due to the need for specific object-end modifications, such as applying perceptual markers like color-labeled plates and reflective balls. LiDAR presents a solution in terms of object arbitrariness, as it addresses the challenges of feature-free object modeling and continuous registration. However, achieving complete environmental coverage with LiDAR remains prohibitively expensive, particularly in extensive areas. Hence, this study proposes a cross-modal localization approach using adaptive modeling, employing LiDAR for object modeling and cost-effective sensor cameras for object tracking through image-point-cloud registration. Accurate correspondence between the model and observation can be estimated in real-time. The experiments are conducted in a typical scenario that requires adaptive modeling: Autonomous Valet Parking (AVP). Results demonstrate that the proposed system achieves comparable performance with significantly reduced system costs, highlighting its potential for large-scale deployment.},
  archive   = {C_ICRA},
  author    = {Fei Wang and Yuesheng He and Hanyang Zhuang and Chenxi Yang and Ming Yang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610265},
  month     = {5},
  pages     = {18377-18383},
  title     = {Cross-modal registration using adaptive modeling in infrastructure-based vehicle localization*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A framework for real-time generation of multi-directional
traversability maps in unstructured environments. <em>ICRA</em>,
18370–18376. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In complex unstructured environments, accurate terrain traversability analysis is a fundamental requirement for the successful execution of any movements of ground robots, especially given that terrain traversability often exhibits anisotropy. However, the difficulty in obtaining multi-directional terrain labels hinders the emergence of end-to-end multi-directional traversability network. This paper introduces a framework for real-time multi-directional traversability maps (MTraMap) generation tailored for unstructured environments. It involves pre-training a uni-directional traversability classifier, termed UniTraT, through self-supervised learning using ground robot travel simulation. Furthermore, it employs Uni-directional to Multi-directional Traversability Distillation (UMTraDistill) to distill a multi-directional traversability network, termed MultiTCNN, which is capable of directly generating MTraMap. We evaluated both networks on our traversability dataset, achieving an 89% accuracy in terrain traversability classification with the UniTraT. Compared to UniTraT, the accuracy of the MultiTCNN distilled via UMTraDistill only decreases by 1.8%, and it can process 10 m × 10 m elevation map at a speed of 74 fps. Field robotics experiments were also conducted and showed that MultiTCNN can generate MTraMap of the surrounding 20 m × 20 m environment at a rate of 9.39 fps, with a slight reduction of 0.61 fps compared to the lidar data publishing rate, and the generated MTraMap can clearly delineate the multi-directional traversability of the surrounding environments.},
  archive   = {C_ICRA},
  author    = {Tao Huang and Gang Wang and Hongliang Liu and Jun Luo and Lang Wu and Tao Zhu and Huayan Pu and Jun Luo and Shuxin Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610312},
  month     = {5},
  pages     = {18370-18376},
  title     = {A framework for real-time generation of multi-directional traversability maps in unstructured environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Boosting offline reinforcement learning for autonomous
driving with hierarchical latent skills. <em>ICRA</em>, 18362–18369. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning-based vehicle planning is receiving increasing attention with the emergence of diverse driving simulators and large-scale driving datasets. While offline reinforcement learning (RL) is well suited for these safety-critical tasks, it still struggles to plan over extended periods. In this work, we present a skill-based framework that enhances offline RL to overcome the long-horizon vehicle planning challenge. Specifically, we design a variational autoencoder (VAE) to learn skills from offline demonstrations. To mitigate posterior collapse of common VAEs, we introduce a two-branch sequence encoder to capture both discrete options and continuous variations of the complex driving skills. The final policy treats learned skills as actions and can be trained by any off-the-shelf offline RL algorithms. This facilitates a shift in focus from per-step actions to temporally extended skills, thereby enabling long-term reasoning into the future. Extensive results on CARLA prove that our model consistently outperforms strong baselines at both training and new scenarios. Additional visualizations and experiments demonstrate the interpretability and transferability of extracted skills.},
  archive   = {C_ICRA},
  author    = {Zenan Li and Fan Nie and Qiao Sun and Fang Da and Hang Zhao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611197},
  month     = {5},
  pages     = {18362-18369},
  title     = {Boosting offline reinforcement learning for autonomous driving with hierarchical latent skills},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uncertainty-aware reinforcement learning for autonomous
driving with multimodal digital driver guidance. <em>ICRA</em>,
18355–18361. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While existing Learning from intervention (LfI) methods within the human-in-the-loop reinforcement learning (HiL-RL) paradigm mainly operate on the assumption that human policies are homogeneous and deterministic with low variance, natural human driving behaviors are multimodal with intrinsic uncertainties, and hence, accommodating diverse human capabilities is significant for its practical applications. This work proposes an enhanced LfI approach for learning the optimal RL policy by leveraging multimodal human behaviors in the setting of N-driver concurrent interventions. Specifically, we first learn the N number of human digital drivers from the multi-human demonstration dataset, wherein each driver possesses its own policy distribution. Then, the post-trained drivers will be kept in the training loop of the RL algorithms, providing diverse driving guidance whenever the intervention is required. Additionally, to better utilize the provided guidance, we augment the RL regarding the fundamental architecture and optimization objectives to facilitate the proposed uncertainty-aware reinforcement learning (UnaRL) algorithm. The proposed approach, which won 2 nd place in the Alibaba Future Car Innovation Challenge 2022, is solidly compared in two challenging autonomous driving scenarios against state-of-the-art (SOTA) LfI baselines, and results of both simulation and real-world experiment confirm the superiority of our method in terms of learning robustness and driving performance. Videos and source code are provided. 1},
  archive   = {C_ICRA},
  author    = {Wenhui Huang and Zitong Shan and Shanhe Lou and Chen Lv},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610272},
  month     = {5},
  pages     = {18355-18361},
  title     = {Uncertainty-aware reinforcement learning for autonomous driving with multimodal digital driver guidance},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel wide-area multiobject detection system with
high-probability region searching. <em>ICRA</em>, 18316–18322. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, wide-area visual surveillance systems have been widely applied in various industrial and transportation scenarios. These systems, however, face significant challenges when implementing multi-object detection due to conflicts arising from the need for high-resolution imaging, efficient object searching, and accurate localization. To address these challenges, this paper presents a hybrid system that incorporates a wide-angle camera, a high-speed search camera, and a galvano-mirror. In this system, the wide-angle camera offers panoramic images as prior information, which helps the search camera capture detailed images of the targeted objects. This integrated approach enhances the overall efficiency and effectiveness of wide-area visual detection systems. Specifically, in this study, we introduce a wide-angle camera-based method to generate a panoramic probability map (PPM) for estimating high-probability regions of target object presence. Then, we propose a probability searching module that uses the PPM-generated prior information to dynamically adjust the sampling range and refine target coordinates based on uncertainty variance computed by the object detector. Finally, the integration of PPM and the probability searching module yields an efficient hybrid vision system capable of achieving 120 fps multi-object search and detection. Extensive experiments are conducted to verify the system’s effectiveness and robustness.},
  archive   = {C_ICRA},
  author    = {Xianlei Long and Hui Zhao and Chao Chen and Fuqiang Gu and Qingyi Gu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611655},
  month     = {5},
  pages     = {18316-18322},
  title     = {A novel wide-area multiobject detection system with high-probability region searching},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical meta-learning-based adaptive controller.
<em>ICRA</em>, 18309–10315. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study how to design learning-based adaptive controllers that enable fast and accurate online adaptation in changing environments. In these settings, learning is typically done during an initial (offline) design phase, where the vehicle is exposed to different environmental conditions and disturbances (e.g., a drone exposed to different winds) to collect training data. Our work is motivated by the observation that real-world disturbances fall into two categories: 1) those that can be directly monitored or controlled during training, which we call &quot;manageable&quot;; and 2) those that cannot be directly measured or controlled (e.g., nominal model mismatch, air plate effects, and unpredictable wind), which we call &quot;latent&quot;. Imprecise modeling of these effects can result in degraded control performance, particularly when latent disturbances continuously vary. This paper presents the Hierarchical Meta-learning-based Adaptive Controller (HMAC) to learn and adapt to such multi-source disturbances. Within HMAC, we develop two techniques: 1) Hierarchical Iterative Learning, which jointly trains representations to caption the various sources of disturbances, and 2) Smoothed Streaming Meta-Learning, which learns to capture the evolving structure of latent disturbances over time (in addition to standard meta-learning on the manageable disturbances). Experimental results demonstrate that HMAC exhibits more precise and rapid adaptation to multi-source disturbances than other adaptive controllers. 1},
  archive   = {C_ICRA},
  author    = {Fengze Xie and Guanya Shi and Michael O’Connell and Yisong Yue and Soon-Jo Chung},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611562},
  month     = {5},
  pages     = {18309-10315},
  title     = {Hierarchical meta-learning-based adaptive controller},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MBot: A modular ecosystem for scalable robotics education.
<em>ICRA</em>, 18294–18300. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Michigan Robotics MBot is a low-cost mobile robot platform that has been used to train over 1,400 students in autonomous navigation since 2014 at the University of Michigan and our collaborating colleges. The MBot platform was designed to meet the needs of teaching robotics at scale to match the growth of robotics as a field and an academic discipline. Transformative advancements in robot navigation over the past decades have led to a significant demand for skilled roboticists across industry and academia. This demand has sparked a need for robotics courses in higher education, spanning all levels of undergraduate and graduate experiences. Incorporating real robot platforms into such courses and curricula is effective for conveying the unique challenges of programming embodied agents in real-world environments and sparking student interest. However, teaching with real robots remains challenging due to the cost of hardware and the development effort involved in adapting existing hardware for a new course. In this paper, we describe the design and evolution of the MBot platform, and the underlying principals of scalability and flexibility which are keys to its success.},
  archive   = {C_ICRA},
  author    = {Peter Gaskell and Jana Pavlasek and Tom Gao and Abhishek Narula and Stanley Lewis and Odest Chadwicke Jenkins},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610587},
  month     = {5},
  pages     = {18294-18300},
  title     = {MBot: A modular ecosystem for scalable robotics education},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pluck and play: Self-supervised exploration of chordophones
for robotic playing. <em>ICRA</em>, 18286–18293. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing robotic musicians utilize detailed handcrafted instrument models to generate or learn policies for playing because model-free or inaccurate policy rollouts might easily damage or wear out fragile instruments. We introduce an approach to characterize geometric models of chordophones and their audio onset responses directly through audio-tactile exploration with a physical robot arm. Initially, the system refines prior estimates of string positions, provided by kinesthetic teaching or visual estimation, through repeated attempts to pluck individual strings. A subsequent stage implements a Safe Active Exploration paradigm based on Gaussian Processes to explore and characterize the audio onset response of feasible plucking motions while minimizing invalid attempts. The resulting models can be used to actuate an imprecise robotic arm to play sequences of notes with varying loudness on a Chinese Guzheng.},
  archive   = {C_ICRA},
  author    = {Michael Görner and Norman Hendrich and Jianwei Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610120},
  month     = {5},
  pages     = {18286-18293},
  title     = {Pluck and play: Self-supervised exploration of chordophones for robotic playing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Safe table tennis swing stroke with low-cost hardware.
<em>ICRA</em>, 18279–18285. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Playing table tennis with a human player is a challenging robotic task due to its dynamic nature. Despite a number of researches being devoted to developing robotic table tennis systems, most of the works have demanding hardware requirements and ignore safety measures when generating the swing stoke. To address these issues, we propose a safe motion planning framework that fully pushes the robotic hardware performance limits to play table tennis. In particular, we propose a pipeline to generate manipulator joint trajectories with environmental safety constraints and scale the trajectories to satisfy joint movement limitations. We use three different agents to validate the planning algorithm with our handmade robot platform in both simulation and real-world environments.},
  archive   = {C_ICRA},
  author    = {Francesco Cursi and Marcus Kalander and Shuang Wu and Xidi Xue and Yu Tian and Guangjian Tian and Xingyue Quan and Jianye Hao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610119},
  month     = {5},
  pages     = {18279-18285},
  title     = {Safe table tennis swing stroke with low-cost hardware},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sim-to-real robotic sketching using behavior cloning and
reinforcement learning. <em>ICRA</em>, 18272–18278. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic sketching in real-world scenarios poses a challenging problem with diverse applications in art, robotics, and digital design. We present a novel approach that bridges the gap between digital and robotic sketching, leveraging behavior cloning and reinforcement learning techniques. This paper introduces an approach aimed at bringing the gap between simulated and real-world robotic sketching closer together through the integration of behavior cloning and reinforcement learning techniques. Our approach trains painting policies that operate effectively in both virtual environments and real-world robotic sketching systems. We have implemented a robotic sketching system featuring an UltraArm robot equipped with a RealSense D415 camera, closely emulating the MyPaint virtual environment. Our system can perceive its environment and adapt painting policies to natural painting media. Our results highlight the effectiveness of our agent in terms of acquiring policies for high-dimensional continuous action spaces, enabling the seamless transfer of brush manipulation techniques from simulation to practical robotic sketching. Furthermore, we demonstrate our robotic sketching system’s capability to generate complex images and strokes using various configurations. https://sites.google.com/view/sketchingrobot},
  archive   = {C_ICRA},
  author    = {Biao Jia and Dinesh Manocha},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610286},
  month     = {5},
  pages     = {18272-18278},
  title     = {Sim-to-real robotic sketching using behavior cloning and reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trajectory-prediction-based dynamic tracking of a UGV to a
moving target under multi-disturbed conditions. <em>ICRA</em>,
18265–18271. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tracking dynamic targets poses a significant challenge for Unmanned Ground Vehicles (UGVs). Existing methods often lack research on multi-disturbed conditions. To address this issue, we propose a trajectory-prediction-based dynamic tracking scheme, which includes target localization, trajectory prediction, and UGV control. Firstly, an estimation algorithm based on the Extended Kalman Filter (EKF) is employed to mitigate noise and estimate the absolute states of the target accurately. To enhance robustness, we present an Adaptive Trajectory Prediction (ATP) algorithm based on prediction anchors. In this method, a quantization standard for trajectory disturbance is designed for adaptive control. Subsequently, we iteratively solve prediction anchor points based on two motion models to robustly predict the target trajectory even in the presence of unknown disturbances. Finally, the Linear Time-Varying Model Predictive Control (LTV-MPC) is utilized in the UGV controller for dynamic tracking. Experimental results demonstrate that the ATP exhibits superior prediction robustness and accuracy in perturbed environments compared to other prediction algorithms. In addition, the proposed scheme effectively achieves dynamic tracking of the Unmanned Aerial Vehicle (UAV) by the UGV under multi-disturbed conditions. Specifically, when the target moves at a speed of 1.0 m/s, the UGV can maintain a tracking error within 0.346 m.},
  archive   = {C_ICRA},
  author    = {Jinge Si and Bin Li and Yongkang Xu and Liang Wang and Chencheng Deng and Shoukun Wang and Junzheng Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611690},
  month     = {5},
  pages     = {18265-18271},
  title     = {Trajectory-prediction-based dynamic tracking of a UGV to a moving target under multi-disturbed conditions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BaSAL: Size-balanced warm start active learning for LiDAR
semantic segmentation. <em>ICRA</em>, 18258–18264. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Active learning strives to reduce the need for costly data annotation, by repeatedly querying an annotator to label the most informative samples from a pool of unlabeled data, and then training a model from these samples. We identify two problems with existing active learning methods for LiDAR semantic segmentation. First, they overlook the severe class imbalance inherent in LiDAR semantic segmentation datasets. Second, to bootstrap the active learning loop when there is no labeled data available, they train their initial model from randomly selected data samples, leading to low performance. This situation is referred to as the cold start problem. To address these problems we propose BaSAL, a size-balanced warm start active learning model, based on the observation that each object class has a characteristic size. By sampling object clusters according to their size, we can thus create a size-balanced dataset that is also more class-balanced. Furthermore, in contrast to existing information measures like entropy or CoreSet, size-based sampling does not require a pretrained model, thus addressing the cold start problem effectively. Results show that we are able to improve the performance of the initial model by a large margin. Combining warm start and size-balanced sampling with established information measures, our approach achieves comparable performance to training on the entire SemanticKITTI dataset, despite using only 5% of the annotations, outperforming existing active learning methods. We also match the existing state-of-the-art in active learning on nuScenes. Our code is available at: https://github.com/Tony-WJR/BaSAL.},
  archive   = {C_ICRA},
  author    = {Jiarong Wei and Yancong Lin and Holger Caesar},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611122},
  month     = {5},
  pages     = {18258-18264},
  title     = {BaSAL: Size-balanced warm start active learning for LiDAR semantic segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LiRaFusion: Deep adaptive LiDAR-radar fusion for 3D object
detection. <em>ICRA</em>, 18250–18257. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose LiRaFusion to tackle LiDAR-radar fusion for 3D object detection to fill the performance gap of existing LiDAR-radar detectors. To improve the feature extraction capabilities from these two modalities, we design an early fusion module for joint voxel feature encoding, and a middle fusion module to adaptively fuse feature maps via a gated network. We perform extensive evaluation on nuScenes to demonstrate that LiRaFusion leverages the complementary information of LiDAR and radar effectively and achieves notable improvement over existing methods.},
  archive   = {C_ICRA},
  author    = {Jingyu Song and Lingjun Zhao and Katherine A. Skinner},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611436},
  month     = {5},
  pages     = {18250-18257},
  title     = {LiRaFusion: Deep adaptive LiDAR-radar fusion for 3D object detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). One-vs-all semi-automatic labeling tool for semantic
segmentation in autonomous driving. <em>ICRA</em>, 18243–18249. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Semantic image segmentation plays a pivotal role in creating High-Definition (HD) maps for autonomous driving, where every pixel in an image is assigned a label from a specific semantic class. However, obtaining dense pixel-level annotations for model training is a laborious and expensive process. Active learning holds promise as a method to reduce the human annotation effort needed for semantic segmentation. However, existing active learning methods often perform well in the majority classes but struggle with the minority classes, negatively impacting segmentation performance. To tackle this challenge, we propose a novel One-vs-All (OVA) active learning framework, known as OVAAL. This paper explains how OVAAL can shift more attention towards the minority classes and thoroughly analyzes its contributions to performance enhancement. Additionally, we introduce an OVA-based semi-supervised learning method as the final training phase, referred to as OVAAL+. Our results demonstrate that both OVAAL and OVAAL+ lead to significant improvements, with mean Intersection over Union (mIoU) gains of 4.55% and 6.38%, respectively, compared to the state-of-the-art active learning method Pixelpick on the Cityscapes semantic segmentation benchmark. These improvements are achieved while maintaining an economical annotation budget of 1.44% of the training data. We foresee further research exploring the potential of OVA-based active selection to address challenges in cold start scenarios and resource-constrained training environments.},
  archive   = {C_ICRA},
  author    = {Jing Gu and Guillermo Gallego and Amine Ben Arab},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611557},
  month     = {5},
  pages     = {18243-18249},
  title     = {One-vs-all semi-automatic labeling tool for semantic segmentation in autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RCM-fusion: Radar-camera multi-level fusion for 3D object
detection. <em>ICRA</em>, 18236–18242. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While LiDAR sensors have been successfully applied to 3D object detection, the affordability of radar and camera sensors has led to a growing interest in fusing radars and cameras for 3D object detection. However, previous radar-camera fusion models could not fully utilize the potential of radar information. In this paper, we propose Radar-Camera Multi-level fusion (RCM-Fusion), which attempts to fuse both modalities at feature and instance levels. For feature-level fusion, we propose a Radar Guided BEV Encoder which transforms camera features into precise BEV representations using the guidance of radar Bird’s-Eye-View (BEV) features and combines the radar and camera BEV features. For instance-level fusion, we propose a Radar Grid Point Refinement module that reduces localization error by accounting for the characteristics of the radar point clouds. The experiments on the public nuScenes dataset demonstrate that our proposed RCM-Fusion achieves state-of-the-art performances among single frame-based radar-camera fusion methods in the nuScenes 3D object detection benchmark. The code will be made publicly available.},
  archive   = {C_ICRA},
  author    = {Jisong Kim and Minjae Seong and Geonho Bang and Dongsuk Kum and Jun Won Choi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611449},
  month     = {5},
  pages     = {18236-18242},
  title     = {RCM-fusion: Radar-camera multi-level fusion for 3D object detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TerrainSense: Vision-driven mapless navigation for
unstructured off-road environments. <em>ICRA</em>, 18229–18235. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Navigating autonomous vehicles efficiently across unstructured and off-road terrains remains a formidable challenge, often requiring intricate mapping or multi-step pipelines. However, these conventional approaches struggle to adapt to dynamic environments. This paper presents TerrainSense, an end-to-end framework that overcomes these limitations. By utilizing a transformers, TerrainSense detects lane semantics and topology from camera images, enabling mapless path planning without the reliance on highly detailed maps. The efficacy of TerrainSense was rigorously assessed on six diverse datasets, evaluating its efficacy in detection, segmentation, and path prediction using various metrics. Notably, it outperforms the other state-of-the-art methods by 9.32% in precisely predicting the path with 18.28% faster inference time.},
  archive   = {C_ICRA},
  author    = {Bilal Hassan and Arjun Sharma and Nadya Abdel Madjid and Majid Khonji and Jorge Dias},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610128},
  month     = {5},
  pages     = {18229-18235},
  title     = {TerrainSense: Vision-driven mapless navigation for unstructured off-road environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WLST: Weak labels guided self-training for weakly-supervised
domain adaptation on 3D object detection. <em>ICRA</em>, 18214–18220.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the field of domain adaptation (DA) on 3D object detection, most of the work is dedicated to unsupervised domain adaptation (UDA). Yet, without any target annotations, the performance gap between the UDA approaches and the fully-supervised approach is still noticeable, which is impractical for real-world applications. On the other hand, weakly-supervised domain adaptation (WDA) is an underexplored yet practical task that only requires few labeling effort on the target domain. To improve the DA performance in a cost-effective way, we propose a general weak labels guided self-training framework, WLST, designed for WDA on 3D object detection. By incorporating autolabeler, which can generate 3D pseudo labels from 2D bounding boxes, into the existing self-training pipeline, our method is able to generate more robust and consistent pseudo labels that would benefit the training process on the target domain. Extensive experiments demonstrate the effectiveness, robustness, and detector-agnosticism of our WLST framework. Notably, it outperforms previous state-of-the-art methods on all evaluation tasks. Code and models are available at https://github.com/jacky121298/WLST. Note that the complete version with appendix is available on arXiv.},
  archive   = {C_ICRA},
  author    = {Tsung-Lin Tsou and Tsung-Han Wu and Winston H. Hsu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610801},
  month     = {5},
  pages     = {18214-18220},
  title     = {WLST: Weak labels guided self-training for weakly-supervised domain adaptation on 3D object detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attention-based cloth manipulation from model-free
topological representation. <em>ICRA</em>, 18207–18213. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The robotic manipulation of deformable objects, such as clothes and fabric, is known as a complex task from both the perception and planning perspectives. Indeed, the stochastic nature of the underlying environment dynamics makes it an interesting research field for statistical learning approaches and neural policies. In this work, we introduce a novel attention-based neural architecture capable of solving a smoothing task for such objects by means of a single robotic arm. To train our network, we leverage an oracle policy, executed in simulation, which uses the topological description of a mesh of points for representing the object to smooth. In a second step, we transfer the resulting behavior in the real world with imitation learning using the cloth point cloud as decision support, which is captured from a single RGBD camera placed egocentrically on the wrist of the arm. This approach allows fast training of the real-world manipulation neural policy while not requiring scene reconstruction at test time, but solely a point cloud acquired from a single RGBD camera. Our resulting policy first predicts the desired point to choose from the given point cloud and then the correct displacement to achieve a smoothed cloth. Experimentally, we first assess our results in a simulation environment by comparing them with an existing heuristic policy, as well as several baseline attention architectures. Then, we validate the performance of our approach in a real-world scenario. Project website: link},
  archive   = {C_ICRA},
  author    = {Kevin Galassi and Bingbing Wu and Julien Perez and Gianluca Palli and Jean-Michel Renders},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610241},
  month     = {5},
  pages     = {18207-18213},
  title     = {Attention-based cloth manipulation from model-free topological representation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EDOPT: Event-camera 6-DoF dynamic object pose tracking.
<em>ICRA</em>, 18200–18206. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {High-frequency, low-latency, 6-DoF object tracking is useful for grasping objects in motion, taking robots beyond pick-and-place tasks. We propose using an event-camera for tracking the objects to leverage the low-latency and continuous (i.e. not fixed-rate) data capture for high-frequency tracking. We propose the EDOPT algorithm, which maintains real-time operation with a variable event-rate (which occurs due to variation in camera velocity and scene texture) and avoids frame-jumps and motion-blur which are problematic in traditional computer vision solutions. EDOPT uses a strong object prior, leading to a novel solution possible only with the event-camera. To our knowledge, this is the first method for 6-DoF object pose estimation with only the event-camera. The proposed method achieves comparable results to a state-of-the-art DNN technique that fuses frames, depth, and events. We demonstrate smooth, online object pose tracking with a live camera feed at &gt; 300 Hz.},
  archive   = {C_ICRA},
  author    = {Arren Glover and Luna Gava and Zhichao Li and Chiara Bartolozzi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611511},
  month     = {5},
  pages     = {18200-18206},
  title     = {EDOPT: Event-camera 6-DoF dynamic object pose tracking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient object rearrangement via multi-view fusion.
<em>ICRA</em>, 18193–18199. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The prospect of assistive robots aiding in object organization has always been compelling. In an image-goal setting, the robot rearranges the current scene to match the single image captured from the goal scene. The key to an image-goal rearrangement system is estimating the desired placement pose of each object based on the single goal image and observations from the current scene. In order to establish sufficient associations for accurate estimation, the system should observe an object from a viewpoint similar to that in the goal image. Existing image-goal rearrangement systems, due to their reliance on a fixed viewpoint for perception, often require redundant manipulations to randomly adjust an object’s pose for a better perspective. Addressing this inefficiency, we introduce a novel object rearrangement system that employs multi-view fusion. By observing the current scene from multiple viewpoints before manipulating objects, our approach can estimate a more accurate pose without redundant manipulation times. A standard visual localization pipeline at the object level is developed to capitalize on the advantages of multi-view observations. Simulation results demonstrate that the efficiency of our system outperforms existing single-view systems. The effectiveness of our system is further validated in a physical experiment. For videos, please visit https: //sites.google.com/view/multi-view-rearr.},
  archive   = {C_ICRA},
  author    = {Dehao Huang and Chao Tang and Hong Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611213},
  month     = {5},
  pages     = {18193-18199},
  title     = {Efficient object rearrangement via multi-view fusion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fit-NGP: Fitting object models to neural graphics
primitives. <em>ICRA</em>, 18186–18192. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate 3D object pose estimation is key to enabling many robotic applications that involve challenging object interactions. In this work, we show that the density field created by a state-of-the-art efficient radiance field reconstruction method is suitable for highly accurate and robust pose estimation for objects with known 3D models, even when they are very small and with challenging reflective surfaces. We present a fully automatic object pose estimation system based on a robot arm with a single wrist-mounted camera, which can scan a scene from scratch, detect and estimate the 6-Degrees of Freedom (DoF) poses of multiple objects within a couple of minutes of operation. Small objects such as bolts and nuts are estimated with accuracy on order of 1mm.},
  archive   = {C_ICRA},
  author    = {Marwan Taher and Ignacio Alzugaray and Andrew J. Davison},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611272},
  month     = {5},
  pages     = {18186-18192},
  title     = {Fit-NGP: Fitting object models to neural graphics primitives},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kinesthetic-based in-hand object recognition with an
underactuated robotic hand. <em>ICRA</em>, 18179–18185. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tendon-based underactuated hands are intended to be simple, compliant and affordable. Often, they are 3D printed and do not include tactile sensors. Hence, performing in-hand object recognition with direct touch sensing is not feasible. Adding tactile sensors can complicate the hardware and introduce extra costs to the robotic hand. Also, the common approach of visual perception may not be available due to occlusions. In this paper, we explore whether kinesthetic haptics can provide in-direct information regarding the geometry of a grasped object during in-hand manipulation with an underactuated hand. By solely sensing actuator positions and torques over a period of time during motion, we show that a classifier can recognize an object from a set of trained ones with a high success rate of almost 95%. In addition, the implementation of a real-time majority vote during manipulation further improves recognition. Additionally, a trained classifier is also shown to be successful in distinguishing between shape categories rather than just specific objects.},
  archive   = {C_ICRA},
  author    = {Julius Arolovitch and Osher Azulay and Avishai Sintov},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611291},
  month     = {5},
  pages     = {18179-18185},
  title     = {Kinesthetic-based in-hand object recognition with an underactuated robotic hand},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning to grasp in clutter with interactive visual failure
prediction. <em>ICRA</em>, 18172–18178. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern warehouses process millions of unique objects which are often stored in densely packed containers. To automate tasks in this environment, a robot must be able to pick diverse objects from highly cluttered scenes. Real-world learning is a promising approach, but executing picks in the real world is time-consuming, can induce costly failures, and often requires extensive human intervention, which causes operational burden and limits the scope of data collection and deployments. In this work, we leverage interactive probes to visually evaluate grasps in clutter without fully executing picks, a capability we refer to as Interactive Visual Failure Prediction (IVFP). This enables autonomous verification of grasps during execution to avoid costly downstream failures as well as autonomous reward assignment, providing supervision to continuously shape and improve grasping behavior as the robot gathers experience in the real world, without constantly requiring human intervention. Through experiments on a Stretch RE1 robot, we study the effect that IVFP has on performance - both in terms of effective data throughput and success rate, and show that this approach leads to grasping policies that outperform policies trained with human supervision alone, while requiring significantly less human intervention. Code, datasets, and videos available at https://robo-ivfp.github.io},
  archive   = {C_ICRA},
  author    = {Michael Murray and Abhishek Gupta and Maya Cakmak},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611363},
  month     = {5},
  pages     = {18172-18178},
  title     = {Learning to grasp in clutter with interactive visual failure prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Liquids identification and manipulation via digitally
fabricated impedance sensors. <em>ICRA</em>, 18164–18171. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite recent exponential advancements in computer vision and reinforcement learning, it remains challenging for robots to interact with liquids. These challenges are particularly pronounced due to the limitations imposed by opaque containers, transparent liquids, fine-grained splashes, and visual obstructions arising from the robot’s own manipulation activities. Yet, there exists a substantial opportunity for robotics to excel in liquid identification and manipulation, given its potential role in chemical handling in laboratories and various manufacturing sectors such as pharmaceuticals or beverages. In this work, we present a novel approach for liquid class identification and state estimation leveraging electrical impedance sensing. We design and mount a digitally embroidered electrode array to a commercial robot gripper. Coupled with a customized impedance sensing board, we collect data on liquid manipulation with a swept frequency sensing mode and a frequency-specific impedance measuring mode. Our developed learning-based model achieves an accuracy of 93.33% in classifying 9 different types of liquids (8 liquids + air), and 97.65% in estimating the liquid state. We investigate the effectiveness of our system with a series of ablation studies. These findings highlight our work as a promising solution for enhancing robotic manipulation in liquid-related tasks.},
  archive   = {C_ICRA},
  author    = {Junyi Zhu and Young Joong Lee and Yiyue Luo and Tianyu Xu and Chao Liu and Daniela Rus and Stefanie Mueller and Wojciech Matusik},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610518},
  month     = {5},
  pages     = {18164-18171},
  title     = {Liquids identification and manipulation via digitally fabricated impedance sensors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unknown object grasping for assistive robotics.
<em>ICRA</em>, 18157–18163. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel pipeline for unknown object grasping in shared robotic autonomy scenarios. State-of-the-art methods for fully autonomous scenarios are typically learning-based approaches optimised for a specific end-effector, that generate grasp poses directly from sensor input. In the domain of assistive robotics, we seek instead to utilise the user’s cognitive abilities for enhanced satisfaction, grasping performance, and alignment with their high level task-specific goals. Given a pair of stereo images, we perform unknown object instance segmentation and generate a 3D reconstruction of the object of interest. In shared control, the user then guides the robot end-effector across a virtual hemisphere centered around the object to their desired approach direction. A physics-based grasp planner finds the most stable local grasp on the reconstruction, and finally the user is guided by shared control to this grasp. In experiments on the DLR EDAN platform, we report a grasp success rate of 87% for 10 unknown objects, and demonstrate the method’s capability to grasp objects in structured clutter and from shelves.},
  archive   = {C_ICRA},
  author    = {Elle Miller and Maximilian Durner and Matthias Humt and Gabriel Quere and Wout Boerdijk and Ashok M. Sundaram and Freek Stulp and Jörn Vogel},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611347},
  month     = {5},
  pages     = {18157-18163},
  title     = {Unknown object grasping for assistive robotics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). AHPPEBot: Autonomous robot for tomato harvesting based on
phenotyping and pose estimation. <em>ICRA</em>, 18150–18156. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To address the limitations inherent to conventional automated harvesting robots specifically their suboptimal success rates and risk of crop damage, we design a novel bot named AHPPEBot which is capable of autonomous harvesting based on crop phenotyping and pose estimation. Specifically, In phenotyping, the detection, association, and maturity estimation of tomato trusses and individual fruits are accomplished through a multi-task YOLOv5 model coupled with a detectionbased adaptive DBScan clustering algorithm. In pose estimation, we employ a deep learning model to predict seven semantic keypoints on the pedicel. These keypoints assist in the robot’s path planning, minimize target contact, and facilitate the use of our specialized end effector for harvesting. In autonomous tomato harvesting experiments conducted in commercial green-houses, our proposed robot achieved a harvesting success rate of 86.67%, with an average successful harvest time of 32.46 s, showcasing its continuous and robust harvesting capabilities. The result underscores the potential of harvesting robots to bridge the labor gap in agriculture.},
  archive   = {C_ICRA},
  author    = {Xingxu Li and Nan Ma and Yiheng Han and Shun Yang and Siyi Zheng},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610454},
  month     = {5},
  pages     = {18150-18156},
  title     = {AHPPEBot: Autonomous robot for tomato harvesting based on phenotyping and pose estimation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sensorized soft skin for dexterous robotic hands.
<em>ICRA</em>, 18127–18133. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Conventional industrial robots often use two-fingered grippers or suction cups to manipulate objects or interact with the world. Because of their simplified design, they are unable to reproduce the dexterity of human hands when manipulating a wide range of objects. While the control of humanoid hands evolved greatly, hardware platforms still lack capabilities, particularly in tactile sensing and providing soft contact surfaces. In this work, we present a method that equips the skeleton of a tendon-driven humanoid hand with a soft and sensorized tactile skin. Multi-material 3D printing allows us to iteratively approach a cast skin design which preserves the robot’s dexterity in terms of range of motion and speed. We demonstrate that a soft skin enables firmer grasps and piezoresistive sensor integration enhances the hand’s tactile sensing capabilities.},
  archive   = {C_ICRA},
  author    = {Jana Egli and Benedek Forrai and Thomas Buchner and Jiangtao Su and Xiaodong Chen and Robert K. Katzschmann},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611404},
  month     = {5},
  pages     = {18127-18133},
  title     = {Sensorized soft skin for dexterous robotic hands},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WARABI hand: Five-fingered robotic hand with flexible skin
and force sensors for social interaction. <em>ICRA</em>, 18120–18126.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A robotic hand for social interaction should be capable of comfortable touch with humans. However, it is difficult to mount skin, tactile sensors, and driving mechanism required for human contact, especially holding hands, on a slender finger. In addition, in order to unitize the hand for easy use with any robot and maintainability, the mechanism must be contained within the small space of the fingers and palms. In this paper, we propose a human-sized five-fingered robotic hand named WARABI Hand. It is covered with multi-layored rubber skin to realize human-like soft and pleasant feel. Force sensors on each finger link detect contact with humans and adjust gripping force. We conducted experiments in which a humanoid equipped with WARABI Hand grasped forearm, held hands, and interlocked fingers with a person. The performance for object grasping was also evaluated. We demonstrated that our proposed hand is useful for interaction with humans including receiving and handing over things.},
  archive   = {C_ICRA},
  author    = {Aoi Nakane and Iori Yanokura and Shun Hasegawa and Naoya Yamaguchi and Kunio Kojima and Kei Okada and Masayuki Inaba},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610697},
  month     = {5},
  pages     = {18120-18126},
  title     = {WARABI hand: Five-fingered robotic hand with flexible skin and force sensors for social interaction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A wearable robotic hand for hand-over-hand imitation
learning. <em>ICRA</em>, 18113–18119. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dexterous manipulation through imitation learning has gained significant attention in robotics research. The collection of high-quality expert data holds paramount importance when using imitation learning. The existing approaches for acquiring expert data commonly involve utilizing a data glove to capture hand motion information. However, this method suffers from limitations as the collected information cannot be directly mapped to the robotic hand due to discrepancies in their degrees of freedom or structures. Furthermore, it fails to accurately capture force feedback information between the hand and objects during the demonstration process. To overcome these challenges, this paper presents a novel solution in the form of a wearable dexterous hand, namely Handover-hand Imitation learning wearable RObotic Hand (HIRO Hand), which integrates expert data collection and enables the implementation of dexterous operations. This HIRO Hand empowers the operator to utilize their own tactile feedback to determine appropriate force, position, and actions, resulting in more accurate imitation of the expert’s actions. We develop both non-learning and visual behavior cloning based controllers allowing HIRO Hand successfully achieves grasping and in-hand manipulation ability.},
  archive   = {C_ICRA},
  author    = {Dehao Wei and Huazhe Xu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610516},
  month     = {5},
  pages     = {18113-18119},
  title     = {A wearable robotic hand for hand-over-hand imitation learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TPGP: Temporal-parametric optimization with deep grasp prior
for dexterous motion planning. <em>ICRA</em>, 18106–18112. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Grasping motion planning aims to find a feasible grasping trajectory in the configuration space given an input target grasp. While optimizing grasp motion with two or three-fingered grippers has been well studied, the study on natural grasp motion planning with a dexterous hand remains a very challenging problem due to the high dimensional working space. In this work, we propose a novel temporal-parametric grasp prior (TPGP) optimization method to simplify the difficulty of grasping trajectory optimization for the dexterous hand while maintaining smooth and natural properties of the grasping motion. Specifically, we formulate the discrete trajectory parameters into a temporal-based parameterization, where the prior constraint provided by a hand poser network, is introduced to ensure that hand pose is natural and reasonable throughout the trajectory. Finally, we present a joint target optimization strategy to enhance the target pose for more feasible trajectories. Extensive validations on two public datasets show that our method outperforms state-of-the-art methods regarding grasp motion on various metrics.},
  archive   = {C_ICRA},
  author    = {Haoming Li and Qi Ye and Yuchi Huo and Qingtao Liu and Shijian Jiang and Tao Zhou and Xiang Li and Yang Zhou and Jiming Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610408},
  month     = {5},
  pages     = {18106-18112},
  title     = {TPGP: Temporal-parametric optimization with deep grasp prior for dexterous motion planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fully 3D printable robot hand and soft tactile sensor based
on air-pressure and capacitive proximity sensing. <em>ICRA</em>,
18100–18105. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft tactile sensors can enable robots to grasp objects easily and stably by simultaneously providing tactile data and mechanical compliance to robotic hands. If there are low-cost and easy-to-build robotic hands equipped with soft tactile sensors, they would be highly accessible and facilitate many robotics projects. To this end, we propose an accessible robot hand capable of tactile sensing, which can be produced through digital fabrication. We made the robot hand using commercial servo motors as well as components 3D printed from PETG, TPU, and conductive TPU. These materials allow the robot hand to have a soft, durable, and even functional structure. Specifically, the soft fingertip was crafted from TPU and conductive TPU, and their mechanical and electrical properties enable easy implementation of tactile sensing capabilities, such as force and capacitive touch, simply by adding off-the-shelf sensors (air-pressure and capacitance). The proposed robot hand could effectively sense interaction forces and proximity to conductive objects, and its utilization in various tasks was also demonstrated successfully.},
  archive   = {C_ICRA},
  author    = {Sean Taylor and Kyungseo Park and Sankalp Yamsani and Joohyung Kim},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610731},
  month     = {5},
  pages     = {18100-18105},
  title     = {Fully 3D printable robot hand and soft tactile sensor based on air-pressure and capacitive proximity sensing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The new dexterity modular, dexterous, anthropomorphic,
open-source, bimanual manipulation platform: Combining adaptive and
hybrid actuation systems with lockable joints. <em>ICRA</em>,
18094–18099. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work introduces the New Dexterity modular, dexterous, anthropomorphic, open-source, bimanual manipulation platform (OpenBMP) that is designed for research and rapid experimentation in robot grasping, dexterous manipulation, and bimanual manipulation. The platform combines adaptive and hybrid actuation systems with lockable joints, facilitating transitions between the execution of delicate and forceful tasks. Antagonistic tendon-driven elbows and inline actuator transmissions reduce the system’s inertial mass while enhancing energy efficiency and overall performance. Leveraging 3D printing and carbon fiber reinforced manufacturing of core parts, the platform is easy to replicate and highly modular. This paper presents the details of the design, the actuation principles, and the experimental validation of the efficiency of the platform with the execution of complex teleoperation and telemanipulation tasks. The designs, the electronics, and the code are open-sourced to allow replication by others.},
  archive   = {C_ICRA},
  author    = {Che-Ming Chang and Felipe Sanches and Geng Gao and Minas Liarokapis},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610343},
  month     = {5},
  pages     = {18094-18099},
  title     = {The new dexterity modular, dexterous, anthropomorphic, open-source, bimanual manipulation platform: Combining adaptive and hybrid actuation systems with lockable joints},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). L-VIWO: Visual-inertial-wheel odometry based on lane lines.
<em>ICRA</em>, 18079–18085. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To achieve precise localization for autonomous vehicles and mitigate the problem of accumulated drift error in odometry, this paper proposes L-VIWO, a Visual-Inertial-Wheel Odometry based on lane lines. This method effectively utilizes the lateral constraints provided by lane lines to eliminate and relieve the incrementally accumulated pose errors. Firstly, we introduce a lane line tracking method that enables multi-frame tracking of the same lane line, thereby obtaining multi-frame data of a lane line. Then, we utilize multi-frame data of the lane lines and the curvature characteristics of adjacent lane lines to optimize the positions of the lane line sample points, thus building a reliable lane line map. Finally, we use the built local lane line map to correct the position of the vehicle. Based on the corrected position and prior pose from the odometry, we build a graph optimization model to optimize the pose of the vehicle. Through localization experiments on the KAIST dataset, it has been demonstrated that the proposed method effectively enhances the localization accuracy of odometry, thus confirming the effectiveness of the method.},
  archive   = {C_ICRA},
  author    = {Bin Zhao and Yunzhou Zhang and Junjie Huang and Xichen Zhang and Zeyu Long and Yulong Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610139},
  month     = {5},
  pages     = {18079-18085},
  title     = {L-VIWO: Visual-inertial-wheel odometry based on lane lines},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Probabilistic active loop closure for autonomous
exploration. <em>ICRA</em>, 18048–18054. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When a mobile robot autonomously explores an indoor space to produce a localization and navigation map, it is important to create both a stable pose graph and a high-quality occupancy map that covers all the navigable areas. In this work, we propose a novel probabilistic active loop closure framework which attempts to maximally reduce pose graph uncertainty during exploration and improves occupancy map quality. We calculate a probabilistic reward of getting a loop closure at any pose on a pose graph, which considers both how much pose graph uncertainty would be reduced by getting a loop closure there, and the robot’s travel cost to navigate to that pose. By choosing poses that provide the largest rewards, we can maximally reduce pose graph uncertainty while avoiding long travel times. The effectiveness of the method is illustrated through on-device testing in various floor plans.},
  archive   = {C_ICRA},
  author    = {He Yin and Jong Jin Park and Marcelino Almeida and Martin Labrie and Jim Zamiska and Richard Kim},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610213},
  month     = {5},
  pages     = {18048-18054},
  title     = {Probabilistic active loop closure for autonomous exploration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Autonomous implicit indoor scene reconstruction with
frontier exploration. <em>ICRA</em>, 18041–18047. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Implicit neural representations have demonstrated significant promise for 3D scene reconstruction. Recent works have extended their applications to autonomous implicit reconstruction through the Next Best View (NBV) based method. However, the NBV method cannot guarantee complete scene coverage and often necessitates extensive viewpoint sampling, particularly in complex scenes. In the paper, we propose to 1) incorporate frontier-based exploration tasks for global coverage with implicit surface uncertainty-based reconstruction tasks to achieve high-quality reconstruction. and 2) introduce a method to achieve implicit surface uncertainty using color uncertainty, which reduces the time needed for view selection. Further with these two tasks, we propose an adaptive strategy for switching modes in view path planning, to reduce time and maintain superior reconstruction quality. Our method exhibits the highest reconstruction quality among all planning methods and superior planning efficiency in methods involving reconstruction tasks. We deploy our method on a UAV and the results show that our method can plan multi-task views and reconstruct a scene with high quality.},
  archive   = {C_ICRA},
  author    = {Jing Zeng and Yanxu Li and Jiahao Sun and Qi Ye and Yunlong Ran and Jiming Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611382},
  month     = {5},
  pages     = {18041-18047},
  title     = {Autonomous implicit indoor scene reconstruction with frontier exploration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active visual localization for multi-agent collaboration: A
data-driven approach. <em>ICRA</em>, 18034–18040. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Rather than having each newly deployed robot create its own map of its surroundings, the growing availability of SLAM-enabled devices provides the option of simply localizing in a map of another robot or device. In cases such as multi-robot or human-robot collaboration, localizing all agents in the same map is even necessary. However, localizing e.g. a ground robot in the map of a drone or head-mounted MR headset presents unique challenges due to viewpoint changes. This work investigates how active visual localization can be used to overcome such challenges of viewpoint changes. Specifically, we focus on the problem of selecting the optimal viewpoint at a given location. We compare existing approaches in the literature with additional proposed baselines and propose a novel data-driven approach. The result demonstrates the superior performance of our data-driven approach when compared to existing methods, both in controlled simulation experiments and real-world deployment.},
  archive   = {C_ICRA},
  author    = {Matthew Hanlon and Boyang Sun and Marc Pollefeys and Hermann Blum},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610357},
  month     = {5},
  pages     = {18034-18040},
  title     = {Active visual localization for multi-agent collaboration: A data-driven approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tightly-coupled LiDAR-visual-inertial SLAM and large-scale
volumetric occupancy mapping. <em>ICRA</em>, 18027–18033. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous navigation is one of the key requirements for every potential application of mobile robots in the real-world. Besides high-accuracy state estimation, a suitable and globally consistent representation of the 3D environment is indispensable. We present a fully tightly-coupled LiDAR-Visual-Inertial SLAM system and 3D mapping framework applying local submapping strategies to achieve scalability to large-scale environments. A novel and correspondence-free, inherently probabilistic, formulation of LiDAR residuals is introduced, expressed only in terms of the occupancy fields and its respective gradients. These residuals can be added to a factor graph optimisation problem, either as frame-to-map factors for the live estimates or as map-to-map factors aligning the submaps with respect to one another. Experimental validation demonstrates that the approach achieves state-of-the-art pose accuracy and furthermore produces globally consistent volumetric occupancy submaps which can be directly used in downstream tasks such as navigation or exploration.},
  archive   = {C_ICRA},
  author    = {Simon Boche and Sebastián Barbas Laina and Stefan Leutenegger},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610460},
  month     = {5},
  pages     = {18027-18033},
  title     = {Tightly-coupled LiDAR-visual-inertial SLAM and large-scale volumetric occupancy mapping},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VICAN: Very efficient calibration algorithm for large camera
networks. <em>ICRA</em>, 18020–18026. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The precise estimation of camera poses within large camera networks is a foundational problem in computer vision and robotics, with broad applications spanning autonomous navigation, surveillance, and augmented reality. In this paper, we introduce a novel methodology that extends state-of-the-art Pose Graph Optimization (PGO) techniques. Departing from the conventional PGO paradigm, which primarily relies on camera-camera edges, our approach centers on the introduction of a dynamic element - any rigid object free to move in the scene - whose pose can be reliably inferred from a single image. Specifically, we consider the bipartite graph encompassing cameras, object poses evolving dynamically, and camera-object relative transformations at each time step. This shift not only offers a solution to the challenges encountered in directly estimating relative poses between cameras, particularly in adverse environments, but also leverages the inclusion of numerous object poses to ameliorate and integrate errors, resulting in accurate camera pose estimates. Though our framework retains compatibility with traditional PGO solvers, its efficacy benefits from a custom-tailored optimization scheme. To this end, we introduce an iterative primal-dual algorithm, capable of handling large graphs. Empirical benchmarks, conducted on a new dataset of simulated indoor environments, substantiate the efficacy and efficiency of our approach.},
  archive   = {C_ICRA},
  author    = {Gabriel Moreira and Manuel Marques and João Paulo Costeira and Alexander Hauptmann},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611245},
  month     = {5},
  pages     = {18020-18026},
  title     = {VICAN: Very efficient calibration algorithm for large camera networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Observation time difference: An online dynamic objects
removal method for ground vehicles. <em>ICRA</em>, 17997–18003. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the process of urban environment mapping, the sequential accumulations of dynamic objects will leave a large number of traces in the map. These traces will usually have bad influences on the localization accuracy and navigation performance of the robot. Therefore, dynamic objects removal plays an important role for creating clean map. However, conventional dynamic objects removal methods usually run offline. That is, the map is reprocessed after it is constructed, which undoubtedly increases additional time costs. To tackle the problem, this paper proposes a novel method for online dynamic objects removal for ground vehicles. According to the observation time difference between the object and the ground where it is located, dynamic objects are classified into two types: suddenly appear and suddenly disappear. For these two kinds of dynamic objects, we propose downward retrieval and upward retrieval methods to eliminate them respectively. We validate our method on SemanticKITTI dataset and author-collected dataset with highly dynamic objects. Compared with other state-of-the-art methods, our method is more efficient and robust, and reduces the running time per frame by more than 60% on average. Our method will be open-sourced on GitHub 1 .},
  archive   = {C_ICRA},
  author    = {Rongguang Wu and Chenglin Pang and Xuankang Wu and Zheng Fang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611008},
  month     = {5},
  pages     = {17997-18003},
  title     = {Observation time difference: An online dynamic objects removal method for ground vehicles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LESS-map: Lightweight and evolving semantic map in parking
lots for long-term self-localization. <em>ICRA</em>, 17990–17996. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Precise and long-term stable localization is essential in parking lots for tasks like autonomous driving or autonomous valet parking, etc. Existing methods rely on a fixed and memory-inefficient map, which lacks robust data association approaches. And it is not suitable for precise localization or long-term map maintenance. In this paper, we propose a novel mapping, localization, and map update system based on ground semantic features, utilizing low-cost cameras. We present a precise and lightweight parameterization method to establish improved data association and achieve accurate localization at centimeter-level. Furthermore, we propose a novel map update approach by implementing high-quality data association for parameterized semantic features, allowing continuous map update and refinement during re-localization, while maintaining centimeter-level accuracy. We validate the performance of the proposed method in real-world experiments and compare it against state-of-the-art algorithms. The proposed method achieves an average accuracy improvement of 5cm during the registration process. The generated maps consume only a compact size of 450 KB/km and remain adaptable to evolving environments through continuous update.},
  archive   = {C_ICRA},
  author    = {Mingrui Liu and Xinyang Tang and Yeqiang Qian and Jiming Chen and Liang Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610168},
  month     = {5},
  pages     = {17990-17996},
  title     = {LESS-map: Lightweight and evolving semantic map in parking lots for long-term self-localization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SiLVR: Scalable lidar-visual reconstruction with neural
radiance fields for robotic inspection. <em>ICRA</em>, 17983–17989. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a neural-field-based large-scale reconstruction system that fuses lidar and vision data to generate high-quality reconstructions that are geometrically accurate and capture photo-realistic textures. This system adapts the state-of-the-art neural radiance field (NeRF) representation to also incorporate lidar data which adds strong geometric constraints on the depth and surface normals. We exploit the trajectory from a real-time lidar SLAM system to bootstrap a Structure-from-Motion (SfM) procedure to both significantly reduce the computation time and to provide metric scale which is crucial for lidar depth loss. We use submapping to scale the system to large-scale environments captured over long trajectories. We demonstrate the reconstruction system with data from a multi-camera, lidar sensor suite onboard a legged robot, hand-held while scanning building scenes for 600 metres, and onboard an aerial robot surveying a multi-storey mock disaster site-building. Website: https://ori-drs.github.io/projects/silvr/},
  archive   = {C_ICRA},
  author    = {Yifu Tao and Yash Bhalgat and Lanke Frank Tarimo Fu and Matias Mattamala and Nived Chebrolu and Maurice Fallon},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611278},
  month     = {5},
  pages     = {17983-17989},
  title     = {SiLVR: Scalable lidar-visual reconstruction with neural radiance fields for robotic inspection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dusk till dawn: Self-supervised nighttime stereo depth
estimation using visual foundation models. <em>ICRA</em>, 17976–17982.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-supervised depth estimation algorithms rely heavily on frame-warping relationships, exhibiting substantial performance degradation when applied in challenging circumstances, such as low-visibility and nighttime scenarios with varying illumination conditions. Addressing this challenge, we introduce an algorithm designed to achieve accurate selfsupervised stereo depth estimation focusing on nighttime conditions. Specifically, we use pretrained visual foundation models to extract generalised features across challenging scenes and present an efficient method for matching and integrating these features from stereo frames. Moreover, to prevent pixels violating photometric consistency assumption from negatively affecting the depth predictions, we propose a novel masking approach designed to filter out such pixels. Lastly, addressing weaknesses in the evaluation of current depth estimation algorithms, we present novel evaluation metrics. Our experiments, conducted on challenging datasets including Oxford RobotCar and MultiSpectral Stereo, demonstrate the robust improvements realized by our approach.},
  archive   = {C_ICRA},
  author    = {Madhu Vankadari and Samuel Hodgson and Sangyun Shin and Kaichen Zhou and Andrew Markham and Niki Trigoni},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610318},
  month     = {5},
  pages     = {17976-17982},
  title     = {Dusk till dawn: Self-supervised nighttime stereo depth estimation using visual foundation models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OCC-VO: Dense mapping via 3D occupancy-based visual odometry
for autonomous driving. <em>ICRA</em>, 17961–17967. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual Odometry (VO) plays a pivotal role in autonomous systems, with a principal challenge being the lack of depth information in camera images. This paper introduces OCC-VO, a novel framework that capitalizes on recent advances in deep learning to transform 2D camera images into 3D semantic occupancy, thereby circumventing the traditional need for concurrent estimation of ego poses and landmark locations. Within this framework, we utilize the TPV-Former to convert surround view cameras’ images into 3D semantic occupancy. Addressing the challenges presented by this transformation, we have specifically tailored a pose estimation and mapping algorithm that incorporates Semantic Label Filter, Dynamic Object Filter, and finally, utilizes Voxel PFilter for maintaining a consistent global semantic map. Evaluations on the Occ3D-nuScenes not only showcase a 20.6% improvement in Success Ratio and a 29.6% enhancement in trajectory accuracy against ORB-SLAM3, but also emphasize our ability to construct a comprehensive map. Our implementation is open-sourced and available at: https://github.com/USTCLH/OCC-VO.},
  archive   = {C_ICRA},
  author    = {Heng Li and Yifan Duan and Xinran Zhang and Haiyi Liu and Jianmin Ji and Yanyong Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611516},
  month     = {5},
  pages     = {17961-17967},
  title     = {OCC-VO: Dense mapping via 3D occupancy-based visual odometry for autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Quantized visual-inertial odometry. <em>ICRA</em>,
17954–17960. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As edge devices equipped with cameras and inertial measurement units (IMUs) are emerging, it holds huge implications to endow these mobile devices with spatial computing capability. However, ultra-efficient visual-inertial estimation at the size, weight and power (SWAP)-constrained edge devices to provide accurate 3D motion tracking remains challenging. This is exacerbated by data transfer (between different processors and memory) that consumes significantly more energy than computing itself. To push the state of the art, this paper proposes the first-of-its-kind quantized visual-inertial odometry (QVIO) to offer energy-efficient 3D motion tracking. In particular, we first quantize raw visual measurements in an intuitive way with a given small number of bits and then perform an EKF update with these quantized measurements (termed zQVIO). To improve this ad-hoc quantizer (although it works well in practice), we systematically quantize each measurement residual into a single bit and perform maximum-a-posterior (MAP) estimation. measurements. Thanks to these quantizers, the proposed QVIO estimators significantly reduce the data transfer and thus improve energy efficiency. As shown in our extensive experiments, the proposed residual-quantized VIO (rQVIO) achieves remarkably competing performance even when using an average of only 3.7 bits per measurement, equivalent to a data reduction of 8.6 times compared to transmitting single-precision measurements.},
  archive   = {C_ICRA},
  author    = {Yuxiang Peng and Chuchu Chen and Guoquan Huang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610513},
  month     = {5},
  pages     = {17954-17960},
  title     = {Quantized visual-inertial odometry},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decentralized multi-phase formation control for cattle
herding. <em>ICRA</em>, 17948–17953. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Herding is performed by people or trained animals to control the movement of livestock under the desired direction of an operator. This paper presents a novel decentralized control strategy for a group of robots to herd animals which consists of two phases, a surrounding phase and a driving phase. In the surrounding phase, a custom artificial potential field is employed to simultaneously guide the robots to encircle the herd by tracking the outmost animals and maintaining a safe distance from other neighboring robots. Once the encirclement is complete, the robots transition to drive the animals toward a designated goal by simply maintaining their initial formation and traversing to it. Unlike existing works on herding using flocking control, local observations of the nearest animals and communication with other robots within the sensing range are the only requirements for the robots to surround and herd the animals effectively. Moreover, the animal-robot behavior model resembles the interaction of livestock in the presence of an external predatory threat, where robots act as predators. An analytical proof and empirical results collected from different simulators demonstrate that the proposed control enables the robots to converge around the boundary of the animals and guide them toward the designated goal.},
  archive   = {C_ICRA},
  author    = {Dac Dang Khoa Nguyen and Gavin Paul and Alen Alempijevic},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611069},
  month     = {5},
  pages     = {17948-17953},
  title     = {Decentralized multi-phase formation control for cattle herding},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated testing of spatially-dependent environmental
hypotheses through active transfer learning. <em>ICRA</em>, 17941–17947.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The efficient collection of samples is an important factor in outdoor information gathering applications on account of high sampling costs such as time, energy, and potential destruction to the environment. Utilization of available a-priori data can be a powerful tool for increasing efficiency. However, the relationships of this data with the quantity of interest are often not known ahead of time, limiting the ability to leverage this knowledge for improved planning efficiency. To this end, this work combines transfer learning and active learning through a Multi-Task Gaussian Process and an information-based objective function. Through this combination it can explore the space of hypothetical inter-quantity relationships and evaluate these hypotheses in real-time, allowing this new knowledge to be immediately exploited for future plans. The performance of the proposed method is evaluated against synthetic data and is shown to evaluate multiple hypotheses correctly. Its effectiveness is also demonstrated on real datasets. The technique is able to identify and leverage hypotheses which show a medium or strong correlation to reduce prediction error by a factor of 1.4–3.4 within the first 7 samples, and poor hypotheses are quickly identified and rejected eventually having no adverse effect.},
  archive   = {C_ICRA},
  author    = {Nicholas Harrison and Nathan Wallace and Salah Sukkarieh},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611126},
  month     = {5},
  pages     = {17941-17947},
  title     = {Automated testing of spatially-dependent environmental hypotheses through active transfer learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tree instance segmentation and traits estimation for
forestry environments exploiting LiDAR data collected by mobile robots.
<em>ICRA</em>, 17933–17940. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Forests play a crucial role in our ecosystems, functioning as carbon sinks, climate stabilizers, biodiversity hubs, and sources of wood. By the very nature of their scale, monitoring and maintaining forests is a challenging task. Robotics in forestry can have the potential for substantial automation toward efficient and sustainable foresting practices. In this paper, we address the problem of automatically producing a forest inventory by exploiting LiDAR data collected by a mobile platform. To construct an inventory, we first extract tree instances from point clouds. Then, we process each instance to extract forestry inventory information. Our approach provides the per-tree geometric trait of &quot;diameter at breast height&quot; together with the individual tree locations in a plot. We validate our results against manual measurements collected by foresters during field trials. Our experiments show strong segmentation and tree trait estimation performance, underlining the potential for automating forestry services. Results furthermore show a superior performance compared to the popular baseline methods used in this domain.},
  archive   = {C_ICRA},
  author    = {Meher V. R. Malladi and Tiziano Guadagnino and Luca Lobefaro and Matias Mattamala and Holger Griess and Janine Schweier and Nived Chebrolu and Maurice Fallon and Jens Behley and Cyrill Stachniss},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611169},
  month     = {5},
  pages     = {17933-17940},
  title     = {Tree instance segmentation and traits estimation for forestry environments exploiting LiDAR data collected by mobile robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Region-determined localization method for unmanned ground
vehicle under pole-like feature environment. <em>ICRA</em>, 17927–17932.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, a region-determined localization method applied for unmanned ground vehicles (UGVs) is presented. The method aims to solve GNSS-denied localization problem using pole-like feature such as trees or street lights. The approach includes three parts: mapping, bounding, and localization. To map and reconstruct the environment, the hector mapping approach and circle-fitting method are adopted for the occupancy mapping and feature mapping. To bound out the available working region, we define the intersection area of features&#39; enlarged radius and desired operating area as negative and positive virtual boundaries. While the robot is cruising, the likelihood detection method is adopted for obstacle searching and comparing. Using the detection&#39;s searching results as feedback reference, the Extended Kalman Filter (EKF) can modify the shifting between the GNSS signal and the true waypoints of the mowing robot. Three cruising demonstrations are presented to show the mapping and optimizing results. Different cases of demonstration represent different situations and potential issues.},
  archive   = {C_ICRA},
  author    = {Yu-Hsiang Lai and Chia-Yun Chuang and Yu-Qiang Chen and Feng-Li Lian},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611117},
  month     = {5},
  pages     = {17927-17932},
  title     = {Region-determined localization method for unmanned ground vehicle under pole-like feature environment},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Log loading automation for timber-harvesting industry.
<em>ICRA</em>, 17920–17926. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The timber-harvesting industry is lagging its peer industries, such as mining and agriculture, with respect to deployment of robotic, AI and autonomous technologies. In this paper, we tackle automation of a critical task that arises in transporting logs from the forest to the sawmill: the log loading operation. This work is motivated by the acute shortages of human operators and the need to improve the efficiencies of timber-harvesting processes. To this end, we demonstrate the full autonomy pipeline for the log loading operation with a fixed-base manipulator (a.k.a., the crane), starting with perception of logs around the machine, then grasp planning for where to grasp logs, through motion planning and control of the log loading maneuver. Our main contribution is in the full integration of the necessary elements to achieve a completely autonomous loading cycle, where the crane picks up and loads all logs within its reach on a trailer. Notable features of our implementation are a generalizable perception stack, a grasp planner to pick up multiple logs at a time and an extensive experimental campaign conducted outdoors, on a commercial log loader retrofitted for autonomy. Our results demonstrate an overall 87% success rate of the log loading operation, with primary failure cases due to log segmentation errors and deficiencies in the final height adjustment algorithm for grasping logs. We also present detailed timing results of the main parts of the autonomy pipeline, which support the feasibility of deployment in operational environment.},
  archive   = {C_ICRA},
  author    = {Elie Ayoub and Heshan Fernando and William Larrivée-Hardy and Nicolas Lemieux and Philippe Giguère and Inna Sharf},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610808},
  month     = {5},
  pages     = {17920-17926},
  title     = {Log loading automation for timber-harvesting industry},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Containerized vertical farming using cobots. <em>ICRA</em>,
17897–17903. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Containerized vertical farming is a type of vertical farming practice using hydroponics in which plants are grown in vertical layers within a mobile shipping container. Space limitations within shipping containers make the automation of different farming operations challenging. In this paper, we explore the use of cobots (i.e., collaborative robots) to automate two key farming operations, namely, the transplantation of saplings and the harvesting of grown plants. Our method uses a single demonstration from a farmer to extract the motion constraints associated with the tasks, namely, transplanting and harvesting, and can then generalize to different instances of the same task. For transplantation, the motion constraint arises during insertion of the sapling within the growing tube, whereas for harvesting, it arises during extraction from the growing tube. We present experimental results to show that using RGBD camera images (obtained from an eye-in-hand configuration) and one demonstration for each task, it is feasible to perform transplantation of saplings and harvesting of leafy greens using a cobot, without task-specific programming.},
  archive   = {C_ICRA},
  author    = {Dasharadhan Mahalingam and Aditya Patankar and Khiem Phi and Nilanjan Chakraborty and Ryan McGann and IV Ramakrishnan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609985},
  month     = {5},
  pages     = {17897-17903},
  title     = {Containerized vertical farming using cobots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). N-QR: Natural quick response codes for multi-robot instance
correspondence. <em>ICRA</em>, 17889–17896. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image correspondence serves as the backbone for many tasks in robotics, such as visual fusion, localization, and mapping. However, existing correspondence methods do not scale to large multi-robot systems, and they struggle when image features are weak, ambiguous, or evolving. In response, we propose Natural Quick Response codes, or N-QR, which enables rapid and reliable correspondence between large-scale teams of heterogeneous robots. Our method works like a QR code, using keypoint-based alignment, rapid encoding, and error correction via ensembles of image patches of natural patterns. We deploy our algorithm in a production-scale robotic farm, where groups of growing plants must be matched across many robots. We demonstrate superior performance compared to several baselines, obtaining a retrieval accuracy of 88.2%. Our method generalizes to a farm with 100 robots, achieving a 12.5x reduction in bandwidth and a 20.5x speedup. We leverage our method to correspond 700k plants and confirm a link between a robotic seeding policy and germination.},
  archive   = {C_ICRA},
  author    = {Nathaniel Moore Glaser and Rajashree Ravi and Zsolt Kira},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611236},
  month     = {5},
  pages     = {17889-17896},
  title     = {N-QR: Natural quick response codes for multi-robot instance correspondence},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SANet: Small but accurate detector for aerial flying object.
<em>ICRA</em>, 17882–17888. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes SANet, a small but accurate detector for aerial flying objects. The detector introduces an attention module into the feature extraction module (FEM) for enhancing the accuracy. This FEM with fewer convolutional kernel channels can reduce the parameters, speed up the inference time, and mitigate the computational burden. Furthermore, we optimize the Spatial Pyramid Pooling (SPP) module to enhance both the accuracy and speed. By analyzing the structure characteristic of the ResNet and RepVGG network that are usually utilized to extract features, a feature fusion module named RepNeck is designed to comprehensively fuse features extracted by the FEM, further enhancing the speed and accuracy. Eventually, we develop a neural network with an impressively small model size of only 4.5M. This network can achieve the state-of-the-art performance on three challenging datasets. Apart from its superior performance, our approach enjoys a real-time detection speed of 14.8 frames per second (fps) and power consumption of only 2.9W while the CPU and GPU temperatures are maintained below 50 ◦ C even on an edge-computing device, highlighting the practicality of our approach for long-duration flying object detection and monitoring tasks.},
  archive   = {C_ICRA},
  author    = {Xunkuai Zhou and Benyun Zhao and Guidong Yang and Jihan Zhang and Li Li and Ben M. Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611606},
  month     = {5},
  pages     = {17882-17888},
  title     = {SANet: Small but accurate detector for aerial flying object},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). End-to-end thermal updraft detection and estimation for
autonomous soaring using temporal convolutional networks. <em>ICRA</em>,
17875–17881. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Exploiting thermal updrafts to gain altitude can significantly extend the endurance of fixed-wing aircraft, as has been demonstrated by human glider pilots for decades. In this work, we present a novel end-to-end deep learning approach for the simultaneous detection of multiple thermal updrafts and the estimation of their properties — a key capability to let autonomous unmanned aerial vehicles soar as well. In contrast to previous works, our approach does not require separate algorithms for the detection of individual updrafts. Instead, a sequence of sensor measurements from a time window of interest can be directly fed into our temporal convolutional network, which estimates the position, strength, and spread of the encountered updrafts. We demonstrated in simulations that our approach can reliably detect updrafts solely based on measurements of the aircraft’s position and the local vertical wind velocity. Nevertheless, our method can additionally make use of measurements of the roll moment induced by updrafts, which improves the precision further. Compared with a particle-filter-based method, we can determine the correct number of encountered updrafts with an accuracy of 99.99% instead of 79.50%, significantly improve the precision of strength as well as spread estimates, and reduce the computational demand.},
  archive   = {C_ICRA},
  author    = {Christian Gall and Walter Fichter and Aamir Ahmad},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611479},
  month     = {5},
  pages     = {17875-17881},
  title     = {End-to-end thermal updraft detection and estimation for autonomous soaring using temporal convolutional networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). APACE: Agile and perception-aware trajectory generation for
quadrotor flights. <em>ICRA</em>, 17858–17864. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Various perception-aware planning approaches have attempted to enhance the state estimation accuracy during maneuvers, while the feature matchability among frames, a crucial factor influencing estimation accuracy, has often been overlooked. In this paper, we present APACE, an Agile and Perception-Aware trajeCtory gEneration framework for quadrotors aggressive flight, that takes into account feature matchability during trajectory planning. We seek to generate a perception-aware trajectory that reduces the error of visual-based estimator while satisfying the constraints on smoothness, safety, agility and the quadrotor dynamics. The perception objective is achieved by maximizing the number of covisible features while ensuring small enough parallax angles. Additionally, we propose a differentiable and accurate visibility model that allows decomposition of the trajectory planning problem for efficient optimization resolution. Through validations conducted in both a photorealistic simulator and real-world experiments, we demonstrate that the trajectories generated by our method significantly improve state estimation accuracy, with root mean square error (RMSE) reduced by up to an order of magnitude. The source code will be released to benefit the community 1 .},
  archive   = {C_ICRA},
  author    = {Xinyi Chen and Yichen Zhang and Boyu Zhou and Shaojie Shen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610150},
  month     = {5},
  pages     = {17858-17864},
  title     = {APACE: Agile and perception-aware trajectory generation for quadrotor flights},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flow shadowing: A method to detect multiple flow headings
using an array of densely packed whisker-inspired sensors.
<em>ICRA</em>, 17843–17849. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Understanding airflow around a drone is critical for performing advanced maneuvers while maintaining flight stability. Recent research has worked to understand this flow by employing 2D and 3D flow sensors to measure flow from a single source like wind or the drone’s relative motion. Our current work advances flow detection by introducing a strategy to distinguish between two flow sources applied simultaneously from different directions. By densely packing an array of flow sensors (or whiskers), we alter the path of airflow as it moves through the array. We have named this technique “flow shadowing” because we take advantage of the fact that a downstream whisker shadowed (or occluded) by an upstream whisker receives less incident flow. We show that this relationship is predictable for two whiskers based on the percent of occlusion. We then show that a 2x2 spatial array of whiskers responds asymmetrically when multiple flow sources from different headings are applied to the array. This asymmetry is direction-dependent, allowing us to predict the headings of flow from two different sources, like wind and a drone’s relative motion.},
  archive   = {C_ICRA},
  author    = {Teresa A. Kent and Sarah Bergbreiter},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610885},
  month     = {5},
  pages     = {17843-17849},
  title     = {Flow shadowing: A method to detect multiple flow headings using an array of densely packed whisker-inspired sensors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Control-barrier-aided teleoperation with visual-inertial
SLAM for safe MAV navigation in complex environments. <em>ICRA</em>,
17836–17842. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we consider a Micro Aerial Vehicle (MAV) system teleoperated by a non-expert and introduce a perceptive safety filter that leverages Control Barrier Functions (CBFs) in conjunction with Visual-Inertial Simultaneous Localization and Mapping (VI-SLAM) and dense 3D occupancy mapping to guarantee safe navigation in complex and unstructured environments. Our system relies solely on onboard IMU measurements, stereo infrared images, and depth images and autonomously corrects teleoperated inputs when they are deemed unsafe. We define a point in 3D space as unsafe if it satisfies either of two conditions: (i) it is occupied by an obstacle, or (ii) it remains unmapped. At each time step, an occupancy map of the environment is updated by the VI-SLAM by fusing the onboard measurements, and a CBF is constructed to parameterize the (un)safe region in the 3D space. Given the CBF and state feedback from the VI-SLAM module, a safety filter computes a certified reference that best matches the teleoperation input while satisfying the safety constraint encoded by the CBF. In contrast to existing perception-based safe control frameworks, we directly close the perception-action loop and demonstrate the full capability of safe control in combination with real-time VI-SLAM without any external infrastructure or prior knowledge of the environment. We verify the efficacy of the perceptive safety filter in real-time MAV experiments using exclusively onboard sensing and computation and show that the teleoperated MAV is able to safely navigate through unknown environments despite arbitrary inputs sent by the teleoperator.},
  archive   = {C_ICRA},
  author    = {Siqi Zhou and Sotiris Papatheodorou and Stefan Leutenegger and Angela P. Schoellig},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611280},
  month     = {5},
  pages     = {17836-17842},
  title     = {Control-barrier-aided teleoperation with visual-inertial SLAM for safe MAV navigation in complex environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3D autocomplete: Enhancing UAV teleoperation with AI in the
loop. <em>ICRA</em>, 17829–17835. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Manually teleoperating a flying robot can be a demanding task, especially for users with limited levels of experience. This is primarily due to the non-linear properties of such robots in addition to the difficulty of controlling various degrees of freedom at the same time. 3D Autocomplete helps mitigate such limitations by assisting the users in teleoperation. It aids in teleoperating 3D motions, such as helical motions, which are more challenging to the users. The proposed framework uses Artificial Intelligence (AI) to predict just-in-time the user’s intended motion and then, if the user accepts, completes it autonomously in 3D. The AI component of 3D Autocomplete was presented in our previous work, where we introduced a deep learning model and an algorithm to predict as early as possible the user’s desired motion. Moving forward in this work, we focus on synthesizing and completing the user-intended motion autonomously. Also, we introduce a Mixed Reality (MR) user interface for better human-robot interaction. Finally, we evaluate our system subjectively and objectively through human-subject experiments. Autocomplete outperformed traditional method on all criteria with at least 30% improvement in all objective measures.},
  archive   = {C_ICRA},
  author    = {Batool Ibrahim and Imad H. Elhajj and Daniel Asmar},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610932},
  month     = {5},
  pages     = {17829-17835},
  title     = {3D autocomplete: Enhancing UAV teleoperation with AI in the loop},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical deep learning for intention estimation of
teleoperation manipulation in assembly tasks. <em>ICRA</em>,
17814–17820. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In human-robot collaboration, shared control presents an opportunity to teleoperate robotic manipulation to improve the efficiency of manufacturing and assembly processes. Robots are expected to assist in executing the user’s intentions. To this end, robust and prompt intention estimation is needed, relying on behavioral observations. The framework presents an intention estimation technique at hierarchical levels i.e., low-level actions and high-level tasks, by incorporating multi-scale hierarchical information in neural networks. Technically, we employ hierarchical dependency loss to boost overall accuracy. Furthermore, we propose a multi-window method that assigns proper hierarchical prediction windows of input data. An analysis of the predictive power with various inputs demonstrates the predominance of the deep hierarchical model in the sense of prediction accuracy and early intention identification. We implement the algorithm on a virtual reality (VR) setup to teleoperate robotic hands in a simulation with various assembly tasks to show the effectiveness of online estimation. Video demonstration is available at: https://youtu.be/CMYDgcI4j1g.},
  archive   = {C_ICRA},
  author    = {Mingyu Cai and Karankumar Patel and Soshi Iba and Songpo Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610388},
  month     = {5},
  pages     = {17814-17820},
  title     = {Hierarchical deep learning for intention estimation of teleoperation manipulation in assembly tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrating open-world shared control in immersive avatars.
<em>ICRA</em>, 17807–17813. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Teleoperated avatar robots allow people to transport their manipulation skills to environments that may be difficult or dangerous to work in. Current systems are able to give operators direct control of many components of the robot to immerse them in the remote environment, but operators still struggle to complete tasks as competently as they could in person. We present a framework for incorporating open-world shared control into avatar robots to combine the benefits of direct and shared control. This framework preserves the fluency of our avatar interface by minimizing obstructions to the operator’s view and using the same interface for direct, shared, and fully autonomous control. In a human subjects study (N=19), we find that operators using this framework complete a range of tasks significantly more quickly and reliably than those that do not.},
  archive   = {C_ICRA},
  author    = {Patrick Naughton and James Seungbum Nam and Andrew Stratton and Kris Hauser},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611618},
  month     = {5},
  pages     = {17807-17813},
  title     = {Integrating open-world shared control in immersive avatars},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IRoCo: Intuitive robot control from anywhere using a
smartwatch. <em>ICRA</em>, 17800–17806. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces iRoCo (intuitive Robot Control) – a framework for ubiquitous human-robot collaboration using a single smartwatch and smartphone. By integrating probabilistic differentiable filters, iRoCo optimizes a combination of precise robot control and unrestricted user movement from ubiquitous devices. We demonstrate and evaluate the effectiveness of iRoCo in practical teleoperation and drone piloting applications. Comparative analysis shows no significant difference between task performance with iRoCo and gold-standard control systems in teleoperation tasks. Additionally, iRoCo users complete drone piloting tasks 32% faster than with a traditional remote control and report less frustration in a subjective load index questionnaire. Our findings strongly suggest that iRoCo is a promising new approach for intuitive robot control through smartwatches and smart-phones from anywhere, at any time. The code is available at www.github.com/wearable-motion-capture},
  archive   = {C_ICRA},
  author    = {Fabian C Weigend and Xiao Liu and Shubham Sonawani and Neelesh Kumar and Venugopal Vasudevan and Heni Ben Amor},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610805},
  month     = {5},
  pages     = {17800-17806},
  title     = {IRoCo: Intuitive robot control from anywhere using a smartwatch},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online minimization of the robot silhouette viewed from
eye-to-hand camera. <em>ICRA</em>, 17793–17799. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Redundant robots have the potential to perform internal joints motion without modifying the pose of the end-effector by exploiting the null-space of the Jacobian matrix. Capitalizing on that feature, we developed a control technique for minimizing the robot visual appearance when observed from an eye-to-hand camera. Such algorithm is instrumental in contexts where quickly adjusting the perspective to see objects obstructed by the robot is impractical (e.g., teleoperation in narrow environment). Diminished reality techniques are frequently employed in these cases to mitigate the robot intrusion into the environment, although these techniques may sometimes compromise the perceived realism. The experimental evaluation confirmed the effectiveness of our control algorithm, demonstrating an average reduction of 4.67% of the area covered by the robot within the frame when compared to the case without the optimization action.},
  archive   = {C_ICRA},
  author    = {G. Cortigiani and B. Brogi and A. Villani and T. Lisini Baldi and N. D’Aurizio and D. Prattichizzo},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610007},
  month     = {5},
  pages     = {17793-17799},
  title     = {Online minimization of the robot silhouette viewed from eye-to-hand camera},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SPOTS: Stable placement of objects with reasoning in
semi-autonomous teleoperation systems. <em>ICRA</em>, 17786–17792. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pick-and-place is one of the fundamental tasks in robotics research. However, the attention has been mostly focused on the &quot;pick&quot; task, leaving the &quot;place&quot; task relatively unexplored. In this paper, we address the problem of placing objects in the context of a teleoperation framework. Particularly, we focus on two aspects of the place task: stability robustness and contextual reasonableness of object placements. Our proposed method combines simulation-driven physical stability verification via real-to-sim and the semantic reasoning capability of large language models. In other words, given place context information (e.g., user preferences, object to place, and current scene information), our proposed method outputs a probability distribution over the possible placement candidates, considering the robustness and reasonableness of the place task. Our proposed method is extensively evaluated in two simulation and one real world environments and we show that our method can greatly increase the physical plausibility of the placement as well as contextual soundness while considering user preferences. Code, video, and details are available at: https://joonhyunglee.github.io/spots/},
  archive   = {C_ICRA},
  author    = {Joonhyung Lee and Sangbeom Park and Jeongeun Park and Kyungjae Lee and Sungjoon Choi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611613},
  month     = {5},
  pages     = {17786-17792},
  title     = {SPOTS: Stable placement of objects with reasoning in semi-autonomous teleoperation systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TELESIM: A modular and plug-and-play framework for robotic
arm teleoperation using a digital twin. <em>ICRA</em>, 17770–17777. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Teleoperating robotic arms can be a challenging task for non-experts, particularly when using complex control devices or interfaces. To address the limitations and challenges of existing teleoperation frameworks, such as cognitive strain, control complexity, robot compatibility, and user evaluation, we propose TELESIM, a modular and plug-and-play framework that enables direct teleoperation of any robotic arm using a digital twin as the interface between users and the robotic system. Due to TELESIM’s modular design, it is possible to control the digital twin using any device that outputs a 3D pose, such as a virtual reality controller or a finger-mapping hardware controller. To evaluate the efficacy and user-friendliness of TELESIM, we conducted a user study with 37 participants. The study involved a simple pick-and-place task, which was performed using two different robots equipped with two different control modalities. Our experimental results show that most users were able to succeed by building at least a tower of 3 cubes in 10 minutes, with only 5 minutes of training beforehand, regardless of the control modality or robot used, demonstrating the usability and user-friendliness of TELESIM.},
  archive   = {C_ICRA},
  author    = {Florent P Audonnet and Jonathan Grizou and Andrew Hamilton and Gerardo Aragon-Camarasa},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610935},
  month     = {5},
  pages     = {17770-17777},
  title     = {TELESIM: A modular and plug-and-play framework for robotic arm teleoperation using a digital twin},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Remote control of untethered magnetic robots within a lumen
using x-ray-guided robotic platform. <em>ICRA</em>, 17763–17769. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Until now, the potential of untethered magnetic robots (UMRs), propelled by external time-periodic magnetic fields, has been hindered by the limitations of wireless manipulation systems or noninvasive imaging techniques combined. The need for simultaneous actuation and noninvasive localization imposes a strict constraint on both functionalities. This study addresses this challenge by substantiating the feasibility through experimental validation, showcasing the direct teleoperation of UMRs within a fluid-filled lumen. This teleoperation capability is facilitated by a scalable X-ray-guided robotic platform, extendable to match the dimensions required for in vivo applications, marking a noteworthy advancement. Our methodology is demonstrated by teleoperating a 12-mm-long screw-shaped UMR (5 mm in diameter) within a bifurcated lumen, filled with blood. This navigation is achieved using controlled rotating magnetic fields, guided by real-time X-ray Fluoroscopy images. Incorporating a two-degree-of-freedom control system, we demonstrate the operator’s capability to use X-ray Fluoroscopy images to keep the UMR coupled with the external field during wireless teleoperations, resulting in a success rate of 76.6% when moving along the intended pathways, with a mean absolute position error of 1.6 ± 2.1 mm.},
  archive   = {C_ICRA},
  author    = {Leendert-Jan W. Ligtenberg and Nicole C. A. Rabou and Sander Peters and Trishal Vengetela and Vincent Schut and H. Remco Liefers and Michiel Warlé and Islam S. M. Khalil},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611161},
  month     = {5},
  pages     = {17763-17769},
  title     = {Remote control of untethered magnetic robots within a lumen using X-ray-guided robotic platform},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Acoustically driven micropipette for hydrodynamic
manipulation of mouse oocytes. <em>ICRA</em>, 17757–17762. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Micromanipulation techniques that can achieve controlled fine operations at the micro scale play an important role in biomedical fields including embryo engineering, gene engineering, drug screening, and cell analysis. However, micromanipulation of biological micro-objects, such as cells and micro tissues, suffers from mechanical damage and low efficiency. Several techniques have been introduced to manipulate cells more easily, but most of them are restricted by expensive devices, limited work area, and potential damage to cellular structure. Here we develop a hydrodynamic manipulation method to rotate and transport mouse oocytes, which utilizes acoustic waves and micropipette to generate acoustic radiation force and excite microstreaming. This method can accomplish rotational and translational operations precisely and controllably. We tested the process of trapping, rotation, and transportation of the mouse oocytes, and measured rotational and translational speed with a range of applied voltage. The method was able to shorten the cost time of delivery and posture adjustment before oocyte injection. Our study provides an easy-to-use technique for oocyte manipulation without contact, and it has the potential to be universally applied in many cellular studies.},
  archive   = {C_ICRA},
  author    = {Zhaofeng Zuo and Xiaoming Liu and Zhuo Chen and Yuyang Li and Xiaoqing Tang and Dan Liu and Qiang Huang and Tatsuo Arai},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610242},
  month     = {5},
  pages     = {17757-17762},
  title     = {Acoustically driven micropipette for hydrodynamic manipulation of mouse oocytes},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Fast photoacoustic microscopy with robot controlled
microtrajectory optimization. <em>ICRA</em>, 17750–17756. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Photoacoustic Microscopy (PAM) is a relatively new imaging modality in biomedicine. However, point-by-point raster scanning in PAM suffers from low imaging speed. Sparse sampling has been studied in recent years and with the development of deep learning algorithms, extensive efforts have been devoted to sparse image reconstruction while little attention has been paid to sparse sampling trajectory design required for actual implementation. The use of real-time adaptive robotically controlled sampling with micro-scale accuracy with due consideration of physical constraints can pave the way for using PAM for robot-assisted microsurgery. This work proposes a fast PAM scheme with robot-controlled microtrajectory optimization. The proposed method is adaptive to imaging details of different regions of interest (ROI) and detailed experiments have been conducted on both simulation and in-vivo settings. Results show that our proposed method can achieve faster scanning speed than traditional raster scanning and improved image quality in ROI than the standard spiral trajectory, which demonstrates the effectiveness of our proposed method and its potential to be deployed in other point-by-point scanning systems.},
  archive   = {C_ICRA},
  author    = {Yating Luo and Yuxuan Liu and Jiasheng Zhou and Sung-Liang Chen and Yao Guo and Guang-Zhong Yang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610085},
  month     = {5},
  pages     = {17750-17756},
  title     = {Fast photoacoustic microscopy with robot controlled microtrajectory optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Automated sperm morphology analysis based on instance-aware
part segmentation. <em>ICRA</em>, 17743–17749. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traditional sperm morphology analysis is based on tedious manual annotation. Automated morphology analysis of a high number of sperm requires accurate segmentation of each sperm part and quantitative morphology evaluation. State-of-the-art instance-aware part segmentation networks follow a &quot;detect-then-segment&quot; paradigm. However, due to sperm’s slim shape, their segmentation suffers from large context loss and feature distortion due to bounding box cropping and resizing during ROI Align. Moreover, morphology measurement of sperm tail is demanding because of the long and curved shape and its uneven width. This paper presents automated techniques to measure sperm morphology parameters automatically and quantitatively. A novel attention-based instance-aware part segmentation network is designed to reconstruct lost contexts outside bounding boxes and to fix distorted features, by refining preliminary segmented masks through merging features extracted by feature pyramid network. An automated centerline-based tail morphology measurement method is also proposed, in which an outlier filtering method and endpoint detection algorithm are designed to accurately reconstruct tail endpoints. Experimental results demonstrate that the proposed network outperformed the state-of-the-art top-down RP-R-CNN by 9.2% ${\mathbf{AP}}_{vol}^p$, and the proposed automated tail morphology measurement method achieved high measurement accuracies of 95.34%,96.39%,91.20% for length, width and curvature, respectively.},
  archive   = {C_ICRA},
  author    = {Wenyuan Chen and Haocong Song and Changsheng Dai and Aojun Jiang and Guanqiao Shan and Hang Liu and Yanlong Zhou and Khaled Abdalla and Shivani N Dhanani and Katy Fatemeh Moosavi and Shruti Pathak and Clifford Librach and Zhuoran Zhang and Yu Sun},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611339},
  month     = {5},
  pages     = {17743-17749},
  title     = {Automated sperm morphology analysis based on instance-aware part segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated sperm immobilization with a clinically-compatible
and compact XYZ stage. <em>ICRA</em>, 17736–17742. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automated positioning systems play a pivotal role in micro-scale cell manipulation. In clinical intracytoplasmic sperm injection (ICSI) of in vitro fertilization (IVF) treatment, a motile sperm needs to be immobilized by glass micropipette tapping for subsequent surgical steps. The process requires accurate tracking of the target sperm and precise alignment between the sperm tail and the micropipette. Manual sperm immobilization suffers from inconsistent success rates, and current robotic systems developed for the task fail to comply with the standard clinical setup. Instead of using a motorized micromanipulator as in existing robotic systems, this paper presents an automated, compact three-dimensional positioning stage for sperm immobilization that can be seamlessly integrated into standard clinical platforms. Based on the analysis of the sperm head orientation, an adaptive tail tapping planning strategy is established to avoid the risk of touching the sperm head where DNA is contained. A visual servo controller equipped with a dynamic sperm motion observer is employed to achieve precise tracking and positioning of the target sperm three-dimensionally. Experimental results revealed the system achieved a success rate of 93.5% and a time cost of 5.5 s for automated sperm immobilization.},
  archive   = {C_ICRA},
  author    = {Haocong Song and Wenyuan Chen and Changsheng Dai and Guanqiao Shan and Steven Yang and Aojun Jiang and Zhuoran Zhang and Yu Sun},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611426},
  month     = {5},
  pages     = {17736-17742},
  title     = {Automated sperm immobilization with a clinically-compatible and compact XYZ stage},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robotic mosaic atomic force microscopy through sequential
imaging and multiview iterative closest points method. <em>ICRA</em>,
17729–17735. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a functionality that has been developed for the home-made AFM-in-SEM robotic system at the ISIR laboratory. The method allows extending the range of an Atomic Force Microscope (AFM) and dealing with drift issues by fusing multiple individually AFM topography patches. The merging of the patches into a single image is done through a Generalized Procrustes Analysis Iterative Closest Point (GPA-ICP) algorithm. To validate the effectiveness of the approach, an AFM image of a TGX1 calibration grid and a 3.4billion-year-old organic-walled microfossil are reconstructed by automatically merging 50 AFM elementary topography patches of dimension 0.9 µm × 1.2 µm based on feature matching. The overlap between two adjacent patches is 50 % and 33 % in the X and Y axes respectively. The result is a coherent 3.2 µm × 3.0 µm drift-free long range AFM topography without significant artifacts. The method is tested using an AFM-in-SEM system based on a 3-DOF cartesian robot equipped with inertial piezoelectric actuators. This method can be used to extend the range of any type of AFM with a dual XY stage setup. Thus, it opens the door for high-resolution long-range AFM by adding a long-range coarse resolution stage to a preexisting AFM system all without needing to actuate both stages simultaneously.},
  archive   = {C_ICRA},
  author    = {Freddy Romero Leiro and Stéphane Régnier and Frédéric Delarue and Mokrane Boudaoud},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610473},
  month     = {5},
  pages     = {17729-17735},
  title     = {Robotic mosaic atomic force microscopy through sequential imaging and multiview iterative closest points method},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robotic capillary insertion to the xenopus oocyte using
microscopic image analysis and QCR force sensor. <em>ICRA</em>,
17723–17728. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presented the three-dimensional oocyte manipulation system for the two-electrode voltage clamp (TEVC) experiment under stereomicroscopy. We firstly developed a sequential calibration method to correlate the workspace of the stereomicroscopy with the image and the micromanipulator. Even though the focal depth of the microscopy was limited, the proposed method functioned the three-dimensional position detection and calculated the homogeneous transformation matrix. We secondly employed hybrid use of the image-based manipulation and the quartz crystal resonator (QCR) force sensor. The imaging technique was used to detect the tip of the glass capillary and the contact to the cell membrane, whereas the QCR force sensor was incorporated to detect the force interaction between the sample and the glass capillary. Using the system and proposed technique, we demonstrated the automatic capillary insertion for TEVC experiment, at which the low insertion depth was preferable. The results indicated that the coordination calibration technique provided the positioning accuracy of the capillary tip on the order of 10 μm. The imaging technique could detect the contact to the elastic objects and cell membrane. QCR force sensor achieved quite small force measurement and feedback control at the control frequency of 100 Hz without latency.},
  archive   = {C_ICRA},
  author    = {Kazusa Otani and Hirotaka Sugiura and Shiro Watanabe and Turan Bilal and Satoshi Amaya and Fumihito Arai},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610363},
  month     = {5},
  pages     = {17723-17728},
  title     = {Robotic capillary insertion to the xenopus oocyte using microscopic image analysis and QCR force sensor},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated non-invasive analysis of motile sperms using
cross-scale guidance network. <em>ICRA</em>, 17708–17714. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unbiased measurement of sperm morphometric and motility parameters is essential for assessing fertility potential and guiding visual feedback for microrobotic manipulation. Automated analysis of multiple sperms and selection of an optimal sperm is crucial for in vitro fertilisation treatment such as robotic intracytoplasmic sperm injection. However, conventional image processing methods have limitations in analysing small sperm objects under microscopic imaging. The emergence of convolutional neural networks (CNNs) has offered promising advancements in microscopic image analysis. However, previous CNN methods have struggled to accurately segment tiny objects, requiring staining or fluorescence techniques to enhance visual contrast between sperm and culture medium, leading to clinical impracticality. To address these limitations, we introduce a novel segmentation network named the cross-scale guidance (CSG) network for accurate and efficient segmentation of minute sperm objects. The CSG network employs innovative modules, including collateral multi-scale convolution, cross-scale feature map guide, and multi-scale feature fusion, to preserve essential sperm details despite their small size. Experimental results indicate that the CSG network surpassed the state-of-the-art models designed for small object segmentation, achieving a significant increase up to 18.62% higher mean intersection over union (mIoU). Additionally, the CSG network excelled in sperm morphometric analysis, achieving errors below 20%. Moreover, sperm motility parameters were further derived from the segmentation results for comprehensive sperm fertility analysis.},
  archive   = {C_ICRA},
  author    = {Wei Dai and Zixuan Wu and Jiaqi Wang and Rui Liu and Min Wang and Tianyi Wu and Junxian Zhou and Zhuoran Zhang and Jun Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611526},
  month     = {5},
  pages     = {17708-17714},
  title     = {Automated non-invasive analysis of motile sperms using cross-scale guidance network},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving the ROS 2 navigation stack with real-time local
costmap updates for agricultural applications. <em>ICRA</em>,
17701–17707. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ROS 2 Navigation Stack (Nav2) has emerged as a widely used software component providing the underlying basis to develop a variety of high-level functionalities. However, when used in outdoor environments such as orchards and vineyards, its functionality is notably limited by the presence of obstacles and/or situations not commonly found in indoor settings. One such example is given by tall grass and weeds that can be safely traversed by a robot, but that can be perceived as obstacles by LiDAR sensors, and then force the robot to take longer paths to avoid them, or abort navigation altogether. To overcome these limitations, domain specific extensions must be developed and integrated into the software pipeline. This paper presents a new, lightweight approach to address this challenge and improve outdoor robot navigation. Leveraging the multi-scale nature of the costmaps supporting Nav2, we developed a system that using a depth camera performs pixel level classification on the images, and in real time injects corrections into the local cost map, thus enabling the robot to traverse areas that would otherwise be avoided by the Nav2. Our approach has been implemented and validated on a Clearpath Husky and we demonstrate that with this extension the robot is able to perform navigation tasks that would be otherwise not practical with the standard components.},
  archive   = {C_ICRA},
  author    = {Ettore Sani and Antonio Sgorbissa and Stefano Carpin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610984},
  month     = {5},
  pages     = {17701-17707},
  title     = {Improving the ROS 2 navigation stack with real-time local costmap updates for agricultural applications},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A method for multi-robot asynchronous trajectory execution
in MoveIt2. <em>ICRA</em>, 17694–17700. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a method that enables the parallel independent execution of trajectories for multi-robot / multi-arm systems in a shared workspace in MoveIt2. The proposed method leverages a centralized scheduler in a distributed set up to prevent collisions while the robots move independently. We argue that this approach is better suited than the state of the art (i.e., synchronous execution) for flexible/adaptive robotic tasks where the actions to be performed may vary in planning and execution time depending on sensor data (e.g., pick and place with inspection, assembly) as it is able to reduce the total execution time w.r.t. current approaches leveraging a single arm or multiple arms with synchronous motion planning.},
  archive   = {C_ICRA},
  author    = {Pascal Stoop and Tharaka Ratnayake and Giovanni Toffetti},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611498},
  month     = {5},
  pages     = {17694-17700},
  title     = {A method for multi-robot asynchronous trajectory execution in MoveIt2},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward automated programming for robotic assembly using
ChatGPT. <em>ICRA</em>, 17687–17693. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite significant technological advancements, the process of programming robots for adaptive assembly remains labor-intensive, demanding expertise in multiple domains and often resulting in task-specific, inflexible code. This work explores the potential of Large Language Models (LLMs), like ChatGPT, to automate this process, leveraging their ability to understand natural language instructions, generalize examples to new tasks, and write code. In this paper, we suggest how these abilities can be harnessed and applied to real-world challenges in the manufacturing industry. We present a novel system that uses ChatGPT to automate the process of programming robots for adaptive assembly by decomposing complex tasks into simpler subtasks, generating robot control code, executing the code in a simulated workcell, and debugging syntax and control errors, such as collisions. We outline the architecture of this system and strategies for task decomposition and code generation. Finally, we demonstrate how our system can autonomously program robots for various assembly tasks in a real-world project.},
  archive   = {C_ICRA},
  author    = {Annabella Macaluso and Nicholas Cote and Sachin Chitta},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610554},
  month     = {5},
  pages     = {17687-17693},
  title     = {Toward automated programming for robotic assembly using ChatGPT},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cook2LTL: Translating cooking recipes to LTL formulae using
large language models. <em>ICRA</em>, 17679–17686. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cooking recipes are challenging to translate to robot plans as they feature rich linguistic complexity, temporally-extended interconnected tasks, and an almost infinite space of possible actions. Our key insight is that combining a source of cooking domain knowledge with a formalism that captures the temporal richness of cooking recipes could enable the extraction of unambiguous, robot-executable plans. In this work, we use Linear Temporal Logic (LTL) as a formal language expressive enough to model the temporal nature of cooking recipes. Leveraging a pretrained Large Language Model (LLM), we present Cook2LTL, a system that translates instruction steps from an arbitrary cooking recipe found on the internet to a set of LTL formulae, grounding high-level cooking actions to a set of primitive actions that are executable by a manipulator in a kitchen environment. Cook2LTL makes use of a caching scheme that dynamically builds a queryable action library at runtime. We instantiate Cook2LTL in a realistic simulation environment (AI2-THOR), and evaluate its performance across a series of cooking recipes. We demonstrate that our system significantly decreases LLM API calls (−51%), latency (−59%), and cost (−42%) compared to a baseline that queries the LLM for every newly encountered action at runtime.},
  archive   = {C_ICRA},
  author    = {Angelos Mavrogiannis and Christoforos Mavrogiannis and Yiannis Aloimonos},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611086},
  month     = {5},
  pages     = {17679-17686},
  title     = {Cook2LTL: Translating cooking recipes to LTL formulae using large language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GSL-bench: High fidelity gas source localization
benchmarking tool. <em>ICRA</em>, 17672–17678. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Gas Source Localization (GSL) is a challenging field of research within the robotics community, with high-stakes search-and-rescue applications. Existing methods vary widely and each has its strengths and weaknesses. Comparisons of different methods are limited due to the lack of a broadly adopted and standardized testing methodology. Existing GSL evaluations vary in environment size, wind conditions, and gas simulation fidelity. They also lack photo-realistic rendering for the integration of obstacle avoidance. In this paper, we propose GSL-Bench, a benchmarking tool that can evaluate the performance of existing GSL algorithms. GSL-Bench features high-fidelity graphics and gas simulation, featuring NVIDIA’s ® Isaac Sim and OpenFOAM computational fluid dynamics software (CFD). Realism is further increased by simulating relevant gas and wind sensors. Scene generation is simplified with the introduction of AutoGDM+, capable of procedural environment generation, CFD and particle-based gas dispersion simulation. To illustrate GSL-Bench’s capabilities, three algorithms are compared in six warehouse settings of increasing complexity: E. Coli, dung beetle, and a random walker. Our results demonstrate GSL-Bench’s ability to provide valuable insights into algorithm performance.Site: https://sites.google.com/view/gslbench/},
  archive   = {C_ICRA},
  author    = {Hajo H. Erwich and Bardienus P. Duisterhof and Guido C.H.E. de Croon},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610755},
  month     = {5},
  pages     = {17672-17678},
  title     = {GSL-bench: High fidelity gas source localization benchmarking tool},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CoBRA: A composable benchmark for robotics applications.
<em>ICRA</em>, 17665–17671. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Selecting an optimal robot, its base pose, and trajectory for a given task is currently mainly done by human expertise or trial and error. To evaluate automatic approaches to this combined optimization problem, we introduce a benchmark suite encompassing a unified format for robots, environments, and task descriptions. Our benchmark suite is especially useful for modular robots, where the multitude of robots that can be assembled creates a host of additional parameters to optimize. We include tasks such as machine tending and welding in synthetic environments and 3D scans of real-world machine shops. All benchmarks are accessible through cobra.cps.cit.tum.de, a platform to conveniently share, reference, and compare tasks, robot models, and solutions.},
  archive   = {C_ICRA},
  author    = {Matthias Mayer and Jonathan Külz and Matthias Althoff},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610776},
  month     = {5},
  pages     = {17665-17671},
  title     = {CoBRA: A composable benchmark for robotics applications},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enabling the deployment of any-scale robotic applications in
microservice architectures through automated containerization*.
<em>ICRA</em>, 17650–17656. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In an increasingly automated world – from ware-house robots to self-driving cars – streamlining the development and deployment process and operations of robotic applications becomes ever more important. Automated DevOps processes and microservice architectures have already proven successful in other domains such as large-scale customer-oriented web services (e.g., Netflix). We recommend to employ similar microservice architectures for the deployment of small- to large-scale robotic applications in order to accelerate development cycles, loosen functional dependence, and improve resiliency and elasticity. In order to facilitate involved DevOps processes, we present and release a tooling suite for automating the development of microservices for robotic applications based on the Robot Operating System (ROS). Our tooling suite covers the automated minimal containerization of ROS applications, a collection of useful machine learning-enabled base container images, as well as a CLI tool for simplified interaction with container images during the development phase. Within the scope of this paper, we embed our tooling suite into the overall context of streamlined robotics deployment and compare it to alternative solutions. We release our tools as open-source software at github.com/ika-rwth-aachen/dorotos.},
  archive   = {C_ICRA},
  author    = {Jean-Pierre Busch and Lennart Reiher and Lutz Eckstein},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611586},
  month     = {5},
  pages     = {17650-17656},
  title     = {Enabling the deployment of any-scale robotic applications in microservice architectures through automated containerization*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MoRC—a modular robot controller. <em>ICRA</em>, 17643–17649.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {MoRC is a high-performance modular robot controller based on the Functional Mock-up Interface (FMI) standard. The goal is to control any (industrial) robot with electrical drives using a customizable vendor-agnostic control cabinet and an innovative, self-developed software architecture based on exchangeable multi-rate real-time control components with standardized interfaces. On the hardware side, the use of EtherCAT (Ethernet for Control Automation Technology) allows connecting a freely selectable number of COTS (commercial off-the-shelf) electrical drives and sensors. On the software side, this is matched with exchangeable control software modules based on the FMI standard. Those can be interconnected for forming user-defined multi-rate control structures which can be executed as synchronized real-time threads on a central Linux-based multi-core computing unit. That unlocks additional computational potential for advanced high-frequency control algorithms. Control structures can be switched at runtime to handle highly diverse control tasks. This paper presents the architectural concepts as well as first experiments on an industrial robot testbed.},
  archive   = {C_ICRA},
  author    = {Carsten Oldemeyer and Matthias Hellerer and Matthias Reiner and Bernhard Thiele and Patrick Weber and Tobias Bellmann},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611553},
  month     = {5},
  pages     = {17643-17649},
  title     = {MoRC—A modular robot controller},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fully distributed shape sensing of a flexible surgical
needle using optical frequency domain reflectometry for prostate
interventions. <em>ICRA</em>, 17594–17601. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In minimally invasive procedures such as biopsies and prostate cancer brachytherapy, accurate needle placement remains challenging due to limitations in current tracking methods related to interference, reliability, resolution or image contrast. This often leads to frequent needle adjustments and reinsertions. To address these shortcomings, we introduce an optimized needle shape-sensing method using a fully distributed grating-based sensor. The proposed method uses simple trigonometric and geometric modeling of the fiber using optical frequency domain reflectometry (OFDR), without requiring prior knowledge of tissue properties or needle deflection shape and amplitude. Our optimization process includes a reproducible calibration process and a novel tip curvature compensation method. We validate our approach through experiments in artificial isotropic and inhomogeneous animal tissues, establishing ground truth using 3D stereo vision and cone beam computed tomography (CBCT) acquisitions, respectively. Our results yield an average RMSE ranging from 0.58 ± 0.21 mm to 0.66 ± 0.20 mm depending on the chosen spatial resolution, achieving the submillimeter accuracy required for interventional procedures.},
  archive   = {C_ICRA},
  author    = {Jacynthe Francoeur and Dimitri Lezcano and Yernar Zhetpissov and Raman Kashyap and Iulian Iordachita and Samuel Kadoury},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610256},
  month     = {5},
  pages     = {17594-17601},
  title     = {Fully distributed shape sensing of a flexible surgical needle using optical frequency domain reflectometry for prostate interventions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Iterative PnP and its application in 3D-2D vascular image
registration for robot navigation. <em>ICRA</em>, 17560–17566. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper reports on a new real-time robotcentered 3D-2D vascular image alignment algorithm, which is robust to outliers and can align nonrigid shapes. Few works have managed to achieve both real-time and accurate performance for vascular intervention robots. This work bridges high-accuracy 3D-2D registration techniques and computational efficiency requirements in intervention robot applications. We categorize centerline-based vascular 3D-2D image registration problems as an iterative Perspective-n-Point (PnP) problem and propose using the Levenberg-Marquardt solver on the Lie manifold. Then, the recently developed Reproducing Kernel Hilbert Space (RKHS) algorithm is introduced to overcome the &quot;big-to-small&quot; problem in typical robotic scenarios. Finally, an iterative reweighted least squares is applied to solve RKHSbased formulation efficiently. Experiments indicate that the proposed algorithm processes registration over 50 Hz (rigid) and 20 Hz (nonrigid) and obtains competing registration accuracy similar to other works. Results indicate that our Iterative PnP is suitable for future vascular intervention robot applications.},
  archive   = {C_ICRA},
  author    = {Jingwei Song and Keke Yang and Zheng Zhang and Meng Li and Tuoyu Cao and Maani Ghaffari},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610392},
  month     = {5},
  pages     = {17560-17566},
  title     = {Iterative PnP and its application in 3D-2D vascular image registration for robot navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Recasting generic pretrained vision transformers as
object-centric scene encoders for manipulation policies. <em>ICRA</em>,
17544–17552. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generic re-usable pre-trained image representation encoders have become a standard component of methods for many computer vision tasks. As visual representations for robots however, their utility has been limited, leading to a recent wave of efforts to pre-train robotics-specific image encoders that are better suited to robotic tasks than their generic counterparts. We propose Scene Objects From Transformers, abbreviated as SOFT(•), a wrapper around pre-trained vision transformer (PVT) models that bridges this gap without any further training. Rather than construct representations out of only the final layer activations, SOFT(•) individuates and locates object-like entities from PVT attentions, and describes them with PVT activations, producing an object-centric embedding. Across standard choices of generic pre-trained vision transformers PVT, we demonstrate in each case that policies trained on SOFT(PVT) far outstrip standard PVT representations for manipulation tasks in simulated and real settings, approaching the state-of-the-art robotics-aware representations. Code, appendix and videos: https://sites.google.com/view/robot-soft/},
  archive   = {C_ICRA},
  author    = {Jianing Qian and Anastasios Panagopoulos and Dinesh Jayaraman},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610131},
  month     = {5},
  pages     = {17544-17552},
  title     = {Recasting generic pretrained vision transformers as object-centric scene encoders for manipulation policies},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HIO-SDF: Hierarchical incremental online signed distance
fields. <em>ICRA</em>, 17537–17543. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A good representation of a large, complex mobile robot workspace must be space-efficient yet capable of encoding relevant geometric details. When exploring unknown environments, it needs to be updatable incrementally in an online fashion. We introduce HIO-SDF, a new method that represents the environment as a Signed Distance Field (SDF). State of the art representations of SDFs are based on either neural networks or voxel grids. Neural networks are capable of representing the SDF continuously. However, they are hard to update incrementally as neural networks tend to forget previously observed parts of the environment unless an extensive sensor history is stored for training. Voxel-based representations do not have this problem but they are not space-efficient especially in large environments with fine details. HIO-SDF combines the advantages of these representations using a hierarchical approach which employs a coarse voxel grid that captures the observed parts of the environment together with high-resolution local information to train a neural network. HIO-SDF achieves a 46% lower mean global SDF error across all test scenes than a state of the art continuous representation, and a 30% lower error than a discrete representation at the same resolution as our coarse global SDF grid. Videos and code are available at: https://samsunglabs.github.io/HIO-SDF-project-page/},
  archive   = {C_ICRA},
  author    = {Vasileios Vasilopoulos and Suveer Garg and Jinwook Huh and Bhoram Lee and Volkan Isler},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610367},
  month     = {5},
  pages     = {17537-17543},
  title     = {HIO-SDF: Hierarchical incremental online signed distance fields},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Point cloud models improve visual robustness in robotic
learners. <em>ICRA</em>, 17529–17536. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual control policies can encounter significant performance degradation when visual conditions like lighting or camera position differ from those seen during training – often exhibiting sharp declines in capability even for minor differences. In this work, we examine robustness to a suite of these types of visual changes for RGB-D and point cloud based visual control policies. To perform these experiments on both model-free and model-based reinforcement learners, we introduce a novel Point Cloud World Model (PCWM) and point cloud based control policies. Our experiments show that policies that explicitly encode point clouds are significantly more robust than their RGB-D counterparts. Further, we find our proposed PCWM significantly outperforms prior works in terms of sample efficiency during training. Taken together, these results suggest reasoning about the 3D scene through point clouds can improve performance, reduce learning time, and increase robustness for robotic learners. Code: https://github.com/pvskand/pcwm},
  archive   = {C_ICRA},
  author    = {Skand Peri and Iain Lee and Chanho Kim and Li Fuxin and Tucker Hermans and Stefan Lee},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610710},
  month     = {5},
  pages     = {17529-17536},
  title     = {Point cloud models improve visual robustness in robotic learners},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). L-DYNO: Framework to learn consistent visual features using
robot’s motion. <em>ICRA</em>, 17522–17528. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Historically, feature-based approaches have been used extensively for camera-based robot perception tasks such as localization, mapping, tracking, and others. Several of these approaches also combine other sensors (inertial sensing, for example) to perform combined state estimation. Our work rethinks this approach; we present a representation learning mechanism that identifies visual features that best correspond to robot motion as estimated by an external signal. Specifically, we utilize the robot’s transformations through an external signal (inertial sensing, for example) and give attention to image space that is most consistent with the external signal. We use a pairwise consistency metric as a representation to keep the visual features consistent through a sequence with the robot’s relative pose transformations. This approach enables us to incorporate information from the robot’s perspective instead of solely relying on the image attributes. We evaluate our approach on real-world datasets such as KITTI &amp; EuRoC and compare the refined features with existing feature descriptors. We also evaluate our method using our real robot experiment. We notice an average of 49% reduction in the image search space without compromising the trajectory estimation accuracy. Our method reduces the execution time of visual odometry by 4.3% and also reduces reprojection errors. We demonstrate the need to select only the most important features and show the competitiveness using various feature detection baselines.},
  archive   = {C_ICRA},
  author    = {Kartikeya Singh and Charuvaran Adhivarahan and Karthik Dantu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611097},
  month     = {5},
  pages     = {17522-17528},
  title     = {L-DYNO: Framework to learn consistent visual features using robot’s motion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). What do we learn from a large-scale study of pre-trained
visual representations in sim and real environments? <em>ICRA</em>,
17515–17521. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a large empirical investigation on the use of pre-trained visual representations (PVRs) for training downstream policies that execute real-world tasks. Our study involves five different PVRs, each trained for five distinct manipulation or indoor navigation tasks. We performed this evaluation using three different robots and two different policy learning paradigms. From this e ort, we can arrive at three insights: 1) the performance trends of PVRs in the simulation are generally indicative of their trends in the real world, 2) the use of PVRs enables a first-of-its-kind result with indoor ImageNav (zero-shot transfer to a held-out scene in the real world), and 3) the benefits from variations in PVRs, primarily data-augmentation and fine-tuning, also transfer to the real-world performance. See project website 1 for additional details and visuals.},
  archive   = {C_ICRA},
  author    = {Sneha Silwal and Karmesh Yadav and Tingfan Wu and Jay Vakil and Arjun Majumdar and Sergio Arnaud and Claire Chen and Vincent-Pierre Berges and Dhruv Batra and Aravind Rajeswaran and Mrinal Kalakrishnan and Franziska Meier and Oleksandr Maksymets},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610218},
  month     = {5},
  pages     = {17515-17521},
  title     = {What do we learn from a large-scale study of pre-trained visual representations in sim and real environments?},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SlotGNN: Unsupervised discovery of multi-object
representations and visual dynamics. <em>ICRA</em>, 17508–17514. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning multi-object dynamics from visual data using unsupervised techniques is challenging due to the need for robust, object representations that can be learned through robot interactions. This paper presents a novel framework with two new architectures: SlotTransport for discovering object representations from RGB images and SlotGNN for predicting their collective dynamics from RGB images and robot interactions. Our SlotTransport architecture is based on slot attention for unsupervised object discovery and uses a feature transport mechanism to maintain temporal alignment in object-centric representations. This enables the discovery of slots that consistently reflect the composition of multi-object scenes. These slots robustly bind to distinct objects, even under heavy occlusion or absence. Our SlotGNN, a novel unsupervised graph-based dynamics model, predicts the future state of multi-object scenes. SlotGNN learns a graph representation of the scene using the discovered slots from SlotTransport and performs relational and spatial reasoning to predict the future appearance of each slot conditioned on robot actions. We demonstrate the effectiveness of SlotTransport in learning object-centric features that accurately encode both visual and positional information. Further, we highlight the accuracy of SlotGNN in downstream robotic tasks, including challenging multi-object rearrangement and long-horizon prediction. Finally, our unsupervised approach proves effective in the real world. With only minimal additional data, our framework robustly predicts slots and their corresponding dynamics in real-world control tasks. Our project webpage: bit.ly/slotgnn.},
  archive   = {C_ICRA},
  author    = {Alireza Rezazadeh and Athreyi Badithela and Karthik Desingh and Changhyun Choi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611588},
  month     = {5},
  pages     = {17508-17514},
  title     = {SlotGNN: Unsupervised discovery of multi-object representations and visual dynamics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CITR: A coordinate-invariant task representation for robotic
manipulation. <em>ICRA</em>, 17501–17507. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The basis for robotics skill learning is an adequate representation of manipulation tasks based on their physical properties. As manipulation tasks are inherently invariant to the choice of reference frame, an ideal task representation would also exhibit this property. Nevertheless, most robotic learning approaches use unprocessed, coordinate-dependent robot state data for learning new skills, thus inducing challenges regarding the interpretability and transferability of the learned models.In this paper, we propose a transformation from spatial measurements to a coordinate-invariant feature space, based on the pairwise inner product of the input measurements. We describe and mathematically deduce the concept, establish the task fingerprints as an intuitive image-based representation, experimentally collect task fingerprints, and demonstrate the usage of the representation for task classification. This representation motivates further research on data-efficient and transferable learning methods for online manipulation task classification and task-level perception.},
  archive   = {C_ICRA},
  author    = {Peter So and Rafael I. Cabral Muchacho and Robin Jeanne Kirschner and Abdalla Swikir and Luis Figueredo and Fares J. Abu-Dakka and Sami Haddadin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611312},
  month     = {5},
  pages     = {17501-17507},
  title     = {CITR: A coordinate-invariant task representation for robotic manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Pneumatic back exoskeleton for lifting posture detection
and correction. <em>ICRA</em>, 17494–17500. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Low back pain is a widespread issue that affects people worldwide and can lead to serious conditions such as herniated discs, spinal stenosis, or lumbar radiculopathy. Improper posture while lifting heavy weights is a common cause of back pain, especially among laborers. However, current back exoskeletons are often bulky and require electric motors, making them challenging to use and consuming significant power. Some passive exoskeletons don’t require power, but their fixed stiffness constrains normal motion. This paper presents a novel solution: a pneumatic back exoskeleton made of structured fabrics that can adjust stiffness under various air pressures. Additionally, it includes IMU sensors to detect lifting posture and correct it in real time. The exoskeleton’s effectiveness was tested through lifting experiments, demonstrating that it significantly corrects lifting posture, reduces stress on the lumbar spine, and mitigates back muscle stress. This pneumatic back exoskeleton offers a promising solution to prevent low back pain during weight-lifting tasks and provides guidance for future back exoskeleton designs.},
  archive   = {C_ICRA},
  author    = {Yu Chen and Minda Wang and Yifan Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611702},
  month     = {5},
  pages     = {17494-17500},
  title     = {Pneumatic back exoskeleton for lifting posture detection and correction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel funnel-based l1 adaptive fuzzy approach for the
control of an actuated ankle foot orthosis. <em>ICRA</em>, 17487–17493.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a novel funnel-based adaptive ${{\mathcal{L}}_1}$ fuzzy control strategy for assisting ankle joint movement during walking with the use of an actuated ankle foot orthosis (AAFO). A projection-based adaptation mechanism employing a fuzzy system is used to estimate the unknown time-varying parameters of the ${{\mathcal{L}}_1}$ control law, ensuring precise tracking of the AAFO-wearer system by the state estimator. The projection operator guarantees the convergence of the parameters while offering a limited amount of assistance torque. Funnel-based feedback control is used to mitigate the typical time lag seen when using ${{\mathcal{L}}_1}$-based approaches due to the presence of a low-pass filter commonly used in this type of approach. The effectiveness of the proposed control strategy is demonstrated through real-time experiments involving five healthy subjects.},
  archive   = {C_ICRA},
  author    = {Oussama Bey and Rami Jradi and Huiseok Moon and Hala Rifaï and Kaushik Das Sharma and Yacine Amirat and Samer Mohammed},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610584},
  month     = {5},
  pages     = {17487-17493},
  title     = {A novel funnel-based l1 adaptive fuzzy approach for the control of an actuated ankle foot orthosis},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design of a knee-joint exoskeleton to reduce misalignment in
both the sagittal and coronal planes. <em>ICRA</em>, 17472–17478. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many individuals experience knee dysfunctions attributed to the natural aging process and degenerative conditions. To aid individuals in regaining knee functionality, supportive exoskeletons were designed to be affixed to both the shin and thigh. However, a common issue encountered in knee exoskeletons involves the misalignment of joints between the exoskeleton and the user, resulting in discomfort and potential injuries. To reduce misalignment with the knee joint, it is essential for the thigh and shin harnesses of the exoskeleton to replicate the natural trajectories of the knee. However, achieving this is a complex task due to the shifting center of rotation of the knee in both the Sagittal and Coronal planes. Previous knee exoskeletons primarily focus on aligning the joint in the Sagittal plane, neglecting alignment in the other dimension due to inherent design constraints. For the first time, this study introduces a knee-joint exoskeleton capable of conforming to the natural movement of the knee in both the Sagittal and Coronal planes, with the aim of minimizing joint misalignment without the use of inherently soft materials. A spherical scissor linkage mechanism (SSLM) was utilized in conjunction with a customized guide rail to adjust the center of rotation of the SSLM. This configuration facilitates knee flexion/extension while accommodating the knee joint’s center of rotation in both the Sagittal and Coronal planes. The experimental outcomes demonstrated a substantial reduction in misalignment with the knee when compared to a commercial knee-support brace with a one-degree-of-freedom revolute joint.},
  archive   = {C_ICRA},
  author    = {Shubhranil Sengupta and Jee-Hwan Ryu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610400},
  month     = {5},
  pages     = {17472-17478},
  title     = {Design of a knee-joint exoskeleton to reduce misalignment in both the sagittal and coronal planes},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Human-exoskeleton locomotion interaction experience
transfer: Speeding up and improving the performance of preference-based
optimizations of exoskeleton assistance during walking. <em>ICRA</em>,
17465–17471. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Preference-based optimizing methods have shown their advantages and potential in exploring individual, comfortable, and effective control strategies and assistance parameters of exoskeletons during locomotion. Research indicates that compared with naive wearers, knowledgeable wearers with abundant exoskeleton assistance experience have obvious advantages in speeding up the parameters exploration process and improving the assistant performance. However, there is no existing method that could utilize the human-exoskeleton locomotion interaction experience (HELIE) to assist naive wearers during the exploration process. In this work, we propose a novel preference-based human-exoskeleton locomotion interaction experience transfer (LIET) framework, which could speed up the exploration of human-preferred parameters and acquire more satisfying results for naive wearers via the HELIE acquired from knowledgeable wearers. In addition, based on the proposed LIET framework, we establish the mathematical expression of the HELIE transfer during exoskeleton assistance. This will promote the research that concerns utilizing HELIE for exoskeleton control parameters optimizations in the future. Finally, experiments demonstrate the proposed LIET framework could speed up the exploration process and acquire more satisfying optimized results for naive wearers.},
  archive   = {C_ICRA},
  author    = {Hongwu Li and Junchen Liu and Ziqi Wang and Haotian Ju and Tianjiao Zheng and Yongsheng Gao and Jie Zhao and Yanhe Zhu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611497},
  month     = {5},
  pages     = {17465-17471},
  title     = {Human-exoskeleton locomotion interaction experience transfer: Speeding up and improving the performance of preference-based optimizations of exoskeleton assistance during walking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leaf-inspired FSR array and insole-type sensor module for
mobile three-dimensional ground reaction force estimation.
<em>ICRA</em>, 17459–17464. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an insole-type sensor module with a novel leaf-inspired force-sensitive resistor (FSR) array for accurate three-dimensional ground reaction force (GRF) estimation during human’s various motions. Joint torque analysis, essential for numerous applications in biomechanics and wearable robotics, necessitates the measurement of three-dimensional GRF vector information, traditionally achieved in indoor environments using costly force plates. To overcome these limitations, this study proposes an alternative method by incorporating FSRs on three inclined planes within the insole. A vector scaling process transforms the force values from the FSRs into the three-dimensional force vector, enabling continuous and user-independent estimation of GRF. The sensor module is integrated with machine learning, demonstrating its accuracy and usability in various motion scenarios. The results confirm the effectiveness of the leaf-inspired FSR array, giving the possibilities for portable and cost-effective motion analysis systems.},
  archive   = {C_ICRA},
  author    = {Taeyeon Kim and Eunseok Song and Seongbin An and Hyunjin Choi and Kyoungchul Kong},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610058},
  month     = {5},
  pages     = {17459-17464},
  title     = {Leaf-inspired FSR array and insole-type sensor module for mobile three-dimensional ground reaction force estimation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward mass customization of a robot’s morphology design for
improving area coverage. <em>ICRA</em>, 17452–17458. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Floor cleaning robots have been developed to cater to building maintenance needs. Complete area coverage is crucial for a floor cleaning robot, and its morphology design plays a vital role in realizing complete area coverage. However, floor cleaning robots with fixed morphologies have difficulty in achieving a high area coverage performance. Mass customization of a robot’s morphology would improve its productivity in terms of area coverage. This paper proposes a novel system that can be used for mass customizing the morphology of a robot to improve area coverage performance in an environment of interest. The customized morphology is determined through an optimization technique by considering an environment of interest and design constraints. The area coverage of a candidate morphology design is evaluated by simulating the robot navigation in an environment of interest. Generalized pattern search, particle swarm optimization, and surrogate optimization are independently considered optimization techniques. Experiments have been conducted considering the cases of robot deployments. The statistical conclusions on experimental results validate that the proposed system can synthesize a morphology that significantly improves the area coverage performance in an environment of interest.},
  archive   = {C_ICRA},
  author    = {M. A. Viraj J. Muthugala and S. M. Bhagya P. Samarakoon and Raihan E. Abdulkader and Mohan Rajesh Elara},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610203},
  month     = {5},
  pages     = {17452-17458},
  title     = {Toward mass customization of a robot’s morphology design for improving area coverage},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An open and flexible robot perception framework for mobile
manipulation tasks. <em>ICRA</em>, 17445–17451. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Over the last years, powerful methods for solving specific perception problems such as object detection, pose estimation or scene understanding have been developed. While performing mobile manipulation actions, a robot’s perception framework needs to execute a series of these methods in a specific sequence each time it receives a new perception task. Generating proficient combinations of vision methods to solve individual perception tasks remains a challenge, as the combination depends on the requirements of the task and the capabilities of the robot’s hardware.In this paper, we propose RoboKudo, an open-source knowledge-enabled perception framework that leverages the strengths of the Unstructured Information Management (UIM) principle and the flexibility of Behavior Trees to model task-specific perception processes. The framework can combine state-of-the-art computer vision methods to satisfy the requirements of each perception task and scales to different robot platforms. The generality and effectiveness of the framework are evaluated in real world experiments where it solves various perception tasks in the context of mobile manipulation actions in a household domain. Code and additional material are available at https://robokudo.ai.uni-bremen.de/rkop.},
  archive   = {C_ICRA},
  author    = {Patrick Mania and Simon Stelter and Gayane Kazhoyan and Michael Beetz},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610743},
  month     = {5},
  pages     = {17445-17451},
  title     = {An open and flexible robot perception framework for mobile manipulation tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LeagTag: An elongated high-accuracy fiducial marker for
tight spaces *. <em>ICRA</em>, 17438–17444. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fiducial markers enable reliable service robot control. In human-robot coexistence environments, efficient placement of square or circular markers can be challenging due to limited space. In this study, we developed a world-first, elongated fiducial marker, capable of high-accuracy 6-DoF measurements, designed to be installable in tight spaces. We introduced two types of lenticular angle gauges to enhance pose estimation and developed new marker patterns and measurement algorithms to maintain recognition distance and accuracy. The proposed marker achieved a measurement accuracy of 0.1% position error and 0.5° orientation error. This technology will enhance the practicality and applicability of fiducial markers, contributing to the creation of robot-friendly space for future service robots.},
  archive   = {C_ICRA},
  author    = {Hideyuki Tanaka and Kunihiro Ogata},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611391},
  month     = {5},
  pages     = {17438-17444},
  title     = {LeagTag: An elongated high-accuracy fiducial marker for tight spaces *},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design, modeling and analysis of a spherical parallel
continuum manipulator for nursing robots. <em>ICRA</em>, 17431–17437.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the healthcare industry, nursing robots have made great contributions, assisting in the delivery of food and medicine as well as the movement and transfer of patients. However, the traditional continuum manipulator often has the problems of limited workspace and weak carrying capacity. Compared with traditional manipulator, the continuum manipulator has the advantages of a small moment of inertia and high dexterity. This paper proposes a original cable-driven parallel continuum manipulator with a spherical parallel mechanism as the continuous segments. Due to the spherical parallel mechanisms’ characteristics, the proposed cable-driven spherical parallel continuum manipulator offers many inherent advantages for nursing robots. The prototype is tested and analyzed, and the kinematics and statics are verified. The results show that the cable-driven spherical parallel continuum manipulator for nursing robots has low requirements for workspace, suitable for complex spaces and can have a large carrying capacity.},
  archive   = {C_ICRA},
  author    = {Zhenhua Gong and Chuanxin Ning and Jiejunyi Liang and Ting Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610046},
  month     = {5},
  pages     = {17431-17437},
  title     = {Design, modeling and analysis of a spherical parallel continuum manipulator for nursing robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ARIS 1.0: An autonomous multitasking medical service robot
for hospital environments. <em>ICRA</em>, 17424–17430. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Introducing robotics in the healthcare sector revolutionizes medical services by providing advanced treatments, medication management, and robotic assistance while overcoming resource limitations. In the current healthcare domain, an intermediate robotic communication platform is essential for distributing equal medical services, facilitating remote consultations, and maintaining the integrity of medical education, especially in rural areas and during pandemics. This work introduces ARIS, a multitasking medical service robot designed for telemedicine aspects and to facilitate remote medical education activities such as ward rounds. The prototype called ARIS 1.0 was developed, including a three-wheeled omnidirectional mobile platform, a torso and a novel movable neck mechanism with a face. The prototype robot can generate an online summarized report using its integrated language interaction and IoT-based vital sign extraction modules. The ROS-based semi-autonomous navigation facilitates the robot to be an assistive agent, allowing it to either accompany doctors or visit patients individually. Ultimately, ARIS 1.0 serves telepresence and novel regional language capabilities, specifically Sinhala-based self-communication features. This enables inter-party communication among doctors, medical students, and patients. The functionalities of ARIS 1.0 were validated in an emulated indoor environment to evaluate their feasibility. The results indicate that ARIS 1.0 is feasible for providing remote medical services. Furthermore, the paper discusses several promising research directions related to the proposed concept.},
  archive   = {C_ICRA},
  author    = {D. M. A. P. Dunuwila and W. M. L. N. Gunawardhana and M. D. W. H. Basnayake and Y. W. R. Amarasinghe and A. G. B. P. Jayasekara and H. A. G. C. Premachandra and H. Tamura and U-Xuan Tan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610423},
  month     = {5},
  pages     = {17424-17430},
  title     = {ARIS 1.0: An autonomous multitasking medical service robot for hospital environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adapting for calibration disturbances: A neural uncalibrated
visual servoing policy. <em>ICRA</em>, 17417–17423. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual servoing (VS) is a widely used technique in industries where there are hundreds of robots, but it requires accurate camera calibration including camera intrinsic and extrinsic parameters. However, it is labour-intensive to calibrate robots one-by-one in practical use. In this paper, we propose a neural uncalibrated VS policy (NUVS) that can adapt to calibration disturbances with an adaption mechanism and a control-oriented guidance. It bridges the disturbance adaption of classical VS methods and the large convergence of learning-based VS methods. NUVS estimates the calibration embedding from past observations and servos to the desired pose under the supervision of a PBVS that can access the ground truth in simulation. With this adaption mechanism, NUVS outperforms the classical IBUVS algorithm when facing large initial camera pose offsets under the calibration disturbance. Supplementary material in: https://sites.google.com/view/neural-uncalibrated-vs},
  archive   = {C_ICRA},
  author    = {Hongxiang Yu and Anzhe Chen and Kechun Xu and Dashun Guo and Yufei Wei and Zhongxiang Zhou and Xuebo Zhang and Yue Wang and Rong Xiong},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610364},
  month     = {5},
  pages     = {17417-17423},
  title     = {Adapting for calibration disturbances: A neural uncalibrated visual servoing policy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CNS: Correspondence encoded neural image servo policy.
<em>ICRA</em>, 17410–17416. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image servo is an indispensable technique in robotic applications that helps to achieve high precision positioning. The intermediate representation of image servo policy is important to sensor input abstraction and policy output guidance. Classical approaches achieve high precision but require clean keypoint correspondence, and suffer from limited convergence basin or weak feature error robustness. Recent learning-based methods achieve moderate precision and large convergence basin on specific scenes but face issues when generalizing to novel environments. In this paper, we encode keypoints and correspondence into a graph and use graph neural network as architecture of controller. This design utilizes both advantages: generalizable intermediate representation from keypoint correspondence and strong modeling ability from neural network. Other techniques including realistic data generation, feature clustering and distance decoupling are proposed to further improve efficiency, precision and generalization. Experiments in simulation and real-world verify the effectiveness of our method in speed (maximum 40fps along with observer), precision (&lt;0.3° and sub-millimeter accuracy) and generalization (sim-to-real without fine-tuning). Project homepage (full paper with supplementary text, video and code): https://hhcaz.github.io/CNS-home.},
  archive   = {C_ICRA},
  author    = {Anzhe Chen and Hongxiang Yu and Yue Wang and Rong Xiong},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611185},
  month     = {5},
  pages     = {17410-17416},
  title     = {CNS: Correspondence encoded neural image servo policy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Autonomous quilt spreading for caregiving robots.
<em>ICRA</em>, 17403–17409. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we propose a novel strategy to ensure infants, who inadvertently displace their quilts during sleep, are promptly and accurately re-covered. Our approach is formulated into two subsequent steps: interference resolution and quilt spreading. By leveraging the DWPose human skeletal detection and the Segment Anything instance segmentation models, the proposed method can accurately recognize the states of the infant and the quilt over her, which involves addressing the interferences resulted from an infant’s limbs laid on part of the quilt. Building upon prior research, the EM*D deep learning model is employed to forecast quilt state transitions before and after quilt spreading actions. To improve the sensitivity of the network in distinguishing state variation of the handled quilt, we introduce an enhanced loss function that translates the voxelized quilt state into a more representative one. Both simulation and real-world experiments validate the efficacy of our method, in spreading and recover a quilt over an infant.},
  archive   = {C_ICRA},
  author    = {Yuchun Guo and Zhiqing Lu and Yanling Zhou and Xin Jiang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610898},
  month     = {5},
  pages     = {17403-17409},
  title     = {Autonomous quilt spreading for caregiving robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-recovery prompting: Promptable general purpose service
robot system with foundation models and self-recovery. <em>ICRA</em>,
17395–17402. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A general-purpose service robot (GPSR), which can execute diverse tasks in various environments, requires a system with high generalizability and adaptability to tasks and environments. In this paper, we first developed a top-level GPSR system for worldwide competition (RoboCup@Home2023) based on multiple foundation models. This system is both generalizable to variations and adaptive by prompting each model. Then, by analyzing the performance of the developed system, we found three types of failure in more realistic GPSR application settings: insufficient information, incorrect plan generation, and plan execution failure. We then propose the self-recovery prompting pipeline, which explores the necessary information and modifies its prompts to recover from failure. We experimentally confirm that the system with the self-recovery mechanism can accomplish tasks by resolving various failure cases. https://sites.google.com/view/srgpsr},
  archive   = {C_ICRA},
  author    = {Mimo Shirasaka and Tatsuya Matsushima and Soshi Tsunashima and Yuya Ikeda and Aoi Horo and So Ikoma and Chikaha Tsuji and Hikaru Wada and Tsunekazu Omija and Dai Komukai and Yutaka Matsuo and Yusuke Iwasawa},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611640},
  month     = {5},
  pages     = {17395-17402},
  title     = {Self-recovery prompting: Promptable general purpose service robot system with foundation models and self-recovery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Personality- and memory-based software framework for
human-robot interaction. <em>ICRA</em>, 17388–17394. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The synergic orchestration of the cognitive and psychological dimensions characterizes human intelligence. Accordingly, carefully designing this mechanism in artificial intelligence can be a successful strategy to increase human likeness in a robot, enhancing mutual understanding and building a more natural and intuitive interaction. For this purpose, the main contribution of this work is a psychological and cognitive architecture tailored for HRI based on the interplay between robotic personality and memory-based cognitive processes. Indeed, the artificial personality manifests itself not only in various aspects of the behavior but also within the action selection process, which is closely intertwined with personality-dependent hedonic experiences linked to memories. Within this paper, we propose a task- and platform-independent framework, evaluated in a multiparty collaborative scenario. Obtained results show that a robot connected to our proposed framework is perceived as a cognitive agent capable of manifesting perceivable and distinguishable personality traits.},
  archive   = {C_ICRA},
  author    = {Alice Nardelli and Antonio Sgorbissa and Carmine Tommaso Recchiuto},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611168},
  month     = {5},
  pages     = {17388-17394},
  title     = {Personality- and memory-based software framework for human-robot interaction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discuss before moving: Visual language navigation via
multi-expert discussions. <em>ICRA</em>, 17380–17387. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual language navigation (VLN) is an embodied task demanding a wide range of skills encompassing understanding, perception, and planning. For such a multifaceted challenge, previous VLN methods totally rely on one model’s own thinking to make predictions within one round. However, existing models, even the most advanced large language model GPT4, still struggle with dealing with multiple tasks by single-round self-thinking. In this work, drawing inspiration from the expert consultation meeting, we introduce a novel zero-shot VLN framework. Within this framework, large models possessing distinct abilities are served as domain experts. Our proposed navigation agent, namely DiscussNav, can actively discuss with these experts to collect essential information before moving at every step. These discussions cover critical navigation subtasks like instruction understanding, environment perception, and completion estimation. Through comprehensive experiments, we demonstrate that discussions with domain experts can effectively facilitate navigation by perceiving instruction-relevant information, correcting inadvertent errors, and sifting through in-consistent movement decisions. The performances on the representative VLN task R2R show that our method surpasses the leading zero-shot VLN model by a large margin on all metrics. Additionally, real-robot experiments display the obvious advantages of our method over single-round self-thinking. Our project web can be seen at the https://sites.google.com/view/discussnav.},
  archive   = {C_ICRA},
  author    = {Yuxing Long and Xiaoqi Li and Wenzhe Cai and Hao Dong},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611565},
  month     = {5},
  pages     = {17380-17387},
  title     = {Discuss before moving: Visual language navigation via multi-expert discussions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A time-optimal energy planner for safe human-robot
collaboration. <em>ICRA</em>, 17373–17379. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The human-robot collaboration scenarios are characterized by the presence of human operators and robots that work in close contact with each other. As a consequence, the safety regulations have been updated in order to provide guidelines on how to asses safety in these new scenarios. In particular, Power and Force Limiting (PFL) collaborative mode describes how the energy should be regulated during the collaboration. Based on these guidelines, we propose a new optimal trajectory planner which, by exploiting the variability of the robot’s inertia as a function of its configuration, is able to return trajectories that can be travelled at greater speed and in less time, while guaranteeing the safety limits according to the standard. The proposed planner was validated first in simulation, comparing completion times with other state-of-the-art planning algorithms, and then experimentally, demonstrating the performance of the planned trajectories during physical interaction with the environment. Both validations confirm the effectiveness of the proposed planner, which returns shorter completion times while ensuring safe interaction.},
  archive   = {C_ICRA},
  author    = {Andrea Pupa and Marco Minelli and Cristian Secchi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611118},
  month     = {5},
  pages     = {17373-17379},
  title     = {A time-optimal energy planner for safe human-robot collaboration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient ISO/TS 15066 compliance through model predictive
control. <em>ICRA</em>, 17358–17364. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the actual industrial scenarios, human operators and robots work together sharing the workspace. Such proximity requires special attention in ensuring safety for the human operator, which is often translated in collision avoidance behaviour or high speed reduction. Adhering safety however is not the only aspect that must be taken into account. For many tasks, such as welding, it is crucial to ensure that the robot performs exactly the planned path. To optimize robot performance while complying with safety regulations, this work introduces a novel optimal nonlinear control problem. It prioritizes path preservation, exploiting redundancy to minimize task execution time, while explicitly adhering to the constraints imposed by ISO/TS 15066. To achieve high-performance outcomes, the control problem is addressed using the Model Predictive Control (MPC) approach. The proposed strategy has been experimentally validated in both simulations and a real-world industrial task involving a Kuka LWR4+ robot.},
  archive   = {C_ICRA},
  author    = {Andrea Pupa and Cristian Secchi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610039},
  month     = {5},
  pages     = {17358-17364},
  title     = {Efficient ISO/TS 15066 compliance through model predictive control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-agent strategy explanations for human-robot
collaboration. <em>ICRA</em>, 17351–17357. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As robots are deployed in human spaces, it is important that they are able to coordinate their actions with the people around them. Part of such coordination involves ensuring that people have a good understanding of how a robot will act in the environment. This can be achieved through explanations of the robot’s policy. Much prior work in explainable AI and RL focuses on generating explanations for single-agent policies, but little has been explored in generating explanations for collaborative policies. In this work, we investigate how to generate multi-agent strategy explanations for human-robot collaboration. We formulate the problem using a generic multi-agent planner, show how to generate visual explanations through strategy-conditioned landmark states and generate textual explanations by giving the landmarks to an LLM. Through a user study, we find that when presented with explanations from our proposed framework, users are able to better explore the full space of strategies and collaborate more efficiently with new robot partners.},
  archive   = {C_ICRA},
  author    = {Ravi Pandya and Michelle Zhao and Changliu Liu and Reid Simmons and Henny Admoni},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610720},
  month     = {5},
  pages     = {17351-17357},
  title     = {Multi-agent strategy explanations for human-robot collaboration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PlanCollabNL: Leveraging large language models for adaptive
plan generation in human-robot collaboration. <em>ICRA</em>,
17344–17350. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {&quot;Hey, robot. Let’s tidy up the kitchen. By the way, I have back pain today&quot;. How can a robotic system devise a shared plan with an appropriate task allocation from this abstract goal and agent condition? Classical AI task planning has been explored for this purpose, but it involves a tedious definition of an inflexible planning problem. Large Language Models (LLMs) have shown promising generalisation capabilities in robotics decision-making through knowledge extraction from Natural Language (NL). However, the translation of NL information into constrained robotics domains remains a challenge. In this paper, we use LLMs as translators between NL information and a structured AI task planning problem, targeting human-robot collaborative plans. The LLM generates information that is encoded in the planning problem, including specific subgoals derived from an NL abstract goal, as well as recommendations for subgoal allocation based on NL agent conditions. The framework, PlanCollabNL, is evaluated for a number of goals and agent conditions, and the results show that correct and executable plans are found in most cases. With this framework, we intend to add flexibility and generalisation to HRC plan generation, eliminating the need for a manual and laborious definition of restricted planning problems and agent models.},
  archive   = {C_ICRA},
  author    = {Silvia Izquierdo-Badiola and Gerard Canal and Carlos Rizzo and Guillem Alenyà},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610055},
  month     = {5},
  pages     = {17344-17350},
  title     = {PlanCollabNL: Leveraging large language models for adaptive plan generation in human-robot collaboration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust and dexterous dual-arm tele-cooperation using
adaptable impedance control. <em>ICRA</em>, 17337–17343. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, the need for robots to transition from isolated industrial tasks to shared environments, including human-robot collaboration and teleoperation, has become increasingly evident. Building on the foundation of Fractal Impedance Control (FIC) introduced in our previous work, this paper presents a novel extension to dualarm tele-cooperation, leveraging the non-linear stiffness and passivity of FIC to adapt to diverse cooperative scenarios. Unlike traditional impedance controllers, our approach ensures stability without relying on energy tanks, as demonstrated in our prior research. In this paper, we further extend the FIC framework to bimanual operations, allowing for stable and smooth switching between different dynamic tasks without gain tuning. We also introduce a telemanipulation architecture that offers higher transparency and dexterity, addressing the challenges of signal latency and low-bandwidth communication. Through extensive experiments, we validate the robustness of our method and the results confirm the advantages of the FIC approach over traditional impedance controllers, showcasing its potential for applications in planetary exploration and other scenarios requiring dexterous telemanipulation. This paper’s contributions include the seamless integration of FIC into multi-arm systems, the ability to perform robust interactions in highly variable environments, and the provision of a comprehensive comparison with competing approaches, thereby significantly enhancing the robustness and adaptability of robotic systems.},
  archive   = {C_ICRA},
  author    = {Keyhan Kouhkiloui Babarahmati and Mohammadreza Kasaei and Carlo Tiseo and Michael Mistry and Sethu Vijayakumar},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610410},
  month     = {5},
  pages     = {17337-17343},
  title     = {Robust and dexterous dual-arm tele-cooperation using adaptable impedance control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Physics-informed neural networks for continuum robots:
Towards fast approximation of static cosserat rod theory. <em>ICRA</em>,
17293–17299. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sophisticated models can accurately describe deformations of continuum robots while being computationally demanding, which limits their application. Especially when considering sampling-based path planning, the model has to be evaluated frequently, which can lead to substantially increased computation times. We present a new approach to compute the entire shape of a tendon-driven continuum robot by a physics-informed neural network (PINN). The underlying physics is modelled with the Cosserat rod theory and incorporated into the PINN’s loss function. The boundary values for the training are obtained from a reference model, solved by the shooting method. Our approach allows for a computation of the learned Cosserat rod model multiple orders of magnitude faster than a publicly available reference model. The median position deviation from the reference model lies below 1mm (0.5% of the simulated robot length) for each of the robot’s 20 disks.},
  archive   = {C_ICRA},
  author    = {Martin Bensch and Tim-David Job and Tim-Lukas Habich and Thomas Seel and Moritz Schappler},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610742},
  month     = {5},
  pages     = {17293-17299},
  title     = {Physics-informed neural networks for continuum robots: Towards fast approximation of static cosserat rod theory},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Safe deep policy adaptation. <em>ICRA</em>, 17286–17292. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A critical goal of autonomy and artificial intelligence is enabling autonomous robots to rapidly adapt in dynamic and uncertain environments. Classic adaptive control and safe control provide stability and safety guarantees but are limited to specific system classes. In contrast, policy adaptation based on reinforcement learning (RL) offers versatility and generalizability but presents safety and robustness challenges. We propose SafeDPA, a novel RL and control framework that simultaneously tackles the problems of policy adaptation and safe reinforcement learning. SafeDPA jointly learns adaptive policy and dynamics models in simulation, predicts environment configurations, and fine-tunes dynamics models with few-shot real-world data. A safety filter based on the Control Barrier Function (CBF) on top of the RL policy is introduced to ensure safety during real-world deployment. We provide theoretical safety guarantees of SafeDPA and show the robustness of SafeDPA against learning errors and extra perturbations. Comprehensive experiments on (1) classic control problems (Inverted Pendulum), (2) simulation benchmarks (Safety Gym), and (3) a real-world agile robotics platform (RC Car) demonstrate great superiority of SafeDPA in both safety and task performance, over state-of-the-art baselines. Particularly, SafeDPA demonstrates notable generalizability, achieving a 300% increase in safety rate compared to the baselines, under unseen disturbances in real-world experiments.},
  archive   = {C_ICRA},
  author    = {Wenli Xiao and Tairan He and John Dolan and Guanya Shi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611340},
  month     = {5},
  pages     = {17286-17292},
  title     = {Safe deep policy adaptation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SPCGC: Scalable point cloud geometry compression for machine
vision. <em>ICRA</em>, 17272–17278. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the proliferation of sensor devices, the extensive utilization of three-dimensional data in multimedia continues to grow. Point clouds are widely adopted within this domain because they are one of the most intuitive representations of three-dimensional data. However, the substantial volume of point cloud data poses significant challenges for storage and transmission. Moreover, a considerable portion of the data loses its semantic information during transmission. Consequently, how can we ensure both the perceptual quality for the human and the performance of downstream tasks during the transmission? To address this issue, we propose a scalable point cloud geometry compression framework (SPCGC) for machine perception. This framework tackles the fidelity issues associated with point cloud compression and preserves more semantic information, enhancing the performance of machine vision tasks. Our solution consists of a base layer bitstream and an enhancement layer bitstream. The base layer bitstream contains geometry data, while the enhancement layer bitstream utilizes semantic-guided residual data. Additionally, we introduce two modules for extracting and coding residual features. And incorporate classification and segmentation losses from downstream tasks into the Rate-Distortion (RD) optimization. Our approach outperforms existing learning-based lossy point cloud coding methods through empirical validation in downstream tasks without sacrificing point cloud compression performance.},
  archive   = {C_ICRA},
  author    = {Liang Xie and Wei Gao and Huiming Zheng and Ge Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610894},
  month     = {5},
  pages     = {17272-17278},
  title     = {SPCGC: Scalable point cloud geometry compression for machine vision},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robot interaction behavior generation based on social motion
forecasting for human-robot interaction. <em>ICRA</em>, 17264–17271. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Integrating robots into populated environments is a complex challenge that requires an understanding of human social dynamics. In this work, we propose to model social motion forecasting in a shared human-robot representation space, which facilitates us to synthesize robot motions that interact with humans in social scenarios despite not observing any robot in the motion training. We develop a transformer-based architecture called ECHO, which operates in the aforementioned shared space to predict the future motions of the agents encountered in social scenarios. Contrary to prior works, we reformulate the social motion problem as the refinement of the predicted individual motions based on the surrounding agents, which facilitates the training while allowing for single-motion forecasting when only one human is in the scene. We evaluate our model in multi-person and human-robot motion forecasting tasks and obtain state-of-the-art performance by a large margin while being efficient and performing in real-time. Additionally, our qualitative results showcase the effectiveness of our approach in generating human-robot interaction behaviors that can be controlled via text commands.},
  archive   = {C_ICRA},
  author    = {Esteve Valls Mascaro and Yashuai Yan and Dongheui Lee},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610682},
  month     = {5},
  pages     = {17264-17271},
  title     = {Robot interaction behavior generation based on social motion forecasting for human-robot interaction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning a stable dynamic system with a lyapunov energy
function for demonstratives using neural networks. <em>ICRA</em>,
17223–17228. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous Dynamic System (DS)-based algorithms hold a pivotal and foundational role in the field of Learning from Demonstration (LfD). Nevertheless, they confront the formidable challenge of striking a delicate balance between achieving precision in learning and ensuring the overall stability of the system. In response to this substantial challenge, this paper introduces a novel DS algorithm rooted in neural network technology. This algorithm not only possesses the capability to extract critical insights from demonstration data but also demonstrates the capacity to learn a candidate Lyapunov energy function that is consistent with the provided demonstrations. The model presented in this paper employs a simplistic neural network architecture that excels in fulfilling a dual objective: optimizing accuracy while simultaneously preserving global stability. To comprehensively evaluate the effectiveness of the proposed algorithm, rigorous assessments are conducted using the LASA dataset, further reinforced by empirical validation through a robotic experiment.},
  archive   = {C_ICRA},
  author    = {Yu Zhang and Yongxiang Zou and Houcheng Li and Haoyu Zhang and Long Cheng},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610633},
  month     = {5},
  pages     = {17223-17228},
  title     = {Learning a stable dynamic system with a lyapunov energy function for demonstratives using neural networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning complex motion plans using neural ODEs with safety
and stability guarantees. <em>ICRA</em>, 17216–17222. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a Dynamical System (DS) approach to learn complex, possibly periodic motion plans from kinesthetic demonstrations using Neural Ordinary Differential Equations (NODE). To ensure reactivity and robustness to disturbances, we propose a novel approach that selects a target point at each time step for the robot to follow, by combining tools from control theory and the target trajectory generated by the learned NODE. A correction term to the NODE model is computed online by solving a quadratic program that guarantees stability and safety using control Lyapunov functions and control barrier functions, respectively. Our approach outperforms baseline DS learning techniques on the LASA handwriting dataset and complex periodic trajectories. It is also validated on the Franka Emika robot arm to produce stable motions for wiping and stirring tasks that do not have a single attractor, while being robust to perturbations and safe around humans and obstacles. The project’s web-page is https://sites.google.com/view/lfd-neural-ode/home.},
  archive   = {C_ICRA},
  author    = {Farhad Nawaz and Tianyu Li and Nikolai Matni and Nadia Figueroa},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611584},
  month     = {5},
  pages     = {17216-17222},
  title     = {Learning complex motion plans using neural ODEs with safety and stability guarantees},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Domain adaptation of visual policies with a single
demonstration. <em>ICRA</em>, 17208–17215. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deploying machine learning algorithms for robot tasks in real-world applications presents a core challenge: overcoming the domain gap between the training and the deployment environment. This is particularly difficult for visuomotor policies that utilize high-dimensional images as input, particularly when those images are generated via simulation. A common method to tackle this issue is through domain randomization, which aims to broaden the span of the training distribution to cover the test-time distribution. However, this approach is only effective when the domain randomization encompasses the actual shifts in the test-time distribution. We take a different approach, where we make use of a single demonstration (a prompt) to learn policy that adapts to the testing target environment. Our proposed framework, PromptAdapt, leverages the Transformer architecture’s capacity to model sequential data to learn demonstration-conditioned visual policies, allowing for in-context adaptation to a target domain that is distinct from training. Our experiments in both simulation and real-world settings show that PromptAdapt is a strong domain-adapting policy that outperforms baseline methods by a large margin under a range of domain shifts, including variations in lighting, color, texture, and camera pose. Videos and more information can be viewed at project webpage: https://sites.google.com/view/promptadapt.},
  archive   = {C_ICRA},
  author    = {Weiyao Wang and Gregory D. Hager},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610569},
  month     = {5},
  pages     = {17208-17215},
  title     = {Domain adaptation of visual policies with a single demonstration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning barrier-certified polynomial dynamical systems for
obstacle avoidance with robots. <em>ICRA</em>, 17201–17207. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Established techniques that enable robots to learn from demonstrations are based on learning a stable dynamical system (DS). To increase the robots’ resilience to perturbations during tasks that involve static obstacle avoidance, we propose incorporating barrier certificates into an optimization problem to learn a stable and barrier-certified DS. Such optimization problem can be very complex or extremely conservative when the traditional linear parameter-varying formulation is used. Thus, different from previous approaches in the literature, we propose to use polynomial representations for DSs, which yields an optimization problem that can be tackled by sum-of-squares techniques. Finally, our approach can handle obstacle shapes that fall outside the scope of assumptions typically found in the literature concerning obstacle avoidance within the DS learning framework. Supplementary material can be found at the project webpage: https://martinschonger.github.io/abc-ds},
  archive   = {C_ICRA},
  author    = {Martin Schonger and Hugo T. M. Kussaba and Lingyun Chen and Luis Figueredo and Abdalla Swikir and Aude Billard and Sami Haddadin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610828},
  month     = {5},
  pages     = {17201-17207},
  title     = {Learning barrier-certified polynomial dynamical systems for obstacle avoidance with robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Physics-encoded graph neural networks for deformation
prediction under contact. <em>ICRA</em>, 17160–17166. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In robotics, it’s crucial to understand object deformation during tactile interactions. A precise understanding of deformation can elevate robotic simulations and have broad implications across different industries. We introduce a method using Physics-Encoded Graph Neural Networks (GNNs) for such predictions. Similar to robotic grasping and manipulation scenarios, we focus on modeling the dynamics between a rigid mesh contacting a deformable mesh under external forces. Our approach represents both the soft body and the rigid body within graph structures, where nodes hold the physical states of the meshes. We also incorporate cross-attention mechanisms to capture the interplay between the objects. By jointly learning geometry and physics, our model reconstructs consistent and detailed deformations. We’ve made our code and dataset public to advance research in robotic simulation and grasping. †},
  archive   = {C_ICRA},
  author    = {Mahdi Saleh and Michael Sommersperger and Nassir Navab and Federico Tombari},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610366},
  month     = {5},
  pages     = {17160-17166},
  title     = {Physics-encoded graph neural networks for deformation prediction under contact},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TacShade: A new 3D-printed soft optical tactile sensor based
on light, shadow and greyscale for shape reconstruction. <em>ICRA</em>,
17153–17159. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present the TacShade: a newly designed 3D-printed soft optical tactile sensor. The sensor is developed for shape reconstruction under the inspiration of sketch drawing that uses the density of sketch lines to draw light and shadow, resulting in the creation of a 3D-view effect. TacShade, building upon the strengths of the TacTip, a single-camera tactile sensor of large in-depth deformation and being sensitive to edge and surface following, improves the structure in that the markers are distributed within the gap of papillae pins. Variations in light, dark and grey effects can be generated inside the sensor under the external contact interactions. The contours of the contacting objects are outlined by white markers, while the contact depth characteristics can be indirectly obtained from the distribution of black pins and white markers, creating a 2.5D visualization. Based on the imaging effect, we improve the Shape from Shading (SFS) algorithm to process tactile images, enabling a coarse but fast reconstruction for the contact objects. Two experiments are performed. The first verifies TacShade’s ability to reconstruct the shape of the contact objects through one image for object distinction. The second experiment shows the shape reconstruction capability of TacShade for a large panel with ridged patterns based on the location of robots and image splicing technology.},
  archive   = {C_ICRA},
  author    = {Zhenyu Lu and Jialong Yang and Haoran Li and Yifan Li and Weiyong Si and Nathan Lepora and Chenguang Yang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610508},
  month     = {5},
  pages     = {17153-17159},
  title     = {TacShade: A new 3D-printed soft optical tactile sensor based on light, shadow and greyscale for shape reconstruction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Differentiable compliant contact primitives for estimation
and model predictive control. <em>ICRA</em>, 17146–17152. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Control techniques like MPC can realize contact-rich manipulation which exploits dynamic information, maintaining friction limits and safety constraints. However, contact geometry and dynamics are required to be known. This information is often extracted from CAD, limiting scalability and the ability to handle tasks with varying geometry. To reduce the need for a priori models, we propose a framework for estimating contact models online based on torque and position measurements. To do this, compliant contact models are used, connected in parallel to model multi-point contact and constraints such as a hinge. They are parameterized to be differentiable with respect to all of their parameters (rest position, stiffness, contact location), allowing the coupled robot/environment dynamics to be linearized or efficiently used in gradient-based optimization. These models are then applied for: offline gradient-based parameter fitting, online estimation via an extended Kalman filter, and online gradient-based MPC. The proposed approach is validated on two robots, showing the efficacy of sensorless contact estimation and the effects of online estimation on MPC performance. Video results can be seen at https://youtu.be/CuCTcmn3H-o.},
  archive   = {C_ICRA},
  author    = {Kevin Haninger and Kangwagye Samuel and Filippo Rozzi and Sehoon Oh and Loris Roveda},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611406},
  month     = {5},
  pages     = {17146-17152},
  title     = {Differentiable compliant contact primitives for estimation and model predictive control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging compliant tactile perception for haptic blind
surface reconstruction. <em>ICRA</em>, 17139–17145. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Non-flat surfaces pose difficulties for robots operating in unstructured environments. Reconstructions of uneven surfaces may only be partially possible due to non-compliant end-effectors and limitations on vision systems such as transparency, reflections, and occlusions. This study achieves blind surface reconstruction by harnessing the robotic manipulator’s kinematic data and a compliant tactile sensing module, which incorporates inertial, magnetic, and pressure sensors. The module’s flexibility enables us to estimate contact positions and surface normals by analyzing its deformation during interactions with unknown objects. While previous works collect only positional information, we include the local normals in a geometrical approach to estimate curvatures between adjacent contact points. These parameters then guide a spline-based patch generation, which allows us to recreate larger surfaces without an increase in complexity while reducing the time-consuming step of probing the surface. Experimental validation demonstrates that this approach outperforms an off-the-shelf vision system in estimation accuracy. Moreover, this compliant haptic method works effectively even when the manipulator’s approach angle is not aligned with the surface normals, which is ideal for unknown non-flat surfaces.},
  archive   = {C_ICRA},
  author    = {Laurent Y. E. Ramos Cheret and Vinicius Prado Da Fonseca and Thiago E. Alves de Oliveira},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610162},
  month     = {5},
  pages     = {17139-17145},
  title     = {Leveraging compliant tactile perception for haptic blind surface reconstruction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incipient slip-based rotation measurement via visuotactile
sensing during in-hand object pivoting. <em>ICRA</em>, 17132–17138. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In typical in-hand manipulation tasks represented by object pivoting, the real-time perception of rotational slippage has been proven beneficial for improving the dexterity and stability of robotic hands. An effective strategy is to obtain the contact properties for measuring rotation angle through visuotactile sensing. However, existing methods for rotation estimation did not consider the impact of the incipient slip during the pivoting process, which introduces measurement errors and makes it hard to determine the boundary between stable contact and macro slip. This paper describes a generalized 2-d contact model under pivoting, and proposes a rotation measurement method based on the line-features in the stick region. The proposed method was applied to the Tac3D vision-based tactile sensors using continuous marker patterns. Experiments show that the rotation measurement system could achieve an average static measurement error of 0.17°±0.15° and an average dynamic measurement error of 1.34°±0.48°. Besides, the proposed method requires no training data and can achieve real-time sensing during the in-hand object pivoting.},
  archive   = {C_ICRA},
  author    = {Mingxuan Li and Yen Hang Zhou and Tiemin Li and Yao Jiang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610988},
  month     = {5},
  pages     = {17132-17138},
  title     = {Incipient slip-based rotation measurement via visuotactile sensing during in-hand object pivoting},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal planning for timed partial order specifications.
<em>ICRA</em>, 17093–17099. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the challenge of planning a sequence of tasks to be performed by multiple robots while minimizing the overall completion time subject to timing and precedence constraints. Our approach uses the Timed Partial Orders (TPO) model to specify these constraints. We translate this problem into a Traveling Salesman Problem (TSP) variant with timing and precedent constraints, and we solve it as a Mixed Integer Linear Programming (MILP) problem. Our contributions include a general planning framework for TPO specifications, a MILP formulation accommodating time windows and precedent constraints, its extension to multi-robot scenarios, and a method to quantify plan robustness. We demonstrate our framework on several case studies, including an aircraft turnaround task involving three Jackal robots, highlighting the approach’s potential applicability to important real-world problems. Our benchmark results show that our MILP method outperforms state-of-the-art open-source TSP solvers OR-Tools.},
  archive   = {C_ICRA},
  author    = {Kandai Watanabe and Georgios Fainekos and Bardh Hoxha and Morteza Lahijanian and Hideki Okamoto and Sriram Sankaranarayanan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611547},
  month     = {5},
  pages     = {17093-17099},
  title     = {Optimal planning for timed partial order specifications},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Asynchronous task plan refinement for multi-robot task and
motion planning. <em>ICRA</em>, 17086–17092. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper explores general multi-robot task and motion planning, where multiple robots in close proximity manipulate objects while satisfying constraints and a given goal. In particular, we formulate the plan refinement problem—which, given a task plan, finds valid assignments of variables corresponding to solution trajectories—as a hybrid constraint satisfaction problem. The proposed algorithm follows several design principles that yield the following features: (1) efficient solution finding due to sequential heuristics and implicit time and roadmap representations, and (2) maximized feasible solution space obtained by introducing minimally necessary coordination-induced constraints and not relying on prevalent simplifications that exist in the literature. The evaluation results demonstrate the planning efficiency of the proposed algorithm, outperforming the synchronous approach in terms of makespan.},
  archive   = {C_ICRA},
  author    = {Yoonchang Sung and Rahul Shome and Peter Stone},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610503},
  month     = {5},
  pages     = {17086-17092},
  title     = {Asynchronous task plan refinement for multi-robot task and motion planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Non-axiomatic reasoning for an autonomous mobile robot.
<em>ICRA</em>, 17079–17085. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present the integration of a Non-Axiomatic Reasoning System (NARS) with mobile robots for planning and decision making. NARS enables robots to effectively handle uncertainty in real-time with complete sensor and actuator integration, thereby ensuring adaptability to evolving scenarios. We discuss essential parts of the logic, the architecture and working principles of NARS, and the integration of NARS as a ROS node. A case study is provided demonstrating the system’s proficiency to carry out a garbage collection task in an open-air environment by operating a mobile robot with manipulator arm, and we demonstrate its ability to learn about the place-dependent accumulation of garbage items. Case study also reveals that our approach performs more effectively on the overall task than the Belief-Desire-Intention model we compared with.},
  archive   = {C_ICRA},
  author    = {Patrick Hammer and Peter Isaev and Lei Feng and Robert Johansson and Jana Tumova},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611411},
  month     = {5},
  pages     = {17079-17085},
  title     = {Non-axiomatic reasoning for an autonomous mobile robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HAPFI: History-aware planning based on fused information.
<em>ICRA</em>, 17072–17078. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Embodied Instruction Following (EIF) is a task of planning a long sequence of sub-goals given high-level natural language instructions, such as &quot;Rinse a slice of lettuce and place on the white table next to the fork&quot;. To successfully execute these long-term horizon tasks, we argue that an agent must consider its past, i.e., historical data, when making decisions in each step. Nevertheless, recent approaches in EIF often neglects the knowledge from historical data and also do not effectively utilize information across the modalities. To this end, we propose History-Aware Planning based on Fused Information(HAPFI), effectively leveraging the historical data from diverse modalities that agents collect while interacting with the environment. Specifically, HAPFI integrates multiple modalities, including historical RGB observations, bounding boxes, sub-goals, and high-level instructions, by effectively fusing modalities via our Mutually Attentive Fusion method. Through experiments with diverse comparisons, we show that an agent utilizing historical multi-modal information surpasses all the compared methods that neglect the historical data in terms of action planning capability, enabling the generation of well-informed action plans for the next step. Moreover, we provided qualitative evidence highlighting the significance of leveraging historical multi-modal data, particularly in scenarios where the agent encounters intermediate failures, showcasing its robust re-planning capabilities.},
  archive   = {C_ICRA},
  author    = {Sujin Jeon and Suyeon Shin and Byoung-Tak Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610571},
  month     = {5},
  pages     = {17072-17078},
  title     = {HAPFI: History-aware planning based on fused information},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). When prolog meets generative models: A new approach for
managing knowledge and planning in robotic applications. <em>ICRA</em>,
17065–17071. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a robot oriented knowledge representation system based on the use of the Prolog language. Our framework hinges on a special organisation of Knowledge Base (KB) that enables: 1) its efficient population from natural language texts using semi-automated procedures based on Large Language Models (LLMs); 2) the seamless generation of temporal parallel plans for multi-robot systems through a sequence of transformations; 3) the automated translation of the plan into an executable formalism. The framework is supported by a set of open source tools and its functionality is shown with a realistic application.},
  archive   = {C_ICRA},
  author    = {Enrico Saccon and Ahmet Tikna and Davide De Martini and Edoardo Lamon and Luigi Palopoli and Marco Roveri},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610800},
  month     = {5},
  pages     = {17065-17071},
  title     = {When prolog meets generative models: A new approach for managing knowledge and planning in robotic applications},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). RaSim: A range-aware high-fidelity RGB-d data simulation
pipeline for real-world applications. <em>ICRA</em>, 17057–17064. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In robotic vision, a de-facto paradigm is to learn in simulated environments and then transfer to real-world applications, which poses an essential challenge in bridging the sim-to-real domain gap. While mainstream works tackle this problem in the RGB domain, we focus on depth data synthesis and develop a Range-aware RGB-D data Simulation pipeline (RaSim). In particular, high-fidelity depth data is generated by imitating the imaging principle of real-world sensors. A range-aware rendering strategy is further introduced to enrich data diversity. Extensive experiments show that models trained with RaSim can be directly applied to real-world scenarios without any finetuning and excel at downstream RGB-D perception tasks. Data and code are available at https://github.com/shanice-l/RaSim.},
  archive   = {C_ICRA},
  author    = {Xingyu Liu and Chenyangguang Zhang and Gu Wang and Ruida Zhang and Xiangyang Ji},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611208},
  month     = {5},
  pages     = {17057-17064},
  title     = {RaSim: A range-aware high-fidelity RGB-D data simulation pipeline for real-world applications},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RTS-GT: Robotic total stations ground truthing dataset.
<em>ICRA</em>, 17050–17056. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Numerous datasets and benchmarks exist to assess and compare Simultaneous Localization and Mapping (SLAM) algorithms. Nevertheless, their precision must follow the rate at which SLAM algorithms improved in recent years. Moreover, current datasets fall short of comprehensive data-collection protocol for reproducibility and the evaluation of the precision or accuracy of the recorded trajectories. With this objective in mind, we proposed the Robotic Total Stations Ground Truthing dataset (RTS-GT) dataset to support localization research with the generation of six-Degrees Of Freedom (DOF) ground truth trajectories. This novel dataset includes six-DOF ground truth trajectories generated using a system of three Robotic Total Stations (RTSs) tracking moving robotic platforms. Furthermore, we compare the performance of the RTS-based system to a Global Navigation Satellite System (GNSS)-based setup. The dataset comprises around sixty experiments conducted in various conditions over a period of 17 months, and encompasses over 49 kilometers of trajectories, making it the most extensive dataset of RTS-based measurements to date. Additionally, we provide the precision of all poses for each experiment, a feature not found in the current state-of-the-art datasets. Our results demonstrate that RTSs provide measurements that are 22 times more stable than GNSS in various environmental settings, making them a valuable resource for SLAM benchmark development.},
  archive   = {C_ICRA},
  author    = {Maxime Vaidis and Mohsen Hassanzadeh Shahraji and Effie Daum and William Dubois and Philippe Giguère and François Pomerleau},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610998},
  month     = {5},
  pages     = {17050-17056},
  title     = {RTS-GT: Robotic total stations ground truthing dataset},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Close the sim2real gap via physically-based structured light
synthetic data simulation. <em>ICRA</em>, 17035–17041. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite the substantial progress in deep learning, its adoption in industrial robotics projects remains limited, primarily due to challenges in data acquisition and labeling. Previous sim2real approaches using domain randomization require extensive scene and model optimization. To address these issues, we introduce an innovative physically-based structured light simulation system, generating both RGB and physically realistic depth images, surpassing previous dataset generation tools. We create an RGBD dataset tailored for robotic industrial grasping scenarios and evaluate it across various tasks, including object detection, instance segmentation, and embedding sim2real visual perception in industrial robotic grasping. By reducing the sim2real gap and enhancing deep learning training, we facilitate the application of deep learning models in industrial settings. Project details are available at https://baikaixin-public.github.io/structured_light_3D_synthesizer/},
  archive   = {C_ICRA},
  author    = {Kaixin Bai and Lei Zhang and Zhaopeng Chen and Fang Wan and Jianwei Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611401},
  month     = {5},
  pages     = {17035-17041},
  title     = {Close the sim2real gap via physically-based structured light synthetic data simulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PointSSC: A cooperative vehicle-infrastructure point cloud
benchmark for semantic scene completion. <em>ICRA</em>, 17027–17034. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Semantic Scene Completion (SSC) aims to jointly generate space occupancies and semantic labels for complex 3D scenes. Most existing SSC models focus on volumetric representations, which are memory-inefficient for large outdoor spaces. Point clouds provide a lightweight alternative but existing benchmarks lack outdoor point cloud scenes with semantic labels. To address this, we introduce PointSSC, the first cooperative vehicle-infrastructure point cloud benchmark for semantic scene completion. These scenes exhibit long-range perception and minimal occlusion. We develop an automated annotation pipeline leveraging Semantic Segment Anything to efficiently assign semantics. To benchmark progress, we propose a LiDAR-based model with a Spatial-Aware Transformer for global and local feature extraction and a Completion and Segmentation Cooperative Module for joint completion and segmentation. PointSSC provides a challenging testbed to drive advances in semantic point cloud completion for real-world navigation. The code and datasets are available at https://github.com/yyxssm/PointSSC.},
  archive   = {C_ICRA},
  author    = {Yuxiang Yan and Boda Liu and Jianfei Ai and Qinbu Li and Ru Wan and Jian Pu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610043},
  month     = {5},
  pages     = {17027-17034},
  title     = {PointSSC: A cooperative vehicle-infrastructure point cloud benchmark for semantic scene completion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Introducing CEA-IMSOLD: An industrial multi-scale object
localization dataset. <em>ICRA</em>, 17020–17026. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce the CEA Industrial Multi-Scale Object Localization Dataset (CEA-IMSOLD), a new BOP format dataset for 6-DoF object localization, crucial for robotics. This dataset aims to evaluate the current localization methods with respect to a new difficulty: large variations in observation distance and, consequently, large variations in image appearance. Compared to the other publicly available datasets, our dataset provides both images with objects small and completely visible in the image, and images where objects are observed close enough so they appear larger than the field of view of the camera. We also propose to consider the observation distance in the evaluation process and introduce new metrics to do so. Finally, our dataset contains a large variety of industrial objects, from small and simple objects such as bolts to sizable and complex ones such as large car parts. We provide baseline results and the dataset is made publicly available to support the community at https://cea-list.github.io/CEA-IMSOLD/.},
  archive   = {C_ICRA},
  author    = {Boris Meden and Pablo Vega and Fabrice Mayran De Chamisso and Steve Bourgeois},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609999},
  month     = {5},
  pages     = {17020-17026},
  title     = {Introducing CEA-IMSOLD: An industrial multi-scale object localization dataset},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multimodal handover failure detection dataset and
baselines. <em>ICRA</em>, 17013–17019. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An object handover between a robot and a human is a coordinated action which is prone to failure for reasons such as miscommunication, incorrect actions and unexpected object properties. Existing works on handover failure detection and prevention focus on preventing failures due to object slip or external disturbances. However, there is a lack of datasets and evaluation methods that consider unpreventable failures caused by the human participant. To address this deficit, we present the multimodal Handover Failure Detection dataset, which consists of failures induced by the human participant, such as ignoring the robot or not releasing the object. We also present two baseline methods for handover failure detection: (i) a video classification method using 3D CNNs and (ii) a temporal action segmentation approach which jointly classifies the human action, robot action and overall outcome of the action. The results show that video is an important modality, but using force-torque data and gripper position help improve failure detection and action segmentation accuracy.},
  archive   = {C_ICRA},
  author    = {Santosh Thoduka and Nico Hochgeschwender and Juergen Gall and Paul G. Plöger},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610143},
  month     = {5},
  pages     = {17013-17019},
  title     = {A multimodal handover failure detection dataset and baselines},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PanNote: An automatic tool for panoramic image annotation of
people’s positions. <em>ICRA</em>, 17006–17012. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Panoramic cameras offer a 4π steradian field of view, which is desirable for tasks like people detection and tracking since nobody can exit the field of view. Despite the recent diffusion of low-cost panoramic cameras, their usage in robotics remains constrained by the limited availability of datasets featuring annotations in the robot space, including people’s 2D or 3D positions. To tackle this issue, we introduce PanNote, an automatic annotation tool for people’s positions in panoramic videos. Our tool is designed to be cost-effective and straightforward to use without requiring human intervention during the labeling process and enabling the training of machine learning models with low effort. The proposed method introduces a calibration model and a data association algorithm to fuse data from panoramic images and 2D LiDAR readings. We validate the capabilities of PanNote by collecting a real-world dataset. On these data, we compared manual labels, automatic labels and the predictions of a baseline deep neural network. Results clearly show the advantage of using our method, with a 15-fold speed up in labeling time and a considerable gain in performance while training deep neural models on automatically labelled data.},
  archive   = {C_ICRA},
  author    = {Alberto Bacchin and Leonardo Barcellona and Sepideh Shamsizadeh and Emilio Olivastri and Alberto Pretto and Emanuele Menegatti},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610347},
  month     = {5},
  pages     = {17006-17012},
  title     = {PanNote: An automatic tool for panoramic image annotation of people’s positions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring the needle tip interaction force with retinal
tissue deformation in vitreoretinal surgery. <em>ICRA</em>, 16999–17005.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advancements in age-related macular degeneration treatments necessitate precision delivery into the subretinal space, emphasizing minimally invasive procedures targeting the retinal pigment epithelium (RPE)-Bruch’s membrane complex without causing trauma. Even for skilled surgeons, the inherent hand tremors during manual surgery can jeopardize the safety of these critical interventions.This has fostered the evolution of robotic systems designed to prevent such tremors. These robots are enhanced by FBG sensors, which sense the small force interactions between the surgical instruments and retinal tissue. To enable the community to design algorithms taking advantage of such force feedback data, this paper focuses on the need to provide a specialized dataset, integrating optical coherence tomography (OCT) imaging together with the aforementioned force data.We introduce a unique dataset, integrating force sensing data synchronized with OCT B-scan images, derived from a sophisticated setup involving robotic assistance and OCT integrated microscopes. Furthermore, we present a neural network model for image-based force estimation to demonstrate the dataset’s applicability.},
  archive   = {C_ICRA},
  author    = {Simon Pannek and Shervin Dehghani and Michael Sommersperger and Peiyao Zhang and Peter Gehlbach and M. Ali Nasseri and Iulian Iordachita and Nassir Navab},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610807},
  month     = {5},
  pages     = {16999-17005},
  title     = {Exploring the needle tip interaction force with retinal tissue deformation in vitreoretinal surgery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning manipulation of steep granular slopes for fast mini
rover turning. <em>ICRA</em>, 16985–16990. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Future planetary exploration missions will require reaching challenging regions such as craters and steep slopes. Such regions are ubiquitous and present science-rich targets potentially containing information regarding the planet’s internal structure. Steep slopes consisting of low-cohesion regolith are prone to flow downward under small disturbances, making it challenging for autonomous rovers to traverse. Moreover, the navigation trajectories of rovers are heavily limited by the terrain topology and future systems will need to maneuver on flowable surfaces without getting trapped, allowing them to further expand their reach and increase mission efficiency.In this work, we used a robophysical rover model and performed maneuvering experiments on a steep granular slope of poppy seeds to explore the rover’s turning capabilities. The rover is capable of lifting, sweeping, and spinning its wheels, allowing it to execute leg-like gait patterns. The high-dimensional actuation capabilities of the rover facilitate effective manipulation of the underlying granular surface. We used Bayesian Optimization (BO) to gain insight into successful turning gaits in high dimensional search space and found strategies such as differential wheel spinning and pivoting around a single sweeping wheel. We then used these insights to further fine-tune the turning gait, enabling the rover to turn nearly 90 degrees at just above 4 seconds with minimal downhill slip. Combining gait optimization and human-tuning approaches, we found that fast turning is empowered by creating anisotropic torques with the sweeping wheel.},
  archive   = {C_ICRA},
  author    = {Deniz Kerimoglu and Daniel Soto and Malone Lincoln Hemsley and Joseph Brunner and Sehoon Ha and Tingnan Zhang and Daniel I. Goldman},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611700},
  month     = {5},
  pages     = {16985-16990},
  title     = {Learning manipulation of steep granular slopes for fast mini rover turning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robotic offline RL from internet videos via value-function
learning. <em>ICRA</em>, 16977–16984. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pre-training on Internet data has proven to be a key ingredient for broad generalization in many modern ML systems. What would it take to enable such capabilities in robotic reinforcement learning (RL)? Offline RL methods, which learn from datasets of robot experience, offer one way to leverage prior data into the robotic learning pipeline. However, these methods have a &quot;type mismatch&quot; with video data (such as Ego4D), which are the largest prior datasets available for robotics, since video offers observation-only experience without the action or reward annotations needed for RL methods. In this paper, we develop a system for leveraging large-scale human video datasets in robotic offline RL, based entirely on learning value functions via temporal-difference learning. We show that value learning on video datasets learns representations that are more conducive to downstream robotic offline RL than other approaches for learning from video data. Our system, called V-PTR, combines the benefits of pre-training on video data with robotic offline RL approaches that train on diverse robot data, resulting in value functions and policies for manipulation tasks that perform better, act robustly, and generalize broadly. On several manipulation tasks on a real WidowX robot and in simulated settings, our framework produces policies that greatly improve over other prior methods. Our video and additional details can be found at https://dibyaghosh.com/vptr/.},
  archive   = {C_ICRA},
  author    = {Chethan Bhateja and Derek Guo and Dibya Ghosh and Anikait Singh and Manan Tomar and Quan Vuong and Yevgen Chebotar and Sergey Levine and Aviral Kumar},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611575},
  month     = {5},
  pages     = {16977-16984},
  title     = {Robotic offline RL from internet videos via value-function learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving out-of-distribution generalization of learned
dynamics by learning pseudometrics and constraint manifolds.
<em>ICRA</em>, 16970–16976. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a method for improving the prediction accuracy of learned robot dynamics models on out-of-distribution (OOD) states. We achieve this by leveraging two key sources of structure often present in robot dynamics: 1) sparsity, i.e., some components of the state may not affect the dynamics, and 2) physical limits on the set of possible motions, in the form of nonholonomic constraints. Crucially, we do not assume this structure is known a priori, and instead learn it from data. We use contrastive learning to obtain a distance pseudometric that uncovers the sparsity pattern in the dynamics, and use it to reduce the input space when learning the dynamics. We then learn the unknown constraint manifold by approximating the normal space of possible motions from the data, which we use to train a Gaussian process (GP) representation of the constraint manifold. We evaluate our approach on a physical differential-drive robot and a simulated quadrotor, showing improved prediction accuracy on OOD data relative to baselines.},
  archive   = {C_ICRA},
  author    = {Yating Lin and Glen Chou and Dmitry Berenson},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611657},
  month     = {5},
  pages     = {16970-16976},
  title     = {Improving out-of-distribution generalization of learned dynamics by learning pseudometrics and constraint manifolds},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SERL: A software suite for sample-efficient robotic
reinforcement learning. <em>ICRA</em>, 16961–16969. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, significant progress has been made in the field of robotic reinforcement learning (RL), enabling methods that handle complex image observations, train in the real world, and incorporate auxiliary data, such as demonstrations and prior experience. However, despite these advances, robotic RL remains hard to use. It is acknowledged among practitioners that the particular implementation details of these algorithms are often just as important (if not more so) for performance as the choice of algorithm. We posit that a significant challenge to the widespread adoption of robotic RL, as well as the further development of robotic RL methods, is the comparative inaccessibility of such methods. To address this challenge, we developed a carefully implemented library containing a sample efficient off-policy deep RL method, together with methods for computing rewards and resetting the environment, a high-quality controller for a widely adopted robot, and a number of challenging example tasks. We provide this library as a resource for the community, describe its design choices, and present experimental results. Perhaps surprisingly, we find that our implementation can achieve very efficient learning, acquiring policies for PCB board assembly, cable routing, and object relocation between 25 to 50 minutes of training per policy on average, improving over state-of-the-art results reported for similar tasks in the literature. These policies achieve perfect or near-perfect success rates, extreme robustness even under perturbations, and exhibit emergent recovery and correction behaviors. We hope these promising results and our high-quality open-source implementation will provide a tool for the robotics community to facilitate further developments in robotic RL. Our code, documentation, and videos can be found at https://serl-robot.github.io/},
  archive   = {C_ICRA},
  author    = {Jianlan Luo and Zheyuan Hu and Charles Xu and You Liang Tan and Jacob Berg and Archit Sharma and Stefan Schaal and Chelsea Finn and Abhishek Gupta and Sergey Levine},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610040},
  month     = {5},
  pages     = {16961-16969},
  title     = {SERL: A software suite for sample-efficient robotic reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pay attention to how you drive: Safe and adaptive
model-based reinforcement learning for off-road driving. <em>ICRA</em>,
16954–16960. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous off-road driving is challenging as unsafe actions may lead to catastrophic damage. As such, developing controllers in simulation is often desirable. However, robot dynamics in unstructured off-road environments can be highly complex and difficult to simulate accurately. Domain randomization addresses this problem by randomizing simulation dynamics to train policies that are robust towards modeling errors. While these policies are robust across a range of dynamics, they are sub-optimal for any particular system dynamics. We introduce a novel model-based reinforcement learning approach that aims to balance robustness with adaptability. We train a System Identification Transformer (SIT) and an Adaptive Dynamics Model (ADM) under a variety of simulated dynamics. The SIT uses attention mechanisms to distill target system state-transition observations into a context vector, which provides an abstraction for the target dynamics. Conditioned on this, the ADM probabilistically models the system’s dynamics. Online, we use a Risk-Aware Model Predictive Path Integral controller to safely control the robot under its current understanding of dynamics. We demonstrate in simulation and in the real world that this approach enables safer behaviors upon initialization and becomes less conservative (i.e. faster) as its understanding of the target system dynamics improves with more observations. In particular, our approach results in an approximately 41% improvement in lap-time over the non-adaptive baseline while remaining safe across different environments.},
  archive   = {C_ICRA},
  author    = {Sean J. Wang and Honghao Zhu and Aaron M. Johnson},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611059},
  month     = {5},
  pages     = {16954-16960},
  title     = {Pay attention to how you drive: Safe and adaptive model-based reinforcement learning for off-road driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep model predictive optimization. <em>ICRA</em>,
16945–16953. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A major challenge in robotics is to design robust policies which enable complex and agile behaviors in the real world. On one end of the spectrum, we have model-free reinforcement learning (MFRL), which is incredibly flexible and general but often results in brittle policies. In contrast, model predictive control (MPC) continually re-plans at each time step to remain robust to perturbations and model inaccuracies. However, despite its real-world successes, MPC often under-performs the optimal strategy. This is due to model quality, myopic behavior from short planning horizons, and approximations due to computational constraints. And even with a perfect model and enough compute, MPC can get stuck in bad local optima, depending heavily on the quality of the optimization algorithm. To this end, we propose Deep Model Predictive Optimization (DMPO), which learns the inner-loop of an MPC optimization algorithm directly via experience, specifically tailored to the needs of the control problem. We evaluate DMPO on a real quadrotor agile trajectory tracking task, on which it improves performance over a baseline MPC algorithm for a given computational budget. It can outperform the best MPC algorithm by up to 27% with fewer samples and an end-to-end policy trained with MFRL by 19%. Moreover, because DMPO requires fewer samples, it can also achieve these benefits with 4.3× less memory. When we subject the quadrotor to turbulent wind fields with an attached drag plate, DMPO can adapt zero-shot while still outperforming all baselines. Additional results can be found at https://tinyurl.com/mr2ywmnw.},
  archive   = {C_ICRA},
  author    = {Jacob Sacks and Rwik Rana and Kevin Huang and Alex Spitzer and Guanya Shi and Byron Boots},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611492},
  month     = {5},
  pages     = {16945-16953},
  title     = {Deep model predictive optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hamiltonian dynamics learning from point cloud observations
for nonholonomic mobile robot control. <em>ICRA</em>, 16937–16944. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reliable autonomous navigation requires adapting the control policy of a mobile robot in response to dynamics changes in different operational conditions. Hand-designed dynamics models may struggle to capture model variations due to a limited set of parameters. Data-driven dynamics learning approaches offer higher model capacity and better generalization but require large amounts of state-labeled data. This paper develops an approach for learning robot dynamics directly from point-cloud observations, removing the need and associated errors of state estimation, while embedding Hamiltonian structure in the dynamics model to improve data efficiency. We design an observation-space loss that relates motion prediction from the dynamics model with motion prediction from point-cloud registration to train a Hamiltonian neural ordinary differential equation. The learned Hamiltonian model enables the design of an energy-shaping model-based tracking controller for rigid-body robots. We demonstrate dynamics learning and tracking control on a real nonholonomic wheeled robot.},
  archive   = {C_ICRA},
  author    = {Abdullah Altawaitan and Jason Stanley and Sambaran Ghosal and Thai Duong and Nikolay Atanasov},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610395},
  month     = {5},
  pages     = {16937-16944},
  title     = {Hamiltonian dynamics learning from point cloud observations for nonholonomic mobile robot control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sim-to-real learning for humanoid box loco-manipulation.
<em>ICRA</em>, 16930–16936. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work we propose a learning-based approach to box loco-manipulation for a humanoid robot. This is a particularly challenging problem due to the need for whole-body coordination in order to lift boxes of varying weight, position, and orientation while maintaining balance. To address this challenge, we present a sim-to-real reinforcement learning approach for training general box pickup and carrying skills for the bipedal robot Digit. Our reward functions are designed to produce the desired interactions with the box while also valuing balance and gait quality. We combine the learned skills into a full system for box loco-manipulation to achieve the task of moving boxes from one table to another with a variety of sizes, weights, and initial configurations. In addition to quantitative simulation results, we demonstrate successful sim-to-real transfer on the humanoid robot Digit. To our knowledge this is the first demonstration of a learned controller for such a task on real world hardware.},
  archive   = {C_ICRA},
  author    = {Jeremy Dao and Helei Duan and Alan Fern},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610977},
  month     = {5},
  pages     = {16930-16936},
  title     = {Sim-to-real learning for humanoid box loco-manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simulation modeling of highly dynamic omnidirectional mobile
robots based on real-world data. <em>ICRA</em>, 16923–16929. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simulation is a key technology in robotics as it enables the generation of environmental data and testing scenarios for development and maintenance purposes. However, simulations are an imperfect representation of the real-world and the so-called sim-to-real gap between simulation and reality hinders the deployment of virtual developed solutions without additional effort. Modeling complex systems like highly dynamic and holonomic mobile robots presents additional complexities in simulation. This paper addresses these challenges through a case study on creating a model for a highly dynamic logistics robot. The study considers the modeling of the entire system down to creating suitable colliders for the rollers of a Mecanum wheel. Additionally, the impact of significant physics parameters is presented. To bridge the sim-to-real gap, a pipeline is developed that utilizes a Motion Capture system to compare the behavior of a real robot with its simulated counterpart across various motions. By leveraging expert knowledge gained from the real-world data, the simulation model is manually tuned to replicate complex system behaviors, such as sliding effects.},
  archive   = {C_ICRA},
  author    = {Marvin Wiedemann and Ossama Ahmed and Anna Dieckhöfer and Renato Gasoto and Sören Kerner},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611459},
  month     = {5},
  pages     = {16923-16929},
  title     = {Simulation modeling of highly dynamic omnidirectional mobile robots based on real-world data},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Jade: A differentiable physics engine for articulated rigid
bodies with intersection-free frictional contact. <em>ICRA</em>,
16915–16922. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present Jade, a differentiable physics engine for articulated rigid bodies. Jade models contacts as the Linear Complementarity Problem (LCP). Compared to existing differentiable simulations, Jade offers features including intersection-free collision simulation and stable LCP solutions for multiple frictional contacts. We use continuous collision detection to detect the time of impact and adopt the backtracking strategy to prevent intersection between bodies with complex geometry shapes. We derive the gradient calculation to ensure the whole simulation process is differentiable under the backtracking mechanism. We modify the popular Dantzig’s algorithm to get valid solutions under multiple frictional contacts. We conduct extensive experiments to demonstrate the effectiveness of our differentiable physics simulation over a variety of contact-rich tasks. Supplemental materials and videos are available on our project webpage at https://sites.google.com/view/diffsim},
  archive   = {C_ICRA},
  author    = {Gang Yang and Siyuan Luo and Yunhai Feng and Zhixin Sun and Chenrui Tie and Lin Shao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610750},
  month     = {5},
  pages     = {16915-16922},
  title     = {Jade: A differentiable physics engine for articulated rigid bodies with intersection-free frictional contact},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SceneControl: Diffusion for controllable traffic scene
generation. <em>ICRA</em>, 16908–16914. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the task of traffic scene generation. A common approach in the self-driving industry is to use manual creation to generate scenes with specific characteristics and automatic generation to generate canonical scenes at scale. However, manual creation is not scalable, and automatic generation typically use rules-based algorithms that lack realism. In this paper, we propose SceneControl, a framework for controllable traffic scene generation. To capture the complexity of real traffic, SceneControl learns an expressive diffusion model from data. Then, using guided sampling, we can flexibly control the sampling process to generate scenes that exhibit desired characteristics. Our experiments show that SceneControl achieves greater realism and controllability than the existing state-of-the-art. We also illustrate how SceneControl can be used as a tool for interactive traffic scene generation.},
  archive   = {C_ICRA},
  author    = {Jack Lu and Kelvin Wong and Chris Zhang and Simon Suo and Raquel Urtasun},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610324},
  month     = {5},
  pages     = {16908-16914},
  title     = {SceneControl: Diffusion for controllable traffic scene generation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OmniLRS: A photorealistic simulator for lunar robotics.
<em>ICRA</em>, 16901–16907. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Developing algorithms for extra-terrestrial robotic exploration has always been challenging. Along with the complexity associated with these environments, one of the main issues remains the evaluation of said algorithms. With the regained interest in lunar exploration, there is also a demand for quality simulators that will enable the development of lunar robots. In this paper, we propose Omniverse Lunar Robotic-Sim (OmniLRS) that is a photorealistic Lunar simulator based on Nvidia’s robotic simulator. This simulation provides fast procedural environment generation, multi-robot capabilities, along with synthetic data pipeline for machine-learning applications. It comes with ROS1 and ROS2 bindings to control not only the robots, but also the environments. This work also performs sim-to-real rock instance segmentation to show the effectiveness of our simulator for image-based perception. Trained on our synthetic data, a yolov8 model achieves performance close to a model trained on real-world data, with 5% performance gap. When finetuned with real data, the model achieves 14% higher average precision than the model trained on real-world data, demonstrating our simulator’s photorealism. The code is fully open-source, accessible here: https://github.com/AntoineRichard/OmniLRS, and comes with demonstrations.},
  archive   = {C_ICRA},
  author    = {Antoine Richard and Junnosuke Kamohara and Kentaro Uno and Shreya Santra and Dave van der Meer and Miguel Olivares-Mendez and Kazuya Yoshida},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610026},
  month     = {5},
  pages     = {16901-16907},
  title     = {OmniLRS: A photorealistic simulator for lunar robotics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hydrodynamic interactions in schooling fish: Prioritizing
real fish kinematics over travelling-wavy undulation. <em>ICRA</em>,
16895–16900. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hydrodynamic interactions are crucial for understanding fish movement, particularly within the realm of robotic applications. Traditionally, many studies have favoured simplified travelling-wavy undulations derived from observed real fish kinematics. This approach often neglects higher-order undulations, thereby missing the subtleties of authentic fish movements. In this study, we utilised Computational Fluid Dynamics (CFD) to investigate the implications of using real fish kinematics in hydrodynamic interactions among schooling fish. We analysed two scenarios: one driven by real fish kinematics in spatiotemporal formations, and the other by travelling-wavy undulations inferred from the same real fish kinematics. Our results highlight the advantages of using real fish body kinematics for a more accurate representation of hydrodynamics in fish swimming. In contrast, the idealised travelling-wavy undulations tend to apply excessive force, displacing real fish more than expected. Additionally, the vortices and corresponding flow fields generated by real fish kinematics were found to be more stable than those arising from simplified travelling-wavy undulations. Our study underscores the significance of integrating real fish kinematics into robotic fish design and hydrodynamic studies in schooling fish.},
  archive   = {C_ICRA},
  author    = {Li-Ming Chao and Liang Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611390},
  month     = {5},
  pages     = {16895-16900},
  title     = {Hydrodynamic interactions in schooling fish: Prioritizing real fish kinematics over travelling-wavy undulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). STARK: A unified framework for strongly coupled simulation
of rigid and deformable bodies with frictional contact. <em>ICRA</em>,
16888–16894. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The use of simulation in robotics is increasingly widespread for the purpose of testing, synthetic data generation and skill learning. A relevant aspect of simulation for a variety of robot applications is physics-based simulation of robot-object interactions. This involves the challenge of accurately modeling and implementing different mechanical systems such as rigid and deformable bodies as well as their interactions via constraints, contact or friction. Most state-of-the-art physics engines commonly used in robotics either cannot couple deformable and rigid bodies in the same framework, lack important systems such as cloth or shells, have stability issues in complex friction-dominated setups or cannot robustly prevent penetrations. In this paper, we propose a framework for strongly coupled simulation of rigid and deformable bodies with focus on usability, stability, robustness and easy access to state-of-the-art deformation and frictional contact models. Our system uses the Finite Element Method (FEM) to model deformable solids, the Incremental Potential Contact (IPC) approach for frictional contact and a robust second order optimizer to ensure stable and penetration-free solutions to tight tolerances. It is a general purpose framework, not tied to a particular use case such as grasping or learning, it is written in C++ and comes with a Python interface. We demonstrate our system’s ability to reproduce complex real-world experiments where a mobile vacuum robot interacts with a towel on different floor types and towel geometries. Our system is able to reproduce 100% of the qualitative outcomes observed in the laboratory environment. The simulation pipeline, named Stark (the German word for strong, as in strong coupling) is made open-source.},
  archive   = {C_ICRA},
  author    = {José Antonio Fernández-Fernández and Ralph Lange and Stefan Laible and Kai O. Arras and Jan Bender},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610574},
  month     = {5},
  pages     = {16888-16894},
  title     = {STARK: A unified framework for strongly coupled simulation of rigid and deformable bodies with frictional contact},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MuRoSim – a fast and efficient multi-robot simulation for
learning-based navigation. <em>ICRA</em>, 16881–16887. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-robot navigation and dynamic obstacle avoidance are challenging problems in robot learning. Recent advancements in Deep Reinforcement Learning (DRL) have demonstrated great potential in this area. Nonetheless, they often face challenges related to low sample efficiency. To overcome this challenge, some research proposes simulators that incorporate hardware acceleration. Although these simulators improve efficiency, they often lack the flexibility to generate diverse learning scenarios as often needed in multi-robot scenarios, where the different environments have varying numbers of agents.In this paper, we introduce MuRoSim, a multi-robot simulation for lidar-based navigation specifically designed for DRL applications. Due to its high level of abstraction, complete implementation in C++, and rigorous thread pool utilization, MuRoSim achieves high computational performance. We apply MuRoSim for training navigation policies for omnidirectional mobile robots equipped with lidar sensors using DRL. Finally, we conduct extensive Sim-to-Real experiments to confirm the realism of the simulator, by deploying the learned policy for dynamic navigation with up to six robots in numerous of real- world experiments.},
  archive   = {C_ICRA},
  author    = {Christian Jestel and Karol Rösner and Niklas Dietz and Nicolas Bach and Julian Eßer and Jan Finke and Oliver Urbann},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610375},
  month     = {5},
  pages     = {16881-16887},
  title     = {MuRoSim – a fast and efficient multi-robot simulation for learning-based navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SocialGAIL: Faithful crowd simulation for social robot
navigation. <em>ICRA</em>, 16873–16880. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Navigation through crowded human environments is challenging for social robots. While reinforcement learning has been adopted for its capacity to capture complex interactions, the training process often relies on simulators to replicate realistic crowd behaviors, ensuring cost-efficiency. Existing crowd simulation methods typically rely on either handcrafted rules, which may lead to overly aggressive navigation, or learning from human trajectory demonstrations, which can be challenging to generalize effectively. In this paper, we introduce a data-driven crowd simulation method called SocialGAIL, which leverages Generative Adversarial Imitation Learning (GAIL) to emulate real pedestrian navigation in crowded environments. SocialGAIL utilizes an attention-based graph neural network to encode observations and employs a generator-discriminator architecture to closely mimic pedestrian behavior. We propose a set of metrics to evaluate the faithfulness of crowd simulation. Experimental results demonstrate that SocialGAIL outperforms baseline methods in terms of goal-reaching, intermediate state faithfulness, trajectory faithfulness, and adherence to global trajectory patterns. The code of our approach is available at https://github.com/William-island/SocialGAIL.},
  archive   = {C_ICRA},
  author    = {Bo Ling and Yan Lyu and Dongxiao Li and Guanyu Gao and Yi Shi and Xueyong Xu and Weiwei Wu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610371},
  month     = {5},
  pages     = {16873-16880},
  title     = {SocialGAIL: Faithful crowd simulation for social robot navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Neural radiance fields for unbounded lunar surface scene.
<em>ICRA</em>, 16858–16864. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate understanding of lunar surface topography is vital for effective decision-making and remote control of lunar rovers during exploration missions. Conventional sensing methods often struggle to capture the intricate details of the lunar landscape. In response, we propose an innovative approach that leverages NeRF to synthesize new viewpoints within the expansive lunar environment. By blending 3D hash grids and 2D plane grids representations, our approach provides a comprehensive scene representation. We employ the technique of spiral sampling and feature rendering to enhance rendering quality while simultaneously reducing training time. Additionally, we leverage sparse point cloud to aid the model in better learning the geometric structure of the lunar environment. Through experimentation, we have demonstrated that our method is capable of synthesizing realistic images of lunar environments.},
  archive   = {C_ICRA},
  author    = {Xu Zhang and Linyan Cui and Jihao Yin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611137},
  month     = {5},
  pages     = {16858-16864},
  title     = {Neural radiance fields for unbounded lunar surface scene},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bi-KVIL: Keypoints-based visual imitation learning of
bimanual manipulation tasks. <em>ICRA</em>, 16850–16857. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual imitation learning has achieved impressive progress in learning unimanual manipulation tasks from a small set of visual observations, thanks to the latest advances in computer vision. However, learning bimanual coordination strategies and complex object relations from bimanual visual demonstrations, as well as generalizing them to categorical objects in novel cluttered scenes remain unsolved challenges. In this paper, we extend our previous work on keypoints-based visual imitation learning (K-VIL) [1] to bimanual manipulation tasks. The proposed Bi-KVIL jointly extracts so-called Hybrid Master-Slave Relationships (HMSR) among objects and hands, bimanual coordination strategies, and sub-symbolic task representations. Our bimanual task representation is object-centric, embodiment-independent, and viewpoint-invariant, thus generalizing well to categorical objects in novel scenes. We evaluate our approach in various real-world applications, showcasing its ability to learn fine-grained bimanual manipulation tasks from a small number of human demonstration videos. Videos and source code are available at https://sites.google.com/view/bi-kvil.},
  archive   = {C_ICRA},
  author    = {Jianfeng Gao and Xiaoshu Jin and Franziska Krebs and Noémie Jaquier and Tamim Asfour},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610763},
  month     = {5},
  pages     = {16850-16857},
  title     = {Bi-KVIL: Keypoints-based visual imitation learning of bimanual manipulation tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Crossway diffusion: Improving diffusion-based visuomotor
policy via self-supervised learning. <em>ICRA</em>, 16841–16849. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Diffusion models have been adopted for behavioral cloning in a sequence modeling fashion, benefiting from their exceptional capabilities in modeling complex data distributions. The standard diffusion-based policy iteratively denoises action sequences from random noise conditioned on the input states and the model is typically trained with a singular diffusion loss. This paper explores the potential enhancements in such models when the denoising process is informed by a better visual representation. We study the scenario where the model is jointly optimized using the standard diffusion loss alongside an auxiliary objective based on self-supervised learning. After experimenting with various objectives, we introduce Crossway Diffusion, a simple yet effective way to enhance diffusion-based visuomotor policy learning via a state decoder and an auxiliary reconstruction objective. During training, the state decoder reconstructs raw image pixels and other states from the intermediate representations of the model. Experiments demonstrate the effectiveness of our method in various simulated and real-world tasks, confirming its consistent advantages over the standard diffusion-based policy and other baselines.},
  archive   = {C_ICRA},
  author    = {Xiang Li and Varun Belagali and Jinghuan Shang and Michael S. Ryoo},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610175},
  month     = {5},
  pages     = {16841-16849},
  title     = {Crossway diffusion: Improving diffusion-based visuomotor policy via self-supervised learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). DL-PoseNet: A differential lightweight network for pose
regression over SE(3). <em>ICRA</em>, 16834–16840. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate pose estimation over SE(3) is fundamentally crucial for numerous perception tasks, including camera re-localization. While existing learning-based methods estimated from a series of RGB images have significantly improved the accuracy of pose, the majority of models still face one or two limitations. First, few representations on SE(3) are smooth and differential, making them difficult to apply in deep learning frameworks. Second, they often require high computational resources due to complex deep network designs. We in this paper propose the DL-PoseNet to address these issues. Specifically, we present a novel representation for SE(3) which follows the property of smoothness of the pose. We then design a lightweight neural network to regress the pose by developing a differential pose layer. Finally, we introduce a novel loss function and gradient descent method to better supervise the proposed lightweight pose network. Extensive experiments on the camera re-localization task on the Cambridge Landmarks and 7-Scenes datasets demonstrate the superior predictive accuracy and benefits of our method in comparison with the state-of-the-art.},
  archive   = {C_ICRA},
  author    = {Wenjie Li and Jia Liu and Yanyan Wang and Wei Hao and Dayong Ren and Lijun Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611375},
  month     = {5},
  pages     = {16834-16840},
  title     = {DL-PoseNet: A differential lightweight network for pose regression over SE(3)},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NeRF-enhanced outpainting for faithful field-of-view
extrapolation. <em>ICRA</em>, 16826–16833. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In various applications, such as robotic navigation and remote visual assistance, expanding the field of view (FOV) of the camera proves beneficial for enhancing environmental perception. Unlike image outpainting techniques aimed solely at generating aesthetically pleasing visuals, these applications demand an extended view that faithfully represents the scene. To achieve this, we formulate a new problem of faithful FOV extrapolation that utilizes a set of pre-captured images as prior knowledge of the scene. To address this problem, we present a simple yet effective solution called NeRF-Enhanced Outpainting (NEO) that uses extended-FOV images generated through NeRF to train a scene-specific image outpainting model. To assess the performance of NEO, we conduct comprehensive evaluations on three photorealistic datasets and one real-world dataset. Extensive experiments on the benchmark datasets showcase the robustness and potential of our method in addressing this challenge. We believe our work lays a strong foundation for future exploration within the research community.},
  archive   = {C_ICRA},
  author    = {Rui Yu and Jiachen Liu and Zihan Zhou and Sharon X. Huang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611328},
  month     = {5},
  pages     = {16826-16833},
  title     = {NeRF-enhanced outpainting for faithful field-of-view extrapolation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). SLAM based on camera-2D LiDAR fusion. <em>ICRA</em>,
16818–16825. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The SLAM system plays a pivotal role in robotic mapping and localization, leveraging various sensor technologies to achieve precision. Traditional passive sensors, such as RGB cameras, offer high-resolution imagery at a lower cost for SLAM applications, yet they fall short in accurately estimating 3D positions and camera motions. On the other hand, LiDARs excel in generating accurate 3D maps but often come at a higher price and lower resolution. While active illumination sensors like LiDAR provide precise depth estimation, the prohibitive cost of high-resolution LiDAR systems restricts their widespread adoption across diverse applications. Although 2D single-beam LiDAR is more affordable, its limited depth sensing capability hampers comprehensive environmental perception. Addressing these limitations, this paper introduces a deep learning framework aimed at enhancing SLAM performance through the strategic fusion of camera and 2D LiDAR data. Our approach employs a novel self-supervised network alongside an economical single-beam LiDAR, striving to achieve or surpass the performance of more expensive LiDAR systems. The integration of single-beam LiDAR with our system allows for dynamic adjustment of scale uncertainty in depth maps generated by monocular camera systems within SLAM. Consequently, this fusion method enjoys the high-resolution and accuracy benefits of advanced LiDAR systems with the cost-effectiveness of 2D LiDAR sensors. Through this innovative combination, we demonstrate a SLAM system that not only maintains high fidelity in mapping and localization but also ensures affordability and broad applicability.},
  archive   = {C_ICRA},
  author    = {Guoyu Lu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611546},
  month     = {5},
  pages     = {16818-16825},
  title     = {SLAM based on camera-2D LiDAR fusion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Physical priors augmented event-based 3D reconstruction.
<em>ICRA</em>, 16810–16817. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D neural implicit representations play a significant component in many robotic applications. However, reconstructing neural radiance fields (NeRF) from realistic event data remains a challenge due to the sparsities and the lack of information when only event streams are available. In this paper, we utilize motion, geometry, and density priors behind event data to impose strong physical constraints to augment NeRF training. The proposed novel pipeline can directly benefit from those priors to reconstruct 3D scenes without additional inputs. Moreover, we present a novel density-guided patch-based sampling strategy for robust and efficient learning, which not only accelerates training procedures but also conduces to expressions of local geometries. More importantly, we establish the first large dataset for event-based 3D reconstruction, which contains 101 objects with various materials and geometries, along with the groundtruth of images and depth maps for all camera viewpoints, which significantly facilitates other research in the related fields. The code and dataset will be publicly available at https://github.com/Mercerai/PAEv3d.},
  archive   = {C_ICRA},
  author    = {Jiaxu Wang and Junhao He and Ziyi Zhang and Renjing Xu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611153},
  month     = {5},
  pages     = {16810-16817},
  title     = {Physical priors augmented event-based 3D reconstruction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lightning NeRF: Efficient hybrid scene representation for
autonomous driving. <em>ICRA</em>, 16803–16809. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent studies have highlighted the promising application of NeRF in autonomous driving contexts. However, the complexity of outdoor environments, combined with the restricted viewpoints in driving scenarios, complicates the task of precisely reconstructing scene geometry. Such challenges often lead to diminished quality in reconstructions and extended durations for both training and rendering. To tackle these challenges, we present Lightning NeRF. It uses an efficient hybrid scene representation that effectively utilizes the geometry prior from LiDAR in autonomous driving scenarios. Lightning NeRF significantly improves the novel view synthesis performance of NeRF and reduces computational overheads. Through evaluations on real-world datasets, such as KITTI-360, Argoverse2, and our private dataset, we demonstrate that our approach not only exceeds the current state-of-the-art in novel view synthesis quality but also achieves a five-fold increase in training speed and a ten-fold improvement in rendering speed. Codes are available at https://github.com/VISION-SJTU/Lightning-NeRF.},
  archive   = {C_ICRA},
  author    = {Junyi Cao and Zhichao Li and Naiyan Wang and Chao Ma},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611130},
  month     = {5},
  pages     = {16803-16809},
  title     = {Lightning NeRF: Efficient hybrid scene representation for autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From bird’s-eye to street view: Crafting diverse and
condition-aligned images with latent diffusion model. <em>ICRA</em>,
16795–16802. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We explore Bird’s-Eye View (BEV) generation, converting a BEV map into its corresponding multi-view street images. Valued for its unified spatial representation aiding multi-sensor fusion, BEV is pivotal for various autonomous driving applications. Creating accurate street-view images from BEV maps is essential for portraying complex traffic scenarios and enhancing driving algorithms. Concurrently, diffusion-based conditional image generation models have demonstrated remarkable outcomes, adept at producing diverse, high-quality, and condition-aligned results. Nonetheless, the training of these models demands substantial data and computational resources. Hence, exploring methods to fine-tune these advanced models, like Stable Diffusion, for specific conditional generation tasks emerges as a promising avenue. In this paper, we introduce a practical framework for generating images from a BEV layout. Our approach comprises two main components: the Neural View Transformation and the Street Image Generation. The Neural View Transformation phase converts the BEV map into aligned multi-view semantic segmentation maps by learning the shape correspondence between the BEV and perspective views. Subsequently, the Street Image Generation phase utilizes these segmentations as a condition to guide a fine-tuned latent diffusion model. This finetuning process ensures both view and style consistency. Our model leverages the generative capacity of large pretrained diffusion models within traffic contexts, effectively yielding diverse and condition-coherent street view images.},
  archive   = {C_ICRA},
  author    = {Xiaojie Xu and Tianshuo Xu and Fulong Ma and Yingcong Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611235},
  month     = {5},
  pages     = {16795-16802},
  title     = {From bird’s-eye to street view: Crafting diverse and condition-aligned images with latent diffusion model},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GRF-based predictive flocking control with dynamic pattern
formation. <em>ICRA</em>, 16788–16794. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It is promising but challenging to design flocking control for a robot swarm to autonomously follow changing patterns or shapes in a optimal distributed manner. The optimal flocking control with dynamic pattern formation is, therefore, investigated in this paper. A predictive flocking control algorithm is proposed based on a Gibbs random field (GRF), where bio-inspired potential energies are used to charaterize &quot;robot-robot&quot; and &quot;robot-environment&quot; interactions. Specialized performance-related energies, e.g., motion smoothness, are introduced in the proposed design to improve the flocking behaviors. The optimal control is obtained by maximizing a posterior distribution of a GRF. A region-based shape control is accomplished for pattern formation in light of a mean shift technique. The proposed algorithm is evaluated via the comparison with two state-of-the-art flocking control methods in an environment with obstacles. Both numerical simulations and real-world experiments are conducted to demonstrate the efficiency of the proposed design.},
  archive   = {C_ICRA},
  author    = {Chenghao Yu and Dengyu Zhang and Qingrui Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610337},
  month     = {5},
  pages     = {16788-16794},
  title     = {GRF-based predictive flocking control with dynamic pattern formation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ColAG: A collaborative air-ground framework for
perception-limited UGVs’ navigation. <em>ICRA</em>, 16781–16787. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Perception is necessary for autonomous navigation in an unknown area crowded with obstacles. It’s challenging for a robot to navigate safely without any sensors that can sense the environment, resulting in a blind robot, and becomes more difficult when comes to a group of robots. However, it could be costly to equip all robots with expensive perception or SLAM systems. In this paper, we propose a novel system named ColAG, to solve the problem of autonomous navigation for a group of blind UGVs by introducing cooperation with one UAV, which is the only robot that has full perception capabilities in the group. The UAV uses SLAM for its odometry and mapping while sharing this information with UGVs via limited relative pose estimation. The UGVs plan their trajectories in the received map and predict possible failures caused by the uncertainty of its wheel odometry and unknown risky areas. The UAV dynamically schedules waypoints to prevent UGVs from collisions, formulated as a Vehicle Routing Problem with Time Windows to optimize the UAV’s trajectories and minimize time when UGVs have to wait to guarantee safety. We validate our system through extensive simulation with up to 7 UGVs and real-world experiments with 3 UGVs.},
  archive   = {C_ICRA},
  author    = {Zhehan Li and Rui Mao and Nanhe Chen and Chao Xu and Fei Gao and Yanjun Cao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611264},
  month     = {5},
  pages     = {16781-16787},
  title     = {ColAG: A collaborative air-ground framework for perception-limited UGVs’ navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Collaborative manipulation of deformable objects with
predictive obstacle avoidance. <em>ICRA</em>, 16766–16772. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Manipulating deformable objects arises in daily life and numerous applications. Despite phenomenal advances in industrial robotics, manipulation of deformable objects remains mostly a manual task. This is because of the high number of internal degrees of freedom and the complexity of predicting its motion. In this paper, we apply the computationally efficient position-based dynamics method to predict object motion and distance to obstacles. This distance is incorporated in a control barrier function for the resolved motion kinematic control for one or more robots to adjust their motion to avoid colliding with the obstacles. The controller has been applied in simulations to 1D and 2D deformable objects with varying numbers of assistant agents, demonstrating its versatility across different object types and multi-agent systems. Results indicate the feasibility of real-time collision avoidance through deformable object simulation, minimizing path tracking error while maintaining a predefined minimum distance from obstacles and preventing overstretching of the deformable object. The implementation is performed in ROS, allowing ready portability to different applications.},
  archive   = {C_ICRA},
  author    = {Burak Aksoy and John T. Wen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609995},
  month     = {5},
  pages     = {16766-16772},
  title     = {Collaborative manipulation of deformable objects with predictive obstacle avoidance},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WiBot 1.0: A modular reconfigurable glass cleaning robot for
high-rise buildings. <em>ICRA</em>, 16759–16765. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cleaning glass surfaces is a prevailing maintenance problem in high-rise buildings. In the traditional methods of cleaning windows, hanging on ropes poses significant occupational hazards to workers. Furthermore, most glass facades feature window frames to securely fasten the glass panels to the building structure, ensuring durability and elegance. In this context, existing robotic cleaning methods are limited by their capability to move-over window frames and need more flexibility to access tight corners and curved surfaces. This paper presents a novel reconfigurable glass cleaning robot called &quot;WiBot&quot; to address these limitations. WiBot is a kinematic chain comprising modular linkages with a prismatic joint and two revolute joints at each end. Each revolute joint has a suction unit that enables locomotion and adhesion. Window frames are detected using image processing with an onboard camera, and design optimizations were performed to improve the robot’s capabilities. The prototype WiBot 1.0 was developed, and several experiments were conducted to evaluate the feasibility of the proposed system focusing on robot motion, window frame detection and move-over mechanism. The results show that WiBot can overcome the limitations of existing window cleaning solutions. Finally, several promising research directions are mentioned involving the proposed reconfigurable robot architecture in cleaning operations.},
  archive   = {C_ICRA},
  author    = {S. A. Kariyawasam and G. H. Sandeepa and M. K. A. Pathirana and Y. W. R. Amarasinghe and A. G. B. P. Jayasekara and H. A. G. C. Premachandra and U-Xuan Tan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610253},
  month     = {5},
  pages     = {16759-16765},
  title     = {WiBot 1.0: A modular reconfigurable glass cleaning robot for high-rise buildings},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing modular robot composition: A lexicographic
genetic algorithm approach. <em>ICRA</em>, 16752–16758. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Industrial robots are designed as general-purpose hardware with limited ability to adapt to changing task requirements or environments. Modular robots, on the other hand, offer flexibility and can be easily customized to suit diverse needs. The morphology, i.e., the form and structure of a robot, significantly impacts the primary performance metrics acquisition cost, cycle time, and energy efficiency. However, identifying an optimal module composition for a specific task remains an open problem, presenting a substantial hurdle in developing task-tailored modular robots. Previous approaches either lack adequate exploration of the design space or the possibility to adapt to complex tasks. We propose combining a genetic algorithm with a lexicographic evaluation of solution candidates to overcome this problem and navigate search spaces exceeding those in prior work by magnitudes in the number of possible compositions. We demonstrate that our approach outperforms a state-of-the-art baseline and is able to synthesize modular robots for industrial tasks in cluttered environments.},
  archive   = {C_ICRA},
  author    = {Jonathan Külz and Matthias Althoff},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609979},
  month     = {5},
  pages     = {16752-16758},
  title     = {Optimizing modular robot composition: A lexicographic genetic algorithm approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ArrayBot: Reinforcement learning for generalizable
distributed manipulation through touch. <em>ICRA</em>, 16744–16751. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present ArrayBot, a distributed manipulation system consisting of a 16 × 16 array of vertically sliding pillars integrated with tactile sensors. Functionally, ArrayBot is designed to simultaneously support, perceive, and manipulate the tabletop objects. Towards generalizable distributed manipulation, we leverage reinforcement learning (RL) algorithms for the automatic discovery of control policies. In the face of the massively redundant actions, we propose to reshape the action space by considering the spatially local action patch and the low-frequency actions in the frequency domain. With this reshaped action space, we train RL agents that can relocate diverse objects through tactile observations only. Intriguingly, we find that the discovered policy can not only generalize to unseen object shapes in the simulator but also have the ability to transfer to the physical robot without any sim-to-real fine-tuning. |Leveraging the deployed policy, we derive more real-world manipulation skills on ArrayBot to further illustrate the distinctive merits of our proposed system.},
  archive   = {C_ICRA},
  author    = {Zhengrong Xue and Han Zhang and Jingwen Cheng and Zhengmao He and Yuanchen Ju and Changyi Lin and Gu Zhang and Huazhe Xu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610350},
  month     = {5},
  pages     = {16744-16751},
  title     = {ArrayBot: Reinforcement learning for generalizable distributed manipulation through touch},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LB-R2R-calib: Accurate and robust extrinsic calibration of
multiple long baseline 4D imaging radars for V2X. <em>ICRA</em>,
16729–16735. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As a new sensor, 4D radar (x, y, z, velocity) has great potential for V2X, due to its 3D point cloud, direct doppler velocity output, long distance ranging, low-cost, and more importantly, robust perception in all weathers. However, the extrinsic calibration of multiple long baseline 4D radars is rarely researched in V2X, which is the key to fuse multi-radars. The main reasons are three-folds: (1) New sensor. Thus, it is not surprising that little related work can be found. (2) Long baseline and large viewpoint-difference. Current works are mainly focused on unmanned vehicles, which is short baseline and small viewpoint-difference. (3) Sparse, noisy, and very cluttered 4D radar point cloud. Thus, it is challenging to rapidly and accurately locate the target and extract the feature. In this paper, LB-R2R-Calib (Long Baseline Radar to Radar extrinsic Calibration) is proposed to address these problems. The novelties are: (1) A new target is introduced: an eight-quadrant corner reflector enclosed by a foam sphere. The benefit is the target center is a viewpoint-invariant feature. Thus, it is ideal for large viewpoint-difference calibration. (2) A new feature extraction algorithm is proposed to rapidly locate the target and extract the target center from a very cluttered point cloud, as we observed some important characteristics of 4D radar. Experiments with two 4D radars in real environments with four configurations demonstrate our method is highly accurate and robust.},
  archive   = {C_ICRA},
  author    = {Jun Zhang and Zihan Yang and Fangwei Zhang and Zhenyu Wu and Guohao Peng and Yiyao Liu and Qiyang Lyu and Mingxing Wen and Danwei Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611470},
  month     = {5},
  pages     = {16729-16735},
  title     = {LB-R2R-calib: Accurate and robust extrinsic calibration of multiple long baseline 4D imaging radars for V2X},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CalibFormer: A transformer-based automatic LiDAR-camera
calibration network. <em>ICRA</em>, 16714–16720. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The fusion of LiDARs and cameras has been increasingly adopted in autonomous driving for perception tasks. The performance of such fusion-based algorithms largely depends on the accuracy of sensor calibration, which is challenging due to the difficulty of identifying common features across different data modalities. Previously, many calibration methods involved specific targets and/or manual intervention, which has proven to be cumbersome and costly. Learning-based online calibration methods have been proposed, but their performance is barely satisfactory in most cases. These methods usually suffer from issues such as sparse feature maps, unreliable cross-modality association, inaccurate calibration parameter regression, etc. In this paper, to address these issues, we propose CalibFormer, an end-to-end network for automatic LiDAR-camera calibration. We aggregate multiple layers of camera and LiDAR image features to achieve high-resolution representations. A multi-head correlation module is utilized to identify correlations between features more accurately. Lastly, we employ transformer architectures to estimate accurate calibration parameters from the correlation information. Our method achieved a mean translation error of 0.8751cm and a mean rotation error of 0.0562° on the KITTI dataset, surpassing existing state-of-the-art methods and demonstrating strong robustness, accuracy, and generalization capabilities.},
  archive   = {C_ICRA},
  author    = {Yuxuan Xiao and Yao Li and Chengzhen Meng and Xingchen Li and Jianmin Ji and Yanyong Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610018},
  month     = {5},
  pages     = {16714-16720},
  title     = {CalibFormer: A transformer-based automatic LiDAR-camera calibration network},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). GBEC: Geometry-based hand-eye calibration. <em>ICRA</em>,
16698–16705. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hand-eye calibration is the problem of solving the transformation from the end-effector of a robot to the sensor attached to it. Commonly employed techniques, such as AXXB or AXZB formulations, rely on regression methods that require collecting pose data from different robot configurations, which can produce low accuracy and repeatability. However, the derived transformation should solely depend on the geometry of the end-effector and the sensor attachment. We propose Geometry-Based End-Effector Calibration (GBEC) that enhances the repeatability and accuracy of the derived transformation compared to traditional hand-eye calibrations. To demonstrate improvements, we apply the approach to two different robot-assisted procedures: Transcranial Magnetic Stimulation (TMS) and femoroplasty. We also discuss the generalizability of GBEC for camera-in-hand and marker-in-hand sensor mounting methods. In the experiments, we perform GBEC between the robot end-effector and an optical tracker’s rigid body marker attached to the TMS coil or femoroplasty drill guide. Previous research documents low repeatability and accuracy of the conventional methods for robot-assisted TMS hand-eye calibration. Applying GBEC to repeated calibrations, we obtain transformations with standard deviations of 0.37mm, 0.65mm, and 0.40mm (translation) along x, y, and z axes of the end-effector, respectively. The tool alignment experiments after using GBEC achieve a mean accuracy around 0.2mm in Euclidean distance. When compared to some existing methods, the proposed method relies solely on the geometry of the flange and the pose of the rigid-body marker, making it independent of workspace constraints or robot accuracy, without sacrificing the orthogonality of the rotation matrix. Our results validate the accuracy and applicability of the approach, providing a new and generalizable methodology for obtaining the transformation from the end-effector to a sensor.},
  archive   = {C_ICRA},
  author    = {Yihao Liu and Jiaming Zhang and Zhangcong She and Amir Kheradmand and Mehran Armand},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611550},
  month     = {5},
  pages     = {16698-16705},
  title     = {GBEC: Geometry-based hand-eye calibration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LiDAR-camera extrinsic calibration with hierachical and
iterative feature matching. <em>ICRA</em>, 16691–16697. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In autonomous driving, the LiDAR-Camera system plays a crucial role in a vehicle’s perception of 3D environments. To effectively fuse information from both camera and LiDAR, extrinsic calibration is indispensable. Recently, some researchers have proposed deep learning-based methods that utilize convolutional networks to automatically extract features from LiDAR depth images and RGB images for calibration. However, these features do not sufficiently interact during feature matching, which limits the calibration accuracy. To this end, we introduce a novel extrinsic calibration network (HIFM-Net) in this paper. It establishes a comprehensive connection between camera and LiDAR features by calculating a globally-aware map-to-map cost volume and hierachical point-to-map cost volumes. The former is used to regress large extrinsic offsets. The latter is employed to iteratively fine-tune extrinsic parameters, while the rigidity of LiDAR points is considered in each iteration to enhance regression robustness. Extensive experiments on the KITTI-odometry dataset demonstrate the superior performance of our HIFMNet compared to other state-of-the-art learning-based methods.},
  archive   = {C_ICRA},
  author    = {Xuzhong Hu and Zaipeng Duan and Junfeng Ding and Zhe Zhang and Xiao Huang and Jie Ma},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611119},
  month     = {5},
  pages     = {16691-16697},
  title     = {LiDAR-camera extrinsic calibration with hierachical and iterative feature matching},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating material properties of interacting objects using
sum-GP-UCB. <em>ICRA</em>, 16684–16690. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots need to estimate the material and dynamic properties of objects from observations in order to simulate them accurately. We present a Bayesian optimization approach to identifying the material property parameters of objects based on a set of observations. Our focus is on estimating these properties based on observations of scenes with different sets of interacting objects. We propose an approach that exploits the structure of the reward function by modeling the reward for each observation separately and using only the parameters of the objects in that scene as inputs. The resulting lower-dimensional models generalize better over the parameter space, which in turn results in a faster optimization. To speed up the optimization process further, and reduce the number of simulation runs needed to find good parameter values, we also propose partial evaluations of the reward function, wherein the selected parameters are only evaluated on a subset of real world evaluations. The approach was successfully evaluated on a set of scenes with a wide range of object interactions, and we showed that our method can effectively perform incremental learning without resetting the rewards of the gathered observations.},
  archive   = {C_ICRA},
  author    = {M. Yunus Seker and Oliver Kroemer},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610129},
  month     = {5},
  pages     = {16684-16690},
  title     = {Estimating material properties of interacting objects using sum-GP-UCB},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LCCRAFT: LiDAR and camera calibration using recurrent
all-pairs field transforms without precise initial guess. <em>ICRA</em>,
16669–16675. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {LiDAR-camera fusion plays a pivotal role in 3D reconstruction for self-driving applications. A fundamental prerequisite for effective fusion is the precise calibration between LiDAR and camera systems. Many existing calibration methods are constrained by predefined mis-calibration ranges in the training data, essentially tying the network to a specific data distribution. However, if the range of evaluation data differs from what the network has been trained on, the resulting estimates may not meet expectations. Moreover, most methods require a precise initial guess for calibration to succeed. In this paper, we introduce LCCRAFT, an online calibration network designed for LiDAR and camera systems. Leveraging the 4D correlation volume and correlation lookup techniques inherited from RAFT, we apply them to correlate RGB images and depth maps derived from the projection of point clouds. Through weight sharing between update iterations and by enabling the update operator to learn from data with varying degrees of error, LCCRAFT demonstrates adaptability to diverse miscalibration scenarios. This includes cases where the initial mis- calibration is even more severe than what the system encountered during training, demonstrating the robustness of the model. The calibration process executes in 93ms on a single GPU, meeting real-time requirements. Despite the modest 9M model parameters, LCCRAFT achieves competitive performance as compared to the state-of-the-art method, which entails 69M parameters.},
  archive   = {C_ICRA},
  author    = {Yu-Chen Lee and Kuan-Wen Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610756},
  month     = {5},
  pages     = {16669-16675},
  title     = {LCCRAFT: LiDAR and camera calibration using recurrent all-pairs field transforms without precise initial guess},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DyHGDAT: Dynamic hypergraph dual attention network for
multi-agent trajectory prediction. <em>ICRA</em>, 16662–16668. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modeling the interactions among agents based on their historical trajectories is key to precise multi-agent trajectory prediction. Hypergraph Convolutional Networks (HGCN) have become a proper choice for capturing high-order interactions among agents in this field. However, most existing works only consider static hypergraphs, and ignore that in a hypergraph, the power of influence varies between vertices (or hyperedges). Therefore, we propose DyHGDAT, a dynamic hypergraph dual attention network to capture the high-order interactions among agents, which not only models the evolution of hypergraph over time but also highlights the vertices and hyperedges with larger impacts. We apply DyHGDAT to a CVAE-based prediction system for predicting plausible trajectories. To validate the effectiveness of prediction, we evaluate our proposed method on two well-established trajectory prediction datasets: the ETH/UCY datasets and the Stanford Drone Dataset (SDD). The experimental results show that with DyHGDAT, the CVAE-based prediction system out-performs state-of-the-art methods by 12.5%/5.3% in ADE/FDE on ETH/UCY, and the improvement on SDD is 6.4%/7.4%.},
  archive   = {C_ICRA},
  author    = {Weilong Lin and Xinhua Zeng and Chengxin Pang and Jing Teng and Jing Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609870},
  month     = {5},
  pages     = {16662-16668},
  title     = {DyHGDAT: Dynamic hypergraph dual attention network for multi-agent trajectory prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LLM-BT: Performing robotic adaptive tasks based on large
language models and behavior trees. <em>ICRA</em>, 16655–16661. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large Language Models (LLMs) have been widely utilized to perform complex robotic tasks. However, handling external disturbances during tasks is still an open challenge. This paper proposes a novel method to achieve robotic adaptive tasks based on LLMs and Behavior Trees (BTs). It utilizes ChatGPT to reason the descriptive steps of tasks. In order to enable ChatGPT to understand the environment, semantic maps are constructed by an object recognition algorithm. Then, we design a Parser module based on Bidirectional Encoder Representations from Transformers (BERT) to parse these steps into initial BTs. Subsequently, a BTs Update algorithm is proposed to expand the initial BTs dynamically to control robots to perform adaptive tasks. Different from other LLM-based methods for complex robotic tasks, our method outputs variable BTs that can add and execute new actions according to environmental changes, which is robust to external disturbances. Our method is validated with simulation in different practical scenarios.},
  archive   = {C_ICRA},
  author    = {Haotian Zhou and Yunhan Lin and Longwu Yan and Jihong Zhu and Huasong Min},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610183},
  month     = {5},
  pages     = {16655-16661},
  title     = {LLM-BT: Performing robotic adaptive tasks based on large language models and behavior trees},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AutoExplorers: Autoencoder-based strategies for high-entropy
exploration in unknown environments for mobile robots. <em>ICRA</em>,
16648–16654. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deciding where to go next is a challenging task for humans. However, for robots in unknown environments, this becomes even more demanding. In planetary explorations, the robots are continuously challenged with the task of exploring novel areas, yet so far, humans decide for the robots where to go. Even then, prioritizing the next target based on previous knowledge is complex. In our proposed work, the robot utilizes data about its surroundings from drone or satellite images. Alternatively, a volumetric representation can be reduced to form a suitable input. From the input, tiles are selected and embedded by different autoencoder variants. The robot can select the most promising next exploration goal through the distance in the embedding to the previous samples. In this work, a variational autoencoder, a Wasserstein autoencoder, and a spherical autoencoder are evaluated against each other. The latter two variants yield a high information gain when evaluated on satellite data from the Netherlands. Additionally, the framework was employed on data from an analog mission in the Tabernas desert. Through the framework, the robots get an understanding of which goals yield the most information gain and, therefore, can quickly improve their knowledge about their surroundings.},
  archive   = {C_ICRA},
  author    = {Lennart Puck and Maximilian Schik and Tristan Schnell and Timothée Buettner and Arne Roennau and Rüdiger Dillmann},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610693},
  month     = {5},
  pages     = {16648-16654},
  title     = {AutoExplorers: Autoencoder-based strategies for high-entropy exploration in unknown environments for mobile robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Convolutional vision transformer as a path following
controller for omnidirectional robots. <em>ICRA</em>, 16633–16639. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A novel deep neural network (DNN) based controller for omnidirectional robots is proposed. The controller decomposes the prescribed reference path, corresponding to a fixed prediction horizon, into multiple paths of shorter horizons. This implicitly enforces a Hankel structure in the input and consequently also on the output. Taking advantage of this, a convolutional vision transformer model is used to realize the controller which is then trained to predict state and controls over multiple prediction horizons. Model training is performed in a self-supervised manner using a synthetic dataset. The proposed controller is shown to be more efficient than a model designed for a single prediction horizon. In comparison to a model predictive controller, the proposed approach exhibits competitive performance in path following tasks and is three times faster on average for the same prediction length.},
  archive   = {C_ICRA},
  author    = {Sandesh Hiremath and Cheng-Yi Huang and Argtim Tika and Naim Bajcinca},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610893},
  month     = {5},
  pages     = {16633-16639},
  title     = {Convolutional vision transformer as a path following controller for omnidirectional robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive pedestrian agent modeling for scenario-based
testing of autonomous vehicles through behavior retargeting.
<em>ICRA</em>, 16595–16602. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work proposes a new representation of pedestrian crossing scenarios and a hybrid modeling approach, RePed, that facilitates transferring microscopic behavior models from behavior research to higher-level trajectories. With this, real-world trajectory-based scenarios can be augmented with a diverse set of human crossing maneuvers, producing a wealth of new scenarios and addressing the scarcity of rare case data that existing works struggle to deal with. Leveraging the controllability of this modeling approach, perturbation-based augmentation can be applied to enrich scenarios further. In addition, the representation is rooted in the Ego vehicle’s coordinate system with a logical representation of roads. This design enables scenario retargeting to various road structures, traffic conditions, and ego vehicle behaviors. Thus, it strongly supports scenario-based testing by forcing pedestrians to produce certain situations in simulation even when the Ego Vehicle tries to evade them.},
  archive   = {C_ICRA},
  author    = {Golam Md Muktadir and Jim Whitehead},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610078},
  month     = {5},
  pages     = {16595-16602},
  title     = {Adaptive pedestrian agent modeling for scenario-based testing of autonomous vehicles through behavior retargeting},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Accounting for travel time and arrival time coordination
during task allocations in legged-robot teams. <em>ICRA</em>,
16588–16594. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many applications require the deployment of legged-robot teams to effectively and efficiently carry out missions. The use of multiple robots allows tasks to be executed concurrently, expediting mission completion. It also enhances resilience by enabling task transfer in case of a robot failure. This paper presents a formulation based on Mixed Integer Linear Programming (MILP) for allocating tasks to robots by taking into account travel time and ensuring efficient execution of collaborative tasks. We extended the MILP formulation to account for complexities with legged robot teams. Our results demonstrate that this approach leads to improved performance in terms of the makespan of the mission. We demonstrate the usefulness of this approach using a case study involving the disinfection of a building consisting of multiple rooms.},
  archive   = {C_ICRA},
  author    = {Shengqiang Chen and Yiyu Chen and Ronak Jain and Xiaopan Zhang and Quan Nguyen and Satyandra K. Gupta},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610869},
  month     = {5},
  pages     = {16588-16594},
  title     = {Accounting for travel time and arrival time coordination during task allocations in legged-robot teams},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Traffic flow learning enhanced large-scale multi-robot
cooperative path planning under uncertainties. <em>ICRA</em>,
16581–16587. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic systems with hundreds or even thousands of robots are widely implemented in logistic and industrial applications. In such systems, cooperative path planning is of great importance, as local congestion and motion conflict may greatly degrade system performance, especially in the presence of uncertainties. Our idea is to consider traffic flow equilibrium in path planning to relieve any potential congestion and increase efficiency. In this paper, we propose a hierarchical framework, which includes a traffic flow prediction layer, a sector-level planning layer, and a road-level coordination layer. In traffic flow prediction, we propose a spatio-temporal graph neural network that integrates local information to predict the evolution of future robot density distribution. In sector-level planning, we generate sector-level paths that consider travel distance and traffic flow equilibrium simultaneously. In road-level coordination, we implement the conflict-based search algorithm within each sector to ensure conflict-free local paths. In addition, we also explicitly consider motion/communication uncertainties that are unavoided in practical systems. We validate our effectiveness in simulations with over 1000 robots, what’s more, real experiments are provided.},
  archive   = {C_ICRA},
  author    = {Xingyao Han and Siyuan Chen and Xinye Xiong and Qiming Liu and Shunbo Zhou and Heng Zhang and Zhe Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610592},
  month     = {5},
  pages     = {16581-16587},
  title     = {Traffic flow learning enhanced large-scale multi-robot cooperative path planning under uncertainties},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-robot task allocation under uncertainty via hindsight
optimization. <em>ICRA</em>, 16574–16580. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-robot systems are becoming increasingly prevalent in various real-world applications, such as manufacturing and warehouse logistics. These systems face complex challenges in 1) task allocation due to factors like time-extended tasks, and agent specialization, and 2) uncertainties in task execution. Potential task failures can add further contingency tasks to recover from the failure, thereby causing delays. This paper addresses the problem of Multi-Robot Task Allocation under Uncertainty by proposing a hierarchical approach that decouples the problem into two levels. We use a low-level optimization formulation to find the optimal solution for a deterministic multi-robot task allocation problem with known task outcomes. The higher-level search intelligently generates more likely combinations of failures and calls the inner-level search repeatedly to find the optimal task allocation sequence, given the known outcomes. We validate our results in simulation for a manufacturing domain and demonstrate that our method can reduce the effect of potential delays from contingencies. We show that our algorithm is computationally efficient while improving average makespan compared to other baselines.},
  archive   = {C_ICRA},
  author    = {Neel Dhanaraj and Jeon Ho Kang and Anirban Mukherjee and Heramb Nemlekar and Stefanos Nikolaidis and Satyandra K. Gupta},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611370},
  month     = {5},
  pages     = {16574-16580},
  title     = {Multi-robot task allocation under uncertainty via hindsight optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic coalition formation and routing for multirobot task
allocation via reinforcement learning. <em>ICRA</em>, 16567–16573. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many multi-robot deployments, such as automated construction of buildings, distributed search, or cooperative mapping, often require agents to intelligently coordinate their trajectories and form coalition over a large domain, to complete spatially distributed tasks as quickly as possible. We focus on scenarios involving homogeneous robots, but where tasks vary in the number of agents required to start them. For example, construction robots may need to collaboratively air-lift heavy objects at different locations (e.g., prefabricated rooms, crates of material/equipment), where the weight of each payload defines the required coalition size. To balance the total travel time of the agents and their waiting time (before task initiation), agents need to carefully sequence tasks but also dynamically form/disband coalitions. While simpler problems can be approached using heuristics or optimization, these methods struggle with more complex instances involving large task-to-agent ratios, where frequent coalition changes are needed. In this work, we propose to let agents learn to iteratively build cooperative schedules to solve such problems, by casting the problem in the reinforcement learning framework. Our approach relies on an attention-based neural network, allowing agents to reason about the current state of the system to sequence movement decisions that optimize short-term coalition formation and long-term task scheduling. We further propose a novel leader-follower technique to boost cooperation learning and compare our performance to conventional baselines in a wide variety of scenarios. There, our method closely matches or outperforms the baselines; in particular, it yields higher-quality solutions and is at least 2 orders of magnitude faster than exact solver in cases where frequent coalition updates are required.},
  archive   = {C_ICRA},
  author    = {Weiheng Dai and Aditya Bidwai and Guillaume Sartoretti},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611244},
  month     = {5},
  pages     = {16567-16573},
  title     = {Dynamic coalition formation and routing for multirobot task allocation via reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Well-connected set and its application to multi-robot path
planning. <em>ICRA</em>, 16560–16566. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Parking lots and autonomous warehouses for accommodating many vehicles/robots adopt designs in which the underlying graphs are well-connected to simplify planning and reduce congestion. In this study, we formulate and delve into the largest well-connected set (LWCS) problem and explore its applications in layout design for multi-robot path planning. Roughly speaking, a well-connected set over a connected graph is a set of vertices such that there is a path on the graph connecting any pair of vertices in the set without passing through any additional vertices of the set. Identifying an LWCS has many potential high-utility applications, e.g., for determining parking garage layout and capacity, as prioritized planning can be shown to be complete when start/goal configurations belong to an LWCS. In this work, we establish that computing an LWCS is NP-complete. We further develop optimal and near-optimal LWCS algorithms, with the near-optimal algorithm targeting large maps. A complete prioritized planning method is given for planning paths for multiple robots residing on an LWCS.},
  archive   = {C_ICRA},
  author    = {Teng Guo and Jingjin Yu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610741},
  month     = {5},
  pages     = {16560-16566},
  title     = {Well-connected set and its application to multi-robot path planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tree-based representation of locally shortest paths for 2D
k-shortest non-homotopic path planning. <em>ICRA</em>, 16553–16559. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A novel algorithm to solve the 2D k-shortest non-homotopic path planning (k-SNPP) task is proposed in this paper. The task is of practical significance as a sub-module for higherlevel planning and scheduling tasks, and is gaining increasing attention and focus in recent years. There have existed algorithms that explicitly characterised non-homotopic paths using topological invariants such as ℎ-signature and winding number. However, these algorithms are inefficient due to their separate treatment of topology and geometry: Topological invariants are singularly utilised for distinguishing non-homotopic property among paths, which significantly increases the volume of the robot configuration space. Meanwhile, distance-optimal path planners search for locally shortest paths in the augmented space, which becomes extremely time-consuming. In this paper, a topological tree is proposed to simultaneously leverage topology and geometry. The tree grows from the starting location and explores all topological routes, until the best k of its leaves reach the goal. It is proven that different branches of the tree explore different homotopy classes of paths, and all the branches are locally shortest. Comparative experiments for k-SNPP are conducted in challenging grid-based simulated environments to validate the performance of the proposed algorithm. The C++ implementation of the proposed algorithm is released for the benefit of the robotics community.},
  archive   = {C_ICRA},
  author    = {Tong Yang and Li Huang and Yue Wang and Rong Xiong},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610073},
  month     = {5},
  pages     = {16553-16559},
  title     = {Tree-based representation of locally shortest paths for 2D k-shortest non-homotopic path planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic crane scheduling with reinforcement learning for a
steel coil warehouse. <em>ICRA</em>, 16545–16552. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper tackles the dynamic crane scheduling problem in a steel coil warehouse, involving tasks such as coil storage, retrieval, and shuffling. Tasks arrive dynamically with precedence relations, while multiple cranes share a track, necessitating collision avoidance. We aim to minimize the average task waiting time by allocating tasks to cranes and optimizing their execution sequence. Unlike prior research focusing on static scenarios or rule-based heuristics, we introduce a real-time, reinforcement learning-based algorithm. We propose a policy network based on graph neural networks to effectively handle precedence relations and global information. Experimental results demonstrate its superiority over traditional heuristics, such as dispatching rules in dynamic scenarios.},
  archive   = {C_ICRA},
  author    = {Sang-Hyun Cho and Woo-Jin Shin and Jeongsun Ahn and Sanghyun Joo and Hyun-Jung Kim},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610858},
  month     = {5},
  pages     = {16545-16552},
  title     = {Dynamic crane scheduling with reinforcement learning for a steel coil warehouse},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scaling team coordination on graphs with reinforcement
learning. <em>ICRA</em>, 16538–16544. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies Reinforcement Learning (RL) techniques to enable team coordination behaviors in graph environments with support actions among teammates to reduce the costs of traversing certain risky edges in a centralized manner. While classical approaches can solve this non-standard multi-agent path planning problem by converting the original Environment Graph (EG) into a Joint State Graph (JSG) to implicitly incorporate the support actions, those methods do not scale well to large graphs and teams. To address this curse of dimensionality, we propose to use RL to enable agents to learn such graph traversal and teammate supporting behaviors in a data-driven manner. Specifically, through a new formulation of the team coordination on graphs with risky edges problem into Markov Decision Processes (MDPs) with a novel state and action space, we investigate how RL can solve it in two paradigms: First, we use RL for a team of agents to learn how to coordinate and reach the goal with minimal cost on a single EG. We show that RL efficiently solves problems with up to 20/4 or 25/3 nodes/agents, using a fraction of the time needed for JSG to solve such complex problems; Second, we learn a general RL policy for any N-node EGs to produce efficient supporting behaviors. We present extensive experiments and compare our RL approaches against their classical counterparts.},
  archive   = {C_ICRA},
  author    = {Manshi Limbu and Zechen Hu and Xuan Wang and Daigo Shishika and Xuesu Xiao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610619},
  month     = {5},
  pages     = {16538-16544},
  title     = {Scaling team coordination on graphs with reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quadcopter trajectory time minimization and robust collision
avoidance via optimal time allocation. <em>ICRA</em>, 16531–16537. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous navigation requires robots to generate trajectories for collision avoidance efficiently. Although plenty of previous works have proven successful in generating smooth and spatially collision-free trajectories, their solutions often suffer from suboptimal time efficiency and potential un-safety, particularly when accounting for uncertainties in robot perception and control. To address this issue, this paper presents the Robust Optimal Time Allocation (ROTA) framework. This framework is designed to optimize the time progress of the trajectories temporally, serving as a post-processing tool to enhance trajectory time efficiency and safety under uncertainties. In this study, we begin by formulating a non-convex optimization problem aimed at minimizing trajectory execution time while incorporating constraints on collision probability as the robot approaches obstacles. Subsequently, we introduce the concept of the trajectory braking zone and adopt the chance-constrained formulation for robust collision avoidance in the braking zones. Finally, the non-convex optimization problem is reformulated into a second-order cone programming problem to achieve real-time performance. Through simulations and physical flight experiments, we demonstrate that the proposed approach effectively reduces trajectory execution time while enabling robust collision avoidance in complex environments. Our software 1 is available on GitHub, along with the developed autonomy framework 2 , as open-source ROS packages.},
  archive   = {C_ICRA},
  author    = {Zhefan Xu and Kenji Shimada},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610290},
  month     = {5},
  pages     = {16531-16537},
  title     = {Quadcopter trajectory time minimization and robust collision avoidance via optimal time allocation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Subgoal diffuser: Coarse-to-fine subgoal generation to
guide model predictive control for robot manipulation. <em>ICRA</em>,
16489–16495. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Manipulation of articulated and deformable objects can be difficult due to their compliant and under-actuated nature. Unexpected disturbances can cause the object to deviate from a predicted state, making it necessary to use Model-Predictive Control (MPC) methods to plan motion. However, these methods need a short planning horizon to be practical. Thus, MPC is ill-suited for long-horizon manipulation tasks due to local minima. In this paper, we present a diffusion-based method that guides an MPC method to accomplish long-horizon manipulation tasks by dynamically specifying sequences of subgoals for the MPC to follow. Our method, called Subgoal Diffuser, generates subgoals in a coarse-to-fine manner, producing sparse subgoals when the task is easily accomplished by MPC and more dense subgoals when the MPC method needs more guidance. The density of subgoals is determined dynamically based on a learned estimate of reachability, and subgoals are distributed to focus on challenging parts of the task. We evaluate our method on two robot manipulation tasks and find it improves the planning performance of an MPC method, and also outperforms prior diffusion-based methods. More visualizations and results can be found at https://sites.google.com/view/subgoal-diffuser-mpc},
  archive   = {C_ICRA},
  author    = {Zixuan Huang and Yating Lin and Fan Yang and Dmitry Berenson},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610189},
  month     = {5},
  pages     = {16489-16495},
  title     = {Subgoal diffuser: Coarse-to-fine subgoal generation to guide model predictive control for robot manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning-based motion planning with mixture density
networks. <em>ICRA</em>, 16482–16488. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The trade-off between computation time and path optimality is a key consideration in motion planning algorithms. While classical sampling based algorithms fall short of computational efficiency in high dimensional planning, learning based methods have shown great potential in achieving time efficient and optimal motion planning. The SOTA learning based motion planning algorithms utilize paths generated by sampling based methods as expert supervision data and train networks via regression techniques. However, these methods often overlook the important multimodal property of the optimal paths in the training set, making them incapable of finding good paths in some scenarios. In this paper, we propose a Multimodal Neuron Planner (MNP) based on the mixture density networks that explicitly takes into account the multimodality of the training data and simultaneously achieves time efficiency and path optimality. For environments represented by point clouds, MNP first efficiently compresses point clouds into a latent vector by encoding networks that are suitable for processing point clouds. We then design multimodal planning networks which enables MNP to learn and predict multiple optimal solutions. Simulation results show that our method outperforms SOTA learning based method MPNet and advanced sampling based methods IRRT* and BIT*.},
  archive   = {C_ICRA},
  author    = {Yinghan Wang and Xiaoming Duan and Jianping He},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610881},
  month     = {5},
  pages     = {16482-16488},
  title     = {Learning-based motion planning with mixture density networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mitigating causal confusion in vector-based behavior cloning
for safer autonomous planning. <em>ICRA</em>, 16475–16481. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The utilization of vector-based deep learning techniques has great prospects in the realm of autonomous driving, particularly in the domains of prediction and planning tasks. However, the application of vector-based backbones for prediction and planning tasks may lead to the occurrence of causal confusion. Previous studies have explored the phenomenon of causal confusion, with a specific emphasis on the context of visual imitation learning. As for the vector-based model, we observe that the states of surrounding vehicles can be a nuisance shortcut. In our work, an off-policy approach is proposed to alleviate the issue by incorporating de-confounding supervision. Additionally, to better capture the environmental cues, such as route and traffic lights, in vectorized representation, a decoder utilizing iterative route fusion is devised. By incorporating auxiliary supervision and employing a dedicated decoder, we demonstrate the effectiveness of our methods in reducing causal confusion and improving performance in planning tasks through reactive and nonreactive closed-loop simulations on the nuPlan dataset.},
  archive   = {C_ICRA},
  author    = {Jiayu Guo and Mingyue Feng and Pengfei Zhu and Jinsheng Dou and Di Feng and Chengjun Li and Ru Wan and Jian Pu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610470},
  month     = {5},
  pages     = {16475-16481},
  title     = {Mitigating causal confusion in vector-based behavior cloning for safer autonomous planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Motion memory: Leveraging past experiences to accelerate
future motion planning. <em>ICRA</em>, 16467–16474. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When facing a new motion-planning problem, most motion planners solve it from scratch, e.g., via sampling and exploration or starting optimization from a straight-line path. However, most motion planners have to experience a variety of planning problems throughout their lifetimes, which are yet to be leveraged for future planning. In this paper, we present a simple but efficient method called Motion Memory, which allows different motion planners to accelerate future planning using past experiences. Treating existing motion planners as either a closed or open box, we present a variety of ways that Motion Memory can contribute to reduce the planning time when facing a new planning problem. We provide extensive experiment results with three different motion planners on three classes of planning problems with over 30,000 problem instances and show that planning speed can be significantly reduced by up to 89% with the proposed Motion Memory technique and with increasing past planning experiences.},
  archive   = {C_ICRA},
  author    = {Dibyendu Das and Yuanjie Lu and Erion Plaku and Xuesu Xiao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610790},
  month     = {5},
  pages     = {16467-16474},
  title     = {Motion memory: Leveraging past experiences to accelerate future motion planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BeBOP - combining reactive planning and bayesian
optimization to solve robotic manipulation tasks. <em>ICRA</em>,
16459–16466. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic systems for manipulation tasks are increasingly expected to be easy to configure for new tasks. While in the past, robot programs were often written statically and tuned manually, the current, faster transition times call for robust, modular and interpretable solutions that also allow a robotic system to learn how to perform a task. We propose the method Behavior-based Bayesian Optimization and Planning (BeBOP) that combines two approaches for generating behavior trees: we build the structure using a reactive planner and learn specific parameters with Bayesian optimization. The method is evaluated on a set of robotic manipulation benchmarks and is shown to outperform state-of-the-art reinforcement learning algorithms by being up to 46 times faster while simultaneously being less dependent on reward shaping. We also propose a modification to the uncertainty estimate for the random forest surrogate models that drastically improves the results.},
  archive   = {C_ICRA},
  author    = {Jonathan Styrud and Matthias Mayr and Erik Hellsten and Volker Krueger and Christian Smith},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611468},
  month     = {5},
  pages     = {16459-16466},
  title     = {BeBOP - combining reactive planning and bayesian optimization to solve robotic manipulation tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Risk-predictive planning for off-road autonomy.
<em>ICRA</em>, 16452–16458. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Efficiently navigating off-road environments presents a number of challenges arising from their unstructured nature. In the absence of high-fidelity maps, occlusions from obstacles and terrain lead to limited information available to inform planning decisions. Furthermore, resolution and latency limitations of real-world perception systems lead to potentially of degraded perception performance when traversing such environments at high speeds. We address these problems by proposing an algorithm which plans trajectories while anticipating future observations. In particular, we introduce a model which learns to predict the evolution of future riskmaps conditioned on the future path and speed profile of the vehicle. The model is trained in a self-supervised fashion using recordings of vehicle trajectories. We then present an algorithm which leverages a way to efficiently query the model along candidate paths and speed profiles to produce time-optimal trajectories while maintaining a bound on the future expected risk. We assess the predictive performance of our risk model through a comparison with real vehicle driving logs. Furthermore, our closed-loop simulations of several benchmark scenarios demonstrate how the behavior of our planner leads to qualitatively distinct trajectories, leading to improvements in both success rate and speed by up to 60%.},
  archive   = {C_ICRA},
  author    = {Lukas Lao Beyer and Gilhyun Ryou and Patrick Spieler and Sertac Karaman},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611509},
  month     = {5},
  pages     = {16452-16458},
  title     = {Risk-predictive planning for off-road autonomy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Task-oriented active learning of model preconditions for
inaccurate dynamics models. <em>ICRA</em>, 16445. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When planning with an inaccurate dynamics model, a practical strategy is to restrict planning to regions of state-action space where the model is accurate: also known as a model precondition. Empirical real-world trajectory data is valuable for defining data-driven model preconditions regard-less of the model form (analytical, simulator, learned, etc…). However, real-world data is often expensive and dangerous to collect. In order to achieve data efficiency, this paper presents an algorithm for actively selecting trajectories to learn a model precondition for an inaccurate pre-specified dynamics model. Our proposed techniques address challenges arising from the sequential nature of trajectories, and potential benefit of prioritizing task-relevant data. The experimental analysis shows how algorithmic properties affect performance in three planning scenarios: icy gridworld, simulated plant watering, and real-world plant watering. Results demonstrate an improvement of approximately 80% after only four real-world trajectories when using our proposed techniques. More material can be found on our project website: https://sites.google.com/view/active-mde.},
  archive   = {C_ICRA},
  author    = {Alex LaGrassa and Moonyoung Lee and Oliver Kroemer},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611488},
  month     = {5},
  pages     = {16445},
  title     = {Task-oriented active learning of model preconditions for inaccurate dynamics models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A tube-based reinforcement learning approach for optimal
motion planning in unknown workspaces. <em>ICRA</em>, 16439–16444. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, a tube-based nearly optimal solution to motion planning in unknown workspaces is presented. The advantages of reactive motion planning are combined with a Policy Iteration Reinforcement Learning scheme to yield a novel solution for unknown workspaces that inherits provable safety, convergence and optimality. Moreover, in simply-connected workspaces, our method is proven to asymptotically provide the globally optimal path. Our method is compared against a provably asymptotically optimal RRT ⋆ method, as well as a relevant reactive method and provides satisfactory performance, closely matching or outperforming the former.},
  archive   = {C_ICRA},
  author    = {Panagiotis Rousseas and Charalampos P. Bechlioulis and Kostas J. Kyriakopoulos},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610301},
  month     = {5},
  pages     = {16439-16444},
  title     = {A tube-based reinforcement learning approach for optimal motion planning in unknown workspaces},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LPFormer: LiDAR pose estimation transformer with multi-task
network. <em>ICRA</em>, 16432–16438. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Due to the difficulty of acquiring large-scale 3D human keypoint annotation, previous methods for 3D human pose estimation (HPE) have often relied on 2D image features and sequential 2D annotations. Furthermore, the training of these networks typically assumes the prediction of a human bounding box and the accurate alignment of 3D point clouds with 2D images, making direct application in real-world scenarios challenging. In this paper, we present the 1 st framework for end-to-end 3D human pose estimation, named LPFormer, which uses only LiDAR as its input along with its corresponding 3D annotations. LPFormer consists of two stages: firstly, it identifies the human bounding box and extracts multi-level feature representations, and secondly, it utilizes a transformer-based network to predict human keypoints based on these features. Our method demonstrates that 3D HPE can be seamlessly integrated into a strong LiDAR perception network and benefit from the features extracted by the network. Experimental results on the Waymo Open Dataset demonstrate the state-of-the-art performance, and improvements even compared to previous multi-modal solutions.},
  archive   = {C_ICRA},
  author    = {Dongqiangzi Ye and Yufei Xie and Weijia Chen and Zixiang Zhou and Lingting Ge and Hassan Foroosh},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611405},
  month     = {5},
  pages     = {16432-16438},
  title     = {LPFormer: LiDAR pose estimation transformer with multi-task network},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FastOcc: Accelerating 3D occupancy prediction by fusing the
2D bird’s-eye view and perspective view. <em>ICRA</em>, 16425–16431. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In autonomous driving, 3D occupancy prediction outputs voxel-wise status and semantic labels for more comprehensive understandings of 3D scenes compared with traditional perception tasks, such as 3D object detection and bird’s-eye view (BEV) semantic segmentation. Recent researchers have extensively explored various aspects of this task, including view transformation techniques, ground-truth label generation, and elaborate network design, aiming to achieve superior performance. However, the inference speed, crucial for running on an autonomous vehicle, is neglected. To this end, a new method, dubbed FastOcc, is proposed. By carefully analyzing the network effect and latency from four parts, including the input image resolution, image backbone, view transformation, and occupancy prediction head, it is found that the occupancy prediction head holds considerable potential for accelerating the model while keeping its accuracy. Targeted at improving this component, the time-consuming 3D convolution network is replaced with a novel residual-like architecture, where features are mainly digested by a lightweight 2D BEV convolution network and compensated by integrating the 3D voxel features interpolated from the original image features. Experiments on the Occ3D-nuScenes benchmark demonstrate that our FastOcc achieves state-of-the-art results with a fast inference speed.},
  archive   = {C_ICRA},
  author    = {Jiawei Hou and Xiaoyan Li and Wenhao Guan and Gang Zhang and Di Feng and Yuheng Du and Xiangyang Xue and Jian Pu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610625},
  month     = {5},
  pages     = {16425-16431},
  title     = {FastOcc: Accelerating 3D occupancy prediction by fusing the 2D bird’s-eye view and perspective view},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HPL-ViT: A unified perception framework for heterogeneous
parallel LiDARs in V2V. <em>ICRA</em>, 16417–16424. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To develop the next generation of intelligent LiDARs, we propose a novel framework of parallel LiDARs and construct a hardware prototype in our experimental platform, DAWN (Digital Artificial World for Natural). It emphasizes the tight integration of physical and digital space in LiDAR systems, with networking being one of its supported core features. In the context of autonomous driving, V2V (Vehicle-to-Vehicle) technology enables efficient information sharing between different agents which significantly promotes the development of LiDAR networks. However, current research operates under an ideal situation where all vehicles are equipped with identical LiDAR, ignoring the diversity of LiDAR categories and operating frequencies. In this paper, we first utilize OpenCDA and RLS (Realistic LiDAR Simulation) to construct a novel heterogeneous LiDAR dataset named OPV2V-HPL. Additionally, we present HPL-ViT, a pioneering architecture designed for robust feature fusion in heterogeneous and dynamic scenarios. It uses a graph-attention Transformer to extract domain-specific features for each agent, coupled with a cross-attention mechanism for the final fusion. Extensive experiments on OPV2V-HPL demonstrate that HPL-ViT achieves SOTA (state-of-the-art) performance in all settings and exhibits outstanding generalization capabilities.},
  archive   = {C_ICRA},
  author    = {Yuhang Liu and Boyi Sun and Yuke Li and Yuzheng Hu and Fei-Yue Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611513},
  month     = {5},
  pages     = {16417-16424},
  title     = {HPL-ViT: A unified perception framework for heterogeneous parallel LiDARs in V2V},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-class road defect detection and segmentation using
spatial and channel-wise attention for autonomous road repairing.
<em>ICRA</em>, 16409–16416. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Road pavement detection and segmentation are critical for developing autonomous road repair systems. However, developing an instance segmentation method that simultaneously performs multi-class defect detection and segmentation is challenging due to the textural simplicity of road pavement image, the diversity of defect geometries, and the morphological ambiguity between classes. We propose a novel end-to-end method for multi-class road defect detection and segmentation. The proposed method comprises multiple spatial and channel-wise attention blocks available to learn global representations across spatial and channel-wise dimensions. Through these attention blocks, more globally generalised representations of morphological information (spatial characteristics) of road defects and colour and depth information of images can be learned. To demonstrate the effectiveness of our framework, we conducted various ablation studies and comparisons with prior methods on a newly collected dataset annotated with nine road defect classes. The experiments show that our proposed method outperforms existing state-of-the-art methods for multi-class road defect detection and segmentation methods.},
  archive   = {C_ICRA},
  author    = {Jongmin Yu and Chen Bene Chi and Sebastiano Fichera and Paolo Paoletti and Devansh Mehta and Shan Luo},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611081},
  month     = {5},
  pages     = {16409-16416},
  title     = {Multi-class road defect detection and segmentation using spatial and channel-wise attention for autonomous road repairing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CFDNet: A generalizable foggy stereo matching network with
contrastive feature distillation. <em>ICRA</em>, 16402–16408. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Stereo matching under foggy scenes remains a challenging task since the scattering effect degrades the visibility and results in less distinctive features for dense correspondence matching. While some previous learning-based methods integrated a physical scattering function for simultaneous stereo-matching and dehazing, simply removing fog might not aid depth estimation because the fog itself can provide crucial depth cues. In this work, we introduce a framework based on contrastive feature distillation (CFD). This strategy combines feature distillation from merged clean-fog features with contrastive learning, ensuring balanced dependence on fog depth hints and clean matching features. This framework helps to enhance model generalization across both clean and foggy environments. Comprehensive experiments on synthetic and real-world datasets affirm the superior strength and adapt-ability of our method.},
  archive   = {C_ICRA},
  author    = {Zihua Liu and Yizhou Li and Masatoshi Okutomi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610001},
  month     = {5},
  pages     = {16402-16408},
  title     = {CFDNet: A generalizable foggy stereo matching network with contrastive feature distillation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vehicle intention classification using visual clues.
<em>ICRA</em>, 16395–16401. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Classifying intentions of other traffic agents is an essential task for intelligent transportation systems. To simplify this task, vehicles are equipped with various illumination systems, including turn indicators, emergency lights, rear lights, and brake lights. We extend the Waymo open perception dataset with ground truth annotations for different visual intentions to develop methods designed to classify the state of such systems. Furthermore, we propose the VISUAL INTENTION FORMER, a two-step transformer-based architecture to classify visual intentions in image sequences of tracked traffic participants. We use a vision transformer to extract image features, which are passed into a transformer encoder that reasons about temporal dependencies among them. We evaluate against different baseline architectures where our proposed method achieves state-of-the-art results. Additionally, we conduct an in-depth performance analysis of our method regarding different input sequence lengths, vehicle headings, and daytime conditions.},
  archive   = {C_ICRA},
  author    = {Marvin Klemp and Royden Wagner and Kevin Rosch and Martin Lauer and Christoph Stiller},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610547},
  month     = {5},
  pages     = {16395-16401},
  title     = {Vehicle intention classification using visual clues},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EMIFF: Enhanced multi-scale image feature fusion for
vehicle-infrastructure cooperative 3D object detection. <em>ICRA</em>,
16388–16394. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In autonomous driving, cooperative perception makes use of multi-view cameras from both vehicles and infrastructure, providing a global vantage point with rich semantic context of road conditions beyond a single vehicle viewpoint. Currently, two major challenges persist in vehicle-infrastructure cooperative 3D (VIC3D) object detection: 1) inherent pose errors when fusing multi-view images, caused by time asynchrony across cameras; 2) information loss in transmission process resulted from limited communication bandwidth. To address these issues, we propose a novel camera-based 3D detection framework for VIC3D task, Enhanced Multi-scale Image Feature Fusion (EMIFF). To fully exploit holistic perspectives from both vehicles and infrastructure, we propose Multi-scale Cross Attention (MCA) and Camera-aware Channel Masking (CCM) modules to enhance infrastructure and vehicle features at scale, spatial, and channel levels to correct the pose error introduced by camera asynchrony. We also introduce a Feature Compression (FC) module with channel and spatial compression blocks for transmission efficiency. Experiments show that EMIFF achieves SOTA on DAIR-V2X-C datasets, significantly outperforming previous early-fusion and late-fusion methods with comparable transmission costs.},
  archive   = {C_ICRA},
  author    = {Zhe Wang and Siqi Fan and Xiaoliang Huo and Tongda Xu and Yan Wang and Jingjing Liu and Yilun Chen and Ya-Qin Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610545},
  month     = {5},
  pages     = {16388-16394},
  title     = {EMIFF: Enhanced multi-scale image feature fusion for vehicle-infrastructure cooperative 3D object detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Eliminating cross-modal conflicts in BEV space for
LiDAR-camera 3D object detection. <em>ICRA</em>, 16381–16387. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent 3D object detectors typically utilize multi-sensor data and unify multi-modal features in the shared bird’s-eye view (BEV) representation space. However, our empirical findings indicate that previous methods have limitations in generating fusion BEV features free from cross-modal conflicts. These conflicts encompass extrinsic conflicts caused by BEV feature construction and inherent conflicts stemming from heterogeneous sensor signals. Therefore, we propose a novel Eliminating Conflicts Fusion (ECFusion) method to explicitly eliminate the extrinsic/inherent conflicts in BEV space and produce improved multi-modal BEV features. Specifically, we devise a Semantic-guided Flow-based Alignment (SFA) module to resolve extrinsic conflicts via unifying spatial distribution in BEV space before fusion. Moreover, we design a Dissolved Query Recovering (DQR) mechanism to remedy inherent conflicts by preserving objectness clues that are lost in the fusion BEV feature. In general, our method maximizes the effective information utilization of each modality and leverages inter-modal complementarity. Our method achieves state-of-the-art performance in the highly competitive nuScenes 3D object detection dataset. The code is released at https://github.com/fjhzhixi/ECFusion.},
  archive   = {C_ICRA},
  author    = {Jiahui Fu and Chen Gao and Zitian Wang and Lirong Yang and Xiaofei Wang and Beipeng Mu and Si Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610230},
  month     = {5},
  pages     = {16381-16387},
  title     = {Eliminating cross-modal conflicts in BEV space for LiDAR-camera 3D object detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). S2R-ViT for multi-agent cooperative perception: Bridging
the gap from simulation to reality. <em>ICRA</em>, 16374–16380. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Due to the lack of enough real multi-agent data and time-consuming of labeling, existing multi-agent cooperative perception algorithms usually select the simulated sensor data for training and validating. However, the perception performance is degraded when these simulation-trained models are deployed to the real world, due to the significant domain gap between the simulated and real data. In this paper, we propose the first Simulation-to-Reality transfer learning framework for multi-agent cooperative perception using a novel Vision Transformer, named as S2R-ViT, which considers both the Deployment Gap and Feature Gap between simulated and real data. We investigate the effects of these two types of domain gaps and propose a novel uncertainty-aware vision transformer to effectively relief the Deployment Gap and an agent-based feature adaptation module with inter-agent and ego-agent discriminators to reduce the Feature Gap. Our intensive experiments on the public multi-agent cooperative perception datasets OPV2V and V2V4Real demonstrate that the proposed S2R-ViT can effectively bridge the gap from simulation to reality and outperform other methods significantly for point cloud-based 3D object detection.},
  archive   = {C_ICRA},
  author    = {Jinlong Li and Runsheng Xu and Xinyu Liu and Baolu Li and Qin Zou and Jiaqi Ma and Hongkai Yu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611572},
  month     = {5},
  pages     = {16374-16380},
  title     = {S2R-ViT for multi-agent cooperative perception: Bridging the gap from simulation to reality},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UniGen: Unified modeling of initial agent states and
trajectories for generating autonomous driving scenarios. <em>ICRA</em>,
16367–16373. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces UniGen, a novel approach to generating new traffic scenarios for evaluating and improving autonomous driving software through simulation. Our approach models all driving scenario elements in a unified model: the position of new agents, their initial state, and their future motion trajectories. By predicting the distributions of all these variables from a shared global scenario embedding, we ensure that the final generated scenario is fully conditioned on all available context in the existing scene. Our unified modeling approach, combined with autoregressive agent injection, conditions the placement and motion trajectory of every new agent on all existing agents and their trajectories, leading to realistic scenarios with low collision rates. Our experimental results show that UniGen outperforms prior state of the art on the Waymo Open Motion Dataset.},
  archive   = {C_ICRA},
  author    = {Reza Mahjourian and Rongbing Mu and Valerii Likhosherstov and Paul Mougin and Xiukun Huang and Joao Messias and Shimon Whiteson},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611115},
  month     = {5},
  pages     = {16367-16373},
  title     = {UniGen: Unified modeling of initial agent states and trajectories for generating autonomous driving scenarios},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SCALE: Self-correcting visual navigation for mobile robots
via anti-novelty estimation. <em>ICRA</em>, 16360–16366. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although visual navigation has been extensively studied using deep reinforcement learning, online learning for real-world robots remains a challenging task. Recent work directly learned from offline dataset to achieve broader generalization in the real-world tasks, which, however, faces the out-of-distribution (OOD) issue and potential robot localization failures in a given map for unseen observation. This significantly drops the success rates and even induces collision. In this paper, we present a self-correcting visual navigation method, SCALE, that can autonomously prevent the robot from the OOD situations without human intervention. Specifically, we develop an image-goal conditioned offline reinforcement learning method based on implicit Q-learning (IQL). When facing OOD observation, our novel localization recovery method generates the potential future trajectories by learning from the navigation affordance, and estimates the future novelty via random network distillation (RND). A tailored cost function searches for the candidates with the least novelty that can lead the robot to the familiar places. We collect offline data and conduct evaluation experiments in three real-world urban scenarios. Experiment results show that SCALE outperforms the previous state-of-the-art methods for open-world navigation with a unique capability of localization recovery, significantly reducing the need for human intervention. Code is available at https://github.com/KubeEdge4Robotics/ScaleNav.},
  archive   = {C_ICRA},
  author    = {Chang Chen and Yuecheng Liu and Yuzheng Zhuang and Sitong Mao and Kaiwen Xue and Shunbo Zhou},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610847},
  month     = {5},
  pages     = {16360-16366},
  title     = {SCALE: Self-correcting visual navigation for mobile robots via anti-novelty estimation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DualAT: Dual attention transformer for end-to-end autonomous
driving. <em>ICRA</em>, 16353–16359. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The effective reasoning of integrated multimodal perception information is crucial for achieving enhanced end-to-end autonomous driving performance. In this paper, we introduce a novel multitask imitation learning framework for end-to-end autonomous driving that leverages a dual attention transformer (DualAT) to enhance the multimodal fusion and waypoint prediction processes. A self-attention mechanism captures global context information and models the long-term temporal dependencies of waypoints for multiple time steps. On the other hand, a cross-attention mechanism implicitly associates the latent feature representations derived from different modalities through a learnable geometrically linked positional embedding. Specifically, the DualAT excels at processing and fusing information from multiple camera views and LiDAR sensors, enabling comprehensive scene understanding for multitask learning. Furthermore, the DualAT introduces a novel waypoint prediction architecture that combines the temporal relationships between waypoints with the spatial features extracted from sensor inputs. We evaluate our approach on both the Town05 and Longest6 benchmarks using the closed-loop CARLA urban driving simulator and provide extensive ablation studies. The experimental results demonstrate that our approach significantly outperforms the state-of-the-art methods.},
  archive   = {C_ICRA},
  author    = {Zesong Chen and Ze Yu and Jun Li and Linlin You and Xiaojun Tan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610334},
  month     = {5},
  pages     = {16353-16359},
  title     = {DualAT: Dual attention transformer for end-to-end autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Talk2BEV: Language-enhanced bird’s-eye view maps for
autonomous driving. <em>ICRA</em>, 16345–16352. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work introduces Talk2BEV, a large vision-language model (LVLM) 1 interface for bird’s-eye view (BEV) maps commonly used in autonomous driving. While existing perception systems for autonomous driving scenarios have largely focused on a pre-defined (closed) set of object categories and driving scenarios, Talk2BEV eliminates the need for BEV-specific training, relying instead on well-performing pre-trained LVLMs. This enables a single system to cater to a variety of autonomous driving tasks encompassing visual and spatial reasoning, predicting the intents of traffic actors, and decision-making based on visual cues. We extensively evaluate Talk2BEV on a large number of scene understanding tasks that rely on both the ability to interpret freeform natural language queries, and in grounding these queries to the visual context embedded into the language-enhanced BEV map. To enable further research in LVLMs for autonomous driving scenarios, we develop and release Talk2BEV-Bench, a benchmark encompassing 1000 human-annotated BEV scenarios, with more than 20,000 questions and ground-truth responses from the NuScenes dataset. We encourage the reader to view the demos on our project page: https://llmbev.github.io/talk2bev/},
  archive   = {C_ICRA},
  author    = {Tushar Choudhary and Vikrant Dewangan and Shivam Chandhok and Shubham Priyadarshan and Anushka Jain and Arun K. Singh and Siddharth Srivastava and Krishna Murthy Jatavallabhula and K. Madhava Krishna},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611485},
  month     = {5},
  pages     = {16345-16352},
  title     = {Talk2BEV: Language-enhanced bird’s-eye view maps for autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Continuous robotic tracking of dynamic targets in complex
environments based on detectability. <em>ICRA</em>, 16338–16344. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Target tracking is a fundamental task in the domain of robotics. The effectiveness of target tracking hinges upon various factors, such as tracking distance, occlusions, collision avoidance, etc. However, few existing works can simultaneously tackle these considerations of tracking single and multiple targets in complex environments. In this study, the interaction mechanism of target tracking between the robot, the environment and the targets is analyzed, and a general measure named detectability is introduced to correlate the tracking performance for guiding robotic motion planning. Based on the detectability measure, the robotic motion planning framework based on Model Predictive Control (MPC) is proposed to achieve continuous and robust tracking of single, two and three targets in complex environments. Simulations and experiments are performed and verify the performances of our method better than the state-of-the-art methods.},
  archive   = {C_ICRA},
  author    = {Zhihao Wang and Shixing Huang and Minghang Li and Junyuan Ouyang and Yu Wang and Haoyao Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610993},
  month     = {5},
  pages     = {16338-16344},
  title     = {Continuous robotic tracking of dynamic targets in complex environments based on detectability},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rethinking social robot navigation: Leveraging the best of
two worlds. <em>ICRA</em>, 16330–16337. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Empowering robots to navigate in a socially compliant manner is essential for the acceptance of robots moving in human-inhabited environments. Previously, roboticists have developed geometric navigation systems with decades of empirical validation to achieve safety and efficiency. However, the many complex factors of social compliance make geometric navigation systems hard to adapt to social situations, where no amount of tuning enables them to be both safe (people are too unpredictable) and efficient (the frozen robot problem). With recent advances in deep learning approaches, the common reaction has been to entirely discard these classical navigation systems and start from scratch, building a completely new learning-based social navigation planner. In this work, we find that this reaction is unnecessarily extreme: using a large-scale real-world social navigation dataset, SCAND, we find that geometric systems can produce trajectory plans that align with the human demonstrations in a large number of social situations. We, therefore, ask if we can rethink the social robot navigation problem by leveraging the advantages of both geometric and learning-based methods. We validate this hybrid paradigm through a proof-of-concept experiment, in which we develop a hybrid planner that switches between geometric and learning-based planning. Our experiments on both SCAND and two physical robots show that the hybrid planner can achieve better social compliance compared to using either the geometric or learning-based approach alone.},
  archive   = {C_ICRA},
  author    = {Amir Hossain Raj and Zichao Hu and Haresh Karnan and Rohan Chandra and Amirreza Payandeh and Luisa Mao and Peter Stone and Joydeep Biswas and Xuesu Xiao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611710},
  month     = {5},
  pages     = {16330-16337},
  title     = {Rethinking social robot navigation: Leveraging the best of two worlds},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward wheeled mobility on vertically challenging terrain:
Platforms, datasets, and algorithms. <em>ICRA</em>, 16322–16329. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most conventional wheeled robots can only move in flat environments and simply divide their planar workspaces into free spaces and obstacles. Deeming obstacles as non-traversable significantly limits wheeled robots’ mobility in real-world, extremely rugged, off-road environments, where part of the terrain (e.g., irregular boulders and fallen trees) will be treated as non-traversable obstacles. To improve wheeled mobility in those environments with vertically challenging terrain, we present two wheeled platforms with little hardware modification compared to conventional wheeled robots; we collect datasets of our wheeled robots crawling over previously non-traversable, vertically challenging terrain to facilitate data-driven mobility; we also present algorithms and their experimental results to show that conventional wheeled robots have previously unrealized potential of moving through vertically challenging terrain. We make our platforms, datasets, and algorithms publicly available to facilitate future research on wheeled mobility. 1},
  archive   = {C_ICRA},
  author    = {Aniket Datar and Chenhui Pan and Mohammad Nazeri and Xuesu Xiao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610079},
  month     = {5},
  pages     = {16322-16329},
  title     = {Toward wheeled mobility on vertically challenging terrain: Platforms, datasets, and algorithms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new perspective of deep learning testing framework:
Human-computer interaction based neural network testing. <em>ICRA</em>,
16299–16305. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep learning models have revolutionized various domains but have also raised concerns regarding their security and reliability. Adversarial attacks and coverage-based testing have been extensively studied to assess and enhance the dependability of deep neural networks. However, current research in this area has reached a state of stagnation. Adversarial attacks focus on exploiting vulnerabilities in models, while coverage-based testing aims to achieve comprehensive testing but overlooks application scenarios. Moreover, evaluating test cases solely based on their fault-revealing capability is insufficient. To address these limitations, we propose an innovative interdisciplinary framework that incorporates human-computer interaction methods in deep learning security testing. By considering the attributes of model application scenarios, we can design more effective test suites that intend to reveal the model&#39;s behavior across various scenarios, aiding in the identification of potential defects. Consequently, the test suite plays a crucial role in the testing process of deep learning models, contributing to the assurance of model robustness and reliability. Additionally, we establish a comprehensive evaluation metric for test suite quality, considering factors such as diversity and naturalness. This framework promotes reliable and secure deployment of deep learning models, fostering interdisciplinary collaboration between artificial intelligence and human-computer interaction.},
  archive   = {C_ICRA},
  author    = {Wei Kong and Hu Li and Qianjin Du and Huayang Cao and Xiaohui Kuang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611437},
  month     = {5},
  pages     = {16299-16305},
  title     = {A new perspective of deep learning testing framework: Human-computer interaction based neural network testing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Resampling-free particle filters in high-dimensions.
<em>ICRA</em>, 16292–16298. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {State estimation is crucial for the performance and safety of numerous robotic applications. Among the suite of estimation techniques, particle filters have been identified as a powerful solution due to their non-parametric nature. Yet, in high-dimensional state spaces, these filters face challenges such as ’particle deprivation’ which hinders accurate representation of the true posterior distribution. This paper introduces a novel resampling-free particle filter designed to mitigate particle deprivation by forgoing the traditional resampling step. This ensures a broader and more diverse particle set, especially vital in high-dimensional scenarios. Theoretically, our proposed filter is shown to offer a near-accurate representation of the desired posterior distribution in high-dimensional contexts. Empirically, the effectiveness of our approach is underscored through a high-dimensional synthetic state estimation task and a 6D pose estimation derived from videos. We posit that as robotic systems evolve with greater degrees of freedom, particle filters tailored for high-dimensional state spaces will be indispensable.},
  archive   = {C_ICRA},
  author    = {Akhilan Boopathy and Aneesh Muppidi and Peggy Yang and Abhiram Iyer and William Yue and Ila Fiete},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611361},
  month     = {5},
  pages     = {16292-16298},
  title     = {Resampling-free particle filters in high-dimensions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conformal policy learning for sensorimotor control under
distribution shifts. <em>ICRA</em>, 16285–16291. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper focuses on the problem of detecting and reacting to changes in the distribution of a sensorimotor controller’s observables. The key idea is the design of policies that can take conformal quantiles as input, to detect distribution shifts with formal statistical guarantees, which we define as conformal policy learning. We show how to design such policies by using conformal quantiles to switch between base policies with different characteristics, e.g. safety or speed, or directly augmenting a policy observation with a quantile and training it with reinforcement learning. Theoretically, we show that such policies achieve the formal convergence guarantees in finite time. In addition, we thoroughly evaluate their advantages and limitations on two use cases: simulated autonomous driving and active perception with a physical quadruped. Empirical results demonstrate that our approach outperforms five baselines. It is also the simplest of the baseline strategies besides one ablation. Being easy to use, flexible, and with formal guarantees, our work demonstrates how conformal prediction can be an effective tool for sensorimotor learning under uncertainty.},
  archive   = {C_ICRA},
  author    = {Huang Huang and Satvik Sharma and Antonio Loquercio and Anastasios Angelopoulos and Ken Goldberg and Jitendra Malik},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610223},
  month     = {5},
  pages     = {16285-16291},
  title     = {Conformal policy learning for sensorimotor control under distribution shifts},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online distribution shift detection via recency prediction.
<em>ICRA</em>, 16251–16263. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When deploying modern machine learning-enabled robotic systems in high-stakes applications, detecting distribution shift is critical. However, most existing methods for detecting distribution shift are not well-suited to robotics settings, where data often arrives in a streaming fashion and may be very high-dimensional. In this work, we present an online method for detecting distribution shift with guarantees on the false positive rate — i.e., when there is no distribution shift, our system is very unlikely (with probability &lt; ε) to falsely issue an alert; any alerts that are issued should therefore be heeded. Our method is specifically designed for efficient detection even with high dimensional data, and it empirically achieves up to 11x faster detection on realistic robotics settings compared to prior work while maintaining a low false negative rate in practice (whenever there is a distribution shift in our experiments, our method indeed emits an alert). We demonstrate our approach in both simulation and hardware for a visual servoing task, and show that our method indeed issues an alert before a failure occurs.},
  archive   = {C_ICRA},
  author    = {Rachel Luo and Rohan Sinha and Yixiao Sun and Ali Hindy and Shengjia Zhao and Silvio Savarese and Edward Schmerling and Marco Pavone},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611114},
  month     = {5},
  pages     = {16251-16263},
  title     = {Online distribution shift detection via recency prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Infer and adapt: Bipedal locomotion reward learning from
demonstrations via inverse reinforcement learning. <em>ICRA</em>,
16243–16250. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Enabling bipedal walking robots to learn how to maneuver over highly uneven, dynamically changing terrains is challenging due to the complexity of robot dynamics and interacted environments. Recent advancements in learning from demonstrations have shown promising results for robot learning in complex environments. While imitation learning of expert policies has been well-explored, the study of learning expert reward functions is largely under-explored in legged locomotion. This paper brings state-of-the-art Inverse Reinforcement Learning (IRL) techniques to solving bipedal locomotion problems over complex terrains. We propose algorithms for learning expert reward functions, and we subsequently analyze the learned functions. Through nonlinear function approximation, we uncover meaningful insights into the expert’s locomotion strategies. Furthermore, we empirically demonstrate that training a bipedal locomotion policy with the inferred reward functions enhances its walking performance on unseen terrains, highlighting the adaptability offered by reward learning.},
  archive   = {C_ICRA},
  author    = {Feiyang Wu and Zhaoyuan Gu and Hanran Wu and Anqi Wu and Ye Zhao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611685},
  month     = {5},
  pages     = {16243-16250},
  title     = {Infer and adapt: Bipedal locomotion reward learning from demonstrations via inverse reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prompt, plan, perform: LLM-based humanoid control via
quantized imitation learning. <em>ICRA</em>, 16236–16242. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, reinforcement learning and imitation learning have shown great potential for controlling humanoid robots’ motion. However, these methods typically create simulation environments and rewards for specific tasks, resulting in the requirements of multiple policies and limited capabilities for tackling complex and unknown tasks. To overcome these issues, we present a novel approach that combines adversarial imitation learning with large language models (LLMs). This innovative method enables the agent to learn reusable skills with a single policy and solve zero-shot tasks under the guidance of LLMs. In particular, we utilize the LLM as a strategic planner for applying previously learned skills to novel tasks through the comprehension of task-specific prompts. This empowers the robot to perform the specified actions in a sequence. To improve our model, we incorporate codebook-based vector quantization, allowing the agent to generate suitable actions in response to unseen textual commands from LLMs. Furthermore, we design general reward functions that consider the distinct motion features of humanoid robots, ensuring the agent imitates the motion data while maintaining goal orientation without additional guiding direction approaches or policies. To the best of our knowledge, this is the first framework that controls humanoid robots using a single learning policy network and LLM as a planner. Extensive experiments demonstrate that our method exhibits efficient and adaptive ability in complicated motion tasks.},
  archive   = {C_ICRA},
  author    = {Jingkai Sun and Qiang Zhang and Yiqun Duan and Xiaoyang Jiang and Chong Cheng and Renjing Xu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610948},
  month     = {5},
  pages     = {16236-16242},
  title     = {Prompt, plan, perform: LLM-based humanoid control via quantized imitation learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ethically compliant autonomous systems under partial
observability. <em>ICRA</em>, 16229–16235. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ethically compliant autonomous systems (ECAS) are the prevailing approach to building robotic systems that perform sequential decision making subject to ethical theories in fully observable environments. However, in real-world robotics settings, these systems often operate under partial observability because of sensor limitations, environmental conditions, or limited inference due to bounded computational resources. Therefore, this paper proposes a partially observable ECAS (PO-ECAS), bringing this work one step closer to being a practical and useful tool for roboticists. First, we formally introduce the PO-ECAS framework and a MILP-based solution method for approximating an optimal ethically compliant policy. Next, we extend an existing ethical framework for prima facie duties to belief space and offer an ethical framework for virtue ethics inspired by Aristotle’s Doctrine of the Mean. Finally, we demonstrate that our approach is effective in a simulated campus patrol robot domain.},
  archive   = {C_ICRA},
  author    = {Qingyuan Lu and Justin Svegliato and Samer B. Nashed and Shlomo Zilberstein and Stuart Russell},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610525},
  month     = {5},
  pages     = {16229-16235},
  title     = {Ethically compliant autonomous systems under partial observability},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FocoTrack: Multi object tracking by focusing on overlap at
low frame rate. <em>ICRA</em>, 16222–16228. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-object tracking (MOT) presents a crucial challenge in robotics. Due to limited resources embedded in robots, one time step per processing time for algorithms can be considerably large. This scenario necessitates the operation of MOT at a low frame rate. However, algorithms within the MOT research field have been constructed around datasets functioning at 10–30 frames per second (fps) which can be difficult to operate in the limited resources. In response to it, we introduce a new algorithm, called FocoTrack, which maintains tracking ability in four situations, one of which is when objects are overlapped by each other. Our algorithm exhibits remarkable performance without using any deep appearance descriptor, surpassing existing MOT methods which even use the deep appearance descriptor on a 2.5 fps dataset. We also demonstrate strong results with our algorithm on DanceTrack dataset at 20 fps and provide comprehensive insights through detailed analysis of our tracking model.},
  archive   = {C_ICRA},
  author    = {Jae Hyeok Lee and Jae-Hyeon Park and Dong Eui Chang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610679},
  month     = {5},
  pages     = {16222-16228},
  title     = {FocoTrack: Multi object tracking by focusing on overlap at low frame rate},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust collaborative perception against temporal information
disturbance. <em>ICRA</em>, 16207–16213. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collaborative perception facilitates a more comprehensive representation of the environment by leveraging complementary information shared among various agents and sensors. However, practical applications often encounter information disturbance which includes perception packet loss and time delays, and a comprehensive framework that can simultaneously address such issues is absent. In addition, the feature extraction process prior to fusion is not sufficient, as it lacks exploration of the local semantics and context dependencies of individual features. To enhance both accuracy and robustness, this paper introduces a novel framework named Robust Collaborative Perception against Temporal Information Disturbance, which predicts perception information when disturbance occurs. Specifically, the Historical Frame Prediction (HFP) module is introduced to make compensation for information loss with temporal association excavation of historical features. Based on the predicted features generated by the HFP module, the Pyramid Attention Integration (PAI) module is introduced to augment local semantics and incorporate global long-range dependencies through multi-scale window attention. Compared with existing methods on the publicly available dataset OPV2V, our approach exhibits superior performance and expanded robustness in the 3D object detection task. The code will be publicly available at https://github.com/hexunjie/Ro-temd.},
  archive   = {C_ICRA},
  author    = {Xunjie He and Yiming Li and Te Cui and Meiling Wang and Tong Liu and Yufeng Yue},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611481},
  month     = {5},
  pages     = {16207-16213},
  title     = {Robust collaborative perception against temporal information disturbance},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EfficientDPS: Efficient and end-to-end depth-aware panoptic
segmentation. <em>ICRA</em>, 16199–16206. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Depth-aware panoptic segmentation (DPS) combines image segmentation and monocular depth estimation in a single model to achieve semantic and geometry perception simultaneously. DPS task has important applications in the robot area but the previous DPS models are too heavy to be applied. Thus, we propose EfficientDPS, an efficient, end-to-end, and unified model for DPS. In our method, query features extracted with convolution networks are used to represent things/stuff. In this way, different vision tasks such as classification, segmentation, and depth estimation can be realized in a unified manner, leading to a compact and efficient model. EfficientDPS can be trained and tested in an end-to-end manner via bipartite matching and complex post-process is not needed at inference. To enhance the supervision signal, group query representation is proposed, leading to better performance without affecting the inference speed. Extensive experiments on Cityscapes-DPS and SemKITTI-DPS show that EfficientDPS can achieve the best trade-off between speed and accuracy than the state-of-the-art methods.},
  archive   = {C_ICRA},
  author    = {Shengkai Wu and Liangliang Ren and Linfeng Gao and Yupeng Li and Wenyu Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610194},
  month     = {5},
  pages     = {16199-16206},
  title     = {EfficientDPS: Efficient and end-to-end depth-aware panoptic segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Masked γ-SSL: Learning uncertainty estimation via masked
image modeling. <em>ICRA</em>, 16192–16198. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work proposes a semantic segmentation network that produces high-quality uncertainty estimates in a single forward pass. We exploit general representations from foundation models and unlabelled datasets through a Masked Image Modeling (MIM) approach, which is robust to augmentation hyper-parameters and simpler than previous techniques. For neural networks used in safety-critical applications, bias in the training data can lead to errors; therefore it is crucial to understand a network’s limitations at run time and act accordingly. To this end, we test our proposed method on a number of test domains including the SAX Segmentation benchmark, which includes labelled test data from dense urban, rural and off-road driving domains. The proposed method consistently outperforms uncertainty estimation and Out-of-Distribution (OoD) techniques on this difficult benchmark.},
  archive   = {C_ICRA},
  author    = {David S. W. Williams and Matthew Gadd and Paul Newman and Daniele De Martini},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610398},
  month     = {5},
  pages     = {16192-16198},
  title     = {Masked γ-SSL: Learning uncertainty estimation via masked image modeling},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Orientation-aware multi-modal learning for road intersection
identification and mapping. <em>ICRA</em>, 16185–16191. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate identification of road intersections is the pivotal task for automatic construction of high-definition maps, particularly in unstructured scenes. Existing methods predominantly rely on single-modal data and thus show an obvious unimodal limitation, i.e., lack of contextual information. Moreover, these approaches overlook the benefits of leveraging multi-modal data fusion and representation learning that is crucial for generalizability. To this end, we propose a novel orientation-aware multi-modal learning paradigm, which formulates intersection identification as an oriented object detection task. Specifically, heterogeneous fusion is introduced to harmonize disparate data modalities, i.e., vector maps, point clouds, and vehicle trajectories, into a unified feature space. Concurrently, we present trigonometry-induced adaptive regression to elevate orientation estimation, while mitigating issues related to scale imbalance and boundary confusion through dual-objective matching with spatial adaptation. To evaluate our methodology, we assemble the first-of-its-kind multi-modal benchmark tailored for complex low-speed environments, complete with fine-grained semantic annotations for intersections. Comprehensive empirical analyses, including ablation studies, affirm both the superior performance of our proposed framework and the efficacy of its constituent modules.},
  archive   = {C_ICRA},
  author    = {Qibin He and Zhongyang Xiao and Ze Huang and Hongyuan Yuan and Li Sun},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610015},
  month     = {5},
  pages     = {16185-16191},
  title     = {Orientation-aware multi-modal learning for road intersection identification and mapping},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). ProEqBEV: Product group equivariant BEV network for 3D
object detection in road scenes of autonomous driving. <em>ICRA</em>,
16178–16184. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the rapid development of autonomous driving systems, 3D object detection based on Bird’s Eye View (BEV) in road scenes has witnessed great progress over the past few years. As a road scene exhibits a part-whole hierarchy between the within objects and the scene itself, simple parts (e.g., roads, lane lines, vehicles and pedestrians) can be assembled into progressively more complex shapes to form a BEV representation of the whole road scene. Therefore, a BEV often has multiple levels of freedom on motion, i.e., the rotation and the moving shift of the whole BEV, and the random movements of objects (e.g., pedestrians and vehicles) inside the BEV. However, most of the current single-sensor or multi-sensor fusion-based BEV object detection methods have not yet taken into account capturing such multi-level motion in a BEV. To address this problem, we propose a product group equivariant object detection network framework that is equivariant with respect to multiple levels of symmetry groups based on multi-sensor fusion. The proposed framework extracts local equivariant features of objects in point clouds, while global equivariant features are extracted in both point clouds and images. Furthermore, the network learns diverse rotation-equivariant features and mitigates a significant amount of detection errors caused by rotations of BEV and objects inside a BEV, thereby further enhancing the performance of object detection. The experiment results show that the network architecture significantly improves object detection on mAP and NDS, respectively. In addition, in order to demonstrate the effectiveness of the proposed local-multi-global equivariant components, we conduct sufficient ablation experiments. The results show that the individual components are indispensable for the object detection performance improvement of the overall network architecture.},
  archive   = {C_ICRA},
  author    = {Hongwei Liu and Jian Yang and Zhengyu Li and Ke Li and Jianzhang Zheng and Xihao Wang and Xuan Tang and Mingsong Chen and Xiong You and Xian Wei},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610492},
  month     = {5},
  pages     = {16178-16184},
  title     = {ProEqBEV: Product group equivariant BEV network for 3D object detection in road scenes of autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Radar tracker: Moving instance tracking in sparse and noisy
radar point clouds. <em>ICRA</em>, 16170–16177. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots and autonomous vehicles should be aware of what happens in their surroundings. The segmentation and tracking of moving objects are essential for reliable path planning, including collision avoidance. We investigate this estimation task for vehicles using radar sensing. We address moving instance tracking in sparse radar point clouds to enhance scene interpretation. We propose a learning-based radar tracker incorporating temporal offset predictions to enable direct center-based association and enhance segmentation performance by including additional motion cues. We implement attention-based tracking for sparse radar scans to include appearance features and enhance performance. The final association combines geometric and appearance features to overcome the limitations of center-based tracking to associate instances reliably. Our approach shows an improved performance on the moving instance tracking benchmark of the RadarScenes dataset compared to the current state of the art.},
  archive   = {C_ICRA},
  author    = {Matthias Zeller and Daniel Casado Herraez and Jens Behley and Michael Heidingsfeld and Cyrill Stachniss},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610198},
  month     = {5},
  pages     = {16170-16177},
  title     = {Radar tracker: Moving instance tracking in sparse and noisy radar point clouds},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uplifting range-view-based 3D semantic segmentation in
real-time with multi-sensor fusion. <em>ICRA</em>, 16162–16169. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Range-View(RV)-based 3D point cloud segmentation is widely adopted due to its compact data form. However, RV-based methods fall short in providing robust segmentation for the occluded points and suffer from distortion of projected RGB images due to the sparse nature of 3D point clouds. To alleviate these problems, we propose a new LiDAR and Camera Range-view-based 3D point cloud semantic segmentation method (LaCRange). Specifically, a distortion-compensating knowledge distillation (DCKD) strategy is designed to remedy the adverse effect of RV projection of RGB images. Moreover, a context-based feature fusion module is introduced for robust and preservative sensor fusion. Finally, in order to address the limited resolution of RV and its insufficiency of 3D topology, a new point refinement scheme is devised for proper aggregation of features in 2D and augmentation of point features in 3D. We evaluated the proposed method on large-scale autonomous driving datasets i.e. SemanticKITTI and nuScenes. In addition to being real-time, the proposed method achieves state-of-the-art results on nuScenes benchmark.},
  archive   = {C_ICRA},
  author    = {Shiqi Tan and Hamidreza Fazlali and Yixuan Xu and Yuan Ren and Bingbing Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610507},
  month     = {5},
  pages     = {16162-16169},
  title     = {Uplifting range-view-based 3D semantic segmentation in real-time with multi-sensor fusion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Force-based semantic representation and estimation of
feature points for robotic cable manipulation with environmental
contacts. <em>ICRA</em>, 16139–16145. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work demonstrates the utility of dual-arm robots with dual-wrist force-torque sensors in manipulating a Deformable Linear Object (DLO) within an unknown environment that imposes constraints on the DLO’s movement through contacts and fixtures. We propose a strategy to estimate the pose of unknown environmental contacts encountered during the manipulation of a DLO, classifying the induced constraints as unilateral, bilateral and fully constrained, exploiting the redundancy of force sensors. A semantic approach to define environmental constraints is introduced and incorporated into a graph-based model of the DLO. This model remains accurate as long as the DLO is under tension and is dynamically updated throughout the manipulation process, built by sequencing a set of primitives. The estimation strategy is validated through simulations and real-world experiments, demonstrating its potential in handling DLOs under various, possibly uncertain, constraints.},
  archive   = {C_ICRA},
  author    = {Andrea Monguzzi and Yiannis Karayiannidis and Paolo Rocco and Andrea Maria Zanchettin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610686},
  month     = {5},
  pages     = {16139-16145},
  title     = {Force-based semantic representation and estimation of feature points for robotic cable manipulation with environmental contacts},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint-loss enhanced self-supervised learning for
refinement-coupled object 6D pose estimation. <em>ICRA</em>,
16132–16138. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {6D object pose estimation plays a crucial role in robot grasping and manipulation. However, the prevalent methods for 6D object pose estimation heavily rely on 6D annotated data to train deep neural networks, which poses challenges due to the difficulty in obtaining sufficient pose annotations. To address this limitation, this paper presents a self-supervised pose estimation method based on a novel pixelwise weighted dense fusion architecture. This method allows for direct learning from unannotated RGB-D data facilitated by an Iterative Annotation Resolver. Furthermore, a self-supervised pose refinement method based on joint loss is proposed to enhance the pose estimation accuracy. This refinement method employs a differentiable renderer to construct joint optimization constraints. The experimental results demonstrate that our approach achieves a level of pose estimation accuracy that closely rivals that of supervised methods.},
  archive   = {C_ICRA},
  author    = {Fengjun Mu and Shixiang Sun and Rui Huang and Chaobin Zou and Wenjiang Li and Huayi Zhan and Hong Cheng},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611061},
  month     = {5},
  pages     = {16132-16138},
  title     = {Joint-loss enhanced self-supervised learning for refinement-coupled object 6D pose estimation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning interaction constraints for robot manipulation via
set correspondences. <em>ICRA</em>, 16125–16131. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cross-pose estimation between rigid objects is a fundamental building block for robotic applications. In this paper, we propose a new cross-pose estimation method that predicts correspondences on a set level as opposed to a point level. This contrasts methods that predict cross-pose from per-point correspondences, which can encounter optimization problems for objects with symmetries, since each point may have multiple valid correspondences. Our method, SCAlign, consists of a Set Correspondence Network (SCN) which predicts these sets and their correspondences, and an alignment module to compute their relative cross-pose. Taking point clouds of two objects as input, SCN predicts a set label for each point such that such that points that share a set label form a cross object correspondence. The alignment module then computes the cross-pose as the SE(3) transformation that aligns these set correspondences. We compare SCAlign against other cross-pose estimation baselines on a synthetically generated dataset, SynWidth, which contains randomly generated width-mate objects with symmetric or near-symmetric intercepts. SCAlign significantly outperforms the baselines on this challenging dataset. Additionally, we show that set correspondences can be leveraged to distinguish positive and negative matches between pegs and holes. Robot experiments further validate the practical application of this approach.},
  archive   = {C_ICRA},
  author    = {Junyu Nan and Jessica Hodgins and Brian Okorn},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611031},
  month     = {5},
  pages     = {16125-16131},
  title     = {Learning interaction constraints for robot manipulation via set correspondences},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning to estimate incipient slip with tactile sensing to
gently grasp objects. <em>ICRA</em>, 16118–16124. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To gently grasp objects, robots need to balance generating enough friction yet avoiding too much force that could damage the object. In practice, the force regulation is challenging to implement since it requires knowledge of the friction coefficient, which can vary from object to object and even from grasp to grasp. Tactile sensing offers a window in the contact mechanics and provides information about friction. Notably touch can detect the precursor of the object slipping away from the grasp. To find this information, tactile sensors measure the deformation field of an artificial skin in both the normal and tangential direction. However, current approaches only react to slip and therefore react too late to perturbations. The object slips, inducing a failure of the grasp and damage. In this study, we introduce a method that uses machine-learning to anticipate slip by computing the so-called safety margin of the grasp. This safety margin represents the extra lateral force that maintains the contact away from the frictional limit. To find this value, we use a high-density camera-based tactile sensor to measure the 3D deformation of the surface via the movement of 82 colored markers. We trained a Convolutional Neural Network (CNN) to estimate the safety margin from the tactile images. Because it gives a distance to slip, the safety margin is a powerful metric for regulating grasp forces. As a testament of this effectiveness, we show that a simple proportional controller can robustly grasp a wide variety of objects. The results show that this control method outperforms slip detection methods, by reducing regrasp reaction times while decreasing grasping forces to 1-3 N.},
  archive   = {C_ICRA},
  author    = {Dirk-Jan Boonstra and Laurence Willemet and Jelle Luijkx and Michaël Wiertlewski},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611517},
  month     = {5},
  pages     = {16118-16124},
  title     = {Learning to estimate incipient slip with tactile sensing to gently grasp objects},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online estimation of articulated objects with factor graphs
using vision and proprioceptive sensing. <em>ICRA</em>, 16111–16117. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {From dishwashers to cabinets, humans interact with articulated objects every day, and for a robot to assist in common manipulation tasks, it must learn a representation of articulation. Recent deep learning methods can provide powerful vision-based priors on the affordance of articulated objects from previous, possibly simulated, experiences. In contrast, many other works estimate articulation by observing the object in motion, requiring the robot to already be interacting with the object. In this work, we propose to use the best of both worlds by introducing an online estimation method that merges vision-based affordance predictions from a neural network with interactive kinematic sensing in an analytical model. Our work has the benefit of using vision to predict an articulation model before touching the object, while also being able to update the model quickly from kinematic sensing during the interaction. In this paper, we implement a full system using shared autonomy for robotic opening of articulated objects, in particular objects in which the articulation is not apparent from vision alone. We implemented our system on a real robot and performed several autonomous closed-loop experiments in which the robot had to open a door with unknown joint while estimating the articulation online. Our system achieved an 80% success rate for autonomous opening of unknown articulated objects.},
  archive   = {C_ICRA},
  author    = {Russell Buchanan and Adrian Röfer and João Moura and Abhinav Valada and Sethu Vijayakumar},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610590},
  month     = {5},
  pages     = {16111-16117},
  title     = {Online estimation of articulated objects with factor graphs using vision and proprioceptive sensing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TactileAR: Active tactile pattern reconstruction.
<em>ICRA</em>, 16104–16110. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {High-resolution (HR) contact surface information is essential for robotic grasping and precise manipulation tasks. However, it remains a challenge for current taxel-based sensors to obtain HR tactile information. In this paper, we focus on utilizing low-resolution (LR) tactile sensors to reconstruct the localized, dense, and HR representation of contact surfaces. In particular, we build a Gaussian triaxial tactile sensor degradation model and propose a tactile pattern reconstruction framework based on the Kalman filter. This framework enables the reconstruction of 2-D HR contact surface shapes using collected LR tactile sequences. In addition, we present an active exploration strategy to enhance the reconstruction efficiency. We evaluate the proposed method in real-world scenarios with comparison to existing prior-information-based approaches. Experimental results confirm the efficiency of the proposed approach and demonstrate satisfactory reconstructions of complex contact surface shapes.},
  archive   = {C_ICRA},
  author    = {Bing Wu and Qian Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610669},
  month     = {5},
  pages     = {16104-16110},
  title     = {TactileAR: Active tactile pattern reconstruction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quadratic programming based inverse kinematics for precise
bimanual manipulation. <em>ICRA</em>, 16024–16030. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We discuss the precise cooperative motion of a dual manipulator. In the inverse kinematics of cooperative redundant manipulators, a hierarchical method using null space and an optimization method prioritizing the end-effectors relative position in the objective function have been proposed. However, there is no guarantee that the relative position will be maintained in regions subject to joint limits and task-space reachability constraints. As a result, unacceptable errors may occur, and some tasks cannot be accomplished. We propose designing the maximum permissible errors in advance by expressing the target relative position as inequality constraints in the Quadratic Programming (QP) problem. By extending its description to include a virtual spring, we have also achieved subtle force application by two cooperated manipulators. The proposed method was verified by simulation and experiments.},
  archive   = {C_ICRA},
  author    = {Tomohiro Chaki and Tomohiro Kawakami},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611295},
  month     = {5},
  pages     = {16024-16030},
  title     = {Quadratic programming based inverse kinematics for precise bimanual manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Everyday finger: A robotic finger that meets the needs of
everyday interactive manipulation. <em>ICRA</em>, 16016–16023. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We provide the mechanical and dynamical requirements for a robotic finger capable of performing a large number of everyday tasks. To match these requirements, we present a novel actuator and finger design, the everyday finger, that comes close to many characteristics of the human fingers. In particular, we focus on minimizing the size of components to get proper performance without sacrificing compactness. A robotic hand that uses two Everyday fingers demonstrated an 80% success rate in picking up and placing dishes in a rack, and the ability to pick up flat objects like napkins and delicate ones like strawberries. Videos are available at the project website: https://sites.google.com/view/everydayfinger.},
  archive   = {C_ICRA},
  author    = {Rubén Castro Ornelas and Tomás Cantú and Isabel Sperandio and Alexander H. Slocum and Pulkit Agrawal},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611452},
  month     = {5},
  pages     = {16016-16023},
  title     = {Everyday finger: A robotic finger that meets the needs of everyday interactive manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DexDLO: Learning goal-conditioned dexterous policy for
dynamic manipulation of deformable linear objects. <em>ICRA</em>,
16009–16015. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deformable linear object (DLO) manipulation is needed in many fields. Previous research on deformable linear object (DLO) manipulation has primarily involved parallel jaw gripper manipulation with fixed grasping positions. However, the potential for dexterous manipulation of DLOs using an anthropomorphic hand is under-explored. We present DexDLO, a model-free framework that learns dexterous dynamic manipulation policies for deformable linear objects with a fixed-base dexterous hand in an end-to-end way. By abstracting several common DLO manipulation tasks into goal-conditioned tasks, DexDLO can perform tasks such as DLO grabbing, DLO pulling, DLO end-tip position controlling, etc. Using the Mujoco physics simulator, we demonstrate that our framework can efficiently and effectively learn five different DLO manipulation tasks with the same framework parameters. We further provide a thorough analysis of learned policies, reward functions, and reduced observations for a comprehensive understanding of the framework.},
  archive   = {C_ICRA},
  author    = {Sun Zhaole and Jihong Zhu and Robert B. Fisher},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610754},
  month     = {5},
  pages     = {16009-16015},
  title     = {DexDLO: Learning goal-conditioned dexterous policy for dynamic manipulation of deformable linear objects},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 1 kHz behavior tree for self-adaptable tactile insertion.
<em>ICRA</em>, 16002–16008. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Insertion is an essential skill for robots in both modern manufacturing and services robotics. In our previous study, we proposed an insertion skill framework based on forcedomain wiggle motion. The main limitation of this method lies in the robot’s inability to adjust its behavior according to changing contact state during interaction. In this paper, we extend the skill formalism by incorporating a behavior tree-based primitive switching mechanism that leverages highfrequency tactile data for the estimation of contact state. The efficacy of our proposed framework is validated with a series of experiments that involve the execution of tightly constrained peg-in-hole tasks. The experiment results demonstrate a significant improvement in performance, characterized by reduced execution time, heightened robustness, and superior adaptability when confronted with unknown tasks. Moreover, in the context of transfer learning, our paper provides empirical evidence indicating that the proposed skill framework contributes to enhanced transferability across distinct operational contexts and tasks.},
  archive   = {C_ICRA},
  author    = {Yansong Wu and Fan Wu and Lingyun Chen and Kejia Chen and Samuel Schneider and Lars Johannsmeier and Zhenshan Bing and Fares J. Abu-Dakka and Alois Knoll and Sami Haddadin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610835},
  month     = {5},
  pages     = {16002-16008},
  title     = {1 kHz behavior tree for self-adaptable tactile insertion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Helical control in latent space: Enhancing robotic
craniotomy precision in uncertain environments. <em>ICRA</em>,
15995–16001. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce a double-stage transfer learning framework based on expert data. It employs probabilistic graphical models to effectively capture helical periodic features in the latent space, integrating Bayesian variational inference and neural networks for implementation. Compared to traditional methods, it achieves high precision and stable control even in environments with limited observation signals and high noise levels. We have successfully applied this method to a biomedical task of a simulated cranial window procedure. Preliminary results show promising performance comparable to those of human experts with only image information, further validating the efficacy of the proposed method.},
  archive   = {C_ICRA},
  author    = {Yuanyuan Jia and Jessica Ziyu Qu and Tadahiro Taniguchi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611473},
  month     = {5},
  pages     = {15995-16001},
  title     = {Helical control in latent space: Enhancing robotic craniotomy precision in uncertain environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Language-EXtended indoor SLAM (LEXIS): A versatile system
for real-time visual scene understanding. <em>ICRA</em>, 15988–15994.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Versatile and adaptive semantic understanding would enable autonomous systems to comprehend and interact with their surroundings. Existing fixed-class models limit the adaptability of indoor mobile and assistive autonomous systems. In this work, we introduce LEXIS, a real-time indoor Simultaneous Localization and Mapping (SLAM) system that harnesses the open-vocabulary nature of Large Language Models (LLMs) to create a unified approach to scene understanding and place recognition. The approach first builds a topological SLAM graph of the environment (using visual-inertial odometry) and embeds Contrastive Language-Image Pretraining (CLIP) features in the graph nodes. We use this representation for flexible room classification and segmentation, serving as a basis for room-centric place recognition. This allows loop closure searches to be directed towards semantically relevant places. Our proposed system is evaluated using both public, simulated data and real-world data, covering office and home environments. It successfully categorizes rooms with varying layouts and dimensions and outperforms the state-of-the-art (SOTA). For place recognition and trajectory estimation tasks we achieve equivalent performance to the SOTA, all also utilizing the same pre-trained model. Lastly, we demonstrate the system’s potential for planning. Video at: https://youtu.be/gRqF3euDfX8},
  archive   = {C_ICRA},
  author    = {Christina Kassab and Matias Mattamala and Lintong Zhang and Maurice Fallon},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610341},
  month     = {5},
  pages     = {15988-15994},
  title     = {Language-EXtended indoor SLAM (LEXIS): A versatile system for real-time visual scene understanding},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). IBoW3D: Place recognition based on incremental and general
bag of words in 3D scans. <em>ICRA</em>, 15981–15987. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing methods for place recognition in 3D point clouds either ignore partial structure information by converting 3D scans to 2D images or construct constrained bag-of-words (BoW) representations reliant on specific feature extraction algorithms. In this paper, we propose a novel method based on incremental and general bag of words. Incorporating an adaptable keypoint and 3D local feature extraction method, we employ an incremental BoW model that is updated regularly. This enables a coarse-to-fine candidate selection from the database. And a revisit can be identified following geometric verification. In addition, we propose a new supplementary metric that addresses the leaving-out issue of the conventional metric, enhancing the identification of true loops. Employing a state-of-the-art (SOTA) keypoint and feature extraction algorithm, we evaluate our method as well as SOTA place recognition methods using diverse datasets with varying qualities. Experimental results demonstrate that our method outperforms the baselines across all three datasets, showcasing robust performance and notable generalization capabilities.},
  archive   = {C_ICRA},
  author    = {Yuxiaotong Lin and Jiming Chen and Liang Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610036},
  month     = {5},
  pages     = {15981-15987},
  title     = {IBoW3D: Place recognition based on incremental and general bag of words in 3D scans},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Less is more: Physical-enhanced radar-inertial odometry.
<em>ICRA</em>, 15966–15972. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Radar offers the advantage of providing additional physical properties related to observed objects. In this study, we design a physical-enhanced radar-inertial odometry system that capitalizes on the Doppler velocities and radar cross-section information. The filter for static radar points, correspondence estimation, and residual functions are all strengthened by integrating the physical properties. We conduct experiments on both public datasets and our self-collected data, with different mobile platforms and sensor types. Our quantitative results demonstrate that the proposed radar-inertial odometry system outperforms alternative methods using the physical-enhanced components. Our findings also reveal that using the physical properties results in fewer radar points for odometry estimation, but the performance is still guaranteed and even improved, thus aligning with the &quot;less is more&quot; principle.},
  archive   = {C_ICRA},
  author    = {Qiucan Huang and Yuchen Liang and Zhijian Qiao and Shaojie Shen and Huan Yin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611471},
  month     = {5},
  pages     = {15966-15972},
  title     = {Less is more: Physical-enhanced radar-inertial odometry},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning covariances for estimation with constrained bilevel
optimization. <em>ICRA</em>, 15951–15957. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problem of learning error covariance matrices for robotic state estimation. The convergence of a state estimator to the correct belief over the robot state is dependent on the proper tuning of noise models. During inference, these models are used to weigh different blocks of the Jacobian and error vector resulting from linearization and hence, additionally affect the stability and convergence of the non-linear system. We propose a gradient-based method to estimate well-conditioned covariance matrices by formulating the learning process as a constrained bilevel optimization problem over factor graphs. We evaluate our method against baselines across a range of simulated and real-world tasks and demonstrate that our technique converges to model estimates that lead to better solutions as evidenced by the improved tracking accuracy on unseen test trajectories.},
  archive   = {C_ICRA},
  author    = {Mohamad Qadri and Zachary Manchester and Michael Kaess},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610955},
  month     = {5},
  pages     = {15951-15957},
  title     = {Learning covariances for estimation with constrained bilevel optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 2D-3D object shape alignment for camera-object pose
compensation in object-visual SLAM. <em>ICRA</em>, 15936–15942. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this study, we propose an object shape alignment method through a robust optimization scheme for 6-degrees-of-freedom (DOF) object pose compensation. Although the pose estimation of the 3D object by the camera has been rapidly improved in recent years with the development of deep learning, the estimate still contains errors due to several factors. To compensate for this, we perform a shape alignment between the 2D segmentation of the object and the projection of the 3D object in the image plane. To avoid convergence to a local minimum in nonlinear optimization, we separate the pose into translation and rotation. This approach derives the optimization of a linear form in terms of a translation with reduced computational cost. For the rotation, the parallel optimization is performed with multiple initial values, reflecting to the uncertainty of an initial value. We formulate an invariant extended Kalman filter (EKF)-based object-visual simultaneous localization and mapping (SLAM) with a camera-object relative pose as the measurement model. To verify the performance of the proposed algorithm, we present the improved results of camera-object relative pose accuracy and localization and mapping accuracy in the several sequences of YCB-video dataset.},
  archive   = {C_ICRA},
  author    = {Hanyeol Lee and Jae Hyung Jung and Chan Gook Park},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610659},
  month     = {5},
  pages     = {15936-15942},
  title     = {2D-3D object shape alignment for camera-object pose compensation in object-visual SLAM},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HPF-SLAM: An efficient visual SLAM system leveraging hybrid
point features. <em>ICRA</em>, 15929–15935. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual SLAM is an essential tool in diverse applications such as robot perception and extended reality, where feature-based methods are prevalent due to their accuracy and robustness. However, existing methods employ either hand-crafted or solely learnable point features and are thus limited by the feature attributes. In this paper, we propose incorporating hybrid point features efficiently into a single system. By integrating hand-crafted and learnable features, we seek to capitalize on their complementary attributes in both key-point identification and descriptor expressiveness. To this purpose, we design a pre-processing module, which includes extraction, inter-class processing, and post-processing of hybrid point features. We present an efficient matching approach to exclusively perform the data association within the same class of features. Moreover, we design a Hybrid Bag-of-Words (H-BoW) model to deal with hybrid point features in matching and loop-closure-detection. By integrating the proposed framework into a modern feature-based system, we introduce HPF-SLAM. We evaluate the system on EuRoC-MAV and TUM-RGBD benchmarks. The experimental results show that our method consistently surpasses the baseline at comparable speed.},
  archive   = {C_ICRA},
  author    = {Xin Su and Sebastian Eger and Adam Misik and Dong Yang and Rastin Pries and Eckehard Steinbach},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610220},
  month     = {5},
  pages     = {15929-15935},
  title     = {HPF-SLAM: An efficient visual SLAM system leveraging hybrid point features},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MBFusion: A new multi-modal BEV feature fusion method for HD
map construction. <em>ICRA</em>, 15922–15928. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {HD map construction is a fundamental and challenging task in autonomous driving to understand the surrounding environment. Recently, Camera-LiDAR BEV feature fusion methods have attracted increasing attention in HD map construction task, which can significantly boost the benchmark. However, existing fusion methods ignore modal interaction and utilize very simple fusion strategy, which suffers from the problems of misalignment and information loss. To tackle this, we propose a novel Multi-modal BEV feature fusion method named MBFusion. Specifically, to solve the semantic misalignment problem between Camera and LiDAR features, we design Cross-modal Interaction Transform (CIT) module to make these two feature spaces interact knowledge with each other to enhance the feature representation by the cross-attention mechanism. Then, we propose a Dual Dynamic Fusion (DDF) module to automatically select valuable information from different modalities for better feature fusion. Moreover, MBFusion is simple, and can be plug-and-played into existing pipelines. We evaluate MBFusion on three architectures, including HDMapNet, VectorMapNet, and MapTR, to show its versatility and effectiveness. Compared with the state-of-the-art methods, MBFusion achieves 3.6% and 4.1% absolute improvements on mAP on the nuScenes and the Argoverse2 datasets, respectively, demonstrating the superiority of our method.},
  archive   = {C_ICRA},
  author    = {Xiaoshuai Hao and Hui Zhang and Yifan Yang and Yi Zhou and Sangil Jung and Seung-In Park and ByungIn Yoo},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609873},
  month     = {5},
  pages     = {15922-15928},
  title     = {MBFusion: A new multi-modal BEV feature fusion method for HD map construction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing visual place recognition with multi-modal features
and time-constrained graph attention aggregation. <em>ICRA</em>,
15914–15921. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual place recognition(VPR) is a crucial technology for autonomous driving and robotic navigation. However, severe appearance and perspective changes often lead to degradation of algorithm performance. Current methods mainly utilize single-modality RGB images, which are sensitive to environmental changes. To address this challenge, we propose a novel multi-modal visual place recognition method by incorporating depth information as auxiliary data to enhance the robustness of the VPR algorithm. The pipeline involves dual-branch feature extraction and shared multi-modal feature fusion based on transformer(SFFM) to enable full interaction between semantic and structural information. Furthermore, we introduces a time-constrained graph attention aggregation(TC-GAT) that propagates node information across time and space to deal with perceptual aliasing. Extensive experiments on the Oxford Robotcar and MSLS datasets demonstrate that the proposed algorithm is not only effective in appearance changes but also competitive in opposing viewpoints.},
  archive   = {C_ICRA},
  author    = {Zhuo Wang and Yunzhou Zhang and Xinge Zhao and Jian Ning and Dehao Zou and Meiqi Pei},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611102},
  month     = {5},
  pages     = {15914-15921},
  title     = {Enhancing visual place recognition with multi-modal features and time-constrained graph attention aggregation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Regressing transformers for data-efficient visual place
recognition. <em>ICRA</em>, 15898–15904. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual place recognition is a critical task in computer vision, especially for localization and navigation systems. Existing methods often rely on contrastive learning: image descriptors are trained to have small distance for similar images and larger distance for dissimilar ones in a latent space. However, this approach struggles to ensure accurate distance-based image similarity representation, particularly when training with binary pairwise labels, and complex re-ranking strategies are required. This work introduces a fresh perspective by framing place recognition as a regression problem, using camera field-of-view overlap as similarity ground truth for learning. By optimizing image descriptors to align directly with graded similarity labels, this approach enhances ranking capabilities without expensive re-ranking, offering data-efficient training and strong generalization across several benchmark datasets.},
  archive   = {C_ICRA},
  author    = {María Leyva-Vallina and Nicola Strisciuglio and Nicolai Petkov},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611288},
  month     = {5},
  pages     = {15898-15904},
  title     = {Regressing transformers for data-efficient visual place recognition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VDNA-PR: Using general dataset representations for robust
sequential visual place recognition. <em>ICRA</em>, 15883–15889. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper adapts a general dataset representation technique to produce robust Visual Place Recognition (VPR) descriptors, crucial to enable real-world mobile robot localisation. Two parallel lines of work on VPR have shown, on one side, that general-purpose off-the-shelf feature representations can provide robustness to domain shifts, and, on the other, that fused information from sequences of images improves performance. In our recent work on measuring domain gaps between image datasets, we proposed a Visual Distribution of Neuron Activations (VDNA) representation to represent datasets of images. This representation can naturally handle image sequences and provides a general and granular feature representation derived from a general-purpose model. Moreover, our representation is based on tracking neuron activation values over the list of images to represent and is not limited to a particular neural network layer, therefore having access to high- and low-level concepts. This work shows how VDNAs can be used for VPR by learning a very lightweight and simple encoder to generate task-specific descriptors. Our experiments show that our representation can allow for better robustness than current solutions to serious domain shifts away from the training data distribution, such as to indoor environments and aerial imagery.},
  archive   = {C_ICRA},
  author    = {Benjamin Ramtoula and Daniele De Martini and Matthew Gadd and Paul Newman},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611379},
  month     = {5},
  pages     = {15883-15889},
  title     = {VDNA-PR: Using general dataset representations for robust sequential visual place recognition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VBR: A vision benchmark in rome. <em>ICRA</em>, 15868–15874.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a vision and perception research dataset collected in Rome, featuring RGB data, 3D point clouds, IMU, and GPS data. We introduce a new benchmark targeting visual odometry and SLAM, to advance the research in autonomous robotics and computer vision. This work complements existing datasets by simultaneously addressing several issues, such as environment diversity, motion patterns, and sensor frequency. It uses up-to-date devices and presents effective procedures to accurately calibrate the intrinsic and extrinsic of the sensors while addressing temporal synchronization. During recording, we cover multi-floor buildings, gardens, urban and highway scenarios. Combining handheld and car-based data collections, our setup can simulate any robot (quadrupeds, quadrotors, autonomous vehicles). The dataset includes an accurate 6-dof ground truth based on a novel methodology that refines the RTK-GPS estimate with LiDAR point clouds through Bundle Adjustment (BA). All sequences divided in training and testing are accessible at www.rvp-group.net/datasets/slam.},
  archive   = {C_ICRA},
  author    = {Leonardo Brizi and Emanuele Giacomini and Luca Di Giammarino and Simone Ferrari and Omar Salem and Lorenzo De Rebotti and Giorgio Grisetti},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611395},
  month     = {5},
  pages     = {15868-15874},
  title     = {VBR: A vision benchmark in rome},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A vision-centric approach for static map element annotation.
<em>ICRA</em>, 15861–15867. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The recent development of online static map element (a.k.a. HD Map) construction algorithms has raised a vast demand for data with ground truth annotations. However, available public datasets currently cannot provide high-quality training data regarding consistency and accuracy. To this end, we present CAMA: a vision-centric approach for Consistent and Accurate Map Annotation. Without LiDAR inputs, our proposed framework can still generate high-quality 3D annotations of static map elements. Specifically, the annotation can achieve high reprojection accuracy across all surrounding cameras and is spatial-temporal consistent across the whole sequence. We apply our proposed framework to the popular nuScenes dataset to provide efficient and highly accurate annotations. Compared with the original nuScenes static map element, models trained with annotations from CAMA achieve lower reprojection errors (e.g., 4.73 vs. 8.03 pixels).},
  archive   = {C_ICRA},
  author    = {Jiaxin Zhang and Shiyuan Chen and Haoran Yin and Ruohong Mei and Xuan Liu and Cong Yang and Qian Zhang and Wei Sui},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611167},
  month     = {5},
  pages     = {15861-15867},
  title     = {A vision-centric approach for static map element annotation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gradient-based local next-best-view planning for improved
perception of targeted plant nodes. <em>ICRA</em>, 15854–15860. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots are increasingly used in tomato greenhouses to automate labour-intensive tasks such as selective harvesting and de-leafing. To perform these tasks, robots must be able to accurately and efficiently perceive the plant nodes that need to be cut, despite the high levels of occlusion from other plant parts. We formulate this problem as a local next-best-view (NBV) planning task where the robot has to plan an efficient set of camera viewpoints to overcome occlusion and improve the quality of perception. Our formulation focuses on quickly improving the perception accuracy of a single target node to maximise its chances of being cut. Previous methods of NBV planning mostly focused on global view planning and used random sampling of candidate viewpoints for exploration, which could suffer from high computational costs, ineffective view selection due to poor candidates, or non-smooth trajectories due to inefficient sampling. We propose a gradient-based NBV planner using differentiable ray sampling, which directly estimates the local gradient direction for viewpoint planning to overcome occlusion and improve perception. Through simulation experiments, we showed that our planner can handle occlusions and improve the 3D reconstruction and position estimation of nodes equally well as a sampling-based NBV planner, while taking ten times less computation and generating 28% more efficient trajectories.},
  archive   = {C_ICRA},
  author    = {Akshay K. Burusa and Eldert J. van Henten and Gert Kootstra},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610397},
  month     = {5},
  pages     = {15854-15860},
  title     = {Gradient-based local next-best-view planning for improved perception of targeted plant nodes},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Autonomous apple fruitlet sizing with next best view
planning. <em>ICRA</em>, 15847–15853. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a next-best-view planning approach to autonomously size apple fruitlets. State-of-the-art viewpoint planners in agriculture are designed to size large and more sparsely populated fruit. They rely on lower resolution maps and sizing methods that do not generalize to smaller fruit sizes. To overcome these limitations, our method combines viewpoint sampling around semantically labeled regions of interest, along with an attention-guided information gain mechanism to more strategically select viewpoints that target the small fruits’ volume. Additionally, we integrate a dual-map representation of the environment that is able to both speed up expensive ray casting operations and maintain the high occupancy resolution required to informatively plan around the fruit. When sizing, a robust estimation and graph clustering approach is introduced to associate fruit detections across images. Through simulated experiments, we demonstrate that our viewpoint planner improves sizing accuracy compared to state of the art and ablations. We also provide quantitative results on data collected by a real robotic system in the field.},
  archive   = {C_ICRA},
  author    = {Harry Freeman and George Kantor},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610226},
  month     = {5},
  pages     = {15847-15853},
  title     = {Autonomous apple fruitlet sizing with next best view planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On-the-go tree detection and geometric traits estimation
with ground mobile robots in fruit tree groves. <em>ICRA</em>,
15840–15846. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {By-tree information gathering is an essential task in precision agriculture achieved by ground mobile sensors, but it can be time- and labor-intensive. In this paper we present an algorithmic framework to perform real-time and on-the-go detection of trees and key geometric characteristics (namely, width and height) with wheeled mobile robots in the field. Our method is based on the fusion of 2D domain-specific data (normalized difference vegetation index [NDVI] acquired via a red-green-near-infrared [RGN] camera) and 3D LiDAR point clouds, via a customized tree landmark association and parameter estimation algorithm. The proposed system features a multi-modal and entropy-based landmark correspondences approach, integrated into an underlying Kalman filter system to recognize the surrounding trees and jointly estimate their spatial and vegetation-based characteristics. Realistic simulated tests are used to evaluate our proposed algorithm’s behavior in a variety of settings. Physical experiments in agricultural fields help validate our method’s efficacy in acquiring accurate by-tree information on-the-go and in real-time by employing only onboard computational and sensing resources.},
  archive   = {C_ICRA},
  author    = {Dimitrios Chatziparaschis and Hanzhe Teng and Yipeng Wang and Pamodya Peiris and Elia Seudiero and Konstantinos Karydis},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610355},
  month     = {5},
  pages     = {15840-15846},
  title     = {On-the-go tree detection and geometric traits estimation with ground mobile robots in fruit tree groves},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EdgeSoil 2.0 – soil analyzer using convolutional neural
network and camera imaging for agricultural robotics. <em>ICRA</em>,
15825–15831. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soil is the most important building element of agriculture and its analysis is crucial for healthy plants and a high crop yield. But apart from its importance, soil analysis is a tedious and time-consuming task. This paper presents EdgeSoil 2.0, a non-invasive, accurate, and real-time robotic system for soil pH prediction, a key parameter of soil status for farmers. The EdgeSoil 2.0 predicts the pH value of the soil in real-time, using a live video stream from a webcam with an average of 7 FPS. The method is suitable to be implemented on edge devices necessary for the application: we are using a mobile robot with the NVIDIA Jetson Nano module which is running a pH-estimator trained with a Convolutional Neural Network (CNN) on a novel dataset we built for this purpose. Predictions are performed while the robot is moving over the plowed field before the planting process starts. In order to achieve the best performance, we train the pH-estimator with different input modalities and validate each result using Mean Squared Error (MSE) and Standard Deviation (SD). We are able to achieve accurate results with the MSE value of 0.08, the SD value of 0.15, and with testing results from the field showing up to ± 0.3 deviation from the GT value during prediction, which is sufficient to comply with agricultural standards.},
  archive   = {C_ICRA},
  author    = {Roni Kasemi and Lara Lammer and Stefan Thalhammer and Markus Vincze},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611446},
  month     = {5},
  pages     = {15825-15831},
  title     = {EdgeSoil 2.0 – soil analyzer using convolutional neural network and camera imaging for agricultural robotics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LiDAR-based robot transplanter. <em>ICRA</em>, 15818–15824.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In Japan, labor shortage of agriculture is becoming increasingly severe due to the lack of farmers and aging. Therefore, the development of automation of vegetable production such as transplanting, harvesting and transporting is required. In this paper, a self-localization method by using LiDAR and a robust control method of a transplanter are proposed for accurate transplanting. In this system, the path of transplanter is generated by using 3D point cloud data, and the transplanting part follows it and plant seedlings of cabbage accurately. Path generation is performed considering vehicle tilt in the roll direction depending on the environment of grooves. An accurate calculation of lateral and angular position of the transplanting part is also proposed. For path following control, sliding-mode control and inverse optimal control are applied to transplanter. The experimental results demonstrated the effectiveness of these proposed methods and problems we have to tackle on. Basically, it was possible to perform automated transplanting accurately, but there was an occasional problem of offset error from 0. It was confirmed that inverse optimal control is superior to sliding-mode control and is more robust to environmental changes.},
  archive   = {C_ICRA},
  author    = {Masaki Asano and Takanori Fukao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611087},
  month     = {5},
  pages     = {15818-15824},
  title     = {LiDAR-based robot transplanter},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Development of an automatic sweet pepper harvesting robot
and experimental evaluation. <em>ICRA</em>, 15811–15817. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The aging population and diminishing working population in agriculture motivate the development of autonomous harvesting robots. Although autonomous harvesting is expanding rapidly, the commercial application of sweet pepper harvesting robots still faces challenges. This paper presents the development of a sweet pepper harvesting robot and reports its experimental verification, which mainly includes end-effector design, visual perception, and grasping pose control. The end-effector adopts electrical control, mainly composed of a servo-electric two-finger parallel clamping module, a swing-cutting module, and a fruit recovery device. Equipped with a tactile sensor array, it can accurately sense the sweet pepper peduncle position and the end-effector state (harvesting failure) to complete the precise cutting. An end-effector grasping pose control algorithm of the manipulator is proposed, which can control the end-effector to grasp along the direction of the fruit peduncle and perpendicular to the tangent direction of the picking point by estimating the pose of the sweet pepper peduncle. Finally, the robot and proposed method were verified in a plant factory. The experimental findings demonstrate that the developed harvesting robot can complete robust detection of fruit peduncles and non-destructive picking of sweet pepper, with an average picking time of about 15 seconds.},
  archive   = {C_ICRA},
  author    = {Qinghui Pan and Dong Wang and Jie Lian and Yongxiang Dong and Chaochao Qiu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610866},
  month     = {5},
  pages     = {15811-15817},
  title     = {Development of an automatic sweet pepper harvesting robot and experimental evaluation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Streamlined acquisition of large sensor data for autonomous
mobile robots to enable efficient creation and analysis of datasets.
<em>ICRA</em>, 15804–15810. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The increasing usage of modern AI techniques represents a transforming shift in the robotics domain. Training and accessing new models requires substantial amounts of application-specific data, but the limited resources onboard mobile robots (like processing power, network bandwidth, etc.) pose a challenge for the development of efficient data recording and provisioning pipelines. Furthermore, accessing specific information based on a combination of spatial, temporal and semantic information is generally not supported by currently available tools. In this paper, we present a methodology which allows the efficient recording of robotic sensor data streams. We show that our approach reduces the overall time needed until the data can be served via the spatio-temporal-semantic query interface of the semantic environment representation SEEREP. We further present that the maximum sensor data rate which can be stored to disk in real-time is increased for large robotic data types like images and point clouds in comparison to frequently employed solutions within the ROS ecosystem.},
  archive   = {C_ICRA},
  author    = {Mark Niemeyer and Julian Arkenau and Sebastian Pütz and Joachim Hertzberg},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611096},
  month     = {5},
  pages     = {15804-15810},
  title     = {Streamlined acquisition of large sensor data for autonomous mobile robots to enable efficient creation and analysis of datasets},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Osiris: Building hierarchical representations for
agricultural environments. <em>ICRA</em>, 15797–15803. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D scene graphs have recently emerged as a powerful and human-understandable way of representing complex 3D environments. These describe environments through a layered or hierarchical graph where nodes represent different spatial concepts (from low-level geometry to higher-level scene-scale reasoning) and the edges between them represent relationships. While these representations have shown great promise in indoor well-structured environments, their use in outdoor structured environments such as agricultural environments has been under-explored. A key challenge here is that concepts and structures often observed in urban indoor environments cannot be easily transferred to these novel scenes.Motivated by this challenge, this paper presents Osiris which is a 3D scene graph builder for agricultural environments. We first propose a structure of the hierarchical graph for agricultural environments consisting of rowed crops and through our proposed system Osiris incrementally construct a 3D scene graph of agricultural environments from data taken onboard a mobile robot. We validate and evaluate the performance of Osiris using real-world data collected at several farms and show that this system is able to accurately get to the underlying structure of these agricultural environments while presenting a metrically accurate and human-understandable representation.},
  archive   = {C_ICRA},
  author    = {Adam Mukuddem and Paul Amayo},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610723},
  month     = {5},
  pages     = {15797-15803},
  title     = {Osiris: Building hierarchical representations for agricultural environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforcement learning for collision-free flight exploiting
deep collision encoding. <em>ICRA</em>, 15781–15788. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work contributes a novel deep navigation policy that enables collision-free flight of aerial robots based on a modular approach exploiting deep collision encoding and reinforcement learning. The proposed solution builds upon a deep collision encoder that is trained on both simulated and real depth images using supervised learning such that it compresses the high-dimensional depth data to a low-dimensional latent space encoding collision information while accounting for the robot size. This compressed encoding is combined with an estimate of the robot’s odometry and the desired target location to train a deep reinforcement learning navigation policy that offers low-latency computation and robust sim2real performance. A set of simulation and experimental studies in diverse environments are conducted and demonstrate the efficiency of the emerged behavior and its resilience in real-life deployments.},
  archive   = {C_ICRA},
  author    = {Mihir Kulkarni and Kostas Alexis},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610287},
  month     = {5},
  pages     = {15781-15788},
  title     = {Reinforcement learning for collision-free flight exploiting deep collision encoding},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning to explore indoor environments using autonomous
micro aerial vehicles. <em>ICRA</em>, 15758–15764. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we address the challenge of exploring unknown indoor environments using autonomous aerial robots with Size Weight and Power (SWaP) constraints. The SWaP constraints induce limits on mission time requiring efficiency in exploration. We present a novel exploration framework that uses Deep Learning (DL) to predict the most likely indoor map given the previous observations, and Deep Reinforcement Learning (DRL) for exploration, designed to run on modern SWaP constraints neural processors. The DL-based map predictor provides a prediction of the occupancy of the unseen environment while the DRL-based planner determines the best navigation goals that can be safely reached to provide the most information. The two modules are tightly coupled and run onboard allowing the vehicle to safely map an unknown environment. Extensive experimental and simulation results show that our approach surpasses state-of-the-art methods by 50-60% in efficiency, which we measure by the fraction of the explored space as a function of the trajectory length.},
  archive   = {C_ICRA},
  author    = {Yuezhan Tao and Eran Iceland and Beiming Li and Elchanan Zwecher and Uri Heinemann and Avraham Cohen and Amir Avni and Oren Gal and Ariel Barel and Vijay Kumar},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610464},
  month     = {5},
  pages     = {15758-15764},
  title     = {Learning to explore indoor environments using autonomous micro aerial vehicles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Air bumper: A collision detection and reaction framework for
autonomous MAV navigation. <em>ICRA</em>, 15735–15741. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous navigation in unknown environments with obstacles remains challenging for micro aerial vehicles (MAVs) due to their limited onboard computing and sensing resources. Although various collision avoidance methods have been developed, it is still possible for drones to collide with unobserved obstacles due to unpredictable disturbances, sensor limitations, and control uncertainty. Instead of completely avoiding collisions, this article proposes Air Bumper, a collision detection and reaction framework, for fully autonomous flight in 3D environments to improve flight safety. Our framework only utilizes the onboard inertial measurement unit (IMU) to detect and estimate collisions. We further design a collision recovery control for rapid recovery and collision-aware mapping to integrate collision information into general LiDAR-based sensing and planning frameworks. Our simulation and experimental results show that the drone can rapidly detect, estimate, and recover from collisions with obstacles in 3D space and continue the flight smoothly with the help of the collision-aware map. In addition, we will open-source the implementation of Air Bumper on GitHub 1 .},
  archive   = {C_ICRA},
  author    = {Ruoyu Wang and Zixuan Guo and Yizhou Chen and Xinyi Wang and Ben M. Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611410},
  month     = {5},
  pages     = {15735-15741},
  title     = {Air bumper: A collision detection and reaction framework for autonomous MAV navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A probabilistic approach for learning and adapting shared
control skills with the human in the loop. <em>ICRA</em>, 15728–15734.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Assistive robots promise to be of great help to wheelchair users with motor impairments, for example for activities of daily living. Using shared control to provide task-specific assistance – for instance with the Shared Control Templates (SCT) framework – facilitates user control, even with low-dimensional input signals. However, designing SCTs is a laborious task requiring robotic expertise. To facilitate their design, we propose a method to learn one of their core components – active constraints – from demonstrated end-effector trajectories. We use a probabilistic model, Kernelized Movement Primitives, which additionally allows adaptation from user commands to improve the shared control skills, during both design and execution. We demonstrate that the SCTs so acquired can be successfully used to pick up an object, as well as adjusted for new environmental constraints, with our assistive robot EDAN.},
  archive   = {C_ICRA},
  author    = {Gabriel Quere and Freek Stulp and David Filliat and João Silvério},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610956},
  month     = {5},
  pages     = {15728-15734},
  title     = {A probabilistic approach for learning and adapting shared control skills with the human in the loop},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive haptic control interface for safeguarding robotic
teleoperation in hazardous steelmaking environments. <em>ICRA</em>,
15721–15727. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Steel mill is one of the most extreme and hazardous working environments due to molten iron erupted from blast furnace. Current manual labor to remove lump iron near the outlet, which is essential to prevent lump iron from scattering or blocking of molten iron, is performed by equipped human workers using a long stick tool. Thus, implementation of robotic teleoperation system is in demand to ensure the safety of workers. However, the conventional command interface is not intuitive for tool manipulation (i.e. pivoting, sweeping). Besides, haptic interface, which is used to render interaction results efficiently, still limits performance due to narrow workspace and insufficient kinesthetic feedback output compared to requirements. This paper proposes a novel haptic command interface (POstick) specified to lump iron removal task with two types (KF and VF). Both POsticks have rod-shaped end tip which is identical to actual tool already used to accelerate training. POstick-KF has large workspace and high kinesthetic feedback output satisfying requirements. Further, POstick-VF has strength with unlimited workspace at the expense of the amount of haptic information from simple vibrotactile feedback. User study to compare the performance of POsticks and conventional interface reveals that POstick-KF and VF showed superior interaction and tracking ability, respectively. Moreover, these two properties are in trade-off relationship that cannot be compatible. Finally, we proposed a seamless and automatic conversion mechanism from POstick-VF to KF, and vice versa, to cover up inherent limits of haptic devices.},
  archive   = {C_ICRA},
  author    = {Jaehyun Park and Il Seop Choi and Sang-Woo Choi and Keehoon Kim},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611107},
  month     = {5},
  pages     = {15721-15727},
  title     = {Adaptive haptic control interface for safeguarding robotic teleoperation in hazardous steelmaking environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Autonomous and teleoperation control of a drawing robot
avatar. <em>ICRA</em>, 15714–15720. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A drawing robot avatar is a robotic system that allows for telepresence-based drawing, enabling users to remotely control a robotic arm and create drawings in real-time from a remote location. The proposed control framework aims to improve bimanual robot telepresence quality by reducing the user workload and required prior knowledge through the automation of secondary or auxiliary tasks. The introduced novel method calculates the near-optimal Cartesian end-effector pose in terms of visual feedback quality for the attached eye-to-hand camera with motion constraints in consideration. The effectiveness is demonstrated by conducting user studies of drawing reference shapes using the implemented robot avatar compared to stationary and teleoperated camera pose conditions. Our results demonstrate that the proposed control framework offers improved visual feedback quality and drawing performance.},
  archive   = {C_ICRA},
  author    = {Lingyun Chen and Abdeldjallil Naceri and Abdalla Swikir and Sandra Hirche and Sami Haddadin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610310},
  month     = {5},
  pages     = {15714-15720},
  title     = {Autonomous and teleoperation control of a drawing robot avatar},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design octree-based method to improve model-mediated
teleoperation in tactile internet. <em>ICRA</em>, 15707–15713. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a model-mediated tele-operation (MMT) system using an octree-based model (OBM) to spatially map the environment impedance for the emerging use cases in Tactile Internet. Different from the existing just-noticeable-difference (JND) based MMT, our method avoids continuous transmission of environment impedance. Moreover, it allows the local model to generate accurate force feedback and reduces the number of model updates. Furthermore, the OBM can be deployed with or without previous knowledge of the environment. An online estimation of the OBM is proposed using a JND and a rate-of-change threshold. An offline estimation method is also proposed when the geometry and impedance parameters of the remote environment are known. In addition, a point cloud-based force rendering algorithm is tailored to use the OBM, thereby allowing the generating of force feedback for complex environments. An experiment without human-in-the-loop was conducted, showing that for an online estimated OBM, the accuracy of the force feedback was improved by up to 44 percent while using less than half the number of model updates when compared to JND-based MMT. Another experiment with a human operator interacting with a virtual environment showed that using an offline estimated OBM improves the accuracy of the force feedback and is reliable against packet loss and short temporal breakdown of the communication link.},
  archive   = {C_ICRA},
  author    = {Mads Antonsen and Francesco Chinello and Qi Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610295},
  month     = {5},
  pages     = {15707-15713},
  title     = {Design octree-based method to improve model-mediated teleoperation in tactile internet},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intelligent mode-switching framework for teleoperation.
<em>ICRA</em>, 15692–15698. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Teleoperation can be very difficult due to limited perception, high communication latency, and limited degrees of freedom (DoFs) at the operator side. Autonomous teleoperation is proposed to overcome this difficulty by predicting user intentions and performing some parts of the task autonomously to decrease the demand on the operator and increase the task completion rate. However, decision-making for mode-switching is generally assumed to be done by the operator, which brings an extra DoF to be controlled by the operator and introduces extra mental demand. On the other hand, the communication perspective is not investigated in the current literature, although communication imperfections and resource limitations are the main bottlenecks for teleoperation. In this study, we propose an intelligent mode-switching framework by jointly considering mode-switching and communication systems. User intention recognition is done at the operator side. Based on user intention recognition, a deep reinforcement learning (DRL) agent is trained and deployed at the operator side to seamlessly switch between autonomous and teleoperation modes. A real-world data set is collected from our teleoperation testbed to train both user intention recognition and DRL algorithms. Our results show that the proposed framework can achieve up to 50% communication load reduction with improved task completion probability.},
  archive   = {C_ICRA},
  author    = {Burak Kizilkaya and Changyang She and Guodong Zhao and Muhammad Ali Imran},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611333},
  month     = {5},
  pages     = {15692-15698},
  title     = {Intelligent mode-switching framework for teleoperation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lightweight and compliant bilateral teleoperation system
with anthropomorphic arms for aerial and ground service operations.
<em>ICRA</em>, 15685–15691. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a bilateral teleoperation system based on smart servos for the realization of dexterous manipulation tasks with aerial robots or in ground service applications, facilitating the transferability of cognitive capabilities of human workers to robots operating remotely or in high altitude workspaces. The system consists of a pair of lightweight and compliant anthropomorphic dual arm manipulators (LiCAS) in leader-follower configuration. The leader dual arm (LDA) captures the movements of the operator’s arms to obtain the desired joint references, sent to the follower dual arm (FDA) to reproduce in a natural and intuitive way the manipulation task. A model of the smart servos is derived, exploiting the feedback from the FDA actuators to provide the kinesthetic feedback to the LDA, using the pulse width modulation signal (PWM) along with the joint speed to estimate the interaction torque. The mechanical joint compliance of the FDA allows the passive accommodation of the arms to the physical interactions with the manipulated objects or the environment, whereas the very low weight of the arms (1.0 kg LDA, 2.5 kg FDA) and the human-size and human-like kinematics facilitate their use in a wide variety of applications. The performance of the system is evaluated using an industrial task board for benchmarking, and in two illustrative bimanual aerial manipulation tasks.},
  archive   = {C_ICRA},
  author    = {Alejandro Suarez and Antonio Gonzalez-Morgado and Anibal Ollero},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611383},
  month     = {5},
  pages     = {15685-15691},
  title     = {Lightweight and compliant bilateral teleoperation system with anthropomorphic arms for aerial and ground service operations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Wearable haptics for a marionette-inspired teleoperation of
highly redundant robotic systems. <em>ICRA</em>, 15670–15676. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The teleoperation of complex, kinematically redundant robots with loco-manipulation capabilities represents a challenge for human operators, who have to learn how to operate the many degrees of freedom of the robot to accomplish a desired task. In this context, developing an easy-to-learn and easy-to-use human-robot interface is paramount. Recent works introduced a novel teleoperation concept, which relies on a virtual physical interaction interface between the human operator and the remote robot equivalent to a &quot;Marionette&quot; control, but whose feedback was limited to only visual feedback on the human side. In this paper, we propose extending the &quot;Marionette&quot; interface by adding a wearable haptic interface to cope with the limitations given by the previous works. Leveraging the additional haptic feedback modality, the human operator gains full sensorimotor control over the robot, and the awareness about the robot’s response and interactions with the environment is greatly improved. We evaluated the proposed interface and the related teleoperation framework with naive users, assessing the teleoperation performance and the user experience with and without haptic feedback. The conducted experiments consisted in a loco-manipulation mission with the CENTAURO robot, a hybrid leg-wheel quadruped with a humanoid dual-arm upper body.},
  archive   = {C_ICRA},
  author    = {Davide Torielli and Leonardo Franco and Maria Pozzi and Luca Muratore and Monica Malvezzi and Nikos Tsagarakis and Domenico Prattichizzo},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610788},
  month     = {5},
  pages     = {15670-15676},
  title     = {Wearable haptics for a marionette-inspired teleoperation of highly redundant robotic systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Development of a 3-RRS micromanipulator based on
origami-inspired spherical joint. <em>ICRA</em>, 15636–15641. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, micromanipulation technology has achieved extensive applications in industry and life science. Improving the precision and bandwidth of the micromanipulator and simultaneously reducing size, weight, and cost pose significant challenges to the existing micromanipulator design and fabrication methods. Here, we propose a 3-RRS micromanipulator with an origami-inspired spherical joint based on the PC-MEMS process, aiming for miniaturization and cost-effectiveness. The spherical joint allows rotations of 140° around the x-axis approximately, 140° around the y-axis approximately, and 20° around the z-axis approximately. The micromanipulator has weights of 0.8 g, dimensions of 16 mm × 16 mm × 22 mm, and workspace of 0.7 mm 3 . The end platform of the micromanipulator can be equipped with various effectors to accomplish different kinds of tasks. Experimental results validated its high precision and bandwidth, exhibiting its potential to perform intricate micromanipulation tasks.},
  archive   = {C_ICRA},
  author    = {Haoqi Han and Xiaoming Liu and Yan Chen and Hao Pang and Xiaoqing Tang and Dan Liu and Qiang Huang and Tatsuo Arai},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610385},
  month     = {5},
  pages     = {15636-15641},
  title     = {Development of a 3-RRS micromanipulator based on origami-inspired spherical joint},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic adaptive imaging system on optoelectronic tweezers
platform. <em>ICRA</em>, 15622–15627. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Optoelectronic tweezers (OET) has shown great promise in various applications, especially in the precise manipulation of microparticles and microorganisms on a micron and nanometer scale. This technology significantly enhances the efficiency of single-cell sorting and the development of antibody-based drugs. However, conventional OET platforms are limited by issues such as low autofocusing accuracy, restricted imaging field of view, and uneven illumination. To overcome these limitations, we have innovatively developed a dynamic adaptive imaging system. By incorporating peak-finding and in situ Gaussian blur compensation algorithms, we achieved rapid automatic focusing and illumination shadow compensation across an expanded field of view. At the same time, the system can also dynamically adjust compensation parameters under different lighting conditions. Our system has successfully completed comprehensive scanning of the optoelectronic tweezers chip, achieving a 60% reduction in autofocus time and a 15.8% improvement in lighting uniformity. Moreover, this imaging system demonstrates robust versatility and can serve as a reference for other optical systems.},
  archive   = {C_ICRA},
  author    = {Ao Wang and Chunyuan Gan and Haocheng Han and Hongyi Xiong and Jiawei Zhao and Chutian Wang and Lin Feng},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611608},
  month     = {5},
  pages     = {15622-15627},
  title     = {Dynamic adaptive imaging system on optoelectronic tweezers platform},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weakly-supervised depth completion during robotic
micromanipulation from a monocular microscopic image. <em>ICRA</em>,
15615–15621. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Obtaining three-dimensional information, especially the z-axis depth information, is crucial for robotic micromanipulation. Due to the unavailability of depth sensors such as lidars in micromanipulation setups, traditional depth acquisition methods such as depth from focus or depth from defocus directly infer depth from microscopic images and suffer from poor resolution. Alternatively, micromanipulation tasks obtain accurate depth information by detecting the contact between an end-effector and an object (e.g., a cell). Despite its high accuracy, only sparse depth data can be obtained due to its low efficiency. This paper aims to address the challenge of acquiring dense depth information during robotic cell micromanipulation. A weakly-supervised depth completion network is proposed to take cell images and sparse depth data obtained by contact detection as input to generate a dense depth map. A two-stage data augmentation method is proposed to augment the sparse depth data, and the depth map is optimized by a network refinement method. The experimental results show that the MAE value of the depth prediction error is less than 0.3 µm, which proves the accuracy and effectiveness of the method. This deep learning network pipeline can be seamlessly integrated with the robotic micromanipulation tasks to provide accurate depth information.},
  archive   = {C_ICRA},
  author    = {Han Yang and Yufei Jin and Guanqiao Shan and Yibin Wang and Yongbin Zheng and Jiangfan Yu and Yu Sun and Zhouran Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611357},
  month     = {5},
  pages     = {15615-15621},
  title     = {Weakly-supervised depth completion during robotic micromanipulation from a monocular microscopic image},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated surgical knot tying on mini-incision with
micro-suture based on dual-arm nanorobot under stereo microscope.
<em>ICRA</em>, 15608–15614. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Knot tying is an essential task for robotic surgery, which is routinely realized by dual-arm robotic manipulation. Despite the well-established protocol and progress at macro scale so far, there remain challenges to further advance robotic knot tying technique, particularly in terms of decreasing space consumption with better dexterity, higher precision, and well biomechanical compatibility. In this paper, we propose a novel dual-arm nanorobotic system setup for automated knot tying performed on mini-incision under stereo microscope, featured by an additional rotation degree of freedom mounted on each arm. With this setup, an optimized motion trajectory planning under standard knot-tying protocol is also presented in order to support tying knots with shorter and thinner suture. Leveraging the natural advantage of nanorobotics and microscope, the proposed system is capable of tying consecutive throws with micro-suture on mini-incision, like in vascular anastomosis or microsurgery. We successfully evaluated the knot tying system on 2.0 mm wide bionic blood vessel with 30 mm long #8-0 micro-suture. We finally tested the mechanical strength of the knots for potential medical assessment.},
  archive   = {C_ICRA},
  author    = {Yujie Jiang and Xiang Fu and Chengxi Zhong and Teng Li and Haojian Lu and Song Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610173},
  month     = {5},
  pages     = {15608-15614},
  title     = {Automated surgical knot tying on mini-incision with micro-suture based on dual-arm nanorobot under stereo microscope},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Skill learning in robot-assisted micro-manipulation through
human demonstrations with attention guidance. <em>ICRA</em>,
15601–15607. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For the development of robotic systems for micromanipulation, it is challenging to design appropriate control strategies due to either the lack of sufficient information for feedback or the difficulty in extracting subtle yet critical visual features. With the same system under the teleoperated mode, however, human operators seem to be able to complete the task more successfully with an inherent motion and control strategy. The extraction of implicit human attention during the task and integration of this with robot control could provide crucial guidance in the design of feature extraction and motion control algorithms. In this paper, a micro-assembly task of miniature thin membrane sensors is considered. For human demonstrations, we collected data from repeated tests performed by ten operators following three motion strategies. The human attention during the task is explored according to the coordinates of the eye gaze, and then a neural network with gaze-guided attention is trained to segment the visual Region of Interest (ROI). After quantitative evaluation of operator results in terms of success rate, efficiency, reset time, and the Index of Pupillary Activity (IPA), an optimized motion strategy based on the &quot;palpation&quot; framework was derived. Consequently, we apply this strategy to automated tasks and achieve superior results than human operators, showing an average task completion time of 34.8±5.9s and a success rate of over 90%.},
  archive   = {C_ICRA},
  author    = {Yujian An and Jianxin Yang and Jinkai Li and Bingze He and Yao Guo and Guang-Zhong Yang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610945},
  month     = {5},
  pages     = {15601-15607},
  title     = {Skill learning in robot-assisted micro-manipulation through human demonstrations with attention guidance},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Using large language models to generate and apply
contingency handling procedures in collaborative assembly applications.
<em>ICRA</em>, 15585–15592. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In manufacturing, minimizing operational delays is crucial for efficiency and resilience. Therefore, efficiently handling contingencies is essential in human-robot teams working on assembly (i.e., collaborative assembly) applications. This paper introduces a novel approach to generating contingency handling procedures by leveraging recent advances in Large Language Models (LLMs). Our approach uses LLMs to update the required tasks in hierarchical task networks (HTNs) to handle contingencies. The results demonstrate that our approach can handle various contingencies in assembly applications and minimize the impact on the assembly completion time.},
  archive   = {C_ICRA},
  author    = {Jeon Ho Kang and Neel Dhanaraj and Siddhant Wadaskar and Satyandra K. Gupta},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610875},
  month     = {5},
  pages     = {15585-15592},
  title     = {Using large language models to generate and apply contingency handling procedures in collaborative assembly applications},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Specifying and monitoring safe driving properties with scene
graphs. <em>ICRA</em>, 15577–15584. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the proliferation of autonomous vehicles (AVs) comes the need to ensure they abide by safe driving properties. Specifying and monitoring such properties, however, is challenging because of the mismatch between the semantic space over which typical driving properties are asserted (e.g., vehicles, pedestrians, intersections) and the sensed inputs of AVs. Existing efforts either assume for such semantic data to be available or develop bespoke methods for capturing it. Instead, this work introduces a framework that can extract scene graphs (SGs) from sensor inputs to capture the entities related to the AV, and a domain-specific language that enables building propositions over those graphs and composing them through temporal logic. We implemented the framework to monitor for specification violations of 3 top AVs from the CARLA Autonomous Driving Leaderboard, and found that the AVs violated 71% of properties during at least one test. Artifact available at https://github.com/less-lab-uva/SGSM.},
  archive   = {C_ICRA},
  author    = {Felipe Toledo and Trey Woodlief and Sebastian Elbaum and Matthew B. Dwyer},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610973},
  month     = {5},
  pages     = {15577-15584},
  title     = {Specifying and monitoring safe driving properties with scene graphs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel metric for detecting quadrotor loss-of-control.
<em>ICRA</em>, 15570–15576. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unmanned aerial vehicles (UAVs) are becoming an integral part of both industry and society. In particular, the quadrotor is now invaluable across a plethora of fields and recent developments, such as the inclusion of aerial manipulators, only extends their versatility. As UAVs become more widespread, preventing loss-of-control (LOC) is an ever growing concern. Unfortunately, LOC is not clearly defined for quadrotors, or indeed, many other autonomous systems. Moreover, any existing definitions are often incomplete and restrictive. A novel metric, based on actuator capabilities, is introduced to detect LOC in quadrotors. The potential of this metric for LOC detection is demonstrated through both simulated and real quadrotor flight data. It is able to detect LOC induced by actuator faults without explicit knowledge of the occurrence and nature of the failure. The proposed metric is also sensitive enough to detect LOC in more nuanced cases, where the quadrotor remains undamaged but nevertheless losses control through an aggressive yawing manoeuvre. As the metric depends only on system and actuator models, it is sufficiently general to be applied to other systems.},
  archive   = {C_ICRA},
  author    = {Jasper J. Van Beers and Prashant Solanki and Coen C. De Visser},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610662},
  month     = {5},
  pages     = {15570-15576},
  title     = {A novel metric for detecting quadrotor loss-of-control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Utilizing a malfunctioning 3D printer by modeling its
dynamics with machine learning. <em>ICRA</em>, 15562–15569. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To create a self-repairing 3D printer, it must continue operating even after experiencing corruption. This work focuses on developing a method to effectively utilize a malfunctioning printer for reliable printing. This method can be applied by the printer itself for self-repair and enhance the reliability of commercial 3D printers. We achieve this by modeling the dynamics of the corrupted printer using a machine learning model that by observing one trajectory infers the corrupted printer dynamics to improve its accuracy. Our method is evaluated on a digital twin of the 3D printer, demonstrating its capability to enable the printer to operate reliably, even when encountering new corruptions not encountered during training. The scripts are public on https://github.com/piotrpiekos/adaptive-printer.},
  archive   = {C_ICRA},
  author    = {Renzo Caballero and Piotr Piękos and Eric Feron and Jürgen Schmidhuber},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611304},
  month     = {5},
  pages     = {15562-15569},
  title     = {Utilizing a malfunctioning 3D printer by modeling its dynamics with machine learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Relaxed hover solution based control for a bi-copter with
rotor and servo stuck failure. <em>ICRA</em>, 15517–15523. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As the usage of bi-copters increases in military and civilian fields, the demand for reliable bi-copters is on the rise. This study focuses on controlling a bi-copter under rotor or servo stuck failure. A relaxed hover solution is derived for the bi-copter, by solving an optimization problem subject to rotor and servo stuck failures. The solution is used for designing a reduced attitude controller based on linear quadratic regulator (LQR). To ensure hover capability, we introduce a position controller based on a cascaded-PID. The numerical simulations are conducted to demonstrate that position control is possible, even with complete rotor or servo stuck failure, by driving the bi-copter into relaxed hover state through the abandonment of the yaw channel. Meanwhile, the FTC scheme is examined under constant wind disturbances and uncertainties in the rotational damping parameters.},
  archive   = {C_ICRA},
  author    = {Haixin Zhao and Ruifeng Li and Quan Quan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611350},
  month     = {5},
  pages     = {15517-15523},
  title     = {Relaxed hover solution based control for a bi-copter with rotor and servo stuck failure},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Orbit-surgical: An open-simulation framework for learning
surgical augmented dexterity. <em>ICRA</em>, 15509–15516. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Physics-based simulations have accelerated progress in robot learning for driving, manipulation, and locomotion. Yet, a fast, accurate, and robust surgical simulation environment remains a challenge. In this paper, we present Orbit-Surgical, a physics-based surgical robot simulation framework with photorealistic rendering in NVIDIA Omniverse. We provide 14 benchmark surgical tasks for the da Vinci Research Kit (dVRK) and Smart Tissue Autonomous Robot (STAR) which represent common subtasks in surgical training. Orbit-Surgical leverages GPU parallelization to train reinforcement learning and imitation learning algorithms to facilitate study of robot learning to augment human surgical skills. Orbit-Surgical also facilitates realistic synthetic data generation for active perception tasks. We demonstrate Orbit-Surgical sim-to-real transfer of learned policies onto a physical dVRK robot.Project website: orbit-surgical.github.io},
  archive   = {C_ICRA},
  author    = {Qinxi Yu and Masoud Moghani and Karthik Dharmarajan and Vincent Schorp and William Chung-Ho Panitch and Jingzhou Liu and Kush Hari and Huang Huang and Mayank Mittal and Ken Goldberg and Animesh Garg},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611637},
  month     = {5},
  pages     = {15509-15516},
  title     = {Orbit-surgical: An open-simulation framework for learning surgical augmented dexterity},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lens capsule tearing in cataract surgery using reinforcement
learning. <em>ICRA</em>, 15501–15508. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cataract is the leading cause of blindness worldwide with an increasing number of patients due to changing demographics, making automation an important part in future surgical treatment. In this work, we focus on a substep of cataract surgery, the Continuous Curvilinear Capsulorhexis (CCC). With a high complexity, this task is an ideal candidate for Reinforcement Learning (RL) in simulation. First, we present an interactive and physically realistic simulation based on the Finite Element Method (FEM) that mimics the tearing behavior of soft tissue during CCC. Then, we train and evaluate RL models in simulation, demonstrating that the trained policies can complete the CCC in 85% of cases. We also show that applying domain randomization techniques make the policy more robust against changes in geometrical and biomechanical boundary conditions.},
  archive   = {C_ICRA},
  author    = {Rebekka Charlotte Peter and Steffen Peikert and Ludwig Haide and Doan Xuan Viet Pham and Tahar Chettaoui and Eleonora Tagliabue and Paul Maria Scheikl and Johannes Fauser and Matthias Hillenbrand and Gerhard Neumann and Franziska Mathis-Ullrich},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611714},
  month     = {5},
  pages     = {15501-15508},
  title     = {Lens capsule tearing in cataract surgery using reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient and accurate mapping of subsurface anatomy via
online trajectory optimization for robot assisted surgery.
<em>ICRA</em>, 15478–15484. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic surgical subtask automation has the potential to reduce the per-patient workload of human surgeons. There are a variety of surgical subtasks that require geometric information of subsurface anatomy, such as the location of tumors, which necessitates accurate and efficient surgical sensing. In this work, we propose an automated sensing method that maps 3D subsurface anatomy to provide such geometric knowledge. We model the anatomy via a Bayesian Hilbert map-based probabilistic 3D occupancy map. Using the 3D occupancy map, we plan sensing paths on the surface of the anatomy via a graph search algorithm, A * search, with a cost function that enables the trajectories generated to balance between exploration of unsensed regions and refining the existing probabilistic understanding. We demonstrate the performance of our proposed method by comparing it against 3 different methods in several anatomical environments including a real-life CT scan dataset. The experimental results show that our method efficiently detects relevant subsurface anatomy with shorter trajectories than the comparison methods, and the resulting occupancy map achieves high accuracy.},
  archive   = {C_ICRA},
  author    = {Brian Y. Cho and Alan Kuntz},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610456},
  month     = {5},
  pages     = {15478-15484},
  title     = {Efficient and accurate mapping of subsurface anatomy via online trajectory optimization for robot assisted surgery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-to-sim deformable object manipulation: Optimizing
physics models with residual mappings for robotic surgery.
<em>ICRA</em>, 15471–15477. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate deformable object manipulation (DOM) is essential for achieving autonomy in robotic surgery, where soft tissues are being displaced, stretched, and dissected. Many DOM methods can be powered by simulation, which ensures realistic deformation by adhering to the governing physical constraints and allowing for model prediction and control. However, real soft objects in robotic surgery, such as membranes and soft tissues, have complex, anisotropic physical parameters that a simulation with simple initialization from cameras may not fully capture. To use the simulation techniques in real surgical tasks, the real-to-sim gap needs to be properly compensated. In this work, we propose an online, adaptive parameter tuning approach for simulation optimization that (1) bridges the real-to-sim gap between a physics simulation and observations obtained 3D perceptions through estimating a residual mapping and (2) optimizes its stiffness parameters online. Our method ensures a small residual gap between the simulation and observation and improves the simulation’s predictive capabilities. The effectiveness of the proposed mechanism is evaluated in the manipulation of both a thin-shell and volumetric tissue, representative of most tissue scenarios. This work contributes to the advancement of simulation-based deformable tissue manipulation and holds potential for improving surgical autonomy.},
  archive   = {C_ICRA},
  author    = {Xiao Liang and Fei Liu and Yutong Zhang and Yuelei Li and Shan Lin and Michael Yip},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610263},
  month     = {5},
  pages     = {15471-15477},
  title     = {Real-to-sim deformable object manipulation: Optimizing physics models with residual mappings for robotic surgery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ada-tracker: Soft tissue tracking via inter-frame and
adaptive-template matching. <em>ICRA</em>, 15463–15470. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft tissue tracking is crucial for computer-assisted interventions. Existing approaches mainly rely on extracting discriminative features from the template and videos to recover corresponding matches. However, it is difficult to adopt these techniques in surgical scenes, where tissues are changing in shape and appearance throughout the surgery. To address this problem, we exploit optical flow to naturally capture the pixel-wise tissue deformations and adaptively correct the tracked template. Specifically, we first implement an inter-frame matching mechanism to extract a coarse region of interest based on optical flow from consecutive frames. To accommodate appearance change and alleviate drift, we then propose an adaptive-template matching method, which updates the tracked template based on the reliability of the estimates. Our approach, Ada-Tracker, enjoys both short-term dynamics modeling by capturing local deformations and long-term dynamics modeling by introducing global temporal compensation. We evaluate our approach on the public SurgT benchmark, which is generated from Hamlyn, SCARED, and Kidney boundary datasets. The experimental results show that Ada-Tracker achieves superior accuracy and performs more robustly against prior works. Code is available at https://github.com/wrld/Ada-Tracker.},
  archive   = {C_ICRA},
  author    = {Jiaxin Guo and Jiangliu Wang and Zhaoshuo Li and Tongyu Jia and Qi Dou and Yun-Hui Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611030},
  month     = {5},
  pages     = {15463-15470},
  title     = {Ada-tracker: Soft tissue tracking via inter-frame and adaptive-template matching},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust surgical tool tracking with pixel-based probabilities
for projected geometric primitives. <em>ICRA</em>, 15455–15462. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Controlling robotic manipulators via visual feedback requires a known coordinate frame transformation between the robot and the camera. Uncertainties in mechanical systems as well as camera calibration create errors in this coordinate frame transformation. These errors result in poor localization of robotic manipulators and create a significant challenge for applications that rely on precise interactions between manipulators and the environment. In this work, we estimate the camera-to-base transform and joint angle measurement errors for surgical robotic tools using an image based insertion-shaft detection algorithm and probabilistic models. We apply our proposed approach in both a structured environment as well as an unstructured environment and measure to demonstrate the efficacy of our methods.},
  archive   = {C_ICRA},
  author    = {Christopher D’Ambrosia and Florian Richter and Zih-Yun Chiu and Nikhil Shinde and Fei Liu and Henrik I. Christensen and Michael C. Yip},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610378},
  month     = {5},
  pages     = {15455-15462},
  title     = {Robust surgical tool tracking with pixel-based probabilities for projected geometric primitives},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel robotic bronchoscope with a spring-based extensible
segment for improving steering ability. <em>ICRA</em>, 15448–15454. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bronchoscopy, as an essential minimally invasive diagnostic and therapeutic modality, assumes a pivotal role in the early detection of lung cancer. However, the complex anatomy of the airway and the fixed length of the bronchoscope’s bending segment, along with its external propulsion property, pose challenges, including the risk of bleeding. This paper introduces a 4 mm diameter robot-assisted bronchoscope with a spring-based extensible segment. By manipulating two driven rods, the segment can be lengthened or shortened. The advantages of the extensible segment are discussed in two main aspects through theoretical analysis and experimentation. Firstly, the extensible segment enables the bronchoscope to move in a follow-the-leader motion mode or fixed-angle motion mode, navigating through narrow corners that are inaccessible to fixed-length bronchoscopes. It can also be shortened to increase its stiffness when it reaches the target position, creating a stable surgical platform for procedures like biopsies. In addition, a tailored master device has been developed to control the extensible bronchoscope in an isotropic manner. Phantom experiments confirm the feasibility and effectiveness of the extensible bronchoscope.},
  archive   = {C_ICRA},
  author    = {Jie Wang and Chengquan Hu and Jingyi Kang and Jiayuan Liu and Longfei Ma and Hongen Liao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611673},
  month     = {5},
  pages     = {15448-15454},
  title     = {A novel robotic bronchoscope with a spring-based extensible segment for improving steering ability},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Object-centric cross-modal feature distillation for
event-based object detection. <em>ICRA</em>, 15440–15447. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Event cameras are gaining popularity due to their unique properties, such as their low latency and high dynamic range. One task where these benefits can be crucial is real-time object detection. However, RGB detectors still outperform event-based detectors due to the sparsity of the event data and missing visual details. In this paper, we propose a cross-modality feature distillation method that can focus on regions where the knowledge distillation works best to shrink the detection performance gap between these two modalities. We achieve this by using an object-centric slot attention mechanism that can iteratively decouple feature maps into object-centric features and corresponding pixel-features used for distillation. We evaluate our novel distillation approach on a synthetic and a real event dataset with aligned grayscale images as a teacher modality. We show that object-centric distillation allows to significantly improve the performance of the event-based student object detector, nearly halving the performance gap with respect to the teacher.},
  archive   = {C_ICRA},
  author    = {Lei Li and Alexander Linger and Mario Millhäusler and Vagia Tsiminaki and Yuanyou Li and Dengxin Dai},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610325},
  month     = {5},
  pages     = {15440-15447},
  title     = {Object-centric cross-modal feature distillation for event-based object detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SKT-hang: Hanging everyday objects via object-agnostic
semantic keypoint trajectory generation. <em>ICRA</em>, 15433–15439. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of hanging a wide range of grasped objects on diverse supporting items. Hanging objects is a ubiquitous task that is encountered in numerous aspects of our everyday lives. However, both the objects and supporting items can exhibit substantial variations in their shapes and structures, bringing two challenging issues: (1) determining the task-relevant geometric structures across different objects and supporting items, and (2) identifying a robust action sequence to accommodate the shape variations of supporting items. To this end, we propose Semantic Keypoint Trajectory (SKT), an object-agnostic representation that is highly versatile and applicable to various everyday objects. We also propose Shape-conditioned Trajectory Deformation Network (SCTDN), a model that learns to generate SKT by deforming a template trajectory based on the task-relevant geometric structure features of the supporting items. We conduct extensive experiments and demonstrate substantial improvements in our framework over existing robot hanging methods in the success rate and inference time. Finally, our simulation-trained framework shows promising hanging results in the real world. For videos and supplementary materials, please visit our project webpage: https://hcis-lab.github.io/SKT-Hang/.},
  archive   = {C_ICRA},
  author    = {Chia-Liang Kuo and Yu-Wei Chao and Yi-Ting Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610266},
  month     = {5},
  pages     = {15433-15439},
  title     = {SKT-hang: Hanging everyday objects via object-agnostic semantic keypoint trajectory generation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Composing pre-trained object-centric representations for
robotics from “what” and “where” foundation models. <em>ICRA</em>,
15424–15432. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There have recently been large advances both in pre-training visual representations for robotic control and segmenting unknown category objects in general images. To leverage these for improved robot learning, we propose POCR, a new framework for building pre-trained object-centric representations for robotic control. Building on theories of &quot;what-where&quot; representations in psychology and computer vision, we use segmentations from a pre-trained model to stably locate across timesteps, various entities in the scene, capturing &quot;where&quot; information. To each such segmented entity, we apply other pre-trained models that build vector descriptions suitable for robotic control tasks, thus capturing &quot;what&quot; the entity is. Thus, our pre-trained object-centric representations for control are constructed by appropriately combining the outputs of off-the-shelf pre-trained models, with no new training. On various simulated and real robotic tasks, we show that imitation policies for robotic manipulators trained on POCR achieve better performance and systematic generalization than state of the art pre-trained representations for robotics, as well as prior object-centric representations that are typically trained from scratch.},
  archive   = {C_ICRA},
  author    = {Junyao Shi and Jianing Qian and Yecheng Jason Ma and Dinesh Jayaraman},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610695},
  month     = {5},
  pages     = {15424-15432},
  title     = {Composing pre-trained object-centric representations for robotics from &quot;What&quot; and &quot;Where&quot; foundation models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3D-OAE: Occlusion auto-encoders for self-supervised learning
on point clouds. <em>ICRA</em>, 15416–15423. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The manual annotation for large-scale point clouds is still tedious and unavailable for many harsh real-world tasks. Self-supervised learning, which is used on raw and unlabeled data to pre-train deep neural networks, is a promising approach to address this issue. Existing works usually take the common aid from auto-encoders to establish the self-supervision by the self-reconstruction schema. However, the previous auto-encoders merely focus on the global shapes and do not distinguish the local and global geometric features apart. To address this problem, we present a novel and efficient self-supervised point cloud representation learning framework, named 3D Occlusion Auto-Encoder (3D-OAE), to facilitate the detailed supervision inherited in local regions and global shapes. We propose to randomly occlude some local patches of point clouds and establish the supervision via inpainting the occluded patches using the remaining ones. Specifically, we design an asymmetrical encoder-decoder architecture based on standard Transformer, where the encoder operates only on the visible subset of patches to learn local patterns, and a lightweight decoder is designed to leverage these visible patterns to infer the missing geometries via self-attention. We find that occluding a very high proportion of the input point cloud (e.g. 75%) will still yield a nontrivial self-supervisory performance, which enables us to achieve 3-4 times faster during training but also improve accuracy. Experimental results show that our approach outperforms the state-of-the-art on a diverse range of down-stream discriminative and generative tasks. Code is available at https://github.com/junshengzhou/3D-OAE.},
  archive   = {C_ICRA},
  author    = {Junsheng Zhou and Xin Wen and Baorui Ma and Yu-Shen Liu and Yue Gao and Yi Fang and Zhizhong Han},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610588},
  month     = {5},
  pages     = {15416-15423},
  title     = {3D-OAE: Occlusion auto-encoders for self-supervised learning on point clouds},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reg-NF: Efficient registration of implicit surfaces within
neural fields. <em>ICRA</em>, 15409–15415. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Neural fields, coordinate-based neural networks, have recently gained popularity for implicitly representing a scene. In contrast to classical methods that are based on explicit representations such as point clouds, neural fields provide a continuous scene representation able to represent 3D geometry and appearance in a way which is compact and ideal for robotics applications. However, limited prior methods have investigated registering multiple neural fields by directly utilising these continuous implicit representations. In this paper, we present Reg-NF, a neural fields-based registration that optimises for the relative 6-DoF transformation between two arbitrary neural fields, even if those two fields have different scale factors. Key components of Reg-NF include a bidirectional registration loss, multi-view surface sampling, and utilisation of volumetric signed distance functions (SDFs). We showcase our approach on a new neural field dataset for evaluating registration problems. We provide an exhaustive set of experiments and ablation studies to identify the performance of our approach, while also discussing limitations to provide future direction to the research community on open challenges in utilizing neural fields in unconstrained environments.},
  archive   = {C_ICRA},
  author    = {Stephen Hausler and David Hall and Sutharsan Mahendren and Peyman Moghadam},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610982},
  month     = {5},
  pages     = {15409-15415},
  title     = {Reg-NF: Efficient registration of implicit surfaces within neural fields},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural implicit swept volume models for fast collision
detection. <em>ICRA</em>, 15402–15408. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collision detection is one of the most time-consuming operations during motion planning. Thus, there is an increasing interest in exploring machine learning techniques to speed up collision detection and sampling-based motion planning. A recent line of research focuses on utilizing neural signed distance functions of either the robot geometry or the swept volume of the robot motion. Building on this, we present a novel neural implicit swept volume model to continuously represent arbitrary motions parameterized by their start and goal configurations. This allows to quickly compute signed distances for any point in the task space to the robot motion. Further, we present an algorithm combining the speed of the deep learning-based signed distance computations with the strong accuracy guarantees of geometric collision checkers. We validate our approach in simulated and real-world robotic experiments, and demonstrate that it is able to speed up a commercial bin picking application.},
  archive   = {C_ICRA},
  author    = {Dominik Joho and Jonas Schwinn and Kirill Safronov},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611687},
  month     = {5},
  pages     = {15402-15408},
  title     = {Neural implicit swept volume models for fast collision detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MMPI: A flexible radiance field representation by multiple
multi-plane images blending. <em>ICRA</em>, 15395–15401. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a flexible representation of neural radiance fields based on multi-plane images (MPI), for high-quality view synthesis of complex scenes. MPI with Normalized Device Coordinate (NDC) parameterization is widely used in NeRF learning for its simple definition, easy calculation, and powerful ability to represent unbounded scenes. However, existing NeRF works that adopt MPI representation for novel view synthesis can only handle simple forward-facing unbounded scenes (e.g., the scenes in the LLFF dataset), where the input cameras are all observing in similar directions with small relative translations. Hence, extending these MPIbased methods to more complex scenes like large-range or even 360-degree scenes is very challenging. In this paper, we explore the potential of MPI and show that MPI can synthesize high-quality novel views of complex scenes with diverse camera distributions and view directions, which are not only limited to simple forward-facing scenes. Our key idea is to encode the neural radiance field with multiple MPIs facing different directions and blend them with an adaptive blending operation. For each region of the scene, the blending operation gives larger blending weights to those advantaged MPIs with stronger local representation abilities while giving lower weights to those with weaker representation abilities. Such blending operation automatically modulates the multiple MPIs to appropriately represent the diverse local density and color information. Experiments on the KITTI dataset and ScanNet dataset demonstrate that our proposed MMPI synthesizes high-quality images from diverse camera pose distributions and is fast to train, outperforming the previous fast-training NeRF methods for novel view synthesis. Moreover, we show that MMPI can encode extremely long trajectories and produce novel view renderings, demonstrating its potential in applications like autonomous driving. Our demo video is available at https://youtube.com/watch?v=mbNKwN5urC8.},
  archive   = {C_ICRA},
  author    = {Yuze He and Peng Wang and Yubin Hu and Wang Zhao and Ran Yi and Yong-Jin Liu and Wenping Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611248},
  month     = {5},
  pages     = {15395-15401},
  title     = {MMPI: A flexible radiance field representation by multiple multi-plane images blending},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural rearrangement planning for object retrieval from
confined spaces perceivable by robot’s in-hand RGB-d sensor.
<em>ICRA</em>, 15388–15394. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Rearrangement planning for object retrieval tasks from confined spaces is a challenging problem, primarily due to the lack of open space for robot motion and limited perception. Several traditional methods exist to solve object retrieval tasks, but they require overhead cameras for perception and a time-consuming exhaustive search to find a solution and often make unrealistic assumptions, such as having identical, simple geometry objects in the environment. This paper presents a neural object retrieval framework that efficiently performs rearrangement planning of unknown, arbitrary objects in confined spaces to retrieve the desired object using a given robot grasp. Our method actively senses the environment with the robot’s in-hand camera. It then selects and relocates the non-target objects such that they do not block the robot path homotopy to the target object, thus also aiding an underlying path planner in quickly finding robot motion sequences. Furthermore, we demonstrate our framework in challenging scenarios, including real-world cabinet-like environments with arbitrary household objects. The results show that our framework achieves the best performance among all presented methods and is, on average, two orders of magnitude computationally faster than the best-performing baselines.},
  archive   = {C_ICRA},
  author    = {Hanwen Ren and Ahmed H. Qureshi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610092},
  month     = {5},
  pages     = {15388-15394},
  title     = {Neural rearrangement planning for object retrieval from confined spaces perceivable by robot’s in-hand RGB-D sensor},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MOSAIC: Learning unified multi-sensory object property
representations for robot learning via interactive perception.
<em>ICRA</em>, 15381–15387. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A holistic understanding of object properties across diverse sensory modalities (e.g., visual, audio, and haptic) is essential for tasks ranging from object categorization to complex manipulation. Drawing inspiration from cognitive science studies that emphasize the significance of multi-sensory integration in human perception, we introduce MOSAIC (Multimodal Object property learning with Self-Attention and Interactive Comprehension), a novel framework designed to facilitate the learning of unified multi-sensory object property representations. While it is undeniable that visual information plays a prominent role, we acknowledge that many fundamental object properties extend beyond the visual domain to encompass attributes like texture, mass distribution, or sounds, which significantly influence how we interact with objects. In MOSAIC, we leverage this profound insight by distilling knowledge from multimodal foundation models and aligning these representations not only across vision but also haptic and auditory sensory modalities. Through extensive experiments on a dataset where a humanoid robot interacts with 100 objects across 10 exploratory behaviors, we demonstrate the versatility of MOSAIC in two task families: object categorization and object-fetching tasks. Our results underscore the efficacy of MOSAIC&#39;s unified representations, showing competitive performance in category recognition through a simple linear probe setup and excelling in the fetch object task under zero-shot transfer conditions. This work pioneers the application of sensory grounding in foundation models for robotics, promising a significant leap in multi-sensory perception capabilities for autonomous systems. We have released the code, datasets, and additional results: https://github.com/gtatiya/MOSAIC.},
  archive   = {C_ICRA},
  author    = {Gyan Tatiya and Jonathan Francis and Ho-Hsiang Wu and Yonatan Bisk and Jivko Sinapov},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609998},
  month     = {5},
  pages     = {15381-15387},
  title     = {MOSAIC: Learning unified multi-sensory object property representations for robot learning via interactive perception},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning force control for legged manipulation.
<em>ICRA</em>, 15366–15372. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Controlling the contact force during interactions is an inherent requirement for locomotion and manipulation tasks. Current reinforcement learning approaches to locomotion and manipulation rely implicitly on forceful interaction to accomplish tasks but do not explicitly regulate it. This paper proposes a reinforcement learning task specification that focuses on matching desired contact force levels. Integrating force control with the coordination of a robot’s body and arm, we present an end-to-end policy for legged manipulator control. Force control enables us to realize compliant gripper and whole-body pulling movements that have not been previously demonstrated using a learned policy. It also facilitates a characterization of the force-tracking performance of learned policies in simulation and the real world, indicating their performance potential for force-critical tasks. Video is available at the project website: https://tif-twirl-13.github.io/learning-compliance.},
  archive   = {C_ICRA},
  author    = {Tifanny Portela and Gabriel B. Margolis and Yandong Ji and Pulkit Agrawal},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611066},
  month     = {5},
  pages     = {15366-15372},
  title     = {Learning force control for legged manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Singularity-robust prioritized whole-body tracking and
interaction control with smooth task transitions. <em>ICRA</em>,
15358–15365. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we propose a singularity-robust whole-body control framework that ensures smooth task transitions while maintaining strict priorities. The weighted generalized inverse is adopted to derive a hierarchical control law compatible with singular and redundant tasks. Moreover, a smooth activation matrix is proposed to continuously shape both null-space projectors and task-level control actions. Validation has been conducted in MATLAB/Simulink and MuJoCo simulations with Rollin’ Justin.},
  archive   = {C_ICRA},
  author    = {Xuwei Wu and Alin Albu-Schäffer and Alexander Dietrich},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610961},
  month     = {5},
  pages     = {15358-15365},
  title     = {Singularity-robust prioritized whole-body tracking and interaction control with smooth task transitions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Representing robot geometry as distance fields:
Applications to whole-body manipulation. <em>ICRA</em>, 15351–15357. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we propose a novel approach to represent robot geometry as distance fields (RDF) that extends the principle of signed distance fields (SDFs) to articulated kinematic chains. Our method employs a combination of Bernstein polynomials to encode the signed distance for each robot link with high accuracy and efficiency while ensuring the mathematical continuity and differentiability of SDFs. We further leverage the kinematics chain of the robot to produce the SDF representation in joint space, allowing robust distance queries in arbitrary joint configurations. The proposed RDF representation is differentiable and smooth in both task and joint spaces, enabling its direct integration to optimization problems. Additionally, the 0-level set of the robot corresponds to the robot surface, which can be seamlessly integrated into whole-body manipulation tasks. We conduct various experiments in both simulations and with 7-axis Franka Emika robots, comparing against baseline methods, and demonstrating its effectiveness in collision avoidance and whole-body manipulation tasks. Project page: https://sites.google.com/view/lrdf/home},
  archive   = {C_ICRA},
  author    = {Yiming Li and Yan Zhang and Amirreza Razmjoo and Sylvain Calinon},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611674},
  month     = {5},
  pages     = {15351-15357},
  title     = {Representing robot geometry as distance fields: Applications to whole-body manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Design of morphable StateNet based on pseudo-generalization
of standing up motions for humanoid with variable body structure.
<em>ICRA</em>, 15336–15342. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we explain the Morphable StateNet as the StateNet with pseudo-generalized behaviors for robots with various degree-of-freedom arrangements and link lengths. Pseudo-generalization is performed by analytically calculating joint angles that satisfy the desired support conditions, focusing on link lengths and antigravity joints that contribute to motion, with constraints placed on the contact conditions between the environment and the robot body. We apply Morphable StateNet to the standing-up motion of humanoids with variable body structures and conduct evaluation experiments. We have demonstrated the usefulness of the proposed method in environments with low friction coefficients with the environment by conducting evaluations using both a simulator and an actual humanoid.},
  archive   = {C_ICRA},
  author    = {Tasuku Makabe and Kei Okada and Masayuki Inaba},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610947},
  month     = {5},
  pages     = {15336-15342},
  title     = {Design of morphable StateNet based on pseudo-generalization of standing up motions for humanoid with variable body structure},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward self-righting and recovery in the wild: Challenges
and benchmarks. <em>ICRA</em>, 15329–15335. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-recovery is a critical capability for robust, agile robots operating in the real world. Given truly challenging terrain, it is nearly inevitable that, at some point, the robot will fail and subsequently need to recover if it is to continue its task. One critical subset of recovery is standing back up after falling down (aka &quot;self-righting&quot;), an essential early milestone for babies learning to walk, and an existential capability for animals. While some robots can be designed with multiple orientations for mobility, most seeking to affect the world would significantly benefit from planners/policies that facilitate self-righting whenever possible.In this work, we present a series of challenges that outline why recovery in the wild is difficult. We then present a set of benchmark policies trained in simulation using deep reinforcement learning (RL) and the Student-Teacher approach. Finally, we evaluate the performance of these policies on a set of benchmark contexts in simulation, and provide baseline validation on a physical robot.},
  archive   = {C_ICRA},
  author    = {Rosario Scalise and Ege Caglar and Byron Boots and Chad C. Kessens},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611544},
  month     = {5},
  pages     = {15329-15335},
  title     = {Toward self-righting and recovery in the wild: Challenges and benchmarks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical optimization-based control for whole-body
loco-manipulation of heavy objects. <em>ICRA</em>, 15322–15328. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, the field of legged robotics has seen growing interest in enhancing the capabilities of these robots through the integration of articulated robotic arms. However, achieving successful loco-manipulation, especially involving interaction with heavy objects, is far from straightforward, as object manipulation can introduce substantial disturbances that impact the robot’s locomotion. This paper presents a novel framework for legged loco-manipulation that considers whole-body coordination through a hierarchical optimization-based control framework. First, an online manipulation planner computes the manipulation forces and manipulated object task-based reference trajectory. Then, pose optimization aligns the robot’s trajectory with kinematic constraints. The resultant robot reference trajectory is executed via a linear MPC controller incorporating the desired manipulation forces into its prediction model. Our approach has been validated in simulation and hardware experiments, highlighting the necessity of whole-body optimization compared to the baseline locomotion MPC when interacting with heavy objects. Experimental results with Unitree Aliengo, equipped with a custom-made robotic arm, showcase its ability to lift and carry an 8kg payload and manipulate doors.},
  archive   = {C_ICRA},
  author    = {Alberto Rigo and Muqun Hu and Satyandra K. Gupta and Quan Nguyen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611656},
  month     = {5},
  pages     = {15322-15328},
  title     = {Hierarchical optimization-based control for whole-body loco-manipulation of heavy objects},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). Safe and individualized motion planning for upper-limb
exoskeleton robots using human demonstration and interactive learning.
<em>ICRA</em>, 15307–15313. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A typical application of upper-limb exoskeleton robots is deployment in rehabilitation training, helping patients to regain manipulative abilities. However, as the patient is not always capable of following the robot, safety issues may arise during the training. Due to the bias in different patients, an individualized scheme is also important to ensure that the robot suits the specific conditions (e.g., movement habits) of a patient, hence guaranteeing effectiveness. To fulfill this requirement, this paper proposes a new motion planning scheme for upper-limb exoskeleton robots, which drives the robot to provide customized, safe, and individualized assistance using both human demonstration and interactive learning. Specifically, the robot first learns from a group of healthy subjects to generate a reference motion trajectory via probabilistic movement primitives (ProMP). It then learns from the patient during the training process to further shape the trajectory inside a moving safe region. The interactive data is fed back into the ProMP iteratively to enhance the individualized features for as long as the training process continues. The robot tracks the individualized trajectory under a variable impedance model to realize the assistance. Finally, the experimental results are presented in this paper to validate the proposed control scheme.},
  archive   = {C_ICRA},
  author    = {Yu Chen and Gong Chen and Jing Ye and Xiangjun Qiu and Xiang Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610552},
  month     = {5},
  pages     = {15307-15313},
  title     = {Safe and individualized motion planning for upper-limb exoskeleton robots using human demonstration and interactive learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active, quasi-passive, pneumatic, and portable knee
exoskeleton with bidirectional energy flow for efficient air recovery in
sit-stand tasks. <em>ICRA</em>, 15292–15298. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While existing literature encompasses exoskeleton-assisted sit-stand tasks, the integration of energy recovery mechanisms remains unexplored. To push the boundaries further, this study introduces a portable pneumatic knee exoskeleton that operates in both quasi-passive and active modes, where active mode is utilized for aiding in standing up (power generation), thus the energy flows from the exoskeleton to the user, and quasi-passive mode for aiding in sitting down (power absorption), where the device absorbs and can store energy in the form of compressed air, leading to energy savings in active mode. The absorbed energy can be stored and later reused without compromising exoskeleton transparency in the meantime. In active mode, an air pump inflates the pneumatic artificial muscle (PAM), which stores the compressed air, that can then be released into a pneumatic cylinder to generate torque. All electronic and pneumatic components are integrated into the system, and the exoskeleton weighs 3.9 kg with a maximum torque of 20 Nm at the knee joint. The paper describes the mechatronic design, mathematical model and includes a pilot study with an able-bodied subject performing sit-to-stand tasks. The results show that the exoskeleton can recover energy while assisting the subject and reducing mean muscle activity by ~31%. Further results highlight air regeneration’s potential for energy saving in portable pneumatic exoskeletons, showing that the proposed device extends exoskeleton operation by ~27%.},
  archive   = {C_ICRA},
  author    = {Luka Mišković and Tilen Brecelj and Miha Dežman and Tadej Petrič},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610457},
  month     = {5},
  pages     = {15292-15298},
  title     = {Active, quasi-passive, pneumatic, and portable knee exoskeleton with bidirectional energy flow for efficient air recovery in sit-stand tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vision-based wearable steering assistance for people with
impaired vision in jogging. <em>ICRA</em>, 15270–15275. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Outdoor sports pose a challenge for people with impaired vision. The demand for higher-speed mobility inspired us to develop a vision-based wearable steering assistance. To ensure broad applicability, we focused on a representative sports environment, the athletics track. Our efforts centered on improving the speed and accuracy of perception, enhancing planning adaptability for the real world, and providing swift and safe assistance for people with impaired vision. In perception, we engineered a lightweight multitask network capable of simultaneously detecting track lines and obstacles. Additionally, due to the limitations of existing datasets for supporting multi-task detection in athletics tracks, we diligently collected and annotated a new dataset (MAT) containing 1000 images. In planning, we integrated the methods of sampling and spline curves, addressing the planning challenges of curves. Meanwhile, we utilized the positions of the track lines and obstacles as constraints to guide people with impaired vision safely along the current track. Our system is deployed on an embedded device, Jetson Orin NX. Through outdoor experiments, it demonstrated adaptability in different sports scenarios, assisting users in achieving free movement of 400meter at an average speed of 1.34 m/s, meeting the level of normal people in jogging. Our MAT dataset is publicly available from https://github.com/snoopy-l/MAT},
  archive   = {C_ICRA},
  author    = {Xiaotong Liu and Binglu Wang and Zhijun Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610660},
  month     = {5},
  pages     = {15270-15275},
  title     = {Vision-based wearable steering assistance for people with impaired vision in jogging},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generating environment-based explanations of motion planner
failure: Evolutionary and joint-optimization algorithms. <em>ICRA</em>,
15263–15269. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion planning algorithms are important components of autonomous robots, which are difficult to understand and debug when they fail to find a solution to a problem. In this paper we propose a solution to the failure-explanation problem, which are automatically-generated environment-based explanations. These explanations reveal the objects in the environment that are responsible for the failure, and how their location in the world should change so as to make the planning problem feasible.Concretely, we propose two methods—one based on evolutionary optimization and another on joint trajectory-and-environment continuous-optimization. We show that the evolutionary method is well-suited to explain sampling-based motion planners, or even optimization-based motion planners in situations where computation speed is not a concern (e.g. post-hoc debugging). However, the optimization-based method is 4000 times faster and thus more attractive for interactive applications, even though at the cost of a slightly lower success rate. We demonstrate the capabilities of the methods through concrete examples and quantitative evaluation.},
  archive   = {C_ICRA},
  author    = {Qishuai Liu and Martim Brandão},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611577},
  month     = {5},
  pages     = {15263-15269},
  title     = {Generating environment-based explanations of motion planner failure: Evolutionary and joint-optimization algorithms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MAC-ID: Multi-agent reinforcement learning with local
coordination for individual diversity. <em>ICRA</em>, 15233–15239. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the increase of robots navigating through crowded environments in our daily lives, the demand for designing a socially-aware navigation method considering humanrobot interaction has risen. When developing and assessing socially-aware navigation methods, pedestrian motion modeling plays a significant role. However, existing pedestrian models often struggle in complex environments and do not have the capacity to generate diverse pedestrian styles.In this paper, we propose multi-agent reinforcement learning with local coordination for individual diversity (MAC-ID), which can synthesize diverse pedestrian motions via local coordination factor (LCF). Our experiments have demonstrated that the manipulation of the LCF induces interpretable changes in pedestrian behaviors, along with a superior performance compared to existing pedestrian motion models. For evaluating socially-aware navigation methods using MAC-ID, we present a novel benchmark called BSON. It offers realistic and diverse social environments with pedestrians modeled via MAC-ID. We have trained and compared various navigation methods in BSON using a newly proposed metric called socially-aware navigation score (SNS). Through BSON, users can evaluate their socially-aware navigation methods and compare them to baselines.},
  archive   = {C_ICRA},
  author    = {Hojun Chung and Jeongwoo Oh and Jaeseok Heo and Gunmin Lee and Songhwai Oh},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610314},
  month     = {5},
  pages     = {15233-15239},
  title     = {MAC-ID: Multi-agent reinforcement learning with local coordination for individual diversity},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stranger danger! Identifying and avoiding unpredictable
pedestrians in RL-based social robot navigation. <em>ICRA</em>,
15217–15224. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning (RL) methods for social robot navigation show great success navigating robots through large crowds of people, but the performance of these learning-based methods tends to degrade in particularly challenging or unfamiliar situations due to the models’ dependency on representative training data. To ensure human safety and comfort, it is critical that these algorithms handle uncommon cases appropriately, but the low frequency and wide diversity of such situations present a significant challenge for these data-driven methods. To overcome this challenge, we propose modifications to the learning process that encourage these RL policies to maintain additional caution in unfamiliar situations. Specifically, we improve the Socially Attentive Reinforcement Learning (SARL) policy by (1) modifying the training process to systematically introduce deviations into a pedestrian model, (2) updating the value network to estimate and utilize pedestrian-unpredictability features, and (3) implementing a reward function to learn an effective response to pedestrian unpredictability. Compared to the original SARL policy, our modified policy maintains similar navigation times and path lengths, while reducing the number of collisions by 82% and reducing the proportion of time spent in the pedestrians’ personal space by up to 19 percentage points for the most difficult cases. We also describe how to apply these modifications to other RL policies and demonstrate that some key high-level behaviors of our approach transfer to a physical robot.},
  archive   = {C_ICRA},
  author    = {Sara Pohland and Alvin Tan and Prabal Dutta and Claire Tomlin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610413},
  month     = {5},
  pages     = {15217-15224},
  title     = {Stranger danger! identifying and avoiding unpredictable pedestrians in RL-based social robot navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trajectory prediction for robot navigation using flow-guided
markov neural operator. <em>ICRA</em>, 15209–15216. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Predicting pedestrian movements remains a complex and persistent challenge in robot navigation research. We must evaluate several factors to achieve accurate predictions, such as pedestrian interactions, the environment, crowd density, and social and cultural norms. Accurate prediction of pedestrian paths is vital for ensuring safe human-robot interaction, especially in robot navigation. Furthermore, this research has potential applications in autonomous vehicles, pedestrian tracking, and human-robot collaboration. Therefore, in this paper, we introduce FlowMNO, an Optical Flow-Integrated Markov Neural Operator designed to capture pedestrian behavior across diverse scenarios. Our paper models trajectory prediction as a Markovian process, where future pedestrian coordinates depend solely on the current state. This problem formulation eliminates the need to store previous states. We conducted experiments using standard benchmark datasets like ETH, HOTEL, ZARA1, ZARA2, UCY, and RGB-D pedestrian datasets. Our study demonstrates that FlowMNO outperforms some of the state-of-the-art deep learning methods like LSTM, GAN, and CNN-based approaches, by approximately 86.46% when predicting pedestrian trajectories. Thus, we show that FlowMNO can seamlessly integrate into robot navigation systems, enhancing their ability to navigate crowded areas smoothly.},
  archive   = {C_ICRA},
  author    = {Rashmi Bhaskara and Hrishikesh Viswanath and Aniket Bera},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611154},
  month     = {5},
  pages     = {15209-15216},
  title     = {Trajectory prediction for robot navigation using flow-guided markov neural operator},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Virtual borders in 3D: Defining a drone’s movement space
using augmented reality. <em>ICRA</em>, 15202–15208. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots are increasingly finding their way into home environments, where they can assist with household tasks like vacuuming or surveilling. While the robots can navigate on their own, users might not want them to go everywhere or not in a specific way. For example, users might not want a drone to fly over a table where important letters and the newspaper are stored, even though it is the shortest path to the goal. Therefore, an application is required, that is easy to learn and to apply even for inexperienced users.In this paper, we present a framework that uses a tablet as augmented reality (AR) device to modify a robot’s movement space in 3D. A user can define virtual borders in the real world with the tablet and add them to a map, changing the navigational behavior of the robot. The framework is evaluated by a user study with inexperienced participants that verifies our approach. Further analyses show, that even complex scenarios can be covered with our framework.},
  archive   = {C_ICRA},
  author    = {Malte Riechmann and André Kirsch and Matthias Koenig and Jan Rexilius},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610259},
  month     = {5},
  pages     = {15202-15208},
  title     = {Virtual borders in 3D: Defining a drone’s movement space using augmented reality},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A user-centered shared control scheme with learning from
demonstration for robotic surgery. <em>ICRA</em>, 15195–15201. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The utilization of shared control in the realm of surgical robotics augments precision and safety by amalgamating human expertise with autonomous assistance. This paper proposes a user-centered shared control framework enabling a robot to learn from expert demonstration, predict operators’ intent and modulate control authority to provide natural assistance when needed. We employ deep inverse reinforcement learning (IRL) to enable the robot to learn path planning from expert demonstrations with fast convergence, subsequently enhancing the policy with a potential field method. The control authority is allocated seamlessly between the human operator and the autonomous agent based on the prediction of operators’ movement from an adaptive filter and fuzzy logic inference. The proposed method is executed using the da Vinci Research Kit (dVRK) robot in a simulation environment, and its effectiveness is assessed through user performance evaluation in a trajectory tracking task. Compared to direct control and simple shared control, the proposed shared control scheme exhibits superior tracking accuracy and trajectory smoothness under external disturbances. Subjective responses underscore users’ perception of the method’s efficacy in enhancing their performance.},
  archive   = {C_ICRA},
  author    = {Haoyi Zheng and Zhaoyang Jacopo Hu and Yanpei Huang and Xiaoxiao Cheng and Ziwei Wang and Etienne Burdet},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611089},
  month     = {5},
  pages     = {15195-15201},
  title     = {A user-centered shared control scheme with learning from demonstration for robotic surgery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An intuitive manual guidance scheme to operate rotation and
translation simultaneously. <em>ICRA</em>, 15180–15186. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {During certain human-robot collaboration tasks, the operator interacts with the robot by hand guidance to adjust the end-effector pose for spatial operations. The rotational operation is less intuitive to humans than translation. In fact, imagining the path to the target orientation is more challenging. In the literature related to control strategies for robot manual guidance, it is usually proposed to control translation and rotation independently. Our research explored and quantified the factors that influence operational intuition. A Virtual Fixture spatial guidance framework with intuition maintenance is proposed. This novel guidance scheme enables operators to effortlessly and simultaneously control both orientation and position in an intuitive way. High operation precision and efficiency can be achieved without interfering with the main task by exploring the null space with constraint optimization.},
  archive   = {C_ICRA},
  author    = {Fan Shao and Fanny Ficuciello},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610298},
  month     = {5},
  pages     = {15180-15186},
  title     = {An intuitive manual guidance scheme to operate rotation and translation simultaneously},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distilling and retrieving generalizable knowledge for robot
manipulation via language corrections. <em>ICRA</em>, 15172–15179. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Today’s robot policies exhibit subpar performance when faced with the challenge of generalizing to novel environments. Human corrective feedback is a crucial form of guidance to enable such generalization. However, adapting to and learning from online human corrections is a non-trivial endeavor: not only do robots need to remember human feedback over time to retrieve the right information in new settings and reduce the intervention rate, but also they would need to be able to respond to feedback that can be arbitrary corrections about high-level human preferences to low-level adjustments to skill parameters. In this work, we present Distillation and Retrieval of Online Corrections (DROC), a large language model (LLM)-based system that can respond to arbitrary forms of language feedback, distill generalizable knowledge from corrections, and retrieve relevant past experiences based on textual and visual similarity for improving performance in novel settings. DROC is able to respond to a sequence of online language corrections that address failures in both high-level task plans and low-level skill primitives. We demonstrate that DROC effectively distills the relevant information from the sequence of online corrections in a knowledge base and retrieves that knowledge in settings with new task or object instances. DROC outperforms other techniques that directly generate robot code via LLMs [1] by using only half of the total number of corrections needed in the first round and requires little to no corrections after two iterations. We show further results and videos on our project website: https://sites.google.com/stanford.edu/droc.},
  archive   = {C_ICRA},
  author    = {Lihan Zha and Yuchen Cui and Li-Heng Lin and Minae Kwon and Montserrat Gonzalez Arenas and Andy Zeng and Fei Xia and Dorsa Sadigh},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610455},
  month     = {5},
  pages     = {15172-15179},
  title     = {Distilling and retrieving generalizable knowledge for robot manipulation via language corrections},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human robot shared control in surgery: A performance
assessment. <em>ICRA</em>, 15165–15171. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While surgical robots, such as the da Vinci Surgical System, have become prevalent in minimally invasive surgery, they are predominantly used by the human operator to directly teleoperate the tools. This paper aims to analyse the different methods of human robot shared control in the surgical domain. We propose a reinforcement learning algorithm, transverse generative adversarial imitation learning (tGAIL), which is employed to train the robot from the expert’s demonstration and show competitive generalization ability compared to inverse reinforcement learning and conventional GAIL. We then propose a priority-changing shared control method to effectively combine the surgeon and robot’s strengths by dynamically adjusting control priority based on the deviation distance. We show that using this method in a supervision framework boosts the performance of the human operator when completing the peg transfer task. By learning from the expert and collaborating with the human during the task, the intelligent agent can help to reduce operation time by 31.7% and the human input by 60.5% compared to direct teleoperation.},
  archive   = {C_ICRA},
  author    = {Longrui Chen and Zhaoyang Jacopo Hu and Yanpei Huang and Etienne Burdet and Ferdinando Rodriguez y Baena},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611642},
  month     = {5},
  pages     = {15165-15171},
  title     = {Human robot shared control in surgery: A performance assessment},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robustifying a policy in multi-agent RL with diverse
cooperative behaviors and adversarial style sampling for assistive
tasks. <em>ICRA</em>, 15158–15164. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous assistance of people with motor impairments is one of the most promising applications of autonomous robotic systems. Recent studies have reported encouraging results using deep reinforcement learning (RL) in the healthcare domain. Previous studies showed that assistive tasks can be formulated as multi-agent RL, wherein there are two agents: a caregiver and a care-receiver. However, policies trained in multi-agent RL are often sensitive to the policies of other agents. In such a case, a trained caregiver’s policy may not work for different care-receivers. To alleviate this issue, we propose a framework that learns a robust caregiver’s policy by training it for diverse care-receiver responses. In our framework, diverse care-receiver responses are autonomously learned through trials and errors. In addition, to robustify the care-giver’s policy, we propose a strategy for sampling a care-receiver’s response in an adversarial manner during the training. We evaluated the proposed method using tasks in an Assistive Gym. We demonstrate that policies trained with a popular deep RL method are vulnerable to changes in policies of other agents and that the proposed framework improves the robustness against such changes.},
  archive   = {C_ICRA},
  author    = {Takayuki Osa and Tatsuya Harada},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611719},
  month     = {5},
  pages     = {15158-15164},
  title     = {Robustifying a policy in multi-agent RL with diverse cooperative behaviors and adversarial style sampling for assistive tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Shared autonomy via variable impedance control and virtual
potential fields for encoding human demonstrations*. <em>ICRA</em>,
15151–15157. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This article introduces a framework for complex human-robot collaboration tasks, such as the co-manufacturing of furniture. For these tasks, it is essential to encode tasks from human demonstration and reproduce these skills in a compliant and safe manner. Therefore, two key components are addressed in this work: motion generation and shared autonomy. We propose a motion generator based on a time-invariant potential field, capable of encoding wrench profiles, complex and closed-loop trajectories, and additionally incorporates obstacle avoidance. Additionally, the paper addresses shared autonomy (SA) which enables synergetic collaboration between human operators and robots by dynamically allocating authority. Variable impedance control (VIC) and force control are employed, where impedance and wrench are adapted based on the human-robot autonomy factor derived from interaction forces. System passivity is ensured by an energy-tank based task passivation strategy. The framework’s efficacy is validated through simulations and an experimental study employing a Franka Emika Research 3 robot.},
  archive   = {C_ICRA},
  author    = {Shail Jadav and Johannes Heidersberger and Christian Ott and Dongheui Lee},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610761},
  month     = {5},
  pages     = {15151-15157},
  title     = {Shared autonomy via variable impedance control and virtual potential fields for encoding human demonstrations*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sim2Real bilevel adaptation for object surface
classification using vision-based tactile sensors. <em>ICRA</em>,
15128–15134. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we address the Sim2Real gap in the field of vision-based tactile sensors for classifying object surfaces. We train a Diffusion Model to bridge this gap using a relatively small dataset of real-world images randomly collected from unlabeled everyday objects via the DIGIT sensor. Subsequently, we employ a simulator to generate images by uniformly sampling the surface of objects from the YCB Model Set. These simulated images are then translated into the real domain using the Diffusion Model and automatically labeled to train a classifier. During this training, we further align features of the two domains using an adversarial procedure. Our evaluation is conducted on a dataset of tactile images obtained from a set of ten 3D-printed YCB objects. The results reveal a total accuracy of 81.9%, a significant improvement compared to the 34.7% achieved by the classifier trained solely on simulated images. This demonstrates the effectiveness of our approach. We further validate our approach using the classifier on a 6D object pose estimation task from tactile data.},
  archive   = {C_ICRA},
  author    = {Gabriele M. Caddeo and Andrea Maracani and Paolo Didier Alfano and Nicola A. Piga and Lorenzo Rosasco and Lorenzo Natale},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610028},
  month     = {5},
  pages     = {15128-15134},
  title     = {Sim2Real bilevel adaptation for object surface classification using vision-based tactile sensors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incremental 3D reconstruction through a hybrid
explicit-and-implicit representation. <em>ICRA</em>, 15121–15127. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D reconstruction is an important task in computer vision and is widely used in robotics and autonomous driving. When building large-scale scenes, limitations in computing resources and the difficulty of accessing the entire dataset in a single task are inevitable. Therefore, an incremental reconstruction approach is desired. On the one hand, traditional explicit 3D reconstruction methods such as SLAM and SFM require global optimization, which means that time and space resources increase dramatically with the growth of training data. On the other hand, implicit methods like Neural Radiation Fields (NeRF) suffer from catastrophic forgetting if trained incrementally. In this paper, we incrementally reconstruct 3D models in a hybrid representation, where the density of the radiation field is formulated by a voxel grid, and the view-dependent color information of the points is inferred by a shallow MLP. The expansion of the voxel grid and the distillation of the shallow MLP are efficient in this case. Experimental results demonstrate that our incremental method achieves a level of accuracy on par with approaches employing global optimization techniques.},
  archive   = {C_ICRA},
  author    = {Feifei Li and Panwen Hu and Qi Song and Rui Huang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610868},
  month     = {5},
  pages     = {15121-15127},
  title     = {Incremental 3D reconstruction through a hybrid explicit-and-implicit representation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distill-then-prune: An efficient compression framework for
real-time stereo matching network on edge devices. <em>ICRA</em>,
15113–15120. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, numerous real-time stereo matching methods have been introduced, but they often lack accuracy. These methods attempt to improve accuracy by introducing new modules or integrating traditional methods. However, the improvements are only modest. In this paper, we propose a novel strategy by incorporating knowledge distillation and model pruning to overcome the inherent trade-off between speed and accuracy. As a result, we obtained a model that maintains real-time performance while delivering high accuracy on edge devices. Our proposed method involves three key steps. Firstly, we review state-of-the-art methods and design our lightweight model by removing redundant modules from those efficient models through a comparison of their contributions. Next, we leverage the efficient model as the teacher to distill knowledge into the lightweight model. Finally, we systematically prune the lightweight model to obtain the final model. Through extensive experiments conducted on two widely-used benchmarks, Sceneflow and KITTI, we perform ablation studies to analyze the effectiveness of each module and present our state-of-the-art results.},
  archive   = {C_ICRA},
  author    = {Baiyu Pan and Jichao Jiao and Jianxing Pang and Jun Cheng},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611085},
  month     = {5},
  pages     = {15113-15120},
  title     = {Distill-then-prune: An efficient compression framework for real-time stereo matching network on edge devices},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning with chemical versus electrical synapses does it
make a difference? <em>ICRA</em>, 15106–15112. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bio-inspired neural networks have the potential to advance our understanding of neural computation and improve the state-of-the-art of AI systems. Bio-electrical synapses directly transmit neural signals, by enabling fast current flow between neurons. In contrast, bio-chemical synapses transmit neural signals indirectly, through neurotransmitters. Prior work showed that interpretable dynamics for complex robotic control, can be achieved by using chemical synapses, within a sparse, bio-inspired architecture, called Neural Circuit Policies (NCPs). However, a comparison of these two synaptic models, within the same architecture, remains an unexplored area. In this work we aim to determine the impact of using chemical synapses compared to electrical synapses, in both sparse and all-to-all connected networks. We conduct experiments with autonomous lane-keeping through a photorealistic autonomous driving simulator to evaluate their performance under diverse conditions and in the presence of noise. The experiments highlight the substantial influence of the architectural and synaptic-model choices, respectively. Our results show that employing chemical synapses yields noticeable improvements compared to electrical synapses, and that NCPs lead to better results in both synaptic models.},
  archive   = {C_ICRA},
  author    = {Mónika Farsang and Mathias Lechner and David Lung and Ramin Hasani and Daniela Rus and Radu Grosu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611016},
  month     = {5},
  pages     = {15106-15112},
  title     = {Learning with chemical versus electrical synapses does it make a difference?},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). What matters for active texture recognition with
vision-based tactile sensors. <em>ICRA</em>, 15099–15105. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper explores active sensing strategies that employ vision-based tactile sensors for robotic perception and classification of fabric textures. We formalize the active sampling problem in the context of tactile fabric recognition and provide an implementation of information-theoretic exploration strategies based on minimizing predictive entropy and variance of probabilistic models. Through ablation studies and human experiments, we investigate which components are crucial for quick and reliable texture recognition. Along with the active sampling strategies, we evaluate neural network architectures, representations of uncertainty, influence of data augmentation, and dataset variability. By evaluating our method on a previously published Active Clothing Perception Dataset and on a real robotic system, we establish that the choice of the active exploration strategy has only a minor influence on the recognition accuracy, whereas data augmentation and dropout rate play a significantly larger role. In a comparison study, while humans achieve 66.9% recognition accuracy, our best approach reaches 90.0% in under 5 touches, highlighting that vision-based tactile sensors are highly effective for fabric texture recognition.},
  archive   = {C_ICRA},
  author    = {Alina Böhm and Tim Schneider and Boris Belousov and Alap Kshirsagar and Lisa Lin and Katja Doerschner and Knut Drewing and Constantin A. Rothkopf and Jan Peters},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610274},
  month     = {5},
  pages     = {15099-15105},
  title     = {What matters for active texture recognition with vision-based tactile sensors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-granular transformer for motion prediction with LiDAR.
<em>ICRA</em>, 15092–15098. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion prediction has been an essential component of autonomous driving systems since it handles highly uncertain and complex scenarios involving moving agents of different types. In this paper, we propose a Multi-Granular TRansformer (MGTR) framework, an encoder-decoder network that exploits context features in different granularities for different kinds of traffic agents. To further enhance MGTR’s capabilities, we leverage LiDAR point cloud data by incorporating LiDAR semantic features from an off-the-shelf LiDAR feature extractor. We evaluate MGTR on Waymo Open Dataset motion prediction benchmark and show that the proposed method achieved state-of-the-art performance, ranking 1st on its leaderboard 1 .},
  archive   = {C_ICRA},
  author    = {Yiqian Gan and Hao Xiao and Yizhe Zhao and Ethan Zhang and Zhe Huang and Xin Ye and Lingting Ge},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610704},
  month     = {5},
  pages     = {15092-15098},
  title     = {Multi-granular transformer for motion prediction with LiDAR},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statler: State-maintaining language models for embodied
reasoning. <em>ICRA</em>, 15083–15091. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There has been a significant research interest in employing large language models to empower intelligent robots with complex reasoning. Existing work focuses on harnessing their abilities to reason about the histories of their actions and observations. In this paper, we explore a new dimension in which large language models may benefit robotics planning. In particular, we propose Statler, a framework in which large language models are prompted to maintain an estimate of the world state, which are often unobservable, and track its transition as new actions are taken. Our framework then conditions each action on the estimate of the current world state. Despite being conceptually simple, our Statler framework significantly outperforms strong competing methods (e.g., Code-as-Policies) on several robot planning tasks. Additionally, it has the potential advantage of scaling up to more challenging long-horizon planning tasks. We release our code here.},
  archive   = {C_ICRA},
  author    = {Takuma Yoneda and Jiading Fang and Peng Li and Huanyu Zhang and Tianchong Jiang and Shengjie Lin and Ben Picker and David Yunis and Hongyuan Mei and Matthew R. Walter},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610634},
  month     = {5},
  pages     = {15083-15091},
  title     = {Statler: State-maintaining language models for embodied reasoning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). POAQL: A partially observable altruistic q-learning method
for cooperative multi-agent reinforcement learning. <em>ICRA</em>,
15076–15082. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-Agent Path Finding (MAPF) is an important issue in multi-agent cooperation. Many studies apply MultiAgent Reinforcement Learning (MARL) to solve MAPF in partially observable settings. The objective of cooperative MARL is to maximize the cumulative team reward. Nevertheless, in partially observable settings, the team reward is misleading due to unpredictable factors from the behavior and state of unobserved agents. To address this issue, we propose a Partially Observable Altruistic Q-learning (POAQL) method. POAQL considers the cumulative reward of the observed subteam instead of the whole team, where Altruistic Q-learning plays an important role in learning the subteam action value. In addition, we design a new conflict resolution without additional guidance to emphasize the cooperative nature of MARL frameworks. Experimental results show that POAQL outperforms existing reinforcement learning methods in terms of efficiency and performance.},
  archive   = {C_ICRA},
  author    = {Lesong Tao and Miao Kang and Jinpeng Dong and Songyi Zhang and Ke Ye and Shitao Chen and Nanning Zheng},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610745},
  month     = {5},
  pages     = {15076-15082},
  title     = {POAQL: A partially observable altruistic Q-learning method for cooperative multi-agent reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pedestrian trajectory prediction using dynamics-based deep
learning. <em>ICRA</em>, 15068–15075. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pedestrian trajectory prediction plays an important role in autonomous driving systems and robotics. Recent work utilizing prominent deep learning models for pedestrian motion prediction makes limited a priori assumptions about human movements, resulting in a lack of explainability and explicit constraints enforced on predicted trajectories. We present a dynamics-based deep learning framework with a novel asymptotically stable dynamical system integrated into a Transformer-based model. We use an asymptotically stable dynamical system to model human goal-targeted motion by enforcing the human walking trajectory, which converges to a predicted goal position, and to provide the Transformer model with prior knowledge and explainability. Our framework features the Transformer model that works with a goal estimator and dynamical system to learn features from pedestrian motion history. The results show that our framework outperforms prominent models using five benchmark human motion datasets.},
  archive   = {C_ICRA},
  author    = {Honghui Wang and Weiming Zhi and Gustavo Batista and Rohitash Chandra},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609993},
  month     = {5},
  pages     = {15068-15075},
  title     = {Pedestrian trajectory prediction using dynamics-based deep learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Globally stable neural imitation policies. <em>ICRA</em>,
15061–15067. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Imitation learning mitigates the resource-intensive nature of learning policies from scratch by mimicking expert behavior. While existing methods can accurately replicate expert demonstrations, they often exhibit unpredictability in unexplored regions of the state space, thereby raising major safety concerns when facing perturbations. We propose SNDS, an imitation learning approach aimed at efficient training of scalable neural policies while formally ensuring global stability. SNDS leverages a neural architecture that enables the joint training of the policy and its associated Lyapunov candidate to ensure global stability throughout the learning process. We validate our approach through extensive simulations and deploy the trained policies on a real-world manipulator arm. The results confirm SNDS’s ability to address instability, accuracy, and computational intensity challenges highlighted in the literature, positioning it as a promising solution for scalable and stable policy learning in complex environments.},
  archive   = {C_ICRA},
  author    = {Amin Abyaneh and Mariana Sosa Guzmán and Hsiu-Chin Lin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610791},
  month     = {5},
  pages     = {15061-15067},
  title     = {Globally stable neural imitation policies},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning distributional demonstration spaces for
task-specific cross-pose estimation. <em>ICRA</em>, 15054–15060. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Relative placement tasks are an important category of tasks in which one object needs to be placed in a desired pose relative to another object. Previous work has shown success in learning relative placement tasks from just a small number of demonstrations when using relational reasoning networks with geometric inductive biases. However, such methods cannot flexibly represent multimodal tasks, like a mug hanging on any of n racks. We propose a method that incorporates additional properties that enable learning multimodal relative placement solutions, while retaining the provably translation-invariant and relational properties of prior work. We show that our method is able to learn precise relative placement tasks with only 10-20 multimodal demonstrations with no human annotations across a diverse set of objects within a category. Supplementary information can be found on the website: https://sites.google.com/view/tax-posed/home.},
  archive   = {C_ICRA},
  author    = {Jenny Wang and Octavian Donca and David Held},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611004},
  month     = {5},
  pages     = {15054-15060},
  title     = {Learning distributional demonstration spaces for task-specific cross-pose estimation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Instructing robots by sketching: Learning from demonstration
via probabilistic diagrammatic teaching. <em>ICRA</em>, 15047–15053. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning from Demonstration (LfD) enables robots to acquire new skills by imitating expert demonstrations, allowing users to communicate their instructions intuitively. Recent progress in LfD often relies on kinesthetic teaching or teleoperation as the medium for users to specify the demonstrations. Kinesthetic teaching requires physical handling of the robot, while teleoperation demands proficiency with additional hardware. This paper introduces an alternative paradigm for LfD called Diagrammatic Teaching. Diagrammatic Teaching aims to teach robots novel skills by prompting the user to sketch out demonstration trajectories on 2D images of the scene, these are then synthesised as a generative model of motion trajectories in 3D task space. Additionally, we present the Ray-tracing Probabilistic Trajectory Learning (RPTL) framework for Diagrammatic Teaching. RPTL extracts time-varying probability densities from the 2D sketches, then applies ray-tracing to find corresponding regions in 3D Cartesian space, and fits a probabilistic model of motion trajectories to these regions. New motion trajectories, which mimic those sketched by the user, can then be generated from the probabilistic model. We empirically validate our framework both in simulation and on real robots, which include a fixed-base manipulator and a quadruped-mounted manipulator.},
  archive   = {C_ICRA},
  author    = {Weiming Zhi and Tianyi Zhang and Matthew Johnson-Roberson},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611349},
  month     = {5},
  pages     = {15047-15053},
  title     = {Instructing robots by sketching: Learning from demonstration via probabilistic diagrammatic teaching},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian constraint inference from user demonstrations based
on margin-respecting preference models. <em>ICRA</em>, 15039–15046. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It is crucial for robots to be aware of the presence of constraints in order to acquire safe policies. However, explicitly specifying all constraints in an environment can be a challenging task. State-of-the-art constraint inference algorithms learn constraints from demonstrations, but tend to be computationally expensive and prone to instability issues. In this paper, we propose a novel Bayesian method that infers constraints based on preferences over demonstrations. The main advantages of our proposed approach are that it 1) infers constraints without calculating a new policy at each iteration, 2) uses a simple and more realistic ranking of groups of demonstrations, without requiring pairwise comparisons over all demonstrations, and 3) adapts to cases where there are varying levels of constraint violation. Our empirical results demonstrate that our proposed Bayesian approach infers constraints of varying severity, more accurately than state-of-the-art constraint inference methods. Code and videos: https://sites.google.com/berkeley.edu/pbicrl.},
  archive   = {C_ICRA},
  author    = {Dimitris Papadimitriou and Daniel S. Brown},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611095},
  month     = {5},
  pages     = {15039-15046},
  title     = {Bayesian constraint inference from user demonstrations based on margin-respecting preference models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AirExo: Low-cost exoskeletons for learning whole-arm
manipulation in the wild. <em>ICRA</em>, 15031–15038. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While humans can use parts of their arms other than the hands for manipulations like gathering and supporting, whether robots can effectively learn and perform the same type of operations remains relatively unexplored. As these manipulations require joint-level control to regulate the complete poses of the robots, we develop AirExo, a low-cost, adaptable, and portable dual-arm exoskeleton, for teleoperation and demonstration collection. As collecting teleoperated data is expensive and time-consuming, we further leverage AirExo to collect cheap in-the-wild demonstrations at scale. Under our in-the-wild learning framework, we show that with only 3 minutes of the teleoperated demonstrations, augmented by diverse and extensive in-the-wild data collected by AirExo, robots can learn a policy that is comparable to or even better than one learned from teleoperated demonstrations lasting over 20 minutes. Experiments demonstrate that our approach enables the model to learn a more general and robust policy across the various stages of the task, enhancing the success rates in task completion even with the presence of disturbances. Project website: airexo.github.io.},
  archive   = {C_ICRA},
  author    = {Hongjie Fang and Hao-Shu Fang and Yiming Wang and Jieji Ren and Jingjing Chen and Ruo Zhang and Weiming Wang and Cewu Lu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610799},
  month     = {5},
  pages     = {15031-15038},
  title     = {AirExo: Low-cost exoskeletons for learning whole-arm manipulation in the wild},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fitting parameters of linear dynamical systems to regularize
forcing terms in dynamical movement primitives. <em>ICRA</em>,
15024–15030. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Due to their flexibility and ease of use, Dynamical Movement Primitives (DMPs) are widely used in robotics applications and research. DMPs combine linear dynamical systems to achieve robustness to perturbations and adaptation to moving targets with non-linear function approximators to fit a wide range of demonstrated trajectories.We propose a novel DMP formulation with a generalized logistic function as a delayed goal system. This formulation inherently has low initial jerk, and generates the bell-shaped velocity profiles that are typical of human movement. As the novel formulation is more expressive, it is able to fit a wide range of human demonstrations well, also without a non-linear forcing term. We exploit this increased expressiveness by automating the fitting of the dynamical system parameters through opti-mization. Our experimental evaluation demonstrates that this optimization regularizes the forcing term, and improves the interpolation accuracy of parametric DMPs.},
  archive   = {C_ICRA},
  author    = {Freek Stulp and Adrià Colomé and Carme Torras},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610581},
  month     = {5},
  pages     = {15024-15030},
  title     = {Fitting parameters of linear dynamical systems to regularize forcing terms in dynamical movement primitives},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generating robotic elliptical excisions with human-like
tool-tissue interactions. <em>ICRA</em>, 15017–15023. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In surgery, the application of appropriate force levels is critical for the success and safety of a given procedure. While many studies are focused on measuring in situ forces, little attention has been devoted to relating these observed forces to surgical techniques. Answering questions like &quot;Can certain changes to a surgical technique result in lower forces and increased safety margins?&quot; could lead to improved surgical practice, and importantly, patient outcomes. However, such studies would require a large number of trials and professional surgeons, which is generally impractical to arrange. Instead, we show how robots can learn several variations of a surgical technique from a smaller number of surgical demonstrations and interpolate learnt behaviour via a parameterised skill model. This enables a large number of trials to be performed by a robotic system and the analysis of surgical techniques and their downstream effects on tissue. Here, we introduce a parameterised model of the elliptical excision skill and apply a Bayesian optimisation scheme to optimise the excision behaviour with respect to expert ratings, as well as individual characteristics of excision forces. Results show that the proposed framework can successfully align the generated robot behaviour with subjects across varying levels of proficiency in terms of excision forces.},
  archive   = {C_ICRA},
  author    = {Artūras Straižys and Michael Burke and Subramanian Ramamoorthy},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610990},
  month     = {5},
  pages     = {15017-15023},
  title     = {Generating robotic elliptical excisions with human-like tool-tissue interactions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WayEx: Waypoint exploration using a single demonstration.
<em>ICRA</em>, 15009–15016. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose WayEx, a new method for learning complex goal-conditioned robotics tasks from a single demonstration. Our approach distinguishes itself from existing imitation learning methods by demanding fewer expert examples and eliminating the need for information about the actions taken during the demonstration. This is accomplished by introducing a new reward function and employing a knowledge expansion technique. We demonstrate the effectiveness of WayEx, our waypoint exploration strategy, across six diverse tasks, showcasing its applicability in various environments. Notably, our method significantly reduces training time by ∼50% as compared to traditional reinforcement learning methods. WayEx obtains a higher reward than existing imitation learning methods given only a single demonstration. Furthermore, we demonstrate its success in tackling complex environments where standard approaches fall short. Appendix is available at: https://waypoint-ex.github.io.},
  archive   = {C_ICRA},
  author    = {Mara Levy and Nirat Saini and Abhinav Shrivastava},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611088},
  month     = {5},
  pages     = {15009-15016},
  title     = {WayEx: Waypoint exploration using a single demonstration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A combination of a controllable clutch and an oscillating
slider crank mechanism for ease of direct-teaching with various
payloads. <em>ICRA</em>, 15002–15008. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Direct teaching is a straightforward way of teaching new motion to robots. Active methods with torque sensors, for example, can be used so that the robot can follow the movements of the human, but such methods introduce delays. Alternatively, series clutch actuators are easily backdrivable without delay. However, vertical joints are subject to gravity torques, which need to be compensated when disengaging the clutch. We implemented passive gravity compensation to counteract the robot’s weight, but this mechanism cannot compensate for varying payloads, as adjustable passive gravity compensation is relatively slow and mechanically complex. The varying payload causes an unintended joint movement, i.e. the arm falls down on its own, which is unacceptable during direct teaching. Therefore, this paper demonstrates how the torque output controlled with series clutch actuators can be used to compensate for varying payloads while maintaining high backdrivability. The proposed method is evaluated on a collaborative robot with a clutch in series for each actuator. Real-world experiments with payloads from 0 to 3 kg are conducted. During the experiments, the operator force is measured to evaluate the proposed method.},
  archive   = {C_ICRA},
  author    = {Muhammad Arifin and Yuta Kage and Yuchen Yang and Alexander Schmitz and Shigeki Sugano},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610146},
  month     = {5},
  pages     = {15002-15008},
  title     = {A combination of a controllable clutch and an oscillating slider crank mechanism for ease of direct-teaching with various payloads},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling and control of intrinsically elasticity coupled
soft-rigid robots. <em>ICRA</em>, 14995–15001. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While much work has been done recently in the realm of model-based control of soft robots and soft-rigid hybrids, most works examine robots that have an inherently serial structure. While these systems have been prevalent in the literature, there is an increasing trend toward designing soft-rigid hybrids with intrinsically coupled elasticity between various degrees of freedom. In this work, we seek to address the issues of modeling and controlling such structures, particularly when underactuated. We introduce several simple models for elastic coupling, typical of those seen in these systems. We then propose a controller that compensates for the elasticity, and we prove its stability with Lyapunov methods without relying on the elastic dominance assumption. This controller is applicable to the general class of underactuated soft robots. After evaluating the controller in simulated cases, we then develop a simple hardware platform to evaluate both the models and the controller. Finally, using the hardware, we demonstrate a novel use case for underactuated, elastically coupled systems in &quot;sensorless&quot; force control.},
  archive   = {C_ICRA},
  author    = {Zach J. Patterson and Cosimo Della Santina and Daniela Rus},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610229},
  month     = {5},
  pages     = {14995-15001},
  title     = {Modeling and control of intrinsically elasticity coupled soft-rigid robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A data-driven approach to geometric modeling of systems with
low-bandwidth actuator dynamics. <em>ICRA</em>, 14988–14994. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It is challenging to perform system identification on soft robots due to their underactuated, high-dimensional dynamics. In this work, we present a data-driven modeling framework, based on geometric mechanics (also known as gauge theory) that can be applied to systems with low-bandwidth control of the system’s internal configuration. This method constructs a series of connected models comprising actuator and locomotor dynamics based on data points from stochastically perturbed, repeated behaviors. By deriving these connected models from general formulations of dissipative Lagrangian systems with symmetry, we offer a method that can be applied broadly to robots with first-order, low-pass actuator dynamics, including swelling-driven actuators used in hydrogel crawlers. These models accurately capture the dynamics of the system shape and body movements of a simplified swimming robot model. We further apply our approach to a stimulus-responsive hydrogel simulator that captures the complexity of chemomechanical interactions that drive shape changes in biomedically relevant micromachines. Finally, we propose an approach of numerically optimizing control signals by iteratively refining models, which is applied to optimize the input waveform for the hydrogel crawler. This transfer to realistic environments provides promise for applications in locomotor design and biomedical engineering.},
  archive   = {C_ICRA},
  author    = {Siming Deng and Junning Liu and Bibekananda Datta and Aishwarya Pantula and David H. Gracias and Thao D. Nguyen and Brian A. Bittner and Noah J. Cowan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610880},
  month     = {5},
  pages     = {14988-14994},
  title     = {A data-driven approach to geometric modeling of systems with low-bandwidth actuator dynamics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). POE: Acoustic soft robotic proprioception for
omnidirectional end-effectors. <em>ICRA</em>, 14980–14987. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Shape estimation is crucial for precise control of soft robots. However, soft robot shape estimation and proprioception are challenging due to their complex deformation behaviors and infinite degrees of freedom. Their continuously deforming bodies complicate integrating rigid sensors and reliably estimating its shape. In this work, we present Proprioceptive Omnidirectional End-effector (POE), a tendon-driven soft robot with six embedded microphones. We first introduce novel applications of 3D reconstruction methods to acoustic signals from the microphones for soft robot shape proprioception. To improve the proprioception pipeline’s training efficiency and model prediction consistency, we present POE-M. POE-M predicts key point positions from acoustic signal observations and uses an energy-minimization method to reconstruct a physically admissible high-resolution mesh of POE. We evaluate mesh reconstruction on simulated data and the POE-M pipeline with real-world experiments. Ablation studies suggest POE-M’s guidance of the key points during the mesh reconstruction process provides robustness and stability to the pipeline. POE-M reduced the maximum Chamfer distance error by 23.1 % compared to the state-of-the-art end-to-end soft robot proprioception models and achieved 4.91 mm average Chamfer distance error during evaluation. Supplemental materials, experiment data, and visualizations are available at sites.google.com/view/acoustic-poe.},
  archive   = {C_ICRA},
  author    = {Uksang Yoo and Ziven Lopez and Jeffrey Ichnowski and Jean Oh},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611598},
  month     = {5},
  pages     = {14980-14987},
  title     = {POE: Acoustic soft robotic proprioception for omnidirectional end-effectors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A soft robot inverse kinematics for virtual reality.
<em>ICRA</em>, 14957–14963. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We show how a variety of techniques from Computer Graphics can be leveraged to intuitively control the shape (configuration) of arbitrary 3D Soft Robots in VR. Our pipeline, Virtual Reality Soft Robot Inverse Kinematics (VR-Soft IK), overcomes fundamental limitations of general-purpose drag-and-drop soft robot control interfaces by leaving the 2D computer screen for 3D Virtual Reality (VR). VR-Soft IK uses a simulation based on the Finite Element Method (FEM) and a control method based on sensitivity analysis. Additionally, we show that our general control pipeline can be fused with techniques from 3D character animation to skin our simulation with a high-resolution surface mesh, pointing a way toward Mixed Reality Soft Robots. This full Skinned VR-Soft IK pipeline uses skeletal animation and GPU picking. We demonstrate the utility of our pipeline by doing real-time, open-loop control of the real-world 3D soft robotic arm Helix.},
  archive   = {C_ICRA},
  author    = {James M. Bern and William C. May and Austin Osborn and Francesco Stella and Sadra Zargarzadeh and Josie Hughes},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611603},
  month     = {5},
  pages     = {14957-14963},
  title     = {A soft robot inverse kinematics for virtual reality},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive state estimation with constant-curvature dynamics
using force-torque sensors with application to a soft pneumatic
actuator. <em>ICRA</em>, 14939–14945. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Using compliant materials leads to continuum robots undergoing large deformations. Their nonlinear behavior motivates the use of model-based controllers. They require state estimation as an essential step to be deployed. Available sensors are usually realized by introducing rigid bodies to the soft robot or inserting soft sensors made of materials different from the robot itself. Both approaches result in changes in the system’s dynamics. Optical measurements are problematic, especially in confined spaces. This can be avoided when the sensor is located at the robot&#39;s base. This paper studies the state estimation of a pneumatically actuated soft robot using the measured forces and torques at its base. For the first time, this is done using an unscented Kalman filter without restraining the dynamics to a planar or quasi-static motion while applying it to a real system. Real-time capability is achieved with our implementation. The state estimation is tested in a Cosserat rod simulation and on the physical system. The position is estimated with an accuracy of three to five millimeters for a 130 millimeter long pneumatic robot.},
  archive   = {C_ICRA},
  author    = {Maximilian Mehl and Max Bartholdt and Simon F. G. Ehlers and Thomas Seel and Moritz Schappler},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610370},
  month     = {5},
  pages     = {14939-14945},
  title     = {Adaptive state estimation with constant-curvature dynamics using force-torque sensors with application to a soft pneumatic actuator},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Solving sequential manipulation puzzles by finding easier
subproblems. <em>ICRA</em>, 14924–14930. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider a set of challenging sequential manipulation puzzles, where an agent has to interact with multiple movable objects and navigate narrow passages. Such settings are notoriously difficult for Task-and-Motion Planners, as they require interdependent regrasps and solving hard motion planning problems.In this paper, we propose to search over sequences of easier pick-and-place subproblems, which can lead to the solution of the manipulation puzzle. Our method combines a heuristic-driven forward search of subproblems with an optimization-based Task-and-Motion Planning solver. To guide the search, we introduce heuristics to generate and prioritize useful subgoals. We evaluate our approach on various manually designed and automatically generated scenes, demonstrating the benefits of auxiliary subproblems in sequential manipulation planning.},
  archive   = {C_ICRA},
  author    = {Svetlana Levit and Joaquim Ortiz-Haro and Marc Toussaint},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610974},
  month     = {5},
  pages     = {14924-14930},
  title     = {Solving sequential manipulation puzzles by finding easier subproblems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). R-LGP: A reachability-guided logic-geometric programming
framework for optimal task and motion planning on mobile manipulators.
<em>ICRA</em>, 14917–14923. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an optimization-based solution to task and motion planning (TAMP) on mobile manipulators. Logic-geometric programming (LGP) has shown promising capabilities for optimally dealing with hybrid TAMP problems that involve abstract and geometric constraints. However, LGP does not scale well to high-dimensional systems (e.g. mobile manipulators) and can suffer from obstacle avoidance issues due to local minima. In this work, we extend LGP with a sampling-based reachability graph to enable solving optimal TAMP on high-DoF mobile manipulators. The proposed reachability graph can incorporate environmental information (obstacles) to provide the planner with sufficient geometric constraints. This reachability-aware heuristic efficiently prunes infeasible sequences of actions in the continuous domain, hence, it reduces replanning by securing feasibility at the final full path trajectory optimization. Our framework proves to be time-efficient in computing optimal and collision-free solutions, while outperforming the current state of the art on metrics of success rate, planning time, path length and number of steps. We validate our framework on the physical Toyota HSR robot and report comparisons on a series of mobile manipulation tasks of increasing difficulty. Videos of the experiments are available here.},
  archive   = {C_ICRA},
  author    = {Kim Tien Ly and Valeriy Semenov and Mattia Risiglione and Wolfgang Merkt and Ioannis Havoutis},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611389},
  month     = {5},
  pages     = {14917-14923},
  title     = {R-LGP: A reachability-guided logic-geometric programming framework for optimal task and motion planning on mobile manipulators},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sense in motion with belief clustering: Efficient gas source
localization with mobile robots. <em>ICRA</em>, 14909–14916. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Given the patchy nature of gas plumes and the slow response of conventional gas sensors, the use of mobile robots for Gas Source Localization (GSL) tasks presents significant challenges. These aspects increase the difficulties in obtaining gas measurements, encompassing both qualitative and quantitative aspects. Most existing model-based GSL algorithms rely on lengthy stops at each sampling point to ensure accurate gas measurements. However, this approach not only prolongs the time required for a single measurement but also hinders sampling during robot motion, thus exacerbating the scarcity of available gas measurements. In this work, our goal is to push the boundaries in terms of continuity in sampling to enhance system efficiency. Firstly, we decouple and comprehensively evaluate the impact of both plume dynamics and gas sensor properties on the GSL performance. Secondly, we demonstrate that adopting a continuous sampling strategy, which has been generally overlooked in prior research, markedly enhances the system efficiency by obviating the prolonged measurement pauses and leveraging all the data gathered during the robot motion. Thirdly, we further expand the capabilities of the continuous sampling by introducing a novel informative path-planning strategy, which takes into account all the information gathered along the robot&#39;s movement. The proposed method is evaluated in both simulation and reality under different scenarios emulating indoor environmental conditions.},
  archive   = {C_ICRA},
  author    = {Wanting Jin and Alcherio Martinoli},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611440},
  month     = {5},
  pages     = {14909-14916},
  title     = {Sense in motion with belief clustering: Efficient gas source localization with mobile robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effort level search in infinite completion trees with
application to task-and-motion planning. <em>ICRA</em>, 14902–14908. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Solving a Task-and-Motion Planning (TAMP) problem can be represented as a sequential (meta-) decision process, where early decisions concern the skeleton (sequence of logic actions) and later decisions concern what to compute for such skeletons (e.g., action parameters, bounds, RRT paths, or full optimal manipulation trajectories). We consider the general problem of how to schedule compute effort in such hierarchical solution processes. More specifically, we introduce infinite completion trees as a problem formalization, where before we can expand or evaluate a node, we have to solve a preemptible computational sub-problem of a priori unknown compute effort. Infinite branchings represent an infinite choice of random initializations of computational sub-problems. Decision making in such trees means to decide on where to invest compute or where to widen a branch. We propose a heuristic to balance branching width and compute depth using polynomial level sets. We show completeness of the resulting solver and that a round robin baseline strategy used previously for TAMP becomes a special case. Experiments confirm the robustness and efficiency of the method on problems including stochastic bandits and a suite of TAMP problems, and compare our approach to a round robin baseline. An appendix comparing the framework to bandit methods and proposing a corresponding tree policy version is found on the supplementary webpage 1 .},
  archive   = {C_ICRA},
  author    = {Marc Toussaint and Joaquim Ortiz-Haro and Valentin N. Hartmann and Erez Karpas and Wolfgang Hönig},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611722},
  month     = {5},
  pages     = {14902-14908},
  title     = {Effort level search in infinite completion trees with application to task-and-motion planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Indoor exploration and simultaneous trolley collection
through task-oriented environment partitioning. <em>ICRA</em>,
14895–14901. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a simultaneous exploration and object search framework for the application of autonomous trolley collection. For environment representation, a task-oriented environment partitioning algorithm is presented to extract diverse information for each sub-task. First, LiDAR data is classified as potential objects, walls, and obstacles after outlier removal. Segmented point clouds are then transformed into a hybrid map with the following functional components: object proposals to avoid missing trolleys during exploration; room layouts for semantic space segmentation; and polygonal obstacles containing geometry information for efficient motion planning. For exploration and simultaneous trolley collection, we propose an efficient exploration-based object search method. First, a traveling salesman problem with precedence constraints (TSP-PC) is formulated by grouping frontiers and object proposals. The next target is selected by prioritizing object search while avoiding excessive robot backtracking. Then, feasible trajectories with adequate obstacle clearance are generated by topological graph search. We validate the proposed framework through simulations and demonstrate the system with real-world autonomous trolley collection tasks.},
  archive   = {C_ICRA},
  author    = {Junjie Gao and Peijia Xie and Xuheng Gao and Zhirui Sun and Jiankun Wang and Max Q.-H. Meng},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610500},
  month     = {5},
  pages     = {14895-14901},
  title     = {Indoor exploration and simultaneous trolley collection through task-oriented environment partitioning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). D-LGP: Dynamic logic-geometric program for reactive task and
motion planning. <em>ICRA</em>, 14888–14894. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many real-world sequential manipulation tasks involve a combination of discrete symbolic search and continuous motion planning, collectively known as combined task and motion planning (TAMP). However, prevailing methods often struggle with the computational burden and intricate combinatorial challenges, limiting their applications for online replanning in the real world. To address this, we propose Dynamic Logic-Geometric Program (D-LGP), a novel approach integrating Dynamic Tree Search and global optimization for efficient hybrid planning. Through empirical evaluation on three benchmarks, we demonstrate the efficacy of our approach, showcasing superior performance in comparison to state-of-the-art techniques. We validate our approach through simulation and demonstrate its reactive capability to cope with online uncertainty and external disturbances in the real world. Project webpage: https://sites.google.com/view/dyn-lgp.},
  archive   = {C_ICRA},
  author    = {Teng Xue and Amirreza Razmjoo and Sylvain Calinon},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610285},
  month     = {5},
  pages     = {14888-14894},
  title     = {D-LGP: Dynamic logic-geometric program for reactive task and motion planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Extending the cooperative dual-task space in conformal
geometric algebra. <em>ICRA</em>, 14882–14887. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we are presenting an extension of the cooperative dual-task space (CDTS) in conformal geometric algebra. The CDTS was first defined using dual quaternion algebra and is a well established framework for the simplified definition of tasks using two manipulators. By integrating conformal geometric algebra, we aim to further enhance the geometric expressiveness and thus simplify the modeling of various tasks. We show this formulation by first presenting the CDTS and then its extension that is based around a cooperative pointpair. This extension keeps all the benefits of the original formulation that is based on dual quaternions, but adds more tools for geometric modeling of the dual-arm tasks. We also present how this CGACDTS can be seamlessly integrated with an optimal control framework in geometric algebra that was derived in previous work. In the experiments, we demonstrate how to model different objectives and constraints using the CGA-CDTS. Using a setup of two Franka Emika robots we then show the effectiveness of our approach using model predictive control in real world experiments.},
  archive   = {C_ICRA},
  author    = {Tobias Löw and Sylvain Calinon},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610558},
  month     = {5},
  pages     = {14882-14887},
  title     = {Extending the cooperative dual-task space in conformal geometric algebra},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). COAST: COnstraints and STreams for task and motion planning.
<em>ICRA</em>, 14875–14881. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Task and Motion Planning (TAMP) algorithms solve long-horizon robotics tasks by integrating task planning with motion planning; the task planner proposes a sequence of actions towards a goal state and the motion planner verifies whether this action sequence is geometrically feasible for the robot. However, state-of-the-art TAMP algorithms do not scale well with the difficulty of the task and require an impractical amount of time to solve relatively small problems. We propose Constraints and Streams for Task and Motion Planning (COAST), a probabilistically-complete, sampling-based TAMP algorithm that combines stream-based motion planning with an efficient, constrained task planning strategy. We validate COAST on three challenging TAMP domains and demonstrate that our method outperforms baselines in terms of cumulative task planning time by an order of magnitude. You can find more supplementary materials on our project ${\color{blue}{\mathbf{website}}}$.},
  archive   = {C_ICRA},
  author    = {Brandon Vu and Toki Migimatsu and Jeannette Bohg},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611670},
  month     = {5},
  pages     = {14875-14881},
  title     = {COAST: COnstraints and STreams for task and motion planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Long-HOT: A modular hierarchical approach for long-horizon
object transport. <em>ICRA</em>, 14867–14874. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We aim to address key challenges in long-horizon embodied exploration and navigation by proposing a long-horizon object transport task called Long-HOT and a novel modular framework for temporally extended navigation. Agents in Long-HOT need to efficiently find and pick up target objects that are scattered in the environment, carry them to a goal location with load constraints, and optionally have access to a container. We propose a modular topological graph-based transport policy (HTP) that explores efficiently with the help of weighted frontiers. Our hierarchical approach uses a combination of motion planning algorithms to reach point goals within explored locations and object navigation policies for moving towards semantic targets at unknown locations. Experiments on both our proposed Habitat transport task and on MultiOn benchmarks show that our method outperforms baselines and prior works. Further, we analyze the agent’s behavior for the usage of the container and demonstrate meaningful generalization to harder transport scenes with training only on simpler versions of the task.},
  archive   = {C_ICRA},
  author    = {Sriram Narayanan and Dinesh Jayaraman and Manmohan Chandraker},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611192},
  month     = {5},
  pages     = {14867-14874},
  title     = {Long-HOT: A modular hierarchical approach for long-horizon object transport},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TreeScope: An agricultural robotics dataset for LiDAR-based
mapping of trees in forests and orchards. <em>ICRA</em>, 14860–14866.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Data collection for forestry, timber, and agriculture relies on manual techniques which are labor-intensive and time-consuming. We seek to demonstrate that robotics offers improvements over these techniques and can accelerate agricultural research, beginning with semantic segmentation and diameter estimation of trees in forests and orchards. We present TreeScope v1.0, the first robotics dataset for precision agriculture and forestry addressing the counting and mapping of trees in forestry and orchards. TreeScope provides LiDAR data from agricultural environments collected with robotics platforms, such as UAV and mobile robot platforms carried by vehicles and human operators. In the first release of this dataset, we provide ground-truth data with over 1,800 manually annotated semantic labels for tree stems and field-measured tree diameters. We share benchmark scripts for these tasks that researchers may use to evaluate the accuracy of their algorithms. Finally, we run our open-source diameter estimation and off-the-shelf semantic segmentation algorithms and share our baseline results.The dataset can be found at https://treescope.org, and the data pre-processing and benchmark code is available at https://github.com/KumarRobotics/treescope.},
  archive   = {C_ICRA},
  author    = {Derek Cheng and Fernando Cladera and Ankit Prabhu and Xu Liu and Alan Zhu and P. Corey Green and Reza Ehsani and Pratik Chaudhari and Vijay Kumar},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611103},
  month     = {5},
  pages     = {14860-14866},
  title     = {TreeScope: An agricultural robotics dataset for LiDAR-based mapping of trees in forests and orchards},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NYC-indoor-VPR: A long-term indoor visual place recognition
dataset with semi-automatic annotation. <em>ICRA</em>, 14853–14859. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual Place Recognition (VPR) in indoor environments is beneficial to humans and robots for better localization and navigation. It is challenging due to appearance changes at various frequencies, and difficulties of obtaining ground truth metric trajectories for training and evaluation. This paper introduces the NYC-Indoor-VPR dataset, a unique and rich collection of over 36,000 images compiled from 13 distinct crowded scenes in New York City taken under varying lighting conditions with appearance changes. Each scene has multiple revisits across a year. To establish the ground truth for VPR, we propose a semiautomatic annotation approach that computes the positional information of each image. Our method specifically takes pairs of videos as input and yields matched pairs of images along with their estimated relative locations. The accuracy of this matching is refined by human annotators, who utilize our annotation software to correlate the selected keyframes. Finally, we present a benchmark evaluation of several state-of-the-art VPR algorithms using our annotated dataset, revealing its challenge and thus value for VPR research.},
  archive   = {C_ICRA},
  author    = {Diwei Sheng and Anbang Yang and John-Ross Rizzo and Chen Feng},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610564},
  month     = {5},
  pages     = {14853-14859},
  title     = {NYC-indoor-VPR: A long-term indoor visual place recognition dataset with semi-automatic annotation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The GOOSE dataset for perception in unstructured
environments. <em>ICRA</em>, 14838–14844. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The potential for deploying autonomous systems can be significantly increased by improving the perception and interpretation of the environment. However, the development of deep learning-based techniques for autonomous systems in unstructured outdoor environments poses challenges due to limited data availability for training and testing. To address this gap, we present the German Outdoor and Offroad Dataset (GOOSE), a comprehensive dataset specifically designed for unstructured outdoor environments. The GOOSE dataset incorporates 10000 labeled pairs of images and point clouds, which are utilized to train a range of state-of-the-art segmentation models on both image and point cloud data. We open source the dataset, along with an ontology for unstructured terrain, as well as dataset standards and guidelines. This initiative aims to establish a common framework, enabling the seamless inclusion of existing datasets and a fast way to enhance the perception capabilities of various robots operating in unstructured environments. This framework also makes it possible to query data for specific weather conditions or sensor setups from a database in future. The dataset, pre-trained models for offroad perception, and additional documentation can be found at https://goose-dataset.de/.},
  archive   = {C_ICRA},
  author    = {Peter Mortimer and Raphael Hagmanns and Miguel Granero and Thorsten Luettel and Janko Petereit and Hans-Joachim Wuensche},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611298},
  month     = {5},
  pages     = {14838-14844},
  title     = {The GOOSE dataset for perception in unstructured environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LiDAR-CS dataset: LiDAR point cloud dataset with
cross-sensors for 3D object detection. <em>ICRA</em>, 14822–14829. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Over the past few years, there has been remarkable progress in research on 3D point clouds and their use in autonomous driving scenarios has become widespread. However, deep learning methods heavily rely on annotated data and often face domain generalization issues. Unlike 2D images whose domains usually pertain to the texture information present in them, the features derived from a 3D point cloud are affected by the distribution of the points. The lack of a 3D domain adaptation benchmark leads to the common practice of training a model on one benchmark (e.g. Waymo) and then assessing it on another dataset (e.g. KITTI). This setting results in two distinct domain gaps: scenarios and sensors, making it difficult to analyze and evaluate the method accurately. To tackle this problem, this paper presents ${\color{Red}\text{LiDAR}}$ Dataset with ${\color{Red}\text{C}}{\text{ross}} - {\color{Red}\text{S}}{\text{ensors}}$ (LiDAR-CS Dataset), which contains large-scale annotated LiDAR point cloud under six groups of different sensors but with the same corresponding scenarios, captured from hybrid realistic LiDAR simulator. To our knowledge, LiDAR-CS Dataset is the first dataset that addresses the sensor-related gaps in the domain of 3D object detection in real traffic. Furthermore, we evaluate and analyze the performance using various baseline detectors and demonstrated its potential applications. Project page: https://opendriving.github.io/lidar-cs.},
  archive   = {C_ICRA},
  author    = {Jin Fang and Dingfu Zhou and Jingjing Zhao and Chenming Wu and Chulin Tang and Cheng-Zhong Xu and Liangjun Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611136},
  month     = {5},
  pages     = {14822-14829},
  title     = {LiDAR-CS dataset: LiDAR point cloud dataset with cross-sensors for 3D object detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IDD-x: A multi-view dataset for ego-relative important
object localization and explanation in dense and unstructured traffic.
<em>ICRA</em>, 14815–14821. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Intelligent vehicle systems require a deep understanding of the interplay between road conditions, surrounding entities, and the ego vehicle’s driving behavior for safe and efficient navigation. This is particularly critical in developing countries where traffic situations are often dense and unstructured with heterogeneous road occupants. Existing datasets, predominantly geared towards structured and sparse traffic scenarios, fall short of capturing the complexity of driving in such environments. To fill this gap, we present IDD-X, a large-scale dual-view driving video dataset. With 697K bounding boxes, 9K important object tracks, and 1-12 objects per video, IDD-X offers comprehensive ego-relative annotations for multiple important road objects covering 10 categories and 19 explanation label categories. The dataset also incorporates rearview information to provide a more complete representation of the driving environment. We also introduce custom-designed deep networks aimed at multiple important object localization and per-object explanation prediction. Overall, our dataset and introduced prediction models form the foundation for studying how road conditions and surrounding entities affect driving behavior in complex traffic situations.},
  archive   = {C_ICRA},
  author    = {Chirag Parikh and Rohit Saluja and C. V. Jawahar and Ravi Kiran Sarvadevabhatla},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609989},
  month     = {5},
  pages     = {14815-14821},
  title     = {IDD-X: A multi-view dataset for ego-relative important object localization and explanation in dense and unstructured traffic},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing inland water safety: The lake constance obstacle
detection benchmark. <em>ICRA</em>, 14808–14814. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous navigation on inland waters requires an accurate understanding of the environment in order to react to possible obstacles. Deep learning is a promising technique to detect obstacles robustly. However, supervised deep learning models require large data-sets to adjust their weights and to generalize to unseen data. Therefore, we equipped our research vessel with a laser scanner and a stereo camera to record a novel obstacle detection data-set for inland waters. We annotated 1974 stereo images and lidar point clouds with 3d bounding boxes. Furthermore, we provide an initial approach and a suitable metric 2 to compare the results on the test data-set. The data-set is publicly available 3 and seeks to make a contribution towards increasing the safety on inland waters.},
  archive   = {C_ICRA},
  author    = {Dennis Griesser and Matthias O. Franz and Georg Umlauf},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610600},
  month     = {5},
  pages     = {14808-14814},
  title     = {Enhancing inland water safety: The lake constance obstacle detection benchmark},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RiskBench: A scenario-based benchmark for risk
identification. <em>ICRA</em>, 14800–14807. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Intelligent driving systems aim to achieve a zero-collision mobility experience, requiring interdisciplinary efforts to enhance safety performance. This work focuses on risk identification, the process of identifying and analyzing risks stemming from dynamic traffic participants and unexpected events. While significant advances have been made in the community, the current evaluation of different risk identification algorithms uses independent datasets, leading to difficulty in direct comparison and hindering collective progress toward safety performance enhancement. To address this limitation, we introduce RiskBench, a large-scale scenario-based benchmark for risk identification. We design a scenario taxonomy and augmentation pipeline to enable a systematic collection of ground truth risks under diverse scenarios. We assess the ability of ten algorithms to (1) detect and locate risks, (2) anticipate risks, and (3) facilitate decision-making. We conduct extensive experiments and summarize future research on risk identification. Our aim is to encourage collaborative endeavors in achieving a society with zero collisions. We have made our dataset and benchmark toolkit publicly at this project webpage.},
  archive   = {C_ICRA},
  author    = {Chi-Hsi Kung and Chieh-Chi Yang and Pang-Yuan Pao and Shu-Wei Lu and Pin-Lun Chen and Hsin-Cheng Lu and Yi-Ting Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610270},
  month     = {5},
  pages     = {14800-14807},
  title     = {RiskBench: A scenario-based benchmark for risk identification},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GenDOM: Generalizable one-shot deformable object
manipulation with parameter-aware policy. <em>ICRA</em>, 14792–14799.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Due to the inherent uncertainty in their deformability during motion, previous methods in deformable object manipulation, such as rope and cloth, often required hundreds of real-world demonstrations to train a manipulation policy for each object, which hinders their applications in our ever-changing world. To address this issue, we introduce GenDOM, a framework that allows the manipulation policy to handle different deformable objects with only a single real-world demonstration. To achieve this, we augment the policy by conditioning it on deformable object parameters and training it with a diverse range of simulated deformable objects so that the policy can adjust actions based on different object parameters. At the time of inference, given a new object, GenDOM can estimate the deformable object parameters with only a single real-world demonstration by minimizing the disparity between the grid density of point clouds of real-world demonstrations and simulations in a differentiable physics simulator. Empirical validations on both simulated and real-world object manipulation setups clearly show that our method can manipulate different objects with a single demonstration and significantly outperforms the baseline in both environments (a 62% improvement for in-domain ropes and a 15% improvement for out-of-distribution ropes in simulation, as well as a 26% improvement for ropes and a 50% improvement for cloths in the real world), demonstrating the effectiveness of our approach in one-shot deformable object manipulation. https://sites.google.com/view/gendom/home.},
  archive   = {C_ICRA},
  author    = {So Kuroki and Jiaxian Guo and Tatsuya Matsushima and Takuya Okubo and Masato Kobayashi and Yuya Ikeda and Ryosuke Takanami and Paul Yoo and Yutaka Matsuo and Yusuke Iwasawa},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611378},
  month     = {5},
  pages     = {14792-14799},
  title     = {GenDOM: Generalizable one-shot deformable object manipulation with parameter-aware policy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tractable joint prediction and planning over discrete
behavior modes for urban driving. <em>ICRA</em>, 14785–14791. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Significant progress has been made in training multimodal trajectory forecasting models for autonomous driving. However, effectively integrating these models with downstream planners and model-based control approaches is still an open problem. Although these models have conventionally been evaluated for open-loop prediction, we show that they can be used to parameterize autoregressive closed-loop models without retraining. We consider recent trajectory prediction approaches which leverage learned anchor embeddings to predict multiple trajectories, finding that these anchor embeddings can parameterize discrete and distinct modes representing high-level driving behaviors. We propose to perform fully reactive closed-loop planning over these discrete latent modes, allowing us to tractably model the causal interactions between agents at each step. We validate our approach on a suite of more dynamic merging scenarios, finding that our approach avoids the frozen robot problem which is pervasive in conventional planners. Our approach also outperforms the previous state-of-the-art in CARLA on challenging dense traffic scenarios when evaluated at realistic speeds.},
  archive   = {C_ICRA},
  author    = {Adam Villaflor and Brian Yang and Huangyuan Su and Katerina Fragkiadaki and John Dolan and Jeff Schneider},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610804},
  month     = {5},
  pages     = {14785-14791},
  title     = {Tractable joint prediction and planning over discrete behavior modes for urban driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Actor-critic model predictive control. <em>ICRA</em>,
14777–14784. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An open research question in robotics is how to combine the benefits of model-free reinforcement learning (RL)—known for its strong task performance and flexibility in optimizing general reward formulations—with the robustness and online replanning capabilities of model predictive control (MPC). This paper provides an answer by introducing a new framework called Actor-Critic Model Predictive Control. The key idea is to embed a differentiable MPC within an actor-critic RL framework. The proposed approach leverages the short-term predictive optimization capabilities of MPC with the exploratory and end-to-end training properties of RL. The resulting policy effectively manages both short-term decisions through the MPC-based actor and long-term prediction via the critic network, unifying the benefits of both model-based control and end-to-end learning. We validate our method in both simulation and the real world with a quadcopter platform across various high-level tasks. We show that the proposed architecture can achieve real-time control performance, learn complex behaviors via trial and error, and retain the predictive properties of the MPC to better handle out of distribution behaviour.},
  archive   = {C_ICRA},
  author    = {Angel Romero and Yunlong Song and Davide Scaramuzza},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610381},
  month     = {5},
  pages     = {14777-14784},
  title     = {Actor-critic model predictive control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DeformNet: Latent space modeling and dynamics prediction for
deformable object manipulation. <em>ICRA</em>, 14770–14776. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Manipulating deformable objects is a ubiquitous task in household environments, demanding adequate representation and accurate dynamics prediction due to the objects’ infinite degrees of freedom. This work proposes DeformNet, which utilizes latent space modeling with a learned 3D representation model to tackle these challenges effectively. The proposed representation model combines a PointNet encoder and a conditional neural radiance field (NeRF), facilitating a thorough acquisition of object deformations and variations in lighting conditions. To model the complex dynamics, we employ a recurrent state-space model (RSSM) that accurately predicts the transformation of the latent representation over time. Extensive simulation experiments with diverse objectives demonstrate the generalization capabilities of DeformNet for various deformable object manipulation tasks, even in the presence of previously unseen goals. Finally, we deploy DeformNet on an actual UR5 robotic arm to demonstrate its capability in real-world scenarios.},
  archive   = {C_ICRA},
  author    = {Chenchang Li and Zihao Ai and Tong Wu and Xiaosa Li and Wenbo Ding and Huazhe Xu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611243},
  month     = {5},
  pages     = {14770-14776},
  title     = {DeformNet: Latent space modeling and dynamics prediction for deformable object manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time adaptive safety-critical control with gaussian
processes in high-order uncertain models. <em>ICRA</em>, 14763–14769.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an adaptive online learning framework for systems with uncertain parameters to ensure safety-critical control in non-stationary environments. Our approach consists of two phases. The initial phase is centered on a novel sparse Gaussian process (GP) framework. We first integrate a forgetting factor to refine a variational sparse GP algorithm, thus enhancing its adaptability. Subsequently, the hyperparameters of the Gaussian model are trained with a specially compound kernel, and the Gaussian model’s online inferential capability and computational efficiency are strengthened by updating a solitary inducing point derived from newly samples, in conjunction with the learned hyperparameters. In the second phase, we propose a safety filter based on high order control barrier functions (HOCBFs), synergized with the previously trained learning model. By leveraging the compound kernel from the first phase, we effectively address the inherent limitations of GPs in handling high-dimensional problems for real-time applications. The derived controller ensures a rigorous lower bound on the probability of satisfying the safety specification. Finally, the efficacy of our proposed algorithm is demonstrated through real-time obstacle avoidance experiments executed using both simulation platform and a real-world 7-DOF robot.},
  archive   = {C_ICRA},
  author    = {Yu Zhang and Long Wen and Xiangtong Yao and Zhenshan Bing and Linghuan Kong and Wei He and Alois Knoll},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610624},
  month     = {5},
  pages     = {14763-14769},
  title     = {Real-time adaptive safety-critical control with gaussian processes in high-order uncertain models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NaviFormer: A data-driven robot navigation approach via
sequence modeling and path planning with safety verification.
<em>ICRA</em>, 14756–14762. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning has shown great potential in improving the performance of robot navigation. In response to the increasing deployments of mobile robots within various scenarios, a data-driven paradigm of navigation approach with safety verification is preferred where one can train RL algorithms with large amounts of prior data, keep learning continuously, and ensure safe navigation in applications. Conventional end-to-end reinforcement learning navigation paradigms have encountered multiple challenges in meeting these demands. In this work, we introduce a novel robot navigation approach termed NaviFormer. This approach handles navigation tasks based on sequence modeling to obtain the data-driven ability. It also integrates rule-based verification for safety insurance. We conduct a series of experiments to validate the data-driven ability of our approach and to compare it with existing navigation methods. We also perform quantitative tests on a real-world robot platform, TurtleBot. The experimental results show our method’s outstanding data-driven ability and highlight its superior arrival rate and generalization compared to other state-of-the-art methods like the PPO-based navigation method.},
  archive   = {C_ICRA},
  author    = {Xuyang Zhang and Ziyang Feng and Quecheng Qiu and Yu’an Chen and Bei Hua and Jianmin Ji},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610076},
  month     = {5},
  pages     = {14756-14762},
  title     = {NaviFormer: A data-driven robot navigation approach via sequence modeling and path planning with safety verification},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LiDARFormer: A unified transformer-based multi-task network
for LiDAR perception. <em>ICRA</em>, 14740–14747. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There is a recent need in the LiDAR perception field for unifying multiple tasks in a single strong network with improved performance, as opposed to using separate networks for each task. In this paper, we introduce a new LiDAR multi-task learning paradigm based on the transformer. The proposed LiDARFormer utilizes cross-space global contextual feature information and exploits cross-task synergy to boost the performance of LiDAR perception tasks across multiple large-scale datasets and benchmarks. Our novel transformer-based framework includes a cross-space transformer module that learns attentive features between the 2D dense Bird’s Eye View (BEV) and 3D sparse voxel feature maps. Additionally, we propose a transformer decoder for the segmentation task to dynamically adjust the learned features by leveraging the categorical feature representations. Furthermore, we combine the segmentation and detection features in a shared transformer decoder with cross-task attention layers to enhance and integrate the object-level and class-level features. LiDARFormer is evaluated on the large-scale nuScenes and the Waymo Open datasets for both 3D detection and semantic segmentation tasks, and it achieves state-of-the-art performance on both tasks.},
  archive   = {C_ICRA},
  author    = {Zixiang Zhou and Dongqiangzi Ye and Weijia Chen and Yufei Xie and Yu Wang and Panqu Wang and Hassan Foroosh},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610374},
  month     = {5},
  pages     = {14740-14747},
  title     = {LiDARFormer: A unified transformer-based multi-task network for LiDAR perception},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-task learning for real-time autonomous driving
leveraging task-adaptive attention generator. <em>ICRA</em>,
14732–14739. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-time processing is crucial in autonomous driving systems due to the imperative of instantaneous decision-making and rapid response. In real-world scenarios, autonomous vehicles are continuously tasked with interpreting their surroundings, analyzing intricate sensor data, and making decisions within split seconds to ensure safety through numerous computer vision tasks. In this paper, we present a new real-time multi-task network adept at three vital autonomous driving tasks: monocular 3D object detection, semantic segmentation, and dense depth estimation. To counter the challenge of negative transfer — the prevalent issue in multi-task learning — we introduce a task-adaptive attention generator. This generator is designed to automatically discern interrelations across the three tasks and arrange the task-sharing pattern, all while leveraging the efficiency of the hard-parameter sharing approach. To the best of our knowledge, the proposed model is pioneering in its capability to concurrently handle multiple tasks, notably 3D object detection, while maintaining real-time processing speeds. Our rigorously optimized network, when tested on the Cityscapes-3D datasets, consistently outperforms various base-line models. Moreover, an in-depth ablation study substantiates the efficacy of the methodologies integrated into our framework.},
  archive   = {C_ICRA},
  author    = {Wonhyeok Choi and Mingyu Shin and Hyukzae Lee and Jaehoon Cho and Jaehyeon Park and Sunghoon Im},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610716},
  month     = {5},
  pages     = {14732-14739},
  title     = {Multi-task learning for real-time autonomous driving leveraging task-adaptive attention generator},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LiDAR data synthesis with denoising diffusion probabilistic
models. <em>ICRA</em>, 14724–14731. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generative modeling of 3D LiDAR data is an emerging task with promising applications for autonomous mobile robots, such as scalable simulation, scene manipulation, and sparse-to-dense completion of LiDAR point clouds. While existing approaches have demonstrated the feasibility of image-based LiDAR data generation using deep generative models, they still struggle with fidelity and training stability. In this work, we present R2DM, a novel generative model for LiDAR data that can generate diverse and high-fidelity 3D scene point clouds based on the image representation of range and reflectance intensity. Our method is built upon denoising diffusion probabilistic models (DDPMs), which have shown impressive results among generative model frameworks in recent years. To effectively train DDPMs in the LiDAR domain, we first conduct an in-depth analysis of data representation, loss functions, and spatial inductive biases. Leveraging our R2DM model, we also introduce a flexible LiDAR completion pipeline based on the powerful capabilities of DDPMs. We demonstrate that our method surpasses existing methods in generating tasks on the KITTI-360 and KITTI-Raw datasets, as well as in the completion task on the KITTI-360 dataset. Our project page can be found at https://kazuto1011.github.io/r2dm.},
  archive   = {C_ICRA},
  author    = {Kazuto Nakashima and Ryo Kurazume},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611480},
  month     = {5},
  pages     = {14724-14731},
  title     = {LiDAR data synthesis with denoising diffusion probabilistic models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ActFormer: Scalable collaborative perception via active
queries. <em>ICRA</em>, 14716–14723. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collaborative perception leverages rich visual observations from multiple robots to extend a single robot’s perception ability beyond its field of view. Many prior works receive messages broadcast from all collaborators, leading to a scalability challenge when dealing with a large number of robots and sensors. In this work, we aim to address scalable camera-based collaborative perception with a Transformer-based architecture. Our key idea is to enable a single robot to intelligently discern the relevance of the collaborators and their associated cameras according to a learned spatial prior. This proactive understanding of the visual features’ relevance does not require the transmission of the features themselves, enhancing both communication and computation efficiency. Specifically, we present ActFormer, a Transformer that learns bird’s eye view (BEV) representations by using predefined BEV queries to interact with multi-robot multi-camera inputs. Each BEV query can actively select relevant cameras for information aggregation based on pose information, instead of interacting with all cameras indiscriminately. Experiments on the V2X-Sim dataset demonstrate that ActFormer improves the detection performance from 29.89% to 45.15% in terms of AP@0.7 with about 50% fewer queries, showcasing the effectiveness of ActFormer in multi-agent collaborative 3D object detection.},
  archive   = {C_ICRA},
  author    = {Suozhi Huang and Juexiao Zhang and Yiming Li and Chen Feng},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610997},
  month     = {5},
  pages     = {14716-14723},
  title     = {ActFormer: Scalable collaborative perception via active queries},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lightweight event-based optical flow estimation via
iterative deblurring. <em>ICRA</em>, 14708–14715. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inspired by frame-based methods, state-of-the-art event-based optical flow networks rely on the explicit construction of correlation volumes, which are expensive to compute and store, rendering them unsuitable for robotic applications with limited compute and energy budget. Moreover, correlation volumes scale poorly with resolution, prohibiting them from estimating high-resolution flow. We observe that the spatiotemporally continuous traces of events provide a natural search direction for seeking pixel correspondences, obviating the need to rely on gradients of explicit correlation volumes as such search directions. We introduce IDNet (Iterative Deblurring Network), a lightweight yet high-performing event-based optical flow network directly estimating flow from event traces without using correlation volumes. We further propose two iterative update schemes: &quot;ID&quot; which iterates over the same batch of events, and &quot;TID&quot; which iterates over time with streaming events in an online fashion. Our top-performing model (ID) sets a new state of the art on DSEC benchmark. Meanwhile, the base model (TID) is competitive with prior arts while using 80% fewer parameters, consuming 20x less memory footprint and running 40% faster on the NVidia Jetson Xavier NX. Furthermore, the TID scheme is even more efficient offering an additional 5x faster inference speed and 8 ms ultra-low latency at the cost of only a 9% performance drop, making it the only model among current literature capable of real-time operation while maintaining decent performance.Code: https://github.com/tudelft/idnet.},
  archive   = {C_ICRA},
  author    = {Yilun Wu and Federico Paredes-Vallés and Guido C. H. E. de Croon},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610353},
  month     = {5},
  pages     = {14708-14715},
  title     = {Lightweight event-based optical flow estimation via iterative deblurring},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CVFormer: Learning circum-view representation and
consistency for vision-based occupancy prediction via transformers.
<em>ICRA</em>, 14701–14707. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the increasing demands for perception accuracy in autonomous driving, there is a growing focus on fine-grained 3D semantic occupancy prediction. Effectively representing detailed three-dimensional scenes has become a significant challenge in the development of this task. In this paper, we present a novel transformer-based framework named CVFormer, which leverages two-dimensional circum-views from the ego to excavate three-dimensional features of the surrounding environment. Circum-views provide a novel solution for effectively addressing the representation of dense and fine-grained scenes. Specifically, a multi-attention module CTMA is designed for fusing temporal features from circum-views to fully exploit the spatiotemporal correlations between frames and capture more comprehensive clues. Furthermore, a novel 2D projection constraint is established by observing objects from different perspective directions, and multiple 3D constraints based on object invariance and semantic consistency are also conducted for supervising the network, which enhances its performance of understanding the scene. Experimental results on nuScenes dataset demonstrate that the proposed CVFormer obviously outperforms existing methods for occupancy prediction.},
  archive   = {C_ICRA},
  author    = {Zhengqi Bai and Wenjun Shi and Dongchen Zhu and Hanlong Kang and Guanghui Zhang and Gang Ye and Yang Xiao and Lei Wang and Xiaolin Zhang and Bo Li and Jiamao Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611679},
  month     = {5},
  pages     = {14701-14707},
  title     = {CVFormer: Learning circum-view representation and consistency for vision-based occupancy prediction via transformers},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Amodal optical flow. <em>ICRA</em>, 14677–14684. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Optical flow estimation is very challenging in situations with transparent or occluded objects. In this work, we address these challenges at the task level by introducing Amodal Optical Flow, which integrates optical flow with amodal perception. Instead of only representing the visible regions, we define amodal optical flow as a multi-layered pixel-level motion field that encompasses both visible and occluded regions of the scene. To facilitate research on this new task, we extend the AmodalSynthDrive dataset to include pixel-level labels for amodal optical flow estimation. We present several strong baselines, along with the Amodal Flow Quality metric to quantify the performance in an interpretable manner. Furthermore, we propose the novel AmodalFlowNet as an initial step toward addressing this task. AmodalFlowNet consists of a transformer-based cost-volume encoder paired with a recurrent transformer decoder which facilitates recurrent hierarchical feature propagation and amodal semantic grounding. We demonstrate the tractability of amodal optical flow in extensive experiments and show its utility for downstream tasks such as panoptic tracking. We make the dataset, code, and trained models publicly available at http://amodal-flow.cs.uni-freiburg.de.},
  archive   = {C_ICRA},
  author    = {Maximilian Luz and Rohit Mohan and Ahmed Rida Sekkat and Oliver Sawade and Elmar Matthes and Thomas Brox and Abhinav Valada},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611314},
  month     = {5},
  pages     = {14677-14684},
  title     = {Amodal optical flow},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised pretraining and finetuning for monocular
depth and visual odometry. <em>ICRA</em>, 14669–14676. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For the task of simultaneous monocular depth and visual odometry estimation, we propose learning self-supervised transformer-based models in two steps. Our first step consists in a generic pretraining to learn 3D geometry, using cross-view completion objective (CroCo), followed by self-supervised finetuning on non-annotated videos. We show that our self-supervised models can reach state-of-the-art performance ’without bells and whistles’ using standard components such as visual transformers, dense prediction transformers and adapters. We demonstrate the effectiveness of our proposed method by running evaluations on six benchmark datasets, both static and dynamic, indoor and outdoor, with synthetic and real images. For all datasets, our method outperforms state-of-the-art methods, in particular for depth prediction task.},
  archive   = {C_ICRA},
  author    = {Leonid Antsfeld and Boris Chidlovskii},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611058},
  month     = {5},
  pages     = {14669-14676},
  title     = {Self-supervised pretraining and finetuning for monocular depth and visual odometry},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). TVFusionGAN: Thermal-visible image fusion based on
multi-level adversarial network strategy. <em>ICRA</em>, 14653–14660.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Thermal imaging is effective in low-light or night-time conditions due to its ability to capture thermal radiation differences, but lacks texture compared to visible images. Conversely, visible images retain more texture information, particularly during the daytime, but perform poorly at night. To address the limitations of both modalities, recent methods have utilized fusion techniques to generate images that combine thermal and visible properties. This paper presents an end-to-end fusion network leveraging generative adversarial networks (GANs) to fuse salient components from both modalities. Our network includes a generator and two discriminators. The generator produces fusion images with salient objects using a specially designed CIoU loss, while the discriminators ensure that the fused images are salient at both holistic and local scales. One discriminator encourages the fused images to resemble visible images overall, while the other ensures that targeted objects in the fused images are as salient as in thermal images. Our method effectively preserves thermal radiation of salient objects in infrared images while incorporating the textures of visible images.},
  archive   = {C_ICRA},
  author    = {Guoyu Lu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611671},
  month     = {5},
  pages     = {14653-14660},
  title     = {TVFusionGAN: Thermal-visible image fusion based on multi-level adversarial network strategy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TiV-ODE: A neural ODE-based approach for controllable video
generation from text-image pairs. <em>ICRA</em>, 14645–14652. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Videos capture the evolution of continuous dynamical systems over time in the form of discrete image sequences. Recently, video generation models have been widely used in robotic research. However, generating controllable videos from image-text pairs is an important yet underexplored research topic in both robotic and computer vision communities. This paper introduces an innovative and elegant framework named TiV-ODE, formulating this task as modeling the dynamical system in a continuous space. Specifically, our framework leverages the ability of Neural Ordinary Differential Equations (Neural ODEs) to model the complex dynamical system depicted by videos as a nonlinear ordinary differential equation. The resulting framework offers control over the generated videos’ dynamics, content, and frame rate, a feature not provided by previous methods. Experiments demonstrate the ability of the proposed method to generate highly controllable and visually consistent videos and its capability of modeling dynamical systems. Overall, this work is a significant step towards developing advanced controllable video generation models that can handle complex and dynamic scenes.},
  archive   = {C_ICRA},
  author    = {Yucheng Xu and Nanbo Li and Arushi Goel and Zonghai Yao and Zijian Guo and Hamidreza Kasaei and Mohammadreza Kasaei and Zhibin Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610149},
  month     = {5},
  pages     = {14645-14652},
  title     = {TiV-ODE: A neural ODE-based approach for controllable video generation from text-image pairs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). FE-DeTr: Keypoint detection and tracking in low-quality
image frames with events. <em>ICRA</em>, 14638–14644. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Keypoint detection and tracking in traditional image frames are often compromised by image quality issues such as motion blur and extreme lighting conditions. Event cameras offer potential solutions to these challenges by virtue of their high temporal resolution and high dynamic range. However, they have limited performance in practical applications due to their inherent noise in event data. This paper advocates fusing the complementary information from image frames and event streams to achieve more robust keypoint detection and tracking. Specifically, we propose a novel keypoint detection network that fuses the textural and structural information from image frames with the high-temporal-resolution motion information from event streams, namely FE-DeTr. The network leverages a temporal response consistency for supervision, ensuring stable and efficient keypoint detection. Moreover, we use a spatio-temporal nearest-neighbor search strategy for robust keypoint tracking. Extensive experiments are conducted on a new dataset featuring both image frames and event data captured under extreme conditions. The experimental results confirm the superior performance of our method over both existing frame-based and event-based methods. Our code, pre-trained models, and dataset are available at https://github.com/yuyangpoi/FE-DeTr.},
  archive   = {C_ICRA},
  author    = {Xiangyuan Wang and Kuangyi Chen and Wen Yang and Lei Yu and Yannan Xing and Huai Yu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610579},
  month     = {5},
  pages     = {14638-14644},
  title     = {FE-DeTr: Keypoint detection and tracking in low-quality image frames with events},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FSD: Fast self-supervised single RGB-d to categorical 3D
objects. <em>ICRA</em>, 14630–14637. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we address the challenging task of 3D object recognition without the reliance on real-world 3D labeled data. Our goal is to predict the 3D shape, size, and 6D pose of objects within a single RGB-D image, operating at the category level and eliminating the need for CAD models during inference. While existing self-supervised methods have made strides in this field, they often suffer from inefficiencies arising from non-end-to-end processing, reliance on separate models for different object categories, and slow surface extraction during the training of implicit reconstruction models; thus hindering both the speed and real-world applicability of the 3D recognition process. Our proposed method leverages a multi-stage training pipeline, designed to efficiently transfer synthetic performance to the real-world domain. This approach is achieved through a combination of 2D and 3D supervised losses during the synthetic domain training, followed by the incorporation of 2D supervised and 3D self-supervised losses on real-world data in two additional learning stages. By adopting this comprehensive strategy, our method successfully overcomes the aforementioned limitations and outperforms existing self-supervised 6D pose and size estimation baselines on the NOCS test-set with a 16.4% absolute improvement in mAP for 6D pose estimation while running in near real-time at 5 Hz. Project page: fsd6d.github.io},
  archive   = {C_ICRA},
  author    = {Mayank Lunayach and Sergey Zakharov and Dian Chen and Rares Ambrus and Zsolt Kira and Muhammad Zubair Irshad},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611012},
  month     = {5},
  pages     = {14630-14637},
  title     = {FSD: Fast self-supervised single RGB-D to categorical 3D objects},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OSSAR: Towards open-set surgical activity recognition in
robot-assisted surgery. <em>ICRA</em>, 14622–14629. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the realm of automated robotic surgery and computer-assisted interventions, understanding robotic surgical activities stands paramount. Existing algorithms dedicated to surgical activity recognition predominantly cater to pre-defined closed-set paradigms, ignoring the challenges of real-world open-set scenarios. Such algorithms often falter in the presence of test samples originating from classes unseen during training phases. To tackle this problem, we introduce an innovative Open-Set Surgical Activity Recognition (OSSAR) framework. Our solution leverages the hyperspherical reciprocal point strategy to enhance the distinction between known and unknown classes in the feature space. Additionally, we address the issue of over-confidence in the closed set by refining model calibration, avoiding misclassification of unknown classes as known ones. To support our assertions, we establish an open-set surgical activity benchmark utilizing the public JIGSAWS dataset. Besides, we also collect a novel dataset on endoscopic submucosal dissection for surgical activity tasks. Extensive comparisons and ablation experiments on these datasets demonstrate the significant outperformance of our method over existing state-of-the-art approaches. Our proposed solution can effectively address the challenges of real-world surgical scenarios. Our code is publicly accessible at github.com/longbai1006/OSSAR.},
  archive   = {C_ICRA},
  author    = {Long Bai and Guankun Wang and Jie Wang and Xiaoxiao Yang and Huxin Gao and Xin Liang and An Wang and Mobarakol Islam and Hongliang Ren},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610246},
  month     = {5},
  pages     = {14622-14629},
  title     = {OSSAR: Towards open-set surgical activity recognition in robot-assisted surgery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalizable thermal-based depth estimation via pre-trained
visual foundation model. <em>ICRA</em>, 14614–14621. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Depth estimation is a crucial task in computer vision, applicable to various domains such as 3D reconstruction, robotics, and autonomous driving. In particular, thermal-based depth estimation has unique advantages, including night-time vision. However, the existing depth estimation method remains challenging in robust generalization due to limited data resources and spectral differences between thermal and RGB images. In this paper, we present a self-supervised approach to enhance thermal-based depth estimation by leveraging pre-trained visual models initially designed for RGB data. In detail, we design a novel two-stage training strategy, incorporating Low-rank Adapters and Convolutional Adapters, which not only significantly improves accuracy and robustness but also enables impressive zero-shot generalization capabilities. Our method outperforms existing thermal-based depth estimation models, opening new possibilities for cross-modal applications in computer vision and robotics research.},
  archive   = {C_ICRA},
  author    = {Ruoyu Fan and Wang Zhao and Matthieu Lin and Qi Wang and Yong-Jin Liu and Wenping Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610394},
  month     = {5},
  pages     = {14614-14621},
  title     = {Generalizable thermal-based depth estimation via pre-trained visual foundation model},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MOTPose: Multi-object 6D pose estimation for dynamic video
sequences using attention-based temporal fusion. <em>ICRA</em>,
14606–14613. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cluttered bin-picking environments are challenging for pose estimation models. Despite the impressive progress enabled by deep learning, single-view RGB pose estimation models perform poorly in cluttered dynamic environments. Imbuing the rich temporal information contained in the video of scenes has the potential to enhance models’ ability to deal with the adverse effects of occlusion and the dynamic nature of the environments. Moreover, joint object detection and pose estimation models are better suited to leverage the co-dependent nature of the tasks for improving the accuracy of both tasks. To this end, we propose attention-based temporal fusion for multi-object 6D pose estimation that accumulates information across multiple frames of a video sequence. Our MOTPose method takes a sequence of images as input and performs joint object detection and pose estimation for all objects in one forward pass. It learns to aggregate both object embeddings and object parameters over multiple time steps using cross-attention-based fusion modules. We evaluate our method on the physically-realistic cluttered bin-picking dataset SynPick and the YCB-Video dataset and demonstrate improved pose estimation accuracy as well as better object detection accuracy.},
  archive   = {C_ICRA},
  author    = {Arul Selvam Periyasamy and Sven Behnke},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610674},
  month     = {5},
  pages     = {14606-14613},
  title     = {MOTPose: Multi-object 6D pose estimation for dynamic video sequences using attention-based temporal fusion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online on-demand multi-robot coverage path planning.
<em>ICRA</em>, 14583–14589. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an online centralized path planning algorithm to cover a large, complex, unknown workspace with multiple homogeneous mobile robots. Our algorithm is horizon-based, synchronous, and on-demand. The recently proposed horizon-based synchronous algorithms compute all the robots’ paths in each horizon, significantly increasing the computation burden in large workspaces with many robots. As a remedy, we propose an algorithm that computes the paths for a subset of robots that have traversed previously computed paths entirely (thus on-demand) and reuses the remaining paths for the other robots. We formally prove that the algorithm guarantees complete coverage of the unknown workspace. Experimental results on several standard benchmark workspaces show that our algorithm scales to hundreds of robots in large complex workspaces and consistently beats a state-of-the-art online centralized multi-robot coverage path planning algorithm in terms of the time needed to achieve complete coverage. For its validation, we perform ROS+Gazebo simulations in five 2D grid benchmark workspaces with 10 Quadcopters and 10 TurtleBots, respectively. Also, to demonstrate its practical feasibility, we conduct one indoor experiment with two real TurtleBot2 robots and one outdoor experiment with three real Quadcopters.},
  archive   = {C_ICRA},
  author    = {Ratijit Mitra and Indranil Saha},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611610},
  month     = {5},
  pages     = {14583-14589},
  title     = {Online on-demand multi-robot coverage path planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ALPHA: Attention-based long-horizon pathfinding in
highly-structured areas. <em>ICRA</em>, 14576–14582. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The multi-agent pathfinding (MAPF) problem seeks collision-free paths for a team of agents from their current positions to their pre-set goals in a known environment, and is an essential problem found at the core of many logistics, transportation, and general robotics applications. Existing learning-based MAPF approaches typically only let each agent make decisions based on a limited field-of-view (FOV) around its position, as a natural means to fix the input dimensions of its policy network. However, this often makes policies shortsighted, since agents lack the ability to perceive and plan for obstacles/agents beyond their FOV. To address this challenge, we propose ALPHA, a new framework combining the use of ground truth proximal (local) information and fuzzy distal (global) information to let agents sequence local decisions based on the full current state of the system, and avoid such myopicity. We further allow agents to make short-term predictions about each others’ paths, as a means to reason about each others’ path intentions, thereby enhancing the level of cooperation among agents at the whole system level. Our neural structure relies on a Graph Transformer architecture to allow agents to selectively combine these different sources of information and reason about their inter-dependencies at different spatial scales. Our simulation experiments demonstrate that ALPHA outperforms both globally-guided MAPF solvers and communication-learning based ones, showcasing its potential towards scalability in realistic deployments.},
  archive   = {C_ICRA},
  author    = {Chengyang He and Tianze Yang and Tanishq Duhan and Yutong Wang and Guillaume Sartoretti},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611301},
  month     = {5},
  pages     = {14576-14582},
  title     = {ALPHA: Attention-based long-horizon pathfinding in highly-structured areas},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Db-CBS: Discontinuity-bounded conflict-based search for
multi-robot kinodynamic motion planning. <em>ICRA</em>, 14569–14575. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a multi-robot kinodynamic motion planner that enables a team of robots with different dynamics, actuation limits, and shapes to reach their goals in challenging environments. We solve this problem by combining Conflict-Based Search (CBS), a multi-agent path finding method, and discontinuity-bounded A*, a single-robot kinodynamic motion planner. Our method, db-CBS, operates in three levels. Initially, we compute trajectories for individual robots using a graph search that allows bounded discontinuities between precomputed motion primitives. The second level identifies inter-robot collisions and resolves them by imposing constraints on the first level. The third and final level uses the resulting solution with discontinuities as an initial guess for a joint space trajectory optimization. The procedure is repeated with a reduced discontinuity bound. Our approach is anytime, probabilistically complete, asymptotically optimal, and finds near-optimal solutions quickly. Experimental results with robot dynamics such as unicycle, double integrator, and car with trailer in different settings show that our method is capable of solving challenging tasks with a higher success rate and lower cost than the existing state-of-the-art.},
  archive   = {C_ICRA},
  author    = {Akmaral Moldagalieva and Joaquim Ortiz-Haro and Marc Toussaint and Wolfgang Hönig},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610999},
  month     = {5},
  pages     = {14569-14575},
  title     = {Db-CBS: Discontinuity-bounded conflict-based search for multi-robot kinodynamic motion planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conflict-based model predictive control for scalable
multi-robot motion planning. <em>ICRA</em>, 14562–14568. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a scalable multi-robot motion planning algorithm called Conflict-Based Model Predictive Control (CB-MPC). Inspired by Conflict-Based Search (CBS), the planner leverages a modified high-level conflict tree to efficiently resolve robot-robot conflicts in the continuous space, while reasoning about each agent’s kinematic and dynamic constraints and actuation limits using MPC as the low-level planner. We show that tracking high-level multi-robot plans with a vanilla MPC controller is insufficient, and results in unexpected collisions in tight navigation scenarios under realistic execution. Compared to other variations of multi-robot MPC like joint, prioritized, and distributed, we demonstrate that CB-MPC improves the executability and success rate, allows for closer robot-robot interactions, and scales better with higher numbers of robots without compromising the solution quality across a variety of environments.},
  archive   = {C_ICRA},
  author    = {Ardalan Tajbakhsh and Lorenz T. Biegler and Aaron M. Johnson},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611078},
  month     = {5},
  pages     = {14562-14568},
  title     = {Conflict-based model predictive control for scalable multi-robot motion planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AMSwarmX: Safe swarm coordination in CompleX environments
via implicit non-convex decomposition of the obstacle-free space.
<em>ICRA</em>, 14555–14561. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Quadrotor motion planning in complex environments leverage the concept of safe flight corridor (SFC) to facilitate static obstacle avoidance. Typically, SFCs are constructed through convex decomposition of the environment’s free space into cuboids, convex polyhedra, or spheres. However, such SFCs can be overly conservative when dealing with a quadrotor swarm, substantially limiting the available free space for quadrotors to coordinate. This paper presents an Alternating Minimization-based approach that does not require building a conservative free-space approximation. Instead, both static and dynamic collision constraints are treated in a unified manner. Dynamic collisions are handled based on shared position trajectories of the quadrotors. Static obstacle avoidance is coupled with distance queries from the Octomap, providing an implicit non-convex decomposition of free space. As a result, our approach is scalable to arbitrary complex environments. Through extensive comparisons in simulation, we demonstrate a 60% improvement in success rate, an average 1.8× reduction in mission completion time, and an average 23× reduction in per-agent computation time compared to SFC-based approaches. We also experimentally validated our approach using a Crazyflie quadrotor swarm of up to 12 quadrotors in obstacle-rich environments. The code, supplementary materials, and videos are released for reference.},
  archive   = {C_ICRA},
  author    = {Vivek K. Adajania and Siqi Zhou and Arun Kumar Singh and Angela P. Schoellig},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610428},
  month     = {5},
  pages     = {14555-14561},
  title     = {AMSwarmX: Safe swarm coordination in CompleX environments via implicit non-convex decomposition of the obstacle-free space},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conflict area prediction for boosting search-based
multi-agent pathfinding algorithms. <em>ICRA</em>, 14548–14554. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the challenge of efficiently controlling multi-agent systems, crucial in fields like logistics and traffic management. We propose a novel approach that combines learning-based techniques with search-based methods, focusing on enhancing the conflict-based search (CBS). The CBS ensures optimality but suffers from increasing complexity as agents or maps grow. To tackle this, we leverage learning-based approaches to enhance computational efficiency. By training a conflict area prediction (CAP) network, we anticipate potential conflict areas, allowing for low-level path planners to explore conflict-free paths. Our experiments demonstrate the effectiveness of our method in reducing computational demands compared to existing approaches.},
  archive   = {C_ICRA},
  author    = {Jaesung Ryu and Youngjoon Kwon and Sangho Yoon and Kyungjae Lee},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610843},
  month     = {5},
  pages     = {14548-14554},
  title     = {Conflict area prediction for boosting search-based multi-agent pathfinding algorithms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Benchmarking multi-robot coordination in realistic,
unstructured human-shared environments. <em>ICRA</em>, 14541–14547. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Coordinating a fleet of robots in unstructured, human-shared environments is challenging. Human behavior is hard to predict, and its uncertainty impacts the performance of the robotic fleet. Various multi-robot planning and coordination algorithms have been proposed, including Multi-Agent Path Finding (MAPF) methods to precedence-based algorithms. However, it is still unclear how human presence impacts different coordination strategies in both simulated environments and the real world. With the goal of studying and further improving multi-robot planning capabilities in those settings, we propose a method to develop and benchmark different multi-robot coordination algorithms in realistic, unstructured and human-shared environments. To this end, we introduce a multi-robot benchmark framework that is based on state-of-the-art open-source navigation and simulation frameworks and can use different types of robots, environments and human motion models. We show a possible application of the benchmark framework with two different environments and three centralized coordination methods (two MAPF algorithms and a loosely-coupled coordination method based on precedence constraints). We evaluate each environment for different human densities to investigate its impact on each coordination method. We also present preliminary results that show how informing each coordination method about human presence can help the coordination method to find faster paths for the robots.},
  archive   = {C_ICRA},
  author    = {Lukas Heuer and Luigi Palmieri and Anna Mannucci and Sven Koenig and Martin Magnusson},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611005},
  month     = {5},
  pages     = {14541-14547},
  title     = {Benchmarking multi-robot coordination in realistic, unstructured human-shared environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EWand: An extrinsic calibration framework for wide baseline
frame-based and event-based camera systems. <em>ICRA</em>, 14534–14540.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate calibration is crucial for using multiple cameras to triangulate the position of objects precisely. However, it is also a time-consuming process that needs to be repeated for every displacement of the cameras. The standard approach is to use a printed pattern with known geometry to estimate the intrinsic and extrinsic parameters of the cameras. The same idea can be applied to event-based cameras, though it requires extra work. By using frame reconstruction from events, a printed pattern can be detected. A blinking pattern can also be displayed on a screen. Then, the pattern can be directly detected from the events. Such calibration methods can provide accurate intrinsic calibration for both frame- and event-based cameras. However, using 2D patterns has several limitations for multi-camera extrinsic calibration, with cameras possessing highly different points of view and a wide baseline. The 2D pattern can only be detected from one direction and needs to be of significant size to compensate for its distance to the camera. This makes the extrinsic calibration time-consuming and cumbersome. To overcome these limitations, we propose eWand, a new method that uses blinking LEDs inside opaque spheres instead of a printed or displayed pattern. Our method provides a faster, easier-to-use extrinsic calibration approach that maintains high accuracy for both event- and frame-based cameras.},
  archive   = {C_ICRA},
  author    = {Thomas Gossard and Andreas Ziegler and Levin Kolmar and Jonas Tebbe and Andreas Zell},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610116},
  month     = {5},
  pages     = {14534-14540},
  title     = {EWand: An extrinsic calibration framework for wide baseline frame-based and event-based camera systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SGCalib: A two-stage camera-LiDAR calibration method using
semantic information and geometric features. <em>ICRA</em>, 14527–14533.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Extrinsic calibration is an essential prerequisite for the applications of camera-LiDAR fusion. Existing methods either suffer from the complex offline setting of man-made targets or tend to produce suboptimal and unrobust results. In this paper, we propose an online two-stage calibration method that estimates robust and accurate extrinsic parameters between camera and LiDAR. This is a novel work to use semantic information and geometric features jointly in calibration to promote accuracy and robustness. In the first stage, we detect objects in the image and point cloud and build graphs on the objects using Delaunay triangulation. Then, we design a novel graph matching algorithm to associate the objects in the two data domains and extract pairs of 2D-3D points. Using the PnP solver, we get robust initial extrinsic parameters. Then, in the second stage, we design a new optimization formulation with semantic information and geometric features to generate accurate extrinsic parameters with the initial value from the first stage. Extensive experiments on solid-state LiDAR, conventional spinning LiDAR and KITTI datasets have verified the robustness and accuracy of our method which outperforms existing works. We will share the code publicly to benefit the community (after review stages).},
  archive   = {C_ICRA},
  author    = {Zhipeng Lin and Zhi Gao and Xinyi Liu and Jialiang Wang and Weiwei Song and Ben M. Chen and Chenyang Li and Yue Huang and Yuhan Zhu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610560},
  month     = {5},
  pages     = {14527-14533},
  title     = {SGCalib: A two-stage camera-LiDAR calibration method using semantic information and geometric features},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An extrinsic calibration method between LiDAR and GNSS/INS
for autonomous driving. <em>ICRA</em>, 14520–14526. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate and reliable sensor calibration is critical for fusing LiDAR and inertial measurements in autonomous driving. This paper proposes a novel three-stage extrinsic calibration method between LiDAR and GNSS/INS for autonomous driving. The first stage can quickly calibrate the extrinsic parameters between the sensors through point cloud surface features so that the extrinsic can be narrowed from a large initial error to a small error range in little time. The second stage can further calibrate the extrinsic parameters based on LiDAR-mapping space occupancy while removing motion distortion. In the final stage, the z-axis (the vertical direction relative to the ground plane) errors caused by the plane motion of the autonomous vehicle are corrected, and an accurate extrinsic parameter is finally obtained. Specifically, This method utilizes the planar features in the environment, making it possible to quickly carry out calibration. Experimental results on real-world datasets demonstrate the reliability and accuracy of our method. The codes are open-sourced on the Github website. The code link is https://github.com/OpenCalib/LiDAR2INS.},
  archive   = {C_ICRA},
  author    = {Jiahao Pi and Guohang Yan and Chengjie Wang and Xinyu Cai and Botian Shi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610541},
  month     = {5},
  pages     = {14520-14526},
  title     = {An extrinsic calibration method between LiDAR and GNSS/INS for autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). A novel, efficient and accurate method for lidar camera
calibration. <em>ICRA</em>, 14513–14519. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As autonomous systems evolve, the precise calibration of lidar and camera sensors remains a pivotal concern. Among the myriad of available techniques, target-based calibration methods, which employ planar boards with distinct geometry and image patterns, have been a popular choice. These methods simplify the task of extracting corresponding features between the image and lidar point cloud. But many of these approaches also face a significant challenge, which is their sensitivity to lidar resolution and Field of View (FOV), which may degrade the reliability of the calibration results. Therefore, our research introduces a novel calibration method using a uniquely designed acrylic checkerboard which allows the lidar beam to pass through the white grids and reflect back from the black grids. This innovative technique sidesteps the common challenges associated with lidar feature extraction. Our method’s distinct advantage lies in its ability to perform accurate calibrations at close distances, owing to the efficient feature extraction from both lidar and camera sensors. This novel, efficient, and accurate method can provide state-of-the-art results for camera lidar calibration in the field. Please also check our Github repository: https://github.com/WPI-APA-Lab/Acrylic-Board-Lidar-Camera-Calibration},
  archive   = {C_ICRA},
  author    = {Zhanhong Huang and Xiao Zhang and Antony Garcia and Xinming Huang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611162},
  month     = {5},
  pages     = {14513-14519},
  title     = {A novel, efficient and accurate method for lidar camera calibration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PeLiCal: Targetless extrinsic calibration via penetrating
lines for RGB-d cameras with limited co-visibility. <em>ICRA</em>,
14506–14512. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {RGB-D cameras are crucial in robotic perception, given their ability to produce images augmented with depth data. However, their limited field of view (FOV) often requires multiple cameras to cover a broader area. In multi-camera RGB-D setups, the goal is typically to reduce camera overlap, optimizing spatial coverage with as few cameras as possible. The extrinsic calibration of these systems introduces additional complexities. Existing methods for extrinsic calibration either necessitate specific tools or highly depend on the accuracy of camera motion estimation. To address these issues, we present PeLiCal, a novel line-based calibration approach for RGB-D camera systems exhibiting limited overlap. Our method leverages long line features from surroundings, and filters out outliers with a novel convergence voting algorithm, achieving targetless, real-time, and outlier-robust performance compared to existing methods. We open source our implementation on https://github.com/joomeok/PeLiCal.git.},
  archive   = {C_ICRA},
  author    = {Jaeho Shin and Seungsang Yun and Ayoung Kim},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611415},
  month     = {5},
  pages     = {14506-14512},
  title     = {PeLiCal: Targetless extrinsic calibration via penetrating lines for RGB-D cameras with limited co-visibility},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dive deeper into rectifying homography for stereo camera
online self-calibration. <em>ICRA</em>, 14479–14485. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate estimation of stereo camera extrinsic parameters is crucial to guarantee the performance of stereo matching algorithms. In prior arts, the online self-calibration of stereo cameras has commonly been formulated as a specialized visual odometry problem, without taking into account the principles of stereo rectification. In this paper, we first delve deeply into the concept of rectifying homography, which serves as the cornerstone for the development of our novel stereo camera online self-calibration algorithm, for cases where only a single pair of images is available. Furthermore, we introduce a simple yet effective solution for global optimum extrinsic parameter estimation in the presence of stereo video sequences. Additionally, we emphasize the impracticality of using three Euler angles and three components in the translation vectors for performance quantification. Instead, we introduce four new evaluation metrics to quantify the robustness and accuracy of extrinsic parameter estimation, applicable to both single-pair and multi-pair cases. Extensive experiments conducted across indoor and outdoor environments using various experimental setups validate the effectiveness of our proposed algorithm. The comprehensive evaluation results demonstrate its superior performance in comparison to the baseline algorithm. Our source code, demo video, and supplement are publicly available at mias.group/StereoCalibrator.},
  archive   = {C_ICRA},
  author    = {Hongbo Zhao and Yikang Zhang and Qijun Chen and Rui Fan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611521},
  month     = {5},
  pages     = {14479-14485},
  title     = {Dive deeper into rectifying homography for stereo camera online self-calibration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Zero-training LiDAR-camera extrinsic calibration method
using segment anything model. <em>ICRA</em>, 14472–14478. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Extrinsic calibration for LiDAR and camera is an essential prerequisite for sensor fusion. Recently, automatic and target-less extrinsic calibration has become the mainstream of academic research. However, geometric feature-based methods still have requirements on the scene. Deep learning methods, while achieving high accuracy and good adaptability, rely on large annotated dataset and need additional training. We propose a novel LiDAR-camera calibration method by using the Segment Anything Model(SAM) without additional training. With the automatically generated masks, we optimize the extrinsic parameters by maximizing the consistency score of the point attributes that fall on each mask. The point cloud attributes include intensity, normal vector and segmentation class. Experiments on different real-world dataset demonstrate the accuracy and robustness of our proposed method. The code is available at https://github.com/OpenCalib/CalibAnything.},
  archive   = {C_ICRA},
  author    = {Zhaotong Luo and Guohang Yan and Xinyu Cai and Botian Shi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610983},
  month     = {5},
  pages     = {14472-14478},
  title     = {Zero-training LiDAR-camera extrinsic calibration method using segment anything model},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FIMP: Future interaction modeling for multi-agent motion
prediction. <em>ICRA</em>, 14457–14463. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent motion prediction is a crucial concern in autonomous driving, yet it remains a challenge owing to the ambiguous intentions of dynamic agents and their intricate interactions. Existing studies have attempted to capture interactions between road entities by using the definite data in history timesteps, as future information is not available and involves high uncertainty. However, without sufficient guidance for capturing future states of interacting agents, they frequently produce unrealistic trajectory overlaps. In this work, we propose Future Interaction modeling for Motion Prediction (FIMP), which captures potential future interactions in an end-to-end manner. FIMP adopts a future decoder that implicitly extracts the potential future information in an intermediate feature-level, and identifies the interacting entity pairs through future affinity learning and top-k filtering strategy. Experiments show that our future interaction modeling improves the performance remarkably, leading to superior performance on the Argoverse motion forecasting benchmark.},
  archive   = {C_ICRA},
  author    = {Sungmin Woo and Minjung Kim and Donghyeong Kim and Sungjun Jang and Sangyoun Lee},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611080},
  month     = {5},
  pages     = {14457-14463},
  title     = {FIMP: Future interaction modeling for multi-agent motion prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Improving autonomous driving safety with POP: A framework
for accurate partially observed trajectory predictions. <em>ICRA</em>,
14450–14456. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate trajectory prediction is crucial for safe and efficient autonomous driving, but handling partial observations presents significant challenges. To address this, we propose a novel trajectory prediction framework called Partial Observations Prediction (POP) for congested urban road scenarios. The framework consists of two key stages: self-supervised learning (SSL) and feature distillation. POP first employs SLL to help the model learn to reconstruct history representations, and then utilizes feature distillation as the fine-tuning task to transfer knowledge from the teacher model, which has been pre-trained with complete observations, to the student model, which has only few observations. POP achieves comparable results to topperforming methods in open-loop experiments and outperforms the baseline method in closed-loop simulations, including safety metrics. Qualitative results illustrate the superiority of POP in providing reasonable and safe trajectory predictions. Demo videos and code are available at https://chantsss.github.io/POP/.},
  archive   = {C_ICRA},
  author    = {Sheng Wang and Yingbing Chen and Jie Cheng and Xiaodong Mei and Ren Xin and Yongkang Song and Ming Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610154},
  month     = {5},
  pages     = {14450-14456},
  title     = {Improving autonomous driving safety with POP: A framework for accurate partially observed trajectory predictions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). InterCoop: Spatio-temporal interaction aware cooperative
perception for networked vehicles. <em>ICRA</em>, 14443–14449. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In autonomous driving, cooperative perception through vehicle-to-vehicle (V2V) communication is considered crucial for enhancing traffic safety and efficiency. However, existing methods often simplify the handling of perception data from multiple vehicles. In these approaches, the egovehicle aggregates observations from all neighboring connected cooperative vehicles (CCV), without considering the interactions between the vehicles or making differentiated use of the acquired sensing data. This approach can result in suboptimal performance due to the increase of noise and large transmission delay. In this paper, we introduce a novel approach to cooperative perception. By fusing both the road topology and trajectory histories of neighboring CCVs, our model learns an interaction score for each CCV. These scores prioritize vehicles that are most relevant to the current driving scenario, offering valuable guidance for selective fusion of sensor data, thereby enhancing driving decision-making. The proposed method is validated through experiments conducted on the CARLA simulator. Results demonstrate that our approach surpasses existing methods in terms of performance and robustness.},
  archive   = {C_ICRA},
  author    = {Wentao Wang and Haoran Xu and Guang Tan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610188},
  month     = {5},
  pages     = {14443-14449},
  title     = {InterCoop: Spatio-temporal interaction aware cooperative perception for networked vehicles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Plug in the safety chip: Enforcing constraints for
LLM-driven robot agents. <em>ICRA</em>, 14435–14442. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advancements in large language models (LLMs) have enabled a new research domain, LLM agents, for solving robotics and planning tasks by leveraging the world knowledge and general reasoning abilities of LLMs obtained during pretraining. However, while considerable effort has been made to teach the robot the &quot;dos&quot;, the &quot;don’ts&quot; received relatively less attention. We argue that, for any practical usage, it is as crucial to teach the robot the &quot;don’ts&quot;: conveying explicit instructions about prohibited actions, assessing the robot’s comprehension of these restrictions, and, most importantly, ensuring compliance. Moreover, verifiable safe operation is essential for deployments that satisfy worldwide standards such as ISO 61508, which defines standards for safely deploying robots in industrial factory environments worldwide. Aiming at deploying the LLM agents in a collaborative environment, we propose a queryable safety constraint module based on linear temporal logic (LTL) that simultaneously enables natural language (NL) to temporal constraints encoding, safety violation reasoning and explaining, and unsafe action pruning. To demonstrate the effectiveness of our system, we conducted experiments in VirtualHome environment and on a real robot. The experimental results show that our system strictly adheres to the safety constraints and scales well with complex safety constraints, highlighting its potential for practical utility.},
  archive   = {C_ICRA},
  author    = {Ziyi Yang and Shreyas S. Raman and Ankit Shah and Stefanie Tellex},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611447},
  month     = {5},
  pages     = {14435-14442},
  title     = {Plug in the safety chip: Enforcing constraints for LLM-driven robot agents},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforcement learning with human feedback for realistic
traffic simulation. <em>ICRA</em>, 14428–14434. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In light of the challenges and costs of real-world testing, autonomous vehicle developers often rely on testing in simulation for the creation of reliable systems. A key element of effective simulation is the incorporation of realistic traffic models that align with human knowledge, an aspect that has proven challenging due to the need to balance realism and diversity. Towards this end, in this work we develop a framework that employs reinforcement learning from human feedback (RLHF) to enhance the realism of existing traffic models. This work also identifies two main challenges: capturing the nuances of human preferences on realism and unifying diverse traffic simulation models. To tackle these issues, we propose using human feedback for alignment and employ RLHF due to its sample efficiency. We also introduce the first dataset for realism alignment in traffic modeling to support such research. Our framework, named TrafficRLHF, demonstrates its proficiency in generating realistic traffic scenarios that are well-aligned with human preferences through comprehensive evaluations on the nuScenes dataset.},
  archive   = {C_ICRA},
  author    = {Yulong Cao and Boris Ivanovic and Chaowei Xiao and Marco Pavone},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610878},
  month     = {5},
  pages     = {14428-14434},
  title     = {Reinforcement learning with human feedback for realistic traffic simulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Risk-aware trajectory prediction by incorporating
spatio-temporal traffic interaction analysis. <em>ICRA</em>,
14421–14427. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To operate in open-ended environments where humans interact in complex, diverse ways, autonomous robots must learn to predict their behaviour, especially when that behavior is potentially dangerous to other agents or to the robot. However, reducing the risk of accidents requires prior knowledge of where potential collisions may occur and how. Therefore, we propose to gain this information by analyzing locations and speeds that commonly correspond to high-risk interactions within the dataset, and use it within training to generate better predictions in high risk situations. Through these location-based and speed-based re-weighting techniques, we achieve improved overall performance, as measured by most-likely FDE and KDE, as well as improved performance on high-speed vehicles, and vehicles within high-risk locations.},
  archive   = {C_ICRA},
  author    = {Divya Thuremella and Lewis Ince and Lars Kunze},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611023},
  month     = {5},
  pages     = {14421-14427},
  title     = {Risk-aware trajectory prediction by incorporating spatio-temporal traffic interaction analysis},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Safety-critical scenario generation via reinforcement
learning based editing. <em>ICRA</em>, 14405–14412. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generating safety-critical scenarios is essential for testing and verifying the safety of autonomous vehicles. Traditional optimization techniques suffer from the curse of dimensionality and limit the search space to fixed parameter spaces. To address these challenges, we propose a deep reinforcement learning approach that generates scenarios by sequential editing, such as adding new agents or modifying the trajectories of the existing agents. Our framework employs a reward function consisting of both risk and plausibility objectives. The plausibility objective leverages generative models, such as a variational autoencoder, to learn the likelihood of the generated parameters from the training datasets; It penalizes the generation of unlikely scenarios. Our approach overcomes the dimensionality challenge and explores a wide range of safety-critical scenarios. Our evaluation demonstrates that the proposed method generates safety-critical scenarios of higher quality compared with previous approaches.},
  archive   = {C_ICRA},
  author    = {Haolan Liu and Liangjun Zhang and Siva Kumar Sastry Hari and Jishen Zhao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611555},
  month     = {5},
  pages     = {14405-14412},
  title     = {Safety-critical scenario generation via reinforcement learning based editing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Risk-inspired aerial active exploration for enhancing
autonomous driving of UGV in unknown off-road environments.
<em>ICRA</em>, 14390–14396. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unknown area exploration is a crucial but challenging task for autonomous driving of unmanned ground vehicles (UGV) in unknown off-road environments. However, the exploration efficiency of a single UGV is low due to its limited sensing range. To solve this problem, this paper proposes a risk-inspired aerial active exploration system, which utilizes the flexibility and field of view advantages of Unmanned Aerial Vehicles (UAV) to guide the UGV in unknown off-road environments. Firstly, a fast terrain risk mapping method that can be used for both UAV and UGV is developed. This method efficiently combines quadtree and hash table data structure to enable UAV to analyze large scale terrain point cloud in real time. Based on the risk mapping result, a risk-inspired active exploration method is proposed to actively search a safe reference path for the UGV, which introduces terrain risk information into the process of travel point selection. Finally, the reference path is gradually generated and optimized, so that the UGV can safely and smoothly follow the path to the target location. Compared with single UGV exploration system, our approach reduces the overall path risk by 26.8% in simulated experiments, showing that the proposed system can enhance autonomous driving of the UGV and help it effectively avoid high-risk areas in unknown off-road environments.},
  archive   = {C_ICRA},
  author    = {Rongchuan Wang and Mengyin Fu and Jing Yu and Yi Yang and Wenjie Song},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610352},
  month     = {5},
  pages     = {14390-14396},
  title     = {Risk-inspired aerial active exploration for enhancing autonomous driving of UGV in unknown off-road environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Zero-shot constrained motion planning transformers using
learned sampling dictionaries. <em>ICRA</em>, 14363–14369. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Constrained robot motion planning is a ubiquitous need for robots interacting with everyday environments, but it is a notoriously difficult problem to solve. Many sampled points in a sample-based planner need to be rejected as they fall outside the constraint manifold, or require significant iterative effort to correct. Given this, few solutions exist that present a constraint-satisfying trajectory for robots, in reasonable time and of low path cost. In this work, we present a transformer-based model for motion planning with task space constraints for manipulation systems. Vector Quantized-Motion Planning Transformer (VQ-MPT) is a recent learning-based model that reduces the search space for unconstrained planning for sampling-based motion planners. We propose to adapt a pre-trained VQMPT model to reduce the search space for constraint planning without retraining or finetuning the model. We also propose to update the neural network output to move sampling regions closer to the constraint manifold. Our experiments show how VQ-MPT improves planning times and accuracy compared to traditional planners in simulated and real-world environments. Unlike previous learning methods, which require task-related data, our method uses pre-trained neural network models and requires no additional data for training and finetuning the model making this a one-shot process. We also tested our method on a physical Franka Panda robot with real-world sensor data, demonstrating the generalizability of our algorithm. We anticipate this approach to be an accessible and broadly useful for transfering learned neural planners to various robotic-environment interaction scenarios.},
  archive   = {C_ICRA},
  author    = {Jacob J. Johnson and Ahmed H. Qureshi and Michael C. Yip},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611398},
  month     = {5},
  pages     = {14363-14369},
  title     = {Zero-shot constrained motion planning transformers using learned sampling dictionaries},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Planning optimal trajectories for mobile manipulators under
end-effector trajectory continuity constraint. <em>ICRA</em>,
14356–14362. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile manipulators have been employed in many applications that are traditionally performed by either multiple fixed-base robots or a large robotic system. This capability is enabled by the mobility of the mobile base. However, the mobile base also brings redundancy to the system, which makes mobile manipulator motion planning more challenging. In this paper, we tackle the mobile manipulator motion planning problem under the end-effector trajectory continuity constraint in which the end-effector is required to traverse a continuous task-space trajectory (time-parametrized path), such as in mobile printing or spraying applications. Our method decouples the problem into: (1) planning an optimal base trajectory subject to geometric task constraints, end-effector trajectory continuity constraint, collision avoidance, and base velocity constraint; which ensures that (2) a manipulator trajectory is computed subsequently based on the obtained base trajectory. To validate our method, we propose a discrete optimal base trajectory planning algorithm to solve several mobile printing tasks in hardware experiment and simulations.},
  archive   = {C_ICRA},
  author    = {Quang-Nam Nguyen and Quang-Cuong Pham},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611630},
  month     = {5},
  pages     = {14356-14362},
  title     = {Planning optimal trajectories for mobile manipulators under end-effector trajectory continuity constraint},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient motion planning for manipulators with control
barrier function-induced neural controller. <em>ICRA</em>, 14348–14355.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sampling-based motion planning methods for manipulators in crowded environments often suffer from expensive collision checking and high sampling complexity, which make them difficult to use in real time. To address this issue, we propose a new generalizable control barrier function (CBF)based steering controller to reduce the number of samples needed in a sampling-based motion planner RRT. Our method combines the strength of CBF for real-time collision-avoidance control and RRT for long-horizon motion planning, by using CBF-induced neural controller (CBF-INC) to generate control signals that steer the system towards sampled configurations by RRT. CBF-INC is learned as Neural Networks and has two variants handling different inputs, respectively: state (signed distance) input and point-cloud input from LiDAR. In the latter case, we also study two different settings: fully and partially observed environmental information. Compared to manually crafted CBF which suffers from over-approximating robot geometry, CBF-INC can balance safety and goal-reaching better without being over-conservative. Given state-based input, our neural CBF-induced neural controller-enhanced RRT (CBFINC-RRT) can increase the success rate by 14% while reducing the number of nodes explored by 30%, compared with vanilla RRT on hard test cases. Given LiDAR input where vanilla RRT is not directly applicable, we demonstrate that our CBF-INCRRT can improve the success rate by 10%, compared with planning with other steering controllers. Our project page with supplementary material is at https://mit-realm.github.io/CBFINC-RRT-website/.},
  archive   = {C_ICRA},
  author    = {Mingxin Yu and Chenning Yu and M-Mahdi Naddaf-Sh and Devesh Upadhyay and Sicun Gao and Chuchu Fan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610785},
  month     = {5},
  pages     = {14348-14355},
  title     = {Efficient motion planning for manipulators with control barrier function-induced neural controller},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Online trajectory deformation and tracking for
self-entanglement-free differential-driven robots. <em>ICRA</em>,
14341–14347. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces an optimisation-based trajectory deformation and tracking algorithm for tethered differential-driven mobile robots. The motivation of this work is to generate self-entanglement-free (SEF) commands for a tethered differential-driven robot to track a path. Whilst existing path planners have been capable of generating SEF paths for tethered differential-driven robots lacking an omni-directional tether retracting mechanism, no trajectory planner can handle the unavoidable movement errors that cause robot pose deviate from the pre-defined path. The trajectory deformation and tracking is challenging because the admissible heading direction of the robot is highly constrained by the SEF constraint. As a result, even with an SEF path, the robot still encounters self-entanglement issues during execution.This paper fills this gap by formulating the trajectory deforming and tracking (TDT) problem of a tethered robot into a multi-objective optimisation framework. Explicit consideration of the constraint of the relative angle between the tether stretching direction and the robot’s heading direction to be admissible during its movement is provided in this framework. The proposed algorithm repeatedly deforms the pre-defined path for easier tracking, whilst generating a suitable velocity profile for robot execution. Compared to directly applying the commonly used untethered trajectory deformation and tracking algorithm into tethered cases, the proposed algorithm demonstrates improved performance in terms of minimising the risk of self-entanglement and maximising robot safety. These are validated in both simulated and real scenarios. An open-sourcesourcing implementation has also been provided for the benefit of the robotics community.},
  archive   = {C_ICRA},
  author    = {Jiangpin Liu and Tong Yang and Wangtao Lu and Yue Wang and Rong Xiong},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611150},
  month     = {5},
  pages     = {14341-14347},
  title     = {Online trajectory deformation and tracking for self-entanglement-free differential-driven robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tight motion planning by riemannian optimization for sliding
and rolling with finite number of contact points. <em>ICRA</em>,
14333–14340. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address a challenging problem in motion planning where robots must navigate through narrow passages in their configuration space. Our novel approach leverages optimization techniques to facilitate sliding and rolling movements across critical regions, which represent semi-free configurations, where the robot and the obstacles are in contact. Our algorithm seamlessly traverses widely free regions, follows semi-free paths in narrow passages, and smoothly transitions between the two types. We specifically focus on scenarios resembling 3D puzzles, intentionally designed to be complex for humans by requiring intricate simultaneous translations and rotations. Remarkably, these complexities also present computational challenges. Our contributions are threefold: First, we solve previously unsolved problems; second, we outperform state-of-the-art algorithms on certain problem types; and third, we present a rigorous analysis supporting the consistency of the algorithm. In the Supplementary Material we provide theoretical foundations for our approach. The Supplementary Material and our open source software are available at https://github.com/TAU-CGL/tr-rrt-public. This research sheds light on effective approaches to address motion planning difficulties in intricate 3D puzzle-like scenarios.},
  archive   = {C_ICRA},
  author    = {Dror Livnat and Michael M. Bilevich and Dan Halperin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611716},
  month     = {5},
  pages     = {14333-14340},
  title     = {Tight motion planning by riemannian optimization for sliding and rolling with finite number of contact points},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Eclares: Energy-aware clarity-driven ergodic search.
<em>ICRA</em>, 14326–14332. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Planning informative trajectories while considering the spatial distribution of the information over the environment, as well as constraints such as the robot’s limited battery capacity, makes the long-time horizon persistent coverage problem complex. Ergodic search methods consider the spatial distribution of environmental information while optimizing robot trajectories; however, current methods lack the ability to construct the target information spatial distribution for environments that vary stochastically across space and time. Moreover, current coverage methods dealing with battery capacity constraints either assume simple robot and battery models or are computationally expensive. To address these problems, we propose a framework called Eclares, in which our contribution is two-fold. 1) First, we propose a method to construct the target information spatial distribution for ergodic trajectory optimization using clarity, an information measure bounded between [0, 1]. The clarity dynamics allow us to capture information decay due to a lack of measurements and to quantify the maximum attainable information in stochastic spatiotemporal environments. 2) Second, instead of directly tracking the ergodic trajectory, we introduce the energy-aware (eware) filter, which iteratively validates the ergodic trajectory to ensure that the robot has enough energy to return to the charging station when needed. The proposed eware filter is applicable to nonlinear robot models and is computationally lightweight. We demonstrate the working of the framework through a simulation case study. [Code] a [Video] b},
  archive   = {C_ICRA},
  author    = {Kaleb Ben Naveed and Devansh Agrawal and Christopher Vermillion and Dimitra Panagou},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611286},
  month     = {5},
  pages     = {14326-14332},
  title     = {Eclares: Energy-aware clarity-driven ergodic search},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Characterizing physical adversarial attacks on robot motion
planners. <em>ICRA</em>, 14319–14325. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As the adoption of robots across society increases, so does the importance of considering cybersecurity issues such as vulnerability to adversarial attacks. In this paper we investigate the vulnerability of an important component of autonomous robots to adversarial attacks—robot motion planning algorithms. We particularly focus on attacks on the physical environment, and propose the first such attacks to motion planners: “planner failure” and “blindspot” attacks. Planner failure attacks make changes to the physical environment so as to make planners fail to find a solution. Blindspot attacks exploit occlusions and sensor field-of-view to make planners return a trajectory which is thought to be collision-free, but is actually in collision with unperceived parts of the environment. Our experimental results show that successful attacks need only to make subtle changes to the real world, in order to obtain a drastic increase in failure rates and collision rates—leading the planner to fail 95% of the time and collide 90% of the time in problems generated with an existing planner benchmark tool. We also analyze the transferability of attacks to different planners, and discuss underlying assumptions and future research directions. Overall, the paper shows that physical adversarial attacks on motion planning algorithms pose a serious threat to robotics, which should be taken into account in future research and development.},
  archive   = {C_ICRA},
  author    = {Wenxi Wu and Fabio Pierazzi and Yali Du and Martim Brandão},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610344},
  month     = {5},
  pages     = {14319-14325},
  title     = {Characterizing physical adversarial attacks on robot motion planners},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The virtues of laziness: Multi-query kinodynamic motion
planning with lazy methods. <em>ICRA</em>, 14286–14292. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we introduce LazyBoE, a multi-query method for kinodynamic motion planning with forward propagation. This algorithm allows for the simultaneous exploration of a robot’s state and control spaces, thereby enabling a wider suite of dynamic tasks in real-world applications. Our contributions are three-fold: i) a method for discretizing the state and control spaces to amortize planning times across multiple queries; ii) lazy approaches to collision checking and propagation of control sequences that decrease the cost of physics-based simulation; and iii) LazyBoE, a robust kinodynamic planner that leverages these two contributions to produce dynamically-feasible trajectories. The proposed framework not only reduces planning time but also increases success rate in comparison to previous approaches.},
  archive   = {C_ICRA},
  author    = {Anuj Pasricha and Alessandro Roncone},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611326},
  month     = {5},
  pages     = {14286-14292},
  title     = {The virtues of laziness: Multi-query kinodynamic motion planning with lazy methods},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Output-sampled model predictive path integral control
(o-MPPI) for increased efficiency. <em>ICRA</em>, 14279–14285. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The success of the model predictive path integral control (MPPI) approach depends on the appropriate selection of the input distribution used for sampling. However, it can be challenging to select inputs that satisfy output constraints in dynamic environments. The main contribution of this paper is to propose an output-sampling-based MPPI (o-MPPI), which improves the ability of samples to satisfy output constraints and thereby increases MPPI efficiency. Comparative simulations and experiments of dynamic autonomous driving of bots around a track are provided to show that the proposed o-MPPI is more efficient and requires substantially (20-times) less number of rollouts and (4-times) smaller prediction horizon when compared with the standard MPPI for similar success rates.},
  archive   = {C_ICRA},
  author    = {Leon Liangwu Yan and Santosh Devasia},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611180},
  month     = {5},
  pages     = {14279-14285},
  title     = {Output-sampled model predictive path integral control (o-MPPI) for increased efficiency},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Co-learning planning and control policies constrained by
differentiable logic specifications. <em>ICRA</em>, 14272–14278. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Synthesizing planning and control policies in robotics is a fundamental task, further complicated by factors such as complex logic specifications and high-dimensional robot dynamics. This paper presents a novel reinforcement learning approach to solving high-dimensional robot navigation tasks with complex logic specifications by co-learning planning and control policies. Notably, this approach significantly reduces the sample complexity in training, allowing us to train high-quality policies with much fewer samples compared to existing reinforcement learning algorithms. In addition, our methodology streamlines complex specification extraction from map images and enables the efficient generation of long-horizon robot motion paths across different map layouts. Moreover, our approach also demonstrates capabilities for high-dimensional control and avoiding suboptimal policies via policy alignment. The efficacy of our approach is demonstrated through experiments involving simulated high-dimensional quadruped robot dynamics and a real-world differential drive robot (TurtleBot3) under different types of task specifications.},
  archive   = {C_ICRA},
  author    = {Zikang Xiong and Daniel Lawson and Joe Eappen and Ahmed H. Qureshi and Suresh Jagannathan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610942},
  month     = {5},
  pages     = {14272-14278},
  title     = {Co-learning planning and control policies constrained by differentiable logic specifications},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning-aided warmstart of model predictive control in
uncertain fast-changing traffic. <em>ICRA</em>, 14265–14271. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Model Predictive Control lacks the ability to escape local minima in nonconvex problems. Furthermore, in fast-changing, uncertain environments, the conventional warmstart, using the optimal trajectory from the last timestep, often falls short of providing an adequately close initial guess for the current optimal trajectory. This can potentially result in convergence failures and safety issues. Therefore, this paper proposes a framework for learning-aided warmstarts of Model Predictive Control algorithms. Our method leverages a neural network based multimodal predictor to generate multiple trajectory proposals for the autonomous vehicle, which are further refined by a sampling-based technique. This combined approach enables us to identify multiple distinct local minima and provide an improved initial guess. We validate our approach with Monte Carlo simulations of traffic scenarios.},
  archive   = {C_ICRA},
  author    = {Mohamed-Khalil Bouzidi and Yue Yao and Daniel Goehring and Joerg Reichardt},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610472},
  month     = {5},
  pages     = {14265-14271},
  title     = {Learning-aided warmstart of model predictive control in uncertain fast-changing traffic},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robustified time-optimal collision-free motion planning for
autonomous mobile robots under disturbance conditions. <em>ICRA</em>,
14258–14264. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a robustified time-optimal motion planning approach for navigating an Autonomous Mobile Robot (AMR) from an initial state to a terminal state without colliding with obstacles, even when subjected to disturbances, which are modeled as random process noise and measurement noise. The approach iteratively solves the robustified problem by incorporating updated state-dependent safety margins for collision avoidance, the evolution of which is derived separately from the robustified problem. Additionally, a strategy for selecting an alternative terminal state to reach is introduced, which comes into play when the desired terminal state becomes infeasible considering the disturbances. Both of these contributions are integrated into a robustified motion planning and control pipeline, the efficacy of which is validated through simulation experiments.},
  archive   = {C_ICRA},
  author    = {Shuhao Zhang and Mathias Bos and Bastiaan Vandewal and Wilm Decré and Joris Gillis and Jan Swevers},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610134},
  month     = {5},
  pages     = {14258-14264},
  title     = {Robustified time-optimal collision-free motion planning for autonomous mobile robots under disturbance conditions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NPC: Neural predictive control for fuel-efficient autonomous
trucks. <em>ICRA</em>, 14251–14257. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fuel efficiency is a crucial aspect of long-distance cargo transportation by oil-powered trucks that economize on costs and decrease carbon emissions. Current predictive control methods depend on an accurate model of vehicle dynamics and engine, including weight, drag coefficient, and the Brake-specific Fuel Consumption (BSFC) map of the engine. We propose a pure data-driven method, Neural Predictive Control (NPC), which does not use any physical model for the vehicle. After training with over 20,000 km of historical data, the novel proposed NVFormer implicitly models the relationship between vehicle dynamics, road slope, fuel consumption, and control commands using the attention mechanism. Based on the online sampled primitives from the past of the current freight trip and anchor-based future data synthesis, the NVFormer can infer optimal control command for reasonable fuel consumption. The physical model-free NPC outperforms the base PCC method with 2.41% and 3.45% more significant fuel saving in simulation and open-road highway testing, respectively.},
  archive   = {C_ICRA},
  author    = {Jiaping Ren and Jiahao Xiang and Hongfei Gao and Jinchuan Zhang and Yiming Ren and Yuexin Ma and Yi Wu and Ruigang Yang and Wei Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611146},
  month     = {5},
  pages     = {14251-14257},
  title     = {NPC: Neural predictive control for fuel-efficient autonomous trucks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Safe receding horizon motion planning with infinitesimal
update interval. <em>ICRA</em>, 14244–14250. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safety verification in motion planning is known to be computationally burdensome, despite its importance in robotics. In this paper, we investigate the behavior of safe receding horizon motion planners when the update interval becomes infinitesimal. By requiring the trajectory parameters to evolve continuously in time, the trajectory optimization problem is reformulated into a time-derivative form, whose decision variables are their rate of change. This results in a quadratic programming problem which directly provides safe input, and can be regarded as a real-time safety filter. The input expressivity is also enhanced by leveraging the differentiable structure of the parameter space. The proposed safety filter is experimentally validated using a wheeled ground robot in obstacle-cluttered environments. The result shows that the safety filter is capable of generating safe inputs in real-time, while addressing hundreds of constraints simultaneously.},
  archive   = {C_ICRA},
  author    = {Inkyu Jang and Sunwoo Hwang and Jeonghyun Byun and H. Jin Kim},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610158},
  month     = {5},
  pages     = {14244-14250},
  title     = {Safe receding horizon motion planning with infinitesimal update interval},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). QuAD: Query-based interpretable neural motion planning for
autonomous driving. <em>ICRA</em>, 14236–14243. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A self-driving vehicle must understand its environment to determine the appropriate action. Traditional autonomy systems rely on object detection to find the agents in the scene. However, object detection assumes a discrete set of objects and loses information about uncertainty, so any errors compound when predicting the future behavior of those agents. Alternatively, dense occupancy grid maps have been utilized to understand free-space. However, predicting a grid for the entire scene is wasteful since only certain spatio-temporal regions are reachable and relevant to the self-driving vehicle. We present a unified, interpretable, and efficient autonomy framework that moves away from cascading modules that first perceive, then predict, and finally plan. Instead, we shift the paradigm to have the planner query occupancy at relevant spatio-temporal points, restricting the computation to those regions of interest. Exploiting this representation, we evaluate a candidate trajectory around key factors such as collision avoidance, comfort, and progress for safety and interpretability. Our approach achieves better highway driving quality than the state-of-the-art on high-fidelity closed-loop simulations.},
  archive   = {C_ICRA},
  author    = {Sourav Biswas and Sergio Casas and Quinlan Sykora and Ben Agro and Abbas Sadat and Raquel Urtasun},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610648},
  month     = {5},
  pages     = {14236-14243},
  title     = {QuAD: Query-based interpretable neural motion planning for autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human observation-inspired trajectory prediction for
autonomous driving in mixed-autonomy traffic environments.
<em>ICRA</em>, 14212–14219. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the burgeoning field of autonomous vehicles (AVs), trajectory prediction remains a formidable challenge, especially in mixed autonomy environments. Traditional approaches often rely on computational methods such as time-series analysis. Our research diverges significantly by adopting an interdisciplinary approach that integrates principles of human cognition and observational behavior into trajectory prediction models for AVs. We introduce a novel “adaptive visual sector” mechanism that mimics the dynamic allocation of attention human drivers exhibit based on factors like spatial orientation, proximity, and driving speed. Additionally, we develop a “dynamic traffic graph” using Convolutional Neural Networks (CNN) and Graph Attention Networks (GAT) to capture spatio-temporal dependencies among agents. Benchmark tests on the NGSIM, HighD, and MoCAD datasets reveal that our model (GAVA) outperforms state-of-the-art baselines by at least 15.2%, 19.4%, and 12.0%, respectively. Our findings underscore the potential of leveraging human cognition principles to enhance the proficiency and adaptability of trajectory prediction algorithms in AVs.},
  archive   = {C_ICRA},
  author    = {Haicheng Liao and Shangqian Liu and Yongkang Li and Zhenning Li and Chengyue Wang and Yunjian Li and Shengbo Eben Li and Chengzhong Xu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611104},
  month     = {5},
  pages     = {14212-14219},
  title     = {Human observation-inspired trajectory prediction for autonomous driving in mixed-autonomy traffic environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). Chance-aware lane change with high-level model predictive
control through curriculum reinforcement learning. <em>ICRA</em>,
14205–14211. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Lane change in dense traffic typically requires the recognition of an appropriate opportunity for maneuvers, which remains a challenging problem in self-driving. In this work, we propose a chance-aware lane-change strategy with high-level model predictive control (MPC) through curriculum reinforcement learning (CRL). In our proposed framework, full-state references and regulatory factors concerning the relative importance of each cost term in the embodied MPC are generated by a neural policy. Furthermore, effective curricula are designed and integrated into an episodic reinforcement learning (RL) framework with policy transfer and enhancement, to improve the convergence speed and ensure a high-quality policy. The proposed framework is deployed and evaluated in numerical simulations of dense and dynamic traffic. It is noteworthy that, given a narrow chance, the proposed approach generates high-quality lane-change maneuvers such that the vehicle merges into the traffic flow with a high success rate of 96%. Finally, our framework is validated in the high-fidelity simulator under dense traffic, demonstrating satisfactory practicality and generalizability.},
  archive   = {C_ICRA},
  author    = {Yubin Wang and Yulin Li and Zengqi Peng and Hakim Ghazzai and Jun Ma},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610522},
  month     = {5},
  pages     = {14205-14211},
  title     = {Chance-aware lane change with high-level model predictive control through curriculum reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Informed reinforcement learning for situation-aware traffic
rule exceptions. <em>ICRA</em>, 14198–14204. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement Learning is a highly active research field with promising advancements. In the field of autonomous driving, however, often very simple scenarios are being examined. Common approaches use non-interpretable control commands as the action space and unstructured reward designs, which are unsuitable for complex scenarios. In this work, we introduce Informed Reinforcement Learning, where a structured rulebook is integrated as a knowledge source. We learn trajectories and asses them with a situation-aware reward design, leading to a dynamic reward that allows the agent to learn situations that require controlled traffic rule exceptions. Our method is applicable to arbitrary RL models. We successfully demonstrate high completion rates of complex scenarios with recent model-based agents.},
  archive   = {C_ICRA},
  author    = {Daniel Bogdoll and Jing Qin and Moritz Nekolla and Ahmed Abouelazm and Tim Joseph and J. Marius Zöllner},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610842},
  month     = {5},
  pages     = {14198-14204},
  title     = {Informed reinforcement learning for situation-aware traffic rule exceptions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active learning with dual model predictive path-integral
control for interaction-aware autonomous highway on-ramp merging.
<em>ICRA</em>, 14191–14197. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Merging into dense highway traffic for an autonomous vehicle is a complex decision-making task, wherein the vehicle must identify a potential gap and coordinate with surrounding human drivers, each of whom may exhibit diverse driving behaviors. Many existing methods consider other drivers to be dynamic obstacles and, as a result, they are incapable of capturing the full intent of the human drivers through this passive planning. In this paper, we propose a novel dual control framework based on Model Predictive Path-Integral control to generate interactive trajectories. This framework incorporates a Bayesian inference approach to actively learn the agents’ parameters, i.e., other drivers’ model parameters. The proposed framework employs a sampling-based approach that is suitable for real-time implementation through the utilization of GPUs. We illustrate the effectiveness of our proposed methodology through comprehensive numerical simulations conducted in both high and low-fidelity simulation scenarios focusing on autonomous on-ramp merging.},
  archive   = {C_ICRA},
  author    = {Jacob Knaup and Jovin D’sa and Behdad Chalaki and Tyler Naes and Hossein Nourkhiz Mahjoub and Ehsan Moradi-Pari and Panagiotis Tsiotras},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610405},
  month     = {5},
  pages     = {14191-14197},
  title     = {Active learning with dual model predictive path-integral control for interaction-aware autonomous highway on-ramp merging},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Optimal driver warning generation in dynamic driving
environment. <em>ICRA</em>, 14184–14190. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The driver warning system that alerts the human driver about potential risks during driving is a key feature of an advanced driver assistance system. Existing driver warning technologies, mainly the forward collision warning and unsafe lane change warning, can reduce the risk of collision caused by human errors. However, the current design methods have several major limitations. Firstly, the warnings are mainly generated in a one-shot manner without modeling the ego driver’s reactions and surrounding objects, which reduces the flexibility and generality of the system over different scenarios. Additionally, the triggering conditions of warning are mostly rule-based threshold-checking given the current state, which lacks the prediction of the potential risk in a sufficiently long future horizon. In this work, we study the problem of optimally generating driver warnings by considering the interactions among the generated warning, the driver behavior, and the states of ego and surrounding vehicles on a long horizon. The warning generation problem is formulated as a partially observed Markov decision process (POMDP). An optimal warning generation framework is proposed as a solution to the proposed POMDP. The simulation experiments demonstrate the superiority of the proposed solution to the existing warning generation methods.},
  archive   = {C_ICRA},
  author    = {Chenran Li and Aolin Xu and Enna Sachdeva and Teruhisa Misu and Behzad Dariush},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611250},
  month     = {5},
  pages     = {14184-14190},
  title     = {Optimal driver warning generation in dynamic driving environment},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vehicle behavior prediction by episodic-memory implanted
NDT. <em>ICRA</em>, 14177–14183. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In autonomous driving, predicting the behavior (turning left, stopping, etc.) of target vehicles is crucial for the self-driving vehicle to make safe decisions and avoid accidents. Existing deep learning-based methods have shown excellent and accurate performance, but the black-box nature makes it untrustworthy to apply them in practical use. In this work, we explore the interpretability of behavior prediction of target vehicles by an Episodic Memory implanted Neural Decision Tree (abbrev. eMem-NDT). The structure of eMem-NDT is constructed by hierarchically clustering the text embedding of vehicle behavior descriptions. eMem-NDT is a neural-backed part of a pre-trained deep learning model by changing the soft-max layer of the deep model to eMem-NDT, for grouping and aligning the memory prototypes of the historical vehicle behavior features in training data on a neural decision tree. Each leaf node of eMem-NDT is modeled by a neural network for aligning the behavior memory prototypes. By eMem-NDT, we infer each instance in behavior prediction of vehicles by bottom-up Memory Prototype Matching (MPM) (searching the appropriate leaf node and the links to the root node) and top-down Leaf Link Aggregation (LLA) (obtaining the probability of future behaviors of vehicles for certain instances). We validate eMem-NDT on BLVD and LOKI datasets, and the results show that our model can obtain a superior performance to other methods with clear explainability. The code is available in https://github.com/JWFangit/eMem-NDT.},
  archive   = {C_ICRA},
  author    = {Peining Shen and Jianwu Fang and Hongkai Yu and Jianru Xue},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610995},
  month     = {5},
  pages     = {14177-14183},
  title     = {Vehicle behavior prediction by episodic-memory implanted NDT},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time capable decision making for autonomous driving
using reachable sets. <em>ICRA</em>, 14169–14176. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite large advances in recent years, real-time capable motion planning for autonomous road vehicles remains a huge challenge. In this work, we present a decision module that is based on set-based reachability analysis: First, we identify all possible driving corridors by computing the reachable set for the longitudinal position of the vehicle along the lanelets of the road network, where lane changes are modeled as discrete events. Next, we select the best driving corridor based on a cost function that penalizes lane changes and deviations from a desired velocity profile. Finally, we generate a reference trajectory inside the selected driving corridor, which can be used to guide or warm start low-level trajectory planners. For the numerical evaluation we combine our decision module with a motion-primitive-based and an optimization-based planner and evaluate the performance on 2000 challenging CommonRoad traffic scenarios as well in the realistic CARLA simulator. The results demonstrate that our decision module is real-time capable and yields significant speed-ups compared to executing a motion planner standalone without a decision module.},
  archive   = {C_ICRA},
  author    = {Niklas Kochdumper and Stanley Bak},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610689},
  month     = {5},
  pages     = {14169-14176},
  title     = {Real-time capable decision making for autonomous driving using reachable sets},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Monocular localization with semantics map for autonomous
vehicles. <em>ICRA</em>, 14146–14152. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate and robust localization remains a significant challenge for autonomous vehicles. The cost of sensors and limitations in local computational efficiency make it difficult to scale to large commercial applications. Traditional vision-based approaches focus on texture features that are susceptible to changes in lighting, season, perspective, and appearance. Additionally, the large storage size of maps with descriptors and complex optimization processes hinder system performance. To balance efficiency and accuracy, we propose a novel lightweight visual semantic localization algorithm that employs stable semantic features instead of low-level texture features. First, semantic maps are constructed offline by detecting semantic objects, such as ground markers, lane lines, and poles, using cameras or LiDAR sensors. Then, online visual localization is performed through data association of semantic features and map objects. We evaluated our proposed localization framework in the publicly available KAIST Urban dataset and in scenarios recorded by ourselves. The experimental results demonstrate that our method is a reliable and practical localization solution in various autonomous driving localization tasks.},
  archive   = {C_ICRA},
  author    = {Jixiang Wan and Xudong Zhang and Shuzhou Dong and Yuwei Zhang and Yuchen Yang and Ruoxi Wu and Ye Jiang and Jijunnan Li and Jinquan Lin and Ming Yang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611430},
  month     = {5},
  pages     = {14146-14152},
  title     = {Monocular localization with semantics map for autonomous vehicles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scene informer: Anchor-based occlusion inference and
trajectory prediction in partially observable environments.
<em>ICRA</em>, 14138–14145. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Navigating complex and dynamic environments requires autonomous vehicles (AVs) to reason about both visible and occluded regions. This involves predicting the future motion of observed agents, inferring occluded ones, and modeling their interactions based on vectorized scene representations of the partially observable environment. However, prior work on occlusion inference and trajectory prediction have developed in isolation, with the former based on simplified rasterized methods and the latter assuming full environment observability. We introduce the Scene Informer, a unified approach for predicting both observed agent trajectories and inferring occlusions in a partially observable setting. It uses a transformer to aggregate various input modalities and facilitate selective queries on occlusions that might intersect with the AV’s planned path. The framework estimates occupancy probabilities and likely trajectories for occlusions, as well as forecast motion for observed agents. We explore common observability assumptions in both domains and their performance impact. Our approach outperforms existing methods in both occupancy prediction and trajectory prediction in partially observable setting on the Waymo Open Motion Dataset. Our implementation with additional visualizations is available at https://github.com/sisl/SceneInformer.},
  archive   = {C_ICRA},
  author    = {Bernard Lange and Jiachen Li and Mykel J. Kochenderfer},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611060},
  month     = {5},
  pages     = {14138-14145},
  title     = {Scene informer: Anchor-based occlusion inference and trajectory prediction in partially observable environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rethinking imitation-based planners for autonomous driving.
<em>ICRA</em>, 14123–14130. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, imitation-based driving planners have reported considerable success. However, due to the absence of a standardized benchmark, the effectiveness of various designs remains unclear. The newly released nuPlan addresses this issue by offering a large-scale real-world dataset and a standardized closed-loop benchmark for equitable comparisons. Utilizing this platform, we conduct a comprehensive study on two fundamental yet underexplored aspects of imitation-based planners: the essential features for ego planning and the effective data augmentation techniques to reduce compounding errors. Furthermore, we highlight an imitation gap that has been overlooked by current learning systems. Finally, integrating our findings, we propose a strong baseline model—PlanTF. Our results demonstrate that a well-designed, purely imitation-based planner can achieve highly competitive performance compared to state-of-the-art methods involving hand-crafted rules and exhibit superior generalization capabilities in long-tail cases. Our models and benchmarks are publicly available. Project website https://jchengai.github.io/planTF.},
  archive   = {C_ICRA},
  author    = {Jie Cheng and Yingbing Chen and Xiaodong Mei and Bowen Yang and Bo Li and Ming Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611364},
  month     = {5},
  pages     = {14123-14130},
  title     = {Rethinking imitation-based planners for autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Thermal voyager: A comparative study of RGB and thermal
cameras for night-time autonomous navigation. <em>ICRA</em>,
14116–14122. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Achieving reliable autonomous navigation during nighttime remains a substantial obstacle in the field of robotics. Although systems utilizing Light Detection and Ranging (Li-DAR) and Radio Detection and Ranging (RADAR) enable environmental perception regardless of lighting conditions, they face significant challenges in environments with a high density of agents due to their dependence on active emissions. Cameras operating in the visible spectrum represent a quasi-passive alternative, yet they see a substantial drop in efficiency in low-light conditions, consequently hindering both scene perception and path planning. Here, we introduce a novel end-to-end navigation system, the &quot;Thermal Voyager&quot;, which leverages infrared thermal vision to achieve true passive perception in autonomous entities. The system utilizes our architecture, TrajNet to interpret thermal visual inputs to produce desired trajectories and employs a model predictive control strategy to determine the optimal steering angles needed to actualize those trajectories. We train our TrajNet on a comprehensive video dataset incorporating visible and thermal footage along-side Controller Area Network (CAN) frames. We demonstrate that nighttime navigation facilitated by Long-Wave Infrared (LWIR) thermal cameras can rival the performance of daytime navigation systems using RGB cameras. Our work paves the way for scene perception and trajectory prediction empowered entirely by passive thermal sensing technology, heralding a new era where autonomous navigation is both feasible and reliable irrespective of the time of day. We make our code and thermal trajectory dataset public 1 .},
  archive   = {C_ICRA},
  author    = {Aditya NG and Dhruval PB and Jehan Shalabi and Shubhankar Jape and Xueji Wang and Zubin Jacob},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611311},
  month     = {5},
  pages     = {14116-14122},
  title     = {Thermal voyager: A comparative study of RGB and thermal cameras for night-time autonomous navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Odometry estimation by fusing multiple radar sensors and an
inertial measurement unit. <em>ICRA</em>, 14109–14115. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a framework for odometry estimation in automotive application using six asynchronously operating millimeter wave radar sensors and a combination of gyroscope and accelerometer. Two different motion models are combined to estimate motion with three degrees of freedom. For this purpose, we propose a novel three-part radar filtering method for outlier detection: By analyzing uncertainties and system limits, sensor-specific outliers are detected and removed in the first filter. We introduce knowledge about the previous motion state by a status-quo-ante filter and hereby identify further false positive raw targets in the current measure which are not accessible from the previous state. Moreover, we suggest employing a downstream, resampling-based algorithm for additional outlier detection. Based on the filtered data, radar motion state estimation is performed by use of curve fitting methods. To fuse the radar odometry estimation with the acceleration and yaw rate measurements handling non-linearities, an Unscented Kalman Filter is used. The developed framework is evaluated with reference data in various scenarios. The results demonstrate that it accurately and robustly determines motion and position states even in radar-challenging scenes, such as environments with few radar targets or with heavy metal structures. Our method keeps up with common approaches such as wheel speed sensor odometry while outperforming it in terms of drift-impairment.},
  archive   = {C_ICRA},
  author    = {Tim Brühl and Tim Dieter Eberhardt and Robin Schwager and Lukas Ewecker and Tin Stribor Sohn and Sören Hohmann},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610446},
  month     = {5},
  pages     = {14109-14115},
  title     = {Odometry estimation by fusing multiple radar sensors and an inertial measurement unit},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HHGNN: Heterogeneous hypergraph neural network for traffic
agents trajectory prediction in grouping scenarios. <em>ICRA</em>,
14101–14108. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many intelligent transportation systems, predicting the future motion of heterogeneous traffic participants is a fundamental but challenging task due to various factors encompassing the agents’ dynamic states, interactions with neighboring agents and surrounding traffic infrastructures, and their stochastic and multi-modal natural behavior tendencies. However, existing approaches have limitations as they either focus solely on static, pairwise interactions, ignoring interactions of varied granularity, or fail to tackle agents’ heterogeneity. In this paper, instead of focusing solely on pairwise interactions, we propose a Heterogenous Hypergraph Graph Neural Network (HHGNN) based motion prediction model that leverages the nature of hypergraph to encode the groupwise interactions among traffic participants. Moreover, we propose the type-aware two-level hypergraph message passing module (TTHMS) with learnable hyperedge-type embeddings to model the intra-group and inter-group level interactions among heterogeneous traffic agents (e.g., vehicles, pedestrians, and cyclists). Besides, We integrate a scene context fusion layer in TTHMS to incorporate the scene context. Comparison and ablation experiments on the Waymo Open Motion Dataset (WOMD) demonstrate HHGNN’s effectiveness within the motion prediction task.},
  archive   = {C_ICRA},
  author    = {Hetian Guo and Yingzhi Peng and Zipei Fan and He Zhu and Xuan Song},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611535},
  month     = {5},
  pages     = {14101-14108},
  title     = {HHGNN: Heterogeneous hypergraph neural network for traffic agents trajectory prediction in grouping scenarios},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Driving with LLMs: Fusing object-level vector modality for
explainable autonomous driving. <em>ICRA</em>, 14093–14100. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large Language Models (LLMs) have shown promise in the autonomous driving sector, particularly in generalization and interpretability. We introduce a unique objectlevel multimodal LLM architecture that merges vectorized numeric modalities with a pre-trained LLM to improve context understanding in driving situations. We also present a new dataset of 160k QA pairs derived from 10k driving scenarios, paired with high quality control commands collected with RL agent and question answer pairs generated by teacher LLM (GPT-3.5). A distinct pretraining strategy is devised to align numeric vector modalities with static LLM representations using vector captioning language data. We also introduce an evaluation metric for Driving QA and demonstrate our LLM-driver’s proficiency in interpreting driving scenarios, answering questions, and decision-making. Our findings highlight the potential of LLM-based driving action generation in comparison to traditional behavioral cloning. We make our benchmark, datasets, and model available 1 for further exploration.},
  archive   = {C_ICRA},
  author    = {Long Chen and Oleg Sinavski and Jan Hünermann and Alice Karnsund and Andrew James Willmott and Danny Birch and Daniel Maund and Jamie Shotton},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611018},
  month     = {5},
  pages     = {14093-14100},
  title     = {Driving with LLMs: Fusing object-level vector modality for explainable autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DOS®: A deployment operating system for robots.
<em>ICRA</em>, 14086–14092. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a new system named DOS ® (Deployment Operating System for Robots) for reliably deploying any data-driven robots in both production and simulation environments. Compared to existing systems, DOS ® features a unique CI/CD (continuous integration and continuous deployment) architecture which allows us to seamlessly integrate agile development and reliable operation in a fully automated fashion. With this CI/CD architecture, this paper mainly introduces three essential components that uniquely differentiate DOS ® from existing robotic systems: (i) An environment adapter that provides a systematic and robust approach to handle the deployment complexity in real world environments; (ii) A data replay reservoir that provides a unified data model supporting arbitrary robotic decision models; (iii) An analytical profiler that collects any set of user-defined performance metrics for system optimization. DOS ® significantly increases the reliability and maintainability of the deployed robotic systems. To illustrate this point, we compare DOS ® with more traditional approaches on deploying a navigational robot in a challenging working environment with many new corner case scenarios. Our results show that DOS ® outperforms traditional approach in great magnitudes in terms of deployment time and operational robustness.},
  archive   = {C_ICRA},
  author    = {Guo Ye and Qinjie Lin and Zening Luo and Han Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611631},
  month     = {5},
  pages     = {14086-14092},
  title     = {DOS®: A deployment operating system for robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CAPE: Corrective actions from precondition errors using
large language models. <em>ICRA</em>, 14070–14077. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Extracting knowledge and reasoning from large language models (LLMs) offers a path to designing intelligent robots. Common approaches that leverage LLMs for planning are unable to recover when actions fail and resort to retrying failed actions without resolving the underlying cause. We propose a novel approach (CAPE) that generates corrective actions to resolve precondition errors during planning. CAPE improves the quality of generated plans through few-shot reasoning on action preconditions. Our approach enables embodied agents to execute more tasks than baseline methods while maintaining semantic correctness and minimizing re-prompting. In VirtualHome, CAPE improves a human-annotated plan correctness metric from 28.89% to 49.63% over SayCan, whilst achieving competitive executability. Our improvements transfer to a Boston Dynamics Spot robot initialized with a set of skills (specified in language) and associated preconditions, where CAPE improves correctness by 76.49% with higher executability compared to SayCan. Our approach enables embodied agents to follow natural language commands and robustly recover from failures.},
  archive   = {C_ICRA},
  author    = {Shreyas Sundara Raman and Vanya Cohen and Ifrah Idrees and Eric Rosen and Raymond Mooney and Stefanie Tellex and David Paulius},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611376},
  month     = {5},
  pages     = {14070-14077},
  title     = {CAPE: Corrective actions from precondition errors using large language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal scene graph planning with large language model
guidance. <em>ICRA</em>, 14062–14069. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in metric, semantic, and topological mapping have equipped autonomous robots with concept grounding capabilities to interpret natural language tasks. Leveraging these capabilities, this work develops an efficient task planning algorithm for hierarchical metric-semantic models. We consider a scene graph model of the environment and utilize a large language model (LLM) to convert a natural language task into a linear temporal logic (LTL) automaton. Our main contribution is to enable optimal hierarchical LTL planning with LLM guidance over scene graphs. To achieve efficiency, we construct a hierarchical planning domain that captures the attributes and connectivity of the scene graph and the task automaton, and provide semantic guidance via an LLM heuristic function. To guarantee optimality, we design an LTL heuristic function that is provably consistent and supplements the potentially inadmissible LLM guidance in multi-heuristic planning. We demonstrate efficient planning of complex natural language tasks in scene graphs of virtualized real environments.},
  archive   = {C_ICRA},
  author    = {Zhirui Dai and Arash Asgharivaskasi and Thai Duong and Shusen Lin and Maria-Elizabeth Tzes and George Pappas and Nikolay Atanasov},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610599},
  month     = {5},
  pages     = {14062-14069},
  title     = {Optimal scene graph planning with large language model guidance},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interactive planning using large language models for
partially observable robotic tasks. <em>ICRA</em>, 14054–14061. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Designing robotic agents to perform open vocabulary tasks has been the long-standing goal in robotics and AI. Recently, Large Language Models (LLMs) have achieved impressive results in creating robotic agents for performing open vocabulary tasks. However, planning for these tasks in the presence of uncertainties is challenging as it requires &quot;chain-of-thought&quot; reasoning, aggregating information from the environment, updating state estimates, and generating actions based on the updated state estimates. In this paper, we present an interactive planning technique for partially observable tasks using LLMs. In the proposed method, an LLM is used to collect missing information from the environment using a robot, and infer the state of the underlying problem from collected observations while guiding the robot to perform the required actions. We also use a fine-tuned Llama 2 model via self-instruct and compare its performance against a pre-trained LLM like GPT-4. Results are demonstrated on several tasks in simulation as well as real-world environments.},
  archive   = {C_ICRA},
  author    = {Lingfeng Sun and Devesh K. Jha and Chiori Hori and Siddarth Jain and Radu Corcodel and Xinghao Zhu and Masayoshi Tomizuka and Diego Romeres},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610981},
  month     = {5},
  pages     = {14054-14061},
  title     = {Interactive planning using large language models for partially observable robotic tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conditionally combining robot skills using large language
models. <em>ICRA</em>, 14046–14053. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper combines two contributions. First, we introduce an extension of the Meta-World benchmark, which we call &quot;Language-World,&quot; which allows a large language model to operate in a simulated robotic environment using semi-structured natural language queries and scripted skills described using natural language. By using the same set of tasks as Meta-World, Language-World results can be easily compared to Meta-World results, allowing for a point of comparison between recent methods using Large Language Models (LLMs) and those using Deep Reinforcement Learning. Second, we introduce a method we call Plan Conditioned Behavioral Cloning (PCBC), that allows finetuning the behavior of high-level plans using end-to-end demonstrations. Using Language-World, we show that PCBC is able to achieve strong performance in a variety of few-shot regimes, often achieving task generalization with as little as a single demonstration. We have made Language-World available as open-source software at https://github.com/krzentner/language-world/.},
  archive   = {C_ICRA},
  author    = {K.R. Zentner and Ryan Julian and Brian Ichter and Gaurav S. Sukhatme},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611275},
  month     = {5},
  pages     = {14046-14053},
  title     = {Conditionally combining robot skills using large language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Anticipate &amp; act: Integrating LLMs and classical
planning for efficient task execution in household environments†.
<em>ICRA</em>, 14038–14045. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Assistive agents performing household tasks such as making the bed or cooking breakfast often compute and execute actions that accomplish one task at a time. However, efficiency can be improved by anticipating upcoming tasks and computing an action sequence that jointly achieves these tasks. State-of-the-art methods for task anticipation use data-driven deep networks and Large Language Models (LLMs), but they do so at the level of high-level tasks and/or require many training examples. Our framework leverages the generic knowledge of LLMs through a small number of prompts to perform high-level task anticipation, using the anticipated tasks as goals in a classical planning system to compute a sequence of finer-granularity actions that jointly achieve these goals. We ground and evaluate our framework’s abilities in realistic scenarios in the VirtualHome environment and demonstrate a 31% reduction in execution time compared with a system that does not consider upcoming tasks.},
  archive   = {C_ICRA},
  author    = {Raghav Arora and Shivam Singh and Karthik Swaminathan and Ahana Datta and Snehasis Banerjee and Brojeshwar Bhowmick and Krishna Murthy Jatavallabhula and Mohan Sridharan and Madhava Krishna},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611164},
  month     = {5},
  pages     = {14038-14045},
  title     = {Anticipate &amp; act: Integrating LLMs and classical planning for efficient task execution in household environments†},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Grasp-anything: Large-scale grasp dataset from foundation
models. <em>ICRA</em>, 14030–14037. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Foundation models such as ChatGPT have made significant strides in robotic tasks due to their universal representation of real-world domains. In this paper, we leverage foundation models to tackle grasp detection, a persistent challenge in robotics with broad industrial applications. Despite numerous grasp datasets, their object diversity remains limited compared to real-world figures. Fortunately, foundation models possess an extensive repository of real-world knowledge, including objects we encounter in our daily lives. As a consequence, a promising solution to the limited representation in previous grasp datasets is to harness the universal knowledge embedded in these foundation models. We present Grasp-Anything, a new large-scale grasp dataset synthesized from foundation models to implement this solution. Grasp-Anything excels in diversity and magnitude, boasting 1M samples with text descriptions and more than 3M objects, surpassing prior datasets. Empirically, we show that Grasp-Anything successfully facilitates zero-shot grasp detection on vision-based tasks and real-world robotic experiments. Our dataset and code are available at https://airvlab.github.io/grasp-anything/.},
  archive   = {C_ICRA},
  author    = {A. D. Vuong and M. N. Vu and H. Le and B. Huang and H. T. T. Binh and T. Vo and A. Kugi and A. Nguyen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611277},
  month     = {5},
  pages     = {14030-14037},
  title     = {Grasp-anything: Large-scale grasp dataset from foundation models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3D object detection with VI-SLAM point clouds: The impact of
object and environment characteristics on model performance.
<em>ICRA</em>, 14014–14020. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D object detection (OD) is a crucial element in scene understanding. However, most existing 3D OD models have been tailored to work with light detection and ranging (LiDAR) and RGB-D point cloud data, leaving their performance on commonly available visual-inertial simultaneous localization and mapping (VI-SLAM) point clouds unexamined. In this paper, we create and release two datasets: VIP500, 4772 VI-SLAM point clouds covering 500 different object and environment configurations, and VIP500-D, an accompanying set of 20 RGB-D point clouds for the object classes and shapes in VIP500. We then use these datasets to quantify the differences between VI-SLAM point clouds and dense RGB-D point clouds, as well as the discrepancies between VI-SLAM point clouds generated with different object and environment characteristics. Finally, we evaluate the performance of three leading OD models on the diverse data in our VIP500 dataset, revealing the promise of OD models trained on VI-SLAM data; we examine the extent to which both object and environment characteristics impact performance, along with the underlying causes.},
  archive   = {C_ICRA},
  author    = {Lin Duan and Tim Scargill and Ying Chen and Maria Gorlatova},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610778},
  month     = {5},
  pages     = {14014-14020},
  title     = {3D object detection with VI-SLAM point clouds: The impact of object and environment characteristics on model performance},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic occupancy grids for object detection: A
radar-centric approach. <em>ICRA</em>, 13991–13997. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dynamic Occupancy Grid Mapping is a technique used to generate a local map of the environment, containing both static and dynamic information. Typically, these maps are primarily generated using lidar measurements. However, with improvements in radar sensing, resulting in better accuracy and higher resolution, radar is emerging as a viable alternative to lidar as the primary sensor for mapping. In this paper, we propose a radar-centric dynamic occupancy grid mapping algorithm with adaptations to the state computation, inverse sensor model, and field-of-view computation tailored to the specifics of radar measurements. We extensively evaluate our approach with real data to demonstrate its effectiveness and establish the first benchmark for radar-based dynamic occupancy grid mapping using the publicly available Radarscenes dataset.},
  archive   = {C_ICRA},
  author    = {Max Peter Ronecker and Markus Schratter and Lukas Kuschnig and Daniel Watzenig},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610514},
  month     = {5},
  pages     = {13991-13997},
  title     = {Dynamic occupancy grids for object detection: A radar-centric approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HandyPriors: Physically consistent perception of hand-object
interactions with differentiable priors. <em>ICRA</em>, 13983–13990. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Various heuristic objectives for modeling hand-object interaction have been proposed in past work. However, due to the lack of a cohesive framework, these objectives often possess a narrow scope of applicability and are limited by their efficiency or accuracy. In this paper, we propose HANDYPRIORS, a unified and general pipeline for pose estimation in human-object interaction scenes by leveraging recent advances in differentiable physics and rendering. Our approach employs rendering priors to align with input images and segmentation masks along with physics priors to mitigate penetration and relative-sliding across frames. Furthermore, we present two alternatives for hand and object pose estimation. The optimization-based pose estimation achieves higher accuracy, while the filtering-based tracking, which utilizes the differentiable priors as dynamics and observation models, executes faster. We demonstrate that HANDYPRIORS attains comparable or superior results in the pose estimation task, and that the differentiable physics module can predict contact information for pose refinement. We also show that our approach generalizes to perception tasks, including robotic hand manipulation and human-object pose estimation in the wild.},
  archive   = {C_ICRA},
  author    = {Shutong Zhang and Yi-Ling Qiao and Guanglei Zhu and Eric Heiden and Dylan Turpin and Jingzhou Liu and Ming Lin and Miles Macklin and Animesh Garg},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610748},
  month     = {5},
  pages     = {13983-13990},
  title     = {HandyPriors: Physically consistent perception of hand-object interactions with differentiable priors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MEDL-u: Uncertainty-aware 3D automatic annotation based on
evidential deep learning. <em>ICRA</em>, 13976–13982. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Advancements in deep learning-based 3D object detection necessitate the availability of large-scale datasets. However, this requirement introduces the challenge of manual annotation, which is often both burdensome and time-consuming. To tackle this issue, the literature has seen the emergence of several weakly supervised frameworks for 3D object detection which can automatically generate pseudo labels for unlabeled data. Nevertheless, these generated pseudo labels contain noise and are not as accurate as those labeled by humans. In this paper, we present the first approach that addresses the inherent ambiguities present in pseudo labels by introducing an Evidential Deep Learning (EDL) based uncertainty estimation framework. Specifically, we propose MEDL-U, an EDL framework based on MTrans, which not only generates pseudo labels but also quantifies the associated uncertainties. However, applying EDL to 3D object detection presents three key challenges: (1) lower pseudo label quality in comparison to other autolabelers; (2) high evidential uncertainty estimates; and (3) lack of clear interpretability and effective utilization of uncertainties for downstream tasks. We tackle these issues through the introduction of an uncertainty-aware IoU-based loss, an evidence-aware multi-task loss, and the implementation of a post-processing stage for uncertainty refinement. Our experimental results demonstrate that probabilistic detectors trained using the outputs of MEDL-U surpass deterministic detectors trained using outputs from previous 3D annotators on the KITTI val set for all difficulty levels. Moreover, MEDL-U achieves state-of-the-art results on the KITTI official test set compared to existing 3D automatic annotators. Code is publicly available at https://github.com/paathelb/MEDL-U.},
  archive   = {C_ICRA},
  author    = {Helbert Paat and Qing Lian and Weilong Yao and Tong Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610597},
  month     = {5},
  pages     = {13976-13982},
  title     = {MEDL-U: Uncertainty-aware 3D automatic annotation based on evidential deep learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Open-vocabulary affordance detection using knowledge
distillation and text-point correlation. <em>ICRA</em>, 13968–13975. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Affordance detection presents intricate challenges and has a wide range of robotic applications. Previous works have faced limitations such as the complexities of 3D object shapes, the wide range of potential affordances on real-world objects, and the lack of open-vocabulary support for affordance understanding. In this paper, we introduce a new open-vocabulary affordance detection method in 3D point clouds, leveraging knowledge distillation and text-point correlation. Our approach employs pre-trained 3D models through knowledge distillation to enhance feature extraction and semantic understanding in 3D point clouds. We further introduce a new text-point correlation method to learn the semantic links between point cloud features and open-vocabulary labels. The intensive experiments show that our approach outperforms previous works and adapts to new affordance labels and unseen objects. Notably, our method achieves the improvement of 7.96% mIOU score compared to the baselines. Furthermore, it offers real-time inference which is well-suitable for robotic manipulation applications.},
  archive   = {C_ICRA},
  author    = {Tuan Van Vo and Minh Nhat Vu and Baoru Huang and Toan Nguyen and Ngan Le and Thieu Vo and Anh Nguyen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610247},
  month     = {5},
  pages     = {13968-13975},
  title     = {Open-vocabulary affordance detection using knowledge distillation and text-point correlation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PUMA: Fully decentralized uncertainty-aware multiagent
trajectory planner with real-time image segmentation-based frame
alignment. <em>ICRA</em>, 13961–13967. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fully decentralized, multiagent trajectory planners enable complex tasks like search and rescue or package delivery by ensuring safe navigation in unknown environments. However, deconflicting trajectories with other agents and ensuring collision-free paths in a fully decentralized setting is complicated by dynamic elements and localization uncertainty. To this end, this paper presents (1) an uncertainty-aware multiagent trajectory planner and (2) an image segmentation-based frame alignment pipeline. The uncertainty-aware planner propagates uncertainty associated with the future motion of detected obstacles, and by incorporating this propagated uncertainty into optimization constraints, the planner effectively navigates around obstacles. Unlike conventional methods that emphasize explicit obstacle tracking, our approach integrates implicit tracking. Moreover, sharing trajectories between agents can cause potential collisions due to frame misalignment. Addressing this, we introduce a novel frame alignment pipeline that rectifies inter-agent frame misalignment. This method leverages a zero-shot image segmentation model for detecting objects in the environment and a data association framework based on geometric consistency for map alignment. Our approach accurately aligns frames with only 0.18 m and 2.7° of mean frame alignment error in our most challenging simulation scenario. In addition, we conducted hardware experiments and successfully achieved 0.29 m and 2.59° of frame alignment error. Together with the alignment framework, our planner ensures safe navigation in unknown environments and collision avoidance in decentralized settings.},
  archive   = {C_ICRA},
  author    = {Kota Kondo and Claudius T. Tewari and Mason B. Peterson and Annika Thomas and Jouko Kinnari and Andrea Tagliabue and Jonathan P. How},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610629},
  month     = {5},
  pages     = {13961-13967},
  title     = {PUMA: Fully decentralized uncertainty-aware multiagent trajectory planner with real-time image segmentation-based frame alignment},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RISeg: Robot interactive object segmentation via body
frame-invariant features. <em>ICRA</em>, 13954–13960. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In order to successfully perform manipulation tasks in new environments, such as grasping, robots must be proficient in segmenting unseen objects from the background and/or other objects. Previous works perform unseen object instance segmentation (UOIS) by training deep neural networks on large-scale data to learn RGB/RGB-D feature embeddings, where cluttered environments often result in inaccurate segmentations. We build upon these methods and introduce a novel approach to correct inaccurate segmentation, such as under-segmentation, of static image-based UOIS masks by using robot interaction and a designed body frame-invariant feature. We demonstrate that the relative linear and rotational velocities of frames randomly attached to rigid bodies due to robot interactions can be used to identify objects and accumulate corrected object-level segmentation masks. By introducing motion to regions of segmentation uncertainty, we are able to drastically improve segmentation accuracy in an uncertainty-driven manner with minimal, non-disruptive interactions (ca. 2-3 per scene). We demonstrate the effectiveness of our proposed interactive perception pipeline in accurately segmenting cluttered scenes by achieving an average object segmentation accuracy rate of 80.7%, an increase of 28.2% when compared with other state-of-the-art UOIS methods.},
  archive   = {C_ICRA},
  author    = {Howard H. Qian and Yangxiao Lu and Kejia Ren and Gaotian Wang and Ninad Khargonkar and Yu Xiang and Kaiyu Hang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611015},
  month     = {5},
  pages     = {13954-13960},
  title     = {RISeg: Robot interactive object segmentation via body frame-invariant features},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robotic grasping of harvested tomato trusses using vision
and online learning. <em>ICRA</em>, 13947–13953. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Currently, truss tomato weighing and packaging require significant manual work. The main obstacle to automation lies in the difficulty of developing a reliable robotic grasping system for already harvested trusses. We propose a method to grasp trusses that are stacked in a crate with considerable clutter, which is how they are commonly stored and transported after harvest. The method consists of a deep learning-based vision system to first identify the individual trusses in the crate and then determine a suitable grasping location on the stem. To this end, we have introduced a grasp pose ranking algorithm with online learning capabilities. After selecting the most promising grasp pose, the robot executes a pinch grasp without needing touch sensors or geometric models. Lab experiments with a robotic manipulator equipped with an eye-in-hand RGB-D camera showed a 100% clearance rate when tasked to pick all trusses from a pile. 93% of the trusses were successfully grasped on the first try, while the remaining 7% required more attempts.},
  archive   = {C_ICRA},
  author    = {Luuk van den Bent and Tomás Coleman and Robert Babuška},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610089},
  month     = {5},
  pages     = {13947-13953},
  title     = {Robotic grasping of harvested tomato trusses using vision and online learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HandNeRF: Learning to reconstruct hand-object interaction
scene from a single RGB image. <em>ICRA</em>, 13940–13946. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a method to learn hand-object interaction prior for reconstructing a 3D hand-object scene from a single RGB image. The inference as well as training-data generation for 3D hand-object scene reconstruction is challenging due to the depth ambiguity of a single image and occlusions by the hand and object. We turn this challenge into an opportunity by utilizing the hand shape to constrain the possible relative configuration of the hand and object geometry. We design a generalizable implicit function, HandNeRF, that explicitly encodes the correlation of the 3D hand shape features and 2D object features to predict the hand and object scene geometry. With experiments on real-world datasets, we show that HandNeRF can reconstruct hand-object scenes of novel grasp configurations more accurately than comparable methods. Moreover, we demonstrate that object reconstruction from HandNeRF ensures more accurate execution of downstream tasks, such as grasping for robotic hand-over.},
  archive   = {C_ICRA},
  author    = {Hongsuk Choi and Nikhil Chavan-Dafle and Jiacheng Yuan and Volkan Isler and Hyunsoo Park},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611230},
  month     = {5},
  pages     = {13940-13946},
  title     = {HandNeRF: Learning to reconstruct hand-object interaction scene from a single RGB image},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kitchen artist: Precise control of liquid dispensing for
gourmet plating. <em>ICRA</em>, 13933–13939. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Manipulating liquid is widely required for many tasks, especially in cooking. A common way to address this is extruding viscous liquid from a squeeze bottle. In this work, our goal is to create a sauce plating robot, which requires precise control of the thickness of squeezed liquids on a surface. Different liquids demand different manipulation policies. We command the robot to tilt the container and monitor the liquid response using a force sensor to identify liquid properties. Based on the liquid properties, we predict the liquid behavior with fixed squeezing motions in a data-driven way and calculate the required drawing speed for the desired stroke size. This open-loop system works effectively even without sensor feedback. Our experiments demonstrate accurate stroke size control across different liquids and fill levels. We show that understanding liquid properties can facilitate effective liquid manipulation. More importantly, our dish garnishing robot has a wide range of applications and holds significant commercialization potential.},
  archive   = {C_ICRA},
  author    = {Hung-Jui Huang and Jingyi Xiang and Wenzhen Yuan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611646},
  month     = {5},
  pages     = {13933-13939},
  title     = {Kitchen artist: Precise control of liquid dispensing for gourmet plating},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Residual-NeRF: Learning residual NeRFs for transparent
object manipulation. <em>ICRA</em>, 13918–13924. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Transparent objects are ubiquitous in industry, pharmaceuticals, and households. Grasping and manipulating these objects is a significant challenge for robots. Existing methods have difficulty reconstructing complete depth maps for challenging transparent objects, leaving holes in the depth reconstruction. Recent work has shown neural radiance fields (NeRFs) work well for depth perception in scenes with transparent objects, and these depth maps can be used to grasp transparent objects with high accuracy. NeRF-based depth reconstruction can still struggle with especially challenging transparent objects and lighting conditions. In this work, we propose Residual-NeRF, a method to improve depth perception and training speed for transparent objects. Robots often operate in the same area, such as a kitchen. By first learning a background NeRF of the scene without transparent objects to be manipulated, we reduce the ambiguity faced by learning the changes with the new object. We propose training two additional networks: a residual NeRF learns to infer residual RGB values and densities, and a Mixnet learns how to combine background and residual NeRFs. We contribute synthetic and real experiments that suggest Residual-NeRF improves depth perception of transparent objects. The results on synthetic data suggest Residual-NeRF outperforms the baselines with a 46.1 % lower RMSE and a 29.5 % lower MAE. Real-world qualitative experiments suggest Residual-NeRF leads to more robust depth maps with less noise and fewer holes. Website: https://residual-nerf.github.io},
  archive   = {C_ICRA},
  author    = {Bardienus P. Duisterhof and Yuemin Mao and Si Heng Teng and Jeffrey Ichnowski},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611224},
  month     = {5},
  pages     = {13918-13924},
  title     = {Residual-NeRF: Learning residual NeRFs for transparent object manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sim-to-real grasp detection with global-to-local RGB-d
adaptation. <em>ICRA</em>, 13910–13917. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper focuses on the sim-to-real issue of RGB-D grasp detection and formulates it as a domain adaptation problem. In this case, we present a global-to-local method to address hybrid domain gaps in RGB and depth data and insufficient multi-modal feature alignment. First, a self-supervised rotation pre-training strategy is adopted to deliver robust initialization for RGB and depth networks. We then propose a global-to-local alignment pipeline with individual global domain classifiers for scene features of RGB and depth images as well as a local one specifically working for grasp features in the two modalities. In particular, we propose a grasp prototype adaptation module, which aims to facilitate fine-grained local feature alignment by dynamically updating and matching the grasp prototypes from the simulation and real-world scenarios throughout the training process. Due to such designs, the proposed method substantially reduces the domain shift and thus leads to consistent performance improvements. Extensive experiments are conducted on the GraspNet-Planar benchmark and physical environment, and superior results are achieved which demonstrate the effectiveness of our method. Code is available at https://github.com/mahaoxiang822/GL-MSDA.},
  archive   = {C_ICRA},
  author    = {Haoxiang Ma and Ran Qin and Modi Shi and Boyang Gao and Di Huang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611165},
  month     = {5},
  pages     = {13910-13917},
  title     = {Sim-to-real grasp detection with global-to-local RGB-D adaptation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The GEM-c controller for load compensation in object
manipulation. <em>ICRA</em>, 13904–13909. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Nowadays, robotic arms are ubiquitously employed for object manipulation across a spectrum of applications, spanning from production lines to warehouses, and encompassing both stationary and mobile robotic systems. Among the most prevalent end-effectors, used for the majority of these applications, are suction cups. The rudimentary act of grasping an object and relocating it, devoid of a cognizant awareness of the forces stemming from the object’s motion and grip, can result in suboptimal and inefficient robot movements. In more dire circumstances, such negligent handling may precipitate detachment of the object from the end-effector, potentially incurring damage to either the object or the arm.In this paper, we build upon the advanced sensing and attaching capabilities of our suction cup MIGHTY, and introduce GEM-C, a novel Gravity, External forces and Motion Compensation controller, that constantly adapts the orientation of the suction cup so as to enhance the quality of attachment. Throughout all examined scenarios and experiments, our approach remarkably improved the robot’s performance by providing the optimal end-effector pose while also reducing the stress on the motors and the overall power consumption. The derived results, clearly demonstrate the MIGHTY and GEM-C schema’s potential for a wide range of demanding robotic manipulation tasks.},
  archive   = {C_ICRA},
  author    = {Emmanouil Papadakis and Markos Sigalas and Michail Vangos and Panos Trahanias},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611258},
  month     = {5},
  pages     = {13904-13909},
  title     = {The GEM-C controller for load compensation in object manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Thermoformed electronic skins for conformal tactile sensor
arrays. <em>ICRA</em>, 13898–13903. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots and prostheses are increasingly designed with curvilinear surfaces for functional, aesthetic, aerodynamic, and safety reasons. Electronic skins (e-skins) capable of sensing contact location and pressure across complex, non-developable surfaces are essential for empowering next-generation robots with tactile awareness. This will facilitate safe and natural human-machine interactions while enhancing object manipulation capabilities. Despite the evident advantages of conformal e-skins, current fabrication methods face significant challenges in realizing their full potential. In this paper, we introduce thermoforming as a technique to efficiently fabricate tactile sensitive e-skins that conform to curvilinear surfaces. The performance, repeatability and uniformity of the sensors are characterized in detail. We also present a custom calibration pipeline where accurate digital replicas of conformal e-skins are generated for use in simulations. Finally, we demonstrate the benefits of 3D e-skins in a tool manipulation task.},
  archive   = {C_ICRA},
  author    = {Peng Lu and Jiaming Liang and Bidan Huang and Sicheng Yang and Wang Wei Lee},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610733},
  month     = {5},
  pages     = {13898-13903},
  title     = {Thermoformed electronic skins for conformal tactile sensor arrays},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). You only scan once: A dynamic scene reconstruction pipeline
for 6-DoF robotic grasping of novel objects. <em>ICRA</em>, 13891–13897.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the realm of robotic grasping, achieving accurate and reliable interactions with the environment is a pivotal challenge. Traditional methods of grasp planning methods utilizing partial point clouds derived from depth image often suffer from reduced scene understanding due to occlusion, ultimately impeding their grasping accuracy. Furthermore, scene reconstruction methods have primarily relied upon static techniques, which are susceptible to environment change during manipulation process limits their efficacy in real-time grasping tasks. To address these limitations, this paper introduces a novel two-stage pipeline for dynamic scene reconstruction. In the first stage, our approach takes scene scanning as input to register each target object with mesh reconstruction and novel object pose tracking. In the second stage, pose tracking is still performed to provide object poses in real-time, enabling our approach to transform the reconstructed object point clouds back into the scene. Unlike conventional methodologies, which rely on static scene snapshots, our method continuously captures the evolving scene geometry, resulting in a comprehensive and up-to-date point cloud representation. By circumventing the constraints posed by occlusion, our method enhances the overall grasp planning process and empowers state-of-the-art 6-DoF robotic grasping algorithms to exhibit markedly improved accuracy.},
  archive   = {C_ICRA},
  author    = {Lei Zhou and Haozhe Wang and Zhengshen Zhang and Zhiyang Liu and Francis E.H. Tay and Marcelo H. Ang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611371},
  month     = {5},
  pages     = {13891-13897},
  title     = {You only scan once: A dynamic scene reconstruction pipeline for 6-DoF robotic grasping of novel objects},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robotic manipulation of hand tools: The case of
screwdriving. <em>ICRA</em>, 13883–13890. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite decades of steady research progress, the robotic hand is still far behind the human hand in terms of dexterity and versatility. A milestone in this quest for human-level performance will be possessing the skills of manipulating hand tools, for their non-trivial geometries and for the intricacies of controlling their contact-based interactions with objects, which are the final targets of manipulation. This paper investigates screwdriving by a robotic arm/hand pair, dealing with the chain of contacts connecting the substrate, screw, screwdriver, and fingertips. Considering rolling contacts and finger gaits, our force control scheme is derived through backward chaining to leverage the dynamics of the screwdriver and arm/hand. To maintain the fastening effort, estimations are carried out sequentially for the screwdriver’s pose via optimization under visual and kinematic constraints, and for its applied wrench on the screw via solution drawing upon dynamics. This wrench, adjusted based on position/force feedback, is mapped by the grasp matrix to the desired fingertip forces, which are then used for computing torques to be exerted by the arm and hand to close the loop. Simulation and experiments with a Shadow Hand have been conducted for validations.},
  archive   = {C_ICRA},
  author    = {Ling Tang and Yan-Bin Jia and Yuechuan Xue},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610831},
  month     = {5},
  pages     = {13883-13890},
  title     = {Robotic manipulation of hand tools: The case of screwdriving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tactile estimation of extrinsic contact patch for stable
placement. <em>ICRA</em>, 13876–13882. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Precise perception of contact interactions is essential for fine-grained manipulation skills for robots. In this paper, we present the design of feedback skills for robots that must learn to stack complex-shaped objects on top of each other (see Fig. 1). To design such a system, a robot should be able to reason about the stability of placement from very gentle contact interactions. Our results demonstrate that it is possible to infer the stability of object placement based on tactile readings during contact formation between the object and its environment. In particular, we estimate the contact patch between a grasped object and its environment using force and tactile observations to estimate the stability of the object during a contact formation. The contact patch could be used to estimate the stability of the object upon release of the grasp. The proposed method is demonstrated in various pairs of objects that are used in a very popular board game.},
  archive   = {C_ICRA},
  author    = {Kei Ota and Devesh K. Jha and Krishna Murthy Jatavallabhula and Asako Kanezaki and Joshua B. Tenenbaum},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611504},
  month     = {5},
  pages     = {13876-13882},
  title     = {Tactile estimation of extrinsic contact patch for stable placement},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Direct self-identification of inverse jacobians for
dexterous manipulation through particle filtering. <em>ICRA</em>,
13862–13868. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to plan and control robotic in-hand manipulation is challenged by several issues, including the required amount of prior knowledge of the system and the sophisticated physics that varies across different robot hands or even grasp instances. One of the most direct models of in-hand manipulation is the inverse Jacobian, which can directly map from the desired in-hand object motions to the required hand actuator controls. However, acquiring such inverse Jacobians without complex hand-object system models is typically infeasible. We present a method for controlling in-hand manipulation using inverse Jacobians that are self-identified by a particle filter-based estimation scheme that leverages the ability of underactuated hands to maintain a passively stable grasp during self-identification movements. This method requires no a priori knowledge of the specific hand-object system and learns the system’s inverse Jacobian through small exploratory motions. Our system approximates the underlying inverse Jacobian closely, which can be used to perform manipulation tasks across a range of objects successfully. With extensive experiments on a Yale Model O hand, we show that the proposed system can provide accurate in-hand manipulation of sub-millimeter precision and that the inverse Jacobian-based controller can support real-time manipulation control of up to 900Hz.},
  archive   = {C_ICRA},
  author    = {Joshua T. Grace and Podshara Chanrungmaneekul and Kaiyu Hang and Aaron M. Dollar},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611052},
  month     = {5},
  pages     = {13862-13868},
  title     = {Direct self-identification of inverse jacobians for dexterous manipulation through particle filtering},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Masked visual-tactile pre-training for robot manipulation.
<em>ICRA</em>, 13859–13875. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent works on the pretraining for robot manipulation have demonstrated that representations learning from large human manipulation data can generalize well to new manipulation tasks and environments. However, these approaches mainly focus on human vision or natural language, neglecting tactile feedback. In this article, we make an attempt to explore how to pre-train a representation model for robotic manipulation using both human manipulation visual and tactile data. We develop a system for collecting visual and tactile data, featuring a cost-effective tactile glove to capture human tactile data and Hololens2 for capturing visual data. With this system, we collect a dataset of turning bottle caps. Furthermore, we introduce a novel visual-tactile fusion network and learning strategy M 2 VTP, with one key module to tokenize 20 sparse binary tactile signals sensing touch states for the learning of tactile context and the other key module applying the attention and mask mechanism to the interaction of visual and tactile tokens for visual-tactile representation learning. We utilize our dataset to pre-train the fusion model and embed the pre-trained model into a reinforcement learning framework for downstream tasks. Experimental results demonstrate that our pre-trained model significantly aids in learning manipulation skills. Compared to methods without pre-training, our approach achieves a success rate increase of over 60%. Additionally, when compared to current visual pre-training methods, our success rate exceeds them by more than 50%.},
  archive   = {C_ICRA},
  author    = {Qingtao Liu and Qi Ye and Zhengnan Sun and Yu Cui and Gaofeng Li and Jiming Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610933},
  month     = {5},
  pages     = {13859-13875},
  title     = {Masked visual-tactile pre-training for robot manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Harnessing the synergy between pushing, grasping, and
throwing to enhance object manipulation in cluttered scenarios.
<em>ICRA</em>, 13855–13861. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we delve into the intricate synergy among non-prehensile actions like pushing, and prehensile actions such as grasping and throwing, within the domain of robotic manipulation. We introduce an innovative approach to learning these synergies by leveraging model-free deep reinforcement learning. The robot’s workflow involves detecting the pose of the target object and the basket at each time step, predicting the optimal push configuration to isolate the target object, determining the appropriate grasp configuration, and inferring the necessary parameters for an accurate throw into the basket. This empowers robots to skillfully reconfigure cluttered scenarios through pushing, creating space for collision-free grasping actions. Simultaneously, we integrate throwing behavior, showcasing how this action significantly extends the robot’s operational reach. Ensuring safety, we developed a simulation environment in Gazebo for robot training, applying the learned policy directly to our real robot. Notably, this work represents a pioneering effort to learn the synergy between pushing, grasping, and throwing actions. Extensive experimentation in both simulated and real-robot scenarios substantiates the effectiveness of our approach across diverse settings. Our approach achieves a success rate exceeding 80% in both simulated and real-world scenarios. A video showcasing our experiments is available online at: https://youtu.be/q1l4BJVDbRw},
  archive   = {C_ICRA},
  author    = {Hamidreza Kasaei and Mohammadreza Kasaei},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610548},
  month     = {5},
  pages     = {13855-13861},
  title     = {Harnessing the synergy between pushing, grasping, and throwing to enhance object manipulation in cluttered scenarios},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A robust model predictive controller for tactile servoing.
<em>ICRA</em>, 13848–13854. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tactile servoing is an effective approach to enabling robots to safely interact with unknown environments. One of the core problems in tactile servoing is to robustly converge the contact features to the desired ones via a dedicated controller. This paper proposes a Data-Driven Model Predictive Controller (DDMPC) to compute the motion command given the previous interaction experience and feature deviations in tactile space. Compared with the manually designed PID-based controller, the proposed controller depends on the sound control theory and its convergence is guaranteed from a computational perspective. It is applied to the balancing control of a rolling bottle on a robotic forearm covered by a custom tactile sensor array. The real experiment demonstrates the superior robustness of the proposed approach and shows its great potential for other tactile servoing scenarios with measurement noise, which is inevitable for current tactile sensors.},
  archive   = {C_ICRA},
  author    = {Shuai Wang and Yihao Huang and Wang Wei Lee and Tianliang Liu and Xiao Teng and Yu Zheng and Qiang Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611317},
  month     = {5},
  pages     = {13848-13854},
  title     = {A robust model predictive controller for tactile servoing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised learning for joint pushing and grasping
policies in highly cluttered environments. <em>ICRA</em>, 13840–13847.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic systems often face challenges when attempting to grasp a target object due to interference from surrounding items. We propose a Deep Reinforcement Learning (DRL) method that develops joint policies for grasping and pushing, enabling effective manipulation of target objects within untrained, densely cluttered environments. In particular, a dual RL model is introduced, which presents high resilience in handling complicated scenes, reaching an average of 98% task completion in simulation and real-world scenes. To evaluate the proposed method, we conduct comprehensive simulation experiments in three distinct environments: densely packed building blocks, randomly positioned building blocks, and common household objects. Further, real-world tests are conducted using actual robots to confirm the robustness of our approach in various untrained and highly cluttered environments. The results from experiments underscore the superior efficacy of our method in both simulated and real-world scenarios, outperforming recent state-of-the-art methods. To ensure reproducibility and further the academic discourse, we make available a demonstration video, the trained models, and the source code for public access. https://sites.google.com/view/pushandgrasp/home.},
  archive   = {C_ICRA},
  author    = {Yongliang Wang and Kamal Mokhtar and Cock Heemskerk and Hamidreza Kasaei},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611650},
  month     = {5},
  pages     = {13840-13847},
  title     = {Self-supervised learning for joint pushing and grasping policies in highly cluttered environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time contact state estimation in shape control of
deformable linear objects under small environmental constraints.
<em>ICRA</em>, 13833–13839. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Controlling the shape of deformable linear objects using robots and constraints provided by environmental fixtures has diverse industrial applications. In order to establish robust contacts with these fixtures, accurate estimation of the contact state is essential for preventing and rectifying potential anomalies. However, this task is challenging due to the small sizes of fixtures, the requirement for real-time performances, and the infinite degrees of freedom of the deformable linear objects. In this paper, we propose a real-time approach for estimating both contact establishment and subsequent changes by leveraging the dependency between the applied and detected contact force on the deformable linear objects. We seamlessly integrate this method into the robot control loop and achieve an adaptive shape control framework which avoids, detects and corrects anomalies automatically. Real-world experiments validate the robustness and effectiveness of our contact estimation approach across various scenarios, significantly increasing the success rate of shape control processes.},
  archive   = {C_ICRA},
  author    = {Kejia Chen and Zhenshan Bing and Yansong Wu and Fan Wu and Liding Zhang and Sami Haddadin and Alois Knoll},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611558},
  month     = {5},
  pages     = {13833-13839},
  title     = {Real-time contact state estimation in shape control of deformable linear objects under small environmental constraints},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). See to touch: Learning tactile dexterity through visual
incentives. <em>ICRA</em>, 13825–13832. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Equipping multi-fingered robots with tactile sensing is crucial for achieving the precise, contact-rich, and dexterous manipulation that humans excel at. However, relying solely on tactile sensing fails to provide adequate cues for reasoning about objects’ spatial configurations, limiting the ability to correct errors and adapt to changing situations. In this paper, we present Tactile Adaptation from Visual Incentives (TAVI), a new framework that enhances tactile-based dexterity by optimizing dexterous policies using vision-based rewards. First, we use a contrastive-based objective to learn visual representations. Next, we construct a reward function using these visual representations through optimal-transport based matching on one human demonstration. Finally, we use online reinforcement learning on our robot to optimize tactile-based policies that maximize the visual reward. On six challenging tasks, such as peg pick-and-place, unstacking bowls, and flipping slender objects, TAVI achieves a success rate of 73% using our four-fingered Allegro robot hand. The increase in performance is 108% higher than policies using tactile and vision-based rewards and 135% higher than policies without tactile observational input. Robot videos are best viewed on our project website: https://see-to-touch.github.io/.},
  archive   = {C_ICRA},
  author    = {Irmak Guzey and Yinlong Dai and Ben Evans and Soumith Chintala and Lerrel Pinto},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611407},
  month     = {5},
  pages     = {13825-13832},
  title     = {See to touch: Learning tactile dexterity through visual incentives},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OASIS: Optimal arrangements for sensing in SLAM.
<em>ICRA</em>, 13818–13824. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The number and arrangement of sensors on mobile robot dramatically influence its perception capabilities. Ensuring that sensors are mounted in a manner that enables accurate detection, localization, and mapping is essential for the success of downstream control tasks. However, when designing a new robotic platform, researchers and practitioners alike usually mimic standard configurations or maximize simple heuristics like field-of-view (FOV) coverage to decide where to place exteroceptive sensors. In this work, we conduct an information-theoretic investigation of this overlooked element of robotic perception in the context of simultaneous localization and mapping (SLAM). We show how to formalize the sensor arrangement problem as a form of subset selection under the E-optimality performance criterion. While this formulation is NP-hard in general, we show that a combination of greedy sensor selection and fast convex relaxation-based post-hoc verification enables the efficient recovery of certifiably optimal sensor designs in practice. Results from synthetic experiments reveal that sensors placed with OASIS outperform benchmarks in terms of mean squared error of visual SLAM estimates.},
  archive   = {C_ICRA},
  author    = {Pushyami Kaveti and Matthew Giamou and Hanumant Singh and David M. Rosen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611644},
  month     = {5},
  pages     = {13818-13824},
  title     = {OASIS: Optimal arrangements for sensing in SLAM},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The importance of coordinate frames in dynamic SLAM.
<em>ICRA</em>, 13755–13761. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most Simultaneous localisation and mapping (SLAM) systems have traditionally assumed a static world, which does not align with real-world scenarios. To enable robots to safely navigate and plan in dynamic environments, it is essential to employ representations capable of handling moving objects. Dynamic SLAM is an emerging field in SLAM research as it improves the overall system accuracy while providing additional estimation of object motions. State-of-the-art literature informs two main formulations for Dynamic SLAM, representing dynamic object points in either the world or object coordinate frame. While expressing object points in their local reference frame may seem intuitive, it does not necessarily lead to the most accurate and robust solutions. This paper conducts and presents a thorough analysis of various Dynamic SLAM formulations, identifying the best approach to address the problem. To this end, we introduce a front-end agnostic framework using GTSAM [1] that can be used to evaluate various Dynamic SLAM formulations. 1},
  archive   = {C_ICRA},
  author    = {Jesse Morris and Yiduo Wang and Viorela Ila},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610840},
  month     = {5},
  pages     = {13755-13761},
  title     = {The importance of coordinate frames in dynamic SLAM},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-LIO: A lightweight multiple LiDAR-inertial odometry
system. <em>ICRA</em>, 13748–13754. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The integration of multiple LiDAR sensors has the potential to significantly enhance odometry systems by providing comprehensive environmental measurements. However, current multiple LiDAR-inertial odometry frameworks face challenges in real-time processing due to the voluminous data generated. This paper introduces a real-time, computationally efficient multiple LiDAR-inertial odometry system (Multi-LIO) that outperforms existing state-of-the-art solutions in accuracy and scalability. Utilizing a novel parallel strategy for state updates and a voxelized map format, Multi-LIO optimizes computational efficiency. Furthermore, we introduce a point-wise uncertainty estimation method to augment the accuracy of scan-to-map registration, particularly in large-scale and complex scenarios. We validate our system’s performance through extensive experiments on various challenging sequences. Multi-LIO emerges as a robust, scalable, and extensible solution, adaptable to various LiDAR configurations.},
  archive   = {C_ICRA},
  author    = {Qi Chen and Guanghao Li and Xiangyang Xue and Jian Pu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611257},
  month     = {5},
  pages     = {13748-13754},
  title     = {Multi-LIO: A lightweight multiple LiDAR-inertial odometry system},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LIO-EKF: High frequency LiDAR-inertial odometry using
extended kalman filters. <em>ICRA</em>, 13741–13747. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Odometry estimation is crucial for every autonomous system requiring navigation in an unknown environment. In modern mobile robots, 3D LiDAR-inertial systems are often used for this task. By fusing LiDAR scans and IMU measurements, these systems can reduce the accumulated drift caused by sequentially registering individual LiDAR scans and provide a robust pose estimate. Although effective, LiDAR-inertial odometry systems require proper parameter tuning to be deployed. In this paper, we propose LIO-EKF, a tightly-coupled LiDAR-inertial odometry system based on point-to-point registration and the classical extended Kalman filter scheme. We propose an adaptive data association that considers the relative pose uncertainty, the map discretization errors, and the LiDAR noise. In this way, we can substantially reduce the parameters to tune for a given type of environment. The experimental evaluation suggests that the proposed system performs on par with the state-of-the-art LiDAR-inertial odometry pipelines but is significantly faster in computing the odometry. The source code of our implementation is publicly available (https://github.com/YibinWu/LIO-EKF).},
  archive   = {C_ICRA},
  author    = {Yibin Wu and Tiziano Guadagnino and Louis Wiesmann and Lasse Klingbeil and Cyrill Stachniss and Heiner Kuhlmann},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610667},
  month     = {5},
  pages     = {13741-13747},
  title     = {LIO-EKF: High frequency LiDAR-inertial odometry using extended kalman filters},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast and consistent covariance recovery for sliding-window
optimization-based VINS. <em>ICRA</em>, 13724–13731. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce a novel and efficient technique for consistent covariance recovery in nonlinear optimization-based Visual-Inertial Navigation Systems (VINS). Estimating uncertainty in real-time is crucial for evaluating system performance and enhancing downstream operations such as data association. However accessing the marginal covariance of the state variables of interest in optimization-based VINS presents a significant challenge – a computational bottleneck due to the need to invert the high-dimensional information (Hessian) matrix. In our recent work [1], the First-Estimates Jacobian (FEJ) methodology was used to properly fix state linearization points in the optimization-based VINS, which seems counter-intuitive but improves the estimation performance in both consistency and accuracy. Capitalizing on this unique aspect of the FEJ strategy, in this work we carefully design the covariance recovery algorithm to improve efficiency by avoiding redundant computation. Remarkably, our approach achieves a computational speed that is 4-10 times faster than the existing methods. Through comprehensive numerical evaluations across four state-of-the-art marginalization archetypes, we not only affirm the consistency of our covariance estimates but underscore its superior computational efficiency.},
  archive   = {C_ICRA},
  author    = {Chuchu Chen and Yuxiang Peng and Guoquan Huang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610360},
  month     = {5},
  pages     = {13724-13731},
  title     = {Fast and consistent covariance recovery for sliding-window optimization-based VINS},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Outram: One-shot global localization via triangulated scene
graph and global outlier pruning. <em>ICRA</em>, 13717–13723. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One-shot LiDAR localization refers to the ability to estimate the robot pose from one single point cloud, which yields significant advantages in initialization and relocalization processes. In the point cloud domain, the topic has been extensively studied as a global descriptor retrieval (i.e., loop closure detection) and pose refinement (i.e., point cloud registration) problem both in isolation or combined. However, few have explicitly considered the relationship between candidate retrieval and correspondence generation in pose estimation, leaving them brittle to substructure ambiguities. To this end, we propose a hierarchical one-shot localization algorithm called Outram that leverages substructures of 3D scene graphs for locally consistent correspondence searching and global substructure-wise outlier pruning. Such a hierarchical process couples the feature retrieval and the correspondence extraction to resolve the substructure ambiguities by conducting a local-to-global consistency refinement. We demonstrate the capability of Outram in a variety of scenarios in multiple large-scale outdoor datasets. Our implementation is open-sourced: https://github.com/Pamphlett/Outram.},
  archive   = {C_ICRA},
  author    = {Pengyu Yin and Haozhi Cao and Thien-Minh Nguyen and Shenghai Yuan and Shuyang Zhang and Kangcheng Liu and Lihua Xie},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610206},
  month     = {5},
  pages     = {13717-13723},
  title     = {Outram: One-shot global localization via triangulated scene graph and global outlier pruning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Do we need scan-matching in radar odometry? <em>ICRA</em>,
13710–13716. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There is a current increase in the development of &quot;4D&quot; Doppler-capable radar and lidar range sensors that produce 3D point clouds where all points also have information about the radial velocity relative to the sensor. 4D radars in particular are interesting for object perception and navigation in low-visibility conditions (dust, smoke) where lidars and cameras typically fail. With the advent of high-resolution Doppler-capable radars comes the possibility of estimating odometry from single point clouds, foregoing the need for scan registration which is error-prone in feature-sparse field environments. We compare several odometry estimation methods, from direct integration of Doppler/IMU data and Kalman filter sensor fusion to 3D scan-to-scan and scan-to-map registration, on three datasets with data from two recent 4D radars and two IMUs. Surprisingly, our results show that the odometry from Doppler and IMU data alone give similar or better results than 3D point cloud registration. In our experiments, the position drift can be as low as 0.9% over 1.8 and 4.5km trajectories. That allows accurate estimation of 6-DOF ego-motion over long distances also in feature-sparse mine environments. These results are useful not least for applications of navigation with resource-constrained robot platforms in feature-sparse and low-visibility conditions such as mining, construction, and search &amp; rescue operations.},
  archive   = {C_ICRA},
  author    = {Vladimír Kubelka and Emil Fritz and Martin Magnusson},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610666},
  month     = {5},
  pages     = {13710-13716},
  title     = {Do we need scan-matching in radar odometry?},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Doppler-only single-scan 3D vehicle odometry. <em>ICRA</em>,
13703–13709. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel 3D odometry method that recovers the full motion of a vehicle only from a Doppler-capable range sensor. It leverages the radial velocities measured from the scene, estimating the sensor’s velocity from a single scan. The vehicle’s 3D motion, defined by its linear and angular velocities, is calculated taking into consideration its kinematic model which provides a constraint between the velocity measured at the sensor frame and the vehicle frame.Experiments carried out prove the viability of our single-sensor method compared to mounting an additional IMU. Our method provides a more reliable translation of the sensor, compared to the errors linked to IMUs due to noise and biases. Its short-term accuracy and fast operation (∼5ms) make it a proper candidate to supply the initialization to more complex localization algorithms or mapping pipelines. Not only does it reduce the error of the mapper, but it does so at a comparable level of accuracy as an IMU would. All without the need to mount and calibrate an extra sensor on the vehicle.},
  archive   = {C_ICRA},
  author    = {Andres Galeote-Luque and Vladimír Kubelka and Martin Magnusson and Jose-Raul Ruiz-Sarmiento and Javier Gonzalez-Jimenez},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611199},
  month     = {5},
  pages     = {13703-13709},
  title     = {Doppler-only single-scan 3D vehicle odometry},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-camera asynchronous ball localization and trajectory
prediction with factor graphs and human poses. <em>ICRA</em>,
13695–13702. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The rapid and precise localization and prediction of a ball are critical for developing agile robots in ball sports, particularly in sports like tennis characterized by high-speed ball movements and powerful spins. The Magnus effect induced by spin adds complexity to trajectory prediction during flight and bounce dynamics upon contact with the ground. In this study, we introduce an innovative approach that combines a multi-camera system with factor graphs for real-time and asynchronous 3D tennis ball localization. Additionally, we estimate hidden states like velocity and spin for trajectory prediction. Furthermore, to enhance spin inference early in the ball’s flight, where limited observations are available, we integrate human pose data using a temporal convolutional network (TCN) to compute spin priors within the factor graph. This refinement provides more accurate spin priors at the beginning of the factor graph, leading to improved early-stage hidden state inference for prediction. Our results show the trained TCN can predict the spin priors with RMSE of 5.27 Hz. Integrating TCN into the factor graph reduces the prediction error of landing positions by over 63.6% compared to a baseline method that utilized an adaptive extended Kalman filter.},
  archive   = {C_ICRA},
  author    = {Qingyu Xiao and Zulfiqar Zaidi and Matthew Gombolay},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610631},
  month     = {5},
  pages     = {13695-13702},
  title     = {Multi-camera asynchronous ball localization and trajectory prediction with factor graphs and human poses},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiple update particle filter: Position estimation by
combining GNSS pseudorange and carrier phase observations.
<em>ICRA</em>, 13680–13686. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an efficient method for updating particles in a particle filter (PF) to address the position estimation problem when dealing with sharp-peaked likelihood functions derived from multiple observations. Sharp-peaked likelihood functions commonly arise from millimeter-accurate distance observations of carrier phases in the global navigation satellite system (GNSS). However, when such likelihood functions are used for particle weight updates, the absence of particles within the peaks leads to all particle weights becoming zero. To overcome this problem, in this study, a straightforward and effective approach is introduced for updating particles when dealing with sharp-peaked likelihood functions obtained from multiple observations. The proposed method, termed as the multiple update PF, leverages prior knowledge regarding the spread of distribution for each likelihood function and conducts weight updates and resampling iteratively in the particle update process, prioritizing the likelihood function spreads. Experimental results demonstrate the efficacy of our proposed method, particularly when applied to position estimation utilizing GNSS pseudorange and carrier phase observations. The multiple update PF exhibits faster convergence with fewer particles when compared to the conventional PF. Moreover, vehicle position estimation experiments conducted in urban environments reveal that the proposed method outperforms conventional GNSS positioning techniques, yielding more accurate position estimates.},
  archive   = {C_ICRA},
  author    = {Taro Suzuki},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610209},
  month     = {5},
  pages     = {13680-13686},
  title     = {Multiple update particle filter: Position estimation by combining GNSS pseudorange and carrier phase observations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CLIP-loc: Multi-modal landmark association for global
localization in object-based maps. <em>ICRA</em>, 13673–13679. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper describes a multi-modal data association method for global localization using object-based maps and camera images. In global localization, or relocalization, using object-based maps, existing methods typically resort to matching all possible combinations of detected objects and landmarks with the same object category, followed by inlier extraction using RANSAC or brute-force search. This approach becomes infeasible as the number of landmarks increases due to the exponential growth of correspondence candidates. In this paper, we propose labeling landmarks with natural language descriptions and extracting correspondences based on conceptual similarity with image observations using a Vision Language Model (VLM). By leveraging detailed text information, our approach efficiently extracts correspondences compared to methods using only object categories. Through experiments, we demonstrate that the proposed method enables more accurate global localization with fewer iterations compared to baseline methods, exhibiting its efficiency.},
  archive   = {C_ICRA},
  author    = {Shigemichi Matsuzaki and Takuma Sugino and Kazuhito Tanaka and Zijun Sha and Shintaro Nakaoka and Shintaro Yoshizawa and Kazuhiro Shintani},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611393},
  month     = {5},
  pages     = {13673-13679},
  title     = {CLIP-loc: Multi-modal landmark association for global localization in object-based maps},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multimodal indoor localization using crowdsourced radio
maps. <em>ICRA</em>, 13666–13672. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Indoor Positioning Systems (IPS) traditionally rely on odometry and building infrastructures like WiFi, often supplemented by building floor plans for increased accuracy. However, the limitation of floor plans in terms of availability and timeliness of updates challenges their wide applicability. In contrast, the proliferation of smartphones and WiFi-enabled robots has made crowdsourced radio maps – databases pairing locations with their corresponding Received Signal Strengths (RSS) – increasingly accessible. These radio maps not only provide WiFi fingerprint-location pairs but encode movement regularities akin to the constraints imposed by floor plans. This work investigates the possibility of leveraging these radio maps as a substitute for floor plans in multimodal IPS. We introduce a new framework to address the challenges of radio map inaccuracies and sparse coverage. Our proposed system integrates an uncertainty-aware neural network model for WiFi localization and a bespoken Bayesian fusion technique for optimal fusion. Extensive evaluations on multiple real-world sites indicate a significant performance enhancement, with results showing ∼ 25% improvement over the best baseline.},
  archive   = {C_ICRA},
  author    = {Zhaoguang Yi and Xiangyu Wen and Qiyue Xia and Peize Li and Francisco Zampella and Firas Alsehly and Chris Xiaoxuan Lu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610683},
  month     = {5},
  pages     = {13666-13672},
  title     = {Multimodal indoor localization using crowdsourced radio maps},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A CBF-adaptive control architecture for visual navigation
for UAV in the presence of uncertainties. <em>ICRA</em>, 13659–13665.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this article, we propose a control solution for the safe transfer of a quadrotor UAV between two surface robots positioning itself only using the visual features on the surface robots, which enforces safety constraints for precise landing and visual locking, in the presence of modeling uncertainties and external disturbances. The controller handles the ascending and descending phases of the navigation using a visual locking control barrier function (VCBF) and a parametrizable switching descending CBF (DCBF) respectively, eliminating the need for an external planner. The control scheme has a backstepping approach for the position controller with the CBF filter acting on the position kinematics to produce a filtered virtual velocity control input, which an adaptive controller tracks to overcome modeling uncertainties and external disturbances. The experimental validation is carried out with a UAV that navigates from the base to the target using an RGB camera.},
  archive   = {C_ICRA},
  author    = {Viswa Narayanan Sankaranarayanan and Akshit Saradagi and Sumeet Satpute and George Nikolakopoulos},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611530},
  month     = {5},
  pages     = {13659-13665},
  title     = {A CBF-adaptive control architecture for visual navigation for UAV in the presence of uncertainties},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatio-temporal correspondence estimation of growing plants
by hausdorff distance based skeletonization for organ tracking.
<em>ICRA</em>, 13625–13631. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tracking of plant organs over spatio-temporal sequence of point cloud data is one of the demanding tasks of agricultural robotics for automated plant monitoring and growth analysis. Due to the complex geometry of plants, it is extremely difficult to identify and track the individual organs in different growth stages of plants. In this paper, we present an approach to perform correspondence estimation of different plant organs over a series of spatio-temporal data. The approach is based on two stages. In the first stage we develop a robust skeleton extraction method from unstructured plant point cloud data by adopting Hausdorff distance metric and modified breadth first search algorithm. The proposed skeletonization method is shown to be performing better than state-of-the-art, especially in handling very thin and delicate branches. We also address an overlooked problem of connecting skeleton points in the form of a graph, and demonstrate that different types of plant phenotype parameters can be obtained in a fully automatic manner from the skeleton graph. In the second stage, we exploit the skeleton graphs in developing an algorithm to perform correspondence estimation among the skeleton nodes using a cosine similarity based approach. We demonstrate the effectiveness of the proposed skeletonization technique in tracking different organs of the plant by finding good quality correspondences. Experiments are performed on three datasets on real and synthetic sequence of spatio-temporal plant point cloud data to demonstrate the effectiveness of the proposed method.},
  archive   = {C_ICRA},
  author    = {Sharmistha B Pandey and David Colliaux and Ayan Chaudhury},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610490},
  month     = {5},
  pages     = {13625-13631},
  title     = {Spatio-temporal correspondence estimation of growing plants by hausdorff distance based skeletonization for organ tracking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Few-shot fruit segmentation via transfer learning.
<em>ICRA</em>, 13618–13624. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Advancements in machine learning, computer vision, and robotics have paved the way for transformative solutions in various domains, particularly in agriculture. For example, accurate identification and segmentation of fruits from field images plays a crucial role in automating jobs such as harvesting, disease detection, and yield estimation. However, achieving robust and precise infield fruit segmentation remains a challenging task since large amounts of labeled data are required to handle variations in fruit size, shape, color, and occlusion. In this paper, we develop a few-shot semantic segmentation framework for infield fruits using transfer learning. Concretely, our work is aimed at addressing agricultural domains that lack publicly available labeled data. Motivated by similar success in urban scene parsing, we propose specialized pre-training using a public benchmark dataset for fruit transfer learning. By leveraging pre-trained neural networks, accurate semantic segmentation of fruit in the field is achieved with only a few labeled images. Furthermore, we show that models with pre-training learn to distinguish between fruit still on the trees and fruit that have fallen on the ground, and they can effectively transfer the knowledge to the target fruit dataset.},
  archive   = {C_ICRA},
  author    = {Jordan A. James and Heather K. Manching and Amanda M. Hulse-Kemp and William J. Beksi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610003},
  month     = {5},
  pages     = {13618-13624},
  title     = {Few-shot fruit segmentation via transfer learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancement on target-gripper alignment: A tomato harvesting
robot with dual-camera image-based visual servoing. <em>ICRA</em>,
13611–13617. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automation application in crop harvesting has increased in the past decades. Various types of harvesting robots are emerging in both commercial and research areas. One of the main challenges is the precision alignment of the gripper and the target crop. An undesired dislocation can harm both the gripper and the crop, which is mainly caused by uncertainties from the sensors and the manipulator. To solve the problem, the dual-camera setup is designed and implemented on a self-built robot. The perception of the tomato is done by a fixed depth camera and a camera without depth on the gripper. The proposed dual-camera image-based visual servoing (IBVS) controller is designed to deal with the image feedback from both cameras and the proof of asymptotically convergence is provided. Furthermore, the cumulative error compensation reduces the time for the harvesting process. The experiments were conducted in the greenhouse and tested under various conditions. The time cost is formulated as a function and the success picking rate of tomatoes is 68.4%.},
  archive   = {C_ICRA},
  author    = {Lu-Ching Wang and Yen-Cheng Chu and Yennun Huang and Feng-Li Lian},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610772},
  month     = {5},
  pages     = {13611-13617},
  title     = {Enhancement on target-gripper alignment: A tomato harvesting robot with dual-camera image-based visual servoing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning multi-scale context mask-RCNN network for slant
angled aerial imagery in instance segmentation in a Sim2Real setup.
<em>ICRA</em>, 13573–13580. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While instance segmentation models excel at object detection in satellite imagery, their performance drops when applied to slant-angled aerial images due to occlusion and scale variation. This is mainly caused by a lack of training data for such diverse viewpoints and scales. To address this limitation, we propose the Sim2Real-based Multi-Scale Context Mask-RCNN (MSC-RCNN) network, specifically designed for slant-angled aerial imagery. Sim2Real-based transfer learning is adapted to compensate for the limited availability of real-world slant-angle training data. A synthetic dataset is generated using Unreal Engine, detailing the methodology of replicating the real-world scene, for producing diverse slant-angle drone datasets with various weather conditions and backgrounds. The model leverages two distinct feature pyramid backbones, with one incorporating dilated convolutions to address large-scale objects and the other optimized for regular convolutions. Their outputs are fused to effectively detect objects across various scales and angles. Through experiments, it was demonstrated that incorporating this synthetic data significantly reduces reliance on real data while maintaining high mean Average Precision (mAP) scores. Compared to the baseline Mask R-CNN, the proposed approach with Sim2Real adaptation and the MSC-RCNN architecture achieves a remarkable 7.6% performance improvement in instance segmentation accuracy with only a 6% increase in model size. Code can be found at: https://github.com/MSC-RCNN},
  archive   = {C_ICRA},
  author    = {Qiranul Saadiyean and S P Samprithi and Suresh Sundaram},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610358},
  month     = {5},
  pages     = {13573-13580},
  title     = {Learning multi-scale context mask-RCNN network for slant angled aerial imagery in instance segmentation in a Sim2Real setup},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Autonomous exploration of unknown 3D environments using a
frontier-based collector strategy. <em>ICRA</em>, 13566–13572. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous exploration using unmanned aerial vehicles (UAVs) is essential for various tasks such as building inspections, rescue operations, deliveries, and warehousing. However, there are two main limitations to previous approaches: they may not be able to provide a complete map of the environment and assume that the map built during exploration is accurate enough for safe navigation, which is usually not the case. To address these limitations, a novel exploration method is proposed that combines frontier-based exploration with a collector strategy that achieves global exploration and complete map creation. In each iteration, the collector strategy stores and validates frontiers detected during exploration and selects the next best frontier to navigate to. The collector strategy ensures global exploration by balancing the exploitation of a known map with the exploration of unknown areas. In addition, the online path replanning ensures safe navigation through the map created during motion. The performance of the proposed method is verified by exploring 3D simulation environments in comparison with the state-of-the-art methods. Finally, the proposed approach is validated in a real-world experiment.},
  archive   = {C_ICRA},
  author    = {Iván D. Changoluisa Caiza and Ana Milas and Marco A. Montes Grova and Francisco Javier Pérez-Grau and Tamara Petrovic},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611139},
  month     = {5},
  pages     = {13566-13572},
  title     = {Autonomous exploration of unknown 3D environments using a frontier-based collector strategy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Autonomous overhead powerline recharging for uninterrupted
drone operations. <em>ICRA</em>, 13551–13557. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a fully autonomous self-recharging drone system capable of long-duration sustained operations near powerlines. The drone is equipped with a robust onboard perception and navigation system that enables it to locate powerlines and approach them for landing. A passively actuated gripping mechanism grasps the powerline cable during landing after which a control circuit regulates the magnetic field inside a split-core current transformer to provide sufficient holding force as well as battery recharging.The system is evaluated in an active outdoor three-phase powerline environment. We demonstrate multiple contiguous hours of fully autonomous uninterrupted drone operations composed of several cycles of flying, landing, recharging, and takeoff, validating the capability of extended, essentially unlimited, operational endurance.},
  archive   = {C_ICRA},
  author    = {Viet Duong Hoang and Frederik Falk Nyboe and Nicolaj Haarhøj Malle and Emad Ebeid},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611522},
  month     = {5},
  pages     = {13551-13557},
  title     = {Autonomous overhead powerline recharging for uninterrupted drone operations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). N-MPC for deep neural network-based collision avoidance
exploiting depth images. <em>ICRA</em>, 13536–13542. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a Nonlinear Model Predictive Control (N-MPC) framework exploiting a Deep Neural Network for processing onboard-captured depth images for collision avoidance in trajectory-tracking tasks with UAVs. The network is trained on simulated depth images to output a collision score for queried 3D points within the sensor field of view. Then, this network is translated into an algebraic symbolic equation and included in the N-MPC, explicitly constraining predicted positions to be collision-free throughout the receding horizon. The N-MPC achieves real time control of a UAV with a control frequency of 100Hz. The proposed framework is validated through statistical analysis of the collision classifier network, as well as Gazebo simulations and real experiments to assess the resulting capabilities of the N-MPC to effectively avoid collisions in cluttered environments. The associated code is released open-source.},
  archive   = {C_ICRA},
  author    = {Martin Jacquet and Kostas Alexis},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610572},
  month     = {5},
  pages     = {13536-13542},
  title     = {N-MPC for deep neural network-based collision avoidance exploiting depth images},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On robust control laws trade-off analysis for space
manipulators with uncertain parameters and flexible appendages*.
<em>ICRA</em>, 13529–13535. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To accurately accomplish on-orbit tasks using Space Manipulator Systems (SMS), advanced model-based controllers, dependent on the knowledge of SMS parameters, can be employed. However, these parameters may change on orbit for several reasons. Also, during an SMS task, excitation of flexible appendages, such as solar panels, or fuel sloshing may introduce significant end-effector errors. Therefore, controllers robust to parametric uncertainty and disturbances are needed. A robust controller attractive due to its small computational effort is the Linear Parameter Varying (LPV) gain-scheduled controller. However, its design for spatial SMS is not trivial and has not been studied yet. Therefore, the aim of this work is to study and compare robust controllers and examine their applicability to SMS. An LPV plus H ∞ controller is compared with a Model-Based PD, and a Model-Based PD plus H ∞ controller, in the presence of parametric uncertainty, noisy measurements and disturbances, using a planar example. The criteria considered include: (i) Design Complexity, (ii) Trajectory Errors, (iii) Required Torques, and (iv) Computational Effort.},
  archive   = {C_ICRA},
  author    = {Kostas Nanos and Efstathios Chachamis and Evangelos Papadopoulos},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611335},
  month     = {5},
  pages     = {13529-13535},
  title     = {On robust control laws trade-off analysis for space manipulators with uncertain parameters and flexible appendages*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VWDER: A variable wheel-diameter ellipsoidal robot.
<em>ICRA</em>, 13522–13528. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, many researchers have conducted extensive research on spherical robots due to their high flexibility and anti-overturning capabilities. Nevertheless, compared with legged and traditional wheeled robots, spherical robots face certain limitations. The spherical robot is composed of a closed spherical shell structure, which makes the capacity of carrying workloads weak. At the same time, the single point contact with the ground cause the contact friction force with the ground is small, so it is hard to climb obstacles such as steps and doorsill. Therefore, we propose a new solution: the variable wheel diameter ellipsoidal robot (VWDER), which combines the characteristics of two-wheel differential driven robot and spherical robot driven by equivalent pendulum. VWDER is equipped with six retractable shell-shaped legs on each side and this innovative design allows both wheels to independently change diameter while rolling. The main frame of the VWDER can keep the top of the frame facing up under the action of equivalent pendulum during the locomotion, which makes it possible to carry workloads such as manipulator arms, cameras, IMU etc. The VWDER robot can climb steps or doorsill using its two adjacent shell-shaped legs. This paper introduces the design of the VWDER and analyzes the kinematics and dynamics of the VWDER. The experimental results verified the performance of the VWDER, including its autonomous opening and closing, obstacle crossing, automatic reorientation and slope climbing etc.},
  archive   = {C_ICRA},
  author    = {Ziao Qin and Jingzhou Song and Xinglong Gong and Changrui Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610985},
  month     = {5},
  pages     = {13522-13528},
  title     = {VWDER: A variable wheel-diameter ellipsoidal robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Autonomous perching on flat surfaces for free-flying robots
with gecko adhesive gripper. <em>ICRA</em>, 13516–13521. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Gecko-inspired adhesives have the advantage of being able to grasp and release flat surfaces in a vacuum using their microwedge structures. This makes them an especially attractive solution for perching on and grasping flat objects in space for free-flying robots. To grasp and anchor onto these flat surfaces, the gripper must ensure contact between the gecko adhesives and the surface before applying the appropriate forces to activate their adhesion. However, in the case of a free-flying robot in microgravity, physical contact with the surface induces reaction forces, causing the robot to quickly bounce away from the surface. To solve this issue, we propose a simple passive mechanism and a control method of a robotic arm on a free-flying robot with a gecko adhesive gripper. The gripper utilizes a single-motor controlled tendon-driven mechanism mounted at the end of a robotic arm equipped with controllable stiffness joints and a linear spring-damper system. A free-flying robot on an air-bearing platform can successfully perch on a flat surface with a velocity of up to 72.5mm/s and with an approach angle misalignment of up to 33.0 degrees.},
  archive   = {C_ICRA},
  author    = {Daichi Hirano and Nobutaka Tanishima and Tony G. Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610685},
  month     = {5},
  pages     = {13516-13521},
  title     = {Autonomous perching on flat surfaces for free-flying robots with gecko adhesive gripper},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhanced multifunctional interface for reconfigurability of
robotic teams in planetary applications. <em>ICRA</em>, 13509–13515. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Exploration missions on extra terrestrial celestial bodies are to date performed by complex and heavy robotic systems. The trend is towards lighter modular systems that can be (re)configured in situ according to mission specific requirements. To facilitate flexible configurability, a multifunctional interconnect is used to mechanically couple the involved systems while providing electrical power and data transmission. The paper presents the further development of the reliable electromechanical interface (EMI) from the TransTerrA project, which has been proven in several field tests and reached TRL 4. Docking under loads of up to 550 N has been successfully tested with the new design. The experiments presented include undocking at various inclinations with different loads expected for the application scenario. The maximum determined static load that can be carried by the further developed EMI is 2000 N. In further experiments, new contact blocks responsible for the transfer of electrical power and data were tested for water resistance and resilience to environmental factors, as well as power and data transfer. The obtained results will be helpful in the development of a multi-functional interface suitable for lunar applications and missions having similar challenging environmental conditions.},
  archive   = {C_ICRA},
  author    = {Mehmed Yüksel and Wiebke Brinkmann and Marko Jankovic and Hilmi Doğu Küçüker and Frank Kirchner},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610642},
  month     = {5},
  pages     = {13509-13515},
  title     = {Enhanced multifunctional interface for reconfigurability of robotic teams in planetary applications},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RoboBall: An all-terrain spherical robot with a pressurized
shell. <em>ICRA</em>, 13502–13508. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Spherical robots are a different type of mobility platform. A spherical robot is self-contained within its shell rather than relying on a chassis with wheels to navigate. In this shell, it is completely shielded from dust and the environment. This benefit of geometric simplicity has led to the spherical robot becoming an advantageous option for all-terrain exploration and surveying. This paper focuses on a novel iteration of such a robot with a pressurized pneumatic shell design. A soft robot of this type brings benefits of a passive, compliant contact surface that can affect its performance. However, the added softness of its shell adds new unmodeled dynamics into the system that impair commonly used control schemes. This paper outlines the design and manufacture of a soft, inflatable, spherical shell designed for a robot driven by an internal 2-DOF pendulum. In addition, it presents models for controlling the pendulum and understanding the shell dynamics. The paper concludes with experimental validations of these models and field tests of the system on slopes, gravel, rough grass, and on water.},
  archive   = {C_ICRA},
  author    = {Micah Oevermann and Derek Pravecek and Garrett Jibrail and Rishi Jangale and Robert O. Ambrose},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610555},
  month     = {5},
  pages     = {13502-13508},
  title     = {RoboBall: An all-terrain spherical robot with a pressurized shell},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model design and concept of operations of standard interface
for on-orbit construction. <em>ICRA</em>, 13487–13493. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The construction of large-scale space facilities requires the use of on-orbit construction technology. However, several of its key components, such as standard interface design, compliant control methods, and path planning for multi-branch robots, still need improvement before practical application. This paper presents a comprehensive solution for on-orbit construction tasks, encompassing a novel standard interface, docking control method, and path planning method for space multi-branch robots. Firstly, a novel standard interface is introduced, which features multiple mating modes and a lightweight design. Additionally, a compliant docking method is provided to generate lower contact forces along the Z-direction. Furthermore, for four-armed space robots, a hierarchical planning method is proposed, which innovates in environment map construction and locomotion planning. Specifically, the closed-form Minkowski sum method is employed to solve the robot’s free space, and a concise locomotion method is elucidated based on transition support points. Finally, simulations and experiments are conducted.},
  archive   = {C_ICRA},
  author    = {Jingdong Zhao and Zirui Wang and Ziyi Liu and Liangliang Zhao and Qifan Duan and Hong Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609869},
  month     = {5},
  pages     = {13487-13493},
  title     = {Model design and concept of operations of standard interface for on-orbit construction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). System identification of space manipulator systems and its
implications on robust control performance*. <em>ICRA</em>, 13480–13486.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Space manipulator system (SMS) maneuvers can excite flexible appendages, while fuel sloshing effects impact its dynamics and performance. To predict this behavior and control such systems, sloshing and flexible appendages are modeled. A novel system identification scheme is developed, which identifies all parameters required for the reconstruction of system dynamics despite unmeasurable sloshing and modal states. This is achieved by two identification experiments. In Exp.1 all unmeasurable states are eliminated, while in Exp.2 the unmeasurable sloshing states are eliminated, and a novel estimator is used for the unmeasurable modal states. The significance of accurate SYSID in controller design and performance is demonstrated by simulating a 3D SMS controlled by model-based and robust controllers. In both cases, using the identified parameters results in significant robust control performance enhancement.},
  archive   = {C_ICRA},
  author    = {Georgios Rekleitis and Evangelos Papadopoulos},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610181},
  month     = {5},
  pages     = {13480-13486},
  title     = {System identification of space manipulator systems and its implications on robust control performance*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A flat tendon-driven continuum microrobot for brain
interventions. <em>ICRA</em>, 13466–13471. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Navigating biomedical instruments inside the brain remains challenging and high-risk. The delicate nature of the tissues involved requires the development of cutting-edge robotic technologies to enhance precision and safety. In response to these demands, this paper presents a novel ribbon-shaped, tendon-driven continuum microrobot designed explicitly to navigate through brain tissues. The microrobot has a cross-sectional area of 1 mm 2 , and its design is readily compatible with conventional microfabrication techniques for further miniaturization. The flat geometry aims to provide superior maneuverability and opens up new challenges for modeling and control. We detail the design methodology and fabrication, followed by in vitro characterization and testing within brain tissue phantoms.},
  archive   = {C_ICRA},
  author    = {Lorenzo Noseda and Addison Liu and Lucio Pancaldi and Mahmut Selman Sakar},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611399},
  month     = {5},
  pages     = {13466-13471},
  title     = {A flat tendon-driven continuum microrobot for brain interventions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Magnetic mobile micro-gripping MicroRobots (MMµGRs) with two
independent magnetic actuation modes. <em>ICRA</em>, 13452–13457. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce magnetic mobile micro-gripping microrobots with two independent actuation modes. By aligning two magnets with slight variations in magnetic moment orientations, we create a net magnetic moment for precise position and orientation control through external fields, while harnessing opposing torques on the magnets to induce internal stresses needed for gripping. Our microrobot design features a compliant spring-like structure for significant deflection, enabling a gripping motion under specific magnetic field conditions. Magnet rotation allows precise control over gripper actions, returning to a default state (normally open or closed) when the magnetic field diminishes. This work advances magnetic field-controlled microrobotics, bridging the millimeter-to-micrometer gap. It holds promise for applications in microsurgery, micro-assembly, and microscale exploration.},
  archive   = {C_ICRA},
  author    = {Aaron C. Davis and Emmett Z. Freeman and David J. Cappelleri},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609974},
  month     = {5},
  pages     = {13452-13457},
  title     = {Magnetic mobile micro-gripping MicroRobots (MMµGRs) with two independent magnetic actuation modes},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nature-inspired bubble magnetic microrobots for multimode
locomotion, cargo delivery, imaging, and biosensing. <em>ICRA</em>,
13446–13451. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Wirelessly actuated magnetic microrobots are promising tools in medical applications due to their tiny sizes and attractive robotic properties. However, it remains a huge challenge to integrate sufficient functionalities in a limited volume. Microscopic natural phenomenon is a great reference for current microrobot design, where the underlying intelligence and subtlety spurs related modern artificial systems. Inspired by air bubbles in nature, herein, we report a kind of novel magnetic air bubble microrobots. The air bubble-based structure enables multiple functionalities including cargo delivery, multimode locomotion, micromanipulation, medical imaging, and biosensing. The proposed microrobot is essentially Pickering bubbles composed of magnetic particles and air bubbles. Their hollow structures help produce lighter microrobots with density less than 1 g/cm 3 , enabling buoyancy-based self-propulsion. Buoyancy and magnetic forces actuation enables flexible 3D locomotion in fluidic environments. Experimental results show that the microrobots can be controlled properly for designated assignments. Furthermore, the introduction of air bubble enhances ultrasound imaging, facilitating further in vivo applications. These findings offer a significant microrobot design paradigm by exploiting natural physical intelligence at the small scale.},
  archive   = {C_ICRA},
  author    = {Zichen Xu and Qingsong Xu and Hon Ho Yu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611619},
  month     = {5},
  pages     = {13446-13451},
  title     = {Nature-inspired bubble magnetic microrobots for multimode locomotion, cargo delivery, imaging, and biosensing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Thin-film NiTi microactuator with a magnetic spring for a
tiny launcher mechanism. <em>ICRA</em>, 13439–13445. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we present a thin-film shape memory alloy microactuator with a magnetic spring. This novel actuator design utilizes two permanent magnets and 3D-printed magnet holders to effectively apply a tensile strain on the NiTi thin-film. This actuator is expected to generate 8.7 mN of blocking force, and a free displacement of 30 µm is experimentally characterized. The actuator leverages bare NiTi film (∼ 1 µm thick) for actuation, enabling a high actuator bandwidth up to 50 Hz. A comprehensive analytical model is also studied, which was then validated by comparing to the experimental results. A launcher mechanism was designed and integrated with the NiTi actuator, and this mechanism was used to launch a microscale projectile (a salt grain) thereby demonstrating the relative high power actuation achievable with thin-film NiTi.},
  archive   = {C_ICRA},
  author    = {Sukjun Kim and Sarah Bergbreiter},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610159},
  month     = {5},
  pages     = {13439-13445},
  title     = {Thin-film NiTi microactuator with a magnetic spring for a tiny launcher mechanism},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated assembly by two-fingered microhand for fabrication
of soft magnetic microrobots. <em>ICRA</em>, 13433–13438. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Micro-assembly is an emerging method to fabricate microrobots with multiple modules or particles. However, there is always a lack of a flexible and efficient method to freely create the desired magnetic soft microrobots. In this paper, an automated assembly system based on a two-fingered microhand is presented for fabricating magnetic soft microrobots. Our proposed system can automatically pick and place components to assemble microrobots with a two-fingered micromanipulator, and orient these components through an external magnetic field. The automated assembly has the advantages of high accuracy, high speed, and high success rate. It can endow magnetic microrobots with flexible material selection, arbitrary geometry design, and programable magnetization profile. We can make full use of this system to fabricate multiple magnetic soft microrobots. The experiment results demonstrate that this system can efficiently fabricate microrobots with excellent mechanical properties, which have application potential in robotics, biomedical engineering, and environmental governance.},
  archive   = {C_ICRA},
  author    = {Yue Zhao and Xiaoming Liu and Ruixi Wang and Dan Liu and Masaru Kojima and Qiang Huang and Tatsuo Arai},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611042},
  month     = {5},
  pages     = {13433-13438},
  title     = {Automated assembly by two-fingered microhand for fabrication of soft magnetic microrobots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Constrained passive interaction control: Leveraging
passivity and safety for robot manipulators. <em>ICRA</em>, 13418–13424.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Passivity is necessary for robots to fluidly collaborate and interact with humans physically. Nevertheless, due to the unconstrained nature of passivity-based impedance control laws, the robot is vulnerable to infeasible and unsafe configurations upon physical perturbations. In this paper, we propose a novel control architecture that allows a torque-controlled robot to guarantee safety constraints such as kinematic limits, self-collisions, external collisions and singularities and is passive only when feasible. This is achieved by constraining a dynamical system based impedance control law with a relaxed hierarchical control barrier function quadratic program subject to multiple concurrent, possibly contradicting, constraints. Joint space constraints are formulated from efficient data-driven self- and external ${\mathcal{C}^2}$ collision boundary functions. We theoretically prove constraint satisfaction and show that the robot is passive when feasible. Our approach is validated in simulation and real robot experiments on a 7DoF Franka Research 3 manipulator.},
  archive   = {C_ICRA},
  author    = {Zhiquan Zhang and Tianyu Li and Nadia Figueroa},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610232},
  month     = {5},
  pages     = {13418-13424},
  title     = {Constrained passive interaction control: Leveraging passivity and safety for robot manipulators},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How does perception affect safety: New metrics and strategy.
<em>ICRA</em>, 13411–13417. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Perception plays a pivotal role in enhancing the functionality of autonomous agents. However, the intricate relationship between robotic perception metrics and actuation metrics remains unclear, leading to ambiguity in the development and fine-tuning of perception algorithms. In this paper, we introduce a methodology for quantifying this relationship, taking into account factors such as detection rate, detection quality, and latency. Furthermore, we introduce two novel perception metrics for Human-Robot Collaboration safety predicated upon basic perception metrics: Critical Collision Probability (CCP) and Average Collision Probability (ACP). To validate the utility of these metrics in facilitating algorithm development and tuning, we develop an attentive processing strategy that focuses exclusively on key input features. This approach significantly reduces computational time while preserving a similar level of accuracy. Experimental findings demonstrate that integrating this strategy into an object detector results in a notable maximum reduction of 30.09% in inference time and 26.53% in total time per frame. Additionally, the strategy lowers the CCP and ACP in a baseline model by 11.25% and 13.50%, respectively.},
  archive   = {C_ICRA},
  author    = {Xiaotong Zhang and Jinger Chong and Kamal Youcef-Toumi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610657},
  month     = {5},
  pages     = {13411-13417},
  title     = {How does perception affect safety: New metrics and strategy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrated data-driven inference and planning-based human
motion prediction for safe human-robot interaction. <em>ICRA</em>,
13404–13410. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a unified prediction and planning algorithm for an autonomous vehicle to interact with an uncertain human-driven vehicle. Predicting human motion is challenging due to inherent uncertainties in diverse human internal states, i.e., driving styles and rationality. To address these complexities, we propose a hierarchical prediction strategy that combines data-driven internal state inference and planning-based human motion prediction. First, we employ Long Short Term Memory Networks (LSTM) based inference modules to capture both driving styles and rationality from the observed motion of human driver. With these inferred internal states, we predict the future trajectories of human-driven vehicle by formulating a human planning model as an optimization problem. Lastly, we present a Stochastic Model Predictive Control (SMPC) for the autonomous vehicle to safely interact with the human-driven vehicle while actively inferring human internal states. The simulation results, demonstrating the lane change scenarios, indicate the proposed method outperforms the existing work in both predicting the human motion and achieving the robot’s goal.},
  archive   = {C_ICRA},
  author    = {Youngim Nam and Cheolhyeon Kwon},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611239},
  month     = {5},
  pages     = {13404-13410},
  title     = {Integrated data-driven inference and planning-based human motion prediction for safe human-robot interaction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Legible and proactive robot planning for prosocial
human-robot interactions. <em>ICRA</em>, 13397–13403. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans have a remarkable ability to fluently engage in joint collision avoidance in crowded navigation tasks despite the complexities and uncertainties inherent in human behavior. Underlying these interactions is a mutual understanding that (i) individuals are prosocial, that is, there is equitable responsibility in avoiding collisions, and (ii) individuals should behave legibly, that is, move in a way that clearly conveys their intent to reduce ambiguity in how they intend to avoid others. Toward building robots that can safely and seamlessly interact with humans, we propose a general robot trajectory planning framework for synthesizing legible and proactive behaviors and demonstrate that our robot planner naturally leads to prosocial interactions. Specifically, we introduce the notion of a markup factor to incentivize legible and proactive behaviors and an inconvenience budget constraint to ensure equitable collision avoidance responsibility. We evaluate our approach against well-established multi-agent planning algorithms and show that using our approach produces safe, fluent, and prosocial interactions. We demonstrate the real-time feasibility of our approach with human-in-the-loop simulations. Project page can be found at https://uw-ctrl.github.io/phri/.},
  archive   = {C_ICRA},
  author    = {Jasper Geldenbott and Karen Leung},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611294},
  month     = {5},
  pages     = {13397-13403},
  title     = {Legible and proactive robot planning for prosocial human-robot interactions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Overcoming hand and arm occlusion in human-to-robot
handovers: Predicting safe poses with a multimodal DNN regression model.
<em>ICRA</em>, 13390–13396. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Handovers play a key role in human-robot interactions. However, current research focuses on visible-hand handovers, thereby heavily relying on hand detection. Large objects in human-robot interactions present a unique challenge: they inherently block the person’s hands and arms from the robot&#39;s view. This occlusion raises the robot’s risk of unintended physical contact with the person, leading to discomfort and safety concerns. This study aims to develop a model that can determine a pose for the robot that ensures a handover that avoids physical contact with the person, especially in scenarios when hands and arms are occluded. Toward this goal, a three-branch multimodal Deep Neural Network (DNN) regression model was implemented. First, a robust human-pose keypoints detection to calculate shoulder-elbow angles is applied. Secondly, we extract the refined object’s segmented mask. Thirdly, we compute two intrinsic object properties. The concatenated outputs from these branches pass through extra dense layers, resulting in the prediction of the robot&#39;s 14 arms-joint angles. Compared to an only keypoint data processed-based model, our multimodal approach made a 17.7% accuracy improvement. The experiments highlight each pipeline step’s significance, showing important results even when hands and arms were heavily occluded, adjusting to different variations.},
  archive   = {C_ICRA},
  author    = {Catherine Lollett and Advaith Sriram and Mitsuhiro Kamezaki and Shigeki Sugano},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610777},
  month     = {5},
  pages     = {13390-13396},
  title     = {Overcoming hand and arm occlusion in human-to-robot handovers: Predicting safe poses with a multimodal DNN regression model},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time batched distance computation for time-optimal safe
path tracking. <em>ICRA</em>, 13383–13389. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In human-robot collaboration, there has been a trade-off relationship between the speed of collaborative robots and the safety of human workers. In our previous paper, we introduced a time-optimal path tracking algorithm designed to maximize speed while ensuring safety for human workers [1]. This algorithm runs in real-time and provides the safe and fastest control input for every cycle with respect to ISO standards [2]. However, true optimality has not been achieved due to inaccurate distance computation resulting from conservative model simplification. To attain true optimality, we require a method that can compute distances 1. at many robot configurations to examine along a trajectory 2. in realtime for online robot control 3. as precisely as possible for optimal control. In this paper, we propose a batched, fast and precise distance checking method based on precomputed link-local SDFs. Our method can check distances for 500 waypoints along a trajectory within less than 1 millisecond using a GPU at runtime, making it suited for time-critical robotic control. Additionally, a neural approximation has been proposed to accelerate preprocessing by a factor of 2. Finally, we experimentally demonstrate that our method can navigate a 6-DoF robot earlier than a geometric-primitives-based distance checker in a dynamic and collaborative environment.},
  archive   = {C_ICRA},
  author    = {Shohei Fujii and Quang-Cuong Pham},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610196},
  month     = {5},
  pages     = {13383-13389},
  title     = {Real-time batched distance computation for time-optimal safe path tracking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Safe execution of learned orientation skills with conic
control barrier functions. <em>ICRA</em>, 13376–13382. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the field of Learning from Demonstration (LfD), Dynamical Systems (DSs) have gained significant attention due to their ability to generate real-time motions and reach predefined targets. However, the conventional convergence-centric behavior exhibited by DSs may fall short in safety-critical tasks, specifically, those requiring precise replication of demonstrated trajectories or strict adherence to constrained regions even in the presence of perturbations or human intervention. Moreover, existing DS research often assumes demonstrations solely in Euclidean space, overlooking the crucial aspect of orientation in various applications. To alleviate these shortcomings, we present an innovative approach geared toward ensuring the safe execution of learned orientation skills within constrained regions surrounding a reference trajectory. This involves learning a stable DS on SO(3), extracting time-varying conic constraints from the variability observed in expert demonstrations, and bounding the evolution of the DS with Conic Control Barrier Function (CCBF) to fulfill the constraints. We validated our approach through extensive evaluation in simulation and showcased its effectiveness for a cutting skill in the context of assisted teleoperation.},
  archive   = {C_ICRA},
  author    = {Zheng Shen and Matteo Saveriano and Fares J. Abu-Dakka and Sami Haddadin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611325},
  month     = {5},
  pages     = {13376-13382},
  title     = {Safe execution of learned orientation skills with conic control barrier functions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Safe-by-design digital twins for human-robot interaction: A
use case for humanoid service robots. <em>ICRA</em>, 13369–13375. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Integrating humanoid service mobile robots into human environments presents numerous challenges, primarily concerning the safety of interactions between robots and humans. To address these safety concerns, we propose a novel approach that leverages the capabilities of digital twin technology by tailoring it to incorporate comprehensive and robust safety concepts. This paper introduces a &quot;safe-by-design&quot; digital twin that operates alongside the real twin robot in the loop, engaging real-time safety framework during physical interactions with the surrounding environment, including humans.To validate the effectiveness of our proposed safe-by-design digital twin framework, we conducted experiments using a humanoid service mobile robot alongside simulated human counterparts. Our results demonstrate the capability of the integrated impact safety module within the proposed digital twin approach to limit the velocities of both the robot’s base and arms, adhering to injury biomechanics-based safety thresholds. These findings emphasize the promise of our proposed approach for ensuring the physical safety of humanoid service mobile robots operating in dynamic human environments. It enables the digital twin to preemptively identify potential safety hazards and formulate safe intervention actions to ensure the robot’s compliance with safety regulations, paving the way for safer and more widespread adoption of robotic systems in various service domains.},
  archive   = {C_ICRA},
  author    = {Jon Škerlj and Mazin Hamad and Jean Elsner and Abdeldjallil Naceri and Sami Haddadin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611178},
  month     = {5},
  pages     = {13369-13375},
  title     = {Safe-by-design digital twins for human-robot interaction: A use case for humanoid service robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-objective cross-task learning via goal-conditioned
GPT-based decision transformers for surgical robot task automation.
<em>ICRA</em>, 13362–13368. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Surgical robot task automation has been a promising research topic for improving surgical efficiency and quality. Learning-based methods have been recognized as an interesting paradigm and been increasingly investigated. However, existing approaches encounter difficulties in long-horizon goal-conditioned tasks due to the intricate compositional structure, which requires decision-making for a sequence of sub-steps and understanding of inherent dynamics of goal-reaching tasks. In this paper, we propose a new learning-based framework by leveraging the strong reasoning capability of the GPT-based architecture to automate surgical robotic tasks. The key to our approach is developing a goal-conditioned decision transformer to achieve sequential representations with goal-aware future indicators in order to enhance temporal reasoning. Moreover, considering to exploit a general understanding of dynamics inherent in manipulations, thus making the model’s reasoning ability to be task-agnostic, we also design a cross-task pretraining paradigm that uses multiple training objectives associated with data from diverse tasks. We have conducted extensive experiments on 10 tasks using the surgical robot learning simulator SurRoL [1]. The results show that our new approach achieves promising performance and task versatility compared to existing methods. The learned trajectories can be deployed on the da Vinci Research Kit (dVRK) for validating its practicality in real surgical robot settings. Our project website is at: https://med-air.github.io/SurRoL.},
  archive   = {C_ICRA},
  author    = {Jiawei Fu and Yonghao Long and Kai Chen and Wang Wei and Qi Dou},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611051},
  month     = {5},
  pages     = {13362-13368},
  title     = {Multi-objective cross-task learning via goal-conditioned GPT-based decision transformers for surgical robot task automation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Surgical gym: A high-performance GPU-based platform for
reinforcement learning with surgical robots. <em>ICRA</em>, 13354–13361.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in robot-assisted surgery have resulted in progressively more precise, efficient, and minimally invasive procedures, sparking a new era of robotic surgical intervention. This enables doctors, in collaborative interaction with robots, to perform traditional or minimally invasive surgeries with improved outcomes through smaller incisions. Recent efforts are working toward making robotic surgery more autonomous which has the potential to reduce variability of surgical outcomes and reduce complication rates. Deep reinforcement learning methodologies offer scalable solutions for surgical automation, but their effectiveness relies on extensive data acquisition due to the absence of prior knowledge in successfully accomplishing tasks. Due to the intensive nature of simulated data collection, previous works have focused on making existing algorithms more efficient. In this work, we focus on making the simulator more efficient, making training data much more accessible than previously possible. We introduce Surgical Gym, an open-source high performance platform for surgical robot learning where both the physics simulation and reinforcement learning occur directly on the GPU. We demonstrate between 100-5000× faster training times compared with previous surgical learning platforms. The code is available at: https://github.com/SamuelSchmidgall/SurgicalGym.},
  archive   = {C_ICRA},
  author    = {Samuel Schmidgall and Axel Krieger and Jason Eshraghian},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610448},
  month     = {5},
  pages     = {13354-13361},
  title     = {Surgical gym: A high-performance GPU-based platform for reinforcement learning with surgical robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Realistic data generation for 6D pose estimation of surgical
instruments. <em>ICRA</em>, 13347–13353. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automation in surgical robotics has the potential to improve patient safety and surgical efficiency, but it is difficult to achieve due to the need for robust perception algorithms. In particular, 6D pose estimation of surgical instruments is critical to enable the automatic execution of surgical maneuvers based on visual feedback. In recent years, supervised deep learning algorithms have shown increasingly better performance at 6D pose estimation tasks; yet, their success depends on the availability of large amounts of annotated data. In household and industrial settings, synthetic data, generated with 3D computer graphics software, has been shown as an alternative to minimize annotation costs of 6D pose datasets. However, this strategy does not translate well to surgical domains as commercial graphics software have limited tools to generate images depicting realistic instrument-tissue interactions. To address these limitations, we propose an improved simulation environment for surgical robotics that enables the automatic generation of large and diverse datasets for 6D pose estimation of surgical instruments. Among the improvements, we developed an automated data generation pipeline and an improved surgical scene. To show the applicability of our system, we generated a dataset of 7.5k images with pose annotations of a surgical needle that was used to evaluate a state-of-the-art pose estimation network. The trained model obtained a mean translational error of 2.59mm on a challenging dataset that presented varying levels of occlusion. These results highlight our pipeline’s success in training and evaluating novel vision algorithms for surgical robotics applications.},
  archive   = {C_ICRA},
  author    = {Juan Antonio Barragan and Jintan Zhang and Haoying Zhou and Adnan Munawar and Peter Kazanzides},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611638},
  month     = {5},
  pages     = {13347-13353},
  title     = {Realistic data generation for 6D pose estimation of surgical instruments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multimodal transformers for real-time surgical activity
prediction. <em>ICRA</em>, 13323–13330. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-time recognition and prediction of surgical activities are fundamental to advancing safety and autonomy in robot-assisted surgery. This paper presents a multimodal transformer architecture for real-time recognition and prediction of surgical gestures and trajectories based on short segments of kinematic and video data. We conduct an ablation study to evaluate the impact of fusing different input modalities and their representations on gesture recognition and prediction performance. We perform an end-to-end assessment of the proposed architecture using the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS) dataset. Our model outperforms the state-of-the-art (SOTA) with 89.5% accuracy for gesture prediction through effective fusion of kinematic features with spatial and contextual video features. It achieves the real-time performance of 1.1-1.3ms for processing a 1-second input window by relying on a computationally efficient model.},
  archive   = {C_ICRA},
  author    = {Keshara Weerasinghe and Seyed Hamid Reza Roodabeh and Kay Hutchinson and Homa Alemzadeh},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611048},
  month     = {5},
  pages     = {13323-13330},
  title     = {Multimodal transformers for real-time surgical activity prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Markerless ultrasnd probe pose estimation in mini-invasive
surgery. <em>ICRA</em>, 13293–13299. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In mini-invasive surgery, the laparoscopic ultra-sound probe is visible in the laparoscopic image. We address the problem of estimating the probe pose with respect to the laparoscope without using markers and additional sensors. We propose the first method using a single standard laparoscopic monocular RGB image. It is robust, initialization-free and runs at 10 fps, thus forming a promising tool to improve robotic and augmented reality-based surgery.},
  archive   = {C_ICRA},
  author    = {Mohammad Mahdi Kalantari and Erol Ozgur and Mohammad Alkhatib and Emmanuel Buc and Bertrand Le Roy and Richard Modrzejewski and Youcef Mezouar and Adrien Bartoli},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610837},
  month     = {5},
  pages     = {13293-13299},
  title     = {Markerless ultrasnd probe pose estimation in mini-invasive surgery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ReLU-QP: A GPU-accelerated quadratic programming solver for
model-predictive control. <em>ICRA</em>, 13285–13292. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present ReLU-QP, a GPU-accelerated solver for quadratic programs (QPs) that is capable of solving high-dimensional control problems at real-time rates. ReLU-QP is derived by exactly reformulating the Alternating Direction Method of Multipliers (ADMM) algorithm for solving QPs as a deep, weight-tied neural network with rectified linear unit (ReLU) activations. This reformulation enables the deployment of ReLU-QP on GPUs using standard machine-learning toolboxes. We evaluate the performance of ReLU-QP across three model-predictive control (MPC) benchmarks: stabilizing random linear dynamical systems with control limits, balancing an Atlas humanoid robot on a single foot, and performing a whole-body pick-up motion on a quadruped equipped with a six-degree-of-freedom arm. These benchmarks indicate that ReLU-QP is competitive with state-of-the-art CPU-based solvers for small-to-medium-scale problems and offers order-of-magnitude speed improvements for larger-scale problems.},
  archive   = {C_ICRA},
  author    = {Arun L. Bishop and John Z. Zhang and Swaminathan Gurumurthy and Kevin Tracy and Zachary Manchester},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611249},
  month     = {5},
  pages     = {13285-13292},
  title     = {ReLU-QP: A GPU-accelerated quadratic programming solver for model-predictive control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust co-design of canonical underactuated systems for
increased certifiable stability. <em>ICRA</em>, 13271–13277. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Optimal behaviours of a system to perform a specific task can be achieved by leveraging the coupling between trajectory optimization, stabilization, and design optimization. This approach is particularly advantageous for underactuated systems, which are systems that have fewer actuators than degrees of freedom and thus require for more elaborate control systems. This paper proposes a novel co-design algorithm, namely Robust Trajectory Control with Design optimization (RTC-D). An inner optimization layer (RTC) simultaneously performs direct transcription (DIRTRAN) to find a nominal trajectory while computing optimal hyperparameters for a stabilizing time-varying linear quadratic regulator (TVLQR). RTC-D augments RTC with a design optimization layer, maximizing the system’s robustness through a time-varying Lyapunov-based region of attraction (ROA) analysis. This analysis provides a formal guarantee of stability for a set of off-nominal states. The proposed algorithm has been tested on two different underactuated systems: the torque-limited simple pendulum and the cart-pole. Extensive simulations of off-nominal initial conditions demonstrate improved robustness, while real-system experiments show increased insensitivity to torque disturbances.},
  archive   = {C_ICRA},
  author    = {Federico Girlanda and Lasse Shala and Shivesh Kumar and Frank Kirchner},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611645},
  month     = {5},
  pages     = {13271-13277},
  title     = {Robust co-design of canonical underactuated systems for increased certifiable stability},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust balancing control of biped robots for external
forces. <em>ICRA</em>, 13257–13262. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper develops a controller synthesis method for ensuring an admissible bound of external forces on biped robots in a desired level. We first introduce the authors’ preceding results on the norm-based stability criterion for a biped walking constructed on its linear inverted pendulum model (LIPM). More precisely, an induced norm can be taken to formulate the fact that the balance for a biped robot is achieved if its zero moment point (ZMP) always stays in the supporting region at each step. Based on this norm-based criterion, we aim at making the maximum energy of external forces admissible for balancing the biped robot be a pregiven desired bound γ(&gt; 0). To achieve this objective, a robust controller is designed through the linear matrix inequality (LMI)-based approach. More importantly, a necessary and sufficient condition for the existence of a robust controller leading to the desired bound is characterized by some LMI conditions. The effectiveness of the overall arguments is validated through some comparative simulation results of a biped walking robot with external forces.},
  archive   = {C_ICRA},
  author    = {Hae Yeon Park and Jung Hoon Kim},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611281},
  month     = {5},
  pages     = {13257-13262},
  title     = {Robust balancing control of biped robots for external forces},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning model predictive control with error dynamics
regression for autonomous racing. <em>ICRA</em>, 13250–13256. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents a novel Learning Model Predictive Control (LMPC) strategy for autonomous racing at the handling limit that can iteratively explore and learn unknown dynamics in high-speed operational domains. We start from existing LMPC formulations and modify the system dynamics learning method. In particular, our approach uses a nominal, global, nonlinear, physics-based model with a local, linear, data-driven learning of the error dynamics. We conducted experiments in simulation and on 1/10th scale hardware, and deployed the proposed LMPC on a full-scale autonomous race car used in the Indy Autonomous Challenge (IAC) with closed loop experiments at the Putnam Park Road Course in Indiana, USA. The results show that the proposed control policy exhibits improved robustness to parameter tuning and data scarcity. Incremental and safety-aware exploration toward the limit of handling and iterative learning of the vehicle dynamics in high-speed domains is observed both in simulations and experiments.},
  archive   = {C_ICRA},
  author    = {Haoru Xue and Edward L. Zhu and John M. Dolan and Francesco Borrelli},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611628},
  month     = {5},
  pages     = {13250-13256},
  title     = {Learning model predictive control with error dynamics regression for autonomous racing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal control of granular material. <em>ICRA</em>,
13234–13241. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The control of granular materials, which are found in many industrial applications, is a challenging open research problem. Granular material systems are complex-behavior (as they could have solid-, fluid-, and gas-like behaviors) and high-dimensional (as they could have many grains/particles with at least 3 DOF in 3D) systems. Recently, a machine learning-based Graph Neural Network (GNN) simulator has been proposed to learn the underlying dynamics. In this paper, we perform optimal control of a rigid body-driven granular material system whose dynamics is learned by a GNN model trained by reduced data generated via a physics-based simulator and Principal Component Analysis (PCA). We use Differential Dynamic Programming (DDP) to obtain optimal control commands that can form granular particles into a target shape. The model and results are shown to be relatively fast and accurate. The control commands are also applied to the ground truth model, i.e., physics-based simulator, to further validate the approach.},
  archive   = {C_ICRA},
  author    = {Yuichiro Aoyama and Amin Haeri and Evangelos A. Theodorou},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611171},
  month     = {5},
  pages     = {13234-13241},
  title     = {Optimal control of granular material},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Autonomous mapless navigation on uneven terrains.
<em>ICRA</em>, 13227–13233. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a new method for autonomous navigation in uneven terrains by utilizing a sparse Gaussian Process (SGP) based local perception model. The SGP local perception model is trained on local ranging observation (pointcloud) to learn the terrain elevation profile and extract the feasible navigation subgoals around the robot. Subsequently, a cost function, which prioritizes the safety of the robot in terms of keeping the robot’s roll and pitch angles bounded within a specified range, is used to select a safety-aware subgoal that leads the robot to its final destination. The algorithm is designed to run in real-time and is intensively evaluated in simulation and real-world experiments. The results compellingly demonstrate that our proposed algorithm consistently navigates uneven terrains with high efficiency and surpasses the performance of other planners. The implementation of our method, including the supplementary video showing the experimental and real-world results, is available at https://rb.gy/3ov2r8.},
  archive   = {C_ICRA},
  author    = {Hassan Jardali and Mahmoud Ali and Lantao Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611669},
  month     = {5},
  pages     = {13227-13233},
  title     = {Autonomous mapless navigation on uneven terrains},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Circular field motion planning for highly-dynamic
multi-robot systems with application to robot soccer. <em>ICRA</em>,
13220–13226. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The rise of autonomous driving in everyday life makes efficient and collision-free motion planning more important than ever. However, multi robot applications in highly dynamic environments still pose hard challenges for state-of-the-art motion planners. In this paper, we present a new iteration of a reactive circular fields motion planner with the focus on simultaneous control of multiple robots in robotic soccer games, which is able to operate omnidirectional robots safely and efficiently despite high measurement delays and inaccuracies. Our extension enables the definition and effective execution of complex tasks in soccer specific problems. We extensively evaluated our planner in several complex simulation environments and experimentally verified the approach in realistic scenarios on real soccer robots. Furthermore, we demonstrated the capabilities of our motion planner during the successful participation in the RoboCup 2022 and 2023.},
  archive   = {C_ICRA},
  author    = {Fabrice Zeug and Marvin Becker and Matthias A. Müller},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611138},
  month     = {5},
  pages     = {13220-13226},
  title     = {Circular field motion planning for highly-dynamic multi-robot systems with application to robot soccer},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the fly robotic-assisted medical instrument planning and
execution using mixed reality. <em>ICRA</em>, 13192–13199. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic-assisted medical systems (RAMS) have gained significant attention for their advantages in alleviating surgeons’ fatigue and improving patients’ outcomes. These systems comprise a range of human-computer interactions, including medical scene monitoring, anatomical target planning, and robot manipulation. However, despite its versatility and effectiveness, RAMS demands expertise in robotics, leading to a high learning cost for the operator. In this work, we introduce a novel framework using mixed reality technologies to ease the use of RAMS. The proposed framework achieves real-time planning and execution of medical instruments by providing 3D anatomical image overlay, human-robot collision detection, and robot programming interface. These features, integrated with an easy-to-use calibration method for head-mounted display, improve the effectiveness of human-robot interactions. To assess the feasibility of the framework, two medical applications are presented in this work: 1) coil placement during transcranial magnetic stimulation and 2) drill and injector device positioning during femoroplasty. Results from these use cases demonstrate its potential to extend to a wider range of medical scenarios.},
  archive   = {C_ICRA},
  author    = {Letian Ai and Yihao Liu and Mehran Armand and Amir Kheradmand and Alejandro Martin-Gomez},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611515},
  month     = {5},
  pages     = {13192-13199},
  title     = {On the fly robotic-assisted medical instrument planning and execution using mixed reality},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal prescribed-time control based reactive planning
system for quadruped robot navigation. <em>ICRA</em>, 13185–13191. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a reactive planning system for quadruped robots based on prescribed-time control. The navigation of the quadruped robot is fundamentally depicted as omnidirectional movements, while a feedback control law is formulated to address any deviations the robot may encounter. In particular, our proposed feedback control system is theoretically proven to achieve convergence within a predefined finite time that is specified by the user. To further compute the optimal convergent time and the local goal state, we present a high-level planning node encompassing terrain-aware kinodynamic search and spatiotemporal trajectory optimization, which can generate collision-free, smooth, and efficient trajectories. The effectiveness of our proposed framework is validated through both numerical simulation and real-robot experiments in indoor and outdoor environments, including scenarios with cluttered obstacles, slopes, and external disturbances.},
  archive   = {C_ICRA},
  author    = {Shaohang Xu and Wentao Zhang and Chin Pang Ho and Lijun Zhu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610336},
  month     = {5},
  pages     = {13185-13191},
  title     = {Optimal prescribed-time control based reactive planning system for quadruped robot navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RETOM: Leveraging maneuverability for reactive tool
manipulation using wrench-fields. <em>ICRA</em>, 13178–13184. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper investigates the problem of effective tool manipulation for motion planning in complex human-like scenarios. Vector-field-based real-time strategies, although widely used, usually do not account for unwieldy tools or incorporate systematic methods to handle these extra maneuvers needed. Instead, we formalize the problem and propose a novel field-based reactive planner that explicitly accounts for rotational forces for seamless maneuvers based on the tool’s geometry and featured points. Furthermore, we capture and encode robot performance through capability metrics and improve the same using an additional quality distribution method. This enables seamless integration of the robot’s embodiment with the reactive force-torque (wrench) field giving rise to flexible tool usage in non-stationary environments. Extensive simulation analysis on a 7 DoF collaborative robot manipulating a common tool in an unorganized table-top layout reinforces our claim of robustness in stationary and non-stationary scenarios.},
  archive   = {C_ICRA},
  author    = {Felix Eberle and Riddhiman Laha and Haowen Yao and Abdeldjallil Naceri and Luis F.C. Figueredo and Sami Haddadin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611140},
  month     = {5},
  pages     = {13178-13184},
  title     = {RETOM: Leveraging maneuverability for reactive tool manipulation using wrench-fields},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spline-interpolated model predictive path integral control
with stein variational inference for reactive navigation. <em>ICRA</em>,
13171–13177. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a reactive navigation method that leverages a Model Predictive Path Integral (MPPI) control enhanced with spline interpolation for the control input sequence and Stein Variational Gradient Descent (SVGD). The MPPI framework addresses a nonlinear optimization problem by determining an optimal sequence of control inputs through a sampling-based approach. The efficacy of MPPI is significantly influenced by the sampling noise. To rapidly identify routes that circumvent large and/or newly detected obstacles, it is essential to employ high levels of sampling noise. However, such high noise levels result in jerky control input sequences, leading to non-smooth trajectories. To mitigate this issue, we propose the integration of spline interpolation within the MPPI process, enabling the generation of smooth control input sequences despite the utilization of substantial sampling noises. Nonetheless, the standard MPPI algorithm struggles in scenarios featuring multiple optimal or near-optimal solutions, such as environments with several viable obstacle avoidance paths, due to its assumption that the distribution over an optimal control input sequence can be closely approximated by a Gaussian distribution. To address this limitation, we extend our method by incorporating SVGD into the MPPI framework with spline interpolation. SVGD, rooted in the optimal transportation algorithm, possesses the unique ability to cluster samples around an optimal solution. Consequently, our approach facilitates robust reactive navigation by swiftly identifying obstacle avoidance paths while maintaining the smoothness of the control input sequences. The efficacy of our proposed method is validated on simulations with a quadrotor, demonstrating superior performance over existing baseline techniques.},
  archive   = {C_ICRA},
  author    = {Takato Miura and Naoki Akai and Kohei Honda and Susumu Hara},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610501},
  month     = {5},
  pages     = {13171-13177},
  title     = {Spline-interpolated model predictive path integral control with stein variational inference for reactive navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Whisker-based tactile navigation algorithm for underground
robots. <em>ICRA</em>, 13164–13170. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work explores the use of artificial whiskers as tactile sensors for enhancing the perception and navigation capabilities of mobile robots in challenging settings such as caves and underground mines. These environments exhibit inconsistent lighting conditions, locally self-similar textures, and general poor visibility conditions, that can cause the performance of state-of-the-art vision-based methods to decline. In order to evaluate the efficacy of tactile sensing in this context, three algorithms were developed and tested with simulated and physical experiments: a wall-follower, a navigation algorithm based on Theta*, and a hybrid approach that combines the two. The obtained results highlight the efficacy of tactile sensing for wall-following in intricate environments. When paired with an external method for pose estimation, it further aids in navigating unknown environments. Moreover, by integrating navigation with wall-following, the third, hybrid algorithm enhanced the map traversal speed by roughly 26−43% compared to standard navigation methods without wall-following.},
  archive   = {C_ICRA},
  author    = {Tanel Kossas and Walid Remmas and Roza Gkliva and Asko Ristolainen and Maarja Kruusmaa},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610762},
  month     = {5},
  pages     = {13164-13170},
  title     = {Whisker-based tactile navigation algorithm for underground robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Waverider: Leveraging hierarchical, multi-resolution maps
for efficient and reactive obstacle avoidance. <em>ICRA</em>,
13157–13163. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fast and reliable obstacle avoidance is an important task for mobile robots. In this work, we propose an efficient reactive system that provides high-quality obstacle avoidance while running at hundreds of hertz with minimal resource usage. Our approach combines wavemap, a hierarchical volumetric map representation, with a novel hierarchical and parallelizable obstacle avoidance algorithm formulated through Riemannian Motion Policies (RMP). Leveraging multi-resolution obstacle avoidance policies, the proposed navigation system facilitates precise, low-latency (36ms), and extremely efficient obstacle avoidance with a very large perceptive radius (30m). We perform extensive statistical evaluations on indoor and outdoor maps, verifying that the proposed system compares favorably to fixed-resolution RMP variants and CHOMP. Finally, the RMP formulation allows the seamless fusion of obstacle avoidance with additional objectives, such as goal-seeking, to obtain a fully-fledged navigation system that is versatile and robust. We deploy the system on a Micro Aerial Vehicle and show how it navigates through an indoor obstacle course. Our complete implementation, called waverider, is made available as open source 1 .},
  archive   = {C_ICRA},
  author    = {Victor Reijgwart and Michael Pantic and Roland Siegwart and Lionel Ott},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610580},
  month     = {5},
  pages     = {13157-13163},
  title     = {Waverider: Leveraging hierarchical, multi-resolution maps for efficient and reactive obstacle avoidance},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UKF-based sensor fusion for joint-torque sensorless humanoid
robots. <em>ICRA</em>, 13150–13156. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a novel sensor fusion based on Unscented Kalman Filtering for the online estimation of joint-torques of humanoid robots without joint-torque sensors. At the feature level, the proposed approach considers multimodal measurements (e.g. currents, accelerations, etc.) and non-directly measurable effects, such as external contacts, thus leading to joint torques readily usable in control architectures for human-robot interaction. The proposed sensor fusion can also integrate distributed, non-collocated force/torque sensors, thus being a flexible framework with respect to the underlying robot sensor suit. To validate the approach, we show how the proposed sensor fusion can be integrated into a two-level torque control architecture aiming at task-space torque-control. The performances of the proposed approach are shown through extensive tests on the new humanoid robot ergoCub, currently being developed at Istituto Italiano di Tecnologia. We also compare our strategy with the existing state-of-the-art approach based on the recursive Newton-Euler algorithm. Results demonstrate that our method achieves low root mean square errors in torque tracking, ranging from 0.05 Nm to 2.5 Nm, even in the presence of external contacts.},
  archive   = {C_ICRA},
  author    = {Ines Sorrentino and Giulio Romualdi and Daniele Pucci},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610951},
  month     = {5},
  pages     = {13150-13156},
  title     = {UKF-based sensor fusion for joint-torque sensorless humanoid robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fall prediction for bipedal robots: The standing phase.
<em>ICRA</em>, 13135–13141. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel approach to fall prediction for bipedal robots, specifically targeting the detection of potential falls while standing caused by abrupt, incipient, and intermittent faults. Leveraging a 1D convolutional neural network (CNN), our method aims to maximize lead time for fall prediction while minimizing false positive rates. The proposed algorithm uniquely integrates the detection of various fault types and estimates the lead time for potential falls. Our contributions include the development of an algorithm capable of detecting abrupt, incipient, and intermittent faults in full-sized robots, its implementation using both simulation and hardware data for a humanoid robot, and a method for estimating lead time. Evaluation metrics, including false positive rate, lead time, and response time, demonstrate the efficacy of our approach. Particularly, our model achieves impressive lead times and response times across different fault scenarios with a false positive rate of 0. The findings of this study hold significant implications for enhancing the safety and reliability of bipedal robotic systems.},
  archive   = {C_ICRA},
  author    = {M. Eva Mungai and Gokul Prabhakaran and Jessy W. Grizzle},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611387},
  month     = {5},
  pages     = {13135-13141},
  title     = {Fall prediction for bipedal robots: The standing phase},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HumanMimic: Learning natural locomotion and transitions for
humanoid robot via wasserstein adversarial imitation. <em>ICRA</em>,
13107–13114. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Transferring human motion skills to humanoid robots remains a significant challenge. In this study, we introduce a Wasserstein adversarial imitation learning system, allowing humanoid robots to replicate natural whole-body locomotion patterns and execute seamless transitions by mimicking human motions. First, we present a unified primitive-skeleton motion retargeting to mitigate morphological differences between arbitrary human demonstrators and humanoid robots. An adversarial critic component is integrated with Reinforcement Learning (RL) to guide the control policy to produce behaviors aligned with the data distribution of mixed reference motions. Additionally, we employ a specific Integral Probabilistic Metric (IPM), namely the Wasserstein-1 distance with a novel soft boundary constraint to stabilize the training process and prevent model collapse. Our system is evaluated on a full-sized humanoid JAXON in the simulator. The resulting control policy demonstrates a wide range of locomotion patterns, including standing, push-recovery, squat walking, humanlike straight-leg walking, and dynamic running. Notably, even in the absence of transition motions in the demonstration dataset, the robot showcases an emerging ability to transit naturally between distinct locomotion patterns as desired speed changes.},
  archive   = {C_ICRA},
  author    = {Annan Tang and Takuma Hiraoka and Naoki Hiraoka and Fan Shi and Kento Kawaharazuka and Kunio Kojima and Kei Okada and Masayuki Inaba},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610449},
  month     = {5},
  pages     = {13107-13114},
  title     = {HumanMimic: Learning natural locomotion and transitions for humanoid robot via wasserstein adversarial imitation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive passive biped dynamic walking on unknown uneven
terrain. <em>ICRA</em>, 13100–13106. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose an adaptive controller for virtual passive biped dynamic walking on unknown uneven terrain. The adaptive controller consists of a trajectory tracking control law developed via backstepping method to mimic reference passive gait, and a slope estimator for the inclination angle of the terrain. In addition, a re-planning approach is introduced to correct the robot state off-track from the reference gait due to the terrain changes. The controller is validated through the simulations on the mixed uneven terrain consisting of varying slopes and steps. The results suggest that the controller shows comparable cost of transport and greater adaptability to terrain changes compared with certain existing methods.},
  archive   = {C_ICRA},
  author    = {Lishen Pu and Yixuan Liu and Aiqun Zheng and Bofeng Qi and Chunquan Xu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610661},
  month     = {5},
  pages     = {13100-13106},
  title     = {Adaptive passive biped dynamic walking on unknown uneven terrain},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Foot shape-dependent resistive force model for bipedal
walkers on granular terrains. <em>ICRA</em>, 13093–13099. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Legged robots have demonstrated high efficiency and effectiveness in unstructured and dynamic environments. However, it is still challenging for legged robots to achieve rapid and efficient locomotion on deformable, yielding substrates, such as granular terrains. We present an enhanced resistive force model for bipedal walkers on soft granular terrains by introducing effective intrusion depth correction. The enhanced force model captures fundamental kinetic results considering the robot foot shape, walking gait speed variation, and energy expense. The model is validated by extensive foot intrusion experiments with a bipedal robot. The results confirm the model accuracy on the given type of granular terrains. The model can be further integrated with the motion control of bipedal robotic walkers.},
  archive   = {C_ICRA},
  author    = {Xunjie Chen and Aditya Anikode and Jingang Yi and Tao Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610190},
  month     = {5},
  pages     = {13093-13099},
  title     = {Foot shape-dependent resistive force model for bipedal walkers on granular terrains},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Keypoints-guided lightweight network for single-view 3D
human reconstruction. <em>ICRA</em>, 13070–13076. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Single-view 3D human reconstruction has been a hot topic due to the potential of wide applications. To achieve high accuracy, existing works usually take computationally intensive models as backbone for exhaustive underlying features and then directly estimate human mesh vertices. These factors lead to redundant parameters, large calculations and low efficiency, while lightweight solutions to address these challenges are relatively scarce. In this work, based on the problems studied above, we propose a keypoints-guided lightweight network with an encoding-decoding framework. As the input is an image, a lightweight backbone named multi-stage and global feature enhanced network is designed for 2D encoding, where some operations of multi-scale fusion and frequency domain filtering are performed to extract more informative but low-resolution features. As the output is mesh of human body, we construct a keypoints-based 3D human template, with which the 2D low-resolution features can be mapped to 3D space to guide the 3D decoding with high efficiency and high accuracy. Extensive experiments on popular benchmarks 3DPW and Human3.6M illustrate the favorable trade-off between the accuracy and complexity of our method. Our code is publicly available at https://github.com/ChrisChenYh/EfficientHuman.git.},
  archive   = {C_ICRA},
  author    = {Yuhang Chen and Chenxing Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610020},
  month     = {5},
  pages     = {13070-13076},
  title     = {Keypoints-guided lightweight network for single-view 3D human reconstruction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self model for embodied intelligence: Modeling full-body
human musculoskeletal system and locomotion control with hierarchical
low-dimensional representation. <em>ICRA</em>, 13062–13069. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modeling and control of the human musculoskele-tal system is important for understanding human motor functions, developing embodied intelligence, and optimizing human-robot interaction systems. However, current human musculoskeletal models are restricted to a limited range of body parts and often with a reduced number of muscles. There is also a lack of algorithms capable of controlling over 600 muscles to generate reasonable human movements. To fill this gap, we build a musculoskeletal model (MS-HUMAN-700) with 90 body segments, 206 joints, and 700 muscle-tendon units, allowing simulation of full-body dynamics and interaction with various devices. We develop a new algorithm using low-dimensional representation and hierarchical deep reinforcement learning to achieve state-of-the-art full-body control. We validate the effectiveness of our model and algorithm in simulations with real human locomotion data. The musculoskeletal model, along with its control algorithm, will be made available to the research community to promote a deeper understanding of human motion control and better design of interactive robots.Project page: https://lnsgroup.cc/research/MS-Human-700},
  archive   = {C_ICRA},
  author    = {Chenhui Zuo and Kaibo He and Jing Shao and Yanan Sui},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610081},
  month     = {5},
  pages     = {13062-13069},
  title     = {Self model for embodied intelligence: Modeling full-body human musculoskeletal system and locomotion control with hierarchical low-dimensional representation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SynthAct: Towards generalizable human action recognition
based on synthetic data. <em>ICRA</em>, 13038–13045. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Synthetic data generation is a proven method for augmenting training sets without the need for extensive setups, yet its application in human activity recognition is underexplored. This is particularly crucial for human-robot collaboration in household settings, where data collection is often privacy-sensitive. In this paper, we introduce SynthAct, a synthetic data generation pipeline designed to significantly minimize the reliance on real-world data. Leveraging modern 3D pose estimation techniques, SynthAct can be applied to arbitrary 2D or 3D video action recordings, making it applicable for uncontrolled in-the-field recordings by robotic agents or smarthome monitoring systems. We present two SynthAct datasets: AMARV, a large synthetic collection with over 800k multi-view action clips, and Synthetic Smarthome, mirroring the Toyota Smarthome dataset. SynthAct generates a rich set of data, including RGB videos and depth maps from four synchronized views, 3D body poses, normal maps, segmentation masks and bounding boxes. We validate the efficacy of our datasets through extensive synthetic-to-real experiments on NTU RGB+D and Toyota Smarthome. SynthAct is available on our project page 4 .},
  archive   = {C_ICRA},
  author    = {David Schneider and Marco Keller and Zeyun Zhong and Kunyu Peng and Alina Roitberg and Jürgen Beyerer and Rainer Stiefelhagen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611486},
  month     = {5},
  pages     = {13038-13045},
  title     = {SynthAct: Towards generalizable human action recognition based on synthetic data},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ESP: Extro-spective prediction for long-term behavior
reasoning in emergency scenarios. <em>ICRA</em>, 13030–13037. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Emergent-scene safety is the key milestone for fully autonomous driving, and reliable on-time prediction is essential to maintain safety in emergency scenarios. However, these emergency scenarios are long-tailed and hard to collect, which restricts the system from getting reliable predictions. In this paper, we build a new dataset, which aims at the longterm prediction with the inconspicuous state variation in history for the emergency event, named the Extro-Spective Prediction (ESP) problem. Based on the proposed dataset, a flexible feature encoder for ESP is introduced to various prediction methods as a seamless plug-in, and its consistent performance improvement underscores its efficacy. Furthermore, a new metric named clamped temporal error (CTE) is proposed to give a more comprehensive evaluation of prediction performance, especially in time-sensitive emergency events of subseconds. Interestingly, as our ESP features can be described in human-readable language naturally, the application of integrating into ChatGPT also shows huge potential. The ESP-dataset and all benchmarks are released at https://dingrui-wang.github.io/ESP-Dataset/.},
  archive   = {C_ICRA},
  author    = {Dingrui Wang and Zheyuan Lai and Yuda Li and Yi Wu and Yuexin Ma and Johannes Betz and Ruigang Yang and Wei Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610002},
  month     = {5},
  pages     = {13030-13037},
  title     = {ESP: Extro-spective prediction for long-term behavior reasoning in emergency scenarios},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MiBOT: A head-worn robot that modulates cardiovascular
responses through human-like soft massage. <em>ICRA</em>, 13023–13029.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Massage therapy is helpful for the rehabilitation of various diseases, such as headaches caused by migraines and stress. Existing robotic systems have focused on massage therapy on the torso and limbs, but performing massage motions through suitable actuation on a person’s head has been a challenge. In this paper, we present MiBOT, a head-worn massage robot that actuates two soft tactors to produce touch motions mimicking human massage. A key design principle behind MiBOT is its silent actuation, which we achieve through pneumatic artificial muscles in conjunction with a controller loop to respond to contact pressure. We evaluated the effectiveness of MiBOT in a controlled study and assessed subjects’ blood pressure and heart rate levels while applying MiBOT. We found that our mechanical system generated positive and conclusive quantitative outcomes that are similar to the human-administered massage, decreasing participants’ mean systolic and diastolic blood pressure by 2.8 mmHg and 1.7 mmHg, respectively, as well as calming their heart rate by 8–10% on average.},
  archive   = {C_ICRA},
  author    = {Alice Mylaeus and Stephanie Vogt and Berken Utku Demirel and Marcel Gort and Mirko Meboldt and Manuel Meier and Christian Holz},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610867},
  month     = {5},
  pages     = {13023-13029},
  title     = {MiBOT: A head-worn robot that modulates cardiovascular responses through human-like soft massage},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards unifying human likeness: Evaluating metrics for
human-like motion retargeting on bimanual manipulation tasks.
<em>ICRA</em>, 13015–13022. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generating human-like robot motions is pivotal for achieving smooth human-robot interactions. Such motions contribute to better predictions of robot motions by humans, thus leading to more intuitive interaction and increased acceptability. Human likeness in robot motions has been conventionally measured and realized via the optimization of human-likeness metrics. However, the abundance of such metrics and the absence of standardized criteria impede their usage in novel contexts. In this work, we introduce a unified human-likeness metric built from a hierarchically weighted sum of individual metrics. The proposed metric is derived from a thorough analysis of eleven existing human-likeness criteria and is applicable across various tasks and robot models. We evaluate its performance in the context of motion retargeting of bimanual tasks with three different humanoid robots.},
  archive   = {C_ICRA},
  author    = {Andre Meixner and Mischa Carl and Franziska Krebs and Noémie Jaquier and Tamim Asfour},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611024},
  month     = {5},
  pages     = {13015-13022},
  title     = {Towards unifying human likeness: Evaluating metrics for human-like motion retargeting on bimanual manipulation tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Wait, that feels familiar: Learning to extrapolate human
preferences for preference-aligned path planning. <em>ICRA</em>,
13008–13014. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous mobility tasks such as last-mile delivery require reasoning about operator-indicated preferences over terrains on which the robot should navigate to ensure both robot safety and mission success. However, coping with out of distribution data from novel terrains or appearance changes due to lighting variations remains a fundamental problem in visual terrain-adaptive navigation. Existing solutions either require labor-intensive manual data re-collection and labeling or use hand-coded reward functions that may not align with operator preferences. In this work, we posit that operator preferences for visually novel terrains, which the robot should adhere to, can often be extrapolated from established terrain preferences within the inertial-proprioceptive-tactile domain. Leveraging this insight, we introduce Preference extrApolation for Terrain-awarE Robot Navigation (PATERN), a novel framework for extrapolating operator terrain preferences for visual navigation. PATERN learns to map inertial-proprioceptive-tactile measurements from the robot’s observations to a representation space and performs nearest-neighbor search in this space to estimate operator preferences over novel terrains. Through physical robot experiments in outdoor environments, we assess PATERN’s capability to extrapolate preferences and generalize to novel terrains and challenging lighting conditions. Compared to baseline approaches, our findings indicate that PATERN 1 robustly generalizes to diverse terrains and varied lighting conditions, while navigating in a preference-aligned manner.},
  archive   = {C_ICRA},
  author    = {Haresh Karnan and Elvin Yang and Garrett Warnell and Joydeep Biswas and Peter Stone},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611475},
  month     = {5},
  pages     = {13008-13014},
  title     = {Wait, that feels familiar: Learning to extrapolate human preferences for preference-aligned path planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CoBT: Collaborative programming of behaviour trees from one
demonstration for robot manipulation. <em>ICRA</em>, 12993–12999. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mass customization and shorter manufacturing cycles are becoming more important among small and medium-sized companies. However, classical industrial robots struggle to cope with product variation and dynamic environments. In this paper, we present CoBT, a collaborative programming by demonstration framework for generating reactive and modular behavior trees. CoBT relies on a single demonstration and a combination of data-driven machine learning methods with logic-based declarative learning to learn a task, thus eliminating the need for programming expertise or long development times. The proposed framework is experimentally validated on 7 manipulation tasks and we show that CoBT achieves ≈ 93% success rate overall with an average of 7.5s programming time. We conduct a pilot study with non-expert users to provide feedback regarding the usability of CoBT. More videos and generated behavior trees are available at: https://github.com/jainaayush2006/CoBT.git.},
  archive   = {C_ICRA},
  author    = {Aayush Jain and Philip Long and Valeria Villani and John D. Kelleher and Maria Chiara Leva},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611654},
  month     = {5},
  pages     = {12993-12999},
  title     = {CoBT: Collaborative programming of behaviour trees from one demonstration for robot manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HAC-SLAM: Human assisted collaborative 3D-SLAM through
augmented reality. <em>ICRA</em>, 12978–12984. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simultaneous Localization and Mapping (SLAM) has emerged as a prime autonomous mobile agent localization algorithm. Despite the global research effort to improve SLAM, its mapping component remains limited and serves little more than to satisfy the coupled localization problem. We present a collaborative 3D SLAM approach leveraging the power of augmented reality (AR). The system introduces a trio of diverse agents, each with its unique capability to become an active member in the mapping process: mobile robots, human operators, and AR head-mounted display (AR-HMD). A 3D complementary mapping pipeline is developed to utilize the built-in SLAM capabilities of the AR-HMD as shareable data. Our system aligns and merges the AR-HMD and the robot’s local map automatically, triggered by a human-dictated initial guess. The created merged map proves advantageous in scenarios where the robot is restricted from navigating in certain areas. To correct map imperfections resulting from problematic objects such as transparent or reflective surfaces, the fused map is overlayed onto the environment, and hand gestures are used to add or delete 3D map features in real-time. Our system is implemented in both a lab and a real industrial warehouse setup. The results show a significant improvement in the map quality and mapping duration.},
  archive   = {C_ICRA},
  author    = {Malak Sayour and Mohamad Karim Yassine and Nadim Dib and Imad H. Elhajj and Boulos Asmar and Elie Khoury and Daniel Asmar},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610377},
  month     = {5},
  pages     = {12978-12984},
  title     = {HAC-SLAM: Human assisted collaborative 3D-SLAM through augmented reality},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human-robot complementary collaboration for flexible and
precision assembly. <em>ICRA</em>, 12971–12977. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses human-robot collaborative (HRC) precision assembly that complements natural human ability and the strength of an autonomous robot system. Our approach enables both flexibility and efficiency of tight-clearance assembly of various complex-shaped parts in the presence of uncertainty without requiring assembly skills and knowledge of robotics from the human operator. We demonstrated the effectiveness of our approach in a variety of experiments and comparisons with other HRC assembly approaches.},
  archive   = {C_ICRA},
  author    = {Shichen Cao and Jing Xiao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610825},
  month     = {5},
  pages     = {12971-12977},
  title     = {Human-robot complementary collaboration for flexible and precision assembly},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Risk-bounded online team interventions via theory of mind.
<em>ICRA</em>, 12964–12970. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite advancements in human-robot teamwork, limited progress was made in developing AI assistants capable of advising teams online during task time, due to the challenges of modeling both individual and collective beliefs of the team members. Dynamic epistemic logic has proved to be a viable tool for representing a machine Theory of Mind and for modeling communication in epistemic planning, with applications to human-robot teamwork. However, this approach has yet to be applied in an online teaming assistance context and fails to account for the real-life probabilities of potential team beliefs. We propose a novel blend of epistemic planning and POMDP techniques to create a risk-bounded AI team assistant, that intervenes only when the team’s expected likelihood of failure exceeds a predefined risk threshold or in the case of potential execution deadlocks. Our experiments and simulated demonstration on the Virtualhome testbed show that the assistant can effectively improve team performance.},
  archive   = {C_ICRA},
  author    = {Yuening Zhang and Paul Robertson and Tianmin Shu and Sungkweon Hong and Brian C. Williams},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609865},
  month     = {5},
  pages     = {12964-12970},
  title     = {Risk-bounded online team interventions via theory of mind},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards proactive safe human-robot collaborations via
data-efficient conditional behavior prediction. <em>ICRA</em>,
12956–12963. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We focus on the problem of how we can enable a robot to collaborate seamlessly with a human partner, specifically in scenarios where preexisting data is sparse. Much prior work in human-robot collaboration uses observational models of humans (i.e. models that treat the robot purely as an observer) to choose the robot’s behavior, but such models do not account for the influence the robot has on the human’s actions, which may lead to inefficient interactions. We instead formulate the problem of optimally choosing a collaborative robot’s behavior based on a conditional model of the human that depends on the robot’s future behavior. First, we propose a novel model-based formulation of conditional behavior prediction that allows the robot to infer the human’s intentions based on its future plan in data-sparse environments. We then show how to utilize a conditional model for proactive goal selection and safe trajectory generation around human collaborators. Finally, we use our proposed proactive controller in a collaborative task with real users to show that it can improve users’ interactions with a robot collaborator quantitatively and qualitatively.},
  archive   = {C_ICRA},
  author    = {Ravi Pandya and Zhuoyuan Wang and Yorie Nakahira and Changliu Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610860},
  month     = {5},
  pages     = {12956-12963},
  title     = {Towards proactive safe human-robot collaborations via data-efficient conditional behavior prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trust-aware motion planning for human-robot collaboration
under distribution temporal logic specifications. <em>ICRA</em>,
12949–12955. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent work has considered trust-aware decision making for human-robot collaboration (HRC) with a focus on model learning. In this paper, we are interested in enabling the HRC system to complete complex tasks specified using temporal logic formulas that involve human trust. Since accurately observing human trust in robots is challenging, we adopt the widely used partially observable Markov decision process (POMDP) framework for modelling the interactions between humans and robots. To specify the desired behaviour, we propose to use syntactically co-safe linear distribution temporal logic (scLDTL), a logic that is defined over predicates of states as well as belief states of partially observable systems. The incorporation of belief predicates in scLDTL enhances its expressiveness while simultaneously introducing added complexity. This also presents a new challenge as the belief predicates must be evaluated over the continuous (infinite) belief space. To address this challenge, we present an algorithm for solving the optimal policy synthesis problem. First, we enhance the belief MDP (derived by reformulating the POMDP) with a probabilistic labelling function. Then a product belief MDP is constructed between the probabilistically labelled belief MDP and the automaton translation of the scLDTL formula. Finally, we show that the optimal policy can be obtained by leveraging existing point-based value iteration algorithms with essential modifications. Human subject experiments with 21 participants on a driving simulator demonstrate the effectiveness of the proposed approach.},
  archive   = {C_ICRA},
  author    = {Pian Yu and Shuyang Dong and Shili Sheng and Lu Feng and Marta Kwiatkowska},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610874},
  month     = {5},
  pages     = {12949-12955},
  title     = {Trust-aware motion planning for human-robot collaboration under distribution temporal logic specifications},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Merging decision transformers: Weight averaging for forming
multi-task policies. <em>ICRA</em>, 12942–12948. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent work has shown the promise of creating generalist, transformer-based, models for language, vision, and sequential decision-making problems. To create such models, we generally require centralized training objectives, data, and compute. It is of interest if we can more flexibly create generalist policies by merging together multiple, task-specific, individually trained policies. In this work, we take a preliminary step in this direction through merging, or averaging, subsets of Decision Transformers in parameter space trained on different MuJoCo locomotion problems, forming multi-task models without centralized training. We also demonstrate the importance of various methodological choices when merging policies, such as utilizing common pre-trained initializations, increasing model capacity, and utilizing Fisher information for weighting parameter importance. In general, we believe research in this direction could help democratize and distribute the process that forms multi-task robotics policies. Our implementation is available at https://github.com/daniellawson9999/merging-decision-transformer.},
  archive   = {C_ICRA},
  author    = {Daniel Lawson and Ahmed H. Qureshi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610919},
  month     = {5},
  pages     = {12942-12948},
  title     = {Merging decision transformers: Weight averaging for forming multi-task policies},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sensorless estimation of contact using deep-learning for
human-robot interaction. <em>ICRA</em>, 12935–12941. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Physical human-robot interaction has been an area of interest for decades. Collaborative tasks, such as joint compliance, demand high-quality joint torque sensing. While external torque sensors are reliable, they come with the drawbacks of being expensive and vulnerable to impacts. To address these issues, studies have been conducted to estimate external torques using only internal signals, such as joint states and current measurements. However, insufficient attention has been given to friction hysteresis approximation, which is crucial for tasks involving extensive dynamic to static state transitions. In this paper, we propose a deep-learning-based method that leverages a novel long-term memory scheme to achieve dynamics identification, accurately approximating the static hysteresis. We also introduce modifications to the well-known Residual Learning architecture, retaining high accuracy while reducing inference time. The robustness of the proposed method is illustrated through a joint compliance and task compliance experiment.},
  archive   = {C_ICRA},
  author    = {Shilin Shan and Quang-Cuong Pham},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609990},
  month     = {5},
  pages     = {12935-12941},
  title     = {Sensorless estimation of contact using deep-learning for human-robot interaction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PBP: Path-based trajectory prediction for autonomous
driving. <em>ICRA</em>, 12927–12934. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trajectory prediction plays a crucial role in the autonomous driving stack by enabling autonomous vehicles to anticipate the motion of surrounding agents. Goal-based prediction models have gained traction in recent years for addressing the multimodal nature of future trajectories. Goal-based prediction models simplify multimodal prediction by first predicting 2D goal locations of agents and then predicting trajectories conditioned on each goal. However, a single 2D goal location serves as a weak inductive bias for predicting the whole trajectory, often leading to poor map compliance, i.e., part of the trajectory going off-road or breaking traffic rules. In this paper, we improve upon goal-based prediction by proposing the Path-based prediction (PBP) approach. PBP predicts a discrete probability distribution over reference paths in the HD map using the path features and predicts trajectories in the path-relative Frenet frame. We applied the PBP trajectory decoder on top of the HiVT scene encoder and report results on the Argoverse dataset. Our experiments show that PBP achieves competitive performance on the standard trajectory prediction metrics, while significantly outperforming state-of-the-art baselines in terms of map compliance.},
  archive   = {C_ICRA},
  author    = {Sepideh Afshar and Nachiket Deo and Akshay Bhagat and Titas Chakraborty and Yunming Shao and Balarama Raju Buddharaju and Adwait Deshpande and Henggang Cui Motional},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610610},
  month     = {5},
  pages     = {12927-12934},
  title     = {PBP: Path-based trajectory prediction for autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised learning of neuro-symbolic rules for
generalizable context-aware planning in object arrangement tasks.
<em>ICRA</em>, 12865–12872. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As robots tackle complex object arrangement tasks, it becomes imperative for them to be able to generalize to complex worlds and scale with number of objects. This work postulates that extracting action primitives, such as push operations, their pre-conditions and effects would enable strong generalization to unseen worlds. Hence, we factorize policy learning as inference of such generic rules, which act as strong priors for predicting actions given the world state. Learnt rules act as propositional knowledge and enable robots to reach goals in a zero-shot method by applying the rules independently and incrementally. However, obtaining hand-engineered rules, such as PDDL descriptions is hard, especially for unseen worlds. This work aims to learn generic, sparse, and context-aware rules that govern action primitives in robotic worlds through human demonstrations in simple domains. We demonstrate that our approach, namely RLAP, is able to extract rules without explicit supervision of rule labels and generate goal-reaching plans in complex Sokoban styled domains that scale with number of objects. RLAP furnishes significantly higher goal reaching rate and shorter planning times compared to the state-of-the-art techniques. The code, dataset, and videos are hosted at https://rule-learning-rlap.github.io/.},
  archive   = {C_ICRA},
  author    = {Siddhant Sharma and Shreshth Tuli and Rohan Paul},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610696},
  month     = {5},
  pages     = {12865-12872},
  title     = {Unsupervised learning of neuro-symbolic rules for generalizable context-aware planning in object arrangement tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ConBaT: Control barrier transformer for safe robot learning
from demonstrations. <em>ICRA</em>, 12857–12864. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large-scale self-supervised models have recently revolutionized our ability to perform a variety of tasks within the vision and language domains. However, using such models for autonomous systems is challenging because of safety requirements: besides executing correct actions, an autonomous agent must also avoid the high cost and potentially fatal critical mistakes. Traditionally, self-supervised training mainly focuses on imitating previously observed behaviors, and the training demonstrations carry no notion of which behaviors should be explicitly avoided. In this work, we propose Control Barrier Transformer (ConBaT), an approach that learns safe behaviors from demonstrations in a self-supervised fashion. ConBaT is inspired by the concept of control barrier functions in control theory and uses a causal transformer that learns to predict safe robot actions autoregressively using a critic that requires minimal safety data labeling. During deployment, we employ a lightweight online optimization to find actions that ensure future states lie within the learned safe set. We apply our approach to different simulated control tasks and show that our method results in safer control policies compared to other classical and learning-based methods such as imitation learning, reinforcement learning, and model predictive control.},
  archive   = {C_ICRA},
  author    = {Yue Meng and Sai Vemprela and Rogerio Bonatti and Chuchu Fan and Ashish Kapoor},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611109},
  month     = {5},
  pages     = {12857-12864},
  title     = {ConBaT: Control barrier transformer for safe robot learning from demonstrations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Few-shot learning of force-based motions from demonstration
through pre-training of haptic representation. <em>ICRA</em>,
12839–12845. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many contact-rich tasks, force sensing plays an essential role in adapting the motion to the physical properties of the manipulated object. To enable robots to capture the underlying distribution of object properties necessary for generalising learnt manipulation tasks to unseen objects, existing Learning from Demonstration (LfD) approaches require a large number of costly human demonstrations. Our proposed semi-supervised LfD approach decouples the learnt model into a haptic representation encoder and a motion generation decoder. This enables us to pre-train the first using a large amount of unsupervised data, easily accessible, while using few-shot LfD to train the second, leveraging the benefits of learning skills from humans. We validate the approach on the wiping task using sponges with different stiffness and surface friction. Our results demonstrate that pre-training significantly improves the ability of the LfD model to recognise physical properties and generate desired wiping motions for unseen sponges, outperforming the LfD method without pre-training. We validate the motion generated by our semi-supervised LfD model on the physical robot hardware using the KUKA iiwa robot arm. We also validate that the haptic representation encoder, pre-trained in simulation, captures the properties of real objects, explaining its contribution to improving the generalisation of the downstream task. See our accompanying video: https://youtu.be/zP4JvHaCWHk.},
  archive   = {C_ICRA},
  author    = {Marina Y. Aoyama and João Moura and Namiko Saito and Sethu Vijayakumar},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610502},
  month     = {5},
  pages     = {12839-12845},
  title     = {Few-shot learning of force-based motions from demonstration through pre-training of haptic representation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Soft acoustic end-effector. <em>ICRA</em>, 12772–12778. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Acoustic techniques have been developed as multifunctional tools for various microscale manipulations. In prevalent design paradigms, a position-fixed piezoelectric transducer (PZT) is utilized to generate ultrasound waves. However, the immobility of the PZT restricts the modulation of the acoustic field&#39;s position and orientation, consequently diminishing the adaptability and effectiveness of subsequent acoustic micromanipulation tasks. Here, we proposed a miniaturized soft acoustic end-effector and demonstrated acoustic field modulation and microparticle manipulation by adjusting PZT position and orientation. The PZT is mounted on the end of a soft robotic arm that has three individual degrees of freedom and can be deformed in 3D space by inflating or deflating each chamber. Experiments showed that the soft acoustic end-effector can change the traveling direction of microparticles and modulate the location of a standing wave field. Our approach is simple, flexible, and controllable. We envision that the soft acoustic end-effector will facilitate multiscale acoustic manipulation in interdisciplinary applications, especially, for in vivo acoustic therapies.},
  archive   = {C_ICRA},
  author    = {Zhiyuan Zhang and Michael Koch and Daniel Ahmed},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611503},
  month     = {5},
  pages     = {12772-12778},
  title     = {Soft acoustic end-effector},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Kinematic modeling and control of a soft robotic arm with
non-constant curvature deformation. <em>ICRA</em>, 12749–12755. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The passive compliance of soft robotic arms renders the development of accurate kinematic models and model-based controllers challenging. The most widely used model in soft robotic kinematics assumes Piecewise Constant Curvature (PCC). However, PCC introduces errors when the robot is subject to external forces or even gravity. In this paper, we establish a three-dimensional (3D) kinematic representation of a soft robotic arm with pseudo universal and prismatic joints that are capable of capturing non-constant curvature deformations of the soft segments. We theoretically demonstrate that this constitutes a more general methodology than PCC. Simulations and experiments on the real robot attest to the superior modeling accuracy of our approach in 3D motions with unknown loads. The maximum position/rotation error of the proposed model is verified 6.7×/4.6× lower than the PCC model considering gravity and external forces. Furthermore, we devise an inverse kinematic controller that is capable of positioning the tip, tracking trajectories, as well as performing interactive tasks in the 3D space.},
  archive   = {C_ICRA},
  author    = {Zhanchi Wang and Gaotian Wang and Xiaoping Chen and Nikolaos M. Freris},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611049},
  month     = {5},
  pages     = {12749-12755},
  title     = {Kinematic modeling and control of a soft robotic arm with non-constant curvature deformation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lumped parameter dynamic model of an eversion growing robot:
Analysis, simulation and experimental validation. <em>ICRA</em>,
12734–12740. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a lumped-parameter dynamic model of a pressure driven eversion robot carrying a catheter through its hollow core. A simulation framework based on the model is developed in MATLAB and is used for understanding the underlying physics, for identifying the regions of operation, and for demonstrating that, for a range of input commands, the catheter can be used as an actuation mechanism for propelling eversion; an approach especially useful for miniaturised systems. Simulations are experimentally validated on the MAMMOBOT system, which is a miniature steerable soft growing robot for early breast cancer detection. It was demonstrated that for most regions of operation experimental results compare well with simulation exhibiting an error less than 4%. Only one region of operation demonstrated larger deviations due possibly to unmodeled dynamics, which will be investigated in future work.},
  archive   = {C_ICRA},
  author    = {Panagiotis Vartholomeos and Zicong Wu and S.M. Hadi Sadati and Christos Bergeles},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611157},
  month     = {5},
  pages     = {12734-12740},
  title     = {Lumped parameter dynamic model of an eversion growing robot: Analysis, simulation and experimental validation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel model for layer jamming-based continuum robots.
<em>ICRA</em>, 12727–12733. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Continuum robots with variable stiffness have gained wide popularity in the last decade. Layer jamming (LJ) has emerged as a simple and efficient technique to achieve tunable stiffness for continuum robots. Despite its merits, the development of a control-oriented dynamical model 1 tailored for this specific class of robots remains an open problem in the literature. This paper aims to present the first solution, to the best of our knowledge, to close the gap. We propose an energy-based model that is integrated with the LuGre frictional model for LJ-based continuum robots. Then, we take a comprehensive theoretical analysis for this model, focusing on two fundamental characteristics of LJ-based continuum robots: shape locking and adjustable stiffness. To validate the modeling approach and theoretical results, a series of experiments using our OctRobot-I continuum robotic platform was conducted. The results show that the proposed model is capable of interpreting and predicting the dynamical behaviors in LJ-based continuum robots.},
  archive   = {C_ICRA},
  author    = {Bowen Yi and Yeman Fan and Dikai Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610912},
  month     = {5},
  pages     = {12727-12733},
  title     = {A novel model for layer jamming-based continuum robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian-guided evolutionary strategy with RRT for
multi-robot exploration. <em>ICRA</em>, 12720–12726. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the increasing demand for multi-robot exploration of unknown environments, how to accomplish this problem efficiently has become a focus of research. However, in this kind of task, the formulation of strategies for frontier point detection and task allocation largely determines the overall efficiency of the system. In the task of multi-robot exploration of unknown environments, the strategies of frontier point detection and task assignment determine the overall efficiency of the system. Most of the existing methods implement frontier point detection based on the Rapidly-Exploring Random Tree (RRT) and use greedy algorithms for task allocation. However, the classical RRT algorithm is a fixed growth step, which leads to the difficulty of growing branches in narrow environments, making the efficiency and correctness of detecting frontier points lower. Meanwhile, the allocation strategy of the greedy algorithm causes each robot to consider only the exploration area with the largest gain for itself, which easily leads to repeated exploration and reduces the overall efficiency of the system. To solve these problems, we propose an adaptive RRT tree growth strategy for frontier point detection, which can adjust the step size according to the known map information and thus improve the efficiency and accuracy of detection; and introduce a Bayesian-guided evolutionary strategy(BGE) for efficient task allocation, which can utilize the current and historical information to find the optimal allocation scheme in a global perspective. We conduct a comprehensive test of the proposed strategy in the ROS system as well as in the real world, which proves the efficiency of our strategy. Our code is open-sourced and can be provided under request.},
  archive   = {C_ICRA},
  author    = {Shuge Wu and Chunzheng Wang and Jiayi Pan and Dongming Han and Zhongliang Zhao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610963},
  month     = {5},
  pages     = {12720-12726},
  title     = {Bayesian-guided evolutionary strategy with RRT for multi-robot exploration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From cooking recipes to robot task trees – improving
planning correctness and task efficiency by leveraging LLMs with a
knowledge network. <em>ICRA</em>, 12704–12711. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Task planning for robotic cooking involves generating a sequence of actions for a robot to prepare a meal successfully. This paper introduces a novel task tree generation pipeline producing correct planning and efficient execution for cooking tasks. Our method first uses a large language model (LLM) to retrieve recipe instructions and then utilizes a fine-tuned GPT-3 to convert them into a task tree, capturing sequential and parallel dependencies among subtasks. The pipeline then mitigates the uncertainty and unreliable features of LLM outputs using task tree retrieval. We combine multiple LLM task tree outputs into a graph and perform a task tree retrieval to avoid questionable nodes and high-cost nodes to improve planning correctness and execution efficiency. Our evaluation results show its superior performance in task planning accuracy and efficiency compared to previous works.},
  archive   = {C_ICRA},
  author    = {Md. Sadman Sakib and Yu Sun},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611369},
  month     = {5},
  pages     = {12704-12711},
  title     = {From cooking recipes to robot task trees – improving planning correctness and task efficiency by leveraging LLMs with a knowledge network},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accelerating long-horizon planning with affordance-directed
dynamic grounding of abstract strategies. <em>ICRA</em>, 12688–12695.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Long-horizon task planning is important for robot autonomy, especially as a subroutine for frameworks such as Integrated Task and Motion Planning. However, task planning is computationally challenging and struggles to scale to realistic problem settings. We propose to accelerate task planning over an agent’s lifetime by integrating abstract strategies: a generalizable planning experience encoding introduced in earlier work. In this work, we contribute a practical approach to planning with strategies by introducing a novel formalism of planning in a strategy-augmented domain. We also introduce and formulate the notion of a strategy’s affordance, which indicates its predicted benefit to the solution, and use it to guide the planning and strategy grounding processes. Together, our observations yield an affordance-directed, lazy-search planning algorithm, which can seamlessly compose strategies and actions to solve long-horizon planning problems. We evaluate our planner in an object rearrangement domain, where we demonstrate performance benefits relative to a state-of-the-art task planner.},
  archive   = {C_ICRA},
  author    = {Khen Elimelech and Zachary Kingston and Wil Thomason and Moshe Y. Vardi and Lydia E. Kavraki},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610486},
  month     = {5},
  pages     = {12688-12695},
  title     = {Accelerating long-horizon planning with affordance-directed dynamic grounding of abstract strategies},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards safe robot use with edged or pointed objects: A
surrogate study assembling a human hand injury protection database.
<em>ICRA</em>, 12680–12687. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The use of pointed or edged tools or objects is one of the most challenging aspects of today’s application of physical human-robot interaction (pHRI). One reason for this is that the severity of harm caused by such edged or pointed impactors is less well studied than for blunt impactors. Consequently, the standards specify well-reasoned force and pressure thresholds for blunt impactors and advise avoiding any edges and corners in contacts. Nevertheless, pointed or edged impactor geometries cannot be completely ruled out in real pHRI applications. For example, to allow edged or pointed tools such as screwdrivers near human operators, the knowledge of injury severity needs to be extended so that robot integrators can perform well-reasoned, time-efficient risk assessments. In this paper, we provide the initial datasets on injury prevention for the human hand based on drop tests with surrogates for the human hand, namely pig claws and chicken drumsticks. We then demonstrate the ease and efficiency of robot use using the dataset for contact on two examples. Finally, our experiments provide a set of injuries that may also be expected for human subjects under certain robot mass-velocity constellations in collisions. To extend this work, testing on human samples and a collaborative effort from research institutes worldwide is needed to create a comprehensive human injury avoidance database for any pHRI scenario and thus for safe pHRI applications including edged and pointed geometries.},
  archive   = {C_ICRA},
  author    = {Robin Jeanne Kirschner and Carina M. Micheler and Yangcan Zhou and Sebastian Siegner and Mazin Hamad and Claudio Glowalla and Jan Neumann and Nader Rajaei and Rainer Burgkart and Sami Haddadin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610422},
  month     = {5},
  pages     = {12680-12687},
  title     = {Towards safe robot use with edged or pointed objects: A surrogate study assembling a human hand injury protection database},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Coupled active perception and manipulation planning for a
mobile manipulator in precision agriculture applications. <em>ICRA</em>,
12665–12671. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A mobile manipulator often finds itself in an application where it needs to take a close-up view before performing a manipulation task. Named this as a coupled active perception and manipulation (CAPM) problem, we model the uncertainty in the perception process and devise a key state/task planning algorithm that considers reachability conditions jointly established from perception and manipulation task constraints. By minimizing expected energy usage in body key state planning while satisfying task constraints, our algorithm is able to find an energy-efficient trajectory with less body repositioning motion while ensuring the success of the task. We have implemented the algorithm and tested it in both simulation and physical experiments. The results have confirmed that our algorithm has a lower energy consumption compared to a two-stage decoupled approach, while still maintaining a success rate of 100% for the task.},
  archive   = {C_ICRA},
  author    = {Shuangyu Xie and Chengsong Hu and Di Wang and Joe Johnson and Muthukumar Bagavathiannan and Dezhen Song},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610753},
  month     = {5},
  pages     = {12665-12671},
  title     = {Coupled active perception and manipulation planning for a mobile manipulator in precision agriculture applications},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed multi-robot online sampling with budget
constraints. <em>ICRA</em>, 12658–12664. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In multi-robot informative path planning the problem is to find a route for each robot in a team to visit a set of locations that can provide the most useful data to reconstruct an unknown scalar field. In the budgeted version, each robot is subject to a travel budget limiting the distance it can travel. Our interest in this problem is motivated by applications in precision agriculture, where robots are used to collect measurements to estimate domain-relevant scalar parameters such as soil moisture or nitrates concentrations. In this paper, we propose an online, distributed multi-robot sampling algorithm based on Monte Carlo Tree Search (MCTS) where each robot iteratively selects the next sampling location through communication with other robots and considering its remaining budget.We evaluate our proposed method for varying team sizes and in different environments, and we compare our solution with four different baseline methods. Our experiments show that our solution outperforms the baselines when the budget is tight by collecting measurements leading to smaller reconstruction errors.},
  archive   = {C_ICRA},
  author    = {Azin Shamshirgaran and Sandeep Manjanna and Stefano Carpin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611289},
  month     = {5},
  pages     = {12658-12664},
  title     = {Distributed multi-robot online sampling with budget constraints},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MATRIX: Multi-agent trajectory generation with diverse
contexts. <em>ICRA</em>, 12650–12657. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Data-driven methods have great advantages in modeling complicated human behavioral dynamics and dealing with many human-robot interaction applications. However, collecting massive and annotated real-world human datasets has been a laborious task, especially for highly interactive scenarios. On the other hand, algorithmic data generation methods are usually limited by their model capacities, making them unable to offer realistic and diverse data needed by various application users. In this work, we study trajectory-level data generation for multi-human or human-robot interaction scenarios and propose a learning-based automatic trajectory generation model, which we call Multi-Agent TRajectory generation with dIverse conteXts (MATRIX). MATRIX is capable of generating interactive human behaviors in realistic diverse contexts. We achieve this goal by modeling the explicit and interpretable objectives so that MATRIX can generate human motions based on diverse destinations and heterogeneous behaviors. We carried out extensive comparison and ablation studies to illustrate the effectiveness of our approach across various metrics. We also presented experiments that demonstrate the capability of MATRIX to serve as data augmentation for imitation-based motion planning.},
  archive   = {C_ICRA},
  author    = {Zhuo Xu and Rui Zhou and Yida Yin and Huidong Gao and Masayoshi Tomizuka and Jiachen Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610944},
  month     = {5},
  pages     = {12650-12657},
  title     = {MATRIX: Multi-agent trajectory generation with diverse contexts},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). F3DMP: Foresighted 3D motion planning of mobile robots in
wild environments. <em>ICRA</em>, 12643–12649. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In wild environments, motion planning for mobile robots faces the challenge of local optimal path traps due to limited sensor perception range and lack of spatial awareness. Existing approaches that avoid local optimum by designing heuristic functions or high-quality global paths in wild environments are time-consuming and unstable. This work proposes F3DMP, which consists of two parts to alleviate the local optimum solution and better utilize distant terrain information. First, the entire planning framework is adapted to the three-dimensional space so that the planning result conforms to the geometric characteristics of the terrain. Second, a time allocation function based on offline reinforcement learning is proposed. This function can anticipate potential challenges or opportunities based on semantic information for the image and proactively determine a time allocation. Our planner is integrated into a complete mobile robot system and deployed to a real robot. Experiments in simulation and the real world demonstrate that our method can improve the success rate by 28% and the trajectory smoothness by 27% compared with traditional methods.},
  archive   = {C_ICRA},
  author    = {Andong Yang and Wei Li and Yu Hu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611055},
  month     = {5},
  pages     = {12643-12649},
  title     = {F3DMP: Foresighted 3D motion planning of mobile robots in wild environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robot-dependent traversability estimation for outdoor
environments using deep multimodal variational autoencoders.
<em>ICRA</em>, 12635–12642. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Efficient and reliable navigation in off-road environments poses a significant challenge for robotics, especially when factoring in the varying capabilities of robots across different terrains. To achieve this, the robot system’s traversability is usually estimated to plan traversable routes through an environment. This paper presents a new approach that utilizes Deep Multimodal Variational Autoencoders (DMVAEs) for estimating the traversability of different robots in complex offroad terrains. Our method utilizes DMVAEs to capture essential environmental information and robot properties, effectively modeling factors that influence robotic traversability. The key contribution of this research is a two-stage traversability estimation framework for various robots in diverse off-road conditions that integrates robot properties in addition to environmental information to predict the traversability for various robots in a single model. We validate our method through real-world experiments involving four ground robots navigating an alpine environment. Comparative evaluations against state-of-the-art traversability estimation methods demonstrate the superior accuracy and robustness of our approach. Additionally, we investigate the transfer of trained models to new robots, enhancing their traversability estimation and extending the applicability of our framework.},
  archive   = {C_ICRA},
  author    = {Matthias Eder and Gerald Steinbauer-Wagner},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609988},
  month     = {5},
  pages     = {12635-12642},
  title     = {Robot-dependent traversability estimation for outdoor environments using deep multimodal variational autoencoders},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UNRealNet: Learning uncertainty-aware navigation features
from high-fidelity scans of real environments. <em>ICRA</em>,
12627–12634. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traversability estimation in rugged, unstructured environments remains a challenging problem in field robotics. Often, the need for precise, accurate traversability estimation is in direct opposition to the limited sensing and compute capability present on affordable, small-scale mobile robots. To address this issue, we present a novel method to learn [u]ncertainty-aware [n]avigation features from high-fidelity scans of [real]-world environments (UNRealNet). This network can be deployed on-robot to predict these high-fidelity features using input from lower-quality sensors. UNRealNet predicts dense, metric-space features directly from single-frame lidar scans, thus reducing the effects of occlusion and odometry error. Our approach is label-free, and is able to produce traversability estimates that are robot-agnostic. Additionally, we can leverage UNRealNet’s predictive uncertainty to both produce risk-aware traversability estimates, and refine our feature predictions over time. We find that our method outperforms traditional local mapping and inpainting baselines by up to 40%, and demonstrate its efficacy on multiple legged platforms.},
  archive   = {C_ICRA},
  author    = {Samuel Triest and David D. Fan and Sebastian Scherer and Ali-Akbar Agha-Mohammadi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610724},
  month     = {5},
  pages     = {12627-12634},
  title     = {UNRealNet: Learning uncertainty-aware navigation features from high-fidelity scans of real environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EnYOLO: A real-time framework for domain-adaptive underwater
object detection with image enhancement. <em>ICRA</em>, 12613–12619. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, significant progress has been made in the field of underwater image enhancement (UIE). However, its practical utility for high-level vision tasks, such as underwater object detection (UOD) in Autonomous Underwater Vehicles (AUVs), remains relatively unexplored. It may be attributed to several factors: (1) Existing methods typically employ UIE as a pre-processing step, which inevitably introduces considerable computational overhead and latency. (2) The process of enhancing images prior to training object detectors may not necessarily yield performance improvements. (3) The complex underwater environments can induce significant domain shifts across different scenarios, seriously deteriorating the UOD performance. To address these challenges, we introduce EnYOLO, an integrated real-time framework designed for simultaneous UIE and UOD with domain-adaptation capability. Specifically, both the UIE and UOD task heads share the same network backbone and utilize a lightweight design. Furthermore, to ensure balanced training for both tasks, we present a multi-stage training strategy aimed at consistently enhancing their performance. Additionally, we propose a novel domain-adaptation strategy to align feature embeddings originating from diverse underwater environments. Comprehensive experiments demonstrate that our framework not only achieves state-of-the-art (SOTA) performance in both UIE and UOD tasks, but also shows superior adaptability when applied to different underwater scenarios. Our efficiency analysis further highlights the substantial potential of our framework for onboard deployment.},
  archive   = {C_ICRA},
  author    = {Junjie Wen and Jinqiang Cui and Benyun Zhao and Bingxin Han and Xuchen Liu and Zhi Gao and Ben M. Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610639},
  month     = {5},
  pages     = {12613-12619},
  title     = {EnYOLO: A real-time framework for domain-adaptive underwater object detection with image enhancement},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TartanDrive 2.0: More modalities and better infrastructure
to further self-supervised learning research in off-road driving tasks.
<em>ICRA</em>, 12606. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present TartanDrive 2.0, a large-scale off-road driving dataset for self-supervised learning tasks. In 2021 we released TartanDrive 1.0, which is one of the largest datasets for off-road terrain. As a follow-up to our original dataset, we collected seven hours of data at speeds of up to 15m/s with the addition of three new LiDAR sensors alongside the original camera, inertial, GPS, and proprioceptive sensors. We also release the tools we use for collecting, processing, and querying the data, including our metadata system designed to further the utility of our data. Custom infrastructure allows end users to reconfigure the data to cater to their own platforms. These tools and infrastructure alongside the dataset are useful for a variety of tasks in the field of off-road autonomy and, by releasing them, we encourage collaborative data aggregation. These resources lower the barrier to entry to utilizing large-scale datasets, thereby helping facilitate the advancement of robotics in areas such as self-supervised learning, multi-modal perception, inverse reinforcement learning, and representation learning. The dataset is available at https://theairlab.org/TartanDrive2.},
  archive   = {C_ICRA},
  author    = {Matthew Sivaprakasam and Parv Maheshwari and Mateo Guaman Castro and Samuel Triest and Micah Nye and Steve Willits and Andrew Saba and Wenshan Wang and Sebastian Scherer},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611265},
  month     = {5},
  pages     = {12606},
  title     = {TartanDrive 2.0: More modalities and better infrastructure to further self-supervised learning research in off-road driving tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A probabilistic motion model for skid-steer wheeled mobile
robot navigation on off-road terrains. <em>ICRA</em>, 12599–12605. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Skid-Steer Wheeled Mobile Robots (SSWMRs) are increasingly being used for off-road autonomy applications. When turning at high speeds, these robots tend to undergo significant skidding and slipping. In this work, using Gaussian Process Regression (GPR) and Sigma-Point Transforms, we estimate the non-linear effects of tire-terrain interaction on robot velocities in a probabilistic fashion. Using the mean estimates from GPR, we propose a data-driven dynamic motion model that is more accurate at predicting future robot poses than conventional kinematic motion models. By efficiently solving a convex optimization problem based on the history of past robot motion, the GPR augmented motion model generalizes to previously unseen terrain conditions. The output distribution from the proposed motion model can be used for local motion planning approaches, such as stochastic model predictive control, leveraging model uncertainty to make safe decisions. We validate our work on a benchmark real-world multi-terrain SSWMR dataset. Our results show that the model generalizes to three different terrains while significantly reducing errors in linear and angular motion predictions. As shown in the attached video, we perform a separate set of experiments on a physical robot to demonstrate the robustness of the proposed algorithm.},
  archive   = {C_ICRA},
  author    = {Ananya Trivedi and Mark Zolotas and Adeeb Abbas and Sarvesh Prajapati and Salah Bazzi and Taşkin Padır},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611343},
  month     = {5},
  pages     = {12599-12605},
  title     = {A probabilistic motion model for skid-steer wheeled mobile robot navigation on off-road terrains},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Physics-informed neural network for multirotor slung load
systems modeling. <em>ICRA</em>, 12592–12598. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in aerial robotics have enabled the use of multirotor vehicles for autonomous payload transportation. Resorting only to classical methods to reliably model a quadrotor carrying a cable-slung load poses significant challenges. On the other hand, purely data-driven learning methods do not comply by design with the problem’s physical constraints, especially in states that are not densely represented in training data. In this work, we explore the use of physics-informed neural networks to learn an end-to-end model of the multirotor-slung-load system and, at a given time, estimate a sequence of the future system states. An LSTM encoder-decoder with an attention mechanism is used to capture the dynamics of the system. To guarantee the cohesiveness between the multiple predicted states of the system, we propose the use of a physics-based term in the loss function, which includes a discretized physical model derived from first principles together with slack variables that allow for a small mismatch between expected and predicted values. To train the model, a dataset using a real-world quadrotor carrying a slung load was curated and is made available. Prediction results are presented and corroborate the feasibility of the approach. The proposed method outperforms both the first principles physical model and a comparable neural network model trained without the physics regularization proposed.},
  archive   = {C_ICRA},
  author    = {Gil Serrano and Marcelo Jacinto and José Ribeiro-Gomes and João Pinto and Bruno J. Guerreiro and Alexandre Bernardino and Rita Cunha},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610582},
  month     = {5},
  pages     = {12592-12598},
  title     = {Physics-informed neural network for multirotor slung load systems modeling},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sequential manipulation of deformable linear object networks
with endpoint pose measurements using adaptive model predictive control.
<em>ICRA</em>, 12585–12591. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic manipulation of deformable linear objects (DLOs) is an active area of research, though emerging applications, like automotive wire harness installation, introduce constraints that have not been considered in prior work. Confined workspaces and limited visibility complicate prior assumptions of multi-robot manipulation and direct measurement of DLO configuration (state). This work focuses on single-arm manipulation of stiff DLOs (StDLOs) connected to form a DLO network (DLON), for which the measurements (output) are the endpoint poses of the DLON, which are subject to unknown dynamics during manipulation. To demonstrate feasibility of output-based control without state estimation, direct input-output dynamics are shown to exist by training neural network models on simulated trajectories. Output dynamics are then approximated with polynomials and found to contain well-known rigid body dynamics terms. A composite model consisting of a rigid body model and an online data-driven residual is developed, which predicts output dynamics more accurately than either model alone, and without prior experience with the system. An adaptive model predictive controller is developed with the composite model for DLON manipulation, which completes DLON installation tasks, both in simulation and with a physical automotive wire harness.},
  archive   = {C_ICRA},
  author    = {Tyler Toner and Vahidreza Molazadeh and Miguel Saez and Dawn M. Tilbury and Kira Barton},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611551},
  month     = {5},
  pages     = {12585-12591},
  title     = {Sequential manipulation of deformable linear object networks with endpoint pose measurements using adaptive model predictive control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Recursive least squares with log-determinant divergence
regularisation for online inertia identification. <em>ICRA</em>,
12578–12584. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study presents a recursive algorithm for solving the regularised least squares problem for online identification of rigid body dynamic model parameters with emphasis on the physical consistency of estimated inertial parameters. One of the geometric approaches is to use a regulariser that represents how close the pseudo-inertia matrix is to a given reference on the feasible manifold in the regression problem. The proposed extension enables memory-efficient online learning in addition to the benefits of geometry-aware convex regularisation using the log-determinant divergence of the pseudo-inertia matrix. Also, the recursive version endows the estimator with the capability to deal with time-variation of parameters by introducing an optional forgetting mechanism. The characteristics of the recursive regularised least squares algorithm is demonstrated using the MIT Cheetah 3 leg swinging experiment dataset and compared to the existing batch optimisation method.},
  archive   = {C_ICRA},
  author    = {Namhoon Cho and Taeyoon Lee and Hyo-Sang Shin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610389},
  month     = {5},
  pages     = {12578-12584},
  title     = {Recursive least squares with log-determinant divergence regularisation for online inertia identification},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive gait modeling and optimization for principally
kinematic systems. <em>ICRA</em>, 12571–12577. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic adaptation to unanticipated operating conditions is crucial to achieving persistence and robustness in complex real world settings. For a wide range of cutting-edge robotic systems, such as micro- and nano-scale robots, soft robots, medical robots, and bio-hybrid robots, it is infeasible to anticipate the operating environment a priori due to complexities that arise from numerous factors including imprecision in manufacturing, chemo-mechanical forces, and poorly understood contact mechanics. Drawing inspiration from data-driven modeling, geometric mechanics (or gauge theory), and adaptive control, we employ an adaptive system identification framework and demonstrate its efficacy in enhancing the performance of principally kinematic locomotors (those governed by Rayleigh dissipation or zero momentum conservation). We showcase the capability of the adaptive model to efficiently accommodate varying terrains and iteratively modified behaviors within a behavior optimization framework. This provides both the ability to improve fundamental behaviors and perform motion tracking to precision. Notably, we are capable of optimizing the gaits of the Purcell swimmer using approximately 10 cycles per link, which for the nine-link Purcell swimmer provides a factor of ten improvement in optimization speed over the state of the art. Beyond simply a computational speed up, this tenfold improvement may enable this method to be successfully deployed for in-situ behavior refinement, injury recovery, and terrain adaptation, particularly in domains where simulations provide poor guides for the real world.},
  archive   = {C_ICRA},
  author    = {Siming Deng and Noah J. Cowan and Brian A. Bittner},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611303},
  month     = {5},
  pages     = {12571-12577},
  title     = {Adaptive gait modeling and optimization for principally kinematic systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning-based model predictive control for an autonomous
formula student racing car. <em>ICRA</em>, 12556–12562. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Advancements in Automated Driving Systems (ADSs) have enabled the achievement of a certain level of autonomy while commuting in a car. However, emergency and high-speed maneuvers still arise as significant challenges for ADSs due to the intrinsic nonlinearity and fast-paced behavior of such events. These maneuvers are a distinctive feature within the recently established motorsport discipline of Autonomous Racing (AR). In this work, we explore the use of Learning-based Model Predictive Control (LMPC) to address possible model mismatches of the first principles model in high-speed racing. To this end, a Model Predictive Contouring Control (MPCC) (a specific formulation of the standard Model Predictive Control, MPC) is formulated, and a Neural Network (NN) that leverages the use of Feedforward and Recurrent layers is employed to learn the errors of the first principles model. By combining the NN with the first principles model, the LMPC is born, capable of accurately predicting the future with a computational effort compatible with real-time feasibility, effectively handling the vehicle at its limits. Furthermore, the controller can adapt to changing environments by training the NN during the race. The MPCC (formulation without the NN) is deployed on a real autonomous formula student racing car showing an improvement of 16 % in mean lap times across the same track between a common geometric controller. The LMPC is analyzed in a high-fidelity simulator, achieving an improvement of 8.9 % in mean lap times when compared to the MPCC.},
  archive   = {C_ICRA},
  author    = {David R. Gomes and Miguel Ayala Botto and Pedro U. Lima},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611285},
  month     = {5},
  pages     = {12556-12562},
  title     = {Learning-based model predictive control for an autonomous formula student racing car},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SculptBot: Pre-trained models for 3D deformable object
manipulation. <em>ICRA</em>, 12548–12555. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deformable object manipulation presents a unique set of challenges in robotic manipulation by exhibiting high degrees of freedom and severe self-occlusion. Choosing state representations for materials that exhibit plastic behavior, like modeling clay or bread dough, is also difficult because they permanently deform under stress and are constantly changing shape. In this work, we investigate each of these challenges using the task of robotic sculpting with a parallel gripper. We propose a system that uses point clouds as the state representation and leverages a pre-trained point cloud reconstruction transformer to learn a latent dynamics model to predict material deformations given a grasp action. We design a novel action sampling algorithm that reasons about geometrical differences between point clouds to further improve the efficiency of model-based planners. All data and experiments are conducted entirely in the real world. Our experiments show the proposed system is able to successfully capture the dynamics of clay, and is able to create a variety of simple shapes. Videos and additional figures are available on our project page at: https://sites.google.com/andrew.cmu.edu/sculptbot},
  archive   = {C_ICRA},
  author    = {Alison Bartsch and Charlotte Avra and Amir Barati Farimani},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610899},
  month     = {5},
  pages     = {12548-12555},
  title     = {SculptBot: Pre-trained models for 3D deformable object manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). TRTM: Template-based reconstruction and target-oriented
manipulation of crumpled cloths. <em>ICRA</em>, 12522–12528. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Precise reconstruction and manipulation of the crumpled cloths is challenging due to the high dimensionality of cloth models, as well as the limited observation at self-occluded regions. We leverage the recent progress in the field of single-view reconstruction to template-based reconstruct the crumpled cloths from their top-view depth observations only, with our proposed sim-real registration protocols. In contrast to previous implicit cloth representations, our reconstruction mesh explicitly describes the positions and visibilities of the entire cloth mesh vertices, enabling more efficient dual-arm and single-arm target-oriented manipulations. Experiments demonstrate that our TRTM system can be applied to daily cloths that have similar topologies as our template mesh, but with different shapes, sizes, patterns, and physical properties. Videos, datasets, pre-trained models, and code can be downloaded from our project website: https://wenbwa.github.io/TRTM/.},
  archive   = {C_ICRA},
  author    = {Wenbo Wang and Gen Li and Miguel Zamora and Stelian Coros},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609868},
  month     = {5},
  pages     = {12522-12528},
  title     = {TRTM: Template-based reconstruction and target-oriented manipulation of crumpled cloths},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving neural indoor surface reconstruction with
mask-guided adaptive consistency constraints. <em>ICRA</em>,
12506–12513. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D scene reconstruction from 2D images has been a long-standing task. Instead of estimating per-frame depth maps and fusing them in 3D, recent researches leverage the neural implicit surface as a global representation for 3D reconstruction. Equipped with data-driven pre-trained geometric cues, these methods have demonstrated promising performance. However, the inevitable inaccurate estimation of priors can lead to suboptimal reconstruction quality, particularly in some geometrically complex regions. In this paper, we propose a two-stage training process to further improve the reconstruction quality. It decouples the view-dependent and view-independent colors, and leverages two novel consistency constraints to enhance detail reconstruction performance without requiring extra priors. Additionally, we introduce an essential mask scheme to adaptively influence the selection of supervision constraints, thereby improving performance in a self-supervised paradigm. Experiments on synthetic and real-world datasets show the capability of reducing the side effects of inaccurately estimated priors and achieving high-quality scene reconstruction with rich geometric details.},
  archive   = {C_ICRA},
  author    = {Xinyi Yu and Liqin Lu and Jintao Rong and Guangkai Xu and Linlin Ou},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611101},
  month     = {5},
  pages     = {12506-12513},
  title     = {Improving neural indoor surface reconstruction with mask-guided adaptive consistency constraints},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MF-MOS: A motion-focused model for moving object
segmentation. <em>ICRA</em>, 12499–12505. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Moving object segmentation (MOS) provides a reliable solution for detecting traffic participants and thus is of great interest in the autonomous driving field. Dynamic capture is always critical in the MOS problem. Previous methods capture motion features from the range images directly. Differently, we argue that the residual maps provide greater potential for motion information, while range images contain rich semantic guidance. Based on this intuition, we propose MF-MOS, a novel motion-focused model with a dual-branch structure for LiDAR moving object segmentation. Novelly, we decouple the spatial-temporal information by capturing the motion from residual maps and generating semantic features from range images, which are used as movable object guidance for the motion branch. Our straightforward yet distinctive solution can make the most use of both range images and residual maps, thus greatly improving the performance of the LiDAR-based MOS task. Remarkably, our MF-MOS achieved a leading IoU of 76.7% on the MOS leaderboard of the SemanticKITTI dataset upon submission, demonstrating the current state-of-the-art performance. The implementation of our MF-MOS has been released at https://github.com/SCNU-RISLAB/MF-MOS.},
  archive   = {C_ICRA},
  author    = {Jintao Cheng and Kang Zeng and Zhuoxu Huang and Xiaoyu Tang and Jin Wu and Chengxi Zhang and Xieyuanli Chen and Rui Fan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611400},
  month     = {5},
  pages     = {12499-12505},
  title     = {MF-MOS: A motion-focused model for moving object segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). SM3: Self-supervised multi-task modeling with multi-view 2D
images for articulated objects. <em>ICRA</em>, 12492–12498. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reconstructing real-world objects and estimating their movable joint structures are pivotal technologies within the field of robotics. Previous research has predominantly focused on supervised approaches, relying on annotated datasets to model articulated objects within limited categories. However, these approaches fall short of effectively addressing the diversity present in the real world. To tackle this issue, we propose a self-supervised interaction perception method, referred to as SM 3 , which leverages multi-view RGB images captured before and after interaction to model articulated objects, identify the movable parts, and infer the parameters of their rotating joints. By constructing 3D geometries and textures from the captured 2D images, SM 3 achieves integrated optimization of movable part and joint parameters during the reconstruction process, obviating the need for annotations. Furthermore, we introduce the MMArt dataset, an extension of PartNet-Mobility, encompassing multi-view and multi-modal data of articulated objects spanning diverse categories. Evaluations demonstrate that SM 3 surpasses existing benchmarks across various categories and objects, and its adaptability in real-world scenarios has been thoroughly validated.},
  archive   = {C_ICRA},
  author    = {Haowen Wang and Zhen Zhao and Zhao Jin and Zhengping Che and Liang Qiao and Yakun Huang and Zhipeng Fan and Xiuquan Qiao and Jian Tang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610171},
  month     = {5},
  pages     = {12492-12498},
  title     = {SM3: Self-supervised multi-task modeling with multi-view 2D images for articulated objects},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multimodal object query initialization for 3D object
detection. <em>ICRA</em>, 12484–12491. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D object detection models that exploit both LiDAR and camera sensor features are top performers in large-scale autonomous driving benchmarks. A transformer is a popular network architecture used for this task, in which so-called object queries act as candidate objects. Initializing these object queries based on current sensor inputs is a common practice. For this, existing methods strongly rely on LiDAR data however, and do not fully exploit image features. Besides, they introduce significant latency. To overcome these limitations we propose EfficientQ3M, an efficient, modular, and multimodal solution for object query initialization for transformer-based 3D object detection models. The proposed initialization method is combined with a &quot;modality-balanced&quot; transformer decoder where the queries can access all sensor modalities throughout the decoder. In experiments, we outperform the state of the art in transformer-based LiDAR object detection on the competitive nuScenes benchmark and showcase the benefits of input-dependent multimodal query initialization, while being more efficient than the available alternatives for LiDAR-camera initialization. The proposed method can be applied with any combination of sensor modalities as input, demonstrating its modularity.},
  archive   = {C_ICRA},
  author    = {Mathijs R. Van Geerenstein and Felicia Ruppel and Klaus Dietmayer and Dariu M. Gavrila},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610905},
  month     = {5},
  pages     = {12484-12491},
  title     = {Multimodal object query initialization for 3D object detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active implicit reconstruction using one-shot view planning.
<em>ICRA</em>, 12477–12483. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Active object reconstruction using autonomous robots is gaining great interest. A primary goal in this task is to maximize the information of the object to be reconstructed, given limited on-board resources. Previous view planning methods exhibit inefficiency since they rely on an iterative paradigm based on explicit representations, consisting of (1) planning a path to the next-best view only; and (2) requiring a considerable number of less-gain views in terms of surface coverage. To address these limitations, we propose to integrate implicit representations into the One-Shot View Planning (OSVP). The key idea behind our approach is to use implicit representations to obtain the small missing surface areas instead of observing them with extra views. Therefore, we design a deep neural network, named OSVP, to directly predict a set of views given a dense point cloud refined from an initial sparse observation. To train our OSVP network, we generate supervision labels using dense point clouds refined by implicit representations and set covering optimization problems. Simulated experiments show that our method achieves sufficient reconstruction quality, outperforming several baselines under limited view and movement budgets. We further demonstrate the applicability of our approach in a real-world object reconstruction scenario.},
  archive   = {C_ICRA},
  author    = {Hao Hu and Sicong Pan and Liren Jin and Marija Popović and Maren Bennewitz},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611542},
  month     = {5},
  pages     = {12477-12483},
  title     = {Active implicit reconstruction using one-shot view planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How many views are needed to reconstruct an unknown object
using NeRF? <em>ICRA</em>, 12470–12476. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Neural Radiance Fields (NeRFs) are gaining significant interest for online active object reconstruction due to their exceptional memory efficiency and requirement for only posed RGB inputs. Previous NeRF-based view planning methods exhibit computational inefficiency since they rely on an iterative paradigm, consisting of (1) retraining the NeRF when new images arrive; and (2) planning a path to the next best view only. To address these limitations, we propose a non-iterative pipeline based on the Prediction of the Required number of Views (PRV). The key idea behind our approach is that the required number of views to reconstruct an object depends on its complexity. Therefore, we design a deep neural network, named PRVNet, to predict the required number of views, allowing us to tailor the data acquisition based on the object complexity and plan a globally shortest path. To train our PRVNet, we generate supervision labels using the ShapeNet dataset. Simulated experiments show that our PRV-based view planning method outperforms baselines, achieving good reconstruction quality while significantly reducing movement cost and planning time. We further justify the generalization ability of our approach in a real-world experiment.},
  archive   = {C_ICRA},
  author    = {Sicong Pan and Liren Jin and Hao Hu and Marija Popović and Maren Bennewitz},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610617},
  month     = {5},
  pages     = {12470-12476},
  title     = {How many views are needed to reconstruct an unknown object using NeRF?},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Physically grounded vision-language models for robotic
manipulation. <em>ICRA</em>, 12462–12469. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in vision-language models (VLMs) have led to improved performance on tasks such as visual question answering and image captioning. Consequently, these models are now well-positioned to reason about the physical world, particularly within domains such as robotic manipulation. However, current VLMs are limited in their understanding of the physical concepts (e.g., material, fragility) of common objects, which restricts their usefulness for robotic manipulation tasks that involve interaction and physical reasoning about such objects. To address this limitation, we propose PHYSOBJECTS, an object-centric dataset of 39.6K crowd-sourced and 417K automated physical concept annotations of common household objects. We demonstrate that fine-tuning a VLM on PhysObjects improves its understanding of physical object concepts, including generalization to held-out concepts, by capturing human priors of these concepts from visual appearance. We incorporate this physically grounded VLM in an interactive framework with a large language model-based robotic planner, and show improved planning performance on tasks that require reasoning about physical object concepts, compared to baselines that do not leverage physically grounded VLMs. We additionally illustrate the benefits of our physically grounded VLM on a real robot, where it improves task success rates. We release our dataset and provide further details and visualizations of our results at https://iliad.stanford.edu/pg-vlm/.},
  archive   = {C_ICRA},
  author    = {Jensen Gao and Bidipta Sarkar and Fei Xia and Ted Xiao and Jiajun Wu and Brian Ichter and Anirudha Majumdar and Dorsa Sadigh},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610090},
  month     = {5},
  pages     = {12462-12469},
  title     = {Physically grounded vision-language models for robotic manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). V2CE: Video to continuous events simulator. <em>ICRA</em>,
12455–12461. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dynamic Vision Sensor (DVS)-based solutions have recently garnered significant interest across various computer vision tasks, offering notable benefits in terms of dynamic range, temporal resolution, and inference speed. However, as a relatively nascent vision sensor compared to Active Pixel Sensor (APS) devices such as RGB cameras, DVS suffers from a dearth of ample labeled datasets. Prior efforts to convert APS data into events often grapple with issues such as a considerable domain shift from real events, the absence of quantified validation, and layering problems within the time axis. In this paper, we present a novel method for video-to-events stream conversion from multiple perspectives, considering the specific characteristics of DVS. A series of carefully designed losses helps enhance the quality of generated event voxels significantly. We also propose a novel local dynamic-aware timestamp inference strategy to accurately recover event timestamps from event voxels in a continuous fashion and eliminate the temporal layering problem. Results from rigorous validation through quantified metrics at all stages of the pipeline establish our method unquestionably as the current state-of-the-art (SOTA). The code can be found at bit.ly/v2ce.},
  archive   = {C_ICRA},
  author    = {Zhongyang Zhang and Shuyang Cui and Kaidong Chai and Haowen Yu and Subhasis Dasgupta and Upal Mahbub and Tauhidur Rahman},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609864},
  month     = {5},
  pages     = {12455-12461},
  title     = {V2CE: Video to continuous events simulator},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CrossVideo: Self-supervised cross-modal contrastive learning
for point cloud video understanding. <em>ICRA</em>, 12436–12442. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a novel approach named CrossVideo, which aims to enhance self-supervised cross-modal contrastive learning in the field of point cloud video understanding. Traditional supervised learning methods encounter limitations due to data scarcity and challenges in label acquisition. To address these issues, we propose a self-supervised learning method that leverages the cross-modal relationship between point cloud videos and image videos to acquire meaningful feature representations. Intra-modal and cross-modal contrastive learning techniques are employed to facilitate effective comprehension of point cloud video. We also propose a multi-level contrastive approach for both modalities. Through extensive experiments, we demonstrate that our method significantly surpasses previous state-of-the-art approaches, and we conduct comprehensive ablation studies to validate the effectiveness of our proposed designs.},
  archive   = {C_ICRA},
  author    = {Yunze Liu and Changxi Chen and Zifan Wang and Li Yi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610376},
  month     = {5},
  pages     = {12436-12442},
  title     = {CrossVideo: Self-supervised cross-modal contrastive learning for point cloud video understanding},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RoboLLM: Robotic vision tasks grounded on multimodal large
language models. <em>ICRA</em>, 12428–12435. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic vision applications often necessitate a wide range of visual perception tasks, such as object detection, segmentation, and identification. While there have been substantial advances in these individual tasks, integrating specialized models into a unified vision pipeline presents significant engineering challenges and costs. Recently, Multimodal Large Language Models (MLLMs) have emerged as novel backbones for various downstream tasks. We argue that leveraging the pre-training capabilities of MLLMs enables the creation of a simplified framework, thus mitigating the need for task-specific encoders. Specifically, the large-scale pretrained knowledge in MLLMs allows for easier fine-tuning to downstream robotic vision tasks and yields superior performance. We introduce the RoboLLM framework, equipped with a BEiT-3 backbone, to address all visual perception tasks in the ARMBench challenge—a large-scale robotic manipulation dataset about real-world warehouse scenarios. RoboLLM not only outperforms existing baselines but also substantially reduces the engineering burden associated with model selection and tuning. All the code used in this paper can be found in https://github.com/longkukuhi/RoboLLM.},
  archive   = {C_ICRA},
  author    = {Zijun Long and George Killick and Richard McCreadie and Gerardo Aragon-Camarasa},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610797},
  month     = {5},
  pages     = {12428-12435},
  title     = {RoboLLM: Robotic vision tasks grounded on multimodal large language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Doduo: Learning dense visual correspondence from
unsupervised semantic-aware flow. <em>ICRA</em>, 12420–12427. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dense visual correspondence plays a vital role in robotic perception. This work focuses on establishing the dense correspondence between a pair of images that captures dynamic scenes undergoing substantial transformations. We introduce Doduo to learn general dense visual correspondence from in-the-wild images and videos without ground truth supervision. Given a pair of images, it estimates the dense flow field encoding the displacement of each pixel in one image to its corresponding pixel in the other image. Doduo uses flow-based warping to acquire supervisory signals for the training. Incorporating semantic priors with self-supervised flow training, Doduo produces accurate dense correspondence robust to the dynamic changes of the scenes. Trained on an in-the-wild video dataset, Doduo illustrates superior performance on point-level correspondence estimation over existing self-supervised correspondence learning baselines. We also apply Doduo to articulation estimation and zero-shot goal-conditioned manipulation, underlining its practical applications in robotics. Code and additional visualizations are available at https://ut-austin-rpl.github.io/Doduo/},
  archive   = {C_ICRA},
  author    = {Zhenyu Jiang and Hanwen Jiang and Yuke Zhu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611587},
  month     = {5},
  pages     = {12420-12427},
  title     = {Doduo: Learning dense visual correspondence from unsupervised semantic-aware flow},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RenderOcc: Vision-centric 3D occupancy prediction with 2D
rendering supervision. <em>ICRA</em>, 12404–12411. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D occupancy prediction holds significant promise in the fields of robot perception and autonomous driving, which quantifies 3D scenes into grid cells with semantic labels. Recent works mainly utilize complete occupancy labels in 3D voxel space for supervision. However, the expensive annotation process and sometimes ambiguous labels have severely constrained the usability and scalability of 3D occupancy models. To address this, we present RenderOcc, a novel paradigm for training 3D occupancy models only using 2D labels. Specifically, we extract a NeRF-style 3D volume representation from multi-view images, and employ volume rendering techniques to establish 2D renderings, thus enabling direct 3D supervision from 2D semantics and depth labels. Additionally, we introduce an Auxiliary Ray method to tackle the issue of sparse viewpoints in autonomous driving scenarios, which leverages sequential frames to construct comprehensive 2D rendering for each object. To our best knowledge, RenderOcc is the first attempt to train multi-view 3D occupancy models only using 2D labels, reducing the dependence on costly 3D occupancy annotations. Extensive experiments demonstrate that RenderOcc achieves comparable performance to models fully supervised with 3D labels, underscoring the significance of this approach in real-world applications. Our code is available at https://github.com/pmj110119/RenderOcc.},
  archive   = {C_ICRA},
  author    = {Mingjie Pan and Jiaming Liu and Renrui Zhang and Peixiang Huang and Xiaoqi Li and Hongwei Xie and Bing Wang and Li Liu and Shanghang Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611537},
  month     = {5},
  pages     = {12404-12411},
  title     = {RenderOcc: Vision-centric 3D occupancy prediction with 2D rendering supervision},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AnyOKP: One-shot and instance-aware object keypoint
extraction with pretrained ViT. <em>ICRA</em>, 12397–12403. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Towards flexible object-centric visual perception, we propose a one-shot instance-aware object keypoint (OKP) extraction approach, AnyOKP, which leverages the powerful representation ability of pretrained vision transformer (ViT), and can obtain keypoints on multiple object instances of arbitrary category after learning from a support image. An off-the-shelf petrained ViT is directly deployed for generalizable and transferable feature extraction, which is followed by training-free feature enhancement. The best-prototype pairs (BPPs) are searched for in support and query images based on appearance similarity, to yield instance-unaware candidate keypoints. Then, the entire graph with all candidate keypoints as vertices are divided into sub-graphs according to the feature distributions on the graph edges. Finally, each sub-graph represents an object instance. AnyOKP is evaluated on real object images collected with the cameras of a robot arm, a mobile robot, and a surgical robot, which not only demonstrates the cross-category flexibility and instance awareness, but also show remarkable robustness to domain shift and viewpoint change.},
  archive   = {C_ICRA},
  author    = {Fangbo Qin and Taogang Hou and Shan Lin and Kaiyuan Wang and Michael C. Yip and Shan Yu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610601},
  month     = {5},
  pages     = {12397-12403},
  title     = {AnyOKP: One-shot and instance-aware object keypoint extraction with pretrained ViT},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ASP-LED: Learning ambiguity-aware structural priors for
joint low-light enhancement and deblurring. <em>ICRA</em>, 12389–12396.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Low-light enhancement and deblurring is vital for high-level vision-related nighttime tasks. Most existing cascade and joint enhancement methods may provide undesirable results, suffering from severe artifacts, deteriorating blur, and unclear details. In this paper, we propose a novel ambiguity-aware network (ASP-LED) with structural priors, including high-frequency and edge, to enable effective image representation learning for joint low-light enhancement and deblurring. Specifically, we employ a Transformer backbone to explore the global clues of the image. To compensate for the inadequate local detail optimization, we propose a multi-patch perception pyramid block that models the correlation between different size patches and ambiguity, and identifies non-uniform deblurring spatial features, facilitating the reconstruction of potential high-frequency and edge information. Furthermore, a prior-guided reconstruction block based on the parallel attention mechanism is present to adaptively correct global image with statistical features, which helps guide the model to refine sharp texture and structure. Extensive experiments performed on simulated and real-world datasets demonstrate the efficacy of our proposed method in restoring low-light blurry images with increased visual perception compared to state-of-the-art methods.},
  archive   = {C_ICRA},
  author    = {Jing Ye and Yang Liu and Congjing Yu and Changzhen Qiu and Zhiyong Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611570},
  month     = {5},
  pages     = {12389-12396},
  title     = {ASP-LED: Learning ambiguity-aware structural priors for joint low-light enhancement and deblurring},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-robot informative path planning from regression with
sparse gaussian processes. <em>ICRA</em>, 12382–12388. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses multi-robot informative path planning (IPP) for environmental monitoring. The problem involves determining informative regions in the environment that should be visited by robots to gather the most information about the environment. We propose an efficient sparse Gaussian process-based approach that uses gradient descent to optimize paths in continuous environments. Our approach efficiently scales to both spatially and spatio-temporally correlated environments. Moreover, our approach can simultaneously optimize the informative paths while accounting for routing constraints, such as a distance budget and limits on the robot’s velocity and acceleration. Our approach can be used for IPP with both discrete and continuous sensing robots, with point and non-point field-of-view sensing shapes, and for both single and multi-robot IPP. We demonstrate that the proposed approach is fast and accurate on real-world data.},
  archive   = {C_ICRA},
  author    = {Kalvik Jakkala and Srinivas Akella},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610484},
  month     = {5},
  pages     = {12382-12388},
  title     = {Multi-robot informative path planning from regression with sparse gaussian processes},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Wind field modeling for formation planning in multi-drone
systems. <em>ICRA</em>, 12375–12381. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In multi-drone systems such as drone light shows, drones move in formation while avoiding collisions. However, few existing formation planning algorithms consider the wind fields of drones during planning. Since the wind field effect is prominent when drones have to fly close to each other, we cannot ignore the effect during planning. In this paper, we extend the reservation system in autonomous intersection management for grid-based formation planning by including a new type of reservation called non-exclusive reservations specifically for handling wind fields. We train a deep learning model to predict the deviation of a drone’s trajectory when the drone enters the wind field of another drone and then use the reservation grid to prevent collision. Based on the reservation system, we develop a new formation planning algorithm that focuses on adjusting the start times of motion plans to avoid collision. Our experimental results show that trajectory prediction can help make better decisions in task assignments for minimizing makespans.},
  archive   = {C_ICRA},
  author    = {Minhyuk Park and Tsz-Chiu Au},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610976},
  month     = {5},
  pages     = {12375-12381},
  title     = {Wind field modeling for formation planning in multi-drone systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Communication-aware map compression for online
path-planning. <em>ICRA</em>, 12368–12374. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the problem of the communication of optimally compressed information for mobile robot path-planning. In this context, mobile robots compress their current local maps to assist another robot in reaching a target in an unknown environment. We propose a framework that sequentially selects the optimal level of compression, guided by the robot’s path, by balancing map resolution and communication cost. Our approach is tractable in close-to-real scenarios and does not necessitate prior environment knowledge. We design a novel decoder that leverages compressed information to estimate the unknown environment via convex optimization with linear constraints and an encoder that utilizes the decoder to select the optimal compression. Numerical simulations are conducted both in a large close-to-real map and a maze map and compared with two alternative approaches. The results confirm the effectiveness of our framework in assisting the robot reach its target by reducing transmitted information, on average, by approximately 50%, while maintaining satisfactory performance.},
  archive   = {C_ICRA},
  author    = {Evangelos Psomiadis and Dipankar Maity and Panagiotis Tsiotras},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610133},
  month     = {5},
  pages     = {12368-12374},
  title     = {Communication-aware map compression for online path-planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-agent path finding for cooperative autonomous driving.
<em>ICRA</em>, 12361–12367. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Anticipating possible future deployment of connected and automated vehicles (CAVs), cooperative autonomous driving at intersections has been studied by many works in control theory and intelligent transportation across decades. Simultaneously, recent parallel works in robotics have devised efficient algorithms for multi-agent path finding (MAPF), though often in environments with simplified kinematics. In this work, we hybridize insights and algorithms from MAPF with the structure and heuristics of optimizing the crossing order of CAVs at signal-free intersections. We devise an optimal and complete algorithm, Order-based Search with Kinematics Arrival Time Scheduling (OBS-KATS), which significantly outperforms existing algorithms, fixed heuristics, and prioritized planning with KATS. The performance is maintained under different vehicle arrival rates, lane lengths, crossing speeds, and control horizon. Through ablations and dissections, we offer insight on the contributing factors to OBS-KATS’s performance. Our work is directly applicable to many similarly scaled traffic and multi-robot scenarios with directed lanes.},
  archive   = {C_ICRA},
  author    = {Zhongxia Yan and Han Zheng and Cathy Wu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611649},
  month     = {5},
  pages     = {12361-12367},
  title     = {Multi-agent path finding for cooperative autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-robot cooperative socially-aware navigation using
multi-agent reinforcement learning. <em>ICRA</em>, 12353–12360. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In public spaces shared with humans, ensuring multi-robot systems navigate without collisions while respecting social norms is challenging, particularly with limited communication. Although current robot social navigation techniques leverage advances in reinforcement learning and deep learning, they frequently overlook robot dynamics in simulations, leading to a simulation-to-reality gap. In this paper, we bridge this gap by presenting a new multi-robot social navigation environment crafted using Dec-POSMDP and multi-agent reinforcement learning. Furthermore, we introduce SAMARL: a novel benchmark for cooperative multi-robot social navigation. SAMARL employs a unique spatial-temporal transformer combined with multi-agent reinforcement learning. This approach effectively captures the complex interactions between robots and humans, thus promoting cooperative tendencies in multi-robot systems. Our extensive experiments reveal that SAMARL outperforms existing baseline and ablation models in our designed environment. Demo videos for this work can be found at: https://sites.google.com/view/samarl},
  archive   = {C_ICRA},
  author    = {Weizheng Wang and Le Mao and Ruiqi Wang and Byung-Cheol Min},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611322},
  month     = {5},
  pages     = {12353-12360},
  title     = {Multi-robot cooperative socially-aware navigation using multi-agent reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning heterogeneous multi-agent allocations for ergodic
search. <em>ICRA</em>, 12345–12352. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Information-based coverage directs robots to move over an area to optimize a pre-defined objective function based on some measure of information. Our prior work determined that the spectral decomposition of an information map can be used to guide a set of heterogeneous agents, each with different sensor and motion models, to optimize coverage in a target region, based on a measure called ergodicity. In this paper, we build on this insight to construct a reinforcement learning formulation of the problem of allocating heterogeneous agents to different search regions in the frequency domain. We relate the spectral coefficients of the search map to each other in three different ways. The first method maps agents to predefined sets of spectral coefficients. In the second method, each agent learns a weight distribution over all spectral coefficients. Finally, in the third method, each agent learns weight distributions as parameterized curves over coefficients. Our numerical results demonstrate that distributing and assigning coverage responsibilities to agents depending on their sensing and motion models leads to 40%, 51%, and 46% improvement in coverage performance as measured by the ergodic metric, and 15%, 22%, and 20% improvement in time to find all targets in the search region, for the three methods respectively.},
  archive   = {C_ICRA},
  author    = {Ananya Rao and Guillaume Sartoretti and Howie Choset},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611297},
  month     = {5},
  pages     = {12345-12352},
  title     = {Learning heterogeneous multi-agent allocations for ergodic search},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stackelberg game-theoretic trajectory guidance for
multi-robot systems with koopman operator. <em>ICRA</em>, 12326–12332.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Guided trajectory planning involves a leader robot strategically directing a follower robot to collaboratively reach a designated destination. However, this task becomes notably challenging when the leader lacks complete knowledge of the follower’s decision-making model. There is a need for learning-based methods to effectively design the cooperative plan. To this end, we develop a Stackelberg game-theoretic approach based on the Koopman operator to address the challenge. We first formulate the guided trajectory planning problem through the lens of a dynamic Stackelberg game. We then leverage Koopman operator theory to acquire a learning-based linear system model that approximates the follower’s feedback dynamics. Based on this learned model, the leader devises a collision-free trajectory to guide the follower using receding horizon planning. We use simulations to elaborate on the effectiveness of our approach in generating learning models that accurately predict the follower’s multi-step behavior when compared to alternative learning techniques. Moreover, our approach successfully accomplishes the guidance task and notably reduces the leader’s planning time to nearly half when contrasted with the model-based baseline method 1 .},
  archive   = {C_ICRA},
  author    = {Yuhan Zhao and Quanyan Zhu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610940},
  month     = {5},
  pages     = {12326-12332},
  title     = {Stackelberg game-theoretic trajectory guidance for multi-robot systems with koopman operator},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Collision detection and avoidance for black box multi-robot
navigation. <em>ICRA</em>, 12319–12325. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To date, commercial industrial robots only provide multi-robot coordination for their own fleet of robots and treat robots from other vendors as general obstacles. The ability to enable robots from different vendors to co-exist in the same space is crucial to prevent vendor lock-in. We present the first decentralized system that achieves coordination between a heterogeneous fleet of black box robots for which the internals of the navigation stack are presumed unmodifiable. Our system, which we call CODAK, achieves the coordination by relying on minimum set of interfaces that are commonly available on most industrial and service robots. For each robot, CODAK uses a trained recurrent neural network to anticipate collisions from externally observable metrics. Anticipated collisions are avoided using a simple, but yet effective, concurrency control scheme. We run a series of experiments in simulation and with real robots to demonstrate CODAK’s ability to enable safe navigation in different environments. We also experimentally compare CODAK with previously published white-box solutions to evaluate the penalty of black-box constraint.},
  archive   = {C_ICRA},
  author    = {Sara Ayoubi and Ilija Hadžić and Lou Salaün and Antonio Massaro},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610891},
  month     = {5},
  pages     = {12319-12325},
  title     = {Collision detection and avoidance for black box multi-robot navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Morphable-SfS: Enhancing shape-from-silhouette via
morphable modeling. <em>ICRA</em>, 12312–12318. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reconstructing accurate object shapes based on single image inputs is still a critical and challenging task, mainly due to the potential shape ambiguity and occlusion. Most existing single image 3D reconstruction approaches, either trained on stereo setting or structure-from-motion, estimate 2.5D visible models which generally reconstruct one viewpoint of objects. We propose a method to leverage both the general Morphable Model on common objects and a multi-view synthesis-based shape-from-silhouette model to reconstruct complete object shapes. We use the proposed method to exploit strong geometric and perceptual cues in 3D shape reconstruction. During the inference, the trained model is able to produce high-quality and complete meshes with finely detailed structures from a 2D image captured from arbitrary perspectives. The proposed method is evaluated on both large-scale synthetic ShapeNet and real-world Pascal 3D+ and Pix3D datasets. The proposed work achieves state-of-the-art results compared with other recent self-supervised methods. Moreover, it shows a good capability of being applied in the unseen object reconstruction tasks.},
  archive   = {C_ICRA},
  author    = {Guoyu Lu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610152},
  month     = {5},
  pages     = {12312-12318},
  title     = {Morphable-SfS: Enhancing shape-from-silhouette via morphable modeling},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). I-octree: A fast, lightweight, and dynamic octree for
proximity search. <em>ICRA</em>, 12290–12296. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Establishing the correspondences between newly acquired points and historically accumulated data (i.e., the map) through nearest neighbor search is crucial in numerous robotic applications. However, static tree data structures are inadequate to handle large and dynamically growing maps in real-time. To address this issue, we present the i-Octree, a dynamic octree data structure that supports both fast nearest neighbor search and real-time dynamic updates, such as point insertion, deletion, and on-tree down-sampling. The i-Octree is built upon a leaf-based octree and has two key features: a local spatially continuous storing strategy that allows for fast access to points while minimizing memory usage, and local on-tree updates that significantly reduce computation time compared to existing static or dynamic tree structures. The experiments show that the i-Octree outperforms contemporary state-of-the-art approaches by achieving, on average, a 19% reduction in runtime on real-world open datasets.},
  archive   = {C_ICRA},
  author    = {Jun Zhu and Hongyi Li and Zhepeng Wang and Shengjie Wang and Tao Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611019},
  month     = {5},
  pages     = {12290-12296},
  title     = {I-octree: A fast, lightweight, and dynamic octree for proximity search},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A point-to-distribution degeneracy detection factor for
LiDAR SLAM using local geometric models. <em>ICRA</em>, 12283–12289. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Limited by the working principles, LiDAR-SLAM systems suffer from the degeneration phenomenon in environments such as long corridors and tunnels, due to the lack of sufficient geometric features for frame-to-frame matching. The accuracy and sensitivity of existing degeneracy detection methods need to be further improved. In this paper, we propose a novel method for degeneracy detection using local geometric models based on point-to-distribution matching. To obtain an accurate description of local geometric models, an adaptive adjustment of voxel segmentation according to the point cloud distribution and density is designed. The codes of the proposed method is open-source and available at https://github.com/jisehua/Degenerate-Detection.git. Experiments with public datasets and self-build robots were conducted to evaluate the methods. The results exhibit that our proposed method achieves higher accuracy than the other existing approaches. Applying our proposed method is beneficial for improving the robustness of the LiDAR-SLAM systems.},
  archive   = {C_ICRA},
  author    = {Sehua Ji and Weinan Chen and Zerong Su and Yisheng Guan and Jiehao Li and Hong Zhang and Haifei Zhu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610340},
  month     = {5},
  pages     = {12283-12289},
  title     = {A point-to-distribution degeneracy detection factor for LiDAR SLAM using local geometric models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CppFlow: Generative inverse kinematics for efficient and
robust cartesian path planning. <em>ICRA</em>, 12279–12785. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work we present CppFlow - a novel and performant planner for the Cartesian Path Planning problem, which finds valid trajectories up to 129x faster than current methods, while also succeeding on more difficult problems where others fail. At the core of the proposed algorithm is the use of a learned, generative Inverse Kinematics solver, which is able to efficiently produce promising entire candidate solution trajectories on the GPU. Precise, valid solutions are then found through classical approaches such as differentiable programming, global search, and optimization. In combining approaches from these two paradigms we get the best of both worlds - efficient approximate solutions from generative AI which are made exact using the guarantees of traditional planning and optimization. We evaluate our system against other state of the art methods on a set of established baselines as well as new ones introduced in this work and find that our method significantly outperforms others in terms of the time to find a valid solution and planning success rate, and performs comparably in terms of trajectory length over time. Additional results and an open source implementation is available at https://jstmn.github.io/cppflow-website/.},
  archive   = {C_ICRA},
  author    = {Jeremy Morgan and David Millard and Gaurav S. Sukhatme},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611724},
  month     = {5},
  pages     = {12279-12785},
  title     = {CppFlow: Generative inverse kinematics for efficient and robust cartesian path planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). That’s my point: Compact object-centric LiDAR pose
estimation for large-scale outdoor localisation. <em>ICRA</em>,
12276–12282. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper is about 3D pose estimation on LiDAR scans with extremely minimal storage requirements to enable scalable mapping and localisation. We achieve this by clustering all points of segmented scans into semantic objects and representing them only with their respective centroid and semantic class. In this way, each LiDAR scan is reduced to a compact collection of four-number vectors. This abstracts away important structural information from the scenes, which is crucial for traditional registration approaches. To mitigate this, we introduce an object-matching network based on self- and cross-correlation that captures geometric and semantic relationships between entities. The respective matches allow us to recover the relative transformation between scans through weighted Singular Value Decomposition (SVD) and RANdom SAmple Consensus (RANSAC). We demonstrate that such representation is sufficient for metric localisation by registering point clouds taken under different viewpoints on the KITTI dataset, and at different periods of time localising between KITTI and KITTI-360. We achieve accurate metric estimates comparable with state-of-the-art methods with almost half the representation size, specifically 1.33 kB on average.},
  archive   = {C_ICRA},
  author    = {Georgi Pramatarov and Matthew Gadd and Paul Newman and Daniele De Martini},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611142},
  month     = {5},
  pages     = {12276-12282},
  title     = {That’s my point: Compact object-centric LiDAR pose estimation for large-scale outdoor localisation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RadCloud: Real-time high-resolution point cloud generation
using low-cost radars for aerial and ground vehicles. <em>ICRA</em>,
12269–12275. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we present RadCloud, a novel real-time framework for directly obtaining higher-resolution lidar-like 2D point clouds from low-resolution radar frames on resource-constrained platforms commonly used in unmanned aerial and ground vehicles (UAVs and UGVs, respectively); such point clouds can then be used for accurate environmental mapping, navigating unknown environments, and other robotics tasks. While high-resolution sensing using radar data has been previously reported, existing methods cannot be used on most UAVs, which have limited computational power and energy; thus, existing demonstrations focus on offline radar processing. RadCloud overcomes these challenges by using a radar configuration with 1/4th of the range resolution and employing a deep learning model with 2.25× fewer parameters. Additionally, RadCloud utilizes a novel chirp-based approach that makes obtained point clouds resilient to rapid movements (e.g., aggressive turns or spins) that commonly occur during UAV flights. In real-world experiments, we demonstrate the accuracy and applicability of RadCloud on commercially available UAVs and UGVs, with off-the-shelf radar platforms on-board.},
  archive   = {C_ICRA},
  author    = {David Hunt and Shaocheng Luo and Amir Khazraei and Xiao Zhang and Spencer Hallyburton and Tingjun Chen and Miroslav Pajic},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610839},
  month     = {5},
  pages     = {12269-12275},
  title     = {RadCloud: Real-time high-resolution point cloud generation using low-cost radars for aerial and ground vehicles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On camera model conversions. <em>ICRA</em>, 12262–12268. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {On the one hand, cameras of conventional field-of-view usually considered in computer vision and robotics are very often modeled as a pinhole plus possibly a distortion model. On the other hand, there is a large variety of models for panoramic cameras. Many camera models have been proposed for fisheye cameras, catadioptric cameras, and super fisheye cameras. But in both cases, few models offer the possibility of converting them into another model.This paper contributes to filling this gap in, to allow an algorithm designed with a projection model to accept data of a camera calibrated with another model. So, a pre-existing data set can be used without having to recalibrate the camera. We provide the methodology and mathematical developments for three conversions considering three different types of cameras that are evaluated with respect to calibration and within a visual Simultaneous Localization And Mapping benchmark. The source code of the camera model conversions studied in this paper is shared within the libPeR library for Perception in Robotics: https://github.com/PerceptionRobotique/libPeRbase.},
  archive   = {C_ICRA},
  author    = {Eva Goichon and Guillaume Caron and Pascal Vasseur and Fumio Kanehiro},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610009},
  month     = {5},
  pages     = {12262-12268},
  title     = {On camera model conversions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Geometry-informed distance candidate selection for adaptive
lightweight omnidirectional stereo vision with fisheye images.
<em>ICRA</em>, 12255–12261. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-view stereo omnidirectional distance estimation usually needs to build a cost volume with many hypothetical distance candidates. The cost volume building process is often computationally heavy considering the limited resources a mobile robot has. We propose a new geometry-informed way of distance candidates selection method which enables the use of a very small number of candidates and reduces the computational cost. We demonstrate the use of the geometry-informed candidates in a set of model variants. We find that by adjusting the candidates during robot deployment, our geometry-informed distance candidates also improve a pre-trained model’s accuracy if the extrinsics or the number of cameras changes. Without any re-training or fine-tuning, our models outperform models trained with evenly distributed distance candidates. Models are also released as hardware-accelerated versions with a new dedicated large-scale dataset. The project page, code, and dataset can be found at https://theairlab.org/gicandidates/.},
  archive   = {C_ICRA},
  author    = {Conner Pulling and Je Hon Tan and Yaoyu Hu and Sebastian Scherer},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611359},
  month     = {5},
  pages     = {12255-12261},
  title     = {Geometry-informed distance candidate selection for adaptive lightweight omnidirectional stereo vision with fisheye images},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CrazySim: A software-in-the-loop simulator for the crazyflie
nano quadrotor. <em>ICRA</em>, 12248–12254. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work we develop a software-in-the-loop simulator platform for Crazyflie nano quadrotor drone fleets. One of the challenges in maintaining a large fleet of drones is ensuring that the fleet performs its task as expected without collision, and this becomes more challenging as the number of drones scales, possibly into the hundreds. Software-in-the-loop simulation is an important component in verifying that drone fleets operate correctly and can significantly reduce development time. The simulator interface that we develop runs an instance of the Crazyflie flight stack firmware for each individual drone on a commercial, desktop machine along with a sensors and communication plugin on Gazebo Sim. The plugin transmits simulated sensor information to the firmware along with a socket link interface to run external scripts that would be run on a ground station during hardware deployment. The plugin simulates a radio communication delay between the drones and the ground station to test offboard control algorithms and high-level fleet commands. To validate the proposed simulator, we provide a case study of decentralized model predictive control (MPC) that is run on a ground station to command a fleet of sixteen drones to follow a specified trajectory. We first run the controller on the simulator interface to verify performance and robustness of the algorithm before deployment to a Crazyflie hardware experiment in the Georgia Tech Robotarium.},
  archive   = {C_ICRA},
  author    = {Christian Llanes and Zahi Kakish and Kyle Williams and Samuel Coogan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610906},
  month     = {5},
  pages     = {12248-12254},
  title     = {CrazySim: A software-in-the-loop simulator for the crazyflie nano quadrotor},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatically designing robot swarms in environments
populated by other robots: An experiment in robot shepherding.
<em>ICRA</em>, 12240–12247. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automatic design is a promising approach to realizing robot swarms. Given a mission to be performed by the swarm, an automatic method produces the required control software for the individual robots. Automatic design has concentrated on missions that a swarm can execute independently, interacting only with a static environment and without the involvement of other active entities. In this paper, we investigate the design of robot swarms that perform their mission by interacting with other robots that populate their environment. We frame our research within robot shepherding: the problem of using a small group of robots—the shepherds— to coordinate a relatively larger group—the sheep. In our study, the group of shepherds is the swarm that is automatically designed, and the sheep are pre-programmed robots that populate its environment. We use automatic modular design and neuroevolution to produce the control software for the swarm of shepherds to coordinate the sheep. We show that automatic design can leverage mission-specific interaction strategies to enable an effective coordination between the two groups.},
  archive   = {C_ICRA},
  author    = {David Garzón Ramos and Mauro Birattari},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611309},
  month     = {5},
  pages     = {12240-12247},
  title     = {Automatically designing robot swarms in environments populated by other robots: An experiment in robot shepherding},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reciprocal and non-reciprocal swarmalators with programmable
locomotion and formations for robot swarms. <em>ICRA</em>, 12233–12239.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Natural and robotic swarms often exhibit nonreciprocal interactions; agents do not exhibit equal and opposite forces on each other. By studying the effects of reciprocal and non-reciprocal interactions we are better able to design emergent behaviors in robot collectives composed of agents that exert attractive and repulsive forces on each other. Moreover, by controlling agent-specific coupling forces on-demand, we can enable a collective to exhibit desired behaviors previously not possible. We use a general form of the swarming oscillator, swarmalator, model to study reciprocal and non-reciprocal interactions among agents that affect each other’s motions over long and short distances, we use non-reciprocal coupling to elicit collective locomotion toward or away from target sites, and we use the control barrier function method to optimize the non-reciprocal interactions for a desired spatial formation. This work addresses the interests of the active matter, swarm robotics, and control barrier functions communities and demonstrates various collective behaviors with strong potential to be realized in macro- and micro- length scale robot swarms.},
  archive   = {C_ICRA},
  author    = {Steven Ceron and Wei Xiao and Daniela Rus},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610540},
  month     = {5},
  pages     = {12233-12239},
  title     = {Reciprocal and non-reciprocal swarmalators with programmable locomotion and formations for robot swarms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flock-formation control of multi-agent systems using
imperfect relative distance measurements. <em>ICRA</em>, 12193–12200.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present distributed distance-based control (DDC), a novel approach for controlling a multi-agent system, such that it achieves a desired formation, in a resource-constrained setting. Our controller is fully distributed and only requires local state-estimation and scalar measurements of inter-agent distances. It does not require an external localization system or inter-agent exchange of state information. Our approach uses spatial-predictive control (SPC), to optimize a cost function given strictly in terms of inter-agent distances and the distance to the target location. In DDC, each agent continuously learns and updates a very abstract model of the actual system, in the form of a dictionary of three independent key-value pairs $(\Delta \vec s,\Delta d)$, where ∆d is the partial derivative of the distance measurements along a spatial direction $\Delta \vec s$. This is sufficient for an agent to choose the best next action. We validate our approach by using DDC to control a collection of Crazyflie drones to achieve formation flight and reach a target while maintaining flock formation.},
  archive   = {C_ICRA},
  author    = {Andreas Brandstätter and Scott A. Smolka and Scott D. Stoller and Ashish Tiwari and Radu Grosu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610147},
  month     = {5},
  pages     = {12193-12200},
  title     = {Flock-formation control of multi-agent systems using imperfect relative distance measurements},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Practical and safe navigation function based motion planning
of UAVs. <em>ICRA</em>, 12186–12192. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper offers a practical method for certifiably safe operations of an unmanned aerial vehicle (UAV) with limited power and computation, useful for real-time operations where the UAV is exposed to significant disturbances in non-convex free space. We propose a motion planning method based on the Explicit Reference Governor (ERG) framework to ensure the safety of a flying quadrotor UAV. From a small set of experiment data and assumptions on modeling errors, a Lyapunov function is synthesized by which an ERG is constructed to modify the UAV set-points. The method can handle polyhedral obstacles and constraints imposed on the maximum thrust of the UAV and its maximum tilt. We demonstrate the approach with extensive simulations and experiments using a Crazyflie 2.1.},
  archive   = {C_ICRA},
  author    = {Himani Sinhmar and Marcus Greiff and Stefano Di Cairano},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611255},
  month     = {5},
  pages     = {12186-12192},
  title     = {Practical and safe navigation function based motion planning of UAVs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Physics-informed neural motion planning on constraint
manifolds. <em>ICRA</em>, 12179–12185. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Constrained Motion Planning (CMP) aims to find a collision-free path between the given start and goal configurations on the kinematic constraint manifolds. These problems appear in various scenarios ranging from object manipulation to legged-robot locomotion. However, the zero-volume nature of manifolds makes the CMP problem challenging, and the state-of-the-art methods still take several seconds to find a path and require a computationally expansive path dataset for imitation learning. Recently, physics-informed motion planning methods have emerged that directly solve the Eikonal equation through neural networks for motion planning and do not require expert demonstrations for learning. Inspired by these approaches, we propose the first physics-informed CMP framework that solves the Eikonal equation on the constraint manifolds and trains neural function for CMP without expert data. Our results show that the proposed approach efficiently solves various CMP problems in both simulation and real-world, including object manipulation under orientation constraints and door opening with a high-dimensional 6-DOF robot manipulator. In these complex settings, our method exhibits high success rates and finds paths in sub-seconds, which is many times faster than the state-of-the-art CMP methods.},
  archive   = {C_ICRA},
  author    = {Ruiqi Ni and Ahmed H. Qureshi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610883},
  month     = {5},
  pages     = {12179-12185},
  title     = {Physics-informed neural motion planning on constraint manifolds},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A control barrier function-based motion planning scheme for
a quadruped robot. <em>ICRA</em>, 12172–12178. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A Control Barrier Function (CBF)-based motion planning algorithm is proposed. The algorithm explores an unknown environment to reach a target point, providing velocity commands to the robot controller module. CBFs, along with a circulation inequality are used to generate safe paths toward the goal while preventing collisions with obstacles. The proposed global navigation scheme is experimentally verified on a quadruped platform to demonstrate safe, collision-free exploration over long distances.},
  archive   = {C_ICRA},
  author    = {Halil Utku Unlu and Vinicius Mariano Gonçalves and Dimitris Chaikalis and Anthony Tzes and Farshad Khorrami},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610210},
  month     = {5},
  pages     = {12172-12178},
  title     = {A control barrier function-based motion planning scheme for a quadruped robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IKLink: End-effector trajectory tracking with minimal
reconfigurations. <em>ICRA</em>, 12165–12171. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many applications require a robot to accurately track reference end-effector trajectories. Certain trajectories may not be tracked as single, continuous paths due to the robot’s kinematic constraints or obstacles elsewhere in the environment. In this situation, it becomes necessary to divide the trajectory into shorter segments. Each such division introduces a reconfiguration, in which the robot deviates from the reference trajectory, repositions itself in configuration space, and then resumes task execution. The occurrence of reconfigurations should be minimized because they increase time and energy usage. In this paper, we present IKLink, a method for finding joint motions to track reference end-effector trajectories while executing the minimum number of reconfigurations. Our graph-based method generates a diverse set of Inverse Kinematics (IK) solutions for every waypoint on the reference trajectory and utilizes a dynamic programming algorithm to find the optimal motion by linking the IK solutions. We demonstrate the effectiveness of IKLink through a simulation experiment and an illustrative demonstration using a physical robot.},
  archive   = {C_ICRA},
  author    = {Yeping Wang and Carter Sifferman and Michael Gleicher},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611510},
  month     = {5},
  pages     = {12165-12171},
  title     = {IKLink: End-effector trajectory tracking with minimal reconfigurations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-profile quadratic programming (MPQP) for optimal gap
selection and speed planning of autonomous driving. <em>ICRA</em>,
12158–12164. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Smooth and safe speed planning is imperative for the successful deployment of autonomous vehicles. This paper presents a mathematical formulation for the optimal speed planning of autonomous driving, which has been validated in high-fidelity simulations and real-road demonstrations with practical constraints. The algorithm explores the inter-traffic gaps in the time and space domain using a breadth-first search. For each gap, quadratic programming finds an optimal speed profile, synchronizing the time and space pair along with dynamic obstacles. Qualitative and quantitative analysis in Carla is reported to discuss the smoothness and robustness of the proposed algorithm. Finally, we present a road demonstration result for urban city driving.},
  archive   = {C_ICRA},
  author    = {Alexandre Miranda Añon and Sangjae Bae and Manish Saroya and David Isele},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611173},
  month     = {5},
  pages     = {12158-12164},
  title     = {Multi-profile quadratic programming (MPQP) for optimal gap selection and speed planning of autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A constrained path following method for snake-like
manipulators via controlled winding uncoiling strategy. <em>ICRA</em>,
12151–12157. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Benefiting from its hyper-redundant structure, the biomimetic snake-like manipulator retains its remarkable flexibility even within confined spaces. However, its motion planning and control pose significant challenges. This paper imitates the winding uncoiling behavior of snakes to achieve controllable constrained path following. Firstly, based on control points, a recursive computational model and an equivalent planning angle model are established, enabling efficient and analytical determination of joint positions, collision regions, and motion parameters during the path following. Subsequently, the sliding control point algorithm and motion smoothing restriction algorithm are designed. The former ensures that the remaining segments during following strictly remain within the collision-free regions defined by the base and path controls, while the latter smooths the control parameters based on velocity and acceleration limitations. Finally, simulation and practical experiments demonstrate the feasibility of the proposed methods. The prototype that applied our method can reach targets and accomplish tasks, further validating the applicability of the snake-like manipulator.},
  archive   = {C_ICRA},
  author    = {Mingrui Luo and Yunong Tian and Yinghua Cao and Minghao Chen and Yanfeng Zhang and En Li and Min Tan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610468},
  month     = {5},
  pages     = {12151-12157},
  title     = {A constrained path following method for snake-like manipulators via controlled winding uncoiling strategy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SE(2) assembly planning for magnetic modular cubes.
<em>ICRA</em>, 12144–12150. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Magnetic modular cubes are cube-shaped bodies with embedded permanent magnets. The cubes are uniformly controlled by a global time-varying magnetic field.A 2D physics simulator is used to simulate global control and the resulting continuous movement of magnetic modular cube structures. We develop local plans, closed-loop control algorithms for planning the connection of two structures at desired faces. The global planner generates a building instruction graph for a target structure that we traverse in a depth-first-search approach by repeatedly applying local plans.We analyze how structure size and shape affect planning time. The planner solves 80% of the randomly created instances with up to 12 cubes in an average time of about 200 seconds.},
  archive   = {C_ICRA},
  author    = {Kjell Keune and Aaron T. Becker},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610494},
  month     = {5},
  pages     = {12144-12150},
  title     = {SE(2) assembly planning for magnetic modular cubes},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Leveraging opportunism in sample-based motion planning.
<em>ICRA</em>, 12137–12143. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sample-based motion planning approaches, such as RRT*, have been widely adopted in robotics due to their support for high-dimensional state spaces and guarantees of completeness and optimality. This paper introduces an RRT* approach (ORRT*) that leverages opportunism to (1) find solutions quickly, (2) reduce wasted compute, and (3) improve data efficiency. The key insight of the approach is to make the most of compute when expanding the search tree by adding the last viable configurations found when connecting new nodes rather than rejecting the sampled nodes outright, allowing for more productive exploration of the space. We evaluate the proposed approach in a set of mobility and manipulator postural control domains, contrasting the performance of the opportunistic approach with state-of-the-art RRT* variants. Our analysis shows that such an approach has desirable characteristics and warrants further exploration.},
  archive   = {C_ICRA},
  author    = {Michael W. Lanighan and Oscar Youngquist},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610936},
  month     = {5},
  pages     = {12137-12143},
  title     = {Leveraging opportunism in sample-based motion planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing the tracking performance of passivity-based
high-frequency robot cloud control. <em>ICRA</em>, 12097–12103. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the migration of high-frequency robot controllers to remote computing services, which are connected via a communication channel prone to delays and packet loss. The stability of the networked system is guaranteed by ensuring passivity of each subcomponent in the interconnection, as well as the Time-Domain-Passivity-Approach (TDPA) for the communication channel. We reduce conservatism of the TDPA using the model knowledge on both sides of the communication system to identify passivity excesses. This is further used to avoid over-dissipation of energy in the passivity controller by augmentation of a tolerable passivity-shortage. Tracking offsets are eliminated with a position drift compensation algorithm, for which convergence guarantees are provided. The experimental validation of the results conducted on a 7-DoF Franka Research 3 robot demonstrates a substantial enhancement in tracking performance due to the proposed modifications, particularly in scenarios with high communication delays.},
  archive   = {C_ICRA},
  author    = {Fabian Jakob and Xiao Chen and Hamid Sadeghian and Sami Haddadin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610616},
  month     = {5},
  pages     = {12097-12103},
  title     = {Enhancing the tracking performance of passivity-based high-frequency robot cloud control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Opportunistic communication in robot teams. <em>ICRA</em>,
12090–12096. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we present a new approach to Mobile Infrastructure on Demand (MID) where a dedicated team of robots creates and sustains a wireless network that satisfies the communication requirements of a different team of task-oriented robots seeking to coordinate their actions in the absence of existing communication infrastructure. Different from previous works, our approach forgoes heuristics for network performance such as algebraic-connectivity or network flow optimizations and instead positions communication support robots to directly maximize the probability of packet delivery by the underlying opportunistic routing protocol. Our system is task agnostic and practical to implement and operate on robots equipped with off-the-shelf WiFi radios. We demonstrate this through a set of experiments showing our MID system maintaining the delivery of critical mission data in a situational awareness setting and enabling foraging robots to effectively coordinate their actions during multi-robot exploration.},
  archive   = {C_ICRA},
  author    = {Daniel Mox and Kashish Garg and Alejandro Ribeiro and Vijay Kumar},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610971},
  month     = {5},
  pages     = {12090-12096},
  title     = {Opportunistic communication in robot teams},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FogROS2-config: A toolkit for choosing server configurations
for cloud robotics. <em>ICRA</em>, 12083–12089. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cloud service providers provide over 50,000 distinct and dynamically changing set of cloud server options. To help roboticists make cost-effective decisions, we present FogROS2-Config, an open toolkit that takes ROS2 nodes as input and automatically runs relevant benchmarks to quickly return a menu of cloud compute services that tradeoff latency and cost. Because it is infeasible to try every hardware configuration, FogROS2-Config quickly samples tests a small set of edge-case servers. We evaluate FogROS2-Config on three robotics application tasks: visual SLAM, grasp planning. and motion planning. FogROS2-Config can reduce the cost by up to 20x. By comparing with a Pareto frontier for cost and latency by running the application task on feasible server configurations, we evaluate cost and latency models and confirm that FogROS2-Config selects efficient hardware configurations to balance cost and latency. Videos and code are available on the website https://sites.google.com/view/fogros2-config},
  archive   = {C_ICRA},
  author    = {Kaiyuan Chen and Kush Hari and Rohil Khare and Charlotte Le and Trinity Chung and Jaimyn Drake and Jeffrey Ichnowski and John Kubiatowicz and Ken Goldberg},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611367},
  month     = {5},
  pages     = {12083-12089},
  title     = {FogROS2-config: A toolkit for choosing server configurations for cloud robotics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CloudGripper: An open source cloud robotics testbed for
robotic manipulation research, benchmarking and data collection at
scale. <em>ICRA</em>, 12076–12082. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present CloudGripper, an open source cloud robotics testbed, consisting of a scalable, space and cost- efficient design constructed as a rack of 32 small robot arm work cells. Each robot work cell is fully enclosed and features individual lighting, a low-cost Cartesian robot arm with an attached rotatable parallel jaw gripper and a dual camera setup for experimentation. The system design is focused on continuous operation and features a 10 Gbit/s network connectivity allowing for high throughput remote-controlled experimentation and data collection for robotic manipulation. Furthermore, CloudGripper is intended to form a community testbed to study the challenges of large scale machine learning and cloud and edge-computing in the context of robotic manipulation. In this work, we describe the mechanical design of the system, its initial software stack and evaluate the repeatability of motions executed by the proposed robot arm design. A local network API throughput and latency analysis is also provided. CloudGripper-Rope-100, a dataset of more than a hundred hours of randomized rope pushing interactions and approximately 4 million camera images is collected and serves as a proof of concept demonstrating data collection capabilities. A project website with more information is available at https://cloudgripper.org.},
  archive   = {C_ICRA},
  author    = {Muhammad Zahid and Florian T. Pokorny},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611548},
  month     = {5},
  pages     = {12076-12082},
  title     = {CloudGripper: An open source cloud robotics testbed for robotic manipulation research, benchmarking and data collection at scale},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Environmental awareness dynamic 5G QoS for retaining real
time constraints in robotic applications. <em>ICRA</em>, 12069–12075.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The fifth generation (5G) cellular network technology is mature and increasingly utilized in many industrial and robotics applications, while an important functionality is the advanced Quality of Service (QoS) features. Despite the prevalence of 5G QoS discussions in the related literature, there is a notable absence of real-life implementations and studies concerning their application in time-critical robotics scenarios. This article considers the operation of time-critical applications for 5G-enabled unmanned aerial vehicles (UAVs) and how their operation can be improved by the possibility to dynamically switch between QoS data flows with different priorities. As such, we introduce a robotics oriented analysis on the impact of the 5G QoS functionality on the performance of 5G-enabled UAVs. Furthermore, we introduce a novel framework for the dynamic selection of distinct 5G QoS data flows that is autonomously managed by the 5G-enabled UAV. This problem is addressed in a novel feedback loop fashion utilizing a probabilistic finite state machine (PFSM). Finally, the efficacy of the proposed scheme is experimentally validated with a 5G-enabled UAV in a real-world 5G stand-alone (SA) network. https://www.youtube.com/watch?v=lWtMOlVMEFI&amp;t=1s},
  archive   = {C_ICRA},
  author    = {Gerasimos Damigos and Akshit Saradagi and Sara Sandberg and George Nikolakopoulos},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610698},
  month     = {5},
  pages     = {12069-12075},
  title     = {Environmental awareness dynamic 5G QoS for retaining real time constraints in robotic applications},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A distributed multi-robot framework for exploration,
information acquisition and consensus. <em>ICRA</em>, 12062–12068. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The distributed coordination of robot teams performing complex tasks is challenging to formulate. The different aspects of a complete task such as local planning for obstacle avoidance, global goal coordination and collaborative mapping are often solved separately, when clearly each of these should influence the others for the most efficient behaviour. In this paper we use the example application of distributed information acquisition as a robot team explores a large space to show that we can formulate the whole problem as a single factor graph with multiple connected layers representing each aspect. We use Gaussian Belief Propagation (GBP) as the inference mechanism, which permits parallel, on-demand or asynchronous computation for efficiency when different aspects are more or less important. This is the first time that a distributed GBP multi-robot solver has been proven to enable intelligent collaborative behaviour rather than just guiding robots to individual, selfish goals. We encourage the reader to view our demos at https://aalpatya.github.io/gbpstack.},
  archive   = {C_ICRA},
  author    = {Aalok Patwardhan and Andrew J. Davison},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610185},
  month     = {5},
  pages     = {12062-12068},
  title     = {A distributed multi-robot framework for exploration, information acquisition and consensus},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stability analysis of distance-angle leader-follower
formation control*. <em>ICRA</em>, 12055–12061. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Necessary and sufficient conditions are described for stable distance-angle leader-follower formation control of first- and second-order holonomic and non-holonomic mobile robots. The distance-angle leader-follower formation is a problem of maintaining the desired relative distance and orientation of robots in a group. Our analysis shows that the input constraints on the leader are necessary for stable formation control. These constraints are summarized as follows: 1) In a team of first (second) order holonomic mobile robots, the leader has to be controlled as a first (second) order non-holonomic mobile robot; 2) In a team of first (second) order non-holonomic mobile robots, the control input of the leader must be limited so that the curvature is first (second) order differentiable. We further show that these constraints are sufficient for the followers to maintain formation. Moreover, we present globally asymptotically stable controllers and describe simulation experiments that demonstrate the effectiveness of these controllers.},
  archive   = {C_ICRA},
  author    = {Manao Machida and Masumi Ichien},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611001},
  month     = {5},
  pages     = {12055-12061},
  title     = {Stability analysis of distance-angle leader-follower formation control*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed control barrier functions for global
connectivity maintenance. <em>ICRA</em>, 12048–12054. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we propose a framework for the distributed implementation of Quadratic Programs-based controllers, building upon and rectifying a significant limitation in a previously presented approach. The proposed framework is primarily motivated by the distributed implementation of Control Barrier Functions (CBFs), whose primary objective is to make minimal adjustments to a nominal controller while ensuring constraint satisfaction. By improving over some limitations in the current state-of-the-art, we are able to apply distributed CBFs to the problem of global connectivity maintenance in presence of communication and sensing constraints. Specifically, we consider the problem of preserving connectivity for a group of quadrotors with onboard sensors under distance and field of view constraints. Leveraging distributed control barrier functions, our approach maintains global graph connectivity while optimizing the performance of the desired task. Numerical simulations validate its effectiveness.},
  archive   = {C_ICRA},
  author    = {Nicola De Carli and Paolo Salaris and Paolo Robuffo Giordano},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610611},
  month     = {5},
  pages     = {12048-12054},
  title     = {Distributed control barrier functions for global connectivity maintenance},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Coordinated landing control for cross-domain UAV-USV fleets
using heterogeneous-feature matching. <em>ICRA</em>, 12041–12047. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Coordinated landing control for multiple unmanned aerial vehicles (UAVs) on appropriate multiple unmanned surface vehicles (USVs) is an urgent yet challenging mission with the tremendous development of modern marine industry. To this end, we propose a coordinated multiple UAV-USV landing control algorithm via heterogeneous-feature matching. Specifically, the heterogeneous landing features of different UAVs and USVs are extracted to establish a dynamic UAV-USV cooperative landing ability mapping for the cross-domain UAV-USV fleets (CDUUFs). Then, by incorporating suitable allocation with UAV-USV landing convergence and collision avoidance among UAVs into constraints with the assistance of both control Lyapunov functions (CLFs) and control barrier functions (CBFs), the multiple UAV-USV landing control problem is formulated as a constraint-based optimization one. Therein, slack variables are introduced to fulfill the assignment and facilitate the searching of a balanced solution between control performance and landing safety. Finally, extensive simulations are conducted to substantiate the effectiveness of the present multiple UAV-USV landing control law.},
  archive   = {C_ICRA},
  author    = {Jianing Ding and Hai-Tao Zhang and Bin-Bin Hu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611392},
  month     = {5},
  pages     = {12041-12047},
  title     = {Coordinated landing control for cross-domain UAV-USV fleets using heterogeneous-feature matching},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CTA-LO: Accurate and robust LiDAR odometry using
continuous-time adaptive estimation. <em>ICRA</em>, 12034–12040. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate and robust LiDAR odometry is a crucial technology for robot localization. However, motion distortion and ranging error make it a bottleneck. Most existing methods are limited in accuracy and robustness because they simply compensate for motion distortion by constant velocity motion assumption without accurate model of ranging error. In this paper, we propose a high-precision and robust LiDAR odometry (LO), which utilizes continuous-time estimation to remove LiDAR distortion and builds the spot uncertainty model to quantify the ranging error. Generally, the number of variables in continuous-time estimation is several times higher than that in discrete-time ones, leading to insufficient constraints on the LiDAR odometry. To solve this problem, we propose a marginalization method to retain prior scans’ constraints by exploiting the local support property of the B-spline. To further improve the odometry accuracy, we propose a residual adaptive weighting method and a probabilistic point cloud map based on the spot uncertainty model of LiDAR points. The experimental results show that our method outperforms state-of-the-art LiDAR odometry in accuracy and robustness.},
  archive   = {C_ICRA},
  author    = {Yuezhang Lv and Yunzhou Zhang and Xiaoyu Zhao and Wu Li and Jian Ning and Yang Jin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611453},
  month     = {5},
  pages     = {12034-12040},
  title     = {CTA-LO: Accurate and robust LiDAR odometry using continuous-time adaptive estimation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DMSA - dense multi scan adjustment for LiDAR inertial
odometry and global optimization. <em>ICRA</em>, 12027–12033. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a new method for fine registering multiple point clouds simultaneously. The approach is characterized by being dense, therefore point clouds are not reduced to pre-selected features in advance. Furthermore, the approach is robust against small overlaps and dynamic objects, since no direct correspondences are assumed between point clouds. Instead, all points are merged into a global point cloud, whose scattering is then iteratively reduced. This is achieved by dividing the global point cloud into uniform grid cells whose contents are subsequently modeled by normal distributions. We show that the proposed approach can be used in a sliding window continuous trajectory optimization combined with IMU measurements to obtain a highly accurate and robust LiDAR inertial odometry estimation. Furthermore, we show that the proposed approach is also suitable for large scale keyframe optimization to increase accuracy. We provide the source code and some experimental data on https://github.com/davidskdds/DMSA_LiDAR_SLAM.git.},
  archive   = {C_ICRA},
  author    = {David Skuddis and Norbert Haala},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610818},
  month     = {5},
  pages     = {12027-12033},
  title     = {DMSA - dense multi scan adjustment for LiDAR inertial odometry and global optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DVI-SLAM: A dual visual inertial SLAM network.
<em>ICRA</em>, 12020–12026. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent deep learning based visual simultaneous localization and mapping (SLAM) methods have made significant progress. However, how to make full use of visual information as well as better integrate with inertial measurement unit (IMU) in visual SLAM has potential research value. This paper proposes a novel deep SLAM network with dual visual factors. The basic idea is to integrate both photometric factor and re-projection factor into the end-to-end differentiable structure through multi-factor data association module. We show that the proposed network dynamically learns and adjusts the confidence maps of both visual factors and it can be further extended to include the IMU factors as well. Extensive experiments validate that our proposed method significantly outperforms the state-of-the-art methods on several public datasets, including TartanAir, EuRoC and ETH3D-SLAM. Specifically, when dynamically fusing the three factors together, the absolute trajectory error for both monocular and stereo configurations on EuRoC dataset has reduced by 45.3% and 36.2% respectively.},
  archive   = {C_ICRA},
  author    = {Xiongfeng Peng and Zhihua Liu and Weiming Li and Ping Tan and Soon Yong Cho and Qiang Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610042},
  month     = {5},
  pages     = {12020-12026},
  title     = {DVI-SLAM: A dual visual inertial SLAM network},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantically guided feature matching for visual SLAM.
<em>ICRA</em>, 12013–12019. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a new algorithm that utilizes semantic information to enhance feature matching in visual SLAM pipelines. The proposed method constructs a high-dimensional semantic descriptor for each detected ORB feature. When integrated with traditional visual ones, these descriptors aid in establishing accurate tentative point correspondences between consecutive frames. Additionally, our semantic descriptors enrich 3D map points, enhancing loop closure detection by providing deeper insights into the underlying map regions. Experiments on public large-scale datasets demonstrate that our technique surpasses the accuracy of established methods. Importantly, given its detector-agnostic nature, our algorithm also amplifies the efficacy of modern keypoint detectors, such as SuperPoint. The implementation of our algorithm can be found on Github 3 .},
  archive   = {C_ICRA},
  author    = {Oguzhan Ilter and Iro Armeni and Marc Pollefeys and Daniel Barath},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610238},
  month     = {5},
  pages     = {12013-12019},
  title     = {Semantically guided feature matching for visual SLAM},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-radar inertial odometry for 3D state estimation using
mmWave imaging radar. <em>ICRA</em>, 12006–12012. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {State estimation is a crucial component for the successful implementation of robotic systems, relying on sensors such as cameras, LiDAR, and IMUs. However, in real-world scenarios, the performance of these sensors is degraded by challenging environments, e.g. adverse weather conditions and low-light scenarios. The emerging 4D imaging radar technology is capable of providing robust perception in adverse conditions. Despite its potential, challenges remain for indoor settings where noisy radar data does not present clear geometric features. Moreover, disparities in radar data resolution and field of view (FOV) can lead to inaccurate measurements. While prior research has explored radar-inertial odometry based on Doppler velocity information, challenges remain for the estimation of 3D motion because of the discrepancy in the FOV and resolution of the radar sensor. In this paper, we address Doppler velocity measurement uncertainties. We present a method to optimize body frame velocity while managing Doppler velocity uncertainty. Based on our observations, we propose a dual imaging radar configuration to mitigate the challenge of discrepancy in radar data. To attain high-precision 3D state estimation, we introduce a strategy that seamlessly integrates radar data with a consumer-grade IMU sensor using fixed-lag smoothing optimization. Finally, we evaluate our approach using real-world 3D motion data.},
  archive   = {C_ICRA},
  author    = {Jui-Te Huang and Ruoyang Xu and Akshay Hinduja and Michael Kaess},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611194},
  month     = {5},
  pages     = {12006-12012},
  title     = {Multi-radar inertial odometry for 3D state estimation using mmWave imaging radar},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IMU-aided event-based stereo visual odometry. <em>ICRA</em>,
11977–11983. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Direct methods for event-based visual odometry solve the mapping and camera pose tracking sub-problems by establishing implicit data association in a way that the generative model of events is exploited. The main bottlenecks faced by state-of-the-art work in this field include the high computational complexity of mapping and the limited accuracy of tracking. In this paper, we improve our previous direct pipeline Event-based Stereo Visual Odometry in terms of accuracy and efficiency. To speed up the mapping operation, we propose an efficient strategy of edge-pixel sampling according to the local dynamics of events. The mapping performance in terms of completeness and local smoothness is also improved by combining the temporal stereo results and the static stereo results. To circumvent the degeneracy issue of camera pose tracking in recovering the yaw component of general 6-DoF motion, we introduce as a prior the gyroscope measurements via pre-integration. Experiments on publicly available datasets justify our improvement. We release our pipeline as an open-source software for future research in this field.},
  archive   = {C_ICRA},
  author    = {Junkai Niu and Sheng Zhong and Yi Zhou},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611439},
  month     = {5},
  pages     = {11977-11983},
  title     = {IMU-aided event-based stereo visual odometry},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient pose prediction with rational regression applied
to vSLAM. <em>ICRA</em>, 11970–11976. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Compared to polynomial splines, rational functions are known to be more efficient and well-behaved data fitting models. However, due to the potential presence of zeros in their denominator, rational functions tend to yield notoriously hard optimization problems. In this work, we present a novel least squares method for 6D pose prediction that employs rational regression. Our method can accommodate fixed data points and is able to circumvent the occurrence of zeros for rational quadratic interpolants. We demonstrate the suitability of rational quadratics for pose prediction by applying our approach to real data from the feature tracking stage of a real-time visual SLAM system and showing that it yields far more stable predictions when compared to state-of-the-art rational and polynomial spline methods.},
  archive   = {C_ICRA},
  author    = {George Terzakis and Manolis Lourakis},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611457},
  month     = {5},
  pages     = {11970-11976},
  title     = {Efficient pose prediction with rational regression applied to vSLAM},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust indoor localization with ranging-IMU fusion.
<em>ICRA</em>, 11963–11969. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Indoor wireless ranging localization is a promising approach for low-power and high-accuracy localization of wearable devices. A primary challenge in this domain stems from non-line of sight propagation of radio waves. This study tackles a fundamental issue in wireless ranging: the unpredictability of real-time multipath determination, especially in challenging conditions such as when there is no direct line of sight. We achieve this by fusing range measurements with inertial measurements obtained from a low cost Inertial Measurement Unit (IMU). For this purpose, we introduce a novel asymmetric noise model crafted specifically for non-Gaussian multipath disturbances. Additionally, we present a novel Levenberg-Marquardt (LM)-family trust-region adaptation of the iSAM2 fusion algorithm, which is optimized for robust performance for our ranging-IMU fusion problem. We evaluate our solution in a densely occupied real office environment. Our proposed solution can achieve temporally consistent localization with an average absolute accuracy of ∼0.3m in real-world settings. Furthermore, our results indicate that we can achieve comparable accuracy even with infrequent range measurements down to 1Hz.},
  archive   = {C_ICRA},
  author    = {Fan Jiang and David Caruso and Ashutosh Dhekne and Qi Qu and Jakob Julian Engel and Jing Dong},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611274},
  month     = {5},
  pages     = {11963-11969},
  title     = {Robust indoor localization with ranging-IMU fusion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An equivariant approach to robust state estimation for the
ArduPilot autopilot system. <em>ICRA</em>, 11956–11962. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The majority of commercial and open-source autopilot software for uncrewed aerial vehicles rely on the tried and tested extended Kalman filter (EKF) to provide the state estimation solution for the inertial navigation system (INS). While modern implementations achieve remarkable robustness, it is often due to the careful implementation of exception code for a multitude of corner cases along with significant skilled tuning effort. In this paper, we use the data wealth of the ArduPilot community to identify and highlight the most common real-world challenges in INS state estimation, including sensor self-calibration, robustness in static conditions, global navigation satellite system (GNSS) outliers and shifts, and robustness to faulty inertial measurement units (IMUs). We propose a novel equivariant filter (EqF) formulation for the INS solution that exploits a Semi-Direct-Bias symmetry group for multi-sensor fusion with self-calibration capabilities and incorporates equivariant velocity-type measurements. We augment the filter with a simple innovation-covariance inflation strategy that seamlessly handles GNSS outliers and shifts without requiring coding of a whole set of exception cases. We use real-world data from the Ardupilot community to demonstrate the performance of the proposed filter on known cases where existing filters fail without careful exception handling or case-specific tuning and benchmark against the ArduPilot’s EKF3, the most sophisticated EKF implementation currently available.},
  archive   = {C_ICRA},
  author    = {Alessandro Fornasier and Yixiao Ge and Pieter van Goor and Martin Scheiber and Andrew Tridgell and Robert Mahony and Stephan Weiss},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611108},
  month     = {5},
  pages     = {11956-11962},
  title     = {An equivariant approach to robust state estimation for the ArduPilot autopilot system},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A nonlinear estimator for dead reckoning of aquatic surface
vehicles using an IMU and a doppler velocity log. <em>ICRA</em>,
11941–11947. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Aquatic robots require an accurate and reliable localization system to navigate autonomously and perform practical missions. Kalman filters (KFs) and their variants are typically used in aquatic robots to combine sensor data. The two critical drawbacks of KFs are the requirement for skilled tuning of several filter parameters and the fact that changes to how the Inertial Measurement Unit (IMU) is oriented necessitate modifying the filter. To overcome those problems, this paper presents a novel method of fusing sensor data from a Doppler Velocity Log (DVL) and IMU using an adaptive nonlinear estimator to provide dead reckoning localization for a small autonomous surface vehicle. The proposed method has only one insensitive tuning parameter and is agnostic to the configuration of the IMU. The system was validated using a small ASV in a 2.4×3.6×2.4 m water tank, with a motion capture system as ground truth, and was evaluated against a state-of-the-art method based on KFs. Experiments showed that the average drift error of the nonlinear filter was 0.16 m (s.d. 0.06 m) compared to 0.15 m (s.d. 0.05 m) for the state of the art, meaning that the benefits in terms of tuning and flexible configuration do not come at the expense of performance.},
  archive   = {C_ICRA},
  author    = {Jessica Paterson and Bruno V. Adorno and Barry Lennox and Keir Groves},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611718},
  month     = {5},
  pages     = {11941-11947},
  title     = {A nonlinear estimator for dead reckoning of aquatic surface vehicles using an IMU and a doppler velocity log},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accurate prior-centric monocular positioning with offline
LiDAR fusion. <em>ICRA</em>, 11934–11940. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unmanned vehicles usually rely on Global Positioning System (GPS) and Light Detection and Ranging (LiDAR) sensors to achieve high-precision localization results for navigation purpose. However, this combination with their associated costs and infrastructure demands, poses challenges for widespread adoption in mass-market applications. In this paper, we aim to use only a monocular camera to achieve comparable onboard localization performance by tracking deep-learning visual features on a LiDAR-enhanced visual prior map. Experiments show that the proposed algorithm can provide centimeter-level global positioning results with scale, which is effortlessly integrated and favorable for low-cost robot system deployment in real-world applications.},
  archive   = {C_ICRA},
  author    = {Jinhao He and Huaiyang Huang and Shuyang Zhang and Jianhao Jiao and Chengju Liu and Ming Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611105},
  month     = {5},
  pages     = {11934-11940},
  title     = {Accurate prior-centric monocular positioning with offline LiDAR fusion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual-IMU state estimation for relative localization of two
mobile agents. <em>ICRA</em>, 11927–11933. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we address the problem of relative localization of two mobile agents. Specifically, we consider the Dual-IMU system, where each agent is equipped with one IMU, and employs relative pose observations between them. Previous works, however, typically assumed known ego motion and ignored biases of the IMUs. Instead, we study the most general case of unknown biases for both IMUs. Besides the derivation of dynamic model equations of the proposed system, we focus on the observability analysis, for the observability under general motion and the unobservable directions arising from various special motions. Through numerical simulations, we validate our key observability findings and examine their impact on the estimation accuracy and consistency. Finally, the system is implemented to achieve effective relative localization of an HMD with respect to a vehicle moving in the real world.},
  archive   = {C_ICRA},
  author    = {Wenqian Lai and Ruonan Guo and Kejian J. Wu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611182},
  month     = {5},
  pages     = {11927-11933},
  title     = {Dual-IMU state estimation for relative localization of two mobile agents},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The LuViRA dataset: Synchronized vision, radio, and audio
sensors for indoor localization. <em>ICRA</em>, 11920–11926. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a synchronized multisensory dataset for accurate and robust indoor localization: the Lund University Vision, Radio, and Audio (LuViRA) Dataset. The dataset includes color images, corresponding depth maps, inertial measurement unit (IMU) readings, channel response between a 5G massive multiple-input and multiple-output (MIMO) testbed and user equipment, audio recorded by 12 microphones, and accurate six degrees of freedom (6DOF) pose ground truth of 0.5 mm. We synchronize these sensors to ensure that all data is recorded simultaneously. A camera, speaker, and transmit antenna are placed on top of a slowly moving service robot, and 89 trajectories are recorded. Each trajectory includes 20 to 50 seconds of recorded sensor data and ground truth labels. Data from different sensors can be used separately or jointly to perform localization tasks, and data from the motion capture (mocap) system is used to verify the results obtained by the localization algorithms. The main aim of this dataset is to enable research on sensor fusion with the most commonly used sensors for localization tasks. Moreover, the full dataset or some parts of it can also be used for other research areas such as channel estimation, image classification, etc. Our dataset is available at: https://github.com/ilaydayaman/LuViRA_Dataset},
  archive   = {C_ICRA},
  author    = {Ilayda Yaman and Guoda Tian and Martin Larsson and Patrik Persson and Michiel Sandra and Alexander Dürr and Erik Tegler and Nikhil Challa and Henrik Garde and Fredrik Tufvesson and Kalle Åström and Ove Edfors and Steffen Malkowsky and Liang Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610237},
  month     = {5},
  pages     = {11920-11926},
  title     = {The LuViRA dataset: Synchronized vision, radio, and audio sensors for indoor localization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fully onboard low-power localization with semantic sensor
fusion on a nano-UAV using floor plans. <em>ICRA</em>, 11913–11919. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Nano-sized unmanned aerial vehicles (UAVs) are well-fit for indoor applications and for close proximity to humans. To enable autonomy, the nano-UAV must be able to self-localize in its operating environment. This is a particularly-challenging task due to the limited sensing and compute resources on board. This work presents an online and onboard approach for localization in floor plans annotated with semantic information. Unlike sensor-based maps, floor plans are readily-available, and do not increase the cost and time of deployment. To overcome the difficulty of localizing in sparse maps, the proposed approach fuses geometric information from miniaturized time-of-flight sensors and semantic cues. The semantic information is extracted from images by deploying a state-of-the-art object detection model on a high-performance multi-core microcontroller onboard the drone, consuming only 2.5mJ per frame and executing in 38ms. In our evaluation, we globally localize in a real-world office environment, achieving 90% success rate. We also release an open-source implementation of our work 1 .},
  archive   = {C_ICRA},
  author    = {Nicky Zimmerman and Hanna Müller and Michele Magno and Luca Benini},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610851},
  month     = {5},
  pages     = {11913-11919},
  title     = {Fully onboard low-power localization with semantic sensor fusion on a nano-UAV using floor plans},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GPS-VIO fusion with online rotational calibration.
<em>ICRA</em>, 11906–11912. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate global localization is crucial for autonomous navigation and planning. To this end, various GPS-aided Visual-Inertial Odometry (GPS-VIO) fusion algorithms are proposed in the literature. This paper presents a novel GPS-VIO system that is able to significantly benefit from the online calibration of the rotational extrinsic parameter between the GPS reference frame and the VIO reference frame. The behind reason is this parameter is observable. This paper provides novel proof through nonlinear observability analysis. We also evaluate the proposed algorithm extensively on diverse platforms, including flying UAV and driving vehicle. The experimental results support the observability analysis and show increased localization accuracy in comparison to state-of-the-art (SOTA) tightly-coupled algorithms.},
  archive   = {C_ICRA},
  author    = {Junlin Song and Pedro J. Sanchez-Cuevas and Antoine Richard and Raj Thilak Rajan and Miguel Olivares-Mendez},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611466},
  month     = {5},
  pages     = {11906-11912},
  title     = {GPS-VIO fusion with online rotational calibration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Field evaluation of a prioritized path-planning algorithm
for heterogeneous agricultural tasks of multi-UGVs. <em>ICRA</em>,
11891–11897. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a prioritized path-planning algorithm for heterogeneous tasks performed by multiple unmanned ground vehicles (UGVs) in agricultural environments. The algorithm considers varying robot priorities, thereby extending the traditional multi-agent path finding (MAPF) approach. The proposed algorithm is evaluated in scenarios occurring during representative agricultural operations: harvesting and transportation. An experimental validation is conducted in agriculture-like settings by using multiple simultaneous localization and mapping systems and navigation systems. The results revealed that the path of agent 1 , which was assigned the highest priority in both the indoor and outdoor environments, was shortened considerably (3.38 m, 3.6 m, and 5.6 m, respectively). Especially in the face scenario, the sum of changes in distance, calculated using the proposed algorithm was negative, meaning that traffic congestion in the multi-robot system used in the experiment was alleviated without the need for inter-robot communication.},
  archive   = {C_ICRA},
  author    = {Yuseung Jo and Hyoung Il Son},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610857},
  month     = {5},
  pages     = {11891-11897},
  title     = {Field evaluation of a prioritized path-planning algorithm for heterogeneous agricultural tasks of multi-UGVs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards robotic tree manipulation: Leveraging graph
representations. <em>ICRA</em>, 11884–11890. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There is growing interest in automating agricultural tasks that require intricate and precise interaction with specialty crops, such as trees and vines. However, developing robotic solutions for crop manipulation remains a difficult challenge due to complexities involved in modeling their deformable behavior. In this study, we present a framework for learning the deformation behavior of tree-like crops under contact interaction. Our proposed method involves encoding the state of a spring-damper modeled tree crop as a graph. This representation allows us to employ graph networks to learn both a forward model for predicting resulting deformations, and a contact policy for inferring actions to manipulate tree crops. We conduct a comprehensive set of experiments in a simulated environment and demonstrate generalizability of our method on previously unseen trees. Videos can be found on the project website: https://kantor-lab.github.io/tree_gnn},
  archive   = {C_ICRA},
  author    = {Chung Hee Kim and Moonyoung Lee and Oliver Kroemer and George Kantor},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611327},
  month     = {5},
  pages     = {11884-11890},
  title     = {Towards robotic tree manipulation: Leveraging graph representations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inexpensive, automated pruning weight estimation in
vineyards. <em>ICRA</em>, 11869–11875. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pruning weight is indicative of a vine’s ability to produce a crop the following year, informing vineyard management. Current methods for estimating pruning weight are costly, laborious, and/or require specialized know-how and equipment. In this paper we demonstrate an affordable, simple, computer vision-based method to measure pruning weight using a smartphone camera and structured light which produces results better than state-of-the-art techniques for vertical shoot position (VSP) vines and demonstrate initial steps towards estimating pruning weight in high cordon procumbent (HC) vines such as Concord. The simplicity and affordability of this technique lends its self to deployment by farmers today or on future viticulture robotics platforms. We achieved an R2=.80 for VSP vines (better than state-of-the-art computer vision-based methods) and R2=.29 for HC vines (not previously attempted with computer vision-based methods).},
  archive   = {C_ICRA},
  author    = {Jonathan Jaramillo and Aaron Wilhelm and Nils Napp and Justine Vanden Heuvel and Kirstin Petersen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610164},
  month     = {5},
  pages     = {11869-11875},
  title     = {Inexpensive, automated pruning weight estimation in vineyards},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Aerial image-based inter-day registration for precision
agriculture. <em>ICRA</em>, 11862–11868. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Satellite imagery has traditionally been used to collect crop statistics, but its low resolution and registration accuracy limit agricultural analytics to plant stand levels and large areas. Precision agriculture seeks analytic tools at near single plant level, and this work explores how to improve aerial photogrammetry to enable inter-day precision agriculture analytics for intervals of up to a month.Our work starts by presenting an accurately registered image time series, captured up to twice a week, by an unmanned aerial vehicle over a wheat crop field. The dataset is registered using photogrammetry aided by fiducial ground control points (GCPs). Unfortunately, GCPs severely disrupt crop management activities. To address this, we propose a novel inter-day registration approach that only relies once on GCPs, at the beginning of the season.The method utilises LoFTR [1], a state-of-the-art image-matching transformer. The original LoFTR network was trained using imagery of outdoor urban areas. One of our contributions is to extend LoFTR’s training method, which uses matching images of a static scene, to a dynamic scene of plants undergoing growth. Another contribution is a thorough evaluation of our registration method that integrates intraday crop reconstruction with earlier-day scans in a seven degree-of-freedom alignment. Experimental results show the advantage of our approach over other matching algorithms and demonstrate the importance of retraining using crop scenes, and a training method customised for growing crops, with an average registration error of 27 cm across a season.},
  archive   = {C_ICRA},
  author    = {Chen Gao and Franz Daxinger and Lukas Roth and Fabiola Maffra and Paul Beardsley and Margarita Chli and Lucas Teixeira},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611221},
  month     = {5},
  pages     = {11862-11868},
  title     = {Aerial image-based inter-day registration for precision agriculture},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Strawberry weight estimation based on plane-constrained
binary division point cloud completion. <em>ICRA</em>, 11846–11852. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Labor shortages and the development of digital technology both impose requirements on the fruit industry. Modern agricultural competition has shifted from competition between products to competition between supply chains. Enhancing the digitization of production lines is crucial for gaining a competitive advantage. Strawberries, as fruits with a short shelf life, require sorting and packaging of fruits of different weights after being harvested. Estimating strawberry weight through visual technology can save time and labor costs. Common methods include methods based on feature size and learning-based methods, with the former having larger errors and the latter requiring a large amount of data. To address these issues, we propose a dataset for estimating strawberry weight, which includes strawberries with different heights and angles. Additionally, we propose a strawberry weight estimation method based on plane-constrained binary division point cloud completion. This method separates the plane point cloud and strawberry point cloud, constructs a coordinate system on the strawberry point cloud, generates an axis-aligned bounding box (AABB), and estimates the strawberry weight based on the bounding box and placement plane as constraints. Through comparison with different methods, we achieved a maximum improvement of 20.95% in prediction accuracy, demonstrating that our method provides the best estimation accuracy.},
  archive   = {C_ICRA},
  author    = {Yanjiang Huang and Jiepeng Liu and Xianmin Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610482},
  month     = {5},
  pages     = {11846-11852},
  title     = {Strawberry weight estimation based on plane-constrained binary division point cloud completion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic evaluation of a suction based gripper for fruit
picking using a physical twin. <em>ICRA</em>, 11839–11845. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present and evaluate a novel suction-based gripper designed for fruit picking. This work is motivated by common problems observed in field trials of robotic harvesting: Calibration/perception errors, workspace obstacles, fruit swinging/moving when contacted, and varying stem and branch stiffnesses. The gripper consists of three suction-cups located on the palm, along with in-hand perception. To evaluate the gripper, we developed a physical proxy that approximates a realistic apple-stem-branch dynamic system. We performed 756 apple picks on the proxy with varying branch stiffness, stem strength and gripper pose (yaw, roll and offset w.r.t. the apple). Our results show that grasping performance improves when the gripper yaw w.r.t. the apple has two suction cups on the bottom of the apple and one suction cup on top. Even with ±15mm offset, at least two suction cups engaged with the apple 80% of the time, regardless of branch stiffness. Moreover, the gripper withstands ±20mm offset when it approaches the apple near its equator.},
  archive   = {C_ICRA},
  author    = {Alejandro Velasquez and Cindy Grimm and Joseph R. Davidson},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611299},
  month     = {5},
  pages     = {11839-11845},
  title     = {Dynamic evaluation of a suction based gripper for fruit picking using a physical twin},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heuristic-based incremental probabilistic roadmap for
efficient UAV exploration in dynamic environments. <em>ICRA</em>,
11832–11838. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous exploration in dynamic environments necessitates a planner that can proactively respond to changes and make efficient and safe decisions for robots. Although plenty of sampling-based works have shown success in exploring static environments, their inherent sampling randomness and limited utilization of previous samples often result in sub-optimal exploration efficiency. Additionally, most of these methods struggle with efficient replanning and collision avoidance in dynamic settings. To overcome these limitations, we propose the Heuristic-based Incremental Probabilistic Roadmap Exploration (HIRE) planner for UAVs exploring dynamic environments. The proposed planner adopts an incremental sampling strategy based on the probabilistic roadmap constructed by heuristic sampling toward the unexplored region next to the free space, defined as the heuristic frontier regions. The heuristic frontier regions are detected by applying a lightweight vision-based method to the different levels of the occupancy map. Moreover, our dynamic module ensures that the planner dynamically updates roadmap information based on the environment changes and avoids dynamic obstacles. Simulation and physical experiments prove that our planner can efficiently and safely explore dynamic environments. Our software 1 is available on GitHub with the experiment video 2 .},
  archive   = {C_ICRA},
  author    = {Zhefan Xu and Christopher Suzuki and Xiaoyang Zhan and Kenji Shimada},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610462},
  month     = {5},
  pages     = {11832-11838},
  title     = {Heuristic-based incremental probabilistic roadmap for efficient UAV exploration in dynamic environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bio-inspired visual relative localization for large swarms
of UAVs. <em>ICRA</em>, 11825–11831. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a new approach to visual perception for relative localization of agents within large-scale swarms of Unmanned Aerial Vehicles (UAVs). Inspired by biological perception utilized by schools of sardines, swarms of bees, and other large groups of animals capable of moving in a decentralized yet coherent manner, our method does not rely on detecting individual neighbors by each agent and estimating their relative position, but rather we propose to regress a neighbor density over distance. This allows for a more accurate distance estimation as well as better scalability with respect to the number of neighbors. Additionally, a novel swarm control algorithm is proposed to make it compatible with the new relative localization method. We provide a thorough evaluation of the presented methods and demonstrate that the regressing approach to distance estimation is more robust to varying relative pose of the targets and that it is suitable to be used as the main source of relative localization for swarm stabilization.},
  archive   = {C_ICRA},
  author    = {Martin Křížek and Matouš Vrba and Antonella Barišić Kulaš and Stjepan Bogdan and Martin Saska},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610100},
  month     = {5},
  pages     = {11825-11831},
  title     = {Bio-inspired visual relative localization for large swarms of UAVs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AirFisheye dataset: A multi-model fisheye dataset for UAV
applications. <em>ICRA</em>, 11818–11824. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Drone applications require perception all around the vehicle to avoid obstacles during navigation. Due to the weight and computation limitations on UAVs, using a large number of sensors, such as numerous cameras, could be prohibitive. In such scenarios, usage of fisheye cameras with a wider field of view is very beneficial. Despite the usefulness of fisheye camera for UAV applications, not much work has been carried out to develop perception algorithms for fisheye camera. One of the main problems being the lack of publicly available omnidirectional datasets in relation to drone flight. With this paper, we address this gap by presenting AirFisheye dataset, which is applicable for tasks such as segmentation, depth estimation and depth completion, among other tasks required for autonomous drone navigation. Also, a generic framework for creating synthetic fisheye images is provided. Furthermore, we propose a novel occlusion correction algorithm that removes incorrectly projected LiDAR point clouds into the camera image due to the viewpoint variation of both sensors. We release about 26K images and LiDAR scans along with annotations. Baseline code and supporting scripts are available at https://collaborating.tuhh.de/ilt/airfisheye-dataset},
  archive   = {C_ICRA},
  author    = {Pravin Kumar Jaisawal and Stephanos Papakonstantinou and Volker Gollnick},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611092},
  month     = {5},
  pages     = {11818-11824},
  title     = {AirFisheye dataset: A multi-model fisheye dataset for UAV applications},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tethered lifting-wing multicopter landing like kite.
<em>ICRA</em>, 11811–11817. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automatic landing of tethered unmanned aerial vehicles (UAVs) is an important issue. Typically, UAVs rely on location sensors such as global navigation satellite system (GNSS) and external cameras to obtain location data. However, harsh environments such as denial GNSS or strong winds make it difficult for UAVs to approach the landing area, and common solutions cannot be used for automatic landing. A tethered lifting-wing multicopter has a structure and static stability similar to a kite. Inspired by kites, this paper proposes a new landing method for tethered lifting-wing multicopters, which can be used without location or velocity sensors. During the landing phase, the tethered lifting-wing multicopter only needs to keep the rotor thrust to actively straighten the tethered cable and a constant attitude similar to that of a kite to keep position stability and increase damping. Meanwhile, the winch only needs to recover the cable at a constant speed until the tethered lifting-wing multicopter returns to its base. Real flight experiments demonstrate the feasibility and practicability of this method.},
  archive   = {C_ICRA},
  author    = {Haoyu Wei and Shuai Wang and Quan Quan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611536},
  month     = {5},
  pages     = {11811-11817},
  title     = {Tethered lifting-wing multicopter landing like kite},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Extending guiding vector field to track unbounded UAV paths.
<em>ICRA</em>, 11804–11810. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A recent advance in vector field path following is the introduction of the Parametric Guiding Vector Field method. It allows for singularity-free vector fields with strong convergence guarantees, usable even for self-intersecting paths. However, the method requires significant gain tuning for practical use. In particular, for unbounded paths, the gains will inevitably become ill-suited for efficient path following. We propose a method to overcome this issue by introducing a dynamic step adaptation strategy, which provides additional normalization properties to the field. This allows the following of unbounded curves and reduces the number of gains to tune. The proposed improvements are verified in simulations using the PaparazziUAV software.},
  archive   = {C_ICRA},
  author    = {Mael Feurgard and Gautier Hattenberger and Simon Lacroix},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610787},
  month     = {5},
  pages     = {11804-11810},
  title     = {Extending guiding vector field to track unbounded UAV paths},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trajectory optimization for cooperatively localizing
quadrotor UAVs. <em>ICRA</em>, 11796–11803. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, an Active Cooperative Localization system for Quadrotor Unmanned Aerial Vehicles is developed. The optimal trajectories are determined by minimizing the uncertainty in position estimation by Extended Kalman Filter. In this system, a piecewise polynomial parameterization of trajectories is adopted for the optimizer, and the underlying state estimator is updated with appropriate models of sensors and quadrotor dynamics. This system is verified in extensive simulations in the scenario of a team of quadrotors with heterogeneous GNSS capabilities. These simulations answer an open question, showing that solving for trajectories by minimizing Kalman covariance computed in a noiseless environment is reasonable and that the optimized trajectories offer visible reductions in positioning uncertainty in the presence of noise.},
  archive   = {C_ICRA},
  author    = {H S Helson Go and Hugh H.-T. Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611620},
  month     = {5},
  pages     = {11796-11803},
  title     = {Trajectory optimization for cooperatively localizing quadrotor UAVs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time dynamic-consistent motion planning for
over-actuated UAVs. <em>ICRA</em>, 11789–11795. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing motion planning approaches for over-actuated unmanned aerial vehicle (UAV) platforms can achieve online planning without considering dynamics. However, in many envisioned application areas such as aerial manipulation, payload delivery, and moving target tracking, it is critical to ensure dynamic consistency in the generated trajectory. The dynamics of these platforms introduce a high nonlinearity, leading to a substantial increase in computational burden. This paper presents an efficient method to plan motions that are consistent with the dynamics of over-actuated UAVs. With a hierarchical control structure, the dimension of the optimization problem is greatly reduced with synthesized wrench commands. Additionally, by exploring the dynamics of over-actuated UAVs, the complex planning process is decoupled into two simpler sub-problems. As a result, the proposed planner can be solved as two small quadratic programmings (QPs) and deployed in real-time. The computational efficiency and dynamic consistency of the proposed method are verified through both simulations and experiments, including comparison with other approaches and dynamic target tracking.},
  archive   = {C_ICRA},
  author    = {Yao Su and Jingwen Zhang and Ziyuan Jiao and Hang Li and Meng Wang and Hangxin Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610236},
  month     = {5},
  pages     = {11789-11795},
  title     = {Real-time dynamic-consistent motion planning for over-actuated UAVs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A compiler framework for proactive UAV regulation
enforcement. <em>ICRA</em>, 11781–11788. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the rapidly evolving landscape of Unmanned Aerial Vehicles (UAVs), regulation enforcement is critical. Unfortunately, existing practices are largely manual and reactive in nature. We present Themis 1 , a novel compiler-directed approach for automated and proactive regulation enforcement. By expressing regulations through a specification language and integrating their enforcement into the compilation process, Themis enables safe and regulation-compliant UAV flights by enforcing prohibited and restricted areas, avoiding flights over humans, and managing maximum limits of altitude and speed. Our framework features a bidirectional interface that allows the concrete algorithms used for enforcement to be customized. Our evaluation shows Themis-compiled autopilots can adhere to regulatory constraints amidst complex flight conditions, while significantly reducing the burden of UAV operators.},
  archive   = {C_ICRA},
  author    = {Huaxin Tang and John Henry Burns and Alexander Strong and Yu David Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610359},
  month     = {5},
  pages     = {11781-11788},
  title     = {A compiler framework for proactive UAV regulation enforcement},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VINSat: Solving the lost-in-space problem with
visual-inertial navigation. <em>ICRA</em>, 11774–11780. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Rapid growth in the number of nanosatellite deployments has heightened the need for rapid, cost-effective, and accurate orbit determination (OD). This paper introduces a solution to this &quot;lost-in-space&quot; problem that we call Visual-Inertial Navigation for Satellites (VINSat). VINSat performs OD using data from an inertial measurement unit (IMU) and a low-cost RGB camera. Machine learning techniques are used to identify known landmarks in images captured by the spacecraft. These landmark locations are then combined with IMU data and a dynamics model in a batch nonlinear least-squares state estimator to determine the full state of the spacecraft. We validate VINSat in simulation using real nadir-pointing imagery and find that 85% of simulated satellites are localized to under 5 km within 6 hours (4 orbits). This performance substantially surpasses that of ground radar, demonstrating significantly faster and more precise localization without any reliance on ground infrastructure.},
  archive   = {C_ICRA},
  author    = {Kyle McCleary and Swaminathan Gurumurthy and Paulo R.M. Fisch and Saral Tayal and Zachary Manchester and Brandon Lucia},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610864},
  month     = {5},
  pages     = {11774-11780},
  title     = {VINSat: Solving the lost-in-space problem with visual-inertial navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Covariance based terrain mapping for autonomous mobile
robots. <em>ICRA</em>, 11768–11773. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a local, robot-centric navigation map optimized for autonomous mobile robots operating in unknown environments, enhancing their onboard perception systems for collision-free operation with far look-ahead distances. Utilizing a novel converging covariance cell representation, our approach effectively analyzes hazards such as obstacles and hazardous slopes in both terrestrial and aerial navigation contexts. The new technique specifically targets mapping from stereo scenarios with ultra short baseline and highly oblique viewpoints close to the ground.Our methodology surpasses traditional window-based hazard analysis by resolving sub-cell size obstacles and terrain gradients at the individual cell level, thereby avoiding the computational overhead typically associated with such analyses. It leverages a multi-resolution strategy adaptive to the range errors common in stereo vision systems, making it particularly suitable for embedded systems with computational limitations.Functionality includes constant-time queries for height, obstacle presence, and slope details, boasting improvements in run time, memory usage, precision, and resolvable obstacle size compared to existing grid-based mapping algorithms. We validate our approach through rigorous simulation and real-world testing. This technique will be used for the local mapping and collision avoidance on NASA’s CADRE lunar rovers.},
  archive   = {C_ICRA},
  author    = {Lennart Werner and Pedro Proença and Andreas Nüchter and Roland Brockers},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610010},
  month     = {5},
  pages     = {11768-11773},
  title     = {Covariance based terrain mapping for autonomous mobile robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SPADES: A realistic spacecraft pose estimation dataset using
event sensing. <em>ICRA</em>, 11760–11766. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, there has been a growing demand for improved autonomy for in-orbit operations such as rendezvous, docking, and proximity manoeuvres, leading to increased interest in employing Deep Learning-based Spacecraft Pose Estimation techniques. However, due to limited access to real target datasets, algorithms are often trained using synthetic data and applied in the real domain, resulting in a performance drop due to the domain gap. State-of-the-art approaches employ Domain Adaptation techniques to mitigate this issue. In the search for viable solutions, event sensing has been explored in the past and shown to reduce the domain gap between simulations and real-world scenarios. Event sensors have made significant advancements in hardware and software in recent years. Moreover, the characteristics of the event sensor offer several advantages in space applications compared to RGB sensors. To facilitate further training and evaluation of DL-based models, we introduce a new dataset, SPADES, comprising real event data acquired in a controlled laboratory environment and simulated event data using the same camera intrinsics. Furthermore, we introduce an image-based event representation that performs better than existing representations. In addition, we propose an effective data filtering method to improve the quality of training data, thus enhancing model performance. A multifaceted baseline evaluation was conducted using different event representations, event filtering strategies, and algorithmic frameworks, and the results are summarized. The dataset will be made available at http://cvi2.uni.lu/spades.},
  archive   = {C_ICRA},
  author    = {Arunkumar Rathinam and Haytam Qadadri and Djamila Aouada},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611231},
  month     = {5},
  pages     = {11760-11766},
  title     = {SPADES: A realistic spacecraft pose estimation dataset using event sensing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards real-world efficiency: Domain randomization in
reinforcement learning for pre-capture of free-floating moving targets
by autonomous robots. <em>ICRA</em>, 11753–11759. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this research, we introduce a deep reinforcement learning-based control approach to address the intricate challenge of the robotic pre-grasping phase under microgravity conditions. Leveraging reinforcement learning eliminates the necessity for manual feature design, therefore simplifying the problem and empowering the robot to learn pre-grasping policies through trial and error. Our methodology incorporates an off-policy reinforcement learning framework, employing the soft actor-critic technique to enable the gripper to proficiently approach a free-floating moving object, ensuring optimal pre-grasp success. For effective learning of the pre-grasping approach task, we developed a reward function that offers the agent clear and insightful feedback. Our case study examines a pre-grasping task where a Robotiq 3F gripper is required to navigate towards a free-floating moving target, pursue it, and subsequently position itself at the desired pre-grasp location. We assessed our approach through a series of experiments in both simulated and real-world environments. The source code, along with recordings of real-world robot grasping, is available at Fanuc_Robotiq_Grasp.},
  archive   = {C_ICRA},
  author    = {Bahador Beigomi and Zheng H. Zhu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610017},
  month     = {5},
  pages     = {11753-11759},
  title     = {Towards real-world efficiency: Domain randomization in reinforcement learning for pre-capture of free-floating moving targets by autonomous robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online supervised training of spaceborne vision during
proximity operations using adaptive kalman filtering. <em>ICRA</em>,
11744–11752. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents an Online Supervised Training (OST) method to enable robust vision-based navigation about a non-cooperative spacecraft. Spaceborne Neural Networks (NN) are susceptible to domain gap as they are primarily trained with synthetic images due to the inaccessibility of space. OST aims to close this gap by training a pose estimation NN online using incoming flight images during Rendezvous and Proximity Operations (RPO). The pseudo-labels are provided by an adaptive unscented Kalman filter where the NN is used in the loop as a measurement module. Specifically, the filter tracks the target’s relative orbital and attitude motion, and its accuracy is ensured by robust on-ground training of the NN using only synthetic data. The experiments on real hardware-in-the-loop trajectory images show that OST can improve the NN performance on the target image domain given that OST is performed on images of the target viewed from a diverse set of directions during RPO.},
  archive   = {C_ICRA},
  author    = {Tae Ha Park and Simone D’Amico},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610138},
  month     = {5},
  pages     = {11744-11752},
  title     = {Online supervised training of spaceborne vision during proximity operations using adaptive kalman filtering},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning-aided control of robotic tether-net with
maneuverable nodes to capture large space debris. <em>ICRA</em>,
11737–11743. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Maneuverable tether-net systems launched from an unmanned spacecraft offer a promising solution for the active removal of large space debris. Guaranteeing the successful capture of such space debris is dependent on the ability to reliably maneuver the tether-net system – a flexible, many-DoF (thus complex) system – for a wide range of launch scenarios. Here, scenarios are defined by the relative location of the debris with respect to the chaser spacecraft. This paper represents and solves this problem as a hierarchically decentralized implementation of robotic trajectory planning and control and demonstrates the effectiveness of the approach when applied to two different tether-net systems, with 4 and 8 maneuverable units (MUs), respectively. Reinforcement learning (policy gradient) is used to design the centralized trajectory planner that, based on the relative location of the target debris at the launch of the net, computes the final aiming positions of each MU, from which their trajectory can be derived. Each MU then seeks to follow its assigned trajectory by using a decentralized PID controller that outputs the MU’s thrust vector and is informed by noisy sensor feedback (for realism) of its relative location. System performance is assessed in terms of capture success and overall fuel consumption by the MUs. Reward shaping and surrogate models are used to respectively guide and speed up the RL process. Simulation-based experiments show that this approach allows the successful capture of debris at fuel costs that are notably lower than nominal baselines, including in scenarios where the debris is significantly off-centered compared to the approaching chaser spacecraft.},
  archive   = {C_ICRA},
  author    = {Achira Boonrath and Feng Liu and Eleonora M. Botta and Souma Chowdhury},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610721},
  month     = {5},
  pages     = {11737-11743},
  title     = {Learning-aided control of robotic tether-net with maneuverable nodes to capture large space debris},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PPO-based dynamic control of uncertain floating platforms in
zero-g environment. <em>ICRA</em>, 11730–11736. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the field of space exploration, floating platforms play a crucial role in scientific investigations and technological advancements. However, controlling these platforms in zerogravity environments presents unique challenges, including uncertainties and disturbances. This paper introduces an innovative approach that combines Proximal Policy Optimization (PPO) with Model Predictive Control (MPC) in the zero-gravity laboratory (Zero-G Lab) at the University of Luxembourg. This approach leverages PPO’s reinforcement learning power and MPC’s precision to navigate the complex control dynamics of floating platforms. Unlike traditional control methods, this PPO-MPC approach learns from MPC predictions, adapting to unmodeled dynamics and disturbances, resulting in a resilient control framework tailored to the zerogravity environment. Simulations and experiments in the Zero-G Lab validate this approach, showcasing the adaptability of the PPO agent. This research opens new possibilities for controlling floating platforms in zero-gravity settings, promising advancements in space exploration.},
  archive   = {C_ICRA},
  author    = {Mahya Ramezani and M. Amin Alandihallaj and Andreas M. Hein},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610101},
  month     = {5},
  pages     = {11730-11736},
  title     = {PPO-based dynamic control of uncertain floating platforms in zero-G environment},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analysis and validation of stiffness and payload of
nematode-inspired cable routing method for cable driven redundant
manipulator. <em>ICRA</em>, 11713–11719. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The cable-driven redundant manipulator (CDRM) has significant potential for applications in narrow and hazardous spaces. However, traditional CDRMs have limited stiffness and load capacity due to their cable routing method. To address these limitations, several scholars have proposed new mechanisms and control strategies. Nevertheless, the cable routing method has not changed, and CDRMs continue to suffer from their limitations. Recently, a nematode-inspired cable routing method was proposed; however, stiffness calculations, derivation of inverse kinematics, and validation of stiffness and load capacity were incomplete. In this paper, we calculate the analytic equivalent stiffness of the nematode-inspired cable routing method and compare it with other cable routing methods. Additionally, we derived and simulate the kinematics and an effective inverse kinematics algorithm. Finally, we validate the stiffness and load capacity using a developed prototype.},
  archive   = {C_ICRA},
  author    = {Hoyoung Kim and Jungwon Yoon},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611568},
  month     = {5},
  pages     = {11713-11719},
  title     = {Analysis and validation of stiffness and payload of nematode-inspired cable routing method for cable driven redundant manipulator},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Environment-modulated self-assembly by changes in modules’
buoyancy. <em>ICRA</em>, 11706–11712. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While many inkjet printers employ only four types of ink (i.e. CKMY) to produce a wide range of colors, numerous technical challenges still exist for contemporary 3D printers to fabricate various materials and generate composite products such as electric devices. Conversely, there have been attempts and endeavors to make things through self-assembly of parts, analogous to the autonomous and decentralized development process of the human body from just 20 types of amino acids. In our previous work, we proposed a method for the rapid production of 3D objects using the centimeter-sized modules (referred to as Roblets) capable of generating a 2D structure and subsequently self-folding themselves into a 3D configuration, akin to origami. To further leverage the capability of generating a wide variety of different types of structures by combining different modules, this research studies a method of automatically selecting and supplying modules using environmental cues. More precisely, we developed a mechanism to couple different modules corresponding to three different environments (on a flat surface, on low-dense saline, and on saturated saline) and yielded different module configurations. The process of self-assembly necessitated the application of perturbation, which was realized by imparting magnetic torque originating from an external magnetic field onto the magnets embedded in the modules.},
  archive   = {C_ICRA},
  author    = {Xiao Chen and Junyi Han and Xin Jin and Shuhei Miyashita},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610166},
  month     = {5},
  pages     = {11706-11712},
  title     = {Environment-modulated self-assembly by changes in modules’ buoyancy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Anisotropic body compliance facilitates robotic sidewinding
in complex environments. <em>ICRA</em>, 11691–11697. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sidewinding, a locomotion strategy characterized by the coordination of lateral and vertical body undulations, is frequently observed in rattlesnakes and has been successfully implemented by limbless robotic systems for effective movement across diverse terrestrial terrains. However, the integration of compliant mechanisms into sidewinding limbless robots remains less explored, posing challenges for navigation in complex, rheologically diverse environments. Inspired by a notable control simplification via mechanical intelligence in lateral undulation [1], which offloads feedback control to passive body mechanics and interactions with the environment, we present an innovative design of a mechanically intelligent limbless robot for sidewinding. This robot features a decentralized bilateral cable actuation system that resembles organismal muscle actuation mechanisms. We develop a feedforward controller that incorporates programmable body compliance into the sidewinding gait template. Our experimental results highlight the emergence of mechanical intelligence when the robot is equipped with an appropriate level of body compliance. This allows the robot to 1) locomote more energetically efficiently, as evidenced by a reduced cost of transport, and 2) navigate through terrain heterogeneities, all achieved in an open-loop manner, without the need for environmental awareness.},
  archive   = {C_ICRA},
  author    = {Velin Kojouharov and Tianyu Wang and Matthew Fernandez and Jiyeon Maeng and Daniel I. Goldman},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611625},
  month     = {5},
  pages     = {11691-11697},
  title     = {Anisotropic body compliance facilitates robotic sidewinding in complex environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A learning-based framework for safe human-robot
collaboration with multiple backup control barrier functions.
<em>ICRA</em>, 11676–11682. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ensuring robot safety in complex environments is a difficult task due to actuation limits, such as torque bounds. This paper presents a safety-critical control framework that leverages learning-based switching between multiple backup controllers to formally guarantee safety under bounded control inputs while satisfying driver intention. By leveraging backup controllers designed to uphold safety and input constraints, backup control barrier functions (BCBFs) construct implicitly defined control invariant sets via a feasible quadratic program (QP). However, BCBF performance largely depends on the design and conservativeness of the chosen backup controller, especially in our setting of human-driven vehicles in complex, e.g, off-road, conditions. While conservativeness can be reduced by using multiple backup controllers, determining when to switch is an open problem. Consequently, we develop a broadcast scheme that estimates driver intention and integrates BCBFs with multiple backup strategies for human-robot interaction. An LSTM classifier uses data inputs from the robot, human, and safety algorithms to continually choose a backup controller in real-time. We demonstrate our method’s efficacy on a dualtrack robot in obstacle avoidance scenarios. Our framework guarantees robot safety while adhering to driver intention.},
  archive   = {C_ICRA},
  author    = {Neil C. Janwani and Ersin Daş and Thomas Touma and Skylar X. Wei and Tamas G. Molnar and Joel W. Burdick},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611634},
  month     = {5},
  pages     = {11676-11682},
  title     = {A learning-based framework for safe human-robot collaboration with multiple backup control barrier functions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conformal decision theory: Safe autonomous decisions from
imperfect predictions. <em>ICRA</em>, 11668–11675. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce Conformal Decision Theory, a framework for producing safe autonomous decisions despite imperfect machine learning predictions. Examples of such decisions are ubiquitous, from robot planning algorithms that rely on pedestrian predictions, to calibrating autonomous manufacturing to exhibit high throughput and low error, to the choice of trusting a nominal policy versus switching to a safe backup policy at run-time. The decisions produced by our algorithms are safe in the sense that they come with provable statistical guarantees of having low risk without any assumptions on the world model whatsoever; the observations need not be I.I.D. and can even be adversarial. The theory extends results from conformal prediction to calibrate decisions directly, without requiring the construction of prediction sets. Experiments demonstrate the utility of our approach in robot motion planning around humans and robot manufacturing.},
  archive   = {C_ICRA},
  author    = {Jordan Lekeufack and Anastasios N. Angelopoulos and Andrea Bajcsy and Michael I. Jordan and Jitendra Malik},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610041},
  month     = {5},
  pages     = {11668-11675},
  title     = {Conformal decision theory: Safe autonomous decisions from imperfect predictions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Risk-aware control for robots with non-gaussian belief
spaces. <em>ICRA</em>, 11661–11667. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the problem of safety-critical control of autonomous robots, considering the ubiquitous uncertainties arising from un-modeled dynamics and noisy sensors. To take into account these uncertainties, probabilistic state estimators are often deployed to obtain a belief over possible states. Namely, Particle Filters (PFs) can handle arbitrary non-Gaussian distributions in the robot’s state. In this work, we define the belief state and belief dynamics for continuous-discrete PFs and construct safe sets in the underlying belief space. We design a controller that provably keeps the robot’s belief state within this safe set. As a result, we ensure that the risk of the unknown robot’s state violating a safety specification, such as avoiding a dangerous area, is bounded. We provide an open-source implementation as a ROS2 package and evaluate the solution in simulations and hardware experiments involving high-dimensional belief spaces.},
  archive   = {C_ICRA},
  author    = {Matti Vahs and Jana Tumova},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611412},
  month     = {5},
  pages     = {11661-11667},
  title     = {Risk-aware control for robots with non-gaussian belief spaces},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Closing the perception-action loop for semantically safe
navigation in semi-static environments. <em>ICRA</em>, 11641–11648. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous robots navigating in changing environments demand adaptive navigation strategies for safe long-term operation. While many modern control paradigms offer theoretical guarantees, they often assume known extrinsic safety constraints, overlooking challenges when deployed in real-world environments where objects can appear, disappear, and shift over time. In this paper, we present a closed-loop perception-action pipeline that bridges this gap. Our system encodes an online-constructed dense map, along with object-level semantic and consistency estimates into a control barrier function (CBF) to regulate safe regions in the scene. A model predictive controller (MPC) leverages the CBF-based safety constraints to adapt its navigation behaviour, which is particularly crucial when potential scene changes occur. We test the system in simulations and real-world experiments to demonstrate the impact of semantic information and scene change handling on robot behavior, validating the practicality of our approach.},
  archive   = {C_ICRA},
  author    = {Jingxing Qian and Siqi Zhou and Nicholas Jianrui Ren and Veronica Chatrath and Angela P. Schoellig},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610267},
  month     = {5},
  pages     = {11641-11648},
  title     = {Closing the perception-action loop for semantically safe navigation in semi-static environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Receding-constraint model predictive control using a learned
approximate control-invariant set. <em>ICRA</em>, 11626–11632. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, advanced model-based and data-driven control methods are unlocking the potential of complex robotics systems, and we can expect this trend to continue at an exponential rate in the near future. However, ensuring safety with these advanced control methods remains a challenge. A well-known tool to make controllers (either Model Predictive Controllers or Reinforcement Learning policies) safe, is the so-called control-invariant set (a.k.a. safe set). Unfortunately, for nonlinear systems, such a set cannot be exactly computed in general. Numerical algorithms exist for computing approximate control-invariant sets, but classic theoretic control methods break down if the set is not exact. This paper presents our recent efforts to address this issue. We present a novel Model Predictive Control scheme that can guarantee recursive feasibility and/or safety under weaker assumptions than classic methods. In particular, recursive feasibility is guaranteed by making the safe-set constraint move backward over the horizon, and assuming that such set satisfies a condition that is weaker than control invariance. Safety is instead guaranteed under an even weaker assumption on the safe set, triggering a safe task-abortion strategy whenever a risk of constraint violation is detected. We evaluated our approach on a simulated robot manipulator, empirically demonstrating that it leads to less constraint violations than state-of-the-art approaches, while retaining reasonable performance in terms of tracking cost, number of completed tasks, and computation time.},
  archive   = {C_ICRA},
  author    = {Gianni Lunardi and Asia La Rocca and Matteo Saveriano and Andrea Del Prete},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611467},
  month     = {5},
  pages     = {11626-11632},
  title     = {Receding-constraint model predictive control using a learned approximate control-invariant set},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Safe multi-robot exploration using symbolic control.
<em>ICRA</em>, 11619–11625. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-robot exploration is a complex problem that involves multiple robots working in a shared unknown environment. In such scenarios, the safety of the robots is of paramount importance alongside the completion of the exploration task. In this paper, we propose a modular exploration framework that (i) identifies safe frontier targets for multiple robots while taking into account the system dynamics of each robot to ensure collision avoidance with previously unknown obstacles and (ii) ensures that the robots reach their exploration targets while avoiding any obstacles discovered and each other. We employ a scalable approach to generate symbolic controllers for the multi-robot system, utilizing distance functions. We also provide formal guarantees on the safety of the exploration targets and the completion of each exploration run, with the robots avoiding collisions with each other and the obstacles. We test our approach on simulation experiments and a real-world implementation to validate it.},
  archive   = {C_ICRA},
  author    = {Manas Sashank Juvvi and David Smith Sundarsingh and Ratnangshu Das and Pushpak Jagtap},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610520},
  month     = {5},
  pages     = {11619-11625},
  title     = {Safe multi-robot exploration using symbolic control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning-based inverse perception contracts and
applications. <em>ICRA</em>, 11612–11618. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Perception modules are integral in many modern autonomous systems, but their accuracy can be subject to the vagaries of the environment. In this paper, we propose a learning-based approach that can automatically characterize the error of a perception module from data and use this for safe control. The proposed approach constructs an inverse perception contract (IPC) which generates a set that contains the ground-truth value that is being estimated by the perception module, with high probability. We apply the proposed approach to study a vision pipeline deployed on a quadcopter. With the proposed approach, we successfully constructed an IPC for the vision pipeline. We then designed a control algorithm that utilizes the learned IPC, with the goal of landing the quadcopter safely on a landing pad. Experiments show that with the learned IPC, the control algorithm safely landed the quadcopter despite the error from the perception module, while the baseline algorithm without using the learned IPC failed to do so.},
  archive   = {C_ICRA},
  author    = {Dawei Sun and Benjamin C. Yang and Sayan Mitra},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610329},
  month     = {5},
  pages     = {11612-11618},
  title     = {Learning-based inverse perception contracts and applications},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Excitation trajectory optimization for dynamic parameter
identification using virtual constraints in hands-on robotic system.
<em>ICRA</em>, 11605–11611. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a novel, more computationally efficient method for optimizing robot excitation trajectories for dynamic parameter identification, emphasizing self-collision avoidance. This addresses the system identification challenges for getting high-quality training data associated with co-manipulated robotic arms that can be equipped with a variety of tools, a common scenario in industrial but also clinical and research contexts. Utilizing the Unified Robotics Description Format (URDF) to implement a symbolic Python implementation of the Recursive Newton-Euler Algorithm (RNEA), the approach aids in dynamically estimating parameters such as inertia using regression analyses on data from real robots. The excitation trajectory was evaluated and achieved on par criteria when compared to state-of-the-art reported results which didn’t consider self-collision and tool calibrations. Furthermore, physical Human-Robot Interaction (pHRI) admittance control experiments were conducted in a surgical context to evaluate the derived inverse dynamics model showing a 30.1% workload reduction by the NASA TLX questionnaire.},
  archive   = {C_ICRA},
  author    = {Huanyu Tian and Martin Huber and Christopher E. Mower and Zhe Han and Changsheng Li and Xingguang Duan and Christos Bergeles},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610950},
  month     = {5},
  pages     = {11605-11611},
  title     = {Excitation trajectory optimization for dynamic parameter identification using virtual constraints in hands-on robotic system},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bevel-tip needle deflection modeling, simulation, and
validation in multi-layer tissues. <em>ICRA</em>, 11598–11604. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Percutaneous needle insertions are commonly performed for diagnostic and therapeutic purposes as an effective alternative to more invasive surgical procedures. However, the outcome of needle-based approaches relies heavily on the accuracy of needle placement, which remains a challenge even with robot assistance and medical imaging guidance due to needle deflection caused by contact with soft tissues. In this paper, we present a novel mechanics-based 2D bevel-tip needle model that can account for the effect of nonlinear strain-dependent behavior of biological soft tissues under compression. Real-time finite element simulation allows multiple control inputs along the length of the needle with full three-degree-of-freedom (DOF) planar needle motions. Cross-validation studies using custom-designed multi-layer tissue phantoms as well as heterogeneous chicken breast tissues result in less than 1mm in-plane errors for insertions reaching depths of up to 61 mm, demonstrating the validity and generalizability of the proposed method.},
  archive   = {C_ICRA},
  author    = {Yanzhou Wang and Lidia Al-Zogbi and Guanyun Liu and Jiawei Liu and Junichi Tokuda and Axel Krieger and Iulian Iordachita},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610110},
  month     = {5},
  pages     = {11598-11604},
  title     = {Bevel-tip needle deflection modeling, simulation, and validation in multi-layer tissues},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A generic modeling framework for the design of tendon-driven
continuum manipulators with flexure patterns. <em>ICRA</em>,
11592–11597. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, a novel mathematical framework is introduced for modeling deformation behavior of Tendon-Driven Continuum Manipulators (TD-CMs) featuring discontinuous cross-sectional geometries (i.e., having flexural patterns). Leveraging this framework, we also introduce the concept of design space by which the deformation-behavior space of a TD-CM can intuitively be analyzed via its geometrical design parameters. To thoroughly evaluate the performance of the proposed modeling framework, we have conducted various simulation studies and experiments.},
  archive   = {C_ICRA},
  author    = {Yang Liu and Hansoul Kim and Yash Kulkarni and Farshid Alambeigi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611695},
  month     = {5},
  pages     = {11592-11597},
  title     = {A generic modeling framework for the design of tendon-driven continuum manipulators with flexure patterns},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A soft micro-robotic catheter for aneurysm treatment: A
novel design and enhanced euler-bernoulli model with cross-section
optimization. <em>ICRA</em>, 11585–11591. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Aneurysms, balloon-like bulges in blood vessels, present a significant health risk due to their potential to rupture, leading to life-threatening internal bleeding. Current treatments often involve delivering embolic materials or metal coils to fill these bulges, occluding them from the pressure of blood flow. However, clinical micro-catheters that deploy embolic materials used today face limitations, primarily their rigidity and the lack of active control over the bending tip of the catheter. This paper introduces a new soft micro-robotics catheter, with diameter of only 0.8 mm, equipped with a hollow channel. With this new design, the new device can induce bending motions at its tip for active steerability to reach desired aneurysm targets and then perform the delivery of embolic materials and tools. To enhance the control and precise navigation during procedures, a robust mathematical model and image processing techniques are also introduced and validated. Experiments are also performed to characterise and validate the model’s accuracy and the steerability and navigation capabilities of the new micro-catheter.},
  archive   = {C_ICRA},
  author    = {Emanuele Nicotra and Chi Cong Nguyen and James Davies and Phuoc Thien Phan and Trung Thien Hoang and Bibhu Sharma and Adrienne Ji and Kefan Zhu and Trung Dung Ngo and Van Anh Ho and Hung Manh La and Nigel H. Lovell and Thanh Nho Do},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610709},
  month     = {5},
  pages     = {11585-11591},
  title     = {A soft micro-robotic catheter for aneurysm treatment: A novel design and enhanced euler-bernoulli model with cross-section optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vascular centerline-guided autonomous navigation methods for
robot-lead endovascular interventions. <em>ICRA</em>, 11578–11584. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In minimally invasive endovascular interventional surgery, guidewire navigation is an indispensable process. However, even experienced physicians often encounter difficulties in manually manipulating the guidewire for branch selection, while also facing the risk of radiation exposure. In this study, we investigated robotic autonomous guidewire navigation methods. An electromagnetic system was used to track the real-time position and orientation of the guidewire tip, and a state space representing the guidewire within the vascular environment was constructed to guide the robot in precise guidewire manipulation. Experimental results demonstrated that the proposed trial-and-error and centerline-guided methods successfully completed navigation tasks in a static environment, outperforming human navigation performance in terms of trajectory smoothness, trajectory length, and incorrect branch entry counts. For dynamic environment navigation, dynamic time warping (DTW), a technique for measuring the similarity between two temporal sequences, was integrated into the centerline-guided method. The proposed approaches eliminate the need for visual feedback and thereby minimizing the risk of radiation exposure for both patients and medical staff present in the operating room during the procedure.},
  archive   = {C_ICRA},
  author    = {Naner Li and Yiwei Wang and Haoyuan Cheng and Huan Zhao and Han Ding},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611329},
  month     = {5},
  pages     = {11578-11584},
  title     = {Vascular centerline-guided autonomous navigation methods for robot-lead endovascular interventions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Co-axial slender tubular robot (CAST): Towards robotized
operation for transorbital neurosurgery with minimal invasiveness.
<em>ICRA</em>, 11571–11577. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Transorbital Neuro Surgery (TNS) offers a novel treatment towards the lesion inside skull pursuing minimal invasiveness. Most conventional TNS tools are rigid and straight, limiting the dexterity and accessibility in passing a small port. Bendable and steerable surgical tools provides an alternative for this issue. In this work, we proposed a dual-segment slender surgical robot arm for TNS, which is a Co-Axial Slender Tubular robot (CAST), and modelled it using novel approaches. Another contribution is tendon-mortise shaped slits along the axial direction, enhancing the overall stiffness. The bending of CAST is actuated by pushing/pulling distance, and the maximum diameter is only 1.7mm with high dexterity after mounting on a rigid robot arm. Experiments demonstrates that the proposed the slit design doubles the stiffness properties compared to traditional rectangle slit designs. The path-following task shows that the position error was maximally 3mm in open-looped control. Test on a skull model demonstrates that the whole system could successfully perform electrocoagulation procedure inside the depth of skull in a robotized manner effectively.},
  archive   = {C_ICRA},
  author    = {Shuai Wang and Qingxiang Zhao and Jian Chen and Mingcong Chen and Guanglin Cao and Jian Hu and Runfeng Zhu and Danny Tat Ming Chan and Ming Feng and Hongbin Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609994},
  month     = {5},
  pages     = {11571-11577},
  title     = {Co-axial slender tubular robot (CAST): Towards robotized operation for transorbital neurosurgery with minimal invasiveness},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online multi-contact feedback model predictive control for
interactive robotic tasks. <em>ICRA</em>, 11556–11562. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a model predictive control (MPC) that accomplishes interactive robotic tasks, in which multiple contacts may occur at unknown locations. To address such scenarios, we made an explicit contact feedback loop in the MPC framework. An algorithm called Multi-Contact Particle Filter with Exploration Particle (MCP-EP) is employed to establish real-time feedback of multi-contact information. Then the interaction locations and forces are accommodated in the MPC framework via a spring contact model. Moreover, we achieved real-time control for a 7 degrees of freedom robot without any simplifying assumptions by employing a Differential-Dynamic-Programming algorithm. We achieved 6.8kHz, 1.9kHz, and 1.8kHz update rates of the MPC for 0, 1, and 2 contacts, respectively. This allows the robot to handle unexpected contacts in real time. Real-world experiments show the effectiveness of the proposed method in various scenarios.},
  archive   = {C_ICRA},
  author    = {Seo Wook Han and Maged Iskandar and Jinoh Lee and Min Jun Kim},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611151},
  month     = {5},
  pages     = {11556-11562},
  title     = {Online multi-contact feedback model predictive control for interactive robotic tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stable, safe, and passive teleoperation of multi-robot
systems. <em>ICRA</em>, 11540–11546. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a unified framework to ensure the stability, safety, and passivity of a multi-robot teleoperation system in a holistic fashion. The proposed approach consists of encoding these three properties as constraints in an optimization-based controller using control Lypaunov and (integral) control barrier functions. The result is a stability-safety-passivity (SSP) filter implemented as a convex optimization control policy, which can be efficiently evaluated in an online fashion. The developed filter minimally modifies the teleoperation input in order to ensure that the robotic system remains stable, safe, and passive. The effectiveness of the developed approach is showcased using a team of mobile robots in a human-multi-robot teleoperation scenario.},
  archive   = {C_ICRA},
  author    = {Gennaro Notomista},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610937},
  month     = {5},
  pages     = {11540-11546},
  title     = {Stable, safe, and passive teleoperation of multi-robot systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How to train your neural control barrier function: Learning
safety filters for complex input-constrained systems. <em>ICRA</em>,
11532–11539. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Control barrier functions (CBFs) have become popular as a safety filter to guarantee the safety of nonlinear dynamical systems for arbitrary inputs. However, it is difficult to construct functions that satisfy the CBF constraints for high relative degree systems with input constraints. To address these challenges, recent work has explored learning CBFs using neural networks via neural CBFs (NCBFs). However, such methods face difficulties when scaling to higher dimensional systems under input constraints. In this work, we first identify challenges that NCBFs face during training. Next, to address these challenges, we propose policy neural CBFs (PNCBFs), a method of constructing CBFs by learning the value function of a nominal policy, and show that the value function of the maximum-over-time cost is a CBF. We demonstrate the effectiveness of our method in simulation on a variety of systems ranging from toy linear systems to a jet aircraft with a 16-dimensional state space. Finally, we validate our approach on a two-agent quadcopter system on hardware under tight input constraints.},
  archive   = {C_ICRA},
  author    = {Oswin So and Zachary Serlin and Makai Mann and Jake Gonzales and Kwesi Rutledge and Nicholas Roy and Chuchu Fan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610418},
  month     = {5},
  pages     = {11532-11539},
  title     = {How to train your neural control barrier function: Learning safety filters for complex input-constrained systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trajectory tracking runtime assurance for systems with
partially unknown dynamics. <em>ICRA</em>, 11525–11531. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problem of tracking a reference trajectory for dynamical systems subject to a priori unknown state-dependent disturbance behavior. We propose a formulation that embeds the uncertain system into a higher dimensional deterministic system that accounts for worst case disturbances. Our main insight is that a single controlled trajectory of this embedding system corresponds to a controlled forward invariant interval tube around the reference trajectory. By taking observations of the system, we then propose to estimate the state-dependent uncertainty with Gaussian Process regression, which improves the accuracy of the forward invariant tube as data is collected. Given a safety objective, we also provide conditions on when an additional observation of the unknown disturbance behavior needs to be collected to maintain safety. We demonstrate our formulation on a case study of a planar multirotor attempting a safe landing in an unknown wind field.},
  archive   = {C_ICRA},
  author    = {Michael E. Cao and Samuel Coogan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611237},
  month     = {5},
  pages     = {11525-11531},
  title     = {Trajectory tracking runtime assurance for systems with partially unknown dynamics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Force feedback model-predictive control via online
estimation. <em>ICRA</em>, 11503–11509. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Nonlinear model-predictive control has recently shown its practicability in robotics. However it remains limited in contact interaction tasks due to its inability to leverage sensed efforts. In this work, we propose a novel model-predictive control approach that incorporates direct feedback from force sensors while circumventing explicit modeling of the contact force evolution. Our approach is based on the online estimation of the discrepancy between the force predicted by the dynamics model and force measurements, combined with high-frequency nonlinear model-predictive control. We report an experimental validation on a torque-controlled manipulator in challenging tasks for which accurate force tracking is necessary. We show that a simple reformulation of the optimal control problem combined with standard estimation tools enables to achieve state-of-the-art performance in force control while preserving the benefits of model-predictive control, thereby outperforming traditional force control techniques. This work paves the way toward a more systematic integration of force sensors in model predictive control.},
  archive   = {C_ICRA},
  author    = {Armand Jordana and Sébastien Kleff and Justin Carpentier and Nicolas Mansard and Ludovic Righetti},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611156},
  month     = {5},
  pages     = {11503-11509},
  title     = {Force feedback model-predictive control via online estimation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal control for clutched-elastic robots: A
contact-implicit approach. <em>ICRA</em>, 11481–11487. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Intrinsically elastic robots surpass their rigid counterparts in a range of different characteristics. By temporarily storing potential energy and subsequently converting it to kinetic energy, elastic robots are capable of highly dynamic motions even with limited motor power. However, the time-dependency of this energy storage and release mechanism remains one of the major challenges in controlling elastic robots. A possible remedy is the introduction of locking elements (i.e. clutches and brakes) in the drive train. This gives rise to a new class of robots, so-called clutched-elastic robots (CER), with which it is possible to precisely control the energy-transfer timing. A prevalent challenge in the realm of CERs is the automatic discovery of clutch sequences. Due to complexity, many methods still rely on pre-defined modes. In this paper, we introduce a novel contact-implicit scheme designed to optimize both control input and clutch sequence simultaneously. A penalty in the objective function ensures the prevention of unnecessary clutch transitions. We empirically demonstrate the effectiveness of our proposed method on a double pendulum equipped with two of our newly proposed clutch-based Bi-Stiffness Actuators (BSA).},
  archive   = {C_ICRA},
  author    = {Dennis Ossadnik and Vasilije Rakčević and Mehmet C. Yildirim and Edmundo Pozo Fortunić and Hugo T. M. Kussaba and Abdalla Swikir and Sami Haddadin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610380},
  month     = {5},
  pages     = {11481-11487},
  title     = {Optimal control for clutched-elastic robots: A contact-implicit approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dexterous legged locomotion in confined 3D spaces with
reinforcement learning. <em>ICRA</em>, 11474–11480. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances of locomotion controllers utilizing deep reinforcement learning (RL) have yielded impressive results in terms of achieving rapid and robust locomotion across challenging terrain, such as rugged rocks, non-rigid ground, and slippery surfaces. However, while these controllers primarily address challenges underneath the robot, relatively little research has investigated legged mobility through confined 3D spaces, such as narrow tunnels or irregular voids, which impose all-around constraints. The cyclic gait patterns resulted from existing RL-based methods to learn parameterized locomotion skills characterized by motion parameters, such as velocity and body height, may not be adequate to navigate robots through challenging confined 3D spaces, requiring both agile 3D obstacle avoidance and robust legged locomotion. Instead, we propose to learn locomotion skills end-to-end from goal-oriented navigation in confined 3D spaces. To address the inefficiency of tracking distant navigation goals, we introduce a hierarchical locomotion controller that combines a classical planner tasked with planning waypoints to reach a faraway global goal location, and an RL-based policy trained to follow these waypoints by generating low-level motion commands. This approach allows the policy to explore its own locomotion skills within the entire solution space and facilitates smooth transitions between local goals, enabling long-term navigation towards distant goals. In simulation, our hierarchical approach succeeds at navigating through demanding confined 3D environments, outperforming both pure end-to-end learning approaches and parameterized locomotion skills. We further demonstrate the successful real-world deployment of our simulation-trained controller on a real robot.},
  archive   = {C_ICRA},
  author    = {Zifan Xu and Amir Hossain Raj and Xuesu Xiao and Peter Stone},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610668},
  month     = {5},
  pages     = {11474-11480},
  title     = {Dexterous legged locomotion in confined 3D spaces with reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Maximizing quadruped velocity by minimizing energy.
<em>ICRA</em>, 11467–11473. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement Learning (RL) has been a powerful tool for training robots to acquire agile locomotion skills. To learn locomotion, it is commonly necessary to introduce additional reward-shaping terms, such as an energy minimization term, to guide an algorithm like Proximal Policy Optimization (PPO) to good performance. Prior works rely on hyper-parameter tuning on the weight of the reward shaping terms to obtain satisfactory task performance. To save the efforts of tuning these weights, we adopt the Extrinsic-Intrinsic Policy Optimization (EIPO) framework. The key idea of EIPO is to establish a constrained optimization framework for the primary objective of enhancing task performance and the secondary objective of minimizing energy consumption. It seeks a policy that minimizes the energy consumption objective within the optimal policy space for task performance. This guarantees that the learned policy excels in task performance while conserving energy, all without requiring manual weight adjustments for both objectives. Our experiments evaluate EIPO on various quadruped locomotion tasks, revealing that policies trained with EIPO consistently achieve higher task performance than PPO comparisons while maintaining comparable energy consumption levels. Furthermore, EIPO exhibits superior task performance in real-world evaluations compared to PPO.},
  archive   = {C_ICRA},
  author    = {Srinath Mahankali and Chi-Chang Lee and Gabriel B. Margolis and Zhang-Wei Hong and Pulkit Agrawal},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609983},
  month     = {5},
  pages     = {11467-11473},
  title     = {Maximizing quadruped velocity by minimizing energy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust quadrupedal locomotion via risk-averse policy
learning. <em>ICRA</em>, 11459–11466. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The robustness of legged locomotion is crucial for quadrupedal robots in challenging terrains. Recently, Reinforcement Learning (RL) has shown promising results in legged locomotion and various methods try to integrate privileged distillation, scene modeling, and external sensors to improve the generalization and robustness of locomotion policies. However, these methods are hard to handle uncertain scenarios such as abrupt terrain changes or unexpected external forces. In this paper, we consider a novel risk-sensitive perspective to enhance the robustness of legged locomotion. Specifically, we employ a distributional value function learned by quantile regression to model the aleatoric uncertainty of environments, and perform risk-averse policy learning by optimizing the worst-case scenarios via a risk distortion measure. Extensive experiments in both simulation environments and a real Aliengo robot demonstrate that our method is efficient in handling various external disturbances, and the resulting policy exhibits improved robustness in harsh and uncertain situations in legged locomotion.},
  archive   = {C_ICRA},
  author    = {Jiyuan Shi and Chenjia Bai and Haoran He and Lei Han and Dong Wang and Bin Zhao and Mingguo Zhao and Xiu Li and Xuelong Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610086},
  month     = {5},
  pages     = {11459-11466},
  title     = {Robust quadrupedal locomotion via risk-averse policy learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning risk-aware quadrupedal locomotion using
distributional reinforcement learning. <em>ICRA</em>, 11451–11458. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deployment in hazardous environments requires robots to understand the risks associated with their actions and movements to prevent accidents. Despite its importance, these risks are not explicitly modeled by currently deployed locomotion controllers for legged robots. In this work, we propose a risk sensitive locomotion training method employing distributional reinforcement learning to consider safety explicitly. Instead of relying on a value expectation, we estimate the complete value distribution to account for uncertainty in the robot’s interaction with the environment. The value distribution is consumed by a risk metric to extract risk sensitive value estimates. These are integrated into Proximal Policy Optimization (PPO) to derive our method, Distributional Proximal Policy Optimization (DPPO). The risk preference, ranging from risk-averse to risk-seeking, can be controlled by a single parameter, which enables to adjust the robot’s behavior dynamically. Importantly, our approach removes the need for additional reward function tuning to achieve risk sensitivity. We show emergent risk sensitive locomotion behavior in simulation and on the quadrupedal robot ANYmal. Videos of the experiments and code are available at https://sites.google.com/leggedrobotics.com/risk-aware-locomotion.},
  archive   = {C_ICRA},
  author    = {Lukas Schneider and Jonas Frey and Takahiro Miki and Marco Hutter},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610137},
  month     = {5},
  pages     = {11451-11458},
  title     = {Learning risk-aware quadrupedal locomotion using distributional reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Extreme parkour with legged robots. <em>ICRA</em>,
11443–11450. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans can perform parkour by traversing obstacles in a highly dynamic fashion requiring precise eye-muscle coordination and movement. Getting robots to do the same task requires overcoming similar challenges. Classically, this is done by independently engineering perception, actuation, and control systems to very low tolerances. This restricts them to tightly controlled settings such as a predetermined obstacle course in labs. In contrast, humans are able to learn parkour through practice without significantly changing their underlying biology. In this paper, we take a similar approach to developing robot parkour on a small low-cost robot with imprecise actuation and a single front-facing depth camera for perception which is low-frequency, jittery, and prone to artifacts. We show how a single neural net policy operating directly from a camera image, trained in simulation with large-scale RL, can overcome imprecise sensing and actuation to output highly precise control behavior end-to-end. We show our robot can perform a high jump on obstacles 2x its height, long jump across gaps 2x its length, do a handstand and run across tilted ramps, and generalize to novel obstacle courses with different physical properties. Parkour videos at https: //extreme-parkour.github.io/.},
  archive   = {C_ICRA},
  author    = {Xuxin Cheng and Kexin Shi and Ananye Agarwal and Deepak Pathak},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610200},
  month     = {5},
  pages     = {11443-11450},
  title     = {Extreme parkour with legged robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning agile locomotion and adaptive behaviors via
RL-augmented MPC. <em>ICRA</em>, 11436–11442. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the context of legged robots, adaptive behavior involves adaptive balancing and adaptive swing foot reflection. While adaptive balancing counteracts perturbations to the robot, adaptive swing foot reflection helps the robot to navigate intricate terrains without foot entrapment. In this paper, we manage to bring both aspects of adaptive behavior to quadruped locomotion by combining RL and MPC while improving the robustness and agility of blind legged locomotion. This integration leverages MPC’s strength in predictive capabilities and RL’s adeptness in drawing from past experiences. Unlike traditional locomotion controls that separate stance foot control and swing foot trajectory, our innovative approach unifies them, addressing their lack of synchronization. At the heart of our contribution is the synthesis of stance foot control with swing foot reflection, improving agility and robustness in locomotion with adaptive behavior. A hallmark of our approach is robust blind stair climbing through swing foot reflection. Moreover, we intentionally designed the learning module as a general plugin for different robot platforms. We trained the policy and implemented our approach on the Unitree A1 robot, achieving impressive results: a peak turn rate of 8.5 rad/s, a peak running speed of 3 m/s, and steering at a speed of 2.5 m/s. Remarkably, this framework also allows the robot to maintain stable locomotion while bearing an unexpected load of 10 kg, or 83% of its body mass. We further demonstrate the generalizability and robustness of the same policy where it realizes zero-shot transfer to different robot platforms like Go1 and AlienGo robots for load carrying. Code is made available for the use of the research community at https://github.com/DRCL-USC/RL_augmented_MPC.git},
  archive   = {C_ICRA},
  author    = {Yiyu Chen and Quan Nguyen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610453},
  month     = {5},
  pages     = {11436-11442},
  title     = {Learning agile locomotion and adaptive behaviors via RL-augmented MPC},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep compliant control for legged robots. <em>ICRA</em>,
11421–11427. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Control policies trained using deep reinforcement learning often generate stiff, high-frequency motions in response to unexpected disturbances. To promote more natural and compliant balance recovery strategies, we propose a simple modification to the typical reinforcement learning training process. Our key insight is that stiff responses to perturbations are due to an agent’s incentive to maximize task rewards at all times, even as perturbations are being applied. As an alternative, we introduce an explicit recovery stage where tracking rewards are given irrespective of the motions generated by the control policy. This allows agents a chance to gradually recover from disturbances before attempting to carry out their main tasks. Through an in-depth analysis, we highlight both the compliant nature of the resulting control policies, as well as the benefits that compliance brings to legged locomotion. In our simulation and hardware experiments, the compliant policy achieves more robust, energy-efficient, and safe interactions with the environment.},
  archive   = {C_ICRA},
  author    = {Adrian Hartmann and Dongho Kang and Fatemeh Zargarbashi and Miguel Zamora and Stelian Coros},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611209},
  month     = {5},
  pages     = {11421-11427},
  title     = {Deep compliant control for legged robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mechanical design and kinematics of a multimodal two-wheeled
robot. <em>ICRA</em>, 11398–11404. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A two-wheeled vehicle has a compact structure and high mobility in crowded and complex environments, which has been widely used in urban logistics. The bicycle and self-balancing vehicle are the two main modes of the two-wheeled vehicle, and their combination allows for good balance-control stability at both high and low speeds. Four control inputs by two steerable driving wheels are required to implement transformations between the two modes due to the difference in their configuration spaces. However, the control inputs are redundant for planar motions, which results in an over-constraint of the vehicle. In this work, a two-wheeled robot with an additional structural degree of freedom (DOF) is designed to balance inputs and DOFs to avoid over-constraint. A transition mode based on oblique vehicle motions is used to bridge the transformation of the bicycle and self-balancing vehicle modes. A general kinematic model is developed for the two-wheeled robot’s planar motions, and the three modes’ kinematics are special cases with particular servo constraints. Structural DOF control laws are developed and experimentally validated on a prototype robot. Smooth transformations of the multimodal motions are also validated by using the prototype.},
  archive   = {C_ICRA},
  author    = {Botian Sun and Qinglin Lang and Minghe Li and Xuefeng Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610920},
  month     = {5},
  pages     = {11398-11404},
  title     = {Mechanical design and kinematics of a multimodal two-wheeled robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and central pattern generator control of a new
transformable wheel-legged robot. <em>ICRA</em>, 11383–11389. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a new wheel-legged robot and develops motion controllers based on central pattern generators (CPGs) for the robot to navigate over a range of terrains. A transformable leg-wheel design is considered and characterized in terms of key locomotion characteristics as a function of the design. Kinematic analysis is conducted based on a generalized four-bar mechanism driven by a coaxial hub arrangement. The analysis is used to inform the design of a central pattern generator to control the robot by mapping oscillator states to wheel-leg trajectories and implementing differential steering within the oscillator network. Three oscillator models are used as the basis of the CPGs, and their performance is compared over a range of inputs. The CPG-based controller is used to drive the developed robot prototype on level ground and over obstacles. Additional simulated tests are performed for uneven terrain negotiation and obstacle climbing. Results demonstrate the effectiveness of CPG control in transformable wheel-legged robots.},
  archive   = {C_ICRA},
  author    = {Tyler Bishop and Keran Ye and Konstantinos Karydis},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610884},
  month     = {5},
  pages     = {11383-11389},
  title     = {Design and central pattern generator control of a new transformable wheel-legged robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rolling with planar parametric curves for real-time robot
locomotion algorithms. <em>ICRA</em>, 11356–11362. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots routinely encounter obstacles and rough terrain, but terrain curvature is seldom included in models for real-time algorithms. We present a closed-form dynamic model for rolling with two planar smooth curves, and apply it to sagittal-plane locomotion problems. We assumed that the body rolls without slip and maintains a single point of contact. Using an auxiliary coordinate system to define the rolling body and terrain as parametric curves, we derived rolling constraints and dynamic equations of motion for model-based control algorithms – specifically Operational Space Control. The formulation was used to simulate an arbitrarily curved rock rolling on undulating terrain and to generate control signals to stabilize it on parabolic terrain. The stabilization problem was solved as a quadratic program in &lt; 3 ms which shows that our formulation is suitable for real-time control algorithms. We also applied this framework to dynamically balance an underactuated 2 degree-of-freedom leg on parabolic terrain and achieve prescribed locomotion tasks for a wheel-leg vehicle on sinusoidal terrain in simulation. A supplementary video is available at https://youtu.be/EtPQEzkqsK8.},
  archive   = {C_ICRA},
  author    = {Adwait Mane and Christian Hubicki},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610524},
  month     = {5},
  pages     = {11356-11362},
  title     = {Rolling with planar parametric curves for real-time robot locomotion algorithms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Body velocity estimation in a leg–wheel transformable robot
without a priori knowledge of leg–wheel ground contacts. <em>ICRA</em>,
11349–11355. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The state estimation of legged robots often relies on ground contact detection. However, due to complex mechanisms and other factors, ground contact detection can be challenging to obtain in certain situations. This paper presents a velocity estimation method that combines inertia measurement unit (IMU) and encoders, allowing estimation without using the ground contact state as the a priori. In this paper, the initial estimate derived from IMU integration is refined. Following the computation of velocity and ground contact state probabilities using encoder data, these probabilities are employed to modify particle weights within the particle filter framework. Subsequent resampling ensures that the contact status converges toward the correct result. This paper tests the algorithm through simulations and validates the method with physical experiments, showcasing the feasibility of concurrent ground contact state and velocity estimation.},
  archive   = {C_ICRA},
  author    = {Pei-Chun Huang and I-Chia Chang and Wei-Shun Yu and Pei-Chun Lin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610114},
  month     = {5},
  pages     = {11349-11355},
  title     = {Body velocity estimation in a Leg–Wheel transformable robot without a priori knowledge of Leg–Wheel ground contacts},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast wheeled driving to legged leaping onto a step in a
leg-wheel transformable robot. <em>ICRA</em>, 11342–11348. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The leg-wheel transformable robot has the advantage of smooth, fast, and power-efficient motion on flat terrain and negotiability on rough terrain. This study presents a highly dynamic maneuver of the robot to leap onto a step using its legged form from its original form of wheeled driving, taking full advantage of the rapid switching capabilities of the leg-wheel design of the robot. The robot motion is designed based on a reduced-order model and is planned using an optimization method with multiple constraints. In addition, both position and impedance control strategies are investigated. The proposed strategy is experimentally evaluated. The results show that the robot can leap onto a step higher than itself and then smoothly transition back to the wheeled mode after leaping. The dynamic driving-to-leaping maneuver endows the robot with an alternative and time-efficient approach to negotiate the step obstacles.},
  archive   = {C_ICRA},
  author    = {Zhi-Ren Chen and Wei-Shun Yu and Pei-Chun Lin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610303},
  month     = {5},
  pages     = {11342-11348},
  title     = {Fast wheeled driving to legged leaping onto a step in a leg-wheel transformable robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A 3D mixed reality interface for human-robot teaming.
<em>ICRA</em>, 11327–11333. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a mixed-reality human-robot teaming system. It allows human operators to see in real-time where robots are located, even if they are not in line of sight. The operator can also visualize the map that the robots create of their environment and can easily send robots to new goal positions. The system mainly consists of a mapping and a control module. The mapping module is a real-time multi-agent visual SLAM system that co-localizes all robots and mixed-reality devices to a common reference frame. Visualizations in the mixed-reality device then allow operators to see a virtual life-sized representation of the cumulative 3D map overlaid onto the real environment. As such, the operator can effectively &quot;see through&quot; walls into other rooms. To control robots and send them to new locations, we propose a drag-and-drop interface. An operator can grab any robot hologram in a 3D mini map and drag it to a new desired goal pose. We validate the proposed system through a user study and real-world deployments. We make the mixed-reality application publicly available at github.com/cvg/HoloLens_ros.},
  archive   = {C_ICRA},
  author    = {Jiaqi Chen and Boyang Sun and Marc Pollefeys and Hermann Blum},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611017},
  month     = {5},
  pages     = {11327-11333},
  title     = {A 3D mixed reality interface for human-robot teaming},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatic captioning based on visible and infrared images.
<em>ICRA</em>, 11312–11318. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we tackle the task of image captioning with the complementarity of visible light images and infrared images. To address this problem, we propose an RGBIR image fusion captioning model, which can take full advantage of visible light images and infrared images under different conditions. Meanwhile, we develop a wearable environment-assisted system. In addition, we collect and annotate a new dataset containing 3510 pairs of RGB-IR images to support model training. Finally, we conduct extensive experiments to evaluate the model and system. Experimental results show that our new method and system significantly outperform baselines on multiple metrics and have potential practical value.},
  archive   = {C_ICRA},
  author    = {Yan Wang and Shuli Lou and Kai Wang and Yunzhe Wang and Xiaohu Yuan and Huaping Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610654},
  month     = {5},
  pages     = {11312-11318},
  title     = {Automatic captioning based on visible and infrared images},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High stimuli virtual reality training for a brain controlled
robotic wheelchair. <em>ICRA</em>, 11305–11311. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Smart robotic wheelchairs, as well as other assistive robotic devices, can provide an effective form of independent mobility for those who suffer with motor disabilities. Although many control interfaces exist to operate these devices, brain computer interfaces (BCI) offer a control modality for those who have little to no motor function, as well as being able to re-associate movement with brain functionality. Although BCIs have been designed for robotic wheelchairs, more research and development is required before they can be adopted for use in the ‘real world’. One key challenge on that journey is the user training required to achieve an acceptable accuracy of the control. In this paper, we aim to identify the best training method by comparing users trained on a simple task, in a simulated environment on a 2D display (VR-2DD) and in a virtual environment using a virtual reality headset (VR-HMD). We trained 15 participants in mix of high and low noise virtual environments or on a simple training task, and found a significant improvement in the classification accuracies of the participants who trained using the VR-2DD task compared with those who were trained with the simple task. We also carried out active (online) tests across all participants in the same virtual training environment, with a varying level of external stimuli, and found a significant improvement in the performance of participants in both VR groups compared to participants in the simple task group.},
  archive   = {C_ICRA},
  author    = {Alexander Thomas and Jianan Chen and Anna Hella-Szabo and Merlin Kelly and Tom Carlson},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610636},
  month     = {5},
  pages     = {11305-11311},
  title     = {High stimuli virtual reality training for a brain controlled robotic wheelchair},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human-robot interactive creation of artistic portrait
drawings. <em>ICRA</em>, 11297–11304. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a novel system for Human-Robot Interactive Creation of Artworks (HRICA). Different from previous robot painters, HRICA allows a human user and a robot to alternately draw strokes on a canvas, to collaboratively create a portrait drawing through frequent interactions. The key is to enable the robot to understand human intentions, during the interactive creation process. We here formulate this as a mask-free image inpainting problem, and propose a novel method to estimate the complete version of a portrait drawing, after the human user has drawn some initial strokes. In this way, the robot can select some complementary strokes and draw them on the canvas. To train and evaluate our inpainting method, we construct a novel large-scale portrait drawing dataset, CelebLine, which composes of high-quality portrait line-drawings, with dense labels of both 2D semantic parsing masks and 3D depth maps. Finally, we develop a human-robot interactive drawing system with low-cost hardware, user-friendly interface, and interesting creation experience. Experiments show that our robot can stably cooperate with human users to create diverse styles of portrait drawings. In addition, our portrait drawing inpainting method significantly outperforms previous advanced methods. The code and dataset have been released at: https://github.com/fei-aiart/HRICA.},
  archive   = {C_ICRA},
  author    = {Fei Gao and Lingna Dai and Jingjie Zhu and Mei Du and Yiyuan Zhang and Maoying Qiao and Chenghao Xia and Nannan Wang and Peng Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611451},
  month     = {5},
  pages     = {11297-11304},
  title     = {Human-robot interactive creation of artistic portrait drawings},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interactive continual learning architecture for long-term
personalization of home service robots. <em>ICRA</em>, 11289–11296. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For robots to perform assistive tasks in unstructured home environments, they must learn and reason on the semantic knowledge of the environments. Despite a resurgence in the development of semantic reasoning architectures, these methods assume that all the training data is available a priori. However, each user’s environment is unique and can continue to change over time, which makes these methods unsuitable for personalized home service robots. Although research in continual learning develops methods that can learn and adapt over time, most of these methods are tested in the narrow context of object classification on static image datasets. In this paper, we combine ideas from continual learning, semantic reasoning, and interactive machine learning literature and develop a novel interactive continual learning architecture for continual learning of semantic knowledge in a home environment through human-robot interaction. The architecture builds on core cognitive principles of learning and memory for efficient and real-time learning of new knowledge from humans. We integrate our architecture with a physical mobile manipulator robot and perform extensive system evaluations in a laboratory environment over two months. Our results demonstrate the effectiveness of our architecture to allow a physical robot to continually adapt to the changes in the environment from limited data provided by the users (experimenters), and use the learned knowledge to perform object fetching tasks.},
  archive   = {C_ICRA},
  author    = {Ali Ayub and Chrystopher L. Nehaniv and Kerstin Dautenhahn},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611386},
  month     = {5},
  pages     = {11289-11296},
  title     = {Interactive continual learning architecture for long-term personalization of home service robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LaCE-LHMP: Airflow modelling-inspired long-term human motion
prediction by enhancing laminar characteristics in human flow.
<em>ICRA</em>, 11281–11288. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Long-term human motion prediction (LHMP) is essential for safely operating autonomous robots and vehicles in populated environments. It is fundamental for various applications, including motion planning, tracking, human-robot interaction and safety monitoring. However, accurate prediction of human trajectories is challenging due to complex factors, including, for example, social norms and environmental conditions. The influence of such factors can be captured through Maps of Dynamics (MoDs), which encode spatial motion patterns learned from (possibly scattered and partial) past observations of motion in the environment and which can be used for data-efficient, interpretable motion prediction (MoD-LHMP). To address the limitations of prior work, especially regarding accuracy and sensitivity to anomalies in long-term prediction, we propose the Laminar Component Enhanced LHMP approach (LaCE-LHMP). Our approach is inspired by data-driven airflow modelling, which estimates laminar and turbulent flow components and uses predominantly the laminar components to make flow predictions. Based on the hypothesis that human trajectory patterns also manifest laminar flow (that represents predictable motion) and turbulent flow components (that reflect more unpredictable and arbitrary motion), LaCE-LHMP extracts the laminar patterns in human dynamics and uses them for human motion prediction. We demonstrate the superior prediction performance of LaCE-LHMP through benchmark comparisons with state-of-the-art LHMP methods, offering an unconventional perspective and a more intuitive understanding of human movement patterns.},
  archive   = {C_ICRA},
  author    = {Yufei Zhu and Han Fan and Andrey Rudenko and Martin Magnusson and Erik Schaffernicht and Achim J. Lilienthal},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610717},
  month     = {5},
  pages     = {11281-11288},
  title     = {LaCE-LHMP: Airflow modelling-inspired long-term human motion prediction by enhancing laminar characteristics in human flow},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Recency bias in task performance history affects perceptions
of robot competence and trustworthiness. <em>ICRA</em>, 11274–11280. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human memory of a robot’s competence, and resulting subjective perceptions of that robot, are influenced by numerous cognitive biases. One class of cognitive bias deals with the ordering of items or interactions: information presented last among a grouping is most salient in memory formation (recency bias), followed by information presented first (primacy bias), followed by information in the middle, collectively known as the serial-position effect. For example, if a human’s last observation of a robot involves a task failure, this will disproportionately negatively alter their perception of the robot’s competence, as well as their trust in the robot moving forward. It is valuable to characterize the effect of these biases within human-robot interactions to inform strategies for risk-aware planning that cultivate appropriate levels of human trust. We conducted a human-subjects study (n=53) testing the influence of the serial-position effect on recalled competence (see overview at https://youtu.be/BgH2zhh1s48). Participants viewed videos of a robot performing the same tasks at the same level of competence, with task order differing by experimental condition (rising competence, falling competence, or failures at the midpoint), asking participants to rate robot competence in between every video as well at the very end of the experiment. We found that while the average between-video rating of robot competence remained stable across conditions, the recalled, post-experiment ratings of competence and trust were significantly lower in the condition with decreasing competence than in either of the other two conditions, suggesting a notable recency bias. We conclude with implications for human-subjects experiment design (i.e., how subjective measures are influenced by ordering effects) and provide design recommendations to minimize them. We further discuss practical applications of these results in creating risk-aware robotic planners capable of trust calibration.},
  archive   = {C_ICRA},
  author    = {Matthew B. Luebbers and Aaquib Tabrez and Kanaka Samagna Talanki and Bradley Hayes},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611334},
  month     = {5},
  pages     = {11274-11280},
  title     = {Recency bias in task performance history affects perceptions of robot competence and trustworthiness},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian optimization framework for the automatic tuning
of MPC-based shared controllers. <em>ICRA</em>, 11259–11265. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a Bayesian optimization framework for the automatic tuning of shared controllers which are defined as a Model Predictive Control (MPC) problem. The proposed framework includes the design of performance metrics as well as the representation of user inputs for simulation-based optimization. The framework is applied to the optimization of a shared controller for an Image Guided Therapy robot. VR-based user experiments confirm the increase in performance of the automatically tuned MPC shared controller with respect to a hand-tuned baseline version as well as its generalization ability.},
  archive   = {C_ICRA},
  author    = {Anne van der Horst and Bas Meere and Dinesh Krishnamoorthy and Saray Bakker and Bram van de Vrande and Henry Stoutjesdijk and Marco Alonso and Elena Torta},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610655},
  month     = {5},
  pages     = {11259-11265},
  title     = {A bayesian optimization framework for the automatic tuning of MPC-based shared controllers},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dual closed-loop control strategy for human-following
robots respecting social space. <em>ICRA</em>, 11252–11258. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human following for mobile robots has emerged as a promising technique with widespread applications. To ensure psychological comfort while collaborating, coexisting, and interacting with humans, robots need to respect the social space of the target person. In this study, we propose a dual closed-loop human-following control strategy that combines model predictive control (MPC) and impedance control. The outer-loop MPC ensures precise control of the robot’s posture while tracking the target person’s velocity and direction to coordinate the motion between them. The inner-loop impedance controller is employed to regulate the robot’s motion and interaction force with the target person, enabling the robot to maintain a respectful and comfortable distance from the target person. Concretely, the social interaction dynamics characteristics between the robot and the target person are described by human-robot interaction dynamics, which considers the rules of social space. Furthermore, an obstacle avoidance component constructed using behavioral dynamics is integrated into the impedance controller. Experimental results demonstrate the effectiveness of the proposed method in achieving human following and obstacle avoidance without intruding into the intimate zone of the target person.},
  archive   = {C_ICRA},
  author    = {Jianwei Peng and Zhelin Liao and Zefan Su and Hanchen Yao and Yadan Zeng and Houde Dai},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611263},
  month     = {5},
  pages     = {11252-11258},
  title     = {A dual closed-loop control strategy for human-following robots respecting social space},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatic trust estimation from movement data in industrial
human-robot collaboration based on deep learning. <em>ICRA</em>,
11245–11251. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trust in automation is usually assessed with post-interaction questionnaires. For human robot collaboration it would be beneficial to assess the trust level during the interaction to adjust the robot’s collaboration behavior to the user expectations. In this paper we investigate if trust can be estimated from observable behavior like movements during the interaction with a large industrial manipulator. To this end, we report on a data collection for two tasks during collaborative draping, the transport of large cut pieces and the actual draping process in close proximity to the robot. The data is used to train and compare different deep learning models. Results show that automatic trust estimation is feasible, which opens up to using trust as a parameter for informing the interaction with robots.},
  archive   = {C_ICRA},
  author    = {Matthias Rehm and Ioannis Pontikis and Kasper Hald},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610822},
  month     = {5},
  pages     = {11245-11251},
  title     = {Automatic trust estimation from movement data in industrial human-robot collaboration based on deep learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design of two morphing robot surfaces and results from a
user study on what people want and expect of them, towards a
“robot-room”. <em>ICRA</em>, 11239–11244. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose, examine prototypes of, and collect user input on morphing robotic surfaces, &quot;robot-room&quot; elements that, individually or in combination, change the functionality of the rooms we live in, directly controlled by the room’s occupants engaging with it. Robot-rooms represent an advance in human-robot interaction whereby human interaction is within a machine that physically envelops us. We discuss the motivation for such robot-rooms, present initial work aimed at their physical realization, and report on a user study of 80 participants to learn what people might want of and expect from robot rooms, the results of which will inform both the iterative design of the robot room and the thinking of our community as it grapples with how we want to live with (and &quot;in&quot;) robots.},
  archive   = {C_ICRA},
  author    = {Nithesh Kumar and Hsin-Ming Chao and Bruno Dantas Da Silva Tassari and Elena Sabinson and Ian D. Walker and Keith E. Green},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611246},
  month     = {5},
  pages     = {11239-11244},
  title     = {Design of two morphing robot surfaces and results from a user study on what people want and expect of them, towards a &quot;Robot-room&quot;},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Language-guided active sensing of confined, cluttered
environments via object rearrangement planning. <em>ICRA</em>,
11224–11230. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Language-guided active sensing is a robotics sub-task where a robot with an onboard sensor interacts efficiently with the environment via object manipulation to maximize perceptual information, following given language instructions. These tasks appear in various practical robotics applications, such as household service, search and rescue, and environment monitoring. Despite many applications, the existing works do not account for language instructions and have mainly focused on surface sensing, i.e., perceiving the environment from the outside without rearranging it for dense sensing. Therefore, in this paper, we introduce the first language-guided active sensing approach that allows users to observe specific parts of the environment via object manipulation. Our method spatially associates the environment with language instructions, determines the best camera viewpoints for perception, and then iteratively selects and relocates the best view-blocking objects to provide the dense perception of the region of interest. We evaluate our method against different baseline algorithms in simulation and also demonstrate it in real-world confined cabinet-like settings with multiple unknown objects. Our results show that the proposed method exhibits better performance across different metrics and successfully generalizes to real-world complex scenarios.},
  archive   = {C_ICRA},
  author    = {Weihan Chen and Hanwen Ren and Ahmed H. Qureshi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610296},
  month     = {5},
  pages     = {11224-11230},
  title     = {Language-guided active sensing of confined, cluttered environments via object rearrangement planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Usability evaluation framework for close-proximity
collaboration with large industrial manipulators. <em>ICRA</em>,
11209–11215. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Our goal is to design a framework for holistic evaluation of human-robot collaboration systems. To this end we utilize several standardized questionnaires administered while participants perform collaborative tasks in robot work cells. We used Standard Usability Scale and the Usability metric for user experience questionnaires to access usability, NASA Task-Load for workload, two questionnaires for human-robot trust as well as the Unified theory of acceptance and use of technology questionnaires. We performed two pilot tests of our framework with human-robot collabotation work cells at two test sites as part of the DrapeBot project. The goal of the project is to enable human-robot collaboration in the process of carbon fiber draping the production of outer parts. After utilizing the evaluation framework at the two test sites we found that the collection of questionnaires were easy to adapt to each work cell and the practical limitation around running the experiments. Both work cells scored high in usability, expected increase of productivity, as well as high trust and low anxiety, but both work cells scored low on expectancy of use for work in the future at their current state of development.},
  archive   = {C_ICRA},
  author    = {Kasper Hald and Matthias Rehm},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610568},
  month     = {5},
  pages     = {11209-11215},
  title     = {Usability evaluation framework for close-proximity collaboration with large industrial manipulators},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Closing the visual sim-to-real gap with object-composable
NeRFs. <em>ICRA</em>, 11202–11208. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep learning methods for perception are the cornerstone of many robotic systems. Despite their potential for impressive performance, obtaining real-world training data is expensive, and can be impractically difficult for some tasks. Sim-to-real transfer with domain randomization offers a potential workaround, but often requires extensive manual tuning and results in models that are brittle to distribution shift between sim and real. In this work, we introduce Composable Object Volume NeRF (COV-NeRF), an object-composable NeRF model that is the centerpiece of a real-to-sim pipeline for synthesizing training data targeted to scenes and objects from the real world. COV-NeRF extracts objects from real images and composes them into new scenes, generating photorealistic renderings and many types of 2D and 3D supervision, including depth maps, segmentation masks, and meshes. We show that COV-NeRF matches the rendering quality of modern NeRF methods, and can be used to rapidly close the sim-to-real gap across a variety of perceptual modalities.},
  archive   = {C_ICRA},
  author    = {Nikhil Mishra and Maximilian Sieb and Pieter Abbeel and Xi Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611427},
  month     = {5},
  pages     = {11202-11208},
  title     = {Closing the visual sim-to-real gap with object-composable NeRFs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cycle-correspondence loss: Learning dense view-invariant
visual features from unlabeled and unordered RGB images. <em>ICRA</em>,
11186–11193. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot manipulation relying on learned object-centric descriptors became popular in recent years. Visual descriptors can easily describe manipulation task objectives, they can be learned efficiently using self-supervision, and they can encode actuated and even non-rigid objects. However, learning robust, view-invariant keypoints in a self-supervised approach requires a meticulous data collection approach involving precise calibration and expert supervision. In this paper we introduce Cycle-Correspondence Loss (CCL) for view-invariant dense descriptor learning, which adopts the concept of cycle-consistency, enabling a simple data collection pipeline and training on unpaired RGB camera views. The key idea is to autonomously detect valid pixel correspondences by attempting to use a prediction over a new image to predict the original pixel in the original image, while scaling error terms based on the estimated confidence. Our evaluation shows that we outperform other self-supervised RGB-only methods, and approach performance of supervised methods, both with respect to keypoint tracking as well as for a robot grasping downstream task.},
  archive   = {C_ICRA},
  author    = {David B. Adrian and Andras Gabor Kupcsik and Markus Spies and Heiko Neumann},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610820},
  month     = {5},
  pages     = {11186-11193},
  title     = {Cycle-correspondence loss: Learning dense view-invariant visual features from unlabeled and unordered RGB images},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual noun modifiers: The problem of binding visual and
linguistic cues*. <em>ICRA</em>, 11178–11185. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many robotic applications, especially those involving humans and the environment, linguistic and visual information must be processed jointly and bound together. Existing works either encode the image or the language into a subsymbolic space, like the CLIP model, or create a symbolic space of extracted information, like the object detection models. In this paper, we propose to describe images by nouns and modifiers and introduce a new embedded binding space where the linguistic and visual cues can effectively be bound. We investigate how state-of-the-art models perform in recognizing nouns and modifiers from images, and propose our method by introducing a dataset and CLIP-like recognition techniques based on transfer learning and metric learning. We show real-world experiments that demonstrate the practical applicability of our approach to robotics applications. Our results indicate that our method can surpass the state-of-the-art in recognizing nouns and modifiers from images. Interestingly, our method exhibits a language characteristic related to context sensitivity.},
  archive   = {C_ICRA},
  author    = {Mohamadreza Faridghasemnia and Jennifer Renoux and Alessandro Saffiotti},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611332},
  month     = {5},
  pages     = {11178-11185},
  title     = {Visual noun modifiers: The problem of binding visual and linguistic cues*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Diffusion-based point cloud super-resolution for mmWave
radar data. <em>ICRA</em>, 11171–11177. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The millimeter-wave radar sensor maintains stable performance under adverse environmental conditions, making it a promising solution for all-weather perception tasks, such as outdoor mobile robotics. However, the radar point clouds are relatively sparse and contain massive ghost points, which greatly limits the development of mmWave radar technology. In this paper, we propose a novel point cloud super-resolution approach for 3D mmWave radar data, named Radar-diffusion. Our approach employs the diffusion model defined by mean-reverting stochastic differential equations (SDE). Using our proposed new objective function with supervision from corresponding LiDAR point clouds, our approach efficiently handles radar ghost points and enhances the sparse mmWave radar point clouds to dense LiDAR-like point clouds. We evaluate our approach on two different datasets, and the experimental results show that our method outperforms the state-of-the-art baseline methods in 3D radar super-resolution tasks. Furthermore, we demonstrate that our enhanced radar point cloud is capable of downstream radar point-based registration tasks.},
  archive   = {C_ICRA},
  author    = {Kai Luan and Chenghao Shi and Neng Wang and Yuwei Cheng and Huimin Lu and Xieyuanli Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611026},
  month     = {5},
  pages     = {11171-11177},
  title     = {Diffusion-based point cloud super-resolution for mmWave radar data},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FBPT: A fully binary point transformer. <em>ICRA</em>,
11163–11170. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel Fully Binary Point Cloud Transformer (FBPT) model which has the potential to be widely applied and expanded in the fields of robotics and mobile devices. By compressing the weights and activations of a 32-bit full-precision network to 1-bit binary values, the proposed binary point cloud Transformer network significantly reduces the storage footprint and computational resource requirements of neural network models for point cloud processing tasks, compared to full-precision point cloud networks. However, achieving a fully binary point cloud Transformer network, where all parts except the modules specific to the task are binary, poses challenges and bottlenecks in quantizing the activations of Q, K, V and self-attention in the attention module, as they do not adhere to simple probability distributions and can vary with input data. Furthermore, in our network, the binary attention module undergoes a degradation of the self-attention module due to the uniform distribution that occurs after the softmax operation. The primary focus of this paper is on addressing the performance degradation issue caused by the use of binary point cloud Transformer modules. We propose a novel binarization mechanism called dynamic-static hybridization. Specifically, our approach combines static binarization of the overall network model with fine granularity dynamic binarization of data-sensitive components. Furthermore, we make use of a novel hierarchical training scheme to obtain the optimal model and binarization parameters. These above improvements allow the proposed binarization method to outperform binarization methods applied to convolution neural networks when used in point cloud Transformer structures. To demonstrate the superiority of our algorithm, we conducted experiments on two different tasks: point cloud classification and place recognition. In point cloud classification, our model achieved an accuracy of 90.9%, which is only a 2.3% decrease compared to the full precision network. For the place recognition task, we achieved 91.02% in the top @1% metric and 82.87% in the top @1% metric on the Oxford RobotCar dataset in terms of the average recall rate. Moreover, our model exhibits a significant reduction of over 80% in terms of model size and FLOPs (floating-point operations) compared to the baseline.},
  archive   = {C_ICRA},
  author    = {Zhixing Hou and Yuzhang Shang and Yan Yan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611616},
  month     = {5},
  pages     = {11163-11170},
  title     = {FBPT: A fully binary point transformer},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CrackNex: A few-shot low-light crack segmentation model
based on retinex theory for UAV inspections. <em>ICRA</em>, 11155–11162.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Routine visual inspections of concrete structures are imperative for upholding the safety and integrity of critical infrastructure. Such visual inspections sometimes happen under low-light conditions, e.g., checking for bridge health. Crack segmentation under such conditions is challenging due to the poor contrast between cracks and their surroundings. However, most deep learning methods are designed for well-illuminated crack images and hence their performance drops dramatically in low-light scenes. In addition, conventional approaches require many annotated low-light crack images which is time-consuming. In this paper, we address these challenges by proposing CrackNex, a framework that utilizes reflectance information based on Retinex Theory to learn a unified illumination-invariant representation. Furthermore, we utilize few-shot segmentation to solve the inefficient training data problem. In CrackNex, both a support prototype and a reflectance prototype are extracted from the support set. Then, a prototype fusion module is designed to integrate the features from both prototypes. CrackNex outperforms the SOTA methods on multiple datasets. Additionally, we present the first benchmark dataset, LCSD, for low-light crack segmentation. LCSD consists of 102 well-illuminated crack images and 41 low-light crack images. The dataset and code are available at https://github.com/zy1296/CrackNex.},
  archive   = {C_ICRA},
  author    = {Zhen Yao and Jiawei Xu and Shuhang Hou and Mooi Choo Chuah},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611660},
  month     = {5},
  pages     = {11155-11162},
  title     = {CrackNex: A few-shot low-light crack segmentation model based on retinex theory for UAV inspections},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transformer-CNN cohort: Semi-supervised semantic
segmentation by the best of both students. <em>ICRA</em>, 11147–11154.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The popular methods for semi-supervised semantic segmentation mostly adopt a unitary network model using convolutional neural networks (CNNs) and enforce consistency of the model’s predictions over perturbations applied to the inputs or model. However, such a learning paradigm suffers from two critical limitations: a) learning the discriminative features for the unlabeled data; b) learning both global and local information from the whole image. In this paper, we propose a novel Semi-supervised Learning (SSL) approach, called Transformer-CNN Cohort (TCC), that consists of two students with one based on the vision transformer (ViT) and the other based on the CNN. Our method subtly incorporates the multi-level consistency regularization on the predictions and the heterogeneous feature spaces via pseudo-labeling for the unlabeled data. First, as the inputs of the ViT student are image patches, the feature maps extracted encode crucial class-wise statistics. To this end, we propose class-aware feature consistency distillation (CFCD) that first leverages the outputs of each student as the pseudo labels and generates class-aware feature (CF) maps for knowledge transfer between the two students. Second, as the ViT student has more uniform representations for all layers, we propose consistency-aware cross distillation (CCD) to transfer knowledge between the pixel-wise predictions from the cohort. We validate the TCC framework on Cityscapes and Pascal VOC 2012 datasets, which outperforms existing SSL methods by a large margin. Project page: https://vlislab22.github.io/TCC/.},
  archive   = {C_ICRA},
  author    = {Xu Zheng and Yunhao Luo and Chong Fu and Kangcheng Liu and Lin Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611036},
  month     = {5},
  pages     = {11147-11154},
  title     = {Transformer-CNN cohort: Semi-supervised semantic segmentation by the best of both students},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ATPPNet: Attention based temporal point cloud prediction
network. <em>ICRA</em>, 11140–11146. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Point cloud prediction is an important yet challenging task in the field of autonomous driving. The goal is to predict future point cloud sequences that maintain object structures while accurately representing their temporal motion. These predicted point clouds help in other subsequent tasks like object trajectory estimation for collision avoidance or estimating locations with the least odometry drift. In this work, we present ATPPNet, a novel architecture that predicts future point cloud sequences given a sequence of previous time step point clouds obtained with LiDAR sensor. ATPPNet leverages Conv-LSTM along with channel-wise and spatial attention dually complemented by a 3D-CNN branch for extracting an enhanced spatio-temporal context to recover high quality fidel predictions of future point clouds. We conduct extensive experiments on publicly available datasets and report impressive performance outperforming the existing methods. We also conduct a thorough ablative study of the proposed architecture and provide an application study that highlights the potential of our model for tasks like odometry estimation.},
  archive   = {C_ICRA},
  author    = {Kaustab Pal and Aditya Sharma and Avinash Sharma and K. Madhava Krishna},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610475},
  month     = {5},
  pages     = {11140-11146},
  title     = {ATPPNet: Attention based temporal point cloud prediction network},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AGRNav: Efficient and energy-saving autonomous navigation
for air-ground robots in occlusion-prone environments. <em>ICRA</em>,
11133–11139. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The exceptional mobility and long endurance of air-ground robots are raising interest in their usage to navigate complex environments (e.g., forests and large buildings). However, such environments often contain occluded and unknown regions, and without accurate prediction of unobserved obstacles, the movement of the air-ground robot often suffers a sub-optimal trajectory under existing mapping-based and learning-based navigation methods. In this work, we present AGRNav, a novel framework designed to search for safe and energy-saving air-ground hybrid paths. AGRNav contains a lightweight semantic scene completion network (SCONet) with self-attention to enable accurate obstacle predictions by capturing contextual information and occlusion area features. The framework subsequently employs a query-based method for low-latency updates of prediction results to the grid map. Finally, based on the updated map, the hierarchical path planner efficiently searches for energy-saving paths for navigation. We validate AGRNav’s performance through benchmarks in both simulated and real-world environments, demonstrating its superiority over classical and state-of-the-art methods. The open-source code is available at https://github.com/jmwang0117/AGRNav.},
  archive   = {C_ICRA},
  author    = {Junming Wang and Zekai Sun and Xiuxian Guan and Tianxiang Shen and Zongyuan Zhang and Tianyang Duan and Dong Huang and Shixiong Zhao and Heming Cui},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610829},
  month     = {5},
  pages     = {11133-11139},
  title     = {AGRNav: Efficient and energy-saving autonomous navigation for air-ground robots in occlusion-prone environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BroadBEV: Collaborative LiDAR-camera fusion for
broad-sighted bird’s eye view map construction. <em>ICRA</em>,
11125–11132. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A recent sensor fusion in a Bird’s Eye View (BEV) space has shown its utility in various tasks such as 3D detection, map segmentation, etc. However, the approach struggles with inaccurate camera BEV estimation, and a perception of distant areas due to the sparsity of LiDAR points. In this paper, we propose a BEV fusion (BroadBEV) that aims to enhance camera BEV estimation for broad perception in the pre-defined BEV range, while simultaneously improving the completion of LiDAR’s sparsity in the entire BEV space. Toward that end, we devise Point-scattering that scatters LiDAR BEV distribution to camera depth distribution. The method boosts the learning of depth estimation of the camera branch and induces accurate location of dense camera features in BEV space. For an effective BEV fusion between the spatially synchronized features, we suggest ColFusion that applies self-attention weights of LiDAR and camera BEV features to each other. Our extensive experiments demonstrate that the suggested methods enable a broad BEV perception with remarkable performance gains.},
  archive   = {C_ICRA},
  author    = {Minsu Kim and Giseop Kim and Kyong Hwan Jin and Sunwook Choi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610946},
  month     = {5},
  pages     = {11125-11132},
  title     = {BroadBEV: Collaborative LiDAR-camera fusion for broad-sighted bird’s eye view map construction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Collaborative dynamic 3D scene graphs for automated driving.
<em>ICRA</em>, 11118–11124. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Maps have played an indispensable role in enabling safe and automated driving. Although there have been many advances on different fronts ranging from SLAM to semantics, building an actionable hierarchical semantic representation of urban dynamic scenes and processing information from multiple agents are still challenging problems. In this work, we present Collaborative URBan Scene Graphs (CURB-SG) that enable higher-order reasoning and efficient querying for many functions of automated driving. CURB-SG leverages panoptic LiDAR data from multiple agents to build large-scale maps using an effective graph-based collaborative SLAM approach that detects inter-agent loop closures. To semantically decompose the obtained 3D map, we build a lane graph from the paths of ego agents and their panoptic observations of other vehicles. Based on the connectivity of the lane graph, we segregate the environment into intersecting and non-intersecting road areas. Subsequently, we construct a multi-layered scene graph that includes lane information, the position of static landmarks and their assignment to certain map sections, other vehicles observed by the ego agents, and the pose graph from SLAM including 3D panoptic point clouds. We extensively evaluate CURB-SG in urban scenarios using a photorealistic simulator. We release our code at http://curb.cs.uni-freiburg.de.},
  archive   = {C_ICRA},
  author    = {Elias Greve and Martin Büchner and Niclas Vödisch and Wolfram Burgard and Abhinav Valada},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610112},
  month     = {5},
  pages     = {11118-11124},
  title     = {Collaborative dynamic 3D scene graphs for automated driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Complementary random masking for RGB-thermal semantic
segmentation. <em>ICRA</em>, 11110–11117. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {RGB-thermal semantic segmentation is one potential solution to achieve reliable semantic scene understanding in adverse weather and lighting conditions. However, the previous studies mostly focus on designing a multi-modal fusion module without consideration of the nature of multi-modality inputs. Therefore, the networks easily become over-reliant on a single modality, making it difficult to learn complementary and meaningful representations for each modality. This paper proposes 1) a complementary random masking strategy of RGB-T images and 2) self-distillation loss between clean and masked input modalities. The proposed masking strategy prevents over-reliance on a single modality. It also improves the accuracy and robustness of the neural network by forcing the network to segment and classify objects even when one modality is partially available. Also, the proposed self-distillation loss encourages the network to extract complementary and meaningful representations from a single modality or complementary masked modalities. We achieve state-of-the-art performance over three RGB-T semantic segmentation benchmarks. Our source code is available at https://github.com/UkcheolShin/CRM_RGBTSeg.},
  archive   = {C_ICRA},
  author    = {Ukcheol Shin and Kyunghyun Lee and In So Kweon and Jean Oh},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611200},
  month     = {5},
  pages     = {11110-11117},
  title     = {Complementary random masking for RGB-thermal semantic segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Complementing onboard sensors with satellite maps: A new
perspective for HD map construction. <em>ICRA</em>, 11103–11109. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {High-definition (HD) maps play a crucial role in autonomous driving systems. Recent methods have attempted to construct HD maps in real-time using vehicle onboard sensors. Due to the inherent limitations of onboard sensors, which include sensitivity to detection range and susceptibility to occlusion by nearby vehicles, the performance of these methods significantly declines in complex scenarios and long-range detection tasks. In this paper, we explore a new perspective that boosts HD map construction through the use of satellite maps to complement onboard sensors. We initially generate the satellite map tiles for each sample in nuScenes and release a complementary dataset for further research. To enable better integration of satellite maps with existing methods, we propose a hierarchical fusion module, which includes feature-level fusion and BEV-level fusion. The feature-level fusion, composed of a mask generator and a masked cross-attention mechanism, is used to refine the features from onboard sensors. The BEV-level fusion mitigates the coordinate differences between features obtained from onboard sensors and satellite maps through an alignment module. The experimental results on the augmented nuScenes showcase the seamless integration of our module into three existing HD map construction methods. The satellite maps and our proposed module notably enhance their performance in both HD map semantic segmentation and instance detection tasks. Our code will be available at https://github.com/xjtu-csgao/SatforHDMap.},
  archive   = {C_ICRA},
  author    = {Wenjie Gao and Jiawei Fu and Yanqing Shen and Haodong Jing and Shitao Chen and Nanning Zheng},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611611},
  month     = {5},
  pages     = {11103-11109},
  title     = {Complementing onboard sensors with satellite maps: A new perspective for HD map construction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the overconfidence problem in semantic 3D mapping.
<em>ICRA</em>, 11095–11102. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Semantic 3D mapping, the process of fusing depth and image segmentation information between multiple views to build 3D maps annotated with object classes in real-time, is a recent topic of interest. This paper highlights the fusion overconfidence problem, in which conventional mapping methods assign high confidence to the entire map even when they are incorrect, leading to miscalibrated outputs. Several methods to improve uncertainty calibration at different stages in the fusion pipeline are presented and compared on the ScanNet dataset. We show that the most widely used Bayesian fusion strategy is among the worst calibrated, and propose a learned pipeline that combines fusion and calibration, GLFS, which achieves simultaneously higher accuracy and 3D map calibration while retaining real-time capability and adding only 525 learned parameters to the pipeline. We further illustrate the importance of map calibration on a downstream task by showing that incorporating proper semantic fusion to an indoor object search agent improves its success rates.},
  archive   = {C_ICRA},
  author    = {Joao Marcos Correia Marques and Albert J. Zhai and Shenlong Wang and Kris Hauser},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611306},
  month     = {5},
  pages     = {11095-11102},
  title     = {On the overconfidence problem in semantic 3D mapping},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Noisy few-shot 3D point cloud scene segmentation.
<em>ICRA</em>, 11070–11077. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D scene semantic segmentation plays a crucial role in robotics by enabling robots to understand and interpret their environment in a detailed and context-aware manner, facilitating tasks such as navigation, object manipulation, and interaction within complex spaces. A preponderance of methodology predominantly adopts a fully supervised framework for 3D point cloud scene semantic segmentation. Such paradigms exhibit an intrinsic dependency on extensive labeled datasets, presenting challenges in acquisition and exhibiting incapacity to segment novel classes, especially when the training data are contaminated by noisy samples. To address these limitations, this study introduces a novel few-shot segmentation approach to robustly segment 3D point cloud scenes with noisy labels using a meta-learning scheme. Specifically, we first build a multi-prototype graph and then suppress samples with noisy labels based on the graph structure. A subgraph bagging scheme is then proposed to conduct semi-supervised transductive learning to propagate labels. To optimize the graph structure to learn discriminative prototype features, we design a triplet contrastive loss to increase the compactness of these subgraphs. We evaluated our method on two widely used 3D point cloud scene segmentation benchmarks within few-shot (i.e., 2/3-way 5-shot) segmentation settings with noisy samples. Experimental results demonstrate the improvement of our method over the compared baselines, illustrating the robustness of our method in few-shot 3D scene segmentation against noisy samples. The code is available at: https://github.com/hhuang-code/Noisy_Fewshot_Segmentation.},
  archive   = {C_ICRA},
  author    = {Hao Huang and Shuaihang Yuan and CongCong Wen and Yu Hao and Yi Fang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611583},
  month     = {5},
  pages     = {11070-11077},
  title     = {Noisy few-shot 3D point cloud scene segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SG-RoadSeg: End-to-end collision-free space detection
sharing encoder representations jointly learned via unsupervised deep
stereo. <em>ICRA</em>, 11063–11069. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collision-free space detection is of utmost importance for autonomous robot perception and navigation. State-of-the-art (SoTA) approaches generally extract features from RGB images and an additional source or modality of 3-D information, such as depth or disparity images, using a pair of independent encoders. The extracted features are subsequently fused and decoded to yield semantic predictions of collision-free spaces. Such feature-fusion approaches become infeasible in scenarios, where the sensor for 3-D information acquisition is unavailable, or just when multi-sensor calibration falls short of the necessary precision. To overcome these limitations, this paper introduces a novel end-to-end collision-free space detection network, referred to as SG-RoadSeg, built upon our previous work SNE-RoadSeg. A key contribution of this paper is a strategy for sharing encoder representations that are co-learned through both semantic segmentation and unsupervised stereo matching tasks, enabling the features extracted from RGB images to contain both semantic and spatial geometric information. The unsupervised deep stereo serves as an auxiliary functionality, capable of generating accurate disparity maps that can be used by other perception tasks that require depth-related data. Comprehensive experimental results on the KITTI road and semantics datasets validate the effectiveness of our proposed architecture and encoder representation sharing strategy. SG-RoadSeg also demonstrates superior performance than other SoTA collision-free space detection approaches. Our source code, demo video, and supplement are publicly available at mias.group/SG-RoadSeg.},
  archive   = {C_ICRA},
  author    = {Zhiyuan Wu and Jiaqi Li and Yi Feng and Chengju Liu and Wei Ye and Qijun Chen and Rui Fan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611191},
  month     = {5},
  pages     = {11063-11069},
  title     = {SG-RoadSeg: End-to-end collision-free space detection sharing encoder representations jointly learned via unsupervised deep stereo},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Compliant robotic gripper with integrated ripeness sensing
for blackberry harvesting. <em>ICRA</em>, 11056–11062. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Global blackberry demand has been surging due to their antioxidant and nutritional value in a traditional diet. However, blackberries have extreme fragility (resulting in up to 85% of harvest batches sustaining damage) and near-ripe and ripe blackberries are difficult to distinguish in normal lighting conditions. These challenges in maintaining the blackberry supply motivate the development of an autonomous robotic solution to harvest fully ripe blackberries with minimal damage. The present paper details the mechanical design, methodology, analysis, and experimental results of a compliant robotic gripper created for this purpose. The gripper has a compact form factor and retractable fingers with specialized TPU finger pads for gentle picks, a near-infrared (NIR) reflectance-based probe for detecting full ripeness and a standardized harvesting sequence for effectively picking berries. In an outdoor harvesting experiment, the gripper attempted picking 26 berries without ripeness sensing, with 65.4% (17) being successfully picked and 38.5% (10) sustaining damage. The movements of the robot arm in the harvesting sequence were accordingly adjusted and finalized for following in-lab experiments, in which the gripper was also outfitted with ripeness sensing. Out of 40 berries, 62.5% (25) were successfully picked, with 0% of them sustaining damage. The ripeness probe classified 56 ripe and 11 near-ripe berries, with 89% (50) of the ripe and 64% (7) of the near-ripe berries being correctly classified. In a second in-lab experiment, 16 of 20 berries were successfully picked, with 2 sustaining damage.},
  archive   = {C_ICRA},
  author    = {Arvyn De and Divyam Kumar and Ian Kwuan and Alex Qiu and Ai-Ping Hu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610991},
  month     = {5},
  pages     = {11056-11062},
  title     = {Compliant robotic gripper with integrated ripeness sensing for blackberry harvesting},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Bio-inspired pupal-mode actuator with ultra-crossing
capability for soft robots. <em>ICRA</em>, 11034–11040. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot-assisted Natural Orifice Translu-minal Endoscopic Surgery (NOTES) represents a paradigm shift in surgical practice, significantly mini-mizing patient morbidity. However, the variability of inner diameter and the inter-luminal crossing within the luminal tracts lead to challenge for effective robotic intervention. Inspired by the motion of the chrysalis during its transformation, we designed an innovative pupal-mode actuator for NOTES robots. Through the manipulation of its internal air chambers, this actuator is capable of replicating wriggle-like movements. Through experimental analysis, we have acquired the constitutive characteristics of this actuator. Subsequently, an innovative gastric endoscopy robot is developed base the actuator and tested in a phantom. The results of the task simulations substantiate that the pupal-mode actuator has the capability to reduce resistance and enhance the safety of the endoscopic intervention.},
  archive   = {C_ICRA},
  author    = {Zhenxing Wang and Xiao He and Yuhang Zhang and Cheng Zhang and Lei Sun and Zhidong Wang and Shun Xu and Hao Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610294},
  month     = {5},
  pages     = {11034-11040},
  title     = {Bio-inspired pupal-mode actuator with ultra-crossing capability for soft robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and validation of slender extensible continuum robot
for solar wing re-unfolding in aerospace. <em>ICRA</em>, 11027–11033.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The solar array wing deployment of orbiting satellites cannot be performed due to power failure of the connector caused by uncertain loads such as high temperature or vibration in the launching process of the spacecraft. There is currently a lack of suitable unlocking solutions for solar wing re-unfolding. This paper proposes a solution in which an extensible continuum robot (ECR) carrying the unlocking device enters the gap between the satellite and the solar wing, re-unlocking the solar wing. This solution effectively leverages the advantages of ECR collision buffering and adaptable maneuverability within confined space. In response to the proposed solution, the designed ECR with two segments helical spring structure features scalability, hollowness, lightweight, and a big length-diameter ratio. To perform the critical unlocking task, an end effector with the function of loosening and unplugging the aerospace connector for communication is designed based on the drive device away from itself to reduce the inertia of the manipulator. The information from the cameras and force sensors is used to estimate the extent of task execution. We establish an experimental setup to simulate the process of unlocking. The results validate that the ECR successfully accesses the gap (65mm) and accomplishes the unlocking task. The ECR has great application potential for on-orbit service.},
  archive   = {C_ICRA},
  author    = {Pengyuan Wang and Zheng Zheng and Jiazhen Sun and Yuqiang Liu and Zongbo He and Zhiguang Xing and Jianwen Zhao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610471},
  month     = {5},
  pages     = {11027-11033},
  title     = {Design and validation of slender extensible continuum robot for solar wing re-unfolding in aerospace},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph-based 3D collision-distance estimation network with
probabilistic graph rewiring. <em>ICRA</em>, 10939–10945. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We aim to solve the problem of data-driven collision-distance estimation given 3-dimensional (3D) geometries. Conventional algorithms suffer from low accuracy due to their reliance on limited representations, such as point clouds. In contrast, our previous graph-based model, GraphDistNet, achieves high accuracy using edge information but incurs higher message-passing costs with growing graph size, limiting its applicability to 3D geometries. To overcome these challenges, we propose GDN-R, a novel 3D graph-based estimation network. GDN-R employs a layer-wise probabilistic graph-rewiring algorithm leveraging the differentiable Gumbel-top-K relaxation. Our method accurately infers minimum distances through iterative graph rewiring and updating relevant embeddings. The probabilistic rewiring enables fast and robust embedding with respect to unforeseen categories of geometries. Through 41, 412 random benchmark tasks with 150 pairs of 3D objects, we show GDN-R outperforms state-of-the-art baseline methods in terms of accuracy and generalizability. We also show that the proposed rewiring improves the update performance reducing the size of the estimation model. We finally show its batch prediction and auto-differentiation capabilities for trajectory optimization in both simulated and real-world scenarios.},
  archive   = {C_ICRA},
  author    = {Minjae Song and Yeseung Kim and Min Jun Kim and Daehyung Park},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611465},
  month     = {5},
  pages     = {10939-10945},
  title     = {Graph-based 3D collision-distance estimation network with probabilistic graph rewiring},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active collision-based navigation for wheeled robots.
<em>ICRA</em>, 10932–10938. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collision is typically avoided in robot navigation for safety guarantee. However, when a robot’s exteroceptive sensors fail, which means it becomes &quot;blind&quot;, collision can actually be leveraged to improve localization performance. Our research demonstrates the informative nature of collisions in this context. Moreover, we show that a robot is able to navigate in a known environment with only proprioceptive sensors by actively colliding with its surroundings for more reliable localization. Firstly, we design a collision-based observation model, which is differentiable and can be easily applied to various estimators. Secondly, we integrate this model into a collision-aided localization framework and implement it in two widely used estimators, the Kalman filter and the particle filter. Thirdly, we propose an active collision path planning method, which effectively reduces localization uncertainty.},
  archive   = {C_ICRA},
  author    = {Jingjing Li and Jialin Ji and Qianhao Wang and Huan Yu and Yu Pan and Fei Gao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610726},
  month     = {5},
  pages     = {10932-10938},
  title     = {Active collision-based navigation for wheeled robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gaussian process-based traversability analysis for terrain
mapless navigation. <em>ICRA</em>, 10925–10931. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Efficient navigation through uneven terrain remains a challenging endeavor for autonomous robots. We propose a new geometric-based uneven terrain mapless navigation framework combining a Sparse Gaussian Process (SGP) local map with a Rapidly-Exploring Random Tree* (RRT*) planner. Our approach begins with the generation of a high-resolution SGP local map, providing an interpolated representation of the robot’s immediate environment. This map captures crucial environmental variations, including height, uncertainties, and slope characteristics. Subsequently, we construct a traversability map based on the SGP representation to guide our planning process. The RRT* planner efficiently generates real-time navigation paths, avoiding untraversable terrain in pursuit of the goal. This combination of SGP-based terrain interpretation and RRT* planning enables ground robots to safely navigate environments with varying elevations and steep obstacles. We evaluate the performance of our proposed approach through robust simulation testing, highlighting its effectiveness in achieving safe and efficient navigation compared to existing methods. See the project GitHub 1 for source code and supplementary materials, including a video demonstrating experimental results.},
  archive   = {C_ICRA},
  author    = {Abe Leininger and Mahmoud Ali and Hassan Jardali and Lantao Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610106},
  month     = {5},
  pages     = {10925-10931},
  title     = {Gaussian process-based traversability analysis for terrain mapless navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MIM: Indoor and outdoor navigation in complex environments
using multi-layer intensity maps. <em>ICRA</em>, 10917–10924. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present MIM (Multi-Layer Intensity Map), a novel 3D object representation for robot perception and autonomous navigation. MIMs consist of multiple stacked layers of 2D grid maps each derived from reflected point cloud intensities corresponding to a certain height interval. The different layers of MIMs can be used to simultaneously estimate obstacles’ height, solidity/density, and opacity. We demonstrate that MIMs’ can help accurately differentiate obstacles that are safe to navigate through (e.g. beaded/string curtains, pliable tall grass), from ones that must be avoided (e.g. transparent surfaces such as glass walls, bushes, trees, etc.) in indoor and outdoor environments. Further, to handle narrow passages, and navigate through non-solid obstacles in dense environments, we propose an approach to adaptively inflate or enlarge the obstacles detected on MIMs based on their solidity, and the robot’s preferred velocity direction. We demonstrate these improved navigation capabilities in real-world narrow, dense environments using a real Turtlebot and Boston Dynamics Spot robots. We observe significant increases in success rates to more than 50%, up to a 9.5% decrease in normalized trajectory length, and up to a 22.6% increase in the F-score compared to current navigation methods using other sensor modalities.},
  archive   = {C_ICRA},
  author    = {Adarsh Jagan Sathyamoorthy and Kasun Weerakoon and Mohamed Elnoor and Mason Russell and Jason Pusey and Dinesh Manocha},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610673},
  month     = {5},
  pages     = {10917-10924},
  title     = {MIM: Indoor and outdoor navigation in complex environments using multi-layer intensity maps},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Preprocessing-based kinodynamic motion planning framework
for intercepting projectiles using a robot manipulator. <em>ICRA</em>,
10910–10916. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We are interested in studying sports with robots and starting with the problem of intercepting a projectile moving toward a robot manipulator equipped with a shield. To successfully perform this task, the robot needs to (i) detect the incoming projectile, (ii) predict the projectile’s future motion, (iii) plan a minimum-time rapid trajectory that can evade obstacles and intercept the projectile, and (iv) execute the planned trajectory. These four steps must be performed under the manipulator’s dynamic limits and extreme time constraints (≤ 350ms in our setting) to successfully intercept the projectile. In addition, we want these trajectories to be smooth to reduce the robot’s joint torques and the impulse on the platform on which it is mounted. To this end, we propose a kinodynamic motion planning framework that preprocesses smooth trajectories offline to allow real-time collision-free executions online. We present an end-to-end pipeline along with our planning framework, including perception, prediction, and execution modules. We evaluate our framework experimentally in simulation and show that it has a higher blocking success rate than the baselines. Further, we deploy our pipeline on a robotic system comprising an industrial arm (ABB IRB-1600) and an onboard stereo camera (ZED 2i), which achieves a 78% success rate in projectile interceptions.},
  archive   = {C_ICRA},
  author    = {Ramkumar Natarajan and Hanlan Yang and Qintong Xie and Yash Oza and Manash Pratim Das and Fahad Islam and Muhammad Suhail Saleem and Howie Choset and Maxim Likhachev},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611441},
  month     = {5},
  pages     = {10910-10916},
  title     = {Preprocessing-based kinodynamic motion planning framework for intercepting projectiles using a robot manipulator},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved M4M: Faster and richer planning for manipulation
among movable objects in cluttered 3D workspaces. <em>ICRA</em>,
10902–10909. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We are interested in enabling robots to solve difficult pick-and-place manipulation tasks in cluttered and constrained environments. If the robot does not have collision-free access to the object-of-interest (OoI) which it intends to grasp and extract from the workspace, it must reason about which movable objects to rearrange, where to move them, and how it may do so. In recent work [1] we introduced E-M4M, a graph search-based solver for solving such Manipulation tasks Among Movable Objects (MAMO). In this paper we make several improvements to E-M4M – we introduce the use of prehensile or pick-and-place rearrangement actions in addition to pushes; we show that by running it as a depth-first search improves performance; we show how the search can be run &quot;eagerly lazily&quot; to only simulate actions in a physics-based simulator when necessary; finally we relax the assumption that we require perfect knowledge of the physical properties of objects (mass and coefficient of friction in particular). The improved version of E-M4M presented in this paper, I-M4M, is a faster and more versatile MAMO solver with a rich action space. We discuss the impact of the improvements we make in an extensive simulation study and show previously unachievable results on a real-world PR2 robot.},
  archive   = {C_ICRA},
  author    = {Dhruv Mauria Saxena and Maxim Likhachev},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611234},
  month     = {5},
  pages     = {10902-10909},
  title     = {Improved M4M: Faster and richer planning for manipulation among movable objects in cluttered 3D workspaces},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Articulated object manipulation with coarse-to-fine
affordance for mitigating the effect of point cloud noise.
<em>ICRA</em>, 10895–10901. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D articulated objects are inherently challenging for manipulation due to the varied geometries and intricate functionalities associated with articulated objects. Point-level affordance, which predicts the per-point actionable score and thus proposes the best point to interact with, has demonstrated excellent performance and generalization capabilities in articulated object manipulation. However, a significant challenge remains: while previous works use perfect point cloud generated in simulation, the models cannot directly apply to the noisy point cloud in the real-world. To tackle this challenge, we leverage the property of real-world scanned point cloud that, the point cloud becomes less noisy when the camera is closer to the object. Therefore, we propose a novel coarse-to-fine affordance learning pipeline to mitigate the effect of point cloud noise in two stages. In the first stage, we learn the affordance on the noisy far point cloud which includes the whole object to propose the approximated place to manipulate. Then, we move the camera in front of the approximated place, scan a less noisy point cloud containing precise local geometries for manipulation, and learn affordance on such point cloud to propose fine-grained final actions. The proposed method is thoroughly evaluated both using large-scale simulated noisy point clouds mimicking real-world scans, and in the real world scenarios, with superiority over existing methods, demonstrating the effectiveness in tackling the noisy real-world point cloud problem.},
  archive   = {C_ICRA},
  author    = {Suhan Ling and Yian Wang and Ruihai Wu and Shiguang Wu and Yuzheng Zhuang and Tianyi Xu and Yu Li and Chang Liu and Hao Dong},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610593},
  month     = {5},
  pages     = {10895-10901},
  title     = {Articulated object manipulation with coarse-to-fine affordance for mitigating the effect of point cloud noise},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The grasp loop signature: A topological representation for
manipulation planning with ropes and cables. <em>ICRA</em>, 10888–10894.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies robotic manipulation of deformable, one-dimensional objects (DOOs) like ropes or cables, which has important potential applications in manufacturing, agriculture, and surgery. In such environments, the task may involve threading through or avoiding becoming tangled with other objects. Grasping with multiple grippers can create closed loops between the robot and DOO, and if an obstacle lies within this loop, it may be impossible to reach the goal. However, prior work has only considered the topology of the DOO in isolation, ignoring the arms that are manipulating it. Searching over possible grasps to accomplish the task without considering such topological information is very inefficient, as many grasps will not lead to progress on the task due to topological constraints. Therefore, we propose the ${{\mathcal{G}}_L} - {\text{signature}}$ which categorizes the topology of these grasp loops and show how it can be used to guide planning. We perform experiments in simulation on two DOO manipulation tasks to show that using the ${{\mathcal{G}}_L} - {\text{signature}}$ is faster and more successful than methods that rely on local geometry or additional finite-horizon planning. Finally, we demonstrate using the ${{\mathcal{G}}_L} - {\text{signature}}$ in a real-world dual-arm cable manipulation task.},
  archive   = {C_ICRA},
  author    = {Peter Mitrano and Dmitry Berenson},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611677},
  month     = {5},
  pages     = {10888-10894},
  title     = {The grasp loop signature: A topological representation for manipulation planning with ropes and cables},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unknown object retrieval in confined space through
reinforcement learning with tactile exploration. <em>ICRA</em>,
10881–10887. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The potential of tactile sensing for dexterous robotic manipulation has been demonstrated by its ability to enable nuanced real-world interactions. In this study, the retrieval of unknown objects from confined spaces, which is unsuitable for conventional visual perception and gripper-based manipulation, is identified and addressed. Specifically, a tactile-sensorized tool stick that well fits in the narrow space is utilized to provide multi-point contact sensing for object manipulation. A reinforcement learning (RL) agent with a hybrid action space is then proposed to acquire the optimal policy for manipulating the objects without prior knowledge of their physical properties. To accelerate on-hardware training, a focused training strategy is adopted with the hypothesis that an agent trained on a small set of representative shapes can be generalized to a wide range of everyday objects. Additionally, a curriculum on terminal goals is designed to further accelerate the hardware-based training process. Comparative experiments and ablation studies have been conducted to evaluate the effectiveness and robustness of the proposed approach, which highlights the high success rate of our solution for retrieving everyday objects.},
  archive   = {C_ICRA},
  author    = {Xinyuan Zhao and Wenyu Liang and Xiaoshi Zhang and Chee Meng Chew and Yan Wu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611541},
  month     = {5},
  pages     = {10881-10887},
  title     = {Unknown object retrieval in confined space through reinforcement learning with tactile exploration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ForceSight: Text-guided mobile manipulation with
visual-force goals. <em>ICRA</em>, 10874–10880. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present ForceSight, a system for text-guided mobile manipulation that predicts visual-force goals using a text-conditioned vision transformer. Given a single RGBD image and a text prompt, ForceSight determines a target end-effector pose in the camera frame (kinematic goal) and the associated forces (force goal). Together, these two components form a visual-force goal. Prior work has demonstrated that deep models outputting human-interpretable kinematic goals can enable dexterous manipulation by real robots. Forces are critical to manipulation, yet have typically been relegated to low-level execution in these systems. When deployed on a mobile manipulator equipped with an eye-in-hand RGBD camera, ForceSight performed tasks such as precision grasps, drawer opening, and object handovers with an 81% success rate in unseen environments with object instances that differed significantly from the training data. In a separate experiment, relying exclusively on visual servoing and ignoring force goals dropped the success rate from 90% to 45%, demonstrating that force goals can significantly enhance performance. The appendix, videos, code, and trained models are available at https://force-sight.github.io/.},
  archive   = {C_ICRA},
  author    = {Jeremy A. Collins and Cody Houff and You Liang Tan and Charles C. Kemp},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611210},
  month     = {5},
  pages     = {10874-10880},
  title     = {ForceSight: Text-guided mobile manipulation with visual-force goals},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ReorientDiff: Diffusion model based reorientation for object
manipulation. <em>ICRA</em>, 10867–10873. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to manipulate objects in desired configurations is a fundamental requirement for robots to complete various practical applications. While certain goals can be achieved by picking and placing the objects of interest directly, object reorientation is needed for precise placement in most of the tasks. In such scenarios, the object must be reoriented and re-positioned into intermediate poses that facilitate accurate placement at the target pose. To this end, we propose a reorientation planning method, ReorientDiff, that utilizes a diffusion model-based approach. The proposed method employs both visual inputs from the scene, and goal-specific language prompts to plan intermediate reorientation poses. Specifically, the scene and language-task information are mapped into a joint scene-task representation feature space, which is subsequently leveraged to condition the diffusion model. The diffusion model samples intermediate poses based on the representation using classifier-free guidance and then uses gradients of learned feasibility-score models for implicit iterative pose-refinement. The proposed method is evaluated using a set of YCB-objects and a suction gripper, demonstrating a success rate of 95.2% in simulation. Overall, we present a promising approach to address the reorientation challenge in manipulation by learning a conditional distribution, which is an effective way to move towards generalizable object manipulation. More results can be found on our website: https://utkarshmishra04.github.io/ReorientDiff.},
  archive   = {C_ICRA},
  author    = {Utkarsh A. Mishra and Yongxin Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610749},
  month     = {5},
  pages     = {10867-10873},
  title     = {ReorientDiff: Diffusion model based reorientation for object manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward optimal tabletop rearrangement with multiple
manipulation primitives. <em>ICRA</em>, 10860–10866. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In practice, many types of manipulation actions (e.g., pick-n-place and push) are needed to accomplish real-world manipulation tasks. Yet, limited research exists that explores the synergistic integration of different manipulation actions for optimally solving long-horizon task-and-motion planning problems. In this study, we propose and investigate planning high-quality action sequences for solving long-horizon tabletop rearrangement tasks in which multiple manipulation primitives are required. Denoting the problem rearrangement with multiple manipulation primitives (REMP), we develop two algorithms, hierarchical best-first search (HBFS) and parallel Monte Carlo tree search for multi-primitive rearrangement (PMMR) toward optimally resolving the challenge. Extensive simulation and real robot experiments demonstrate that both methods effectively tackle REMP, with HBFS excelling in planning speed and P M MR producing human-like, high-quality solutions with a nearly 100% success rate. Source code and supplementary materials will be available at https://github.com/arc-l/remp.},
  archive   = {C_ICRA},
  author    = {Baichuan Huang and Xujia Zhang and Jingjin Yu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610565},
  month     = {5},
  pages     = {10860-10866},
  title     = {Toward optimal tabletop rearrangement with multiple manipulation primitives},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-dimensional controller tuning through latent
representations. <em>ICRA</em>, 10853–10859. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a method to automatically and efficiently tune high-dimensional vectors of controller parameters. The proposed method first learns a mapping from the high-dimensional controller parameter space to a lower dimensional space using a machine learning-based algorithm. This mapping is then utilized in an actor-critic framework using Bayesian optimization (BO). The proposed approach is applicable to complex systems (such as quadruped robots). In addition, the proposed approach also enables efficient generalization to different control tasks while also reducing the number of evaluations required while tuning the controller parameters. We evaluate our method on a legged locomotion application. We show the efficacy of the algorithm in tuning the high-dimensional controller parameters and also reducing the number of evaluations required for the tuning. Moreover, it is shown that the method is successful in generalizing to new tasks and is also transferable to other robot dynamics.},
  archive   = {C_ICRA},
  author    = {Alireza Sarmadi and Prashanth Krishnamurthy and Farshad Khorrami},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610607},
  month     = {5},
  pages     = {10853-10859},
  title     = {High-dimensional controller tuning through latent representations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Grow your limits: Continuous improvement with real-world RL
for robotic locomotion. <em>ICRA</em>, 10829–10836. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep reinforcement learning can enable robots to autonomously acquire complex behaviors such as legged locomotion. However, RL in the real world is complicated by constraints on efficiency, safety, and overall training stability, which limits its practical applicability. We present APRL, a policy regularization framework that modulates the robot’s exploration throughout training, striking a balance between flexible improvement potential and focused, efficient exploration. APRL enables a quadrupedal robot to efficiently learn to walk entirely in the real world within minutes and continue to improve with more training where prior work saturates in performance. We demonstrate that continued training with APRL results in a policy that is substantially more capable of navigating challenging situations and adapts to changes in dynamics. Videos and code to reproduce our results are available at: https://sites.google.com/berkeley.edu/aprl},
  archive   = {C_ICRA},
  author    = {Laura Smith and Yunhao Cao and Sergey Levine},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610485},
  month     = {5},
  pages     = {10829-10836},
  title     = {Grow your limits: Continuous improvement with real-world RL for robotic locomotion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HyperPPO: A scalable method for finding small policies for
robotic control. <em>ICRA</em>, 10821–10828. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Models with fewer parameters are necessary for the neural control of memory-limited, performant robots. Finding these smaller neural network architectures can be time-consuming. We propose HyperPPO, an on-policy reinforcement learning algorithm that utilizes graph hypernetworks to estimate the weights of multiple neural architectures simultaneously. Our method estimates weights for networks that are much smaller than those in common-use networks yet encode highly performant policies. We obtain multiple trained policies at the same time while maintaining sample efficiency and provide the user the choice of picking a network architecture that satisfies their computational constraints. We show that our method scales well - more training resources produce faster convergence to higher-performing architectures. We demonstrate that the neural policies estimated by HyperPPO are capable of decentralized control of a Crazyflie2.1 quadrotor. Website: https://sites.google.com/usc.edu/hyperppo},
  archive   = {C_ICRA},
  author    = {Shashank Hegde and Zhehui Huang and Gaurav S. Sukhatme},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610861},
  month     = {5},
  pages     = {10821-10828},
  title     = {HyperPPO: A scalable method for finding small policies for robotic control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AdaptAUG: Adaptive data augmentation framework for
multi-agent reinforcement learning. <em>ICRA</em>, 10814–10820. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent reinforcement learning has emerged as a promising approach for the control of multi-robot systems. Nevertheless, the low sample efficiency of MARL poses a significant obstacle to its broader application in robotics. While data augmentation appears to be a straightforward solution for improving sample efficiency, it usually incurs training instability, making the sample efficiency worse. Moreover, manually choosing suitable augmentations for a variety of tasks is a tedious and time-consuming process. To mitigate these challenges, our research theoretically analyzes the implications of data augmentation on MARL algorithms. Guided by these insights, we present AdaptAUG, an adaptive framework designed to selectively identify beneficial data augmentations, thereby achieving superior sample efficiency and overall performance in multi-robot tasks. Extensive experiments in both simulated and real-world multi-robot scenarios validate the effectiveness of our proposed framework.},
  archive   = {C_ICRA},
  author    = {Xin Yu and Yongkai Tian and Li Wang and Pu Feng and Wenjun Wu and Rongye Shi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611035},
  month     = {5},
  pages     = {10814-10820},
  title     = {AdaptAUG: Adaptive data augmentation framework for multi-agent reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Barrier functions inspired reward shaping for reinforcement
learning. <em>ICRA</em>, 10807–10813. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement Learning (RL) has progressed from simple control tasks to complex real-world challenges with large state spaces. While RL excels in these tasks, training time remains a limitation. Reward shaping is a popular solution, but existing methods often rely on value functions, which face scalability issues. This paper presents a novel safety-oriented reward-shaping framework inspired by barrier functions, offering simplicity and ease of implementation across various environments and tasks. To evaluate the effectiveness of the proposed reward formulations, we conduct simulation experiments on CartPole, Ant, and Humanoid environments, along with real-world deployment on the Unitree Go1 quadruped robot. Our results demonstrate that our method leads to 1.4-2.8 times faster convergence and as low as 50-60% actuation effort compared to the vanilla reward. In a sim-to-real experiment with the Go1 robot, we demonstrated better control and dynamics of the bot with our reward framework. We have open-sourced our code at https://github.com/Safe-RL-IISc/barrier_shaping.},
  archive   = {C_ICRA},
  author    = {Nilaksh and Abhishek Ranjan and Shreenabh Agrawal and Aayush Jain and Pushpak Jagtap and Shishir Kolathaya},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610391},
  month     = {5},
  pages     = {10807-10813},
  title     = {Barrier functions inspired reward shaping for reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). COMPOSER: Scalable and robust modular policies for snake
robots. <em>ICRA</em>, 10800–10806. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Snake robots have showcased remarkable compliance and adaptability in their interaction with environments, mirroring the traits of their natural counterparts. While their hyper-redundant and high-dimensional characteristics add to this adaptability, they also pose great challenges to robot control. Instead of perceiving the hyper-redundancy and flexibility of snake robots as mere challenges, there lies an unexplored potential in leveraging these traits to enhance robustness and generalizability at the control policy level. We seek to develop a control policy that effectively breaks down the high dimensionality of snake robots while harnessing their redundancy. In this work, we consider the snake robot as a modular robot and formulate the control of the snake robot as a cooperative Multi-Agent Reinforcement Learning (MARL) problem. Each segment of the snake robot functions as an individual agent. Specifically, we incorporate a self-attention mechanism to enhance the cooperative behavior between agents. A high-level imagination policy is proposed to provide additional rewards to guide the low-level control policy. We validate the proposed method COMPOSER with five snake robot tasks, including goal reaching, wall climbing, shape formation, tube crossing, and block pushing. COMPOSER achieves the highest success rate across all tasks when compared to a centralized baseline and four modular policy baselines. Additionally, we show enhanced robustness against module corruption and significantly superior zero-shot generalizability in our proposed method. The videos of this work are available on our project page: https://sites.google.com/view/composer-snake/.},
  archive   = {C_ICRA},
  author    = {Yuyou Zhang and Yaru Niu and Xingyu Liu and Ding Zhao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611040},
  month     = {5},
  pages     = {10800-10806},
  title     = {COMPOSER: Scalable and robust modular policies for snake robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weighting online decision transformer with episodic memory
for offline-to-online reinforcement learning. <em>ICRA</em>,
10793–10799. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Offline reinforcement learning (RL) has been shown to be successfully modeled as a sequence modeling problem, drawing inspiration from the success of Transformers. Offline RL is often limited by the quality of the offline dataset, so offline-to-online RL is a more realistic setting. Online decision transformer (ODT) is an effective and representative sequence modeling-based offline-to-online RL method. Despite its effectiveness, ODT still suffers from the sample inefficiency problem during the online fine-tuning phase. This sample inefficiency problem arises because the agent treats all state-action pairs in the replay buffer equally when trying to learn from the replay buffer. In this paper, we propose a simple yet effective method, called weighting online decision transformer with episodic memory (WODTEM), to improve sample efficiency. We first attempt to introduce an episodic memory (EM) mechanism into the sequence modeling-based RL methods. By utilizing the EM mechanism, we propose a novel training objective with a weighting function, based on ODT, to improve sample efficiency. Experimental results on multiple tasks show that WODTEM can improve sample efficiency.},
  archive   = {C_ICRA},
  author    = {Xiao Ma and Wu-Jun Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610701},
  month     = {5},
  pages     = {10793-10799},
  title     = {Weighting online decision transformer with episodic memory for offline-to-online reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CVAE-SM: A conditional variational autoencoder with style
modulation for efficient uncertainty quantification. <em>ICRA</em>,
10786–10792. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep learning has brought transformative advancements to object segmentation, especially in marine robotics contexts such as waste management and subaquatic infrastructure oversight. However, a central challenge persists: calibrating the prediction confidence of the model to ensure robust and reliable outcomes, especially within the demanding underwater environment. Existing solutions for estimating uncertainty are often computationally intensive and have largely centered around Bayesian neural networks or ensemble methods. In this paper, we present a Conditional Variational Autoencoder-based framework (CVAE-SM), which is capable of generating diverse latent codes for improved uncertainty quantification in image segmentation. Our method, enhanced by a style modulator, merges content features, and latent codes more effectively, leading to refined prediction of uncertainty levels. We further introduce a dataset of perturbed underwater images to benchmark uncertainty quantification in this domain. The proposed model not only surpasses peers in segmentation metrics but also matches ensemble models in uncertainty predictions, all while being 2.5 times faster.},
  archive   = {C_ICRA},
  author    = {Amin Ullah and Taiqing Yan and Li Fuxin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611160},
  month     = {5},
  pages     = {10786-10792},
  title     = {CVAE-SM: A conditional variational autoencoder with style modulation for efficient uncertainty quantification},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Procedure recognition by knowledge-driven segmentation in
robotic-assisted vitreoretinal surgery. <em>ICRA</em>, 10779–10785. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Internal limiting membrane (ILM) peeling is a vital vitreoretinal surgery procedure. However, due to the thickness of just 1-2 micrometers and the intricacies associated with its varying density and adhesion, the difficulty of manipulation exceeds the physiological limits of human perception and operation. Surgical robot is characterized by high precision and stability. However, navigating intricate intraocular environments and handling minuscule high-precision areas remain enormous challenges. These include issues of uneven lighting, field-of-view loss, and motion blur. This paper proposed a perception method named ‘Multimodal Surgical Process Recognition based on Domain Knowledge and Segmentation (MSPR-DKS),’ designed to address these challenges and provide input for the precise control of robots. Moreover, a comprehensive dataset focused on ILM peeling during macular hole surgeries was established. Experimental results underscore the efficacy of this approach, with segmentation accuracies exceeding 99.27% for instruments and macular holes and an average accuracy of 98.97% in recognizing surgical processes. This study paves the way for leveraging domain knowledge and image segmentation to improve robot-assisted manipulation of soft tissues in ophthalmology.},
  archive   = {C_ICRA},
  author    = {Zhen Li and Yawen Deng and Qiang Ye and Weihong Yu and Haoxiang Qi and Yaliang Liu and Zhangguo Yu and Gui-Bin Bian},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610315},
  month     = {5},
  pages     = {10779-10785},
  title     = {Procedure recognition by knowledge-driven segmentation in robotic-assisted vitreoretinal surgery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LLM-assisted multi-teacher continual learning for visual
question answering in robotic surgery. <em>ICRA</em>, 10772–10778. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual question answering (VQA) can be fundamentally crucial for promoting robotic-assisted surgical education. In practice, the needs of trainees are constantly evolving, such as learning more surgical types and adapting to new surgical instruments/techniques. Therefore, continually updating the VQA system by a sequential data stream from multiple resources is demanded in robotic surgery to address new tasks. In surgical scenarios, the privacy issue of patient data often restricts the availability of old data when updating the model, necessitating an exemplar-free continual learning (CL) setup. However, prior studies overlooked two vital problems of the surgical domain: i) large domain shifts from diverse surgical operations collected from multiple departments or clinical centers, and ii) severe data imbalance arising from the uneven presence of surgical instruments or activities during surgical procedures. This paper proposes to address these two problems with a multimodal large language model (LLM) and an adaptive weight assignment methodology. We first develop a new multi-teacher CL framework that leverages a multimodal LLM as the additional teacher. The strong generalization ability of the LLM can bridge the knowledge gap when domain shifts and data imbalances occur. We then put forth a novel data processing method that transforms complex LLM embeddings into logits compatible with our CL framework. We also design an adaptive weight assignment approach that balances the generalization ability of the LLM and the domain expertise of the old CL model. Finally, we construct a new dataset for surgical VQA tasks. Extensive experimental results demonstrate the superiority of our method to other advanced CL models.},
  archive   = {C_ICRA},
  author    = {Kexin Chen and Yuyang Du and Tao You and Mobarakol Islam and Ziyu Guo and Yueming Jin and Guangyong Chen and Pheng-Ann Heng},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610603},
  month     = {5},
  pages     = {10772-10778},
  title     = {LLM-assisted multi-teacher continual learning for visual question answering in robotic surgery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RIDE: Self-supervised learning of rotation-equivariant
keypoint detection and invariant description for endoscopy.
<em>ICRA</em>, 10764–10771. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unlike in natural images, in endoscopy there is no clear notion of an up-right camera orientation. Endoscopic videos therefore often contain large rotational motions, which require keypoint detection and description algorithms to be robust to these conditions. While most classical methods achieve rotation-equivariant detection and invariant description by design, many learning-based approaches learn to be robust only up to a certain degree. At the same time learning-based methods under moderate rotations often outperform classical approaches. In order to address this shortcoming, in this paper we propose RIDE, a learning-based method for rotation-equivariant detection and invariant description. Following recent advancements in group-equivariant learning, RIDE models rotation-equivariance implicitly within its architecture. Trained in a self-supervised manner on a large curation of endoscopic images, RIDE requires no manual labeling of training data. We test RIDE in the context of surgical tissue tracking on the SuPeR dataset as well as in the context of relative pose estimation on a repurposed version of the SCARED dataset. In addition we perform explicit studies showing its robustness to large rotations. Our comparison against recent learning-based and classical approaches shows that RIDE sets a new state-ofthe-art performance on matching and relative pose estimation tasks and scores competitively on surgical tissue tracking.},
  archive   = {C_ICRA},
  author    = {Mert Asim Karaoglu and Viktoria Markova and Nassir Navab and Benjamin Busam and Alexander Ladikos},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611381},
  month     = {5},
  pages     = {10764-10771},
  title     = {RIDE: Self-supervised learning of rotation-equivariant keypoint detection and invariant description for endoscopy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Action-by-detection: Efficient forklift action detection for
autonomous mobile robots in warehouses. <em>ICRA</em>, 10757–10763. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Understanding actions of other agents increases the efficiency of autonomous mobile robots (AMRs) since they encompass intention and indicate future movements. We propose a new method that allows us to infer vehicle actions using a shallow image-based classification model. The actions are classified via bird’s-eye view scene crops, where we project the detections of a 3D object detection model onto a context map. We learn map context information and aggregate temporal sequence information without requiring object tracking. This results in a highly efficient classification model that can easily be deployed on embedded AMR hardware. To evaluate our approach, we create new large-scale synthetic datasets showing warehouse traffic based on real vehicle models and geometry.},
  archive   = {C_ICRA},
  author    = {Alexander Prutsch and Horst Possegger and Horst Bischof},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611659},
  month     = {5},
  pages     = {10757-10763},
  title     = {Action-by-detection: Efficient forklift action detection for autonomous mobile robots in warehouses},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sim-to-real object pose estimation for random bin picking.
<em>ICRA</em>, 10749–10756. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In industry, random bin picking is a complex and difficult task where instance segmentation and object pose estimation based on point clouds are key processes. Recently, learning-based segmentation and pose estimation methods for 3D point clouds have been proposed. However, many of them require supervised learning with datasets with annotations of objects. Since it is difficult to annotate all stacked instances in bin picking dataset, learning without real-world datasets has become a major interest. In this paper, we introduce an instance-level object pose estimation method for bin picking, which is trained using only simulated data and seamlessly applied to real-world scenarios without additional adaptation. To enable this, we introduce a method for generating a comprehensive synthetic dataset using a physics simulator, which incorporates 3D CAD models of objects and automatically generates annotations for both segmentation and pose estimation. Our experiments, conducted on synthetic datasets, highlight the competitive performance of our method in terms of recall and accuracy. Furthermore, we demonstrate the successful integration of our approach with real robot random bin picking, resulting in significantly improved picking success rates.},
  archive   = {C_ICRA},
  author    = {Boyoung Kim and Junhong Min},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611240},
  month     = {5},
  pages     = {10749-10756},
  title     = {Sim-to-real object pose estimation for random bin picking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Realtime robust shape estimation of deformable linear
object. <em>ICRA</em>, 10734–10740. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Realtime shape estimation of continuum objects and manipulators is essential for developing accurate planning and control paradigms. The existing methods that create dense point clouds from camera images, and/or use distinguishable markers on a deformable body have limitations in realtime tracking of large continuum objects/manipulators. The physical occlusion of markers can often compromise accurate shape estimation. We propose a robust method to estimate the shape of linear deformable objects in realtime using scattered and unordered key points. By utilizing a robust probability-based labeling algorithm, our approach identifies the true order of the detected key points and then reconstructs the shape using piecewise spline interpolation. The approach only relies on knowing the number of the key points and the interval between two neighboring points. We demonstrate the robustness of the method when key points are partially occluded. The proposed method is also integrated into a simulation in Unity for tracking the shape of a cable with a length of 1m and a radius of 5mm. The simulation results show that our proposed approach achieves an average length error of 1.07% over the continuum’s centerline and an average cross-section error of 2.11mm. The real-world experiments of tracking and estimating a heavy-load cable prove that the proposed approach is robust under occlusion and complex entanglement scenarios.},
  archive   = {C_ICRA},
  author    = {Jiaming Zhang and Zhaomeng Zhang and Yihao Liu and Yaqian Chen and Amir Kheradmand and Mehran Armand},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610432},
  month     = {5},
  pages     = {10734-10740},
  title     = {Realtime robust shape estimation of deformable linear object},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Saturation-aware angular velocity estimation: Extending the
robustness of SLAM to aggressive motions*. <em>ICRA</em>, 10711–10718.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel angular velocity estimation method to increase the robustness of Simultaneous Localization And Mapping (SLAM) algorithms against gyroscope saturations induced by aggressive motions. Field robotics expose robots to various hazards, including steep terrains, landslides, and staircases, where substantial accelerations and angular velocities can occur if the robot loses stability and tumbles. These extreme motions can saturate sensor measurements, especially gyroscopes, which are the first sensors to become inoperative. While the structural integrity of the robot is at risk, the robustness of the SLAM framework is oftentimes given little consideration. Consequently, even if the robot is physically capable of continuing the mission, its operation will be compromised due to a corrupted representation of the world. Regarding this problem, we propose a method to estimate the angular velocity using accelerometers during extreme rotations caused by tumbling. We show that our method reduces the median localization error by 71.5 % in translation and 65.5 % in rotation and is robust to mapping failures, which occurred in 37.5 % of the experiments without our method. We also propose the Tumbling-Induced Gyroscope Saturation (TIGS) dataset, which consists of outdoor experiments recording the motion of a mechanical lidar subject to angular velocities four times higher than other similar datasets available. The dataset is available online at https://github.com/norlab-ulaval/Norlab_wiki/wiki/TIGS-Dataset.},
  archive   = {C_ICRA},
  author    = {Simon-Pierre Deschênes and Dominic Baril and Matěj Boxan and Johann Laconte and Philippe Giguère and François Pomerleau},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610361},
  month     = {5},
  pages     = {10711-10718},
  title     = {Saturation-aware angular velocity estimation: Extending the robustness of SLAM to aggressive motions*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SRFNet: Monocular depth estimation with fine-grained
structure via spatial reliability-oriented fusion of frames and events.
<em>ICRA</em>, 10695–10702. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Monocular depth estimation is a crucial task to measure distance relative to a camera, which is important for applications, such as robot navigation and self-driving. Traditional frame-based methods suffer from performance drops due to the limited dynamic range and motion blur. Therefore, recent works leverage novel event cameras to complement or guide the frame modality via frame-event feature fusion. However, event streams exhibit spatial sparsity, leaving some areas unperceived, especially in regions with marginal light changes. Therefore, direct fusion methods, e.g., RAMNet [6], often ignore the contribution of the most confident regions of each modality. This leads to structural ambiguity in the modality fusion process, thus degrading the depth estimation performance. In this paper, we propose a novel Spatial Reliability-oriented Fusion Network (SRFNet), that can estimate depth with fine-grained structure at both daytime and nighttime. Our method consists of two key technical components. Firstly, we propose an attention-based interactive fusion (AIF) module that applies spatial priors of events and frames as the initial masks and learns the consensus regions to guide the inter-modal feature fusion. The fused feature are then fed back to enhance the frame and event feature learning. Meanwhile, it utilizes an output head to generate a fused mask, which is iteratively updated for learning consensual spatial priors. Secondly, we propose the Reliability-oriented Depth Refinement (RDR) module to estimate dense depth with the fine-grained structure based on the fused features and masks. We evaluate the effectiveness of our method on the synthetic and real-world datasets, which shows that, even without pretraining, our method outperforms the prior methods, e.g., RAMNet [6], especially in night scenes. Our project homepage: https://vlislab22.github.io/SRFNet.},
  archive   = {C_ICRA},
  author    = {Tianbo Pan and Zidong Cao and Lin Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610921},
  month     = {5},
  pages     = {10695-10702},
  title     = {SRFNet: Monocular depth estimation with fine-grained structure via spatial reliability-oriented fusion of frames and events},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LiDAR-camera calibration using intensity variance cost.
<em>ICRA</em>, 10688–10694. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose an extrinsic calibration method for LiDAR-camera fusion systems using variations in intensities projected from camera images to the LiDAR point cloud. As the input, the proposed method uses a sequence of LiDAR data and camera images captured while moving the system. Once the camera motion is calculated, camera images are projected onto the point cloud. The variations in the projected intensities at each point are large in the presence of errors in the estimated motion or calibration parameters. Consequently, the extrinsic parameters are optimized for cost minimization based on the intensity variance. In addition, a suitable geometry is proposed for the calibration and verified using simulations. Our experimental results showed that the proposed method accurately performed calibrations using a camera and a sparse multi-beam LiDAR or one-dimensional LiDAR.},
  archive   = {C_ICRA},
  author    = {Ryoichi Ishikawa and Shuyi Zhou and Yoshihiro Sato and Takeshi Oishi and Katsushi Ikeuchi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610261},
  month     = {5},
  pages     = {10688-10694},
  title     = {LiDAR-camera calibration using intensity variance cost},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HabitatDyn 2.0: Dataset for spatial anticipation and dynamic
object localization. <em>ICRA</em>, 10673–10679. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability of a robot to perceive and understand its environment is crucial for its actions and behavior. Humans are adept at using semantic information for object localization and path planning, a skill that robots need to emulate for intelligent adaptation in dynamic settings. Training of the spatial anticipation ability, which can enhance spatial perception through semantic understanding, necessitates the availability of appropriate data. Although extensive research has been conducted on datasets for outdoor environments, especially in the context of autonomous driving, there is still a notable lack of datasets specifically designed for indoor environments, with a focus on dynamic object localization. This paper introduces HabitatDyn 2.0, a dataset specifically designed for enhancing object localization capabilities with semantic information from a robot’s perspective. Besides RGB videos, semantic annotations, and depth information, HabitatDyn 2.0 also features top-down view labels for dynamic objects, which is required for training the spatial anticipation ability based on semantic information. Additionally, an algorithm that leverages spatial anticipation for dynamic object localization is presented, trained, and evaluated on the dataset.},
  archive   = {C_ICRA},
  author    = {Zhengcheng Shen and Linh Kästner and Yi Gao and Jens Lambrecht},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610719},
  month     = {5},
  pages     = {10673-10679},
  title     = {HabitatDyn 2.0: Dataset for spatial anticipation and dynamic object localization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). RadarCam-depth: Radar-camera fusion for depth estimation
with learned metric scale. <em>ICRA</em>, 10665–10672. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel approach for metric dense depth estimation based on the fusion of a single-view image and a sparse, noisy Radar point cloud. The direct fusion of heterogeneous Radar and image data, or their encodings, tends to yield dense depth maps with significant artifacts, blurred boundaries, and suboptimal accuracy. To circumvent this issue, we learn to augment versatile and robust monocular depth prediction with the dense metric scale induced from sparse and noisy Radar data. We propose a Radar-Camera framework for highly accurate and fine-detailed dense depth estimation with four stages, including monocular depth prediction, global scale alignment of monocular depth with sparse Radar points, quasi-dense scale estimation through learning the association between Radar points and image patches, and local scale refinement of dense depth using a scale map learner. Our proposed method significantly outperforms the state-of-the-art Radar-Camera depth estimation methods by reducing the mean absolute error (MAE) of depth estimation by 25.6% and 40.2% on the challenging nuScenes dataset and our self-collected ZJU-4DRadarCam dataset, respectively. Our code and dataset will be released at https://github.com/MMOCKING/RadarCam-Depth.},
  archive   = {C_ICRA},
  author    = {Han Li and Yukai Ma and Yaqing Gu and Kewei Hu and Yong Liu and Xingxing Zuo},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610929},
  month     = {5},
  pages     = {10665-10672},
  title     = {RadarCam-depth: Radar-camera fusion for depth estimation with learned metric scale},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AYDIV: Adaptable yielding 3D object detection via integrated
contextual vision transformer. <em>ICRA</em>, 10657–10664. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Combining LiDAR and camera data has shown potential in enhancing short-distance object detection in autonomous driving systems. Yet, the fusion encounters difficulties with extended distance detection due to the contrast between LiDAR’s sparse data and the dense resolution of cameras. Besides, discrepancies in the two data representations further complicate fusion methods. We introduce AYDIV, a novel framework integrating a tri-phase alignment process specifically designed to enhance long-distance detection even amidst data discrepancies. AYDIV consists of the Global Contextual Fusion Alignment Transformer (GCFAT), which improves the extraction of camera features and provides a deeper understanding of large-scale patterns; the Sparse Fused Feature Attention (SFFA), which fine-tunes the fusion of LiDAR and camera details; and the Volumetric Grid Attention (VGA) for a comprehensive spatial data fusion. AYDIV’s performance on the Waymo Open Dataset (WOD) with an improvement of 1.24% in mAPH value(L2 difficulty) and the Argoverse2 Dataset with a performance improvement of 7.40% in AP value demonstrates its efficacy in comparison to other existing fusion-based methods. Our code is publicly available at https://github.com/sanjay-810/AYDIV2},
  archive   = {C_ICRA},
  author    = {Tanmoy Dam and Sanjay Bhargav Dharavath and Sameer Alam and Nimrod Lilith and Supriyo Chakraborty and Mir Feroskhan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610908},
  month     = {5},
  pages     = {10657-10664},
  title     = {AYDIV: Adaptable yielding 3D object detection via integrated contextual vision transformer},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). JSTR: Joint spatio-temporal reasoning for event-based moving
object detection. <em>ICRA</em>, 10650–10656. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Event-based moving object detection is a challenging task, where static background and moving object are mixed together. Typically, existing methods mainly align the background events to the same spatial coordinate system via motion compensation to distinguish the moving object. However, they neglect the potential spatial tailing effect of moving object events caused by excessive motion, which may affect the structure integrity of the extracted moving object. We discover that the moving object has a complete columnar structure in the point cloud composed of motion-compensated events along the timestamp. Motivated by this, we propose a novel joint spatio-temporal reasoning method for event-based moving object detection. Specifically, we first compensate the motion of background events using inertial measurement unit. In spatial reasoning stage, we project the compensated events into the same image coordinate, discretize the timestamp of events to obtain a time image that can reflect the motion confidence, and further segment the moving object through adaptive threshold on the time image. In temporal reasoning stage, we construct the events into a point cloud along timestamp, and use RANSAC algorithm to extract the columnar shape in the cloud for peeling off the background. Finally, we fuse the results from the two reasoning stages to extract the final moving object region. This joint spatio-temporal reasoning framework can effectively detect the moving object from motion confidence and geometric structure. Moreover, we conduct extensive experiments on various datasets to verify that the proposed method can improve the moving object detection accuracy by 13%.},
  archive   = {C_ICRA},
  author    = {Hanyu Zhou and Zhiwei Shi and Hao Dong and Shihan Peng and Yi Chang and Luxin Yan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610608},
  month     = {5},
  pages     = {10650-10656},
  title     = {JSTR: Joint spatio-temporal reasoning for event-based moving object detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Sensor-based multi-robot coverage control with spatial
separation in unstructured environments. <em>ICRA</em>, 10623–10629. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-robot systems have increasingly become instrumental in tackling coverage problems. However, the challenge of optimizing task efficiency without compromising task success still persists, particularly in expansive, unstructured scenarios with dense obstacles. This paper presents an innovative, decentralized Voronoi-based coverage control approach to reactively navigate these complexities while guaranteeing safety. This approach leverages the active sensing capabilities of multi-robot systems to supplement GIS (Geographic Information System), offering a more comprehensive and real-time understanding of environments like post-disaster. Based on point cloud data, which is inherently non-convex and unstructured, this method efficiently generates collision-free Voronoi regions using only local sensing information through spatial decomposition and spherical mirroring techniques. Then, deadlock-aware guided map integrated with a gradient-optimized, centroid Voronoi-based coverage control policy, is constructed to improve efficiency by avoiding exhaustive searches and local sensing pitfalls. The effectiveness of our algorithm has been validated through extensive numerical simulations in high-fidelity environments, demonstrating significant improvements in task success rate, coverage ratio, and task execution time compared with others.},
  archive   = {C_ICRA},
  author    = {Xinyi Wang and Jiwen Xu and Chuanxiang Gao and Yizhou Chen and Jihan Zhang and Chenggang Wang and Yulong Ding and Ben M. Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611020},
  month     = {5},
  pages     = {10623-10629},
  title     = {Sensor-based multi-robot coverage control with spatial separation in unstructured environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging tethers for distributed formation control of
simple robots. <em>ICRA</em>, 10588–10594. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tethers have great potential in multi-robot systems from enabling retrieval of deployed robots and facilitating power transfer, to use by the robots as a net or partition. In this paper, we show in simulation that tethers can also be used to do distributed formation control on very simple robots. Specifically, our simulated agents are connected in series by un-actuated, flexible, fixed-length tethers and use tether angle and strain, in conjunction with the physical constraints of the tethers, to adjust their position with respect to their neighbors. This presents a significant simplification over traditional formation control which, at a minimum, requires exteroceptive sensors to perceive bearing and/or distance to nearby agents. We present and evaluate an algorithm on a large set of transitions between formations with 5 agents and an example transition with 35 agents. The convergence time grows with the number of agents, however, the memory and computation time per agent remain constant. Future work will investigate the ability to use tethers and strain for reactive behaviors and more diverse tasks.},
  archive   = {C_ICRA},
  author    = {Sadie Cutler and Kirstin Petersen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611041},
  month     = {5},
  pages     = {10588-10594},
  title     = {Leveraging tethers for distributed formation control of simple robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). FogROS2-LS: A location-independent fog robotics framework
for latency sensitive ROS2 applications. <em>ICRA</em>, 10581–10587. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In Cloud Robotics, long system latency due to varying network conditions can cause instability and collisions. However, this can be minimized in the almost univeral case where there are multiple sources available for cloud servers. By extending anycast routing, we introduce FogROS2-Latency-Sensitive, a Fog Robotics framework that offers secure, location-independent connections between robots and latency-sensitive cloud-based servers. FogROS2-LS offloads conventional on-board state estimators and feedback controllers to Cloud and Edge compute hardware without modifying existing applications in ROS2. In the presence of multiple identical services, FogROS2-LS dynamically identifies and transitions to the optimal service deployment that meets latency requirements, thereby empowering robots with limited on-board computing capacity to safely and efficiently navigate dynamic, human-dense environments. We evaluate FogROS2-LS with two latency sensitive case studies: (1) Collision Avoidance: a robot arm guided by visual feedback from consistent distance estimation and collision checking on Cloud and Edge. FogROS2-LS reduces collision failures by up to 8.5x by selecting the best available server, and (2) Target Tracking: FogROS2-LS enables robust and continuous target following and can recover from network failures. Videos and code are available on the website https://sites.google.com/view/fogros2-ls.},
  archive   = {C_ICRA},
  author    = {Kaiyuan Chen and Michael Wang and Marcus Gualtieri and Nan Tian and Christian Juette and Liu Ren and Jeffrey Ichnowski and John Kubiatowicz and Ken Goldberg},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610759},
  month     = {5},
  pages     = {10581-10587},
  title     = {FogROS2-LS: A location-independent fog robotics framework for latency sensitive ROS2 applications},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic multi-agent deep deterministic policy gradient for
autonomous navigation of reconfigurable unmanned aerial vehicle.
<em>ICRA</em>, 10574–10580. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The reconfigurable unmanned aerial vehicle (RUAV) has the ability to create and break physical links to self-assemble and self-disassemble in midair. For the changes in task or environment, this system can dynamically disassemble the rectangular structure into multiple individual UAV modules or integrate these UAV modules into a whole. For practical applications, the R-UAV requires collaborative decision-making for autonomous navigation in complex environments. However, the navigation problem of the R-UAV has not been investigated. In this paper, we propose a dynamic multi-agent deep deterministic policy gradient (DMADDPG) algorithm for autonomous navigation of R-UAV. This algorithm introduces the leader agent assignment mechanism and a collaborative experience reward. The former deals with the action conflict problem caused by the disappearance of the UAV agent when multiple UAV modules are assembled. The latter provides guidance for the UAV agent to plan a collision-free and efficient trajectory. We validate our strategy in both simulation and practical scenarios, and experimental results demonstrate that the proposed scheme can generate reasonable and efficient paths for R-UAV in the presence of obstacles. The experiment video is available at https://youtu.be/mVm0qCvB7HY.},
  archive   = {C_ICRA},
  author    = {Lu Xin and Wu Zegui and Zhao Ruqing and Li Fusheng},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611422},
  month     = {5},
  pages     = {10574-10580},
  title     = {Dynamic multi-agent deep deterministic policy gradient for autonomous navigation of reconfigurable unmanned aerial vehicle},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RIDER: Reinforcement-based inferred dynamics via emulating
rehearsals for robot navigation in unstructured environments.
<em>ICRA</em>, 10566–10573. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous navigation in unstructured environments is a challenging task due to the complex and dynamic nature of robot-terrain interactions. Existing approaches often struggle to generalize amidst the complexities of real-world settings. They tend to rely on hand-engineered, rule-based robot models or static weightings assigned to obstacles, semantics, and other perceptual cues to estimate traversability. To address these challenges, we propose a novel approach called Reinforcement-Based Inferred Dynamics via Emulating Rehearsals (RIDER), that learns the dynamics of robot-terrain interactions within a compact latent space, capturing robot’s traversability. Operating within a reinforcement learning paradigm, RIDER learns to infer its own dynamics by predicting how future robot observations and states evolve within this latent space in response to navigational behaviors. Furthermore, our approach leverages emulated rehearsals, where the robot learns within the latent space to predict its rewards and generate navigational behaviors, even when real observations have not been updated. Accordingly, RIDER equips robots with the ability to generate navigational behaviors by predicting environmental changes, and plan beyond the speed at which observations from sensors are available. Experimental results and comparisons with baseline methods establish that our proposed method outperforms other approaches in cluttered and unstructured environments and demonstrates an enhanced capacity for autonomous navigation in real-world settings.},
  archive   = {C_ICRA},
  author    = {Sriram Siva and Maggie Wigness},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611692},
  month     = {5},
  pages     = {10566-10573},
  title     = {RIDER: Reinforcement-based inferred dynamics via emulating rehearsals for robot navigation in unstructured environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A deep learning framework for non-symmetrical coulomb
friction identification of robotic manipulators. <em>ICRA</em>,
10510–10516. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The determination of the dynamic properties of a robot is especially important for designing highly accurate and efficient control systems. Conventional methods for dynamic model identification have proven to be effective, where deep learning (DL) approaches have shown limits due to data inefficiencies. However, thanks to novel physics-informed DL architectures, such as Deep Lagrangian Networks (DeLaN) [1], it is possible to control and extract interpretable physical information of a robot. This paper introduces an augmented DeLaN architecture for linear viscous and non-symmetrical Coulomb friction identification, which also learns motor parameters such as rotor inertia. An approach is proposed for comparing this method with the conventional dynamic identification and previous DeLaN implementations. Moreover, our friction and rotor inertia identification is validated, and the performance of our model is analyzed with a real robot (UR5e).},
  archive   = {C_ICRA},
  author    = {M. Lahoud and G. Marchello and M. D’Imperio and A. Müller and F. Cannella},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610737},
  month     = {5},
  pages     = {10510-10516},
  title     = {A deep learning framework for non-symmetrical coulomb friction identification of robotic manipulators},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Synthesis of temporally-robust policies for signal temporal
logic tasks using reinforcement learning. <em>ICRA</em>, 10503–10509.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper investigates the problem of designing control policies that satisfy high-level specifications described by signal temporal logic (STL) in unknown, stochastic environments. While many existing works concentrate on optimizing the spatial robustness of a system, our work takes a step further by also considering temporal robustness as a critical metric to quantify the tolerance of time uncertainty in STL. To this end, we formulate two relevant control objectives to enhance the temporal robustness of the synthesized policies. The first objective is to maximize the probability of being temporally robust for a given threshold. The second objective is to maximize the worst-case spatial robustness value within a bounded time shift. We use reinforcement learning to solve both control synthesis problems for unknown systems. Specifically, we approximate both control objectives in a way that enables us to apply the standard Q-learning algorithm. Theoretical bounds in terms of the approximations are also derived. We present case studies to demonstrate the feasibility of our approach.},
  archive   = {C_ICRA},
  author    = {Siqi Wang and Shaoyuan Li and Li Yin and Xiang Yin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610510},
  month     = {5},
  pages     = {10503-10509},
  title     = {Synthesis of temporally-robust policies for signal temporal logic tasks using reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NNgTL: Neural network guided optimal temporal logic task
planning for mobile robots. <em>ICRA</em>, 10496–10502. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we investigate task planning for mobile robots under linear temporal logic (LTL) specifications. This problem is particularly challenging when robots navigate in continuous workspaces due to the high computational complexity involved. Sampling-based methods have emerged as a promising avenue for addressing this challenge by incrementally constructing random trees, thereby sidestepping the need to explicitly explore the entire state-space. However, the performance of this sampling-based approach hinges crucially on the chosen sampling strategy, and a well-informed heuristic can notably enhance sample efficiency. In this work, we propose a novel neural-network guided (NN-guided) sampling strategy tailored for LTL planning. Specifically, we employ a multi-modal neural network capable of extracting features concurrently from both the workspace and the Büchi automaton. This neural network generates predictions that serve as guidance for random tree construction, directing the sampling process toward more optimal directions. Through numerical experiments, we compare our approach with existing methods and demonstrate its superior efficiency, requiring less than 15% of the time of the existing methods to find a feasible solution.},
  archive   = {C_ICRA},
  author    = {Ruijia Liu and Shaoyuan Li and Xiang Yin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611699},
  month     = {5},
  pages     = {10496-10502},
  title     = {NNgTL: Neural network guided optimal temporal logic task planning for mobile robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attitude control for morphing quadrotor through model
predictive control with constraints*. <em>ICRA</em>, 10489–10495. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Morphing quadrotors that can be potentially applied to confined spaces such as warehouses, tanks, and pipelines have flourished in recent years. Most work has focused on the mechanical feasibility of the morphing systems and high-level flight controller design, with limited discussions on low-level control. In this paper, a constrained model predictive control (MPC) is proposed and applied to solve the attitude control problem of a morphing quadrotor. Prior to controller design, a custom-built morphing quadrotor is introduced with the kinematic and dynamic models established and corresponding issues and challenges presented. In the controller, to eliminate the steady-state error, an embedded integrator is adopted by exploiting the differential variables; then, the constraints of the morphing quadrotor are incorporated into the MPC formulation to simulate real flight conditions, and an orthonormal function is employed to approximate the control input sequences in the controller to alleviate the computational burden. In the comparative studies, several scenarios are considered to demonstrate the effectiveness of the proposed control strategy in attitude control.},
  archive   = {C_ICRA},
  author    = {Na Zhao and Yudong Luo and Chaojun Qin and Xi Luo and Rong Chen and Yantao Shen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610512},
  month     = {5},
  pages     = {10489-10495},
  title     = {Attitude control for morphing quadrotor through model predictive control with constraints*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model predictive control for an autonomous underwater robot
with fully vectored propulsion. <em>ICRA</em>, 10482–10488. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Due to the low motion efficiency and maneuver-ability of underwater robots with six degrees of freedom, it is challenging for them to respond quickly to the attitude requirements during underwater autonomous manipulation. This paper presents a novel autonomous underwater robot with fully vectored propulsion and a model predictive control method to achieve more agile and efficient movements autonomously. In detail, we first design a robot with eight vector-distributed thruster layouts for fully vectored propulsion and construct the software architecture based on the robot operating system (ROS). Then, we establish the hydrodynamic model by adopting the Fossen approach and construct a 13-dimensional system state-space equation, which is discretized using the explicit fourth-order Runge-Kutta method. To achieve autonomous manipulation, model predictive control is employed along with physical constraints of the custom-built robot to enable real-time prediction and optimization of the robot’s states for control purposes. Finally, numerical simulations and experiments of the Point-to-Point Motion are conducted to test the robot’s performance. Experimental results reveal that the average error of each direction is 0.0027 m, 0.0031 m, and 0.0368 m in the x-axis, y-axis, and z-axis, respectively, and 0.8502°, 2.1941°, 0.2408° corresponding to three attitude angles, which verify the performance of employing MPC to control an autonomous underwater robot with fully vectored propulsion.},
  archive   = {C_ICRA},
  author    = {Tianzhu Gao and Yudong Luo and Chao Lv and Weirong Luo and Xianping Fu and Na Zhao and Xi Luo and Yantao Shen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611025},
  month     = {5},
  pages     = {10482-10488},
  title     = {Model predictive control for an autonomous underwater robot with fully vectored propulsion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Global terminal sliding mode control of tethered satellites
formation with chattering reduction via PID laws. <em>ICRA</em>,
10457–10463. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper researches a novel global terminal sliding mode control(GTSMC) on a tethered satellites system(TSS) under outer disturbances, and the effect of PI/PD compensation in restraining chattering on sliding surface is appended. By taking advantage of the finite-time convergence of traditional terminal sliding surface, the sliding surface with global and terminal sliding motion is proposed, and the convergent time by GTSMC is qualitatively evaluated by the sliding surface. Then the integral/derivative function of the low-pass filtered switching control is appended in GTSMC. By virtue of the accuracy of integral and the damping of derivative, respectively, the persisting on sliding surface is eliminated, such that the chattering effect of the controlled system on the surface is restrained consequently. Finally, simulations of the proposed control on TSS is shown to validate the theoretical analyses.},
  archive   = {C_ICRA},
  author    = {Bowen Su and Fan Zhang and Panfeng Huang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611346},
  month     = {5},
  pages     = {10457-10463},
  title     = {Global terminal sliding mode control of tethered satellites formation with chattering reduction via PID laws},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Risk-sensitive extended kalman filter. <em>ICRA</em>,
10450–10456. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Designing robust algorithms in the face of estimation uncertainty is a challenging task. Indeed, controllers seldom consider estimation uncertainty and only rely on the most likely estimated state. Consequently, sudden changes in the environment or the robot’s dynamics can lead to catastrophic behaviors. Leveraging recent results in risk-sensitive optimal control, this paper presents a risk-sensitive Extended Kalman Filter that can adapt its estimation to the control objective, hence allowing safe output-feedback Model Predictive Control (MPC). By taking a pessimistic estimate of the value function resulting from the MPC controller, the filter provides increased robustness to the controller in phases of uncertainty as compared to a standard Extended Kalman Filter (EKF). The filter has the same computational complexity as an EKF and can be used for real-time control. The paper evaluates the risk-sensitive behavior of the proposed filter when used in a nonlinear MPC loop on a planar drone and industrial manipulator in simulation, as well as on an external force estimation task on a real quadruped robot. These experiments demonstrate the ability of the approach to significantly improve performance in face of uncertainties.},
  archive   = {C_ICRA},
  author    = {Armand Jordana and Avadesh Meduri and Etienne Arlaud and Justin Carpentier and Ludovic Righetti},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611266},
  month     = {5},
  pages     = {10450-10456},
  title     = {Risk-sensitive extended kalman filter},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive planning and control with time-varying tire models
for autonomous racing using extreme learning machine. <em>ICRA</em>,
10443–10449. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous racing is a challenging problem, as the vehicle needs to operate at the friction or handling limits in order to achieve minimum lap times. Autonomous race cars require highly accurate perception, state estimation, planning, and control. Adding to this complexity is the need to accurately identify vehicle model parameters governing lateral tire slip effects, which can evolve over time due to factors such as tire wear and tear. Current approaches to this problem typically either propose offline model identification methods or rely on initial parameters within a narrow range (typically within 15-20% of the actual values). However, these approaches fall short in accounting for significant changes in tire models that can occur during actual races, particularly when pushing the vehicle to its handling limits. We present a unified framework that not only learns the tire model in real time from collected data but also adapts the model to environmental changes, even when the model parameters exhibit substantial deviations. The friction estimation, obtained as a byproduct from the learning results, facilitates the selection of the optimal racing line from a library for adaptive speed planning. We validate our approach through testing in simulators, encompassing a 1:43 scale race car and a full-size car, and also through experiments with a physical F1/10 autonomous race car.},
  archive   = {C_ICRA},
  author    = {Dvij Kalaria and Qin Lin and John M. Dolan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610848},
  month     = {5},
  pages     = {10443-10449},
  title     = {Adaptive planning and control with time-varying tire models for autonomous racing using extreme learning machine},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nullspace adaptive model-based trajectory-tracking control
for a 6-DOF underwater vehicle with unknown plant and actuator
parameters: Theory and preliminary simulation evaluation. <em>ICRA</em>,
10436–10442. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We report a novel model-based nullspace adaptive trajectory-tracking control (NS-ATTC) algorithm for fully-actuated 6-degree-of-freedom (DOF) underwater vehicles which estimates unknown plant and actuator model parameters simultaneously. We provide a stability and convergence analysis with proof of asymptotically stable tracking error convergence, as well as a preliminary simulation study demonstrating 6-DOF trajectory tracking. The NS-ATTC algorithm does not require acceleration instrumentation and provides a stable online parameter estimate, enabling robust model-based autonomy.},
  archive   = {C_ICRA},
  author    = {Annie M. Mao and Joseph L. Moore and Louis L. Whitcomb},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611318},
  month     = {5},
  pages     = {10436-10442},
  title     = {Nullspace adaptive model-based trajectory-tracking control for a 6-DOF underwater vehicle with unknown plant and actuator parameters: Theory and preliminary simulation evaluation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Smooth computation without input delay: Robust tube-based
model predictive control for robot manipulator planning. <em>ICRA</em>,
10429–10435. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Model Predictive Control (MPC) has exhibited remarkable capabilities in optimizing objectives and meeting constraints. However, the substantial computational burden associated with solving the Optimal Control Problem (OCP) at each triggering instant introduces significant delays between state sampling and control application. These delays limit the practicality of MPC in resource-constrained systems when engaging in complex tasks. The intuition to address this issue in this paper is that by predicting the successor state, the controller can solve the OCP one time step ahead of time thus avoiding the delay of the next action. To this end, we compute deviations between real and nominal system states, predicting forthcoming real states as initial conditions for the imminent OCP solution. Anticipatory computation stores optimal control based on current nominal states, thus mitigating the delay effects. Additionally, we establish an upper bound for linearization error, effectively linearizing the nonlinear system, reducing OCP complexity, and enhancing response speed. We provide empirical validation through two numerical simulations and corresponding real-world robot tasks, demonstrating significant performance improvements and augmented response speed (up to 90%) resulting from the seamless integration of our proposed approach compared to conventional time-triggered MPC strategies.},
  archive   = {C_ICRA},
  author    = {Yu Luo and Qie Sima and Tianying Ji and Fuchun Sun and Huaping Liu and Jianwei Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610952},
  month     = {5},
  pages     = {10429-10435},
  title     = {Smooth computation without input delay: Robust tube-based model predictive control for robot manipulator planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Non-singular fast terminal adaptive visual tracking control
with reduced tuning parameters for an aerial vehicle under
perturbations. <em>ICRA</em>, 10406–10412. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a robust image-based visual servoing design for a quad-rotor unmanned aerial vehicle performing a visual target-tracking operation in the presence of turbulent wind. Image information is extracted and processed to control the positioning and heading of the aerial vehicle. A novel adaptive non-singular fast terminal sliding mode strategy is introduced to manage the visual servoing error. Unlike other sliding mode methods, the proposed approach diminishes the complexity of the system due to the reduction of its control parameters while providing practical finite-time convergence, robustness against bounded external disturbances and model uncertainties, non-overestimation of the control gains, and chattering attenuation. Furthermore, the stability of the system in closed loop is guaranteed through Lyapunov theory. Finally, simulation results demonstrate the capabilities and performance of such a controller in a high-fidelity scenario using the Robot Operating System and Gazebo frameworks.},
  archive   = {C_ICRA},
  author    = {Gustavo Olivas-Martínez and Armando Miranda-Moya and Carlos Katt and Herman Castañeda},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610345},
  month     = {5},
  pages     = {10406-10412},
  title     = {Non-singular fast terminal adaptive visual tracking control with reduced tuning parameters for an aerial vehicle under perturbations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Asymptotically-optimal multi-robot visibility-based
pursuit-evasion. <em>ICRA</em>, 10366–10373. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The multi-robot visibility-based pursuit-evasion problem tasks a team of robots with systematically searching an environment to detect (capture) an evader. Previous techniques to generate search strategies for the pursuit team have shown to be either computationally intractable or permit poor solution quality. This paper presents a novel asymptotically optimal algorithm for generating a joint motion strategy for the pursuers. To explore the space of possible pursuer motion strategies, the algorithm utilizes a trio of hierarchical graph data structures that each capture certain elements of the problem such as connectivity (valid single pursuer motion), coordination (multiple pursuer motion), and tracking information (evaluating where an evader may be). The algorithm is inspired by well-known methods in the motion planning literature and inherits its asymptotic optimality from those techniques. In addition, we describe a method that can improve upon solutions found during the formative stages of the main algorithm, using a &quot;fast-forward&quot; approach that foregoes guarantees of asymptotic optimality, implementing heuristics that concentrate future samples into improving the path quality of the nominal solution. The algorithms were validated in simulation and results are provided.},
  archive   = {C_ICRA},
  author    = {Nicholas M. Stiffler and Jason M. O’Kane},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610927},
  month     = {5},
  pages     = {10366-10373},
  title     = {Asymptotically-optimal multi-robot visibility-based pursuit-evasion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Approximating robot configuration spaces with few convex
sets using clique covers of visibility graphs. <em>ICRA</em>,
10359–10365. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many computations in robotics can be dramatically accelerated if the robot configuration space is described as a collection of simple sets. For example, recently developed motion planners rely on a convex decomposition of the free space to design collision-free trajectories using fast convex optimization. In this work, we present an efficient method for approximately covering complex configuration spaces with a small number of polytopes. The approach constructs a visibility graph using sampling and generates a clique cover of this graph to find clusters of samples that have mutual line of sight. These clusters are then inflated into large, full-dimensional, polytopes. We evaluate our method on a variety of robotic systems and show that it consistently covers larger portions of free configuration space, with fewer polytopes, and in a fraction of the time compared to previous methods.},
  archive   = {C_ICRA},
  author    = {Peter Werner and Alexandre Amice and Tobia Marcucci and Daniela Rus and Russ Tedrake},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610005},
  month     = {5},
  pages     = {10359-10365},
  title     = {Approximating robot configuration spaces with few convex sets using clique covers of visibility graphs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EDMP: Ensemble-of-costs-guided diffusion for motion
planning. <em>ICRA</em>, 10351–10358. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Classical motion planning for robotic manipulation includes a set of general algorithms that aim to minimize a scene-specific cost of executing a given plan. This approach offers remarkable adaptability, as they can be directly used off-the-shelf for any new scene without needing specific training datasets. However, without a prior understanding of what diverse valid trajectories are and without specially designed cost functions for a given scene, the overall solutions tend to have low success rates within a certain time limit. While deep-learning-based algorithms tremendously improve success rates, they are much harder to adopt without specialized training datasets. We propose EDMP, an Ensemble-of-costs-guided Diffusion for Motion Planning that aims to combine the strengths of classical and deep-learning-based motion planning. Our diffusion-based network is trained on a set of diverse kinematically valid trajectories. Like classical planning, for any new scene at the time of inference, we compute scene-specific costs such as &quot;collision cost&quot; and guide the diffusion to generate valid trajectories that satisfy the scene-specific constraints. Further, instead of a single cost function that may be insufficient in capturing diversity across scenes, we use an ensemble of costs to guide the diffusion process, significantly improving the success rate compared to classical planners. EDMP performs comparably with SOTA deep-learning-based methods while retaining the generalization capabilities primarily associated with classical planners.},
  archive   = {C_ICRA},
  author    = {Kallol Saha and Vishal Mandadi and Jayaram Reddy and Ajit Srikanth and Aditya Agarwal and Bipasha Sen and Arun Singh and Madhava Krishna},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610519},
  month     = {5},
  pages     = {10351-10358},
  title     = {EDMP: Ensemble-of-costs-guided diffusion for motion planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VAPOR: Legged robot navigation in unstructured outdoor
environments using offline reinforcement learning. <em>ICRA</em>,
10344–10350. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present VAPOR, a novel method for autonomous legged robot navigation in unstructured, densely vegetated outdoor environments using offline Reinforcement Learning (RL). Our method trains a novel RL policy using an actor-critic network and arbitrary data collected in real outdoor vegetation. Our policy uses height and intensity-based cost maps derived from 3D LiDAR point clouds, a goal cost map, and processed proprioception data as state inputs, and learns the physical and geometric properties of the surrounding obstacles such as height, density, and solidity/stiffness. The fully-trained policy’s critic network is then used to evaluate the quality of dynamically feasible velocities generated from a novel contextaware planner. Our planner adapts the robot’s velocity space based on the presence of entrapment including vegetation, and narrow passages in dense environments. We demonstrate our method’s capabilities on a Spot robot in complex real-world outdoor scenes, including dense vegetation. We observe that VAPOR’s actions improve success rates by up to 40%, decrease the average current consumption by up to 2.9%, and decrease the normalized trajectory length by up to 11.2% compared to existing end-to-end offline RL and other outdoor navigation methods.},
  archive   = {C_ICRA},
  author    = {Kasun Weerakoon and Adarsh Jagan Sathyamoorthy and Mohamed Elnoor and Dinesh Manocha},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610132},
  month     = {5},
  pages     = {10344-10350},
  title     = {VAPOR: Legged robot navigation in unstructured outdoor environments using offline reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Constant-time motion planning with anytime refinement for
manipulation. <em>ICRA</em>, 10337–10343. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic manipulators are essential for future autonomous systems, yet limited trust in their autonomy has confined them to rigid, task-specific systems. The intricate configuration space of manipulators, coupled with the challenges of obstacle avoidance and constraint satisfaction, often makes motion planning the bottleneck for achieving reliable and adaptable autonomy. Recently, a class of constant-time motion planners (CTMP) was introduced. These planners employ a preprocessing phase to compute data structures that enable online planning provably guarantee the ability to generate motion plans, potentially sub-optimal, within a user defined time bound. This framework has been demonstrated to be effective in a number of time-critical tasks. However, robotic systems often have more time allotted for planning than the online portion of CTMP requires, time that can be used to improve the solution. To this end, we propose an anytime refinement approach that works in combination with CTMP algorithms. Our proposed framework, as it operates as a constant time algorithm, rapidly generates an initial solution within a user-defined time threshold. Furthermore, functioning as an anytime algorithm, it iteratively refines the solution’s quality within the allocated time budget. This enables our approach to strike a balance between guaranteed fast plan generation and the pursuit of optimization over time. We support our approach by elucidating its analytical properties, showing the convergence of the anytime component towards optimal solutions. Additionally, we provide empirical validation through simulation and real-world demonstrations on a 6 degree-of-freedom robot manipulator, applied to an assembly domain.},
  archive   = {C_ICRA},
  author    = {Itamar Mishani and Hayden Feddock and Maxim Likhachev},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611675},
  month     = {5},
  pages     = {10337-10343},
  title     = {Constant-time motion planning with anytime refinement for manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lite-SVO: Towards a lightweight self-supervised semantic
visual odometry exploiting multi-feature sharing architecture.
<em>ICRA</em>, 10305–10311. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Not relying on ground-truth data for training, self-supervised semantic visual odometry (SVO) has recently gained considerable attention. Within self-supervised SVO, feature representation inconsistency between semantic/depth and pose tasks presents a significant challenge, as it may disrupt cross-task feature representations and lead to notable performance degradation. Regrettably, existing self-supervised SVO lacks an effective solution to address this obstacle, for either overlooking this issue or exploiting a too heavy architecture. In response to this challenge, we propose a groundbreaking solution within the Single-Stream architecture, known as Lite-SVO, which is a lightweight yet efficient multi-feature sharing architecture. Lite-SVO is designed to bolster self-supervised SVO, facilitating its adoption on edge devices without compromising accuracy and performance. The crucial innovation lies in the multi-feature sharing architecture, which fuses the semantic and depth maps as pose features, thus significantly reducing the model complexity and boosting the speed in edge devices. Built upon the novel feature sharing framework, Lite-SVO further optimizes the feature sharing representation to improve the performance. Specifically, a cross-feature sharing module alleviates the impact of object boundary in depth estimation, while a multi-feature sharing module focuses on extracting and fusing spatial features to enhance pose estimation. Experimental results demonstrate that our method is at least 84.46% faster than the state-of-the-art Single-Stream approaches, and excitingly, our method’s pose accuracy is about 79.83% higher than theirs.},
  archive   = {C_ICRA},
  author    = {Wenhui Wei and Jiantao Li and Kaizhu Huang and Jiadong Li and Xin Liu and Yangfan Zhou},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610019},
  month     = {5},
  pages     = {10305-10311},
  title     = {Lite-SVO: Towards a lightweight self-supervised semantic visual odometry exploiting multi-feature sharing architecture},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VOOM: Robust visual object odometry and mapping using
hierarchical landmarks. <em>ICRA</em>, 10298–10304. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, object-oriented simultaneous localization and mapping (SLAM) has attracted increasing attention due to its ability to provide high-level semantic information while maintaining computational efficiency. Some researchers have attempted to enhance localization accuracy by integrating the modeled object residuals into bundle adjustment. However, few have demonstrated better results than feature-based visual SLAM systems, as the generic coarse object models, such as cuboids or ellipsoids, are less accurate than feature points. In this paper, we propose a Visual Object Odometry and Mapping framework VOOM using high-level objects and low-level points as the hierarchical landmarks in a coarse-to-fine manner instead of directly using object residuals in bundle adjustment. Firstly, we introduce an improved observation model and a novel data association method for dual quadrics, employed to represent physical objects. It facilitates the creation of a 3D map that closely reflects reality. Next, we use object information to enhance the data association of feature points and consequently update the map. In the visual object odometry backend, the updated map is employed to further optimize the camera pose and the objects. Meanwhile, local bundle adjustment is performed utilizing the objects and points-based covisibility graphs in our visual object mapping process. Experiments show that VOOM outperforms both object-oriented SLAM and feature points SLAM systems such as ORB-SLAM2 in terms of localization. The implementation of our method is available at https://github.com/yutongwangBIT/VOOM.git.},
  archive   = {C_ICRA},
  author    = {Yutong Wang and Chaoyang Jiang and Xieyuanli Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611684},
  month     = {5},
  pages     = {10298-10304},
  title     = {VOOM: Robust visual object odometry and mapping using hierarchical landmarks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalized correspondence matching via flexible
hierarchical refinement and patch descriptor distillation.
<em>ICRA</em>, 10290–10297. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Correspondence matching plays a crucial role in numerous robotics applications. In comparison to conventional hand-crafted methods and recent data-driven approaches, there is significant interest in plug-and-play algorithms that make full use of pre-trained backbone networks for multi-scale feature extraction and leverage hierarchical refinement strategies to generate matched correspondences. The primary focus of this paper is to address the limitations of deep feature matching (DFM), a state-of-the-art (SoTA) plug-and-play correspondence matching approach. First, we eliminate the pre-defined threshold employed in the hierarchical refinement process of DFM by leveraging a more flexible nearest neighbor search strategy, thereby preventing the exclusion of repetitive yet valid matches during the early stages. Our second technical contribution is the integration of a patch descriptor, which extends the applicability of DFM to accommodate a wide range of backbone networks pre-trained across diverse computer vision tasks, including image classification, semantic segmentation, and stereo matching. Taking into account the practical applicability of our method in real-world robotics applications, we also propose a novel patch descriptor distillation strategy to further reduce the computational complexity of correspondence matching. Extensive experiments conducted on three public datasets demonstrate the superior performance of our proposed method. Specifically, it achieves an overall performance in terms of mean matching accuracy of 0.68, 0.92, and 0.95 with respect to the tolerances of 1, 3, and 5 pixels, respectively, on the HPatches dataset, outperforming all other SoTA algorithms. Our source code, demo video, and supplement are publicly available at mias.group/GCM.},
  archive   = {C_ICRA},
  author    = {Yu Han and Ziwei Long and Yanting Zhang and Jin Wu and Zhijun Fang and Rui Fan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611456},
  month     = {5},
  pages     = {10290-10297},
  title     = {Generalized correspondence matching via flexible hierarchical refinement and patch descriptor distillation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IPC: Incremental probabilistic consensus-based consistent
set maximization for SLAM backends. <em>ICRA</em>, 10283–10289. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In SLAM (Simultaneous localization and mapping) problems, Pose Graph Optimization (PGO) is a technique to refine an initial estimate of a set of poses (positions and orientations) from a set of pairwise relative measurements. The optimization procedure can be negatively affected even by a single outlier measurement, with possible catastrophic and meaningless results. Although recent works on robust optimization aim to mitigate the presence of outlier measurements, robust solutions capable of handling large numbers of outliers are yet to come. This paper presents IPC, acronym for Incremental Probabilistic Consensus, a method that approximates the solution to the combinatorial problem of finding the maximally consistent set of measurements in an incremental fashion. It evaluates the consistency of each loop closure measurement through a consensus-based procedure, possibly applied to a subset of the global problem, where all previously integrated inlier measurements have veto power. We evaluated IPC on standard benchmarks against several state-of-the-art methods. Although it is simple and relatively easy to implement, IPC competes with or outperforms the other tested methods in handling outliers while providing online performances. We release with this paper an open-source implementation of the proposed method.},
  archive   = {C_ICRA},
  author    = {Emilio Olivastri and Alberto Pretto},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611214},
  month     = {5},
  pages     = {10283-10289},
  title     = {IPC: Incremental probabilistic consensus-based consistent set maximization for SLAM backends},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Radar-only odometry and mapping for autonomous vehicles.
<em>ICRA</em>, 10275–10282. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Odometry and mapping play a pivotal role in the navigation of autonomous vehicles. In this paper, we address the problem of pose estimation and map creation using only radar sensors. We focus on two odometry estimation approaches followed by a mapping step. The first one is a new point-to-point ICP approach that leverages the velocity information provided by 3D radar sensors. The second one is advantageous for 2D radars with a low number of samples, and particularly useful for scenarios where the sensor is being blocked by large dynamic obstacles. It exploits a constant velocity filter and the measured Doppler velocities to estimate the vehicle’s ego-motion. We enrich this with a filtering step to improve the accuracy of the points in the resulting map. We put our work to the test using the View of Delft and NuScenes datasets, which involve 3D and 2D radar sensors. Our findings illustrate state-of-the-art performance of our odometry techniques in terms of accuracy when compared to existing alternatives. Moreover, we demonstrate that our map filtering methodology achieves higher similarity rates than the raw unfiltered map when benchmarked against a corresponding LiDAR map.},
  archive   = {C_ICRA},
  author    = {Daniel Casado Herraez and Matthias Zeller and Le Chang and Ignacio Vizzo and Michael Heidingsfeld and Cyrill Stachniss},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610311},
  month     = {5},
  pages     = {10275-10282},
  title     = {Radar-only odometry and mapping for autonomous vehicles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effectively detecting loop closures using point cloud
density maps. <em>ICRA</em>, 10260–10266. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to detect loop closures plays an essential role in any SLAM system. Loop closures allow correcting the drifting pose estimates from a sensor odometry pipeline. In this paper, we address the problem of effectively detecting loop closures in LiDAR SLAM systems in various environments with longer lengths of sequences and agnostic of the scanning pattern of the sensor. While many approaches for loop closures using 3D LiDAR sensors rely on individual scans, we propose the usage of local maps generated from locally consistent odometry estimates. Several recent approaches compute the maximum elevation map on a bird’s eye view projection of point clouds to compute feature descriptors. In contrast, we use a density image bird’s eye view representation, which is robust to viewpoint changes. The utilization of dense local maps allows us to reduce the complexity of features describing these maps, as well as the size of the database required to store these features over a long sequence. This yields a real-time application of our approach for a typical robotic 3D LiDAR sensor. We perform extensive experiments to evaluate our approach against other state-of-the-art approaches and show the benefits of our proposed approach.},
  archive   = {C_ICRA},
  author    = {Saurabh Gupta and Tiziano Guadagnino and Benedikt Mersch and Ignacio Vizzo and Cyrill Stachniss},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610962},
  month     = {5},
  pages     = {10260-10266},
  title     = {Effectively detecting loop closures using point cloud density maps},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A two-step nonlinear factor sparsification for scalable
long-term SLAM backend. <em>ICRA</em>, 10253–10259. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a new nonlinear factor sparsification paradigm for general feature-based long-term SLAM backend. Given a pose sparsification policy, we aim to scale the SLAM problem with space explored instead of time in a principled way so that the number of time-indexed poses can be limited. At the same time, their influence and the long-lived landmarks are appropriately maintained. To do this, we propose a new two-step sparsification pipeline. Given a pose node to remove, the first step is performed in the Markov blankets of affected landmarks. It transforms pose-landmark constraints into pose-pose constraints while preserving observability and minimizing information loss in the blanket. Moreover, since landmarks are conditionally independent, we can do this in parallel, disconnecting a pose from all the landmarks. The second step marginalizes the pose of interest with pure pose-wise constraints without affecting landmarks. Our method decouples the management of landmarks from pose-only measurements, making it general for any feature-based SLAM. We also give a practical example of how our backend works by concatenating it to a monocular VIO frontend. In simulation and realworld dataset, our sparsified backend is accurate and efficient. We open-source our backend, along with the VIO+Backend example, to contribute to the community’s betterment.},
  archive   = {C_ICRA},
  author    = {Binqian Jiang and Shaojie Shen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610889},
  month     = {5},
  pages     = {10253-10259},
  title     = {A two-step nonlinear factor sparsification for scalable long-term SLAM backend},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ONeK-SLAM: A robust object-level dense SLAM based on joint
neural radiance fields and keypoints. <em>ICRA</em>, 10245–10252. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Neural implicit representation has recently achieved significant advancements, especially in the field of SLAM(Simultaneous Localization and Mapping). Previous NeRF-based SLAM methods have difficulties with object-level localization and reconstruction and struggle in dynamic and illumination-varied environments. We propose ONeK-SLAM, a robust object-level SLAM system that effectively combines feature points and neural radiance fields. ONeK-SLAM uses the joint information at the object level to improve localization accuracy and enhance reconstruction details. Moreover, our approach detects and eliminates dynamic objects based on the joint errors, while also harnessing the illumination invariance offered by feature points. Consequently, ONeK-SLAM achieves high-precision localization and detailed object-level mapping, even in dynamic and illumination-varying environments. Our evaluations, conducted on three public datasets that include both dynamic and variable lighting sequences, demonstrate that our method outperforms recent NeRF-based SLAM method in both localization and reconstruction.},
  archive   = {C_ICRA},
  author    = {Yue Zhuge and Haiyong Luo and Runze Chen and Yushi Chen and Jiaquan Yan and Zhuqing Jiang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610068},
  month     = {5},
  pages     = {10245-10252},
  title     = {ONeK-SLAM: A robust object-level dense SLAM based on joint neural radiance fields and keypoints},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Night-rider: Nocturnal vision-aided localization in
streetlight maps using invariant extended kalman filtering.
<em>ICRA</em>, 10238–10244. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision-aided localization for low-cost mobile robots in diverse environments has attracted widespread attention recently. Although many current systems are applicable in daytime environments, nocturnal visual localization is still an open problem owing to the lack of stable visual information. An insight from most nocturnal scenes is that the static and bright streetlights are reliable visual information for localization. Hence we propose a nocturnal vision-aided localization system in streetlight maps with a novel data association and matching scheme using object detection methods. We leverage the Invariant Extended Kalman Filter (InEKF) to fuse IMU, odometer, and camera measurements for consistent state estimation at night. Furthermore, a tracking recovery module is also designed for tracking failures. Experimental results indicate that our proposed system achieves accurate and robust localization with less than 0.2% relative error of trajectory length in four nocturnal environments.},
  archive   = {C_ICRA},
  author    = {Tianxiao Gao and Mingle Zhao and Chengzhong Xu and Hui Kong},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611408},
  month     = {5},
  pages     = {10238-10244},
  title     = {Night-rider: Nocturnal vision-aided localization in streetlight maps using invariant extended kalman filtering},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NeRF-VINS: A real-time neural radiance field map-based
visual-inertial navigation system. <em>ICRA</em>, 10230–10237. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Achieving efficient and consistent localization with a prior map remains challenging in robotics. Conventional keyframe-based approaches often suffer from sub-optimal viewpoints due to limited field of view (FOV) and/or constrained motion, thus degrading the localization performance. To address this issue, we design a real-time tightly-coupled Neural Radiance Fields (NeRF)-aided visual-inertial navigation system (VINS). In particular, by effectively leveraging the NeRF’s potential to synthesize novel views, the proposed NeRF-VINS overcomes the limitations of traditional keyframe-based maps (with limited views) and optimally fuses IMU, monocular images, and synthetically rendered images within an efficient filter-based framework. This tightly-coupled fusion enables efficient 3D motion tracking with bounded errors. We extensively validate the proposed NeRF-VINS against the state-of-the-art methods that use prior map information, and demonstrate its ability to perform real-time localization, at 15 Hz, on a resource-constrained Jetson AGX Orin embedded platform.},
  archive   = {C_ICRA},
  author    = {Saimouli Katragadda and Woosik Lee and Yuxiang Peng and Patrick Geneva and Chuchu Chen and Chao Guo and Mingyang Li and Guoquan Huang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610051},
  month     = {5},
  pages     = {10230-10237},
  title     = {NeRF-VINS: A real-time neural radiance field map-based visual-inertial navigation system},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lightweight ground texture localization. <em>ICRA</em>,
10223–10229. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a lightweight ground texture based localization algorithm (L-GROUT) that improves the state of the art in performance and can be run in real-time on single board computers without GPU acceleration. Such computers are ubiquitous on small indoor robots and thus this work enables high-precision, millimeter-level localization without instrumenting, marking, or modifying the environment. The key innovations are an improved database feature extraction algorithm, a dimensionality reduction method based on locality preserving projections (LPP) that can accommodate faster-to-compute binary features, and an improved spatial filtering step that better preserves performance when the databases are tuned for lightweight applications. We demonstrate the approach by running the whole system on a low-cost single board computer (Raspberry Pi 4) to produce global localization estimates at greater than 4Hz on an outdoor asphalt dataset.},
  archive   = {C_ICRA},
  author    = {Aaron Wilhelm and Nils Napp},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611712},
  month     = {5},
  pages     = {10223-10229},
  title     = {Lightweight ground texture localization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 17-point algorithm revisited: Toward a more accurate way.
<em>ICRA</em>, 10208–10214. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {17-point algorithm is a popular method in relative pose estimation of multi-cameras. However, the role of overlap in 17-point algorithm remains unexplored. And the relaxed way in solving constrained normal equation leads to sub-optimal results. Both of them influence accuracy of the estimated pose. In this paper, we theoretically analyze the influence of overlap and the solvability of 17-point algorithm. In addition, we show that the abuse of overlap can harm accuracy in practice. In light of these findings, we propose an improved 17-point algorithm, which avoids using overlaps and derives a simple way to solve normal equation on manifold. Both simulations and real world data experiments demonstrate the proposed one outperforms the traditional 17-point algorithm in term of accuracy.},
  archive   = {C_ICRA},
  author    = {Chen Xie and Rui Xing and Ning Hao and Fenghua He},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611149},
  month     = {5},
  pages     = {10208-10214},
  title     = {17-point algorithm revisited: Toward a more accurate way},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VPRTempo: A fast temporally encoded spiking neural network
for visual place recognition. <em>ICRA</em>, 10200–10207. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Spiking Neural Networks (SNNs) are at the forefront of neuromorphic computing thanks to their potential energy-efficiency, low latencies, and capacity for continual learning. While these capabilities are well suited for robotics tasks, SNNs have seen limited adaptation in this field thus far. This work introduces a SNN for Visual Place Recognition (VPR) that is both trainable within minutes and queryable in milliseconds, making it well suited for deployment on compute-constrained robotic systems. Our proposed system, VPRTempo, overcomes slow training and inference times using an abstracted SNN that trades biological realism for efficiency. VPRTempo employs a temporal code that determines the timing of a single spike based on a pixel’s intensity, as opposed to prior SNNs relying on rate coding that determined the number of spikes; improving spike efficiency by over 100%. VPRTempo is trained using Spike-Timing Dependent Plasticity and a supervised delta learning rule enforcing that each output spiking neuron responds to just a single place. We evaluate our system on the Nordland and Oxford RobotCar benchmark localization datasets, which include up to 27k places. We found that VPRTempo’s accuracy is comparable to prior SNNs and the popular NetVLAD place recognition algorithm, while being several orders of magnitude faster and suitable for real-time deployment – with inference speeds over 50 Hz on CPU. VPRTempo could be integrated as a loop closure component for online SLAM on resource-constrained systems such as space and underwater robots.},
  archive   = {C_ICRA},
  author    = {Adam D Hines and Peter G Stratton and Michael Milford and Tobias Fischer},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610918},
  month     = {5},
  pages     = {10200-10207},
  title     = {VPRTempo: A fast temporally encoded spiking neural network for visual place recognition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VOLoc: Visual place recognition by querying compressed lidar
map. <em>ICRA</em>, 10192–10199. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The availability of city-scale Lidar maps enables the potential of city-scale place recognition using mobile cameras. However, the city-scale Lidar maps generally need to be compressed for storage efficiency, which increases the difficulty of direct visual place recognition in compressed Lidar maps. This paper proposes VOLoc, an accurate and efficient visual place recognition method that exploits geometric similarity to directly query the compressed Lidar map via the real-time captured image sequence. In the offline phase, VOLoc compresses the Lidar maps using a Geometry-Preserving Compressor (GPC), in which the compression is reversible, a crucial requirement for the downstream 6DoF pose estimation. In the online phase, VOLoc proposes an online Geometric Recovery Module (GRM), which is composed of online Visual Odometry (VO) and a point cloud optimization module, such that the local scene structure around the camera is online recovered to build the Querying Point Cloud (QPC). Then the QPC is compressed by the same GPC, and is aggregated into a global descriptor by an attentionbased aggregation module, to query the compressed Lidar map in the vector space. A transfer learning mechanism is also proposed to improve the accuracy and the generality of the aggregation network. Extensive evaluations show that VOLoc provides localization accuracy even better than the Lidar-toLidar place recognition, setting up a new record for utilizing the compressed Lidar map by low-end mobile cameras. The code are publicly available at https://github.com/Master-cai/VOLoc.},
  archive   = {C_ICRA},
  author    = {Xudong Cai and Yongcai Wang and Zhe Huang and Yu Shao and Deying Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610530},
  month     = {5},
  pages     = {10192-10199},
  title     = {VOLoc: Visual place recognition by querying compressed lidar map},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual localization in repetitive and symmetric indoor
parking lots using 3D key text graph. <em>ICRA</em>, 10185–10191. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Indoor parking lots are the GPS-denied spaces to which vision-based localization approaches have usually been applied to solve localization problems. However, due to the repetitiveness and symmetry of the spaces, visual localization methods commonly confront difficulties in estimating precise 3D poses. In this study, we propose four novel modules that improve localization precision by imposing the existing methods with the spatial discerning ability. The first module constructs a key text graph that represents the topology of key texts in the space and becomes the basis for discerning repetitiveness and symmetry. Next, the orientation filtering module estimates the unknown 3D orientation of the query image and resolves spatial symmetric ambiguity. The similarity scoring module sorts out the top-scored database images, discerning the spatial repetitiveness based on detected key text bounding boxes. Our pose verification module evaluates the pose confidence of top-scored candidates and determines the most reliable pose. Our method has been validated in two real indoor parking lots, achieving new state-of-the-art performance levels.},
  archive   = {C_ICRA},
  author    = {Joohyung Kim and Gunhee Koo and Heewon Park and Nakju Doh},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611591},
  month     = {5},
  pages     = {10185-10191},
  title     = {Visual localization in repetitive and symmetric indoor parking lots using 3D key text graph},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On-device self-supervised learning of visual perception
tasks aboard hardware-limited nano-quadrotors. <em>ICRA</em>,
10118–10124. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sub-50g nano-drones are gaining momentum in both academia and industry. Their most compelling applications rely on onboard deep learning models for perception despite severe hardware constraints (i.e., sub-100mW processor). When deployed in unknown environments not represented in the training data, these models often underperform due to domain shift. To cope with this fundamental problem, we propose, for the first time, on-device learning aboard nano-drones, where the first part of the in-field mission is dedicated to self-supervised finetuning of a pre-trained convolutional neural network (CNN). Leveraging a real-world vision-based regression task, we thoroughly explore performance-cost trade-offs of the fine-tuning phase along three axes: i) dataset size (more data increases the regression performance but requires more memory and longer computation); ii) methodologies (e.g., fine-tuning all model parameters vs. only a subset); and iii) self-supervision strategy. Our approach demonstrates an improvement in mean absolute error up to 30% compared to the pre-trained baseline, requiring only 22s fine-tuning on an ultra-low-power GWT GAP9 System-on-Chip. Addressing the domain shift problem via on-device learning aboard nano-drones not only marks a novel result for hardware-limited robots but lays the ground for more general advancements for the entire robotics community.},
  archive   = {C_ICRA},
  author    = {Elia Cereda and Manuele Rusci and Alessandro Giusti and Daniele Palossi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610317},
  month     = {5},
  pages     = {10118-10124},
  title     = {On-device self-supervised learning of visual perception tasks aboard hardware-limited nano-quadrotors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Representing on-orbit rendezvous and proximity operations
with fully-actuated multirotor aerial platforms. <em>ICRA</em>,
10110–10117. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ground testing is of paramount importance to verify and validate space operations and the associated control algorithms before on-orbit deployment. Although state-of-the-art facilities are capable of reproducing zero-G environment with high degree of fidelity, these infrastructures can be complemented with multi-rotors emulating free flying or free floating conditions, exploiting the similarities and analogies between both domains in terms of floating nature, attitude dynamics, and thrust-wrench relation through the mixer matrix. Furthermore, the effective workspace of the testbed can be extended to the dimensions of the flight area and the coverage of the positioning system. Therefore, this papers introduces a new way to recreate orbital motion within an indoor facility, considering the case study of trajectories derived from the Clohessy–Wiltshire equations. This advancement opens up avenues for replicating close-proximity operations between chaser and target satellites employing fully-actuated multi-rotors that allow decoupling translational and attitude dynamics.},
  archive   = {C_ICRA},
  author    = {Alessandro Garzelli and Kumud Darshan Yadav and Alessandro Scalvini and Antonio Gonzalez-Morgado and Alejandro Suarez and Anibal Ollero},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610964},
  month     = {5},
  pages     = {10110-10117},
  title     = {Representing on-orbit rendezvous and proximity operations with fully-actuated multirotor aerial platforms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Perception-and-energy-aware motion planning for UAV using
learning-based model under heteroscedastic uncertainty. <em>ICRA</em>,
10103–10109. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Global navigation satellite systems (GNSS) denied environments/conditions require unmanned aerial vehicles (UAVs) to energy-efficiently and reliably fly. To this end, this study presents perception-and-energy-aware motion planning for UAVs in GNSS-denied environments. The proposed planner solves the trajectory planning problem by optimizing a cost function consisting of two indices: the total energy consumption of a UAV and the perception quality of light detection and ranging (LiDAR) sensor mounted on the UAV. Before online navigation, a high-fidelity simulator acquires a flight dataset to learn energy consumption for the UAV and heteroscedastic uncertainty associated with LiDAR measurements, both as functions of the horizontal velocity of the UAV. The learned models enable the online planner to estimate energy consumption and perception quality, reducing UAV battery usage and localization errors. Simulation experiments in a photorealistic environment confirm that the proposed planner can address the trade-off between energy efficiency and perception quality under heteroscedastic uncertainty. The open-source code is released at https://gitlab.com/ReI08/perception-energy-planner.},
  archive   = {C_ICRA},
  author    = {Reiya Takemura and Genya Ishigami},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610390},
  month     = {5},
  pages     = {10103-10109},
  title     = {Perception-and-energy-aware motion planning for UAV using learning-based model under heteroscedastic uncertainty},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On experimental emulation of printability and fleet aware
generic mesh decomposition for enabling aerial 3D printing.
<em>ICRA</em>, 10080–10086. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This article introduces an experimental emulation of a novel chunk-based flexible multi-DoF aerial 3D printing framework. The experimental demonstration of the overall autonomy focuses on precise motion planning and task allocation for a UAV, traversing through a series of planned space-filling paths involved in the aerial 3D printing process without physically depositing the overlaying material. The flexible multi-DoF aerial 3D printing is a newly developed framework and has the potential to strategically distribute the envisioned 3D model to be printed into small, manageable chunks suitable for distributed 3D printing. Moreover, by harnessing the dexterous flexibility due to the 6 DoF motion of UAV, the framework enables the provision of integrating the overall autonomy stack, potentially opening up an entirely new frontier in additive manufacturing. However, it’s essential to note that the feasibility of this pioneering concept is still in its very early stage of development, which yet needs to be experimentally verified. Towards this direction, experimental emulation serves as the crucial stepping stone, providing a pseudo mockup scenario by virtual material deposition, helping to identify technological gaps from simulation to reality. Experimental emulation results, supported by critical analysis and discussion, lay the foundation for addressing the technological and research challenges to significantly push the boundaries of the state-of-the-art 3D printing mechanism. - Full mission video available at https://youtu.be/gfZuYCA8jAw},
  archive   = {C_ICRA},
  author    = {Marios-Nektarios Stamatopoulos and Avijit Banerjee and George Nikolakopoulos},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610806},
  month     = {5},
  pages     = {10080-10086},
  title     = {On experimental emulation of printability and fleet aware generic mesh decomposition for enabling aerial 3D printing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An NMPC framework for tracking and releasing a
cable-suspended load to a ground target using a multirotor UAV.
<em>ICRA</em>, 10057–10063. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we present a nonlinear Model Predictive Control (NMPC) scheme for tracking a ground target using a multirotor with a cable-suspended load. The NMPC framework relies on the dynamic model of the UAV with the suspended load and, hence, an estimate of the load state is obtained by fusing the measurements of a downward-facing camera and a load cell with an Unscented Kalman Filter (UKF). Additionally, since the NMPC relies on the future behavior of the system, the trajectory of the ground target throughout the predicted time horizon of the NMPC, is required. Towards this direction, Bézier curves are employed in order to predict the future trajectory of the target, which moves in an arbitrary way. The ultimate goal of the proposed framework is to release the suspended load to the ground target and, consequently, a condition is checked at each time instant that triggers the opening of a gripper, located at the lower edge of the cable. The performance of the proposed control scheme is experimentally validated using an octorotor.},
  archive   = {C_ICRA},
  author    = {Fotis Panetsos and George C. Karras and Kostas J. Kyriakopoulos},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610034},
  month     = {5},
  pages     = {10057-10063},
  title     = {An NMPC framework for tracking and releasing a cable-suspended load to a ground target using a multirotor UAV},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and evaluation of motion planners for quadrotors in
environments with varying complexities. <em>ICRA</em>, 10033–10039. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion planning techniques for quadrotors have advanced significantly over the past decade. Most successful planners have two stages: a front-end that determines a path that incorporates geometric (or kinematic or input) constraints and specifies the homotopy class of the trajectory, and a back-end that optimizes this path to respect dynamics and input constraints. While there are many different choices for each stage, the eventual performance depends critically not only on these choices, but also on the environment. Given a new environment, it is difficult to decide a priori how one should design a motion planner. In this work, we develop (i) a procedure to construct parametrized environments, (ii) metrics that characterize the difficulty of motion planning in these environments, and (iii) an open-source software stack that can be used to combine a wide variety of two-stage planners seamlessly. We perform experiments in simulations and a real platform. We find, somewhat conveniently, that geometric front-ends are sufficient for environments with varying complexities if combined with dynamics-aware backends. The metrics we designed faithfully capture the planning difficulty in a given environment. All code is available at https://github.com/KumarRobotics/kr_mp_design.},
  archive   = {C_ICRA},
  author    = {Yifei Simon Shao and Yuwei Wu and Laura Jarin-Lipschitz and Pratik Chaudhari and Vijay Kumar},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610207},
  month     = {5},
  pages     = {10033-10039},
  title     = {Design and evaluation of motion planners for quadrotors in environments with varying complexities},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Light-weight approach for safe landing in populated areas.
<em>ICRA</em>, 10027–10032. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Landing safety is a challenge heavily engaging the research community recently, due to the increasing interest in applications availed by aerial vehicles. In this paper, we propose a landing safety pipeline based on state of the art object detectors and OctoMap. First, a point cloud of surface obstacles is generated, which is then inserted in an OctoMap. The unoccupied areas are identified, thus resulting to a sum of safe landing points. Due to the low processing time achieved by state of the art object detectors and the efficient point cloud manipulation using OctoMap, it is feasible for our approach to deploy on low-weight embedded systems. The proposed pipeline has been evaluated in many simulation scenarios, varying in people density, number, and movement. Simulations were executed with an Nvidia Jetson Nano in the loop to confirm the pipeline’s performance and robustness in a low computing power hardware. The experiments yielded promising results with a 87% success rate.},
  archive   = {C_ICRA},
  author    = {Tilemachos Mitroudas and Vasiliki Balaska and Athanasios Psomoulis and Antonios Gasteratos},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611639},
  month     = {5},
  pages     = {10027-10032},
  title     = {Light-weight approach for safe landing in populated areas},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Underwater volumetric mapping using imaging sonar and
free-space modeling approach. <em>ICRA</em>, 10020–10026. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Lack of information and perceptual ambiguity are key problems in sonar-based mapping applications. We propose a technique for mapping of underwater environments, building on the finite, positive, sonar beamwidth. Our approach models the free-space covered by each emitted acoustic pulse, employing volumetric techniques to create grid-based submaps of the unoccupied water volumes through images collected from imaging sonars. A representation of the occupied space is obtained by exploration of the free-space frontier. Special attention is given to acoustic image preparation and segmentation. Experimental results are provided based on real data collected from a dam shaft scenario.},
  archive   = {C_ICRA},
  author    = {António J. Oliveira and Bruno M. Ferreira and Nuno A. Cruz},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611082},
  month     = {5},
  pages     = {10020-10026},
  title     = {Underwater volumetric mapping using imaging sonar and free-space modeling approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing visual inertial SLAM with magnetic measurements.
<em>ICRA</em>, 10012–10019. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an extension to visual inertial odometry (VIO) by introducing tightly-coupled fusion of magnetometer measurements. A sliding window of keyframes is optimized by minimizing re-projection errors, relative inertial errors, and relative magnetometer orientation errors. The results of IMU orientation propagation are used to efficiently transform magnetometer measurements between frames producing relative orientation constraints between consecutive frames. The soft and hard iron effects are calibrated using an ellipsoid fitting algorithm. The introduction of magnetometer data results in significant reductions in the orientation error and also in recovery of the true yaw orientation with respect to the magnetic north. The proposed framework operates in all environments with slow-varying magnetic fields, mainly outdoors and underwater. We have focused our work on the underwater domain, especially in underwater caves, as the narrow passage and turbulent flow make it difficult to perform loop closures and reset the localization drift. The underwater caves present challenges to VIO due to the absence of ambient light and the confined nature of the environment, while also being a crucial source of fresh water and providing valuable historical records. Experimental results from underwater caves demonstrate the improvements in accuracy and robustness introduced by the proposed VIO extension.},
  archive   = {C_ICRA},
  author    = {Bharat Joshi and Ioannis Rekleitis},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611341},
  month     = {5},
  pages     = {10012-10019},
  title     = {Enhancing visual inertial SLAM with magnetic measurements},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An online self-calibrating refractive camera model with
application to underwater odometry. <em>ICRA</em>, 10005–10011. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents a camera model for refractive media such as water and its application in underwater visual-inertial odometry. The model is self-calibrating in real-time and is free of known correspondences or calibration targets. It is separable as a distortion model (dependent on refractive index n and radial pixel coordinate) and a virtual pinhole model (as a function of n). We derive the self-calibration formulation leveraging epipolar constraints to estimate the refractive index and subsequently correct for distortion. Through experimental studies using an underwater robot integrating cameras and inertial sensing, the model is validated regarding the accurate estimation of the refractive index and its benefits for robust odometry estimation in an extended envelope of conditions. Lastly, we show the transition between media and the estimation of the varying refractive index online, thus allowing computer vision tasks across refractive media.},
  archive   = {C_ICRA},
  author    = {Mohit Singh and Mihir Dharmadhikari and Kostas Alexis},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610643},
  month     = {5},
  pages     = {10005-10011},
  title     = {An online self-calibrating refractive camera model with application to underwater odometry},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive landmark color for AUV docking in visually dynamic
environments. <em>ICRA</em>, 9990–9996. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous Underwater Vehicles (AUVs) conduct missions underwater without the need for human intervention. A docking station (DS) can extend mission times of an AUV by providing a location for the AUV to recharge its batteries and receive updated mission information. Various methods for locating and tracking a DS exist, but most rely on expensive acoustic sensors, or are vision-based, which is significantly affected by water quality. In this paper, we present a vision-based method that utilizes adaptive color LED markers and dynamic color filtering to maximize landmark visibility in varying water conditions. Both AUV and DS utilize cameras to determine the water background color in order to calculate the desired marker color. No communication between AUV and DS is needed to determine marker color. Experiments conducted in a pool and lake show our method performs 10 times better than static color thresholding methods as background color varies. DS detection is possible at a range of 5 meters in clear water with minimal false positives.},
  archive   = {C_ICRA},
  author    = {Corey Knutson and Zhipeng Cao and Junaed Sattar},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611083},
  month     = {5},
  pages     = {9990-9996},
  title     = {Adaptive landmark color for AUV docking in visually dynamic environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vision-based water clearance determination in maritime
environment. <em>ICRA</em>, 9983–9989. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Determining the distances from the hull of the own ship to obstacles or land, i.e. water clearance, is a fundamental task in navigation. This is particularly relevant during maneuvering in the harbor or navigating in confined waters. We introduce the concepts of area water clearance and line water clearance. Area water clearance is important especially for path planning and obstacle avoidance. Line water clearance is critical for maneuvering when approaching the quay.In this work, we present a vision-based approach to determine the water clearance. A single calibrated camera together with a semantic segmentation network is used to detect the water region in an image, and back-projection to determine the water clearance on the sea surface in world units.We validate the proposed approach on real data collected from two distinct vessels, where the proposed method is able to produce reliable water clearance for distances beyond one kilometer. During harbor maneuvering 90% of the relative water clearance errors were found to be between −2.3% and 3%.},
  archive   = {C_ICRA},
  author    = {Carl H. Schiller and Deran Maas and Bruno Arsenali and Jukka Peltola and Kalevi Tervo and Stefano Maranò},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610182},
  month     = {5},
  pages     = {9983-9989},
  title     = {Vision-based water clearance determination in maritime environment},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Boundary factors for seamless state estimation between
autonomous underwater docking phases. <em>ICRA</em>, 9976–9982. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous underwater docking is of the utmost importance for expanding the capabilities of Autonomous Underwater Vehicles (AUVs). Due to a historical focus on underwater docking to only static targets, the research gap in underwater docking to dynamically active targets has been left relatively untouched. We address the state estimation problem that arises when trying to rendezvous a chaser AUV with a dynamic target by modeling the scenario as a factor graph optimization-based Simultaneous Localization and Mapping problem. We present a set of boundary factors that aid the inference process by seamlessly transitioning the target’s state between the different observability stages, intrinsic to any dynamic docking scenario. We benchmark the performance of our approach using the Stonefish simulated environment.},
  archive   = {C_ICRA},
  author    = {Aldo Terán Espinoza and Antonio Terán Espinoza and John Folkesson and Peter Sigray and Jakob Kuttenkeuler},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611552},
  month     = {5},
  pages     = {9976-9982},
  title     = {Boundary factors for seamless state estimation between autonomous underwater docking phases},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An open-source solution for fast and accurate underwater
mapping with a low-cost mechanical scanning sonar. <em>ICRA</em>,
9968–9975. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An open-source software framework is presented that allows real-time underwater mapping with popular marine robotics components, namely a BlueRobotics BlueROV2 with its standard Ping360 Mechanical Scanning Sonar (MSS) and a A50 Doppler Velocity Log (DVL), which are low-cost devices for their respective types - if not even the most affordable ones on the market. The software runs with low computational power on a Raspberry Pi4. The framework builds upon Synthetic Scan Formation (SSF) where single MSS beams or scan-lines are embedded into a pose-graph. The rendering of scans is not only based on navigation, but based on the graph itself. Scans formed from scan-lines can be optimized by online Simultaneous Localization and Mapping (SLAM) and result in improved scans, based on the current state of the graph. In subsequent steps this leads to improved registration results. To this end, a combination of two different types of loop-closures is presented. Namely a consecutive loop closure, and a proximity based loop closure, which reduces the overall drift. The framework is validated in three different test-environments, namely a pool, a test-tank with a gantry for ground truth motion, and the flooded basement of a WW-II submarine bunker. Among others, it is shown that there is an increased accuracy compared to conventional SLAM and that the software is usable in real-time during a mission with the low-cost hardware.},
  archive   = {C_ICRA},
  author    = {Tim Hansen and Andreas Birk},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609976},
  month     = {5},
  pages     = {9968-9975},
  title     = {An open-source solution for fast and accurate underwater mapping with a low-cost mechanical scanning sonar},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Detecting and mitigating system-level anomalies of
vision-based controllers. <em>ICRA</em>, 9953–9959. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous systems, such as self-driving cars and drones, have made significant strides in recent years by leveraging visual inputs and machine learning for decision-making and control. Despite their impressive performance, these vision-based controllers can make erroneous predictions when faced with novel or out-of-distribution inputs. Such errors can cascade to catastrophic system failures and compromise system safety. In this work, we introduce a run-time anomaly monitor to detect and mitigate such closed-loop, system-level failures. Specifically, we leverage a reachability-based framework to stress-test the vision-based controller offline and mine its system-level failures. This data is then used to train a classifier that is leveraged online to flag inputs that might cause system breakdowns. The anomaly detector highlights issues that transcend individual modules and pertain to the safety of the overall system. We also design a fallback controller that robustly handles these detected anomalies to preserve system safety. We validate the proposed approach on an autonomous aircraft taxiing system that uses a vision-based controller for taxiing. Our results show the efficacy of the proposed approach in identifying and handling system-level anomalies, outperforming methods such as prediction error-based detection, and ensembling, thereby enhancing the overall safety and robustness of autonomous systems. Website: phoenixrider12.github.io/FailureMitigation},
  archive   = {C_ICRA},
  author    = {Aryaman Gupta and Kaustav Chakraborty and Somil Bansal},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611397},
  month     = {5},
  pages     = {9953-9959},
  title     = {Detecting and mitigating system-level anomalies of vision-based controllers},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards standardized disturbance rejection testing of legged
robot locomotion with linear impactor: A preliminary study,
observations, and implications. <em>ICRA</em>, 9946–9952. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dynamic locomotion in legged robots is close to industrial collaboration, but a lack of standardized testing obstructs commercialization. The issues are not merely political, theoretical, or algorithmic but also physical, indicating limited studies and comprehension regarding standard testing infrastructure and equipment. For decades, the approaches we have been testing legged robots were rarely standardizable with hand-pushing, foot-kicking, rope-dragging, stick-poking, and ball-swinging. This paper aims to bridge the gap by proposing the use of the linear impactor, a well-established tool in other standardized testing disciplines, to serve as an adaptive, repeatable, and fair disturbance rejection testing equipment for legged robots. A pneumatic linear impactor is also adopted for the case study involving the humanoid robot Digit. Three locomotion controllers are examined, including a commercial one, using a walking-in-place task against frontal impacts. The statistically best controller was able to withstand the impact momentum (26.376 kg • m/s) on par with a reported average effective momentum from straight punches by Olympic boxers (26.506kg•m/s). Moreover, the case study highlights other anti-intuitive observations, demonstrations, and implications that, to the best of the authors’ knowledge, are first-of-its-kind revealed in real-world testing of legged robots.},
  archive   = {C_ICRA},
  author    = {Bowen Weng and Guillermo A. Castillo and Yun-Seok Kang and Ayonga Hereid},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610064},
  month     = {5},
  pages     = {9946-9952},
  title     = {Towards standardized disturbance rejection testing of legged robot locomotion with linear impactor: A preliminary study, observations, and implications},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online data-driven safety certification for systems subject
to unknown disturbances. <em>ICRA</em>, 9939–9945. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deploying autonomous systems in safety critical settings necessitates methods to verify their safety properties. This is challenging because real-world systems may be subject to disturbances that affect their performance, but are unknown a priori. This work develops a safety-verification strategy wherein data is collected online and incorporated into a reachability analysis approach to check in real-time that the system avoids dangerous regions of the state space. Specifically, we employ an optimization-based moving horizon estimator (MHE) to characterize the disturbance affecting the system, which is incorporated into an online reachability calculation. Reachable sets are calculated using a computational graph analysis tool to predict the possible future states of the system and verify that they satisfy safety constraints. We include theoretical arguments proving our approach generates reachable sets that bound the future states of the system, as well as numerical results demonstrating how it can be used for safety verification. Finally, we present results from hardware experiments demonstrating our approach’s ability to perform online reachability calculations for an unmanned surface vehicle subject to currents and actuator failures.},
  archive   = {C_ICRA},
  author    = {Nicholas Rober and Karan Mahesh and Tyler M. Paine and Max L. Greene and Steven Lee and Sildomar T. Monteiro and Michael R. Benjamin and Jonathan P. How},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610163},
  month     = {5},
  pages     = {9939-9945},
  title     = {Online data-driven safety certification for systems subject to unknown disturbances},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Achieving autonomous cloth manipulation with optimal control
via differentiable physics-aware regularization and safety constraints.
<em>ICRA</em>, 9931–9938. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cloth manipulation is a category of deformable object manipulation of great interest to the robotics community, from applications of automated laundry-folding and home organizing to textiles and flexible manufacturing. Despite the desire for automated cloth manipulation, the thin-shell dynamics and under-actuation nature of cloth present significant challenges for robots to effectively interact with them. Many recent works omit explicit modeling in favor of learning-based methods that may yield control policies directly. However, these methods require large training sets that must be collected and curated. In this regard, we create a framework for differentiable modeling of cloth dynamics leveraging an Extended Position-based Dynamics (XPBD) algorithm. Together with the desired control objective, physics-aware regularization terms are designed for better results, including trajectory smoothness and elastic potential energy. In addition, safety constraints, such as avoiding obstacles, can be specified using signed distance functions (SDFs). We formulate the cloth manipulation task with safety constraints as a constrained optimization problem, which can be effectively solved by mainstream gradient-based optimizers thanks to the end-to-end differentiability of our framework. Finally, we assess the framework with various safety thresholds and demonstrate the feasibility of result trajectories on a surgical robot. The effects of the regularization terms are analyzed in an additional ablation study.},
  archive   = {C_ICRA},
  author    = {Yutong Zhang and Fei Liu and Xiao Liang and Michael Yip},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611111},
  month     = {5},
  pages     = {9931-9938},
  title     = {Achieving autonomous cloth manipulation with optimal control via differentiable physics-aware regularization and safety constraints},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel algorithmic approach to obtaining maneuverable
control-invariant sets *. <em>ICRA</em>, 9916–9922. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ensuring safety in autonomous systems is essential as they become more integrated with modern society. One way to accomplish this is to identify and maintain a safe operating space. To this end, much effort has been devoted in the field of reachability analysis to obtaining control-invariant sets which ensure that a system inside of these sets can remain in these sets, and are thus essential for guaranteeing a system’s safety. However, control invariance does not imply that a system can move from any state in the control-invariant set to any other state in the control-invariant set, within a given time horizon. In this paper, we develop an algorithm to obtain a control-invariant set that allows a given system to move from any state in the set to any other state in the set within a given time horizon without having to leave the set. We call this the ‘maneuver set’, $\mathcal{M}$. We substantiate the algorithm’s efficacy through mathematical proof, affirming that the maneuver set obtained through the algorithm is indeed control-invariant. Furthermore, we prove that the system is indeed able to move from any state within this set to any other state in the set. To illustrate the use of our algorithm, we provide the numerical example of a Dubins car, utilising Hamilton-Jacobi-Bellman reachability analysis along with the proposed algorithm in order to obtain $\mathcal{M}$.},
  archive   = {C_ICRA},
  author    = {Prashant Solanki and Jasper J. van Beers and Anahita Jamshidnejad and Coen C. de Visser},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610609},
  month     = {5},
  pages     = {9916-9922},
  title     = {A novel algorithmic approach to obtaining maneuverable control-invariant sets *},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Fault tolerant neural control barrier functions for robotic
systems under sensor faults and attacks. <em>ICRA</em>, 9901–9907. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safety is a fundamental requirement of many robotic systems. Control barrier function (CBF)-based approaches have been proposed to guarantee the safety of robotic systems. However, the effectiveness of these approaches highly relies on the choice of CBFs. Inspired by the universal approximation power of neural networks, there is a growing trend toward representing CBFs using neural networks, leading to the notion of neural CBFs (NCBFs). Current NCBFs, however, are trained and deployed in benign environments, making them ineffective for scenarios where robotic systems experience sensor faults and attacks. In this paper, we study safety-critical control synthesis for robotic systems under sensor faults and attacks. Our main contribution is the development and synthesis of a new class of CBFs that we term fault tolerant neural control barrier function (FT-NCBF). We derive the necessary and sufficient conditions for FT-NCBFs to guarantee safety, and develop a data-driven method to learn FT-NCBFs by minimizing a loss function constructed using the derived conditions. Using the learned FT-NCBF, we synthesize a control input and formally prove the safety guarantee provided by our approach. We demonstrate our proposed approach using two case studies: obstacle avoidance problem for an autonomous mobile robot and spacecraft rendezvous problem, with code available via https://github.com/HongchaoZhang-HZ/FTNCBF.},
  archive   = {C_ICRA},
  author    = {Hongchao Zhang and Luyao Niu and Andrew Clark and Radha Poovendran},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610491},
  month     = {5},
  pages     = {9901-9907},
  title     = {Fault tolerant neural control barrier functions for robotic systems under sensor faults and attacks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analyzing accessibility in robot-assisted vitreoretinal
surgery: Integrating eye posture and robot position. <em>ICRA</em>,
9894–9900. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Several robotic frameworks have been recently developed to assist ophthalmic surgeons in performing complex vitreoretinal procedures such as subretinal injection. However, in order to intuitively integrate robots into the surgical workflow, it is crucial to emphasize that an accessibility analysis framework for vitreoretinal surgery must be considered as an essential component. Such a framework, ideally, considers the comprehensive factors of the eye anatomy and its positioning, the insertion point, and the initial pose and position of the robot. By combining the mobilization of the eyeball and adjusting the pose and position of the robot, the accessibility of such systems is significantly optimized. At the same time, the accessible-visible area is better and faster matched to the working volume of the robot. This paper presents an analysis of an expansion strategy for the robot’s accessibility and visibility area. The outcomes of this method demonstrate the promising potential to enhance the robot’s accessibility, as evidenced in our analytical and experimental findings from 22.4% to 99.0% of the required working area on an adjustable phantom model.},
  archive   = {C_ICRA},
  author    = {Satoshi Inagaki and Alireza Alikhani and Nassir Navab and Mathias Maier and M. Ali Nasseri},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611482},
  month     = {5},
  pages     = {9894-9900},
  title     = {Analyzing accessibility in robot-assisted vitreoretinal surgery: Integrating eye posture and robot position},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and evaluation of a modular robotic system for
microsurgery. <em>ICRA</em>, 9887–9893. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The manipulation of instruments under a microscope suffers from physiological tremor and human errors, which are inevitable in long microsurgery interventions. Robotic systems developed in recent years for microsurgery are expensive and not flexible, as they cannot use standard instruments, and need the surgeon to modify their operative skills and strategies. In this paper, we introduce a modular robotic system for microsurgery enabling the surgeon to operate using conventional instruments. Our system was implemented using a commercial Kinova robot and a dedicated modular end-effector that uses standard microsurgery instruments. An initial teleoperation validation was carried out by eleven participants, who could successfully control the microsurgery tools to perform basic surgical movements. Furthermore, participants performed a simple anastomosis task with the robot and compared it to manual control. The results showed that robotic control is superior to manual control in simple surgical tasks and the converse in complex tasks. Participants preferred the proposed robotic system due to its user-friendliness and effort reduction.},
  archive   = {C_ICRA},
  author    = {Jenireth Torrealba Molina and Toqa AbuBaker and Yanpei Huang and Xiaoxiao Cheng and Alexis Devillard and Etienne Burdet},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610598},
  month     = {5},
  pages     = {9887-9893},
  title     = {Design and evaluation of a modular robotic system for microsurgery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A semi-autonomous data-driven shared control framework for
robotic manipulation and cutting of an unknown deformable tissue.
<em>ICRA</em>, 9881–9886. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we propose a semi-autonomous scheme to synergistically share the complicated task of manipulation and cutting of an unknown deformable tissue (U-DT) between a remote surgeon and a surgical robot. Particularly, utilizing the da Vinci Research Kit (dVRK) platform, we have designed and successfully demonstrated a fully functional shared control scheme for an autonomous tensioning and tele-cutting of a U-DT. We have shown the system’s ability to cooperate with a remote surgeon by leveraging an online data-driven learning and adaptive control method coupled with a reduced-order trajectory planning module that depends on just two parameters. By performing 25 experiments on custom-designed silicon phantoms and defining a set of success/failure metrics, we have put forward findings that establish a causal relationship between these two important parameters and the success or failure of the performed experiments.},
  archive   = {C_ICRA},
  author    = {Nicholas A. Strohmeyer and Ji Hwan Park and Braden P. Murphy and Farshid Alambeigi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610931},
  month     = {5},
  pages     = {9881-9886},
  title     = {A semi-autonomous data-driven shared control framework for robotic manipulation and cutting of an unknown deformable tissue},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simultaneous estimation of shape and force along highly
deformable surgical manipulators using sparse FBG measurement.
<em>ICRA</em>, 9866–9872. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, fiber optic sensors such as fiber Bragg gratings (FBGs) have been widely investigated for shape reconstruction and force estimation of flexible surgical robots. However, most existing approaches need precise model parameters of FBGs inside the fiber and their alignments with the flexible robots for accurate sensing results. Another challenge lies in online acquiring external forces at arbitrary locations along the flexible robots, which is highly required when with large deflections in robotic surgery. In this paper, we propose a novel data-driven paradigm for simultaneous estimation of shape and force along highly deformable flexible robots by using sparse strain measurement from a single-core FBG fiber. A thin-walled soft sensing tube helically embedded with FBG sensors is designed for a robotic-assisted flexible ureteroscope with large deflection up to 270° and a bend radius under 10 mm. We introduce and study three learning models by incorporating spatial strain encoders, and compare their performances in both free space without interactions as well as constrained environments with contact forces at different locations. The experimental results in terms of dynamic shape-force sensing accuracy demonstrate the effectiveness and superiority of the proposed methods.},
  archive   = {C_ICRA},
  author    = {Yiang Lu and Bin Li and Wei Chen and Junyan Yan and Shing Shin Cheng and Jiangliu Wang and Jianshu Zhou and Qi Dou and Yun-Hui Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611403},
  month     = {5},
  pages     = {9866-9872},
  title     = {Simultaneous estimation of shape and force along highly deformable surgical manipulators using sparse FBG measurement},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Magnetic-guided flexible origami robot toward long-term
phototherapy of h. Pylori in the stomach. <em>ICRA</em>, 9851–9857. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Helicobacter pylori, a pervasive bacterial infection associated with gastrointestinal disorders such as gastritis, peptic ulcer disease, and gastric cancer, impacts approximately 50% of the global population. The efficacy of standard clinical eradication therapies is diminishing due to the rise of antibiotic-resistant strains, necessitating alternative treatment strategies. Photodynamic therapy (PDT) emerges as a promising prospect in this context. This study presents the development and implementation of a magnetically-guided origami robot, incorporating flexible printed circuit units for sustained and stable phototherapy of Helicobacter pylori. Each integrated unit is equipped with wireless charging capabilities, producing an optimal power output that can concurrently illuminate up to 15 LEDs at their maximum intensity. Crucially, these units can be remotely manipulated via a magnetic field, facilitating both translational and rotational movements. We propose an open-loop manual control sequence that allows the formation of a stable, compliant triangular structure through the interaction of internal magnets. This adaptable configuration is uniquely designed to withstand the dynamic squeezing environment prevalent in real-world gastric applications. The research herein represents a significant stride in leveraging technology for innovative medical solutions, particularly in the management of antibiotic-resistant Helicobacter pylori infections.},
  archive   = {C_ICRA},
  author    = {Sishen Yuan and Baijia Liang and Po Wa Wong and Mingjing Xu and Chi Hsuan Li and Zhen Li and Hongliang Ren},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611027},
  month     = {5},
  pages     = {9851-9857},
  title     = {Magnetic-guided flexible origami robot toward long-term phototherapy of h. pylori in the stomach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards a novel soft magnetic laparoscope for single
incision laparoscopic surgery. <em>ICRA</em>, 9845–9850. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In single-incision laparoscopic surgery (SILS), magnetic anchoring and guidance system (MAGS) is a promising technique to prevent clutter in the surgical workspace and provide a larger vision field. Existing camera designs mainly rely on rigid structure design, resulting in risks of losing magnetic coupling and impacting tissue during the insertion and coupling procedure. In this paper, we proposed a wireless MAGS consisting of soft material and structure design. The camera can bend at the exit of the trocar and maintain strong coupling with the external actuator. The operation principle and modeling were established to investigate the parameter design. An easier insertion procedure was introduced and demonstrated in the experiment. The bendability was tested showing the camera could reach 20° in bending angle and 16.4mm in displacement. The insertion and deployment took less than 2 minutes on average.},
  archive   = {C_ICRA},
  author    = {Hui Liu and Ning Li and Shuai Li and Gregory J. Mancini and Jindong Tan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611068},
  month     = {5},
  pages     = {9845-9850},
  title     = {Towards a novel soft magnetic laparoscope for single incision laparoscopic surgery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A distributed processing approach for smooth task
transitioning in strict hierarchical control. <em>ICRA</em>, 9830–9836.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To enhance robots’ applicability in real-world scenarios, it is essential to establish a complex and multi-tasking behaviour, inspired by human nature. To this purpose, from a hardware perspective, a high number of degrees of freedom is necessary, as is the case for humanoids and collaborative mobile manipulators. From a software standpoint instead, complex hierarchical strategies are often used to define a set of behaviours that the robot should reflect in strict hierarchical order. Their main issue however, is related to the lack of continuity when their stack of tasks is changed. Existing works that address this issue clearly present a trade-off between optimality assurance during transition and computational costs. Here, we employ a distributed processing approach that enables not only the minimization of computational costs, but also continuous optimality and constraints feasibility even under sharp transitions. The approach is tested during three task transitions, for different tasks such as constrained trajectory tracking, obstacle avoidance, and postural optimization. Two mobile manipulators are used, each having 10 DoF, and the results confirm the smoothness of the generated solutions.},
  archive   = {C_ICRA},
  author    = {Francesco Tassi and Arash Ajoudani},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610227},
  month     = {5},
  pages     = {9830-9836},
  title     = {A distributed processing approach for smooth task transitioning in strict hierarchical control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalizing trajectory retiming to quadratic objective
functions. <em>ICRA</em>, 9823–9829. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trajectory retiming is the task of computing a feasible time parameterization to traverse a path. It is commonly used in the decoupled approach to trajectory optimization whereby a path is first found, then a retiming algorithm computes a speed profile that satisfies kino-dynamic and other constraints. While trajectory retiming is most often formulated with the minimum-time objective (i.e. traverse the path as fast as possible), it is not always the most desirable objective, particularly when we seek to balance multiple objectives or when bang-bang control is unsuitable. In this paper, we present a novel algorithm based on factor graph variable elimination that can solve for the global optimum of the retiming problem with quadratic objectives as well (e.g. minimize control effort or match a nominal speed by minimizing squared error), which may extend to arbitrary objectives with iteration. Our work extends prior works, which find only solutions on the boundary of the feasible region, while maintaining the same linear time complexity from a single forward-backward pass. We experimentally demonstrate that (1) we achieve better real-world robot performance by using quadratic objectives in place of the minimum-time objective, and (2) our implementation is comparable or faster than state-of-the-art retiming algorithms.},
  archive   = {C_ICRA},
  author    = {Gerry Chen and Frank Dellaert and Seth Hutchinson},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610854},
  month     = {5},
  pages     = {9823-9829},
  title     = {Generalizing trajectory retiming to quadratic objective functions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MPCGPU: Real-time nonlinear model predictive control through
preconditioned conjugate gradient on the GPU. <em>ICRA</em>, 9787–9794.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Nonlinear Model Predictive Control (NMPC) is a state-of-the-art approach for locomotion and manipulation which leverages trajectory optimization at each control step. While the performance of this approach is computationally bounded, implementations of direct trajectory optimization that use iterative methods to solve the underlying moderately-large and sparse linear systems, are a natural fit for parallel hardware acceleration. In this work, we introduce MPCGPU, a GPU-accelerated, real-time NMPC solver that leverages an accelerated preconditioned conjugate gradient (PCG) linear system solver at its core. We show that MPCGPU increases the scalability and real-time performance of NMPC, solving larger problems, at faster rates. In particular, for tracking tasks using the Kuka IIWA manipulator, MPCGPU is able to scale to kilohertz control rates with trajectories as long as 512 knot points. This is driven by a custom PCG solver which outperforms state-of-the-art, CPU-based, linear system solvers by at least 10x for a majority of solves and 3.6x on average.},
  archive   = {C_ICRA},
  author    = {Emre Adabag and Miloni Atal and William Gerard and Brian Plancher},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611212},
  month     = {5},
  pages     = {9787-9794},
  title     = {MPCGPU: Real-time nonlinear model predictive control through preconditioned conjugate gradient on the GPU},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Symmetric stair preconditioning of linear systems for
parallel trajectory optimization. <em>ICRA</em>, 9779–9786. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There has been a growing interest in parallel strategies for solving trajectory optimization problems. One key step in many algorithmic approaches to trajectory optimization is the solution of moderately-large and sparse linear systems. Iterative methods are particularly well-suited for parallel solves of such systems. However, fast and stable convergence of iterative methods is reliant on the application of a high-quality preconditioner that reduces the spread and increase the clustering of the eigenvalues of the target matrix. To improve the performance of these approaches, we present a new parallel-friendly symmetric stair preconditioner. We prove that our preconditioner has advantageous theoretical properties when used in conjunction with iterative methods for trajectory optimization such as a more clustered eigenvalue spectrum. Numerical experiments with typical trajectory optimization problems reveal that as compared to the best alternative parallel preconditioner from the literature, our symmetric stair preconditioner provides up to a 34% reduction in condition number and up to a 25% reduction in the number of resulting linear system solver iterations.},
  archive   = {C_ICRA},
  author    = {Xueyi Bu and Brian Plancher},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610386},
  month     = {5},
  pages     = {9779-9786},
  title     = {Symmetric stair preconditioning of linear systems for parallel trajectory optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the performance of jerk-constrained time-optimal
trajectory planning for industrial manipulators. <em>ICRA</em>,
9772–9778. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Jerk-constrained trajectories offer a wide range of advantages that collectively improve the performance of robotic systems, including increased energy efficiency, durability, and safety. In this paper, we present a novel approach to jerk-constrained time-optimal trajectory planning (TOTP), which follows a specified path while satisfying up to third-order constraints to ensure safety and smooth motion. One significant challenge in jerk-constrained TOTP is a non-convex formulation arising from the inclusion of third-order constraints. Approximating inequality constraints can be particularly challenging because the resulting solutions may violate the actual constraints. We address this problem by leveraging convexity within the proposed formulation to form conservative inequality constraints. We then obtain the desired trajectories by solving an n-dimensional Sequential Linear Program (SLP) iteratively until convergence. Lastly, we evaluate in a real robot the performance of trajectories generated with and without jerk limits in terms of peak power, torque efficiency, and tracking capability.},
  archive   = {C_ICRA},
  author    = {Jee-Eun Lee and Andrew Bylard and Robert Sun and Luis Sentis},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610437},
  month     = {5},
  pages     = {9772-9778},
  title     = {On the performance of jerk-constrained time-optimal trajectory planning for industrial manipulators},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Motion planning for 4WS vehicle with autonomous selection of
steering modes via an MIQP-MPC controller. <em>ICRA</em>, 9765–9771. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Navigation in agricultural fields imposes various constraints on manoeuvrability, which can be tackled by using four-wheel steering (4WS) vehicles which are capable of switching between multiple steering mechanisms with distinct kinematic properties. For example, parallel positive steering (PPS) with four wheels in parallel to each other can maintain the vehicle’s heading when moving along a curve. Symmetric negative steering (SNS) with two wheels on each side sharing the same steering angle can turn with a small radius. This paper presents a controller capable of selecting and switching between the two aforementioned modes autonomously for better trajectory tracking performance with special heading requirements for agricultural applications. The controller is implemented as a Model Predictive Control (MPC) controller formulated as a mixed-integer quadratic programming (MIQP) problem for the 4WS vehicle. Practical constraints, such as limits on wheel velocities, steering angles and their rate-of-changes are taken into account. A Python implementation confirms the real-time execution capability of the controller and simulation results highlight its effectiveness.},
  archive   = {C_ICRA},
  author    = {Ngoc Thinh Nguyen and Pranav Tej Gangavarapu and Nicolas Mandel and Ralf Bruder and Floris Ernst},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610461},
  month     = {5},
  pages     = {9765-9771},
  title     = {Motion planning for 4WS vehicle with autonomous selection of steering modes via an MIQP-MPC controller},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-task learning of active fault-tolerant controller for
leg failures in quadruped robots. <em>ICRA</em>, 9758–9764. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Electric quadruped robots used in outdoor exploration are susceptible to leg-related electrical or mechanical failures. Unexpected joint power loss and joint locking can immediately pose a falling threat. Typically, controllers lack the capability to actively sense the condition of their own joints and take proactive actions. Maintaining the original motion patterns could lead to disastrous consequences, as the controller may produce irrational output within a short period of time, further creating the risk of serious physical injuries. This paper presents a hierarchical fault-tolerant control scheme employing a multi-task training architecture capable of actively perceiving and overcoming two types of leg joint faults. The architecture simultaneously trains three joint task policies for health, power loss, and locking scenarios in parallel, introducing a symmetric reflection initialization technique to ensure rapid and stable gait skill transformations. Experiments demonstrate that the control scheme is robust in unexpected scenarios where a single leg experiences concurrent joint faults in two joints. Furthermore, the policy retains the robot’s planar mobility, enabling rough velocity tracking. Finally, zero-shot Sim2Real transfer is achieved on the real-world SOLO8 robot, countering both electrical and mechanical failures.},
  archive   = {C_ICRA},
  author    = {Taixian Hou and Jiaxin Tu and Xiaofei Gao and Zhiyan Dong and Peng Zhai and Lihua Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610151},
  month     = {5},
  pages     = {9758-9764},
  title     = {Multi-task learning of active fault-tolerant controller for leg failures in quadruped robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning quadrupedal locomotion with impaired joints using
random joint masking. <em>ICRA</em>, 9751–9757. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Quadrupedal robots have played a crucial role in various environments, from structured environments to complex harsh terrains, thanks to their agile locomotion ability. However, these robots can easily lose their locomotion functionality if damaged by external accidents or internal malfunctions. In this paper, we propose a novel deep reinforcement learning framework to enable a quadrupedal robot to walk with impaired joints. The proposed framework consists of three components: 1) a random joint masking strategy for simulating impaired joint scenarios, 2) a joint state estimator to predict an implicit status of current joint condition based on past observation history, and 3) progressive curriculum learning to allow a single network to conduct both normal gait and various joint-impaired gaits. We verify that our framework enables the Unitree’s Go1 robot to walk under various impaired joint conditions in real-world indoor and outdoor environments.},
  archive   = {C_ICRA},
  author    = {Mincheol Kim and Ukcheol Shin and Jung-Yup Kim},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610088},
  month     = {5},
  pages     = {9751-9757},
  title     = {Learning quadrupedal locomotion with impaired joints using random joint masking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LAGOON: Language-guided motion control. <em>ICRA</em>,
9743–9750. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We aim to control a robot to physically behave in the real world following any high-level language command like &quot;cartwheel&quot; or &quot;kick&quot;. Although human motion datasets exist, this task remains particularly challenging since generative models can produce physically unrealistic motions, which will be more severe for robots due to different body structures and physical properties. Deploying such a motion to a physical robot can cause even greater difficulties due to the sim2real gap. We develop LAnguage-Guided mOtion cONtrol (LAGOON), a multi-phase reinforcement learning (RL) method to generate physically realistic robot motions under language commands. LAGOON first leverages a pretrained model to generate a human motion from a language command. Then an RL phase trains a control policy in simulation to mimic the generated human motion. Finally, with domain randomization, our learned policy can be deployed to a quadrupedal robot, leading to a quadrupedal robot that can take diverse behaviors in the real world under natural language commands.},
  archive   = {C_ICRA},
  author    = {Shusheng Xu and Huaijie Wang and Yutao Ouyang and Jiaxuan Gao and Zhiyu Mei and Chao Yu and Yi Wu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610467},
  month     = {5},
  pages     = {9743-9750},
  title     = {LAGOON: Language-guided motion control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning agile bipedal motions on a quadrupedal robot.
<em>ICRA</em>, 9735–9742. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Can a quadrupedal robot perform bipedal motions like humans? Although developing human-like behaviors is more often studied on costly bipedal robot platforms, we present a solution over a lightweight quadrupedal robot that unlocks the agility of the quadruped in an upright standing pose and is capable of a variety of human-like motions. Our framework is with a hierarchical structure. At the low level is a motion-conditioned control policy that allows the quadrupedal robot to track desired base and front limb movements while balancing on two hind feet. The policy is commanded by a high-level motion generator that gives trajectories of parameterized human-like motions to the robot from multiple modalities of human input. We for the first time demonstrate various bipedal motions on a quadrupedal robot, and showcase interesting human-robot interaction modes including mimicking human videos, following natural language instructions, and physical interaction. The video is available at https://sites.google.com/view/bipedal-motions-quadruped.},
  archive   = {C_ICRA},
  author    = {Yunfei Li and Jinhan Li and Wei Fu and Yi Wu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611442},
  month     = {5},
  pages     = {9735-9742},
  title     = {Learning agile bipedal motions on a quadrupedal robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Expert composer policy: Scalable skill repertoire for
quadruped robots. <em>ICRA</em>, 9727–9734. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose the expert composer policy, a framework to reliably expand the skill repertoire of quadruped agents. The composer policy links pair of experts via transitions to a sampled target state, allowing experts to be composed sequentially. Each expert specializes in a single skill, such as a locomotion gait or a jumping motion. Instead of a hierarchical or mixture-of-experts architecture, we train a single composer policy in an independent process that is not conditioned on the other expert policies. By reusing the same composer policy, our approach enables adding new experts without affecting existing ones, enabling incremental repertoire expansion and preserving original motion quality. We measured the transition success rate of 72 transition pairs and achieved an average success rate of 99.99%, which is over 10% higher than the baseline random approach, and outperforms other state-of-the-art methods. Using domain randomization during training we ensure a successful transfer to the real world, where we achieve an average transition success rate of 97.22% (N=360) in our experiments.},
  archive   = {C_ICRA},
  author    = {Guilherme Christmann and Ying-Sheng Luo and Wei-Chao Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611039},
  month     = {5},
  pages     = {9727-9734},
  title     = {Expert composer policy: Scalable skill repertoire for quadruped robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two-stage learning of highly dynamic motions with rigid and
articulated soft quadrupeds. <em>ICRA</em>, 9720–9726. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Controlled execution of dynamic motions in quadrupedal robots, especially those with articulated soft bodies, presents a unique set of challenges that traditional methods struggle to address efficiently. In this study, we tackle these issues by relying on a simple yet effective two-stage learning framework to generate dynamic motions for quadrupedal robots. First, a gradient-free evolution strategy is employed to discover simply represented control policies, eliminating the need for a predefined reference motion. Then, we refine these policies using deep reinforcement learning. Our approach enables the acquisition of complex motions like pronking and back-flipping, effectively from scratch. Additionally, our method simplifies the traditionally labour-intensive task of reward shaping, boosting the efficiency of the learning process. Importantly, our framework proves particularly effective for articulated soft quadrupeds, whose inherent compliance and adaptability make them ideal for dynamic tasks but also introduce unique control challenges.},
  archive   = {C_ICRA},
  author    = {Francecso Vezzi and Jiatao Ding and Antonin Raffin and Jens Kober and Cosimo Della Santina},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610561},
  month     = {5},
  pages     = {9720-9726},
  title     = {Two-stage learning of highly dynamic motions with rigid and articulated soft quadrupeds},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Manipulator as a tail: Promoting dynamic stability for
legged locomotion. <em>ICRA</em>, 9712–9719. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For locomotion, is an arm on a legged robot a liability or an asset for locomotion? Biological systems evolved additional limbs beyond legs that facilitates postural control. This work shows how a manipulator can be an asset for legged locomotion at high speeds or under external perturbations, where the arm serves beyond manipulation. Since the system has 15 degrees of freedom (twelve for the legged robot and three for the arm), off-the-shelf reinforcement learning (RL) algorithms struggle to learn effective locomotion policies. Inspired by Bernstein’s neurophysiological theory of animal motor learning, we develop an incremental training procedure that initially freezes some degrees of freedom and gradually releases them, using behaviour cloning (BC) from an early learning procedure to guide optimization in later learning. Simulation experiments show that our policy increases the success rate by up to 61 percentage points over the baselines. Simulation and real robot experiments suggest that our policy learns to use the arm as a &quot;tail&quot; to initiate robot turning at high speeds and to stabilize the quadruped under external perturbations. Quantitatively, in simulation experiments, we cut the failure rate up to 43.6% during high-speed turning and up to 31.8% for quadruped under external forces compared to using a locked arm.},
  archive   = {C_ICRA},
  author    = {Huang Huang and Antonio Loquercio and Ashish Kumar and Neerja Thakkar and Ken Goldberg and Jitendra Malik},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610049},
  month     = {5},
  pages     = {9712-9719},
  title     = {Manipulator as a tail: Promoting dynamic stability for legged locomotion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-stable curved line shape display. <em>ICRA</em>,
9696–9703. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Shape-changing displays enable real-time visualization and haptic exploration of 3D surfaces. However, many shape-changing displays are composed of individually actuated rigid bodies, which makes them both mechanically complex and unable to form smooth surfaces. In this work, we build a multi-stable curved line display inspired by physical splines. By using circular splines to initialize a discrete elastic rods simulator, we can model multiple stable shapes that fit specific boundary conditions. We then generate actuation instructions based on the circular spline initialization to drive the physical display. We demonstrate our display’s ability to create 16 shapes with 8 different boundary conditions. Our display is consistent in shape output, with an average standard deviation in height of 0.75 mm or 0.47% of the display’s maximum vertical range. We also show that our model is consistent with our display, with a mean RMSE of 6.68 mm or 3.85% of the display’s maximum vertical range for shapes we could stably simulate. We then demonstrate potential scalability by simulating a multi-segment version of the system and show the display’s ability to withstand loads during contour following in haptic exploration.},
  archive   = {C_ICRA},
  author    = {Wing-Sum Law and Sofia Di Toro Wyetzner and Raymond Zhen and Sean Follmer},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610902},
  month     = {5},
  pages     = {9696-9703},
  title     = {A multi-stable curved line shape display},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active exploration for real-time haptic training.
<em>ICRA</em>, 9689–9695. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tactile perception is important for robotic systems that interact with the world through touch. Touch is an active sense in which tactile measurements depend on the contact properties of an interaction—e.g., velocity, force, acceleration— as well as properties of the sensor and object under test. These dependencies make training tactile perceptual models challenging. Additionally, the effects of limited sensor life and the near-field nature of tactile sensors preclude the practical collection of exhaustive data sets even for fairly simple objects. Active learning provides a mechanism for focusing on only the most informative aspects of an object during data collection. Here we employ an active learning approach that uses a data-driven model’s entropy as an uncertainty measure and explore relative to that entropy conditioned on the sensor state variables. Using a coverage-based ergodic controller, we train perceptual models in near-real time. We demonstrate our approach using a biomimentic sensor, exploring &quot;tactile scenes&quot; composed of shapes, textures, and objects. Each learned representation provides a perceptual sensor model for a particular tactile scene. Models trained on actively collected data outperform their randomly collected counterparts in real-time training tests. Additionally, we find that the resulting network entropy maps can be used to identify high salience portions of a tactile scene.},
  archive   = {C_ICRA},
  author    = {Jake Ketchum and Ahalya Prabhakar and Todd D. Murphey},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610782},
  month     = {5},
  pages     = {9689-9695},
  title     = {Active exploration for real-time haptic training},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fingertip ultrasonic array for tactile rendering.
<em>ICRA</em>, 9682–9688. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A miniature haptic stimulation device utilizes focused ultrasound to deliver a tactile haptic sensation to the finger. The 1-3 piezocomposite device has a 1 cm 2 footprint, which is an order of magnitude smaller than other ultrasonic haptic devices and is a good candidate for wearable tactile rendering systems. The device focuses energy to a 1 mm 3 voxel. The current prototype was validated with a small, preliminary human subject study and requires an average input voltage of 68.8 V to elicit tactile sensation. The sensory drive voltage threshold will decrease with future refinement of mechanical impedance matching and focusing.},
  archive   = {C_ICRA},
  author    = {Jace Rozsa and Sarah Costrell and Melisa Orta Martinez and Gary K. Fedder},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610307},
  month     = {5},
  pages     = {9682-9688},
  title     = {Fingertip ultrasonic array for tactile rendering},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Implementation and assessment of an augmented training
curriculum for surgical robotics. <em>ICRA</em>, 9666–9673. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The integration of high-level assistance algorithms in surgical robotics training curricula may be beneficial in establishing a more comprehensive and robust skillset for aspiring surgeons, improving their clinical performance as a consequence. This work presents the development and validation of a haptic-enhanced Virtual Reality simulator for surgical robotics training, featuring 8 surgical tasks that the trainee can interact with thanks to the embedded physics engine. This virtual simulated environment is augmented by the introduction of high-level haptic interfaces for robotic assistance that aim at re-directing the motion of the trainee’s hands and wrists toward targets or away from obstacles, and providing a quantitative performance score after the execution of each training exercise.An experimental study shows that the introduction of enhanced robotic assistance into a surgical robotics training curriculum improves performance during the training process and, crucially, promotes the transfer of the acquired skills to an unassisted surgical scenario, like the clinical one.},
  archive   = {C_ICRA},
  author    = {Alberto Rota and Ke Fan and Elena De Momi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610411},
  month     = {5},
  pages     = {9666-9673},
  title     = {Implementation and assessment of an augmented training curriculum for surgical robotics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Point-wise vibration pattern production via a sparse
actuator array for surface tactile feedback. <em>ICRA</em>, 9659–9665.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Surface vibration tactile feedback is capable of conveying various semantic information to humans via handheld electronic devices, such as smartphones, touch panels, and game controllers. However, covering the entire contacting surface of the device with a dense arrangement of actuators can affect its normal use. Determining how to produce desired vibration patterns at any contact point with only a few sparse actuators deployed on the surface of the handheld device remains a significant challenge. In this work, we develop a tactile feedback board in the size of a smartphone with only five actuators, and achieve the precise production of vibration patterns that can focus at any desired position on the board. Specifically, we investigate the vibration characteristics of a single passive coil actuator and construct its vibration pattern model for any position on the feedback board surface. Optimal phase and amplitude modulation, determined using the simulated annealing algorithm, is employed with five actuators in a sparse array. The vibration patterns from all actuators are superimposed linearly to synthetically generate different onboard vibration energy distributions for tactile sensing. Experiments demonstrated that point-wise vibration pattern production on our tactile board achieved an average level of about 0.9 in the Structural Similarity Index Measure (SSIM) evaluation, when compared to the ideal single-point-focused target vibration pattern. Four point-wise patterns focused on the top, bottom, left, and right parts of the tactile board were applied, to guide continuous directional movements without visual assistance, which shows significant implications for machine-assisted cognition based on vibration tactile feedback.},
  archive   = {C_ICRA},
  author    = {Xiaosa Li and Runze Zhao and Chengyue Lu and Xiao Xiao and Wenbo Ding},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611362},
  month     = {5},
  pages     = {9659-9665},
  title     = {Point-wise vibration pattern production via a sparse actuator array for surface tactile feedback},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prosthetic upper-limb sensory enhancement (PULSE): A dual
haptic feedback device in a prosthetic socket. <em>ICRA</em>, 9652–9658.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study presents the Prosthetic Upper-Limb Sensory Enhancement (PULSE), a novel dual feedback device completely integrated into a prosthetic socket. The core of the system includes two compact vibrotactile actuators and two silicone chambers in contact with the user’s skin. These components provide high-frequency tactile cues for initial contact and surface information (e.g. texture) as well as pressure stimuli related to grasping force. Ten able-bodied participants and one subject with limb loss validated the system, accomplishing an object discrimination task in two different modalities (with and without the feedback). Standardized questionnaires evaluate users’ satisfaction and workload, enabling a systematic and robust device assessment. The results show that the PULSE device enhanced performance compared to its absence without causing discomfort for a prosthetic user and able-bodied participants. The findings highlight the potential of dual haptic feedback to enhance sensory perception in prosthetic applications and offer valuable insights for future prosthetic design.},
  archive   = {C_ICRA},
  author    = {Alessia Silvia Ivani and Federica Barontini and Manuel G. Catalano and Giorgio Grioli and Matteo Bianchi and Antonio Bicchi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610327},
  month     = {5},
  pages     = {9652-9658},
  title     = {Prosthetic upper-limb sensory enhancement (PULSE): A dual haptic feedback device in a prosthetic socket},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The joint-space reconstruction of human fingers by using a
highly under-actuated exoskeleton. <em>ICRA</em>, 9645–9651. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hand motion tracking is essential in many fields, e.g., immersive virtual reality, teleoperation of robotic hand, and hand rehabilitation of stroke patient, as human hand plays a crucial role in our daily life. The highly under-actuated hand exoskeleton, which can track the 6-DoF motions of each fingertip via a highly under-actuated kinematic chain, exhibits many benefits in wearability and portability over other solutions. However, due to the non-anthropomorphic linkage, this hand exoskeleton also encounters difficulties in measuring human-finger’s joint angles. While the joint-space is important in many scenarios, such as teleoperating a robotic hand with anthropomorphic kinematics but with different size to human. Here we proposed a new method to reconstruct the human finger joints by using a highly under-actuated hand exoskeleton. Our key contribution is the arc-fitting algorithm, which is able to calibrate the misalignment between the exoskeleton’s and the human-finger’s base frames and estimate the length of human’s phalanxes, by using the fingertip’s circular motions. With knowing the aforementioned informations, the joint angles can be reconstructed in high precision based on the inverse kinematics models of human fingers. Furthermore, our proposed method is compared with a baseline method, in which the joint angles obtained by a motion capture system are served as ground-truth. The results demonstrate that our proposed method exhibits excellent performance in reconstructing finger’s joint configurations.},
  archive   = {C_ICRA},
  author    = {Yuan Su and Gaofeng Li and Yongsheng Deng and Ioannis Sarakoglou and Nikos G. Tsagarakis and Jiming Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610872},
  month     = {5},
  pages     = {9645-9651},
  title     = {The joint-space reconstruction of human fingers by using a highly under-actuated exoskeleton},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). X-tacformer: Spatio-tempral attention model for tactile
recognition. <em>ICRA</em>, 9638–9644. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, tactile sensing has attracted great interests in robotics, especially for exploring unstructured objects. Sensor arrays play an important role in the exploration, which generates rich spatio-temporal information. In this work, we propose an efficient tactile recognition model, X-Tacformer. This model pays attention to both spatial and temporal features of tactile sequences from sensor arrays, which is verified by four public datasets, Ev-Objects, Ev-Containers, Augment8000 and BioTac-Dos. Comparative studies show that our model has resulted in a significant improvement of the recognition accuracy by 0.0223, 0.1416, 0.2735 and 0.1592 in these datasets. In order to verify its performances on dataset with rich spatio-temporal features, a self-designed dataset, ALU-Textures, was constructed with 10 fabrics from everyday textiles, aiming to extend the data collection action modes of current datasets by simulating human rubbing movements with the thumb and index fingers of an Allegro hand. Our model also demonstrates efficient salient feature learning capabilities on ALU-Textures, which is further augmented by tactile data augmentation methods.},
  archive   = {C_ICRA},
  author    = {Jiarui Hu and Yanmin Zhou and Zhipeng Wang and Xin Li and Yongkang Jiang and Bin He},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610365},
  month     = {5},
  pages     = {9638-9644},
  title     = {X-tacformer: Spatio-tempral attention model for tactile recognition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enabling passivity for cartesian workspace restrictions.
<em>ICRA</em>, 9631–9637. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An emerging trend in the field of human-robot collaboration is the disassembly of end-of-life products. Safety is a crucial requirement of the disassembly process since worn-out or damaged products could break, possibly resulting in dangerous behavior of the robot. To protect the user from such behavior, this work addresses this challenge through the implementation of an energy-aware Cartesian impedance controller, combined with virtual workspace restrictions. Hereby, the passivity of the robotic system is ensured. The paper proposed two approaches to ensure the passivity of the system when subjected to workspace restrictions due to unplanned interactions and contact loss. The first approach employs an augmented energy tank with restricted energy flow. The second approach monitors the overall energy flow, regulating and separating non-passive behavior, caused by workspace restrictions. The approaches are evaluated and compared with each other, by using a KUKA LBR iiwa robot. The results highlight the potential of virtual workspace restrictions in human-robot collaborative disassembly tasks.},
  archive   = {C_ICRA},
  author    = {Sebastian Hjorth and Johannes Lachner and Arash Ajoudani and Dimitrios Chrysostomou},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610969},
  month     = {5},
  pages     = {9631-9637},
  title     = {Enabling passivity for cartesian workspace restrictions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A planar compliant contact control applied to
multi-dimensional elastic gripper for unexpected contact. <em>ICRA</em>,
9624–9630. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It is difficult to guarantee an empty living environment to prevent unexpected contact between the object being manipulated by the robot and unplanned obstacles. In this paper, we propose a planar compliant contact control method for planar manipulation to cope with unexpected contact. We first use sheet gel as a multi-dimensional passive elastic element and combine it with a two-finger gripper to design a multi-dimensional elastic gripper. Subsequently, we explore the lumped parameter model for the force-displacement relationship of gel deformation and combine the model with the high impedance motion of robots to design an elastic interaction controller. The controller not only actively adjusts the deformation of the gel to provide the desired contact force and torque depending on contact, but also performs avoidance by following the surface of obstacles. Finally, we design and deploy several planar compliant contact experiments to validate the proposed method and demonstrate the unexpected contact response in human-robot co-packing. The results show that our method enables the robot to remain compliant in the face of unexpected contact caused by unplanned obstacles, which provides a guarantee for safe manipulation. Physics experiments can be viewed in the attached video.},
  archive   = {C_ICRA},
  author    = {Junnan Huang and Xuefeng Wang and Chongkun Xia and Houde Liu and Mingqi Shao and Bin Liang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611428},
  month     = {5},
  pages     = {9624-9630},
  title     = {A planar compliant contact control applied to multi-dimensional elastic gripper for unexpected contact},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human preference-aware rebalancing and charging for shared
electric micromobility vehicles. <em>ICRA</em>, 9608–9615. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Shared electric micromobility has surged to a popular model of urban transportation due to its efficiency in short-distance trips and environmentally friendly characteristics compared to traditional automobiles. However, managing thousands of shared electric micromobility vehicles including rebalancing and charging to meet users’ travel demands still has been a challenge. Existing methods generally ignore human preferences in vehicle selection and assume all nearby vehicles have an equal chance of being selected, which is unrealistic based on our findings. To address this problem, we design PERCEIVE, a human preference-aware rebalancing and charging framework for shared electric micromobility vehicles. Specifically, we model human preferences in vehicle selection based on vehicle usage history and current status (e.g., energy level) and incorporate the vehicle selection model into a robust adversarial reinforcement learning framework. We further utilize conformal prediction to quantify human preference uncertainty and fuse it with the reinforcement learning framework. We evaluate our framework using two months of real-world electric micromobility operation data in a city. Experimental results show that our method achieves a performance gain of at least 4.02% in the net revenue and offers more robust performance in worst-case scenarios compared to state-of-the-art baselines.},
  archive   = {C_ICRA},
  author    = {Heng Tan and Yukun Yuan and Hua Yan and Shuxin Zhong and Yu Yang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610713},
  month     = {5},
  pages     = {9608-9615},
  title     = {Human preference-aware rebalancing and charging for shared electric micromobility vehicles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). JaywalkerVR: A VR system for collecting safety-critical
pedestrian-vehicle interactions. <em>ICRA</em>, 9600–9607. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Developing autonomous vehicles that can safely interact with pedestrians requires large amounts of pedestrian and vehicle data in order to learn accurate pedestrian-vehicle interaction models. However, gathering data that include crucial but rare scenarios - such as pedestrians jaywalking into heavy traffic - can be costly and unsafe to collect. We propose a virtual reality human-in-the-loop simulator, JaywalkerVR, to obtain vehicle-pedestrian interaction data to address these challenges. Our system enables efficient, affordable, and safe collection of long-tail pedestrian-vehicle interaction data. Using our proposed simulator, we create a high-quality dataset with vehicle-pedestrian interaction data from safety critical scenarios called CARLA-VR. The CARLA-VR dataset addresses the lack of long-tail data samples in commonly used real world autonomous driving datasets. We demonstrate that models trained with CARLA-VR improve displacement error and collision rate by 10.7% and 4.9%, respectively, and are more robust in rare vehicle-pedestrian scenarios.},
  archive   = {C_ICRA},
  author    = {Kenta Mukoya and Erica Weng and Rohan Choudhury and Kris Kitani},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610819},
  month     = {5},
  pages     = {9600-9607},
  title     = {JaywalkerVR: A VR system for collecting safety-critical pedestrian-vehicle interactions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning when to ask for help: Efficient interactive
navigation via implicit uncertainty estimation. <em>ICRA</em>,
9593–9599. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots operating alongside humans often encounter unfamiliar environments that make autonomous task completion challenging. Though improving models and increasing dataset size can enhance a robot’s performance in unseen environments, data collection and model refinement may be impractical in every environment. Approaches that utilize human demonstrations through manual operation can aid in refinement and generalization, but often require significant data collection efforts to generate enough demonstration data to achieve satisfactory task performance. Interactive approaches allow for humans to provide correction to robot action in real time, but intervention policies are often based on explicit factors related to state and task understanding that may be difficult to generalize. Addressing these challenges, we train a lightweight interaction policy that allows robots to decide when to proceed autonomously or request expert assistance at estimated times of uncertainty. An implicit estimate of uncertainty is learned via evaluating the feature extraction capabilities of the robot’s visual navigation policy. By incorporating part-time human interaction, robots recover quickly from their mistakes, significantly improving the odds of task completion. Incorporating part-time interaction yields an increase in success of 0.38 with only a 0.3 expert interaction rate within the Habitat simulation environment using a simulated human expert. We further show success transferring this approach to a new domain with a real human expert, improving success from less than 0.1 with an autonomous agent to 0.92 with a 0.23 human interaction rate. This approach provides a practical means for robots to interact and learn from humans in real-world settings.},
  archive   = {C_ICRA},
  author    = {Ifueko Igbinedion and Sertac Karaman},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610965},
  month     = {5},
  pages     = {9593-9599},
  title     = {Learning when to ask for help: Efficient interactive navigation via implicit uncertainty estimation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SEQUEL: Semi-supervised preference-based RL with query
synthesis via latent interpolation. <em>ICRA</em>, 9585–9592. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Preference-based reinforcement learning (RL) poses as a recent research direction in robot learning, by allowing humans to teach robots through preferences on pairs of desired behaviours. Nonetheless, to obtain realistic robot policies, an arbitrarily large number of queries is required to be answered by humans. In this work, we approach the sample-efficiency challenge by presenting a technique which synthesizes queries, in a semi-supervised learning perspective. To achieve this, we leverage latent variational autoencoder (VAE) representations of trajectory segments (sequences of state-action pairs). Our approach manages to produce queries which are closely aligned with those labeled by humans, while avoiding excessive uncertainty according to the human preference predictions as determined by reward estimations. Additionally, by introducing variation without deviating from the original human’s intents, more robust reward function representations are achieved. We compare our approach to recent state-of-the-art preference-based RL semi-supervised learning techniques. Our experimental findings reveal that we can enhance the generalization of the estimated reward function without requiring additional human intervention. Lastly, to confirm the practical applicability of our approach, we conduct experiments involving actual human users in a simulated social navigation setting. Videos of the experiments can be found at https://sites.google.com/view/rl-sequel},
  archive   = {C_ICRA},
  author    = {Daniel Marta and Simon Holk and Christian Pek and Iolanda Leite},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610534},
  month     = {5},
  pages     = {9585-9592},
  title     = {SEQUEL: Semi-supervised preference-based RL with query synthesis via latent interpolation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Facile integration of robots into experimental orchestration
at scientific user facilities. <em>ICRA</em>, 9578–9584. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Integration of robots into scientific user facilities, such as the National Synchrotron Light Source II, improves their efficiency and capacity. Many such facilities use the opensource Bluesky project for experimental control and orchestration. However, there remains an open challenge in deploying robotic solutions at these facilities that are reconfigurable, extensible, and compatible with pre-existing software infrastructure. Herein, we introduce a framework that uses the Robotic Operating System 2 (ROS2) and Bluesky to provide extensible robotic applications, while working under the operational constraints of a large-scale user facility. We demonstrated this framework by integrating a robotic arm to pick and place a sample holder at a beamline, recording a 90% repeatability rate. This provides the groundwork for further new robotics applications at large-scale scientific user facilities that depend on Bluesky.},
  archive   = {C_ICRA},
  author    = {Chandima Fernando and Daniel Olds and Stuart I. Campbell and Phillip M. Maffettone},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611706},
  month     = {5},
  pages     = {9578-9584},
  title     = {Facile integration of robots into experimental orchestration at scientific user facilities},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gaze-based human-robot interaction system for infrastructure
inspections. <em>ICRA</em>, 9571–9577. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Routine inspections for critical infrastructures such as bridges are required in most jurisdictions worldwide. Such routine inspections are largely visual in nature, which are qualitative, subjective, and not repeatable. Although robotic infrastructure inspections address such limitations, they cannot replace the superior ability of experts to make decisions in complex situations, thus making human-robot interaction systems a promising technology. This study presents a novel gaze-based human-robot interaction system, designed to augment the visual inspection performance through mixed reality. Through holograms from a mixed reality device, gaze can be utilized effectively to estimate the properties of the defect in real-time. Additionally, inspectors can monitor the inspection progress on-line, which enhances the speed of the entire inspection process. Limited controlled experiments demonstrate its effectiveness across various users and defect types. To our knowledge, this is the first demonstration of the real-time application of eye gaze in civil infrastructure inspections.},
  archive   = {C_ICRA},
  author    = {Sunwoong Choi and Zaid Abbas Al-Sabbag and Sriram Narasimhan and Chul Min Yeum},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610684},
  month     = {5},
  pages     = {9571-9577},
  title     = {Gaze-based human-robot interaction system for infrastructure inspections},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human-centered autonomy for UAS target search.
<em>ICRA</em>, 9563–9570. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current methods of deploying robots that operate in dynamic, uncertain environments, such as Uncrewed Aerial Systems in search &amp; rescue missions, require nearly continuous human supervision for vehicle guidance and operation. These methods do not consider high-level mission context resulting in cumbersome manual operation or inefficient exhaustive search patterns. We present a human-centered autonomous frame-work that infers geospatial mission context through dynamic feature sets, which then guides a probabilistic target search planner. Operators provide a set of diverse inputs, including priority definition, spatial semantic information about ad-hoc geographical areas, and reference waypoints, which are probabilistically fused with geographical database information and condensed into a geospatial distribution representing an operator’s preferences over an area. An online, POMDP-based planner, optimized for target searching, is augmented with this reward map to generate an operator-constrained policy. Our results, simulated based on input from five professional rescuers, display effective task mental model alignment, 18% more victim finds, and 15 times more efficient guidance plans then current operational methods.},
  archive   = {C_ICRA},
  author    = {Hunter M. Ray and Zakariya Laouar and Zachary Sunberg and Nisar Ahmed},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611708},
  month     = {5},
  pages     = {9563-9570},
  title     = {Human-centered autonomy for UAS target search},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Language to map: Topological map generation from natural
language path instructions. <em>ICRA</em>, 9556–9562. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, a method for generating a map from path information described using natural language (textual path) is proposed. In recent years, robotics research mainly focus on vision-and-language navigation (VLN), a navigation task based on images and textual paths. Although VLN is expected to facilitate user instructions to robots, its current implementation requires users to explain the details of the path for each navigation session, which results in high explanation costs for users. To solve this problem, we proposed a method that creates a map as a topological map from a textual path and automatically creates a new path using this map. We believe that large language models (LLMs) can be used to understand textual path. Therefore, we propose and evaluate two methods, one for storing implicit maps in LLMs, and the other for generating explicit maps using LLMs. The implicit map is in the LLM’s memory. It is created using prompts. In the explicit map, a topological map composed of nodes and edges is constructed and the actions at each node are stored. This makes it possible to estimate the path and actions at waypoints on an undescribed path, if enough information is available. Experimental results on path instructions generated in a real environment demonstrate that generating explicit maps achieves significantly higher accuracy than storing implicit maps in the LLMs.},
  archive   = {C_ICRA},
  author    = {Hideki Deguchi and Kazuki Shibata and Shun Taguchi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611377},
  month     = {5},
  pages     = {9556-9562},
  title     = {Language to map: Topological map generation from natural language path instructions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MORPHeus: A multimodal one-armed robot-assisted peeling
system with human users in-the-loop. <em>ICRA</em>, 9540–9547. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Meal preparation is an important instrumental activity of daily living (IADL). While existing research has explored robotic assistance in meal preparation tasks such as cutting and cooking, the crucial task of peeling has received less attention. Robot-assisted peeling, conventionally a bimanual task, is challenging to deploy in the homes of care recipients using two wheelchair-mounted robot arms due to ergonomic and transferring challenges. This paper introduces a robot-assisted peeling system utilizing a single robotic arm and an assistive cutting board, inspired by the way individuals with one functional hand prepare meals. Our system incorporates a multimodal active perception module to determine whether an area on the food is peeled, a human-in-the-loop long-horizon planner to perform task planning while catering to a user’s preference for peeling coverage, and a compliant controller to peel the food items. We demonstrate the system on 12 food items representing the extremes of different shapes, sizes, skin thickness, surface textures, skin vs flesh colors, and deformability. Check out the Morpheus project at https://emprise.cs.cornell.edu/morpheus/.},
  archive   = {C_ICRA},
  author    = {Ruolin Ye and Yifei Hu and Yuhan Anjelica Bian and Luke Kulm and Tapomayukh Bhattacharjee},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610050},
  month     = {5},
  pages     = {9540-9547},
  title     = {MORPHeus: A multimodal one-armed robot-assisted peeling system with human users in-the-loop},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Shaping social robot to play games with human demonstrations
and evaluative feedback. <em>ICRA</em>, 9517–9523. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, building on recent advances in the fields of gaming AI and social robotics, we present a new approach to facilitate the social robot Haru to imitate game strategies from human players’ demonstrated trajectories and evaluative feedback in a real-time two-player game. Our research shows that Haru is able to learn and imitate human different game strategies from human players in a human time scale. In addition, our results show that human evaluative feedback plays an important role in allowing Haru to obtain a better performance via our method than human player’s demonstrations. Finally, results of our user study indicate that Haru imitating human player’s game strategies via our method is perceived to be more human-like and have better game performance and experience than self-learning from pre-defined reward functions via traditional deep reinforcement learning.},
  archive   = {C_ICRA},
  author    = {Chuanxiong Zheng and Lei Zhang and Hui Wang and Randy Gomez and Eric Nichols and Guangliang Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611605},
  month     = {5},
  pages     = {9517-9523},
  title     = {Shaping social robot to play games with human demonstrations and evaluative feedback},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-level progressive reinforcement learning for control
policy in physical simulations. <em>ICRA</em>, 9502–9508. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Training model-free intelligent agents in complex real-world scenarios using reinforcement learning (RL) often necessitates simulation-based environments due to high physical expenses. However, when simulation takes a long time, e.g., in an unsteady 3D fluid simulation with interactions to the controllable solids, existing RL algorithms meet difficulty to accomplish training within a reasonable timeframes. In this paper, we propose a novel multi-level framework for RL to accelerate convergence as the first attempt to address this difficulty. Motivated by the idea of multi-grid solver, the control policy on a virtual agent over time can be decomposed into different frequency levels, which can be progressively learned via a set of simulations in a coarse-to-fine manner. It is expected that most RL trials are performed in coarser simulations to learn lower control frequency levels with more efficient convergence, while higher frequency levels require much less RL trials, thus significantly accelerating the learning process. To implement our idea, we designed a novel multi-level residual network with a filter module attached, where each level of the network is learned by performing RL for a given simulation resolution. The proposed framework is evaluated by conducting policy learning experiments on a virtual aerial (2D) and an underwater (3D) robot, both requiring time-consuming physical simulations. Our results demonstrate a decrease in almost half in learning time compared to a direct RL approach, while achieving similar control performance.},
  archive   = {C_ICRA},
  author    = {Kefei Wu and Xuming He and Yang Wang and Xiaopei Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610992},
  month     = {5},
  pages     = {9502-9508},
  title     = {Multi-level progressive reinforcement learning for control policy in physical simulations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 6-DoF grasp pose evaluation and optimization via transfer
learning from NeRFs. <em>ICRA</em>, 9495–9501. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the problem of robotic grasping of known and unknown objects using implicit behavior cloning. We train a grasp evaluation model from a small number of demonstrations that outputs higher values for grasp candidates that are more likely to succeed in grasping. This evaluation model serves as an objective function, that we maximize to identify successful grasps. Key to our approach is the utilization of learned implicit representations of visual and geometric features derived from a pre-trained NeRF. Though trained exclusively in a simulated environment with simplified objects and 4-DoF topdown grasps, our evaluation model and optimization procedure demonstrate generalization to 6-DoF grasps and novel objects both in simulation and in real-world settings, without the need for additional data. Supplementary material is available at: https://gergely-soti.github.io/grasp},
  archive   = {C_ICRA},
  author    = {Gergely Sóti and Xi Huang and Christian Wurll and Björn Hein},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610402},
  month     = {5},
  pages     = {9495-9501},
  title     = {6-DoF grasp pose evaluation and optimization via transfer learning from NeRFs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BEVUDA: Multi-geometric space alignments for domain adaptive
BEV 3D object detection. <em>ICRA</em>, 9487–9494. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision-centric bird-eye-view (BEV) perception has shown promising potential in autonomous driving. Recent works mainly focus on improving efficiency or accuracy but neglect the challenges when facing environment changing, resulting in severe degradation of transfer performance. For BEV perception, we figure out the significant domain gaps existing in typical real-world cross-domain scenarios and comprehensively solve the Domain Adaption (DA) problem for multi-view 3D object detection. Since BEV perception approaches are complicated and contain several components, the domain shift accumulation on multiple geometric spaces (i.e., 2D, 3D Voxel, BEV) makes BEV DA even challenging. In this paper, we propose a Multi-space Alignment Teacher-Student (MATS) framework to ease the domain shift accumulation, which consists of a Depth-Aware Teacher (DAT) and a Geometric-space Aligned Student (GAS) model. DAT tactfully combines target lidar and reliable depth prediction to construct depth-aware information, extracting target domain-specific knowledge in Voxel and BEV feature spaces. It then transfers the sufficient domain knowledge of multiple spaces to the student model. In order to jointly alleviate the domain shift, GAS projects multi-geometric space features to a shared geometric embedding space and decreases data distribution distance between two domains. To verify the effectiveness of our method, we conduct BEV 3D object detection experiments on three cross-domain scenarios and achieve state-of-the-art performance. Code: https://github.com/liujiaming1996/BEVUDA.},
  archive   = {C_ICRA},
  author    = {Jiaming Liu and Rongyu Zhang and Xiaoqi Li and Xiaowei Chi and Zehui Chen and Ming Lu and Yandong Guo and Shanghang Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610096},
  month     = {5},
  pages     = {9487-9494},
  title     = {BEVUDA: Multi-geometric space alignments for domain adaptive BEV 3D object detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parameter-efficient prompt learning for 3D point cloud
understanding. <em>ICRA</em>, 9478–9486. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a parameter-efficient prompt tuning method, named PPT, to adapt a large multi-modal model for 3D point cloud understanding. Existing strategies are quite expensive in computation and storage, and depend on timeconsuming prompt engineering. We address the problems from three aspects. Firstly, a PromptLearner module is devised to replace hand-crafted prompts with learnable contexts to automate the prompt tuning process. Then, we lock the pre-trained backbone instead of adopting the full fine-tuning paradigm to substantially improve the parameter efficiency. Finally, a lightweight PointAdapter module is arranged near target tasks to enhance prompt tuning for 3D point cloud understanding. Comprehensive experiments are conducted to demonstrate the superior parameter and data efficiency of the proposed method. Meanwhile, we obtain new records on 4 public datasets and multiple 3D tasks, i.e., point cloud recognition, few-shot learning, and part segmentation. The implementation is available at https://github.com/auniquesun/PPT.},
  archive   = {C_ICRA},
  author    = {Hongyu Sun and Yongcai Wang and Wang Chen and Haoran Deng and Deying Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610093},
  month     = {5},
  pages     = {9478-9486},
  title     = {Parameter-efficient prompt learning for 3D point cloud understanding},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross domain policy transfer with effect cycle-consistency.
<em>ICRA</em>, 9471–9477. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Training a robotic policy from scratch using deep reinforcement learning methods can be prohibitively expensive due to sample inefficiency. To address this challenge, transferring policies trained in the source domain to the target domain becomes an attractive paradigm. Previous research has typically focused on domains with similar state and action spaces but differing in other aspects. In this paper, our primary focus lies in domains with different state and action spaces, which has broader practical implications, i.e. transfer the policy from robot A to robot B. Unlike prior methods that rely on paired data, we propose a novel approach for learning the mapping functions between state and action spaces across domains using unpaired data. We propose effect cycle-consistency, which aligns the effects of transitions across two domains through a symmetrical optimization structure for learning these mapping functions. Once the mapping functions are learned, we can seamlessly transfer the policy from the source domain to the target domain. Our approach has been tested on three locomotion tasks and two robotic manipulation tasks. The empirical results demonstrate that our method can reduce alignment errors significantly and achieve better performance compared to the state-of-the-art method. Project page: https://ricky-zhu.github.io/effect_cycle_consistency.},
  archive   = {C_ICRA},
  author    = {Ruiqi Zhu and Tianhong Dai and Oya Celiktutan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611110},
  month     = {5},
  pages     = {9471-9477},
  title     = {Cross domain policy transfer with effect cycle-consistency},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MoPA: Multi-modal prior aided domain adaptation for 3D
semantic segmentation. <em>ICRA</em>, 9463–9470. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-modal unsupervised domain adaptation (MM-UDA) for 3D semantic segmentation is a practical solution to embed semantic understanding in autonomous systems without expensive point-wise annotations. While previous MM-UDA methods can achieve overall improvement, they suffer from significant class-imbalanced performance, restricting their adoption in real applications. This imbalanced performance is mainly caused by: 1) self-training with imbalanced data and 2) the lack of pixel-wise 2D supervision signals. In this work, we propose Multi-modal Prior Aided (MoPA) domain adaptation to improve the performance of rare objects. Specifically, we develop Valid Ground-based Insertion (VGI) to rectify the imbalance supervision signals by inserting prior rare objects collected from the wild while avoiding introducing artificial artifacts that lead to trivial solutions. Meanwhile, our SAM consistency loss leverages the 2D prior semantic masks from SAM as pixel-wise supervision signals to encourage consistent predictions for each object in the semantic mask. The knowledge learned from modal-specific prior is then shared across modalities to achieve better rare object segmentation. Extensive experiments show that our method achieves state-of-the-art performance on the challenging MM-UDA benchmark. Code will be available at https://github.com/AronCao49/MoPA.},
  archive   = {C_ICRA},
  author    = {Haozhi Cao and Yuecong Xu and Jianfei Yang and Pengyu Yin and Shenghai Yuan and Lihua Xie},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610316},
  month     = {5},
  pages     = {9463-9470},
  title     = {MoPA: Multi-modal prior aided domain adaptation for 3D semantic segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fine-tuning point cloud transformers with dynamic
aggregation. <em>ICRA</em>, 9455–9462. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Point clouds play an important role in 3D analysis, which has broad applications in robotics and autonomous driving. The pre-training fine-tuning paradigm has shown great potential in the point cloud domain. Full fine-tuning is generally effective but leads to a heavy storage and computational burden, which becomes inefficient and unacceptable as the size of pre-trained models scales. Although efficient fine-tuning approaches have significant progress in other domains, they generally perform worse for point clouds. To overcome this dilemma, we revisit the official Point-MAE implementation and find the critical role of aggregation in fine-tuning performances. Inspired by such discoveries, we propose a novel dynamic aggregation (DA) method to replace previous static aggregation like mean or max pooling for pre-trained point cloud Transformers. Besides standard metrics such as accuracy or mIoU, we evaluate the number of tunable parameters and additional FLOPs for a fair comparison of our method to different fine-tuning approaches. We construct several DA variants and validate them through extensive experiments. Experimental results demonstrate that DA has competitive performances against full fine-tuning and other efficient fine-tuning approaches. The code is publicly available at https://github.com/JaronTHU/DynamicAggregation.},
  archive   = {C_ICRA},
  author    = {Jiajun Fei and Zhidong Deng},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610767},
  month     = {5},
  pages     = {9455-9462},
  title     = {Fine-tuning point cloud transformers with dynamic aggregation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A guided gaussian-dirichlet random field for
scientist-in-the-loop inference in underwater robotics. <em>ICRA</em>,
9448–9454. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual topic modeling (VTM) provides key insight into data sets based on learned semantic topic models. The Gaussian-Dirichlet Random Field (GDRF), a state-of-the-art VTM technique, models these semantic topics in continuous space as densities. However, ambiguity in learned topics is a disadvantage of such Dirichlet-based VTM algorithms. We propose the Guided Gaussian-Dirichlet Random Field (GGDRF). Our method applies Dirichlet Forest priors from natural language processing (NLP) to the vision domain as a way to embed visual scientific knowledge into the estimation process. This modification and addition to the GDRF provides a key shift from unsupervised machine learning to semi-supervised machine learning in the robotic VTM domain. We show through simulation and real-world underwater data that the proposed GGDRF outperforms the previous GDRF method both quantitatively and qualitatively by improving alignment between estimated topics and scientific interests.},
  archive   = {C_ICRA},
  author    = {Chad R. Samuelson and Joshua G. Mangelson},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611290},
  month     = {5},
  pages     = {9448-9454},
  title     = {A guided gaussian-dirichlet random field for scientist-in-the-loop inference in underwater robotics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Belief scene graphs: Expanding partial scenes with objects
through computation of expectation. <em>ICRA</em>, 9441–9447. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this article, we propose the novel concept of Belief Scene Graphs, which are utility-driven extensions of partial 3D scene graphs, that enable efficient high-level task planning with partial information. We propose a graph-based learning methodology for the computation of belief (also referred to as expectation) on any given 3D scene graph, which is then used to strategically add new nodes (referred to as blind nodes) that are relevant to a robotic mission. We propose the method of Computation of Expectation based on Correlation Information (CECI), to reasonably approximate real Belief/Expectation, by learning histograms from available training data. A novel Graph Convolutional Neural Network (GCN) model is developed, to learn CECI from a repository of 3D scene graphs. As no database of 3D scene graphs exists for the training of the novel CECI model, we present a novel methodology for generating a 3D scene graph dataset based on semantically annotated real-life 3D spaces. The generated dataset is then utilized to train the proposed CECI model and for extensive validation of the proposed method. We establish the novel concept of Belief Scene Graphs (BSG), as a core component to integrate expectations into abstract representations. This new concept is an evolution of the classical 3D scene graph concept and aims to enable high-level reasoning for task planning and optimization of a variety of robotics missions. The efficacy of the overall framework has been evaluated in an object search scenario, and has also been tested in a real-life experiment to emulate human common sense of unseen-objects.For a video of the article, showcasing the experimental demonstration, please refer to the following link: https://youtu.be/hsGlSCa12iY},
  archive   = {C_ICRA},
  author    = {Mario A.V. Saucedo and Akash Patel and Akshit Saradagi and Christoforos Kanellakis and George Nikolakopoulos},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611352},
  month     = {5},
  pages     = {9441-9447},
  title     = {Belief scene graphs: Expanding partial scenes with objects through computation of expectation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HSPNav: Hierarchical scene prior learning for visual
semantic navigation towards real settings. <em>ICRA</em>, 9434–9440. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual Semantic Navigation (VSN) aims at navigating a robot to a given target object in a previously unseen scene. To tackle this task, the robot must learn a nimble navigation policy by utilizing spatial patterns and semantic co-occurrence relations among objects in the scene. Prevailing approaches extract scene priors from the instant visual observations and solidify them in neural episodic memory to achieve flexible navigation. However, due to the oblivion and underuse of the scene priors, these methods are plagued by repeated exploration, effective-knowledge sparsity, and wrong decisions. To alleviate these issues, we propose a novel VSN policy, HSPNav, based on Hierarchical Scene Priors (HSP) and Deep Reinforcement Learning (DRL). The HSP contains two components, i.e., the egocentric semantic map-based Local Scene Priors (LSP) and the commonsense relational graph-based Global Scene Priors (GSP). Then, efficient semantic navigation is achieved by employing an immediate LSP to retrieve conducive contextual memories from the GSP. By utilizing the MP3D dataset, the experimental results in the Habitat simulator demonstrate that our HSP brings a significant boost over the baselines. Furthermore, we take an essential step from simulation to reality by bridging the gap from Habitat to ROS. The migration evaluations show that HSPNav can generalize to realistic settings well and achieve promising performance.},
  archive   = {C_ICRA},
  author    = {Jiaxu Kang and Bolei Chen and Ping Zhong and Haonan Yang and Yu Sheng and Jianxin Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610061},
  month     = {5},
  pages     = {9434-9440},
  title     = {HSPNav: Hierarchical scene prior learning for visual semantic navigation towards real settings},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mask4Former: Mask transformer for 4D panoptic segmentation.
<em>ICRA</em>, 9418–9425. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurately perceiving and tracking instances over time is essential for the decision-making processes of autonomous agents interacting safely in dynamic environments. With this intention, we propose Mask4Former for the challenging task of 4D panoptic segmentation of LiDAR point clouds. Mask4Former is the first transformer-based approach unifying semantic instance segmentation and tracking of sparse and irregular sequences of 3D point clouds into a single joint model. Our model directly predicts semantic instances and their temporal associations without relying on hand-crafted non-learned association strategies such as probabilistic clustering or voting-based center prediction. Instead, Mask4Former introduces spatio-temporal instance queries that encode the semantic and geometric properties of each semantic tracklet in the sequence. In an in-depth study, we find that promoting spatially compact instance predictions is critical as spatiotemporal instance queries tend to merge multiple semantically similar instances, even if they are spatially distant. To this end, we regress 6-DOF bounding box parameters from spatiotemporal instance queries, which are used as an auxiliary task to foster spatially compact predictions. Mask4Former achieves a new state-of-the-art on the SemanticKITTI test set with a score of 68.4 LSTQ.},
  archive   = {C_ICRA},
  author    = {Kadir Yilmaz and Jonas Schult and Alexey Nekrasov and Bastian Leibe},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610262},
  month     = {5},
  pages     = {9418-9425},
  title     = {Mask4Former: Mask transformer for 4D panoptic segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Open-fusion: Real-time open-vocabulary 3D mapping and
queryable scene representation. <em>ICRA</em>, 9411–9417. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Precise 3D environmental mapping with semantics is essential in robotics. Existing methods often rely on pre-defined concepts during training or are time-intensive when generating semantic maps. This paper presents Open-Fusion, an approach for real-time open-vocabulary 3D mapping and queryable scene representation using RGB-D data. Open-Fusion harnesses the power of a pretrained vision-language foundation model (VLFM) for open-set semantic comprehension and employs the Truncated Signed Distance Function (TSDF) for swift 3D scene reconstruction. By leveraging the VLFM, we extract region-based embeddings and their associated confidence maps. These are then integrated with the 3D knowledge from TSDF using an enhanced Hungarian-based feature-matching mechanism. In particular, Open-Fusion delivers outstanding annotation-free 3D segmentation for open vocabulary query without the need for additional 3D training. Benchmark tests on the ScanNet dataset against leading zero-shot methods highlight Open-Fusion’s superiority. Furthermore, it seamlessly combines the strengths of region-based VLFM and TSDF, facilitating real-time 3D scene comprehension that includes object concepts and open-world semantics. We encourage the readers to view the demos on our project page: https://uark-aicv.github.io/OpenFusion},
  archive   = {C_ICRA},
  author    = {Kashu Yamazaki and Taisei Hanyu and Khoa Vo and Thang Pham and Minh Tran and Gianfranco Doretto and Anh Nguyen and Ngan Le},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610193},
  month     = {5},
  pages     = {9411-9417},
  title     = {Open-fusion: Real-time open-vocabulary 3D mapping and queryable scene representation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robotic exploration through semantic topometric mapping.
<em>ICRA</em>, 9404–9410. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this article, we introduce a novel strategy for robotic exploration in unknown environments using a semantic topometric map. As it will be presented, the semantic topometric map is generated by segmenting the grid map of the currently explored parts of the environment into regions, such as intersections, pathways, dead-ends, and unexplored frontiers, which constitute the structural semantics of an environment. The proposed exploration strategy leverages metric information of the frontier, such as distance and angle to the frontier, similar to existing frameworks, with the key difference being the additional utilization of structural semantic information, such as properties of the intersections leading to frontiers. The algorithm for generating semantic topometric mapping utilized by the proposed method is lightweight, resulting in the method’s online execution being both rapid and computationally efficient. Moreover, the proposed framework can be applied to both structured and unstructured indoor and outdoor environments, which enhances the versatility of the proposed exploration algorithm. We validate our exploration strategy and demonstrate the utility of structural semantics in exploration in two complex indoor environments by utilizing a Turtlebot3 as the robotic agent. Compared to traditional frontier-based methods, our findings indicate that the proposed approach leads to faster exploration and requires less computation time.},
  archive   = {C_ICRA},
  author    = {Scott Fredriksson and Akshit Saradagi and George Nikolakopoulos},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610585},
  month     = {5},
  pages     = {9404-9410},
  title     = {Robotic exploration through semantic topometric mapping},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Translating universal scene descriptions into knowledge
graphs for robotic environment. <em>ICRA</em>, 9389–9395. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots performing human-scale manipulation tasks require an extensive amount of knowledge about their surroundings in order to perform their actions competently and human-like. In this work, we investigate the use of virtual reality technology as an implementation for robot environment modeling, and present a technique for translating scene graphs into knowledge bases. To this end, we take advantage of the Universal Scene Description (USD) format which is an emerging standard for the authoring, visualization and simulation of complex environments. We investigate the conversion of USD-based environment models into Knowledge Graph (KG) representations that facilitate semantic querying and integration with additional knowledge sources. The contributions of the paper are validated through an application scenario in the service robotics domain.},
  archive   = {C_ICRA},
  author    = {Giang Hoang Nguyen and Daniel Beßler and Simon Stelter and Mihai Pomarlan and Michael Beetz},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611691},
  month     = {5},
  pages     = {9389-9395},
  title     = {Translating universal scene descriptions into knowledge graphs for robotic environment},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Field-evaluated closed structure soft gripper enhances the
shelf life of harvested blackberries. <em>ICRA</em>, 9382–9388. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft robotic grippers are intrinsically delicate while grasping objects, and can rely on mechanical deformation to adapt to different shapes without explicit control. These characteristics are particularly appealing for agriculture, where items of produce from the same crop can vary significantly in shape and size, and delicate harvesting is among the first concerns for fruit quality. Various soft robotic grippers have been proposed for harvesting different produce types, however their employment in field testing has been extremely limited. In this paper we developed the first closed structure soft gripper for the harvest of blackberries. We adapted an existing gripper concept, initially testing it on a sensorised raspberry physical twin. Then, followed grower-guided protocols to pick blackberries in farm polytunnels, and to evaluate the shelf life in comparison with berries picked by professional human pickers. Our results with ten experimental varieties showed a picking success rate of 95.4% demonstrating the capability of a closed structure gripper to adapt mechanically to fruit-shape variability. Moreover, a shelf life assessment on seven measured traits reported greatly improved shelf life of between 30 and 150%, across all traits for gripper harvested blackberries. Our study demonstrates the potential of soft grippers for delicate fruit harvesting, and indicates how to increase the impact of robotics in agriculture.},
  archive   = {C_ICRA},
  author    = {Philip H. Johnson and Kai Junge and Charles Whitfield and Josie Hughes and Marcello Calisti},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610387},
  month     = {5},
  pages     = {9382-9388},
  title     = {Field-evaluated closed structure soft gripper enhances the shelf life of harvested blackberries},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Force estimation at the bionic soft arm’s tool-center-point
during the interaction with the environment. <em>ICRA</em>, 9376–9381.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft continuum robots enable new application areas in contrast to standard rigid robots, such as interaction with a varying environment. Due to their compliant continuous structure, they are inherently safe and adaptive to environmental conditions. In this paper, the interaction with the environment is performed at the tool-center-point of a soft continuum manipulator and is realized by a hybrid force-position control. For this, a force estimation model is derived to substitute the force sensor at the tool-center-point. The force estimation is probabilistic and relies on normal distributions considering model parameters and deviations from model identification of the soft continuum robot. It also provides a qualitative measure for the contact estimation. This paper first presents the probabilistic force estimation model and then shows the hybrid force-position control using the presented model. From the results, it is concluded that force sensing is replaceable for the environment interaction.},
  archive   = {C_ICRA},
  author    = {Samuel Pilch and Daniel Klug and Oliver Sawodny},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611006},
  month     = {5},
  pages     = {9376-9381},
  title     = {Force estimation at the bionic soft arm’s tool-center-point during the interaction with the environment},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Symmetry-aware reinforcement learning for robotic assembly
under partial observability with a soft wrist. <em>ICRA</em>, 9369–9375.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study tackles the representative yet challenging contact-rich peg-in-hole task of robotic assembly, using a soft wrist that can operate more safely and tolerate lower-frequency control signals than a rigid one. Previous studies often use a fully observable formulation, requiring external setups or estimators for the peg-to-hole pose. In contrast, we use a partially observable formulation and deep reinforcement learning from demonstrations to learn a memory-based agent that acts purely on haptic and proprioceptive signals. Moreover, previous works do not incorporate potential domain symmetry and thus must search for solutions in a bigger space. Instead, we propose to leverage the symmetry for sample efficiency by augmenting the training data and constructing auxiliary losses to force the agent to adhere to the symmetry. Results in simulation with five different symmetric peg shapes show that our proposed agent can be comparable to or even outperform a state-based agent. In particular, the sample efficiency also allows us to learn directly on the real robot within 3 hours.},
  archive   = {C_ICRA},
  author    = {Hai Nguyen and Tadashi Kozuno and Cristian C. Beltran-Hernandez and Masashi Hamaya},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610103},
  month     = {5},
  pages     = {9369-9375},
  title     = {Symmetry-aware reinforcement learning for robotic assembly under partial observability with a soft wrist},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tip-clutching winch for high tensile force application with
soft growing robots. <em>ICRA</em>, 9362–9368. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The navigational abilities of tip-everting soft growing robots, known as vine robots, are compromised when tip-mount devices are added to enable carrying of payloads. We present a new method for securing a vine robot to objects or its environment that exploits the unique eversion-based growth mechanism and flexibility of vine robots, while keeping the tip of the vine robot free of encumbrance. Our implementation is a tip-clutching winch, into which vine robots can insert themselves and anchor to via powerful overlapping belt friction. The device enables passive, high-strength, and reversible fastening, and can easily release the vine robot. This approach enables carrying of loads of at least 28 kg (limited by the tensile strength of the vine robot body material and winch actuator torque capacity), as well as novel material transport and locomotion capabilities.},
  archive   = {C_ICRA},
  author    = {O. Godson Osele and Kentaro Barhydt and Nicholas Cerone and Allison M. Okamura and H. Harry Asada},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610362},
  month     = {5},
  pages     = {9362-9368},
  title     = {Tip-clutching winch for high tensile force application with soft growing robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hard shell, soft core: Binary actuators for deep-sea
applications. <em>ICRA</em>, 9355–9361. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep-sea research represents invaluable opportunities to unravel hidden ecosystems, uncover unknown biodiversity, and provide critical insights into the Earth’s history and the impacts of climate change. Due to the extreme conditions, exploring the deep-sea traditionally requires costly equipment, such as specific diving robots, engineered to withstand the high pressure. Our research aims to reduce the costs of deep-sea sediment sampling by introducing a novel actuation system for suction samplers, that capitalises the advantages of soft material actuators. At first glance, soft material actuators may not appear suitable for the harsh conditions that prevail in the deep-sea, but when combined with a rigid, bistable mechanism there is great potential for improving the accessibility of sampling and research in this challenging environment. The binary actuation system that results from this combination, is modular, scalable, lightweight, and low cost in comparison to existing solutions.},
  archive   = {C_ICRA},
  author    = {Cora Maria Sourkounis and Ditzia Susana Garcia Morales and Tom Kwasnitschka and Annika Raatz},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610349},
  month     = {5},
  pages     = {9355-9361},
  title     = {Hard shell, soft core: Binary actuators for deep-sea applications},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ultrafast capturing in-flight objects with reprogrammable
working speed ranges. <em>ICRA</em>, 9335–9341. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In-flight high-speed object capturing is crucial in nature to improve survival and adaptation to the environment, such as the predation of frogs, leopards, and eagles. Despite its ubiquitousness in nature, capturing fast-moving objects is extremely challenging in engineering implementations. In this paper, we report an ultrafast gripper based on tunable bistable structures. Different from current designs which are only suitable for objects with certain speed ranges once the grippers are fabricated, the working range of object speed of the proposed gripper could be reprogrammed by controlling the sensitivity of the structures. We present the design and fabrication of the proposed gripper in detail. A theoretical model is introduced to construct the energy landscape of the structures and the force response of the gripper when programmed to different states. The results show that in the original state, the gripper is capable of capturing a flying table tennis ball with a high speed of 15 m/s in only 6 ms. When the proposed gripper is controlled to the ultra-sensitive state, a flying ball with only 1 m/s could also be captured. This work broadens the frontiers of in-flight capturing design, and we envision broader promising applications.},
  archive   = {C_ICRA},
  author    = {Yongkang Jiang and Xin Tong and Zhongqing Sun and Yaimiin Zhou and Zhipeng Wang and Shuo Jiang and Zhen Yin and Yulong Ding and Bin He and Yingtian Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610901},
  month     = {5},
  pages     = {9335-9341},
  title     = {Ultrafast capturing in-flight objects with reprogrammable working speed ranges},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient RRT*-based safety-constrained motion planning for
continuum robots in dynamic environments. <em>ICRA</em>, 9328–9334. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Continuum robots, characterized by their high flexibility and infinite degrees of freedom (DoFs), have gained prominence in applications such as minimally invasive surgery and hazardous environment exploration. However, the intrinsic complexity of continuum robots requires a significant amount of time for their motion planning, posing a hurdle to their practical implementation. To tackle these challenges, efficient motion planning methods such as Rapidly Exploring Random Trees (RRT) and its variant, RRT*, have been employed. This paper introduces a unique RRT*-based motion control method tailored for continuum robots. Our approach embeds safety constraints derived from the robots’ posture states, facilitating autonomous navigation and obstacle avoidance in rapidly changing environments. Simulation results show efficient trajectory planning amidst multiple dynamic obstacles and provide a robust performance evaluation based on the generated postures. Finally, preliminary tests were conducted on a two-segment cable-driven continuum robot prototype, confirming the effectiveness of the proposed planning approach. This method is versatile and can be adapted and deployed for various types of continuum robots through parameter adjustments.},
  archive   = {C_ICRA},
  author    = {Peiyu Luo and Shilong Yao and Yiyao Yue and Jiankun Wang and Hong Yan and Max Q.-H. Meng},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610309},
  month     = {5},
  pages     = {9328-9334},
  title     = {Efficient RRT*-based safety-constrained motion planning for continuum robots in dynamic environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unconstrained model predictive control for robot navigation
under uncertainty. <em>ICRA</em>, 9321–9327. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a probabilistic and unconstrained model predictive control formulation for robot navigation under uncertainty. We present (1) a closed-form approximation of the probability of collision that naturally models the propagation of uncertainty over the planning horizon and is computationally cheap to evaluate, and (2) a collision-cost formulation which provably preserves forward invariance (i.e., keeps the robot away from obstacles) when combined with the probability formulation. Notably, our formulation avoids hard constraints by construction, which in turn avoids abrupt transitions in robot behavior around the constraint boundaries ensuring graceful navigation. Further, we present proof for the forward invariance and the stability of the approach. We compare the efficacy of our method with the baseline [1], which the proposed approach builds on. We demonstrate that the approach results in confident and safe robot navigation in tight spaces by smoothly slowing down the robot in low survivability environments (e.g., tight corridors), but also allows it to move away from obstacles safely when needed.},
  archive   = {C_ICRA},
  author    = {Senthil Hariharan Arul and Jong Jin Park and Vishnu Prem and Yang Zhang and Dinesh Manocha},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610531},
  month     = {5},
  pages     = {9321-9327},
  title     = {Unconstrained model predictive control for robot navigation under uncertainty},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural potential field for obstacle-aware local motion
planning. <em>ICRA</em>, 9313–9320. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Model predictive control (MPC) may provide local motion planning for mobile robotic platforms. The challenging aspect is the analytic representation of collision cost for the case when both the obstacle map and robot footprint are arbitrary. We propose a Neural Potential Field: a neural network model that returns a differentiable collision cost based on robot pose, obstacle map, and robot footprint. The differentiability of our model allows its usage within the MPC solver. It is computationally hard to solve problems with a very high number of parameters. Therefore, our architecture includes neural image encoders, which transform obstacle maps and robot footprints into embeddings, which reduce problem dimensionality by two orders of magnitude. The reference data for network training are generated based on algorithmic calculation of a signed distance function. Comparative experiments showed that the proposed approach is comparable with existing local planners: it provides trajectories with outperforming smoothness, comparable path length, and safe distance from obstacles.},
  archive   = {C_ICRA},
  author    = {Muhammad Alhaddad and Konstantin Mironov and Aleksey Staroverov and Aleksandr Panov},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611635},
  month     = {5},
  pages     = {9313-9320},
  title     = {Neural potential field for obstacle-aware local motion planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Planning with learned subgoals selected by temporal
information. <em>ICRA</em>, 9306–9312. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Path planning in a changing environment is a challenging task in robotics, as moving objects impose time-dependent constraints. Recent planning methods primarily focus on the spatial aspects, lacking the capability to directly incorporate time constraints. In this paper, we propose a method that leverages a generative model to decompose a complex planning problem into small manageable ones by incrementally generating subgoals given the current planning context. Then, we take into account the temporal information and use learned time estimators based on different statistic distributions to examine and select the generated subgoal candidates. Experiments show that planning from the current robot state to the selected subgoal can satisfy the given time-dependent constraints while being goal-oriented.},
  archive   = {C_ICRA},
  author    = {Xi Huang and Gergely Sóti and Christoph Ledermann and Björn Hein and Torsten Kröger},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610538},
  month     = {5},
  pages     = {9306-9312},
  title     = {Planning with learned subgoals selected by temporal information},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross view capture for distributed image compression with
decoder side information. <em>ICRA</em>, 9300–9305. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image compression is increasingly important in applications like intelligent driving and smart surveillance systems. This study presents a novel cross view capture distributed image compression network (CVCDIC) to improve the compression quality by using decoder side information. The CVCDIC’s decoder utilizes feature extraction networks to extract features from both the primary image and the side information. Furthermore, a multi-level cross view attention module is designed to capture interrelated details between images at multiple hierarchical levels. Finally, a spatial refinement module, constructed on the foundation of information distillation networks, is designed to further refine the quality of reconstructed images. The results show that CVCDIC can achieve an MS-SSIM of 0.978 at 0.15 bpp, surpassing DSIN (0.925), NDIC (0.956), and ATN (0.955) on the KITTI Stereo dataset.},
  archive   = {C_ICRA},
  author    = {Yankai Yin and Zhe Sun and Peiying Ruan and Feng Duan and Ruidong Li and Chi Zhu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611242},
  month     = {5},
  pages     = {9300-9305},
  title     = {Cross view capture for distributed image compression with decoder side information},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Certifying bimanual RRT motion plans in a second.
<em>ICRA</em>, 9293–9299. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an efficient method for certifying non-collision for piecewise-polynomial motion plans in algebraic reparametrizations of configuration space. Such motion plans include those generated by popular randomized methods including RRTs and PRMs, as well as those generated by many methods in trajectory optimization. Based on Sums-of-Squares optimization, our method provides exact, rigorous certificates of non-collision; it can never falsely claim that a motion plan containing collisions is collision-free. We demonstrate that our formulation is practical for real world deployment, certifying the safety of a twelve degree of freedom motion plan in just over a second. Moreover, the method is capable of discriminating the safety or lack thereof of two motion plans which differ by only millimeters.},
  archive   = {C_ICRA},
  author    = {Alexandre Amice and Peter Werner and Russ Tedrake},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611296},
  month     = {5},
  pages     = {9293-9299},
  title     = {Certifying bimanual RRT motion plans in a second},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ZAPP! Zonotope agreement of prediction and planning for
continuous-time collision avoidance with discrete-time dynamics.
<em>ICRA</em>, 9285–9292. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The past few years have seen immense progress on two fronts that are critical to safe, widespread mobile robot deployment: predicting uncertain motion of multiple agents, and planning robot motion under uncertainty. However, the numerical methods required on each front have resulted in a mismatch of representation for prediction and planning. In prediction, numerical tractability is usually achieved by coarsely discretizing time, and by representing multimodal multi-agent interactions as distributions with infinite support. On the other hand, safe planning typically requires very fine time discretization, paired with distributions with compact support, to reduce conservativeness and ensure numerical tractability. The result is, when existing predictors are coupled with planning and control, one may often find unsafe motion plans. This paper proposes ZAPP (Zonotope Agreement of Prediction and Planning) to resolve the representation mismatch. ZAPP unites a prediction-friendly coarse time discretization and a planning-friendly zonotope uncertainty representation; the method also enables differentiating through a zonotope collision check, allowing one to integrate prediction and planning within a gradient-based optimization framework. Numerical examples show how ZAPP can produce safer trajectories compared to baselines in interactive scenes.},
  archive   = {C_ICRA},
  author    = {Luca Paparusso and Shreyas Kousik and Edward Schmerling and Francesco Braghin and Marco Pavone},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610953},
  month     = {5},
  pages     = {9285-9292},
  title     = {ZAPP! zonotope agreement of prediction and planning for continuous-time collision avoidance with discrete-time dynamics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PathRL: An end-to-end path generation method for collision
avoidance via deep reinforcement learning. <em>ICRA</em>, 9278–9284. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot navigation using deep reinforcement learning (DRL) has shown great potential in improving the performance of mobile robots. Nevertheless, most existing DRL-based navigation methods primarily focus on training a policy that directly commands the robot with low-level controls, like linear and angular velocities, which leads to unstable speeds and unsmooth trajectories of the robot during the long-term execution. An alternative method is to train a DRL policy that outputs the navigation path directly. Then the robot can follow the generated path smoothly using sophisticated velocity-planning and path-following controllers, whose parameters are specified according to the hardware platform. However, two roadblocks arise for training a DRL policy that outputs paths: (1) The action space for potential paths often involves higher dimensions comparing to low-level commands, which increases the difficulties of training; (2) It takes multiple time steps to track a path instead of a single time step, which requires the path to predicate the interactions of the robot w.r.t. the dynamic environment in multiple time steps. This, in turn, amplifies the challenges associated with training. In response to these challenges, we propose PathRL, a novel DRL method that trains the policy to generate the navigation path for the robot. Specifically, we employ specific action space discretization techniques and tailored state space representation methods to address the associated challenges. Curriculum learning is employed to expedite the training process, while the reward function also takes into account the smooth transition between adjacent paths. In our experiments, PathRL achieves better success rates and reduces angular rotation variability compared to other DRL navigation methods, facilitating stable and smooth robot movement. We demonstrate the competitive edge of PathRL in both real-world scenarios and multiple challenging simulation environments.},
  archive   = {C_ICRA},
  author    = {Wenhao Yu and Jie Peng and Quecheng Qiu and Hanyu Wang and Lu Zhang and Jianmin Ji},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610107},
  month     = {5},
  pages     = {9278-9284},
  title     = {PathRL: An end-to-end path generation method for collision avoidance via deep reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient polynomial sum-of-squares programming for planar
robotic arms. <em>ICRA</em>, 9271. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collision-avoiding motion planning for articulated robotic arms is one of the major challenges in robotics. The difficulty of the problem arises from its high dimensionality and the intricate geometry of the feasible space. Our goal is to seek large convex domains in configuration space, which contain no obstacles. In these domains, simple linear trajectories are guaranteed to be collision free, and can be leveraged for further optimization. To find such domains, practitioners have harnessed a methodology known as Sum-Of-Squares (SOS) Programming. SOS programs, however, are notorious for their poor scaling properties, which makes it challenging to employ them for complex problems. In this paper, we explore a simple formulation for a two-dimensional arm, which results in smaller SOS programs than previous suggested ones. We show that this formulation can express a variety of scenarios in a unified manner.},
  archive   = {C_ICRA},
  author    = {Daniel Keren and Amit Shahar and Roi Poranne},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611508},
  month     = {5},
  pages     = {9271},
  title     = {Efficient polynomial sum-of-squares programming for planar robotic arms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DiPPeR: Diffusion-based 2D path planner applied on legged
robots. <em>ICRA</em>, 9264–9270. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we present DiPPeR, a novel and fast 2D path planning framework for quadrupedal locomotion, leveraging diffusion-driven techniques. Our contributions include a scalable dataset generator for map images and corresponding trajectories, an image-conditioned diffusion planner for mobile robots, and a training/inference pipeline employing CNNs. We validate our approach in several mazes, as well as in real-world deployment scenarios on Boston Dynamic’s Spot and Unitree’s Go1 robots. DiPPeR performs on average 23 times faster for trajectory generation against both search based and data driven path planning algorithms with an average of 87% consistency in producing feasible paths of various length in maps of variable size, and obstacle structure. Website: https://rpl-cs-ucl.github.io/DiPPeR/},
  archive   = {C_ICRA},
  author    = {Jianwei Liu and Maria Stamatopoulou and Dimitrios Kanoulas},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610013},
  month     = {5},
  pages     = {9264-9270},
  title     = {DiPPeR: Diffusion-based 2D path planner applied on legged robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EquivAct: SIM(3)-equivariant visuomotor policies beyond
rigid object manipulation. <em>ICRA</em>, 9249–9255. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {If a robot masters folding a kitchen towel, we would expect it to master folding a large beach towel. However, existing policy learning methods that rely on data augmentation still don’t guarantee such generalization. Our insight is to add equivariance to both the visual object representation and policy architecture. We propose EquivAct which utilizes SIM(3)-equivariant network structures that guarantee generalization across all possible object translations, 3D rotations, and scales by construction. EquivAct is trained in two phases. We first pre-train a SIM(3)-equivariant visual representation on simulated scene point clouds. Then, we learn a SIM(3)-equivariant visuomotor policy using a small amount of source task demonstrations. We show that the learned policy directly transfers to objects that substantially differ from demonstrations in scale, position, and orientation. We evaluate our method in three manipulation tasks involving deformable and articulated objects, going beyond typical rigid object manipulation tasks considered in prior work. We conduct experiments both in simulation and in reality. For real robot experiments, our method uses 20 human demonstrations of a tabletop task and transfers zero-shot to a mobile manipulation task in a much larger setup. Experiments confirm that our contrastive pre-training procedure and equivariant architecture offer significant improvements over prior work. Project website: equivact.github.io},
  archive   = {C_ICRA},
  author    = {Jingyun Yang and Congyue Deng and Jimmy Wu and Rika Antonova and Leonidas Guibas and Jeannette Bohg},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611491},
  month     = {5},
  pages     = {9249-9255},
  title     = {EquivAct: SIM(3)-equivariant visuomotor policies beyond rigid object manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Synchronized dual-arm rearrangement via cooperative mTSP.
<em>ICRA</em>, 9242–9248. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Synchronized dual-arm rearrangement is widely studied as a common scenario in industrial applications. It often faces scalability challenges due to the computational complexity of robotic arm rearrangement and the high-dimensional nature of dual-arm planning. To address these challenges, we formulated the problem as cooperative mTSP, a variant of mTSP where agents share cooperative costs, and utilized reinforcement learning for its solution. Our approach involved representing rearrangement tasks using a task state graph that captured spatial relationships and a cooperative cost matrix that provided details about action costs. Taking these representations as observations, we designed an attention-based network to effectively combine them and provide rational task scheduling. Furthermore, a cost predictor is also introduced to directly evaluate actions during both training and planning, significantly expediting the planning process. Our experimental results demonstrate that our approach outperforms existing methods in terms of both performance and planning efficiency.},
  archive   = {C_ICRA},
  author    = {Wenhao Li and Shishun Zhang and Sisi Dai and Hui Huang and Ruizhen Hu and Xiaohong Chen and Kai Xu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610424},
  month     = {5},
  pages     = {9242-9248},
  title     = {Synchronized dual-arm rearrangement via cooperative mTSP},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sim2Real manipulation on unknown objects with tactile-based
reinforcement learning. <em>ICRA</em>, 9234–9241. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Using tactile sensors for manipulation remains one of the most challenging problems in robotics. At the heart of these challenges is generalization: How can we train a tactile-based policy that can manipulate unseen and diverse objects? In this paper, we propose to perform Reinforcement Learning with only visual tactile sensing inputs on diverse objects in a physical simulator. By training with diverse objects in simulation, it enables the policy to generalize to unseen objects. However, leveraging simulation introduces the Sim2Real transfer problem. To mitigate this problem, we study different tactile representations and evaluate how each affects real-robot manipulation results after transfer. We conduct our experiments on diverse real-world objects and show significant improvements over baselines. Our project page is available at https://tactilerl.github.io/.},
  archive   = {C_ICRA},
  author    = {Entong Su and Chengzhe Jia and Yuzhe Qin and Wenxuan Zhou and Annabella Macaluso and Binghao Huang and Xiaolong Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611113},
  month     = {5},
  pages     = {9234-9241},
  title     = {Sim2Real manipulation on unknown objects with tactile-based reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalize by touching: Tactile ensemble skill transfer for
robotic furniture assembly. <em>ICRA</em>, 9227–9233. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Furniture assembly remains an unsolved problem in robotic manipulation due to its long task horizon and nongeneralizable operations plan. This paper presents the Tactile Ensemble Skill Transfer (TEST) framework, a pioneering offline reinforcement learning (RL) approach that incorporates tactile feedback in the control loop. TEST’s core design is to learn a skill transition model for high-level planning, along with a set of adaptive intra-skill goal-reaching policies. Such design aims to solve the robotic furniture assembly problem in a more generalizable way, facilitating seamless chaining of skills for this long-horizon task. We first sample demonstration from a set of heuristic policies and trajectories consisting of a set of randomized sub-skill segments, enabling the acquisition of rich robot trajectories that capture skill stages, robot states, visual indicators, and crucially, tactile signals. Leveraging these trajectories, our offline RL method discerns skill termination conditions and coordinates skill transitions. Our evaluations highlight the proficiency of TEST on the in-distribution furniture assemblies, its adaptability to unseen furniture configurations, and its robustness against visual disturbances. Ablation studies further accentuate the pivotal role of two algorithmic components: the skill transition model and tactile ensemble policies. Results indicate that TEST can achieve a success rate of 90% and is over 4 times more efficient than the heuristic policy in both in-distribution and generalization settings, suggesting a scalable skill transfer approach for contact-rich manipulation.},
  archive   = {C_ICRA},
  author    = {Haohong Lin and Radu Corcodel and Ding Zhao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610567},
  month     = {5},
  pages     = {9227-9233},
  title     = {Generalize by touching: Tactile ensemble skill transfer for robotic furniture assembly},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging the efficiency of multi-task robot manipulation
via task-evoked planner and reinforcement learning. <em>ICRA</em>,
9220–9226. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-task learning has expanded the boundaries of robotic manipulation, enabling the execution of increasingly complex tasks. However, policies learned through reinforcement learning exhibit limited generalization and narrow distributions, which restrict their effectiveness in multi-task training. Addressing the challenge of obtaining policies with generalization and stability represents a non-trivial problem. To tackle this issue, we propose a planning-guided reinforcement learning method. It leverages a task-evoked planner(TEP) and a reinforcement learning approach with planner’s guidance. TEP utilizes reusable samples as the source, with the aim of learning reachability information across different task scenarios. Then in reinforcement learning, TEP assesses and guides the Actor towards better outputs and smoothly enhances the performance in multi-task benchmarks. We evaluate this approach within the Meta-World framework and compare it with prior works in terms of learning efficiency and effectiveness. Depending on experimental results, our method has more efficiency, higher success rates, and demonstrates more realistic behavior.},
  archive   = {C_ICRA},
  author    = {Haofu Qian and Haoyang Zhang and Jun Shao and Jiatao Zhang and Jason Gu and Wei Song and Shiqiang Zhu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611076},
  month     = {5},
  pages     = {9220-9226},
  title     = {Leveraging the efficiency of multi-task robot manipulation via task-evoked planner and reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing task performance of learned simplified models via
reinforcement learning. <em>ICRA</em>, 9212–9219. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In contact-rich tasks, the hybrid, multi-modal nature of contact dynamics poses great challenges in model representation, planning, and control. Recent efforts have attempted to address these challenges via data-driven methods, learning dynamical models in combination with model predictive control. Those methods, while effective, rely solely on minimizing forward prediction errors to hope for better task performance with MPC controllers. This weak correlation can result in data inefficiency as well as limitations to overall performance. In response, we propose a novel strategy: using a policy gradient algorithm to find a simplified dynamics model that explicitly maximizes task performance. Specifically, we parameterize the stochastic policy as the perturbed output of the MPC controller, thus, the learned model representation can directly associate with the policy or task performance. We apply the proposed method to contact-rich tasks where a three-fingered robotic hand manipulates previously unknown objects. Our method significantly enhances task success rate by up to 15% in manipulating diverse objects compared to the existing method while sustaining data efficiency. Our method can solve some tasks with success rates of 70% or higher using under 30 minutes of data. All videos and codes are available at https://sites.google.com/view/lcs-rl.},
  archive   = {C_ICRA},
  author    = {Hien Bui and Michael Posa},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611461},
  month     = {5},
  pages     = {9212-9219},
  title     = {Enhancing task performance of learned simplified models via reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning to catch reactive objects with a behavior
predictor. <em>ICRA</em>, 9205–9211. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tracking and catching moving objects is an important ability for robots in a dynamic world. Whilst some objects have highly predictable state evolution e.g., the ballistic trajectory of a tennis ball, reactive targets alter their behavior in response to motion of the manipulator. Reactive applications range from gently capturing living animals such as snakes or fish for biological investigations, to smoothly interacting with and assisting a person. Existing works for dynamic catching usually perform target prediction followed by planning, but seldom account for highly non-linear reactive behaviors. Alternatively, Reinforcement Learning (RL) based methods simply treat the target and its motion as part of the observation of the world-state, but perform poorly due to the weak reward signal. In this work, we blend the approach of an explicit, yet learned, target state predictor with RL. We further show how a tightly coupled predictor which ‘observes’ the state of the robot leads to significantly improved anticipatory action, especially with targets that seek to evade the robot following a simple policy. Experiments show that our method achieves an 86.4% (open plane area) and a 73.8% (room) success rate on evasive objects, outperforming monolithic reinforcement learning and other techniques. We also demonstrate the efficacy of our approach across varied targets and trajectories. All code, data, and additional videos are at this GitHub link: https://kl-research.github.io/dyncatch.},
  archive   = {C_ICRA},
  author    = {Kai Lu and Jia-Xing Zhong and Bo Yang and Bing Wang and Andrew Markham},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611106},
  month     = {5},
  pages     = {9205-9211},
  title     = {Learning to catch reactive objects with a behavior predictor},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning vision-based pursuit-evasion robot policies.
<em>ICRA</em>, 9197–9204. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning strategic robot behavior—like that required in pursuit-evasion interactions—under real-world constraints is extremely challenging. It requires exploiting the dynamics of the interaction, and planning through both physical state and latent intent uncertainty. In this paper, we transform this intractable problem into a supervised learning problem, where a fully-observable robot policy generates supervision for a partially-observable one. We find that the quality of the supervision signal for the partially-observable pursuer policy depends on two key factors: the balance of diversity and optimality of the evader’s behavior, and the strength of the modeling assumptions in the fully-observable policy. We deploy our policy on a physical quadruped robot with an RGB-D camera on pursuit-evasion interactions in the wild. Despite all the challenges, the sensing constraints bring about creativity: the robot is pushed to gather information when uncertain, predict intent from noisy measurements, and anticipate in order to intercept.},
  archive   = {C_ICRA},
  author    = {Andrea Bajcsy and Antonio Loquercio and Ashish Kumar and Jitendra Malik},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610498},
  month     = {5},
  pages     = {9197-9204},
  title     = {Learning vision-based pursuit-evasion robot policies},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TWIST: Teacher-student world model distillation for
efficient sim-to-real transfer. <em>ICRA</em>, 9190–9196. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Model-based RL is a promising approach for real-world robotics due to its improved sample efficiency and generalization capabilities compared to model-free RL. However, effective model-based RL solutions for vision-based real-world applications require bridging the sim-to-real gap for any world model learnt. Due to its significant computational cost, standard domain randomisation does not provide an effective solution to this problem. This paper proposes TWIST (Teacher-Student World Model Distillation for Sim-to-Real Transfer) to achieve efficient sim-to-real transfer of vision-based model-based RL using distillation. Specifically, TWIST leverages state observations as readily accessible, privileged information commonly garnered from a simulator to significantly accelerate sim-to-real transfer. Specifically, a teacher world model is trained efficiently on state information. At the same time, a matching dataset is collected of domain-randomised image observations. The teacher world model then supervises a student world model that takes the domain-randomised image observations as input. By distilling the learned latent dynamics model from the teacher to the student model, TWIST achieves efficient and effective sim-to-real transfer for vision-based model-based RL tasks. Experiments in simulated and real robotics tasks demonstrate that our approach outperforms naive domain randomisation and model-free methods in terms of sample efficiency and task performance of sim-to-real transfer.},
  archive   = {C_ICRA},
  author    = {Jun Yamada and Marc Rigter and Jack Collins and Ingmar Posner},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610450},
  month     = {5},
  pages     = {9190-9196},
  title     = {TWIST: Teacher-student world model distillation for efficient sim-to-real transfer},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning highly dynamic behaviors for quadrupedal robots.
<em>ICRA</em>, 9183–9189. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning highly dynamic behaviors for robots has been a longstanding challenge. Traditional approaches have demonstrated robust locomotion, but the exhibited behaviors lack diversity and agility. They employ approximate models, which lead to compromises in performance. Data-driven approaches have been shown to reproduce agile behaviors of animals, but typically have not been able to learn highly dynamic behaviors. In this paper, we propose a learning-based approach to enable robots to learn highly dynamic behaviors from animal motion data. The learned controller is deployed on a quadrupedal robot and the results show that the controller is able to reproduce highly dynamic behaviors including sprinting, jumping and sharp turning. Various behaviors can be activated through human interaction using a stick with markers attached to it. Based on the motion pattern of the stick, the robot exhibits walking, running, sitting and jumping, much like the way humans interact with a pet.},
  archive   = {C_ICRA},
  author    = {Chong Zhang and Jiapeng Sheng and Tingguang Li and He Zhang and Cheng Zhou and Qingxu Zhu and Rui Zhao and Yizheng Zhang and Lei Han},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610440},
  month     = {5},
  pages     = {9183-9189},
  title     = {Learning highly dynamic behaviors for quadrupedal robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effective representation learning is more effective in
reinforcement learning than you think. <em>ICRA</em>, 9176–9182. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In reinforcement learning (RL), learning directly from pixels, is commonly known as vision-based RL. Effective state representations are crucial for high performance in vision-based RL. However, in order to learn effective state representations, most current vision-based RL methods based on contrastive unsupervised learning use auxiliary tasks similar to those in computer vision, which does not guarantee the effective information interaction between representation learning and RL. To learn more efficient states, we propose a simple and effective vision-based RL method. It leverages the representations acquired through contrastive learning by the Teacher Encoder and the Student Encoder to collaboratively estimate the Q-function. This cooperative process utilizes the TD error to steer updates to the Teacher Encoder, thereby ensuring effective information exchange between representation learning and RL. We refer to this approach as Reinforcement Learning with Teacher-Student Collaboration (RLTSC). RLTSC incorporates recent advancements in contrastive unsupervised learning, endowing it with potent representation learning capabilities. It provides a robust estimate of the Q-function with minimal variance and effectively guides the Teacher Ecoder to update and acquire a more efficient representation. RLTSC substantially enhances data efficiency in vision-based RL, surpassing state-of-the-art methods on various continuous and discrete control benchmarks. Remarkably, RLTSC even outperforms RL methods based on physical state features in terms of data efficiency for continuous control benchmarks. This may enlighten us: effective representation learning is more effective in reinforcement learning than you think!},
  archive   = {C_ICRA},
  author    = {Jiawei Zheng and Yonghong Song},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611330},
  month     = {5},
  pages     = {9176-9182},
  title     = {Effective representation learning is more effective in reinforcement learning than you think},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SPRINT: Scalable policy pre-training via language
instruction relabeling. <em>ICRA</em>, 9168–9175. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pre-training robots with a rich set of skills can substantially accelerate the learning of downstream tasks. Prior works have defined pre-training tasks via natural language instructions, but doing so requires tedious human annotation of hundreds of thousands of instructions. Thus, we propose SPRINT, a scalable offline policy pre-training approach which substantially reduces the human effort needed for pre-training a diverse set of skills. Our method uses two core ideas to automatically expand a base set of pre-training tasks: instruction relabeling via large language models and cross-trajectory skill chaining with offline reinforcement learning. As a result, SPRINT pre-training equips robots with a richer repertoire of skills that can help an agent generalize to new tasks. Experiments in a household simulator and on a real robot kitchen manipulation task show that SPRINT leads to substantially faster learning of new long-horizon tasks than previous pre-training approaches. Website at https://clvrai.com/sprint.},
  archive   = {C_ICRA},
  author    = {Jesse Zhang and Karl Pertsch and Jiahui Zhang and Joseph J. Lim},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610606},
  month     = {5},
  pages     = {9168-9175},
  title     = {SPRINT: Scalable policy pre-training via language instruction relabeling},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SLIM: Skill learning with multiple critics. <em>ICRA</em>,
9161–9167. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-supervised skill learning aims to acquire useful behaviors that leverage the underlying dynamics of the environment. Latent variable models, based on mutual information maximization, have been successful in this task but still struggle in the context of robotic manipulation. As it requires impacting a possibly large set of degrees of freedom composing the environment, mutual information maximization fails alone in producing useful and safe manipulation behaviors. Furthermore, tackling this by augmenting skill discovery rewards with additional rewards through a naive combination might fail to produce desired behaviors. To address this limitation, we introduce SLIM, a multi-critic learning approach for skill discovery with a particular focus on robotic manipulation. Our main insight is that utilizing multiple critics in an actor-critic framework to gracefully combine multiple reward functions leads to a significant improvement in latent-variable skill discovery for robotic manipulation while overcoming possible interference occurring among rewards which hinders convergence to useful skills. Furthermore, in the context of tabletop manipulation, we demonstrate the applicability of our novel skill discovery approach to acquire safe and efficient motor primitives in a hierarchical reinforcement learning fashion and leverage them through planning, significantly surpassing baseline approaches for skill discovery.},
  archive   = {C_ICRA},
  author    = {David Emukpere and Bingbing Wu and Julien Perez and Jean-Michel Renders},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610691},
  month     = {5},
  pages     = {9161-9167},
  title     = {SLIM: Skill learning with multiple critics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IQL-TD-MPC: Implicit q-learning for hierarchical model
predictive control. <em>ICRA</em>, 9154–9160. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Model-based reinforcement learning (RL) has shown great promise due to its sample efficiency, but still struggles with long-horizon sparse-reward tasks, especially in offline settings where the agent learns from a fixed dataset. We hypothesize that model-based RL agents struggle in these environments due to a lack of long-term planning capabilities, and that planning in a temporally abstract model of the environment can alleviate this issue. In this paper, we make two key contributions: 1) we introduce an offline model-based RL algorithm, IQL-TD-MPC, that extends the state- of-the-art Temporal Difference Learning for Model Predictive Control (TD-MPC) with Implicit Q-Learning (IQL); and 2) we propose to use IQL-TD-MPC as a Manager in a hierarchical setting with any off-the-shelf offline RL algorithm as a Worker. More specifically, we pre-train a temporally abstract IQL-TD-MPC Manager to predict &quot;intent embeddings&quot;, which roughly correspond to subgoals, via planning. We show that augmenting state representations with intent embeddings generated by an IQL-TD-MPC manager significantly improves off-the-shelf offline RL agents&#39; performance on some of the most challenging D4RL benchmark tasks. For instance, the offline RL algorithms AWAC, TD3-BC, DT, and CQL all get zero or near-zero normalized evaluation scores on the medium and large antmaze tasks, while our modification gives an average score over 40.},
  archive   = {C_ICRA},
  author    = {Rohan Chitnis and Yingchen Xu and Bobak Hashemi and Lucas Lehnert and Urun Dogan and Zheqing Zhu and Olivier Delalleau},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611711},
  month     = {5},
  pages     = {9154-9160},
  title     = {IQL-TD-MPC: Implicit Q-learning for hierarchical model predictive control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Synset boulevard: A synthetic image dataset for VMMR*.
<em>ICRA</em>, 9146–9153. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present and discuss the Synset Boulevard dataset, designed for the task of surveillance-nature vehicle make and model recognition (VMMR)—to the best of our knowledge the first entirely synthetically generated large-scale VMMR image dataset. Through the simulation of image data rather than the manual annotation of real data, we intend to mitigate common challenges in state-of-the-art VMMR datasets, namely bias, human error, privacy, and the challenge of providing systematic updates. On the other hand, the provision and use of synthetic data introduce individual challenges, such as potential domain gaps and a less pronounced intra-class variance. Our approach to address these challenges, using path tracing and physically-based, data-driven models, is evaluated on an existing large real-world dataset. Overall, our synthetic dataset contains 32 400 independent images (each with different imaging simulations and with/without masked license plates, leading to a total of 259 200 images) from 162 different vehicle models of 43 makes depicted in front view. It is split into 8 sub-datasets to investigate the influence of optical/imaging effects on the classification ability.},
  archive   = {C_ICRA},
  author    = {Anne Sielemann and Stefan Wolf and Masoud Roschani and Jens Ziehn and Juergen Beyerer},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610650},
  month     = {5},
  pages     = {9146-9153},
  title     = {Synset boulevard: A synthetic image dataset for VMMR*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MMA-net: Multiple morphology-aware network for automated
cobb angle measurement. <em>ICRA</em>, 9139–9145. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Scoliosis diagnosis and assessment depend largely on the measurement of the Cobb angle in spine X-ray images. With the emergence of deep learning techniques that employ landmark detection, tilt prediction, and spine segmentation, automated Cobb angle measurement has become increasingly popular. However, these methods encounter difficulties such as high noise sensitivity, intricate computational procedures, and exclusive reliance on a single type of morphological information. In this paper, we introduce the Multiple Morphology-Aware Network (MMA-Net), a novel framework that improves Cobb angle measurement accuracy by integrating multiple spine morphology as attention information. In the MMA-Net, we first feed spine X-ray images into the segmentation network to produce multiple morphological information (spine region, centerline, and boundary) and then concatenate the original X-ray image with the resulting segmentation maps as input for the regression module to perform precise Cobb angle measurement. Furthermore, we devise joint loss functions for our segmentation and regression network training, respectively. We evaluate our method on the AASCE challenge dataset and achieve superior performance with the SMAPE of 7.28% and the MAE of 3.18°, indicating a strong competitiveness compared to other outstanding methods. Consequently, we can offer clinicians automated, efficient, and reliable Cobb angle measurement.},
  archive   = {C_ICRA},
  author    = {Zhengxuan Qiu and Jie Yang and Jiankun Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610928},
  month     = {5},
  pages     = {9139-9145},
  title     = {MMA-net: Multiple morphology-aware network for automated cobb angle measurement},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PillarGen: Enhancing radar point cloud density and quality
via pillar-based point generation network. <em>ICRA</em>, 9117–9124. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a novel point generation model, referred to as Pillar-based Point Generation Network (PillarGen), which facilitates the transformation of point clouds from one domain into another. PillarGen can produce synthetic point clouds with enhanced density and quality based on the provided input point clouds. The PillarGen model performs the following three steps: 1) pillar encoding, 2) Occupied Pillar Prediction (OPP), and 3) Pillar to Point Generation (PPG). The input point clouds are encoded using a pillar grid structure to generate pillar features. Then, OPP determines the active pillars used for point generation and predicts the center of points and the number of points to be generated for each active pillar. PPG generates the synthetic points for each active pillar based on the information provided by OPP. We evaluate the performance of PillarGen using our proprietary radar dataset, focusing on enhancing the density and quality of short-range radar data using the long-range radar data as supervision. Our experiments demonstrate that PillarGen outperforms traditional point upsampling methods in quantitative and qualitative measures. We also confirm that when PillarGen is incorporated into bird’s eye view object detection, a significant improvement in detection accuracy is achieved.},
  archive   = {C_ICRA},
  author    = {Jisong Kim and Geonho Bang and Kwangjin Choi and Minjae Seong and Jaechang Yoo and Eunjong Pyo and Jun Won Choi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611144},
  month     = {5},
  pages     = {9117-9124},
  title     = {PillarGen: Enhancing radar point cloud density and quality via pillar-based point generation network},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised spike depth estimation via cross-modality
cross-domain knowledge transfer. <em>ICRA</em>, 9109–9116. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Neuromorphic spike data, an upcoming modality with high temporal resolution, has shown promising potential in autonomous driving by mitigating the challenges posed by high-velocity motion blur. However, training the spike depth estimation network holds significant challenges in two aspects: sparse spatial information for pixel-wise tasks and difficulties in achieving paired depth labels for temporally intensive spike streams. Therefore, we introduce open-source RGB data to support spike depth estimation, leveraging its annotations and spatial information. The inherent differences in modalities and data distribution make it challenging to directly apply transfer learning from open-source RGB to target spike data. To this end, we propose a cross-modality cross-domain (BiCross) framework to realize unsupervised spike depth estimation by introducing simulated mediate source spike data. Specifically, we design a Coarse-to-Fine Knowledge Distillation (CFKD) approach to facilitate comprehensive cross-modality knowledge transfer while preserving the unique strengths of both modalities, utilizing a spike-oriented uncertainty scheme. Then, we propose a Self-Correcting Teacher-Student (SCTS) mechanism to screen out reliable pixel-wise pseudo labels and ease the domain shift of the student model, which avoids error accumulation in target spike data. To verify the effectiveness of BiCross, we conduct extensive experiments on four scenarios, including Synthetic to Real, Extreme Weather, Scene Changing, and Real Spike. Our method achieves state-of-the-art (SOTA) performances, compared with RGB-oriented unsupervised depth estimation methods. Code and dataset: https://github.com/Theia-4869/BiCross.},
  archive   = {C_ICRA},
  author    = {Jiaming Liu and Qizhe Zhang and Xiaoqi Li and Jianing Li and Guanqun Wang and Ming Lu and Tiejun Huang and Shanghang Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610511},
  month     = {5},
  pages     = {9109-9116},
  title     = {Unsupervised spike depth estimation via cross-modality cross-domain knowledge transfer},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SAM-event-adapter: Adapting segment anything model for
event-RGB semantic segmentation. <em>ICRA</em>, 9093–9100. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Semantic segmentation, a fundamental visual task ubiquitously employed in sectors ranging from transportation and robotics to healthcare, has always captivated the research community. In the wake of rapid advancements in large model research, the foundation model for semantic segmentation tasks, termed the Segment Anything Model (SAM), has been introduced. This model substantially addresses the dilemma of poor generalizability of previous segmentation models and the disadvantage in requiring to retrain the whole model on variant datasets. Nonetheless, segmentation models developed on SAM remain constrained by the inherent limitations of RGB sensors, particularly in scenarios characterized by complex lighting conditions and high-speed motion. Motivated by these observations, a natural recourse is to adapt SAM to additional visual modalities without compromising its robust generalizability. To achieve this, we introduce a lightweight SAM-Event-Adapter (SE-Adapter) module, which incorporates event camera data into a cross-modal learning architecture based on SAM, with only limited tunable parameters incremental. Capitalizing on the high dynamic range and temporal resolution afforded by event cameras, our proposed multi-modal Event-RGB learning architecture effectively augments the performance of semantic segmentation tasks. In addition, we propose a novel paradigm for representing event data in a patch format compatible with transformer-based models, employing multi-spatiotemporal scale encoding to efficiently extract motion and semantic correlations from event representations. Exhaustive empirical evaluations conducted on the DSEC-Semantic and DDD17 datasets provide validation of the effectiveness and rationality of our proposed approach.},
  archive   = {C_ICRA},
  author    = {Bowen Yao and Yongjian Deng and Yuhan Liu and Hao Chen and Youfu Li and Zhen Yang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611127},
  month     = {5},
  pages     = {9093-9100},
  title     = {SAM-event-adapter: Adapting segment anything model for event-RGB semantic segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OpenAnnotate3D: Open-vocabulary auto-labeling system for
multi-modal 3D data. <em>ICRA</em>, 9086–9092. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the era of big data and large models, automatic annotating functions for multi-modal data are of great significance for real-world AI-driven applications, such as autonomous driving and embodied AI. Unlike traditional closed-set annotation, open-vocabulary annotation is essential to achieve human-level cognition capability. However, there are few open-vocabulary auto-labeling systems for multi-modal 3D data. In this paper, we introduce OpenAnnotate3D, an open-source open-vocabulary auto-labeling system that can automatically generate 2D masks, 3D masks, and 3D bounding box annotations for vision and point cloud data. Our system integrates the chain-of-thought capabilities of Large Language Models (LLMs) and the cross-modality capabilities of vision-language models (VLMs). To the best of our knowledge, OpenAnnotate3D is one of the pioneering works for open-vocabulary multi-modal 3D auto-labeling. We conduct comprehensive evaluations on both public and in-house real-world datasets, which demonstrate that the system significantly improves annotation efficiency compared to manual annotation while providing accurate open-vocabulary auto-annotating results.},
  archive   = {C_ICRA},
  author    = {Yijie Zhou and Likun Cai and Xianhui Cheng and Zhongxue Gan and Xiangyang Xue and Wenchao Ding},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610779},
  month     = {5},
  pages     = {9086-9092},
  title     = {OpenAnnotate3D: Open-vocabulary auto-labeling system for multi-modal 3D data},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semi-supervised learning for visual bird’s eye view semantic
segmentation. <em>ICRA</em>, 9079–9085. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual bird’s eye view (BEV) semantic segmentation helps autonomous vehicles understand the surrounding environment only from front-view (FV) images, including static elements (e.g., roads) and dynamic elements (e.g., vehicles, pedestrians). However, the high cost of annotation procedures of full-supervised methods limits the capability of the visual BEV semantic segmentation, which usually needs HD maps, 3D object bounding boxes, and camera extrinsic matrixes. In this paper, we present a novel semi-supervised framework for visual BEV semantic segmentation to boost performance by exploiting unlabeled images during the training. A consistency loss that makes full use of unlabeled data is then proposed to constrain the model on not only semantic prediction but also the BEV feature. Furthermore, we propose a novel and effective data augmentation method named conjoint rotation which reasonably augments the dataset while maintaining the geometric relationship between the FV images and the BEV semantic segmentation. Extensive experiments on the nuScenes dataset show that our semi-supervised framework can effectively improve prediction accuracy. To the best of our knowledge, this is the first work that explores improving visual BEV semantic segmentation performance using unlabeled data. The code is available at https://github.com/Junyu-Z/Semi-BEVseg.},
  archive   = {C_ICRA},
  author    = {Junyu Zhu and Lina Liu and Yu Tang and Feng Wen and Wanlong Li and Yong Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611420},
  month     = {5},
  pages     = {9079-9085},
  title     = {Semi-supervised learning for visual bird’s eye view semantic segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SuperFusion: Multilevel LiDAR-camera fusion for long-range
HD map generation. <em>ICRA</em>, 9056–9062. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {High-definition (HD) semantic map generation of the environment is an essential component of autonomous driving. Existing methods have achieved good performance in this task by fusing different sensor modalities, such as LiDAR and camera. However, current works are based on raw data or network feature-level fusion and only consider short-range HD map generation, limiting their deployment to realistic autonomous driving applications. In this paper, we focus on the task of building the HD maps in both short ranges, i.e., within 30m, and also predicting long-range HD maps up to 90m, which is required by downstream path planning and control tasks to improve the smoothness and safety of autonomous driving. To this end, we propose a novel network named SuperFusion, exploiting the fusion of LiDAR and camera data at multiple levels. We use LiDAR depth to improve image depth estimation and use image features to guide long-range LiDAR feature prediction. We benchmark our SuperFusion on the nuScenes dataset and a self-recorded dataset and show that it outperforms the state-of-the-art baseline methods with large margins on all intervals. Additionally, we apply the generated HD map to a downstream path planning task, demonstrating that the long-range HD maps predicted by our method can lead to better path planning for autonomous vehicles. Our code and self-recorded dataset have been released at https://github.com/haomo-ai/SuperFusion.},
  archive   = {C_ICRA},
  author    = {Hao Dong and Weihao Gu and Xianjing Zhang and Jintao Xu and Rui Ai and Huimin Lu and Juho Kannala and Xieyuanli Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611320},
  month     = {5},
  pages     = {9056-9062},
  title     = {SuperFusion: Multilevel LiDAR-camera fusion for long-range HD map generation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SKD-net: Spectral-based knowledge distillation in low-light
thermal imagery for robotic perception. <em>ICRA</em>, 9041–9047. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Enhancing the generalization capacity for semantic segmentation of aerial perception systems for safety-critical applications is vital, especially for environments with low-light and adverse conditions. Multi-spectral fusion techniques aim to maintain the merits of electro-optical (EO) and infrared (IR) images, e.g., retaining low-level features and capturing detailed textures from both modalities. However, these techniques encounter limitations when faced with scenarios involving missing modalities, especially during inference when only IR images are available. In this paper, we propose a novel spectral-based knowledge distillation architecture known as SKD-Net to improve the performance of deep learning models for missing modality scenarios for semantic segmentation tasks. In this architecture, we make use of Gated Spectral Unit to combine information from both modalities. SKD-Net aims to extract valuable semantic information from EO images while preserving spectral knowledge from the IR images within the feature space. The model retains the style information in the shallow layers while simultaneously fusing the high-level semantic context obtained from EO and IR images to improve the feature generation capacity when dealing with only IR images during inference. SKD-Net outperforms state-of-the-art multi-modal fusion and distillation models by 2.8% on average in scenarios with missing modalities when using only IR data during inference in two public benchmarking datasets. This performance increase is achieved without additional computational costs compared to the baseline segmentation models.},
  archive   = {C_ICRA},
  author    = {Aniruddh Sikdar and Jayant Teotia and Suresh Sundaram},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611323},
  month     = {5},
  pages     = {9041-9047},
  title     = {SKD-net: Spectral-based knowledge distillation in low-light thermal imagery for robotic perception},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Chasing day and night: Towards robust and efficient all-day
object detection guided by an event camera. <em>ICRA</em>, 9026–9032.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to detect objects in all lighting (i.e., normal-, over-, and under-exposed) conditions is crucial for real-world applications, such as self-driving. Traditional RGB-based detectors often fail under such varying lighting conditions. Therefore, recent works utilize novel event cameras to supplement or guide the RGB modality; however, these methods typically adopt asymmetric network structures that rely predominantly on the RGB modality, resulting in limited robustness for all-day detection. In this paper, we propose EOLO, a novel object detection framework that achieves robust and efficient all-day detection by fusing both RGB and event modalities. Our EOLO framework is built based on a lightweight spiking neural network (SNN) to efficiently leverage the asynchronous property of events. Buttressed by it, we first introduce an Event Temporal Attention (ETA) module to learn the high temporal information from events while preserving crucial edge information. Secondly, as different modalities exhibit varying levels of importance under diverse lighting conditions, we propose a novel Symmetric RGB-Event Fusion (SREF) module to effectively fuse RGB-Event features without relying on a specific modality, thus ensuring a balanced and adaptive fusion for all-day detection. In addition, to compensate for the lack of paired RGB-Event datasets for all-day training and evaluation, we propose an event synthesis approach based on the randomized optical flow that allows for directly generating the event frame from a single exposure image. We further build two new datasets, E-MSCOCO and E-VOC based on the popular benchmarks MSCOCO and PASCAL VOC. Extensive experiments demonstrate that our EOLO outperforms the state-of-the-art detectors, e.g., RENet [1], by a substantial margin (+3.74% mAP 50 ) in all lighting conditions. Our code and datasets will be available at https://vlislab22.github.io/EOLO/.},
  archive   = {C_ICRA},
  author    = {Jiahang Cao and Xu Zheng and Yuanhuiyi Lyu and Jiaxu Wang and Renjing Xu and Lin Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611705},
  month     = {5},
  pages     = {9026-9032},
  title     = {Chasing day and night: Towards robust and efficient all-day object detection guided by an event camera},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Influence of camera-LiDAR configuration on 3D object
detection for autonomous driving. <em>ICRA</em>, 9018–9025. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cameras and LiDARs are both important sensors for autonomous driving, playing critical roles in 3D object detection. Camera-LiDAR Fusion has been a prevalent solution for robust and accurate driving perception. In contrast to the vast majority of existing arts that focus on how to improve the performance of 3D target detection through cross-modal schemes, deep learning algorithms, and training tricks, we devote attention to the impact of sensor configurations on the performance of learning-based methods. To achieve this, we propose a unified information-theoretic surrogate metric for camera and LiDAR evaluation based on the proposed sensor perception model. We also design an accelerated high-quality framework for data acquisition, model training, and performance evaluation that functions with the CARLA simulator. To show the correlation between detection performance and our surrogate metrics, We conduct experiments using several camera-LiDAR placements and parameters inspired by selfdriving companies and research institutions. Extensive experimental results of representative algorithms on nuScenes dataset validate the effectiveness of our surrogate metric, demonstrating that sensor configurations significantly impact point-cloudimage fusion based detection models, which contribute up to 30% discrepancy in terms of the average precision.},
  archive   = {C_ICRA},
  author    = {Ye Li and Hanjiang Hu and Zuxin Liu and Xiaohao Xu and Xiaonan Huang and Ding Zhao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610896},
  month     = {5},
  pages     = {9018-9025},
  title     = {Influence of camera-LiDAR configuration on 3D object detection for autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing mmWave radar point cloud via visual-inertial
supervision. <em>ICRA</em>, 9010–9017. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Complementary to prevalent LiDAR and camera systems, millimeter-wave (mmWave) radar is robust to adverse weather conditions like fog, rainstorms, and blizzards but offers sparse point clouds. Current techniques enhance the point cloud by the supervision of LiDAR’s data. However, high-performance LiDAR is notably expensive and is not commonly available on vehicles. This paper presents mmEMP, a supervised learning approach that enhances radar point clouds using a low-cost camera and an inertial measurement unit (IMU), enabling crowd-sourcing training data from commercial vehicles. Bringing the visual-inertial (VI) supervision is challenging due to the spatial agnostic of dynamic objects. Moreover, spurious radar points from the curse of RF multipath make robots misunderstand the scene. mmEMP first devises a dynamic 3D reconstruction algorithm that restores the 3D positions of dynamic features. Then, we design a neural network that densifies radar data and eliminates spurious radar points. We build a new dataset in the real world. Extensive experiments show that mmEMP achieves competitive performance compared with the SOTA approach training by LiDAR’s data. In addition, we use the enhanced point cloud to perform object detection, localization, and mapping to demonstrate mmEMP’s effectiveness.},
  archive   = {C_ICRA},
  author    = {Cong Fan and Shengkai Zhang and Kezhong Liu and Shuai Wang and Zheng Yang and Wei Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610091},
  month     = {5},
  pages     = {9010-9017},
  title     = {Enhancing mmWave radar point cloud via visual-inertial supervision},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical planning for long-horizon multi-agent
collective construction. <em>ICRA</em>, 9003–9009. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We develop a planner that directs robots to construct a 3D target structure composed of blocks. The robots themselves are cubes of the same size as the blocks, and they may place, carry, or remove one block at a time. When moving, robots are also allowed to climb or descend a block. A construction plan may thus build a staircase-like scaffolding of blocks to reach other blocks at higher levels. The order of block placement is important; for example, a block that sits atop other blocks must be placed after the blocks below it, and a block that needs scaffolding cannot be placed until after the scaffolding is. Prior works focus on end-to-end approaches that simultaneously plan for block placement order and inter-robot collisions. Larger structures are either intractable or yield high-cost solutions. A prior approach mitigates this by decomposing the structure into smaller components that can be planned for independently, but the computational challenge remains. We present a hierarchical approach that first 1) uses A* to determine a sequence of block placements and removals while ignoring inter-robot collision, then 2) identifies ordering constraints between block placement and removal actions, and finally (3) computes collision-free paths for multiple robots to perform said actions. Compared to an optimization approach that minimizes the number of timesteps to complete the structure, we observe a 100x reduction in computation time for comparable solutions.},
  archive   = {C_ICRA},
  author    = {Shambhavi Singh and Zejian Huang and Akshaya Kesarimangalam Srinivasan and Geordan Gutow and Bhaskar Vundurthy and Howie Choset},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611496},
  month     = {5},
  pages     = {9003-9009},
  title     = {Hierarchical planning for long-horizon multi-agent collective construction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimization and evaluation of a multi robot surface
inspection task through particle swarm optimization. <em>ICRA</em>,
8996–9002. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot swarms can be tasked with a variety of automated sensing and inspection applications in aerial, aquatic, and surface environments. In this paper, we study a simplified two-outcome surface inspection task. We task a group of robots to inspect and collectively classify a 2D surface section based on a binary pattern projected on the surface. We use a decentralized Bayesian decision-making algorithm and deploy a swarm of 3-cm sized wheeled robots to inspect a randomized black and white tiled surface section of size 1m×1m in simulation. We first describe the model parameters that characterize our simulated environment, the robot swarm, and the inspection algorithm. We then employ a noise-resistant heuristic optimization scheme based on the Particle Swarm Optimization (PSO) using a fitness evaluation that combines the swarm’s classification decision accuracy and decision time. We use our fitness measure definition to asses the optimized parameters through 100 randomized simulations that vary surface pattern and initial robot poses. The optimized algorithm parameters show up to 55% improvement in median of fitness evaluations against an empirically chosen parameter set.},
  archive   = {C_ICRA},
  author    = {Darren Chiu and Radhika Nagpal and Bahar Haghighat},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611661},
  month     = {5},
  pages     = {8996-9002},
  title     = {Optimization and evaluation of a multi robot surface inspection task through particle swarm optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Behavioral-based circular formation control for robot
swarms. <em>ICRA</em>, 8989–8995. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper focuses on coordinating a robot swarm orbiting a convex path without collisions among the individuals. The individual robots lack braking capabilities and can only adjust their courses while maintaining their constant but different speeds. Instead of controlling the spatial relations between the robots, our formation control algorithm aims to deploy a dense robot swarm that mimics the behavior of tornado schooling fish. To achieve this objective safely, we employ a combination of a scalable overtaking rule, a guiding vector field, and a control barrier function with an adaptive radius to facilitate smooth overtakes. The decision-making process of the robots is distributed, relying only on local information. Practical applications include defensive structures or escorting missions with the added resiliency of a swarm without a centralized command. We provide a rigorous analysis of the proposed strategy and validate its effectiveness through numerical simulations involving a high density of unicycles.},
  archive   = {C_ICRA},
  author    = {Jesús Bautista and Héctor García de Marina},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610826},
  month     = {5},
  pages     = {8989-8995},
  title     = {Behavioral-based circular formation control for robot swarms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Task allocation in heterogeneous multi-robot systems based
on preference-driven hedonic game. <em>ICRA</em>, 8967–8972. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multiple preferences between robots and tasks have been largely overlooked in previous research on Multi-Robot Task Allocation (MRTA) problems. In this paper, we propose a preference-driven approach based on hedonic game to address the task allocation problem of muti-robot systems in emergency rescue scenarios. We present a distributed framework considering various preferences between robots and tasks to determine the division of coalitions in such problems and evaluate the scalability and adaptability of our algorithm through relevant experiments. Furthermore, considering the strict communication limitations in emergency rescue scenarios, we have verified that our algorithm can efficiently converge to a Nash-stable coalition partition even in conditions of insufficient communication distance.},
  archive   = {C_ICRA},
  author    = {Liwang Zhang and Minglong Li and Wenjing Yang and Shaowu Yang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611476},
  month     = {5},
  pages     = {8967-8972},
  title     = {Task allocation in heterogeneous multi-robot systems based on preference-driven hedonic game},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gaussian process-enhanced, external and internal convertible
form-based control of underactuated balance robots. <em>ICRA</em>,
8927–8933. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {External and internal convertible (EIC) form-based motion control (i.e., EIC-based control) is one of the effective approaches for underactuated balance robots. By sequentially controller design, trajectory tracking of the actuated subsystem and balance of the unactuated subsystem can be achieved simultaneously. However, with certain conditions, there exists uncontrolled robot motion under the EIC-based control. We first identify these conditions and then propose an enhanced EIC-based control with a Gaussian process data-driven robot dynamic model. Under the new enhanced EIC-based control, the stability and performance of the closed-loop system are guaranteed. We demonstrate the GP-enhanced control experimentally using two examples of underactuated balance robots.},
  archive   = {C_ICRA},
  author    = {Feng Han and Jingang Yi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610172},
  month     = {5},
  pages     = {8927-8933},
  title     = {Gaussian process-enhanced, external and internal convertible form-based control of underactuated balance robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ricmonk: A three-link brachiation robot with passive
grippers for energy-efficient brachiation. <em>ICRA</em>, 8920–8926. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents the design, analysis, and performance evaluation of RicMonk, a novel three-link brachiation robot equipped with passive hook-shaped grippers. Brachiation, an agile and energy-efficient mode of locomotion observed in primates, has inspired the development of RicMonk to explore versatile locomotion and maneuvers on ladder-like structures. The robot’s anatomical resemblance to gibbons and the integration of a tail mechanism for energy injection contribute to its unique capabilities. The paper discusses the use of the Direct Collocation methodology for optimizing trajectories for the robot’s dynamic behaviors and stabilization of these trajectories using a Time-varying Linear Quadratic Regulator. With RicMonk we demonstrate bidirectional brachiation, and provide comparative analysis with its predecessor, AcroMonk - a two-link brachiation robot, to demonstrate that the presence of a passive tail helps improve energy efficiency. The system design, controllers, and software implementation are publicly available on GitHub at https://github.com/dfki-ric-underactuated-lab/ricmonk and the video demonstration of the experiments can be viewed at https://youtu.be/hOuDQI7CD8w.},
  archive   = {C_ICRA},
  author    = {Shourie S. Grama and Mahdi Javadi and Shivesh Kumar and Hossein Zamani Boroujeni and Frank Kirchner},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611003},
  month     = {5},
  pages     = {8920-8926},
  title     = {Ricmonk: A three-link brachiation robot with passive grippers for energy-efficient brachiation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards solving cable-driven parallel robot inaccuracy due
to cable elasticity. <em>ICRA</em>, 8898–8904. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cable elasticity can significantly impact the accuracy of Cable-Driven Parallel Robots (CDPRs). However, it’s frequently disregarded as negligible in CDPR simulations and designs. In this paper, we propose a numerical approach, referred to as SEECR, which is designed to estimate the behavior of a CDPR featuring elastic cables while ensuring the Static Equilibrium (SE) of the Moving-Platform (MP). By modeling the cables as elastic springs, the proposed approach correctly predicts which cables become slack, estimates the tension distribution among cables and computes unwanted MP motions, allowing to predict the impact of design choices. The results have been validated experimentally on two cable types and configurations.},
  archive   = {C_ICRA},
  author    = {Adolfo Suárez Roos and Zane Zake and Tahir Rasheed and Nicolò Pedemonte and Stéphane Caro},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610384},
  month     = {5},
  pages     = {8898-8904},
  title     = {Towards solving cable-driven parallel robot inaccuracy due to cable elasticity},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RASCAL: A scalable, high-redundancy robot for automated
storage and retrieval systems. <em>ICRA</em>, 8868–8874. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automated storage and retrieval systems (ASRS) are a key component of the modern storage industry, and are used in a wide range of applications, carrying anything from lightweight tape cartridges to entire pallets of goods. Many of these systems are under pressure to maximise the use of space by growing in height and density, but this can create challenges for the the robots that service them. In this context, we present RASCAL, a novel ASRS robot for small payload items in structured environments, with a focus on system-level scalability and redundancy. We describe the design objectives of RASCAL and how they address some of the limitations of existing robotic systems in this area, such as scalability and redundancy. We then demonstrate the viability of our design with a proof-of-concept implementation of a data centre storage media robot, and show through a series of experiments that its design, speed, accuracy, and energy efficiency are appropriate for this application.},
  archive   = {C_ICRA},
  author    = {Richard Black and Marco Caballero and Andromachi Chatzieleftheriou and Tim Deegan and Philip Heard and Freddie Hong and Russell Joyce and Sergey Legtchenko and Antony Rowstron and Adam Smith and David Sweeney and Hugh Williams},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610551},
  month     = {5},
  pages     = {8868-8874},
  title     = {RASCAL: A scalable, high-redundancy robot for automated storage and retrieval systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A backdrivable axisymmetric kinematically redundant
(6+3)-degree-of-freedom hybrid parallel manipulator. <em>ICRA</em>,
8835–8841. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A kinematically redundant (6+3)-degree-of-freedom (DOF) hybrid parallel robot with an axisymmetric workspace is proposed. By arranging the first revolute joint of each leg such that they have the same rotation axis, this robot can achieve an axisymmetric workspace, resulting in a large reachable workspace. In addition, type II singularities, which critically limit the orientational workspace, can be fully avoided by utilizing kinematic redundancy. A gripper mechanism is developed to increase the orientational workspace by exploiting the redundant DOFs. Moreover, the orientational workspace can be further increased by controlling one of the redundant DOFs to keep a certain constant angle. As a result, the proposed hybrid parallel robot achieves a high workspace-to-footprint ratio comparable to that of serial robots. A CAD model of the robot and computer animations are provided to demonstrate the large workspaces and the gripper mechanism. A significant advantage of the proposed robot over serial architectures is that the robot is backdrivable since it uses direct-drive or quasi-direct-drive actuators.},
  archive   = {C_ICRA},
  author    = {Jehyeok Kim and Clément Gosselin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610821},
  month     = {5},
  pages     = {8835-8841},
  title     = {A backdrivable axisymmetric kinematically redundant (6+3)-degree-of-freedom hybrid parallel manipulator},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Untethered underwater soft robot with thrust vectoring.
<em>ICRA</em>, 8828–8834. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces DRAGON: Deformable Robot for Agile Guided Observation and Navigation, a free-swimming deformable impeller-powered vectored underwater vehicle (VUV). A 3D-printed wave spring structure directs the water drawn through the center of the robot by an impeller, enabling it to move smoothly in different directions. The robot is designed to have a narrow cylindrical profile to lower drag and improve agility. It has a maximum recorded speed of 2.1 BL/s (body lengths per second) and a minimum cost of transport (COT) of 2.9. The robot has two degrees of freedom (DoFs) and is capable of performing a variety of maneuvers including a full circle with a radius of 0.23 m (1.4 BL) and a figure eight, which it completed in 4.98 s (72.3 °/s) and 10.74 s respectively. We operated the robot, untethered, in various environments to test the robustness of the design and analyze its motion and performance.},
  archive   = {C_ICRA},
  author    = {Robin Hall and Cagdas D. Onal},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610430},
  month     = {5},
  pages     = {8828-8834},
  title     = {Untethered underwater soft robot with thrust vectoring},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and testing of a multi-module, tetherless, soft
robotic eel. <em>ICRA</em>, 8821–8827. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a free-swimming, tetherless, cable-driven modular soft robotic fish. The body comprises a series of 3D-printed wave spring structures that create a flexible biologically inspired shape that is capable of an anguilliform swimming gait. A three-module soft robotic fish was designed, fabricated, and evaluated. The motion of the robot was characterized and different combinations of actuation amplitude, frequency, and phase shift were determined experimentally to determine the optimal parameters that maximized speed and minimized the cost of transport (COT). The maximum speed recorded was 0.20 BL/s (body lengths per second) with a COT of 15.82. These results were compared against other robotic and biological fish. We operated the robot, untethered, in a variety of environments to test how it was able to function outside of laboratory settings.},
  archive   = {C_ICRA},
  author    = {Robin Hall and Gabriel Espinosa and Shou-Shan Chiang and Cagdas D. Onal},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611531},
  month     = {5},
  pages     = {8821-8827},
  title     = {Design and testing of a multi-module, tetherless, soft robotic eel},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Singularity analysis of kinova’s link 6 robot arm via
grassmann line geometry. <em>ICRA</em>, 8814–8820. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unlike parallel robots, for which hundreds of different architectures have been proposed, the vast majority of six-degree-of-freedom (DOF) serial robots have one of two simple architectures. In both architectures, the inverse kinematics can be solved in closed form and the singularities described by trivial geometric and algebraic conditions. These conditions can be readily obtained by analyzing the determinant of the robot’s Jacobian matrix, and provide an in-depth understanding of the robot’s singularities, which is essential for its optimal use. However, for various reasons, robot arms with unorthodox architectures are occasionally designed. Such arms do not have closed-form inverse kinematics and little insight into their singularities can be gained by analyzing the determinant of their Jacobian. One such robot arm for which the conventional singularity analysis approach fails is the new Link 6 collaborative robot by Kinova. In this paper, we study the complex singularities of Link 6 by investigating all possibilities for screw dependencies, deriving a simple equation for each case, and then describing each singularity type using Grassmann line geometry. Twelve different singularity configurations are identified and described with seven relatively simple geometric conditions. Our approach is general and can be applied to other robot arms.},
  archive   = {C_ICRA},
  author    = {Milad Asgari and Ilian A. Bonev and Clément Gosselin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610252},
  month     = {5},
  pages     = {8814-8820},
  title     = {Singularity analysis of kinova’s link 6 robot arm via grassmann line geometry},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A helical bistable soft gripper enable by pneumatic
actuation. <em>ICRA</em>, 8807–8813. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There are many instances of helical mechanisms that are used to efficiently grasp different objects with various shapes and sizes in nature. Inspired by the helical grasping in the nature, we proposed a helical bistable soft gripper with high load capacity and energy saving. An off-the-shelf bistable steel shell (BSS) as the stiff element was inserted into a 3D printing soft helical exoskeleton to achieve coiling around and holding the objects without energy consumption. Two air pouches were designed as the actuator to control the transition between the two stable states. To facilitate gripper design, a simplified model of the gripper was conducted, and the geometric parameters of the gripper are listed in a table for reference. The transition pressures between the two stable states were experimentally characterized. Moreover, we conduct experiments to demonstrate the capability of the gripper in two working modes. The gripper exhibits coiling diameters ranging between 40 mm and 60 mm and is successfully attached to various slender objects of different geometries with a maximum holding force of 92.67 N (up to 135.1 times of its mass) in hanging mode. Finally, the gripper was integrated into a robot arm and successfully grasped different objects, and the maximum grasping weight is 221.6 g in the grasping mode.},
  archive   = {C_ICRA},
  author    = {Xuanchun Yin and Junliang Xie and Pengyu Zhou and Sheng Wen and Jiantao Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610729},
  month     = {5},
  pages     = {8807-8813},
  title     = {A helical bistable soft gripper enable by pneumatic actuation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforcement learning for freeform robot design.
<em>ICRA</em>, 8799–8806. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inspired by the necessity of morphological adaptation in animals, a growing body of work has attempted to expand robot training to encompass physical aspects of a robot’s design. However, reinforcement learning methods capable of optimizing the 3D morphology of a robot have been restricted to reorienting or resizing the limbs of a predetermined and static topological genus. Here we show policy gradients for designing freeform robots with arbitrary external and internal structure. This is achieved through actions that deposit or remove bundles of atomic building blocks to form higher-level nonparametric macrostructures such as appendages, organs and cavities. Although results are provided for open loop control only, we discuss how this method could be adapted for closed loop control and sim2real transfer to physical machines in future.},
  archive   = {C_ICRA},
  author    = {Muhan Li and David Matthews and Sam Kriegman},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610048},
  month     = {5},
  pages     = {8799-8806},
  title     = {Reinforcement learning for freeform robot design},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OriTrack: A small, 3 degree-of-freedom, origami solar
tracker. <em>ICRA</em>, 8792–8798. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In response to the need for sustainable energy solutions, solar panels have gained significant traction. One way to increase the energy capture of solar systems is through solar tracking, a means of reorienting solar panels throughout the day in order to face the sun. The energy consumption increase that comes with solar tracking often far outweighs the amount of energy required to move the panel, which makes it a compelling strategy for improving solar systems. Unfortunately, while solar trackers are commonly used in large solar farms, they are rarely used on rooftops, an area where solar panels are commonly installed. This is for two primary reasons: (1) most commercially available solar trackers are too large to be installed on roofs and (2) even if traditional solar trackers were made in a more compact form-factor it would be difficult to densely lay them out on a roof without the trackers substantially shading each other. In order to address these issues, we introduce OriTrack, a small three-degree-of-freedom (3 DOF) solar tracker which reduces the area of its shadow by reducing its height as it tracks the sun. In this paper we discuss the design, manufacturing, and control of OriTrack. We then compare OriTrack to a flat reference panel, the solar energy solution commonly used on roofs today, and find that OriTrack demonstrates 23% increased energy production. This result suggests OriTrack could be used as a future solution for solar tracking on rooftops.},
  archive   = {C_ICRA},
  author    = {Crystal E. Winston and Leo F. Casey},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611374},
  month     = {5},
  pages     = {8792-8798},
  title     = {OriTrack: A small, 3 degree-of-freedom, origami solar tracker},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Differentiable boustrophedon paths that enable optimization
via gradient descent. <em>ICRA</em>, 8778–8783. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a differentiable representation for the optimization of boustrophedon path plans in convex polygons, explores an additional parameter of these path plans that can be optimized, discusses the properties of this representation that can be leveraged during the optimization process and shows that the previously published attempt at optimization of these path plans was too coarse to be practically useful. Experiments were conducted to show that this differentiable representation can reproduce scores from traditional discrete representations of boustrophedon path plans with high fidelity. Finally, optimization via gradient descent was attempted but found to fail because the search space is far more non-convex than was previously considered in the literature. The wide range of applications for boustrophedon path plans means that this work has the potential to improve path planning efficiency in numerous areas of robotics, including mapping and search tasks using uncrewed aerial systems, environmental sampling tasks using uncrewed marine vehicles, and agricultural tasks using ground vehicles, among numerous others applications.},
  archive   = {C_ICRA},
  author    = {Thomas Manzini and Robin Murphy},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610136},
  month     = {5},
  pages     = {8778-8783},
  title     = {Differentiable boustrophedon paths that enable optimization via gradient descent},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WiTHy a*: Winding-constrained motion planning for tethered
robot using hybrid a*. <em>ICRA</em>, 8771–8777. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, a variant of hybrid A* is developed to find the shortest path for a curvature-constrained robot, that is tethered at its start position, such that the tether satisfies user-defined winding angle constraints. A variant of tangent graphs is used as an underlying graph for searching a path using A * in order to reduce the overall computation and define appropriate cost metrics to ensure winding angle constraints are satisfied. Conditions are provided under which the proposed algorithm is guaranteed to find a winding angle constrained path. The effectiveness and performance of the proposed algorithm are studied in simulation.},
  archive   = {C_ICRA},
  author    = {Vishnu S. Chipade and Rahul Kumar and Sze Zheng Yong},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611175},
  month     = {5},
  pages     = {8771-8777},
  title     = {WiTHy a*: Winding-constrained motion planning for tethered robot using hybrid a*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RETRO: Reactive trajectory optimization for real-time robot
motion planning in dynamic environments. <em>ICRA</em>, 8764–8770. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reactive trajectory optimization for robotics presents formidable challenges, demanding the rapid generation of purposeful robot motion in complex and swiftly changing dynamic environments. While much existing research predominantly addresses robotic motion planning with predefined objectives, emerging problems in robotic trajectory optimization frequently involve dynamically evolving objectives and stochastic motion dynamics. However, effectively addressing such reactive trajectory optimization challenges for robot manipulators proves difficult due to inefficient, high-dimensional trajectory representations and a lack of consideration for time optimization.In response, we introduce a novel trajectory optimization framework called RETRO. RETRO employs adaptive optimization techniques that span both spatial and temporal dimensions. As a result, it achieves a remarkable computing complexity of O(T 2.4 )+O(Tn 2 ), a significant improvement over the traditional application of DDP, which leads to a complexity of O(n 4 ) when reasonable time step sizes are used. To evaluate RETRO’s performance in terms of error, we conducted a comprehensive analysis of its regret bounds, comparing it to an Oracle value function obtained through an Oracle trajectory optimization algorithm. Our analytical findings demonstrate that RETRO’s total regret can be upper-bounded by a function of the chosen time step size. Moreover, our approach delivers smoothly optimized robot trajectories within the joint space, offering flexibility and adaptability for various tasks. It can seamlessly integrate task-specific requirements such as collision avoidance while maintaining real-time control rates. We validate the effectiveness of our framework through extensive simulations and real-world robot experiments in closed-loop manipulation scenarios.For further details and supplementary materials, please visit: https://sites.google.com/view/retro-optimal-control/home},
  archive   = {C_ICRA},
  author    = {Apan Dastider and Hao Fang and Mingjie Lin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610542},
  month     = {5},
  pages     = {8764-8770},
  title     = {RETRO: Reactive trajectory optimization for real-time robot motion planning in dynamic environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gathering data from risky situations with pareto-optimal
trajectories. <em>ICRA</em>, 8757–8763. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a formulation for the risk-aware path planning problem which utilizes multi-objective optimization to dynamically plan trajectories that satisfy multiple complex mission specifications. In the setting of persistent monitoring, we develop a method for representing environmental information and risk in a way that allows for local sampling to generate Pareto-dominant solutions over a receding horizon. We propose two algorithms capable of solving these problems: a dense sampling approach and an improved method utilizing noisy gradient descent. Simulation results demonstrate the efficacy of our methods at persistently gathering information while avoiding risk, robust to randomly-generated environments.},
  archive   = {C_ICRA},
  author    = {Brennan Brodt and Alyssa Pierson},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611689},
  month     = {5},
  pages     = {8757-8763},
  title     = {Gathering data from risky situations with pareto-optimal trajectories},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Motions in microseconds via vectorized sampling-based
planning. <em>ICRA</em>, 8749–8756. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern sampling-based motion planning algorithms typically take between hundreds of milliseconds to dozens of seconds to find collision-free motions for high degree-of-freedom problems. This paper presents performance improvements of more than 500x over the state-of-the-art, bringing planning times into the range of microseconds and solution rates into the range of kilohertz, without specialized hardware. Our key insight is how to exploit fine-grained parallelism within planning, providing generality-preserving algorithmic improvements to any such planner and significantly accelerating critical subroutines, such as forward kinematics and collision checking. We demonstrate our approach over a diverse set of challenging, realistic problems for complex robots ranging from 7 to 14 degrees-of-freedom. Moreover, we show our approach does not require high-power hardware by evaluating on a low-power single-board computer. The planning speeds demonstrated are fast enough to reside in the range of control frequencies and open up new avenues of motion planning research.},
  archive   = {C_ICRA},
  author    = {Wil Thomason and Zachary Kingston and Lydia E. Kavraki},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611190},
  month     = {5},
  pages     = {8749-8756},
  title     = {Motions in microseconds via vectorized sampling-based planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural informed RRT*: Learning-based path planning with
point cloud state representations under admissible ellipsoidal
constraints. <em>ICRA</em>, 8742–8748. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sampling-based planning algorithms like Rapidly-exploring Random Tree (RRT) are versatile in solving path planning problems. RRT* offers asymptotic optimality but requires growing the tree uniformly over the free space, which leaves room for efficiency improvement. To accelerate convergence, rule-based informed approaches sample states in an admissible ellipsoidal subset of the space determined by the current path cost. Learning-based alternatives model the topology of the free space and infer the states close to the optimal path to guide planning. We propose Neural Informed RRT* to combine the strengths from both sides. We define point cloud representations of free states. We perform Neural Focus, which constrains the point cloud within the admissible ellipsoidal subset from Informed RRT*, and feeds into PointNet++ for refined guidance state inference. In addition, we introduce Neural Connect to build connectivity of the guidance state set and further boost performance in challenging planning problems. Our method surpasses previous works in path planning benchmarks while preserving probabilistic completeness and asymptotic optimality. We deploy our method on a mobile robot and demonstrate real world navigation around static obstacles and dynamic humans. Code is available at https://github.com/tedhuang96/nirrt_star.},
  archive   = {C_ICRA},
  author    = {Zhe Huang and Hongyu Chen and John Pohovey and Katherine Driggs-Campbell},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611099},
  month     = {5},
  pages     = {8742-8748},
  title     = {Neural informed RRT*: Learning-based path planning with point cloud state representations under admissible ellipsoidal constraints},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reconfiguration of a 2D structure using spatio-temporal
planning and load transferring. <em>ICRA</em>, 8735–8741. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present progress on the problem of reconfiguring a 2D arrangement of building material by a cooperative group of robots. These robots must avoid collisions, deadlocks, and are subjected to the constraint of maintaining connectivity of the structure. We develop two reconfiguration methods, one based on spatio-temporal planning, and one based on target swapping, to increase building efficiency. The first method can significantly reduce planning times compared to other multi-robot planners. The second method helps to reduce the amount of time robots spend waiting for paths to be cleared, and the overall distance traveled by the robots.},
  archive   = {C_ICRA},
  author    = {Javier Garcia and Michael Yannuzzi and Peter Kramer and Christian Rieck and Sándor P. Fekete and Aaron T. Becker},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611057},
  month     = {5},
  pages     = {8735-8741},
  title     = {Reconfiguration of a 2D structure using spatio-temporal planning and load transferring},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quasi-static path planning for continuum robots by sampling
on implicit manifold. <em>ICRA</em>, 8728–8734. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Continuum robots (CR) offer excellent dexterity and compliance in contrast to rigid-link robots, making them suitable for navigating through, and interacting with, confined environments. However, the study of path planning for CRs while considering external elastic contact is limited. The challenge lies in the fact that CRs can have multiple possible configurations when in contact, rendering the forward kinematics not well-defined, and characterizing the set of feasible robot configurations is non-trivial. In this paper, we propose to perform quasi-static path planning on an implicit manifold. We model elastic obstacles as external potential fields and formulate the robot statics in the potential field as the extremal trajectory of an optimal control problem. We show that the set of stable robot configurations is a smooth manifold diffeomorphic to a submanifold embedded in the product space of the CR actuation and base internal wrench. We then propose to perform path planning on this manifold using AtlasRRT*, a sampling-based planner dedicated to planning on implicit manifolds. Simulations in different operation scenarios were conducted and the results show that the proposed planner outperforms Euclidean space planners in terms of success rate and computational efficiency.},
  archive   = {C_ICRA},
  author    = {Yifan Wang and Yue Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611372},
  month     = {5},
  pages     = {8728-8734},
  title     = {Quasi-static path planning for continuum robots by sampling on implicit manifold},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RBI-RRT*: Efficient sampling-based path planning for
high-dimensional state space. <em>ICRA</em>, 8721–8727. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sampling-based planning algorithms such as RRT have been proved to be efficient in solving path planning problems for robotic systems. Various improvements to the RRT algorithm have been presented to improve the performance of the extension and convergence of the random trees, such as Informed RRT*. However, with the growth of spatial dimensions, the time consumption of randomly sampling the entire state space and incrementally rewiring the random trees raises drastically before a feasible solution is found. In this paper, to enhance the convergence performance of optimal solutions, we present Reconstructed Bi-directional Informed RRT* (RBI-RRT*) path planning algorithm. The algorithm acts as RRT-Connect to rapidly find a feasible solution, which helps compress the sampling space as Informed RRT* does. After the random trees are transformed into RRT* structure by the reconstruction process in RBI-RRT*, the algorithm continues to find the near-optimal path. A series of simulations and real-world robot experiments were conducted to evaluate the algorithm against existing planning algorithms. Compared to Informed RRT* Connect, RBI-RRT* reduced the computation time of achieving a specific cost by 22.1% on average in simulations and 11.2% in the real-world robotic arm experiments. The results show that RBI-RRT* is more efficient in high-dimensional planning problems.},
  archive   = {C_ICRA},
  author    = {Fang Chen and Yu Zheng and Zheng Wang and Wanchao Chi and Sicong Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610975},
  month     = {5},
  pages     = {8721-8727},
  title     = {RBI-RRT*: Efficient sampling-based path planning for high-dimensional state space},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A trajectory-based flight assistive system for novice pilots
in drone racing scenario. <em>ICRA</em>, 8714–8720. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Drone racing has become a popular international competition and has attained wide attention in recent years. However, the requirements of high-level operation keep the novice pilots away from participating in it. This paper presents a trajectory-based flight assistive system that enables various operators to fly the drone in a racing scene at a high speed. The whole system is structured hierarchically, consisting of both offline and online components. In the offline part, a global time-optimal trajectory is generated as the expert reference, and a dense flight corridor is constructed to provide sufficiently large safe region. In the online part, a remote control-mapped primitive is designed to fast encapsulate pilots’ inputs, and the time mapping based trajectory progress is customized to further capture intention. Then, a trajectory planner is proposed to generate intention-aligned, smooth, feasible, and safe trajectories periodically. Additionally, a yaw planning that provides the pilot with the best suitable view angle is employed to further alleviate the operation difficulty. Simulations and real world experiments are implemented to verify the performance of our system. The maximum flight speed can reach 6.0 m/s for a novice drone pilot in a real racing scene. Our code is released as an open-source package 1 .},
  archive   = {C_ICRA},
  author    = {Yuhang Zhong and Guangyu Zhao and Qianhao Wang and Guangtong Xu and Chao Xu and Fei Gao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610179},
  month     = {5},
  pages     = {8714-8720},
  title     = {A trajectory-based flight assistive system for novice pilots in drone racing scenario},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatial assisted human-drone collaborative navigation and
interaction through immersive mixed reality. <em>ICRA</em>, 8707–8713.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Aerial robots have the potential to play a crucial role in assisting humans with complex and dangerous tasks. Nevertheless, the future industry demands innovative solutions to streamline the interaction process between humans and drones to enable seamless collaboration and efficient coworking. In this paper, we present a novel tele-immersive framework that promotes cognitive and physical collaboration between humans and robots through Mixed Reality (MR). This framework incorporates a novel bi-directional spatial awareness and a multi-modal virtual-physical interaction approaches. The former seamlessly integrates the physical and virtual worlds, offering bidirectional egocentric and exocentric environmental representations. The latter, leveraging the proposed spatial representation, further enhances the collaboration combining a robot planning algorithm for obstacle avoidance with a variable admittance control. This allows users to issue commands based on virtual forces while maintaining compatibility with the environment map. We validate the proposed approach by performing several collaborative planning and exploration tasks involving a drone and an user equipped with a MR headset.},
  archive   = {C_ICRA},
  author    = {Luca Morando and Giuseppe Loianno},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611351},
  month     = {5},
  pages     = {8707-8713},
  title     = {Spatial assisted human-drone collaborative navigation and interaction through immersive mixed reality},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sequential trajectory optimization for externally-actuated
modular manipulators with joint locking. <em>ICRA</em>, 8700–8706. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a novel trajectory planning method for externally-actuated modular manipulators (EAMMs), consisting of multiple rotor-actuated links with joints that can be either locked or unlocked. This joint-locking feature allows effective balancing of the payload capacity and dexterity of the robot but significantly complicates the planning problem by introducing binary decision variables. To address this challenge, we leverage the problem’s intrinsic structure, i.e., the payload at the end-effector being enhanced by merely locking its immediate connected links; this allows us to break down the complex planning problem into a series of manageable subproblems and solve them sequentially. Our approach significantly reduces the problem’s complexity: in a serial n-link EAMM with m joint-lock mechanisms, where there could potentially be 2 m distinct configurational dynamics, we require solving only n + 1 trajectory optimization problems for single rigid body dynamics sequentially, thereby rendering the problem tractable. We substantiate the efficacy of our method through various simulation and experimental studies, covering ground-free and ground-bound configurations as well as both motion-only and manipulation tasks.},
  archive   = {C_ICRA},
  author    = {Jaeu Choe and Jeongseob Lee and Hyunsoo Yang and Hai-Nguyen Nguyen and Dongjun Lee},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611268},
  month     = {5},
  pages     = {8700-8706},
  title     = {Sequential trajectory optimization for externally-actuated modular manipulators with joint locking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Time-optimal gate-traversing planner for autonomous drone
racing. <em>ICRA</em>, 8693–8699. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In drone racing, the time-minimum trajectory is affected by the drone’s capabilities, the layout of the race track, and the configurations of the gates (e.g., their shapes and sizes). However, previous studies neglect the configuration of the gates, simply rendering drone racing a waypoint-passing task. This formulation often leads to a conservative choice of paths through the gates, as the spatial potential of the gates is not fully utilized. To address this issue, we present a time-optimal planner that can faithfully model gate constraints with various configurations and thereby generate a more time-efficient trajectory while considering the single-rotor-thrust limits. Our approach excels in computational efficiency which only takes a few seconds to compute the full state and control trajectories of the drone through tracks with dozens of different gates. Extensive simulations and experiments confirm the effectiveness of the proposed methodology, showing that the lap time can be further reduced by taking into account the gate’s configuration. We validate our planner in real-world flights and demonstrate super-extreme flight trajectory through race tracks.},
  archive   = {C_ICRA},
  author    = {Chao Qin and Maxime S.J. Michet and Jingxiang Chen and Hugh H.-T. Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610148},
  month     = {5},
  pages     = {8693-8699},
  title     = {Time-optimal gate-traversing planner for autonomous drone racing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FC-planner: A skeleton-guided planning framework for fast
aerial coverage of complex 3D scenes. <em>ICRA</em>, 8686–8692. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D coverage path planning for UAVs is a crucial problem in diverse practical applications. However, existing methods have shown unsatisfactory system simplicity, computation efficiency, and path quality in large and complex scenes. To address these challenges, we propose FC-Planner, a skeleton-guided planning framework that can achieve fast aerial coverage of complex 3D scenes without pre-processing. We decompose the scene into several simple subspaces by a skeleton-based space decomposition (SSD). Additionally, the skeleton guides us to effortlessly determine free space. We utilize the skeleton to efficiently generate a minimal set of specialized and informative viewpoints for complete coverage. Based on SSD, a hierarchical planner effectively divides the large planning problem into independent sub-problems, enabling parallel planning for each subspace. The carefully designed global and local planning strategies are then incorporated to guarantee both high quality and efficiency in path generation. We conduct extensive benchmark and real-world tests, where FC-Planner computes over 10 times faster compared to state-of-the-art methods with shorter path and more complete coverage. The source code will be made publicly available to benefit the community 3 . Project page: https://hkust-aerial-robotics.github.io/FC-Planner.},
  archive   = {C_ICRA},
  author    = {Chen Feng and Haojia Li and Mingjie Zhang and Xinyi Chen and Boyu Zhou and Shaojie Shen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610621},
  month     = {5},
  pages     = {8686-8692},
  title     = {FC-planner: A skeleton-guided planning framework for fast aerial coverage of complex 3D scenes},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Co-design optimisation of morphing topology and control of
winged drones. <em>ICRA</em>, 8679–8685. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The design and control of winged aircraft and drones is an iterative process aimed at identifying a compromise of mission-specific costs and constraints. When agility is required, shape-shifting (morphing) drones represent an efficient solution. However, morphing drones require the addition of actuated joints that increase the topology and control coupling, making the design process more complex. We propose a co-design optimisation method that assists the engineers by proposing a morphing drone’s conceptual design that includes topology, actuation, morphing strategy, and controller parameters. The method consists of applying multi-objective constraint-based optimisation to a multi-body winged drone with trajectory optimisation to solve the motion intelligence problem under diverse flight mission requirements, such as energy consumption and mission completion time. We show that co-designed morphing drones outperform fixed-winged drones in terms of energy efficiency and mission time, suggesting that the proposed co-design method could be a useful addition to the aircraft engineering toolbox.},
  archive   = {C_ICRA},
  author    = {Fabio Bergonti and Gabriele Nava and Valentin Wüest and Antonello Paolino and Giuseppe L’Erario and Daniele Pucci and Dario Floreano},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611506},
  month     = {5},
  pages     = {8679-8685},
  title     = {Co-design optimisation of morphing topology and control of winged drones},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CalliRewrite: Recovering handwriting behaviors from
calligraphy images without supervision. <em>ICRA</em>, 8671–8678. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human-like planning skills and dexterous manipulation have long posed challenges in the fields of robotics and artificial intelligence (AI). The task of reinterpreting calligraphy presents a formidable challenge, as it involves the decomposition of strokes and dexterous utensil control. Previous efforts have primarily focused on supervised learning of a single instrument, limiting the performance of robots in the realm of cross-domain text replication. To address these challenges, we propose CalliRewrite: a coarse-to-fine approach for robot arms to discover and recover plausible writing orders from diverse calligraphy images without requiring labeled demonstrations. Our model achieves fine-grained control of various writing utensils. Specifically, an unsupervised image-to-sequence model decomposes a given calligraphy glyph to obtain a coarse stroke sequence. Using an RL algorithm, a simulated brush is fine-tuned to generate stylized trajectories for robotic arm control. Evaluation in simulation and physical robot scenarios reveals that our method successfully replicates unseen fonts and styles while achieving integrity in unknown characters. To access our code and supplementary materials, please visit our project page: https://luoprojectpage.github.io/callirewrite/.},
  archive   = {C_ICRA},
  author    = {Yuxuan Luo and Zekun Wu and Zhouhui Lian},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610332},
  month     = {5},
  pages     = {8671-8678},
  title     = {CalliRewrite: Recovering handwriting behaviors from calligraphy images without supervision},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CoPAL: Corrective planning of robot actions with large
language models. <em>ICRA</em>, 8664–8670. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the pursuit of fully autonomous robotic systems capable of taking over tasks traditionally performed by humans, the complexity of open-world environments poses a considerable challenge. Addressing this imperative, this study contributes to the field of Large Language Models (LLMs) applied to task and motion planning for robots. We propose a system architecture that orchestrates a seamless interplay between multiple cognitive levels, encompassing reasoning, planning, and motion generation. At its core lies a novel replanning strategy that handles physically grounded, logical, and semantic errors in the generated plans. We demonstrate the efficacy of the proposed feedback architecture, particularly its impact on executability, correctness, and time complexity via empirical evaluation in the context of a simulation and two intricate real-world scenarios: blocks world, barman and pizza preparation.},
  archive   = {C_ICRA},
  author    = {Frank Joublin and Antonello Ceravola and Pavel Smirnov and Felix Ocker and Joerg Deigmoeller and Anna Belardinelli and Chao Wang and Stephan Hasler and Daniel Tanneberg and Michael Gienger},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610434},
  month     = {5},
  pages     = {8664-8670},
  title     = {CoPAL: Corrective planning of robot actions with large language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient and accurate transformer-based 3D shape completion
and reconstruction of fruits for agricultural robots. <em>ICRA</em>,
8657–8663. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots that operate in agricultural environments need a robust perception system that can deal with occlusions, which are naturally present in agricultural scenarios. In this paper, we address the problem of estimating 3D shapes of fruits when only partial observations are available. Generally speaking, such a shape completion can be realized by exploiting prior knowledge about the geometry of the fruit. This is typically done by template matching using traditional optimization algorithms, which are slow but accurate, or by encoding such knowledge into the weights of a neural network, leading to faster but often less accurate estimates. Our approach combines the best of both worlds. It exploits the benefit of having a template representing our object of interest with the advantages of using a neural network to learn how to deform a template. Our experimental evaluation demonstrates that our approach yields accurate estimation at a competitively low inference time in challenging greenhouse environments.},
  archive   = {C_ICRA},
  author    = {Federico Magistri and Rodrigo Marcuzzi and Elias Marks and Matteo Sodano and Jens Behley and Cyrill Stachniss},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611717},
  month     = {5},
  pages     = {8657-8663},
  title     = {Efficient and accurate transformer-based 3D shape completion and reconstruction of fruits for agricultural robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning to walk in confined spaces using 3D representation.
<em>ICRA</em>, 8649–8656. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Legged robots have the potential to traverse complex terrain and access confined spaces beyond the reach of traditional platforms thanks to their ability to carefully select footholds and flexibly adapt their body posture while walking. However, robust deployment in real-world applications is still an open challenge. In this paper, we present a method for legged locomotion control using reinforcement learning and 3D volumetric representations to enable robust and versatile locomotion in confined and unstructured environments. By employing a two-layer hierarchical policy structure, we exploit the capabilities of a highly robust low-level policy to follow 6D commands and a high-level policy to enable three-dimensional spatial awareness for navigating under overhanging obstacles. Our study includes the development of a procedural terrain generator to create diverse training environments. We present a series of experimental evaluations in both simulation and real-world settings, demonstrating the effectiveness of our approach in controlling a quadruped robot in confined, rough terrain. By achieving this, our work extends the applicability of legged robots to a broader range of scenarios.},
  archive   = {C_ICRA},
  author    = {Takahiro Miki and Joonho Lee and Lorenz Wellhausen and Marco Hutter},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610271},
  month     = {5},
  pages     = {8649-8656},
  title     = {Learning to walk in confined spaces using 3D representation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Censible: A robust and practical global localization
framework for planetary surface missions. <em>ICRA</em>, 8642–8648. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To achieve longer driving distances, planetary robotics missions require accurate localization to counteract position uncertainty. Freedom and precision in driving allows scientists to reach and study sites of interest. Typically, rover global localization has been performed manually by humans, which is accurate but time-consuming as data is relayed between planets. This paper describes a global localization algorithm that is run onboard the Perseverance Mars rover. Our approach matches rover images to orbital maps using a modified census transform to achieve sub-meter accurate, near-human localization performance on a real dataset of 264 Mars rover panoramas. The proposed solution has also been successfully executed on the Perseverance Mars Rover, demonstrating the practicality of our approach.},
  archive   = {C_ICRA},
  author    = {Jeremy Nash and Quintin Dwight and Lucas Saldyt and Haoda Wang and Steven Myint and Adnan Ansar and Vandi Verma},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611697},
  month     = {5},
  pages     = {8642-8648},
  title     = {Censible: A robust and practical global localization framework for planetary surface missions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HERO-SLAM: Hybrid enhanced robust optimization of neural
SLAM. <em>ICRA</em>, 8610–8616. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simultaneous Localization and Mapping (SLAM) is a fundamental task in robotics, driving numerous applications such as autonomous driving and virtual reality. Recent progress on neural implicit SLAM has shown encouraging and impressive results. However, the robustness of neural SLAM, particularly in challenging or data-limited situations, remains an unresolved issue. This paper presents HERO-SLAM, a Hybrid Enhanced Robust Optimization method for neural SLAM, which combines the benefits of neural implicit field and feature-metric optimization. This hybrid method optimizes a multi-resolution implicit field and enhances robustness in challenging environments with sudden viewpoint changes or sparse data collection. Our comprehensive experimental results on benchmarking datasets validate the effectiveness of our hybrid approach, demonstrating its superior performance over existing implicit field-based methods in challenging scenarios. HERO-SLAM provides a new pathway to enhance the stability, performance, and applicability of neural SLAM in real-world scenarios. Project page: https://hero-slam.github.io.},
  archive   = {C_ICRA},
  author    = {Zhe Xin and Yufeng Yue and Liangjun Zhang and Chenming Wu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610000},
  month     = {5},
  pages     = {8610-8616},
  title     = {HERO-SLAM: Hybrid enhanced robust optimization of neural SLAM},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ground-fusion: A low-cost ground SLAM system robust to
corner cases. <em>ICRA</em>, 8603–8609. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce Ground-Fusion, a low-cost sensor fusion simultaneous localization and mapping (SLAM) system for ground vehicles. Our system features efficient initialization, effective sensor anomaly detection and handling, real-time dense color mapping, and robust localization in diverse environments. We tightly integrate RGB-D images, inertial measurements, wheel odometer and GNSS signals within a factor graph to achieve accurate and reliable localization both indoors and outdoors. To ensure successful initialization, we propose an efficient strategy that comprises three different methods: stationary, visual, and dynamic, tailored to handle diverse cases. Furthermore, we develop mechanisms to detect sensor anomalies and degradation, handling them adeptly to maintain system accuracy. Our experimental results on both public and self-collected datasets demonstrate that Ground-Fusion outperforms existing low-cost SLAM systems in corner cases. We release the code and datasets at https://github.com/SJTU-ViSYS/Ground-Fusion.},
  archive   = {C_ICRA},
  author    = {Jie Yin and Ang Li and Wei Xi and Wenxian Yu and Danping Zou},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610070},
  month     = {5},
  pages     = {8603-8609},
  title     = {Ground-fusion: A low-cost ground SLAM system robust to corner cases},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Degradation resilient LiDAR-radar-inertial odometry.
<em>ICRA</em>, 8587–8594. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Enabling autonomous robots to operate robustly in challenging environments is necessary in a future with increased autonomy. For many autonomous systems, estimation and odometry remains a single point of failure, from which it can often be difficult, if not impossible, to recover. As such robust odometry solutions are of key importance. In this work a method for tightly-coupled LiDAR-Radar-Inertial fusion for odometry is proposed, enabling the mitigation of the effects of LiDAR degeneracy by leveraging a complementary perception modality while preserving the accuracy of LiDAR in well-conditioned environments. The proposed approach combines modalities in a factor graph-based windowed smoother with sensor information-specific factor formulations which enable, in the case of degeneracy, partial information to be conveyed to the graph along the non-degenerate axes. The proposed method is evaluated in real-world tests on a flying robot experiencing degraded conditions including geometric self-similarity as well as obscurant occlusion. For the benefit of the community we release the datasets presented: https://github.com/ntnu-arl/lidar_degeneracy_datasets.},
  archive   = {C_ICRA},
  author    = {Morten Nissov and Nikhil Khedekar and Kostas Alexis},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611444},
  month     = {5},
  pages     = {8587-8594},
  title     = {Degradation resilient LiDAR-radar-inertial odometry},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CURL-MAP: Continuous mapping and positioning with CURL
representation†. <em>ICRA</em>, 8580–8586. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Maps of LiDAR Simultaneous Localisation and Mapping (SLAM) are often represented as point clouds. They usually take up a huge amount of storage space for large-scale environments, otherwise much structural detail may not be kept. In this paper, a novel paradigm of LiDAR mapping and odometry is designed by leveraging the Continuous and Ultra-compact Representation of LiDAR (CURL) proposed in [1]. Termed CURL-MAP (Mapping and Positioning), the proposed approach can not only reconstruct 3D maps with a continuously varying density but also efficiently reduce map storage space by using CURL’s spherical harmonics implicit encoding. Different from the popular Iterative Closest Point (ICP) based LiDAR odometry techniques, CURL-MAP formulates LiDAR pose estimation as a unique optimisation problem tailored for CURL. Experiment evaluation shows that CURL-MAP achieves state-of-the-art 3D mapping results and competitive LiDAR odometry accuracy. We will release the CURL-MAP codes for the community.},
  archive   = {C_ICRA},
  author    = {Kaicheng Zhang and Yining Ding and Shida Xu and Ziyang Hong and Xianwen Kong and Sen Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610760},
  month     = {5},
  pages     = {8580-8586},
  title     = {CURL-MAP: Continuous mapping and positioning with CURL representation†},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DISO: Direct imaging sonar odometry. <em>ICRA</em>,
8573–8579. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a novel sonar odometry system that estimates the relative spatial transformation between two sonar image frames. Considering the unique challenges, such as low resolution and high noise, of sonar imagery for odometry and Simultaneous Localization and Mapping (SLAM), the proposed Direct Imaging Sonar Odometry (DISO) system is designed to estimate the relative transformation between two sonar frames by minimizing the aggregated sonar intensity errors of points with high intensity gradients. Moreover, DISO is implemented to incorporate a multi-sensor window optimization technique, a data association strategy and an acoustic intensity outlier rejection algorithm for reliability and accuracy. The effectiveness of DISO is evaluated using both simulated and real-world sonar datasets, showing that it outperforms the existing geometric-only method on localization accuracy and achieves state-of-the-art sonar odometry performance. We release the source codes of the DISO implementation to the community. The source code is available at https://github.com/SenseRoboticsLab/DISO.},
  archive   = {C_ICRA},
  author    = {Shida Xu and Kaicheng Zhang and Ziyang Hong and Yuanchang Liu and Sen Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611064},
  month     = {5},
  pages     = {8573-8579},
  title     = {DISO: Direct imaging sonar odometry},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Campus map: A large-scale dataset to support multi-view VO,
SLAM and BEV estimation. <em>ICRA</em>, 8566–8572. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Significant advances in robotics and machine learning have resulted in many datasets designed to support research into autonomous vehicle technology. However, these datasets are rarely suitable for a wide variety of navigation tasks. For example, datasets that include multiple cameras often have short trajectories without loops that are unsuitable for the evaluation of longer-range SLAM or odometry systems, and datasets with a single camera often lack other sensors, making them unsuitable for sensor fusion approaches. Furthermore, alternative environmental representations such as semantic Bird’s Eye View (BEV) maps are growing in popularity, but datasets often lack accurate ground truth and are not flexible enough to adapt to new research trends.To address this gap, we introduce Campus Map, a novel large-scale multi-camera dataset with 2M images from 6 mounted cameras that includes GPS data and 64-beam, 125k point LiDAR scans totalling 8M points (raw packets also provided). The dataset consists of 16 sequences in a large car park and 6 long-term trajectories around a university campus that provide data to support research into a variety of autonomous driving and parking tasks. Long trajectories (average 10 min) and many loops make the dataset ideal for the evaluation of SLAM, odometry and loop closure algorithms, and we provide several state-of-the-art baselines.We also include 40k semantic BEV maps rendered from a digital twin. This novel approach to ground truth generation allows us to produce more accurate and crisp semantic maps than are currently available. We make the simulation environment available to allow researchers to adapt the dataset to their specific needs. Dataset available at: cvssp.org/data/twizy_data},
  archive   = {C_ICRA},
  author    = {James Ross and Nimet Kaygusuz and Oscar Mendez and Richard Bowden},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610656},
  month     = {5},
  pages     = {8566-8572},
  title     = {Campus map: A large-scale dataset to support multi-view VO, SLAM and BEV estimation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). KDD-LOAM: Jointly learned keypoint detector and descriptors
assisted LiDAR odometry and mapping. <em>ICRA</em>, 8559–8565. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sparse keypoint matching based on distinct 3D feature representations can improve the efficiency and robustness of point cloud registration. Existing learning-based 3D descriptors and keypoint detectors are either independent or loosely coupled, so they cannot fully adapt to each other. In this work, we propose a tightly coupled keypoint detector and descriptor (TCKDD) based on a multi-task fully convolutional network with a probabilistic detection loss. In particular, this self-supervised detection loss fully adapts the keypoint detector to any jointly learned descriptors and benefits the self-supervised learning of descriptors. Extensive experiments on both indoor and outdoor datasets show that our TCKDD achieves state-of- the-art performance in point cloud registration. Furthermore, we design a keypoint detector and descriptors-assisted LiDAR odometry and mapping framework (KDD-LOAM), whose real-time odometry relies on keypoint descriptor matching-based RANSAC. The sparse keypoints are further used for efficient scan-to-map registration and mapping. Experiments on KITTI dataset demonstrate that KDD-LOAM significantly surpasses LOAM and shows competitive performance in odometry.},
  archive   = {C_ICRA},
  author    = {Renlang Huang and Minglei Zhao and Jiming Chen and Liang Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610557},
  month     = {5},
  pages     = {8559-8565},
  title     = {KDD-LOAM: Jointly learned keypoint detector and descriptors assisted LiDAR odometry and mapping},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HR-APR: APR-agnostic framework with uncertainty estimation
and hierarchical refinement for camera relocalisation. <em>ICRA</em>,
8544–8550. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Absolute Pose Regressors (APRs) directly estimate camera poses from monocular images, but their accuracy is unstable for different queries. Uncertainty-aware APRs provide uncertainty information on the estimated pose, alleviating the impact of these unreliable predictions. However, existing uncertainty modelling techniques are often coupled with a specific APR architecture, resulting in suboptimal performance compared to state-of-the-art (SOTA) APR methods. This work introduces a novel APR-agnostic framework, HR-APR, that formulates uncertainty estimation as cosine similarity estimation between the query and database features. It does not rely on or affect APR network architecture, which is flexible and computationally efficient. In addition, we take advantage of the uncertainty for pose refinement to enhance the performance of APR. The extensive experiments demonstrate the effectiveness of our framework, reducing 27.4% and 15.2% of computational overhead on the 7Scenes and Cambridge Landmarks datasets while maintaining the SOTA accuracy in single-image APRs.},
  archive   = {C_ICRA},
  author    = {Changkun Liu and Shuai Chen and Yukun Zhao and Huajian Huang and Victor Prisacariu and Tristan Braud},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610903},
  month     = {5},
  pages     = {8544-8550},
  title     = {HR-APR: APR-agnostic framework with uncertainty estimation and hierarchical refinement for camera relocalisation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SAGE-ICP: Semantic information-assisted ICP. <em>ICRA</em>,
8537–8543. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robust and accurate pose estimation in unknown environments is an essential part of robotic applications. We focus on LiDAR-based point-to-point ICP combined with effective semantic information. This paper proposes a novel semantic information-assisted ICP method named SAGE-ICP, which leverages semantics in odometry. The semantic information for the whole scan is timely and efficiently extracted by a 3D convolution network, and these point-wise labels are deeply involved in every part of the registration, including semantic voxel downsampling, data association, adaptive local map, and dynamic vehicle removal. Unlike previous semantic-aided approaches, the proposed method can improve localization accuracy in large-scale scenes even if the semantic information has certain errors. Experimental evaluations on KITTI and KITTI-360 show that our method outperforms the baseline methods, and improves accuracy while maintaining real-time performance, i.e., runs faster than the sensor frame rate.},
  archive   = {C_ICRA},
  author    = {Jiaming Cui and Jiming Chen and Liang Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610280},
  month     = {5},
  pages     = {8537-8543},
  title     = {SAGE-ICP: Semantic information-assisted ICP},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EffLoc: Lightweight vision transformer for efficient 6-DOF
camera relocalization. <em>ICRA</em>, 8529–8536. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Camera relocalization is pivotal in computer vision, with applications in AR, drones, robotics, and autonomous driving. It estimates 3D camera position and orientation (6-DoF) from images. Unlike traditional methods like SLAM, recent strides use deep learning for direct end-to-end pose estimation. We propose EffLoc, a novel efficient Vision Transformer for single-image camera relocalization. EffLoc’s hierarchical layout, memory-bound self-attention, and feed-forward layers boost memory efficiency and inter-channel communication. Our introduced sequential group attention (SGA) module enhances computational efficiency by diversifying input features, reducing redundancy, and expanding model capacity. EffLoc excels in efficiency and accuracy, outperforming prior methods, such as AtLoc and MapNet. It thrives on large-scale outdoor car-driving scenario, ensuring simplicity, end-to-end trainability, and eliminating handcrafted loss functions.},
  archive   = {C_ICRA},
  author    = {Zhendong Xiao and Changhao Chen and Shan Yang and Wu Wei},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611622},
  month     = {5},
  pages     = {8529-8536},
  title     = {EffLoc: Lightweight vision transformer for efficient 6-DOF camera relocalization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Increasing SLAM pose accuracy by ground-to-satellite image
registration. <em>ICRA</em>, 8522–8528. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision-based localization for autonomous driving has been of great interest among researchers. When a pre-built 3D map is not available, the techniques of visual simultaneous localization and mapping (SLAM) are typically adopted. Due to error accumulation, visual SLAM (vSLAM) usually suffers from long-term drift. This paper proposes a framework to increase the localization accuracy by fusing the vSLAM with a deep-learning based ground-to-satellite (G2S) image registration method. In this framework, a coarse (spatial correlation bound check) to fine (visual odometry consistency check) method is designed to select the valid G2S prediction. The selected prediction is then fused with the SLAM measurement by solving a scaled pose graph problem. To further increase the localization accuracy, we provide an iterative trajectory fusion pipeline. The proposed framework is evaluated on two well-known autonomous driving datasets, and the results demonstrate the accuracy and robustness in terms of vehicle localization. The code will be available at https://github.com/YanhaoZhang/SLAM-G2S-Fusion.},
  archive   = {C_ICRA},
  author    = {Yanhao Zhang and Yujiao Shi and Shan Wang and Ankit Vora and Akhil Perincherry and Yongbo Chen and Hongdong Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611079},
  month     = {5},
  pages     = {8522-8528},
  title     = {Increasing SLAM pose accuracy by ground-to-satellite image registration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Looking beneath more: A sequence-based localizing ground
penetrating radar framework. <em>ICRA</em>, 8515–8521. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Localizing ground penetrating radar (LGPR) has been proven to be a promising technology for robot localization in various dynamic environments. However, the extreme scarcity of underground features introduces false candidate matches and brings unique challenges to this task. In this paper, we propose a sequence-based framework for LGPR to address the aforementioned issues. Specifically, we first introduce a trainable strategy to extract robust underground features in multi-weather conditions. By further using sequential information, our LGPR system can observe richer underground scene contexts, and the associated multi-frame scans could also improve the performance of underground place recognition. We demonstrate the superiority of our proposed method by comparing it against several recent state-of-the-art baseline methods applied to GPR image tasks. Experimental results on large public and self-collected datasets show that our proposed framework significantly improves the performance of various baselines in different scenarios.},
  archive   = {C_ICRA},
  author    = {Pengyu Zhang and Shuaifeng Zhi and Yuelin Yuan and Beizhen Bi and Qin Xin and Xiaotao Huang and Liang Shen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610174},
  month     = {5},
  pages     = {8515-8521},
  title     = {Looking beneath more: A sequence-based localizing ground penetrating radar framework},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LHMap-loc: Cross-modal monocular localization using LiDAR
point cloud heat map. <em>ICRA</em>, 8500–8506. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Localization using a monocular camera in the pre-built LiDAR point cloud map has drawn increasing attention in the field of autonomous driving and mobile robotics. However, there are still many challenges (e.g. difficulties of map storage, poor localization robustness in large scenes) in accurately and efficiently implementing cross-modal localization. To solve these problems, a novel pipeline termed LHMap-loc is proposed, which achieves accurate and efficient monocular localization in LiDAR maps. Firstly, feature encoding is carried out on the original LiDAR point cloud map by generating offline heat point clouds, by which the size of the original LiDAR map is compressed. Then, an end-to-end online pose regression network is designed based on optical flow estimation and spatial attention to achieve real-time monocular visual localization in a pre-built map. In addition, a series of experiments have been conducted to prove the effectiveness of the proposed method. Our code is available at: https://github.com/IRMVLab/LHMap-loc.},
  archive   = {C_ICRA},
  author    = {Xinrui Wu and Jianbo Xu and Puyuan Hu and Guangming Wang and Hesheng Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610718},
  month     = {5},
  pages     = {8500-8506},
  title     = {LHMap-loc: Cross-modal monocular localization using LiDAR point cloud heat map},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A coarse-to-fine place recognition approach using
attention-guided descriptors and overlap estimation. <em>ICRA</em>,
8493–8499. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Place recognition is a challenging but crucial task in robotics. Current description-based methods may be limited by representation capabilities, while pairwise similarity-based methods require exhaustive searches, which is time-consuming. In this paper, we present a novel coarse-to-fine approach to address these problems, which combines BEV (Bird’s Eye View) feature extraction, coarse-grained matching and fine-grained verification. In the coarse stage, our approach utilizes an attention-guided network to generate attention-guided descriptors. We then employ a fast affinity-based candidate selection process to identify the Top-K most similar candidates. In the fine stage, we estimate pairwise overlap among the narrowed-down place candidates to determine the final match. Experimental results on the KITTI and KITTI-360 datasets demonstrate that our approach outperforms state-of-the-art methods. The code will be released publicly soon.},
  archive   = {C_ICRA},
  author    = {Chencan Fu and Lin Li and Jianbiao Mei and Yukai Ma and Linpeng Peng and Xiangrui Zhao and Yong Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611569},
  month     = {5},
  pages     = {8493-8499},
  title     = {A coarse-to-fine place recognition approach using attention-guided descriptors and overlap estimation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WayFASTER: A self-supervised traversability prediction for
increased navigation awareness. <em>ICRA</em>, 8486–8492. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate and robust navigation in unstructured environments requires fusing data from multiple sensors. Such fusion ensures that the robot is better aware of its surroundings, including areas of the environment that are not immediately visible but were visible at a different time. To solve this problem, we propose a method for traversability prediction in challenging outdoor environments using a sequence of RGB and depth images fused with pose estimations. Our method, termed WayFASTER (Waypoints-Free Autonomous System for Traversability with Enhanced Robustness), uses experience data recorded from a receding horizon estimator to train a self-supervised neural network for traversability prediction, eliminating the need for heuristics. Our experiments demonstrate that our method excels at avoiding obstacles, and correctly detects that traversable terrains, such as tall grass, can be navigable. By using a sequence of images, WayFASTER significantly enhances the robot’s awareness of its surroundings, enabling it to predict the traversability of terrains that are not immediately visible. This enhanced awareness contributes to better navigation performance in environments where such predictive capabilities are essential.},
  archive   = {C_ICRA},
  author    = {Mateus V. Gasparino and Arun N. Sivakumar and Girish Chowdhary},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610436},
  month     = {5},
  pages     = {8486-8492},
  title     = {WayFASTER: A self-supervised traversability prediction for increased navigation awareness},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). System calibration of a field phenotyping robot with
multiple high-precision profile laser scanners. <em>ICRA</em>,
8471–8477. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The creation of precise and high-resolution crop point clouds in agricultural fields has become a key challenge for high-throughput phenotyping applications. This work implements a novel calibration method to calibrate the laser scanning system of an agricultural field robot consisting of two industrial-grade laser scanners used for high-precise 3D crop point cloud creation. The calibration method optimizes the transformation between the scanner origins and the robot pose by minimizing 3D point omnivariances within the point cloud. Moreover, we present a novel factor graph-based pose estimation method that fuses total station prism measurements with IMU and GNSS heading information for high-precise pose determination during calibration. The root-mean-square error of the distances to a georeferenced ground truth point cloud results in 0.8 cm after parameter optimization. Furthermore, our results show the importance of a reference point cloud in the calibration method needed to estimate the vertical translation of the calibration. Challenges arise due to non-static parameters while the robot moves, indicated by systematic deviations to a ground truth terrestrial laser scan.},
  archive   = {C_ICRA},
  author    = {Felix Esser and Gereon Tombrink and André Cornelißen and Lasse Klingbeil and Heiner Kuhlmann},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610208},
  month     = {5},
  pages     = {8471-8477},
  title     = {System calibration of a field phenotyping robot with multiple high-precision profile laser scanners},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Persistent monitoring of large environments with robot
deployment scheduling in between remote sensing cycles. <em>ICRA</em>,
8464–8470. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a novel decision-making framework for planning &quot;when&quot; and &quot;where&quot; to deploy robots based on prior data with the goal of persistently monitoring a spatio-temporal phenomenon in an environment. We specifically focus on large lake monitoring, where remote sensors, such as satellites, can provide a snapshot of the target phenomenon at regular cycles. Between these cycles, Autonomous Surface Vehicles (ASVs) can be deployed to maintain an up-to-date model of the phenomenon. However, deploying ASVs has a significant logistical overhead in terms of time and cost. It requires a team of people to go on site and spend typically a day to monitor the deployment. It is vital to not only be intentional about where to sample in the environment on a given day, but also determine the worth of deploying the ASVs that day at all. Therefore, we propose a persistent monitoring strategy that provides the days and locations of when and where to sample with the robots by leveraging Gaussian Process model estimates of future trends based on collected remote sensing and point measurement data. Our approach minimizes the number of days and locations for sampling, while preserving the quality of estimates. Through simulation experiments using realistic spatio-temporal datasets, we demonstrate the benefits of our approach over traditional deployment strategies, including significant savings on the effort and operational cost of deploying the ASVs.},
  archive   = {C_ICRA},
  author    = {Kizito Masaba and Monika Roznere and Mingi Jeong and Alberto Quattrini Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611421},
  month     = {5},
  pages     = {8464-8470},
  title     = {Persistent monitoring of large environments with robot deployment scheduling in between remote sensing cycles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Low-to-high resolution path planner for robotic gas
distribution mapping. <em>ICRA</em>, 8457–8463. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic gas distribution mapping improves the understanding of a hazardous gas dispersion while putting the human operator out of danger. Generating an accurate gas distribution map quickly is of utmost importance in situations such as gas leaks and industrial incidents, so that the efficient use of resources in response to incidents can be facilitated. In this paper, to incorporate the operational requirement on map granularity, we propose a low-to-high resolution path planner that first guides a single robots to quickly and sparsely sample the region of interest to generate a low resolution gas distribution map, followed by high resolution sampling informed by the low resolution map as a prior. The low resolution prior acts as a coverage survey allowing the algorithm to perform a relatively exploitative search of high concentration regions, resulting in overall shorter mission times. The proposed framework is designed to iteratively identify the next best T locations to sample, which prioritises the potentially high reward locations, while ensuring that the robot can travel to and sample the chosen locations within a user specified map update cycle. We present a simulation study to demonstrate the alternating exploration-exploitation like behaviour along with bench-marking its performance in contrast to the traditional sampling path planners and various reward functions.},
  archive   = {C_ICRA},
  author    = {Rohit V. Nanavati and Callum Rhodes and Matthew J. Coombes and Cunjia Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610044},
  month     = {5},
  pages     = {8457-8463},
  title     = {Low-to-high resolution path planner for robotic gas distribution mapping},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Autonomous UAV mission cycling: A mobile hub approach for
precise landings and continuous operations in challenging environments.
<em>ICRA</em>, 8450–8456. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Environmental monitoring via UAVs offers unprecedented aerial observation capabilities. However, the limited flight durations of typical multirotors and the demands on human attention in outdoor missions call for more autonomous solutions. Addressing the specific challenges of precise UAV landings – especially amidst wind disturbances, obstacles, and unreliable global localization – we introduce a mobile hub concept. This hub facilitates continuous mission cycling for unmodified off-the-shelf UAVs. Our approach centers on a small landing platform affixed to a robotic arm, adeptly correcting UAV pose errors in windy conditions. Compact enough for installation in an economy car, the system emphasizes two novel strategies. Firstly, external visual tracking of the UAV informs the landing controls for both the drone and the robotic arm. The arm compensates for UAV positioning errors and aligns the platform’s attitude with the UAV for stable landings, even on small platforms under windy conditions. Secondly, the robotic arm can transport the UAV inside the hub, perform maintenance tasks like battery replacements, and then facilitate direct relaunches. Importantly, our design places all operational responsibility on the hub, ensuring the UAV remains unaltered. This ensures broad compatibility with standard UAVs, only necessitating an API for attitude setpoints. Experimental results underscore the efficiency of our model, achieving safe landings with minimal errors (≤ 7 cm) in winds up to 5 Beaufort (8.1 m/s). In essence, our mobile hub concept significantly boosts UAV mission availability, allowing for autonomous operations even under challenging conditions.},
  archive   = {C_ICRA},
  author    = {Alexander Moortgat-Pick and Marie Schwahn and Anna Adamczyk and Daniel A Duecker and Sami Haddadin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611292},
  month     = {5},
  pages     = {8450-8456},
  title     = {Autonomous UAV mission cycling: A mobile hub approach for precise landings and continuous operations in challenging environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Measuring ball joint faults in parabolic-trough solar plants
with data augmentation and deep learning. <em>ICRA</em>, 8435–8441. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automatic inspection of parabolic-trough solar plants is key to preventing failures that can harm the environment and the production of green energy. In this work, we propose a novel methodology to inspect ball joints in parabolic trough collectors, which is a relevant problem that is not adequately covered in the literature. Images collected by an Unmanned Aerial Vehicle are segmented using deep learning to extract ball joint components. In order to generate rich training datasets, we develop a novel data augmentation technique by rotating joints and adding synthetic image background, and demonstrate its impact on the object detection accuracy. Then two types of faults are analyzed: fluid leaks, by means of image color filtering; and geometric shape anomalies, by measuring joint angles of the robotic arms. We propose metrics to quantify these faults and evaluate the damage of the inspected components. Our experimental results with images from operating commercial plants show that we can automatically detect leaks and anomalous angular geometry with a low failure rate compared to human labeling.},
  archive   = {C_ICRA},
  author    = {M. A. Pérez-Cutiño and J. Capitán and J. M. Díaz-Báñez and J. Valverde},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610205},
  month     = {5},
  pages     = {8435-8441},
  title     = {Measuring ball joint faults in parabolic-trough solar plants with data augmentation and deep learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SOL: A compact, portable, telescopic, soft-robotic
sun-tracking mechanism for improved solar power production.
<em>ICRA</em>, 8429–8434. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Solar power is becoming an increasingly popular option for energy production in commercial and private applications. While installing solar panels (photovoltaic cells) in a stationary configuration is simple and inexpensive, such a setup fails to maximise their potential solar energy production. Single- and dual-axis sun trackers automatically adjust the tilt angle of photovoltaic cells so as to directly face towards sun, but these also come with their own drawbacks such as increased cost and weight. This paper presents SOL, a soft-robotic, dual-axis, sun-tracking mechanism for improved solar panel efficiency. The proposed design was built to be compact, portable, and lightweight, and it utilises closed-loop control for the intelligent actuation of a set of soft telescopic structures that raise and tilt the solar panels in the direction of the sun. The performance of the proposed solar tracking platform was experimentally validated in terms of its maximum elevation at different azimuths and its ability to balance different loads. The result is a device that provides solar panel users with an accessible, affordable, and convenient means of increasing the efficiency of their solar energy system.},
  archive   = {C_ICRA},
  author    = {Bryan Busby and Shifei Duan and Marcus Thompson and Minas Liarokapis},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610596},
  month     = {5},
  pages     = {8429-8434},
  title     = {SOL: A compact, portable, telescopic, soft-robotic sun-tracking mechanism for improved solar power production},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MOAR planner: Multi-objective and adaptive risk-aware path
planning for infrastructure inspection with a UAV. <em>ICRA</em>,
8422–8428. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The problem of autonomous navigation for UAV inspection remains challenging as it requires effectively navigating in close proximity to obstacles, while accounting for dynamic risk factors such as weather conditions, communication reliability, and battery autonomy. This paper introduces the MOAR path planner which addresses the complexities of evolving risks during missions. It offers real-time trajectory adaptation while concurrently optimizing safety, time, and energy. The planner employs a risk-aware cost function that integrates pre-computed cost maps, the new concepts of damage and insertion costs, and an adaptive speed planning framework. With that, the optimal path is searched in a graph using a discrete representation of the state and action spaces. The method is evaluated through simulations and real-world flight tests. The results show the capability to generate real-time trajectories spanning a broad range of evaluation metrics—around 90% of the range occupied by popular algorithms. The proposed framework contributes by enabling UAVs to navigate more autonomously and reliably in critical missions.},
  archive   = {C_ICRA},
  author    = {Louis Petit and Alexis Lussier Desbiens},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610907},
  month     = {5},
  pages     = {8422-8428},
  title     = {MOAR planner: Multi-objective and adaptive risk-aware path planning for infrastructure inspection with a UAV},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Observer-based controller design for oscillation damping of
a novel suspended underactuated aerial platform. <em>ICRA</em>,
8415–8421. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we present a novel actuation strategy for a suspended aerial platform. By utilizing an underactuation approach, we demonstrate the successful oscillation damping of the proposed platform, modeled as a spherical double pendulum. A state estimator is designed in order to obtain the deflection angles of the platform, which uses only onboard IMU measurements. The state estimator is an extended Kalman filter (EKF) with intermittent measurements obtained at different frequencies. An optimal state feedback controller and a PD+ controller are designed in order to dampen the oscillations of the platform in the joint space and task space respectively. The proposed underactuated platform is found to be more energy-efficient than an omnidirectional platform and requires fewer actuators. The effectiveness of our proposed system is validated using both simulations and experimental studies.},
  archive   = {C_ICRA},
  author    = {Hemjyoti Das and Minh Nhat Vu and Tobias Egle and Christian Ott},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610305},
  month     = {5},
  pages     = {8415-8421},
  title     = {Observer-based controller design for oscillation damping of a novel suspended underactuated aerial platform},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A modular aerial system based on homogeneous quadrotors with
fault-tolerant control. <em>ICRA</em>, 8408–8414. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The standard quadrotor is one of the most popular and widely used aerial vehicle of recent decades, offering great maneuverability with mechanical simplicity. However, the under-actuation characteristic limits its applications, especially when it comes to generating desired wrench with six degrees of freedom (DOF). Therefore, existing work often compromises between mechanical complexity and the controllable DOF of the aerial system. To take advantage of the mechanical simplicity of a standard quadrotor, we propose a modular aerial system, IdentiQuad, that combines only homogeneous quadrotor-based modules. Each IdentiQuad can be operated alone like a standard quadrotor, but at the same time allows task-specific assembly, increasing the controllable DOF of the system. Each module is interchangeable within its assembly. We also propose a general controller for different configurations of assemblies, capable of tolerating rotor failures and balancing the energy consumption of each module. The functionality and robustness of the system and its controller are validated using physics-based simulations for different assembly configurations.},
  archive   = {C_ICRA},
  author    = {Mengguang Li and Kai Cui and Heinz Koeppl},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610414},
  month     = {5},
  pages     = {8408-8414},
  title     = {A modular aerial system based on homogeneous quadrotors with fault-tolerant control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal collaborative transportation for under-capacitated
vehicle routing problems using aerial drone swarms. <em>ICRA</em>,
8401–8407. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Swarms of aerial drones have recently been considered for last-mile deliveries in urban logistics or automated construction. At the same time, collaborative transportation of payloads by multiple drones is another important area of recent research. However, efficient coordination algorithms for collaborative transportation of many payloads by many drones remain to be considered. In this work, we formulate the collaborative transportation of payloads by a swarm of drones as a novel, under-capacitated generalization of vehicle routing problems (VRP), which may also be of separate interest. In contrast to standard VRP and capacitated VRP, we must additionally consider waiting times for payloads lifted cooperatively by multiple drones, and the corresponding coordination. Algorithmically, we provide a solution encoding that avoids deadlocks and formulate an appropriate alternating minimization scheme to solve the problem. On the hardware side, we integrate our algorithms with collision avoidance and drone controllers. The approach and the impact of the system integration are successfully verified empirically, both on a swarm of real nano-quadcopters and for large swarms in simulation. Overall, we provide a framework for collaborative transportation with aerial drone swarms, that uses only as many drones as necessary for the transportation of any single payload.},
  archive   = {C_ICRA},
  author    = {Akash Kopparam Sreedhara and Deepesh Padala and Shashank Mahesh and Kai Cui and Mengguang Li and Heinz Koeppl},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611698},
  month     = {5},
  pages     = {8401-8407},
  title     = {Optimal collaborative transportation for under-capacitated vehicle routing problems using aerial drone swarms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust and energy-efficient control for multi-task aerial
manipulation with automatic arm-switching. <em>ICRA</em>, 8394–8400. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Aerial manipulation has received increasing research interest with wide applications of drones. To perform specific tasks, robotic arms with various mechanical structures will be mounted on the drone. It results in sudden disturbances to the aerial manipulator when switching the robotic arm or interacting with the environment. Hence, it is challenging to design a generic and robust control strategy adapted to various robotic arms when achieving multi-task aerial manipulation. In this paper, we present a learning-based control algorithm that allows online trajectory optimization and tracking to accomplish various aerial interaction tasks without manual adjustment. The proposed energy-saved trajectory planning approach integrates coupled dynamics model with a single rigid body to generate the energy-efficient trajectory for the aerial manipulator. Addressing the challenges of precise control when performing aerial manipulation tasks, this paper presents a controller based on deep neural networks that classifies and learns accurate forces and moments caused by different robotic arms and interactions. Moreover, the forces arising from robotic arm motions are delicately used as part of the drone’s power to save energy. Extensive real-world experiments demonstrate that the proposed method can adapt to various robotic arms and interactions when performing multi-task aerial manipulation.},
  archive   = {C_ICRA},
  author    = {Ying Wu and Zida Zhou and Mingxin Wei and Hui Cheng},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610031},
  month     = {5},
  pages     = {8394-8400},
  title     = {Robust and energy-efficient control for multi-task aerial manipulation with automatic arm-switching},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Safety-conscious pushing on diverse oriented surfaces with
underactuated aerial vehicles. <em>ICRA</em>, 8376–8382. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pushing tasks performed by aerial manipulators can be used for contact-based industrial inspections. Underactuated aerial vehicles are widely employed in aerial manipulation due to their widespread availability and relatively low cost. Industrial infrastructures often consist of diverse oriented work surfaces. When interacting with such surfaces, the coupled gravity compensation and interaction force generation of underactuated aerial vehicles can present the potential challenge of near-saturation operations. The blind utilization of these platforms for such tasks can lead to instability and accidents, creating unsafe operating conditions and potentially damaging the platform. In order to ensure safe pushing on these surfaces while managing platform saturation, this work establishes a safety assessment process. This process involves the prediction of the saturation level of each actuator during pushing across variable surface orientations. Furthermore, the assessment results are used to plan and execute physical experiments, ensuring safe operations and preventing platform damage.},
  archive   = {C_ICRA},
  author    = {Tong Hui and Manuel J. Fernández González and Matteo Fumagalli},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610459},
  month     = {5},
  pages     = {8376-8382},
  title     = {Safety-conscious pushing on diverse oriented surfaces with underactuated aerial vehicles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MPS: A new method for selecting the stable closed-loop
equilibrium attitude-error quaternion of a UAV during flight.
<em>ICRA</em>, 8363–8369. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present model predictive selection (MPS), a new method for selecting the stable closed-loop (CL) equilibrium attitude-error quaternion (AEQ) of an uncrewed aerial vehicle (UAV) during the execution of high-speed yaw maneuvers. In this approach, we minimize the cost of yawing measured with a performance figure of merit (PFM) that takes into account both the aerodynamic-torque control input and attitude-error state of the UAV. Specifically, this method uses a control law with a term whose sign is dynamically switched in real time to select, between two options, the torque associated with the lesser cost of rotation as predicted by a dynamical model of the UAV derived from first principles. This problem is relevant because the selection of the stable CL equilibrium AEQ significantly impacts the performance of a UAV during high-speed rotational flight, from both the power and control-error perspectives. To test and demonstrate the functionality and performance of the proposed method, we present data collected during one hundred real-time high-speed yaw-tracking flight experiments. These results highlight the superior capabilities of the proposed MPS-based scheme when compared to a benchmark controller commonly used in aerial robotics, as the PFM used to quantify the cost of flight is reduced by 60.30 %, on average. To our best knowledge, these are the first flight-test results that thoroughly demonstrate, evaluate, and compare the performance of a real-time controller capable of selecting the stable CL equilibrium AEQ during operation.},
  archive   = {C_ICRA},
  author    = {Francisco M. F. R. Gonçalves and Ryan M. Bena and Konstantin I. Matveev and Néstor O. Pérez-Arancibia},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610463},
  month     = {5},
  pages     = {8363-8369},
  title     = {MPS: A new method for selecting the stable closed-loop equilibrium attitude-error quaternion of a UAV during flight},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning which side to scan: Multi-view informed active
perception with side scan sonar for autonomous underwater vehicles.
<em>ICRA</em>, 8348–8354. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous underwater vehicles often perform surveys that capture multiple views of targets in order to provide more information for human operators or automatic target recognition algorithms. In this work, we address the problem of choosing the most informative views that minimize survey time while maximizing classifier accuracy. We introduce a novel active perception framework for multi-view adaptive surveying and reacquisition using side scan sonar imagery. Our framework addresses this challenge by using a graph formulation for the adaptive survey task. We then use Graph Neural Networks (GNNs) to both classify acquired sonar views and to choose the next best view based on the collected data. We evaluate our method using simulated surveys in a high-fidelity side scan sonar simulator. Our results demonstrate that our approach is able to surpass the state-of-the-art in classification accuracy and survey efficiency. This framework is a promising approach for more efficient autonomous missions involving side scan sonar, such as underwater exploration, marine archaeology, and environmental monitoring.},
  archive   = {C_ICRA},
  author    = {Advaith V. Sethuraman and Philip Baldoni and Katherine A. Skinner and James McMahon},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611077},
  month     = {5},
  pages     = {8348-8354},
  title     = {Learning which side to scan: Multi-view informed active perception with side scan sonar for autonomous underwater vehicles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sea-u-foil: A hydrofoil marine vehicle with multi-modal
locomotion. <em>ICRA</em>, 8341–8347. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous Marine Vehicles (AMVs) have been widely used in many critical tasks such as surveillance, patrolling, marine environment monitoring, and hydrographic surveying. However, most typical AMVs cannot meet the diverse demands of different marine tasks. In this article, we design a new type of remote-controlled hydrofoil marine vehicle, named Sea-U-Foil, which is suitable for different marine scenarios. Sea-U-Foil features three distinct locomotion modes, displacement mode, foilborne mode, and submarine mode, which enable the platform flexible mobility, high-speed and high-load capacities, and superior concealment. Specifically, the submarine mode makes Sea-U-Foil unique among previous studies. In addition, the performance of Sea-U-Foil in foilborne mode outperforms those of most current unmanned surface vehicles (USVs) in terms of speed and payload. To the best of our knowledge, we are the first to introduce a new type of AMV that can work in displacement mode, foilborne mode, and submarine mode. We elaborate on the design principles and methodologies of Sea-U-Foil first, then validate the effectiveness of its tri-modal locomotion through extensive experiments.},
  archive   = {C_ICRA},
  author    = {Zuoquan Zhao and Yu Zhai and Chuanxiang Gao and Wendi Ding and Ruixin Yan and Songqun Gao and Bingxin Han and Xuchen Liu and Zixuan Guo and Ben M. Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610853},
  month     = {5},
  pages     = {8341-8347},
  title     = {Sea-U-foil: A hydrofoil marine vehicle with multi-modal locomotion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time planning under uncertainty for AUVs using virtual
maps. <em>ICRA</em>, 8334–8340. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reliable localization is an essential capability for marine robots navigating in GPS-denied environments. SLAM, commonly used to mitigate dead reckoning errors, still fails in feature-sparse environments or with limited-range sensors. Pose estimation can be improved by incorporating the uncertainty prediction of future poses into the planning process and choosing actions that reduce uncertainty. However, performing belief propagation is computationally costly, especially when operating in large-scale environments. This work proposes a computationally efficient planning under uncertainty framework suitable for large-scale, feature-sparse environments. Our strategy leverages SLAM graph and occupancy map data obtained from a prior exploration phase to create a virtual map, describing the uncertainty of each map cell using a multivariate Gaussian. The virtual map is then used as a cost map in the planning phase, and performing belief propagation at each step is avoided. A receding horizon planning strategy is implemented, managing a goal-reaching and uncertainty-reduction tradeoff. Simulation experiments in a realistic underwater environment validate this approach. Experimental comparisons against a full belief propagation approach and a standard shortest-distance approach are conducted.},
  archive   = {C_ICRA},
  author    = {Ivana Collado-Gonzalez and John McConnell and Jinkun Wang and Paul Szenher and Brendan Englot},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610245},
  month     = {5},
  pages     = {8334-8340},
  title     = {Real-time planning under uncertainty for AUVs using virtual maps},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decentralized multi-robot navigation for autonomous surface
vehicles with distributional reinforcement learning. <em>ICRA</em>,
8327–8333. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collision avoidance algorithms for Autonomous Surface Vehicles (ASV) that follow the Convention on the International Regulations for Preventing Collisions at Sea (COLREGs) have been proposed in recent years. However, it may be difficult and unsafe to follow COLREGs in congested waters, where multiple ASVs are navigating in the presence of static obstacles and strong currents, due to the complex interactions. To address this problem, we propose a decentralized multi-ASV collision avoidance policy based on Distributional Reinforcement Learning, which considers the interactions among ASVs as well as with static obstacles and current flows. We evaluate the performance of the proposed Distributional RL based policy against a traditional RL-based policy and two classical methods, Artificial Potential Fields (APF) and Reciprocal Velocity Obstacles (RVO), in simulation experiments, which show that the proposed policy achieves superior performance in navigation safety, while requiring minimal travel time and energy. A variant of our framework that automatically adapts its risk sensitivity is also demonstrated to improve ASV safety in highly congested environments.},
  archive   = {C_ICRA},
  author    = {Xi Lin and Yewei Huang and Fanfei Chen and Brendan Englot},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611668},
  month     = {5},
  pages     = {8327-8333},
  title     = {Decentralized multi-robot navigation for autonomous surface vehicles with distributional reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mission planning for multiple autonomous underwater vehicles
with constrained in situ recharging. <em>ICRA</em>, 8320–8326. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Persistent operation of Autonomous Underwater Vehicles (AUVs) without manual interruption for recharging saves time and total cost for offshore monitoring and data collection applications. In order to facilitate AUVs for long mission durations without ship support, they can be equipped with docking capabilities to recharge in situ at Wave Energy Converter (WEC) with dock recharging stations. However, the power generated at the recharging stations may be constrained depending on the sea conditions. Therefore, a robust mission planning framework is proposed using a centralized Evolutionary Algorithm (EA) and a decentralized Monte Carlo Tree Search (MCTS) method. Both methods incorporate the charge availability constraint at the recharging station in addition to the maximum charge capacity of each AUV. The planner utilizes a time-varying power profile of irregular waves incident at WECs for dock charging and generates efficient mission plans for AUVs by optimizing their time to visit the dock based on the imposed constraint. The effects of increasing the number of AUVs, increasing the number of points of interest in the mission area, and varying sea state on the mission duration are also analyzed.},
  archive   = {C_ICRA},
  author    = {Priti Singh and Geoffrey A. Hollinger},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611396},
  month     = {5},
  pages     = {8320-8326},
  title     = {Mission planning for multiple autonomous underwater vehicles with constrained in situ recharging},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A model for multi-agent autonomy that uses opinion dynamics
and multi-objective behavior optimization. <em>ICRA</em>, 8305–8311. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper reports a new hierarchical architecture for modeling autonomous multi-robot systems (MRSs): a nonlinear dynamical opinion process is used to model high-level group choice, and multi-objective behavior optimization is used to model individual decisions. Using previously reported theoretical results, we show it is possible to design the behavior of the MRS by the selection of a relatively small set of parameters. The resulting behavior - both collective actions and individual actions - can be understood intuitively. The approach is entirely decentralized and the communication cost scales by the number of group options, not agents. We demonstrated the effectiveness of this approach using a hypothetical ‘explore-exploit-migrate’ scenario in a two hour field demonstration with eight unmanned surface vessels (USVs). The results from our preliminary field experiment show the collective behavior is robust even with time-varying network topology and agent dropouts.},
  archive   = {C_ICRA},
  author    = {Tyler M. Paine and Michael R. Benjamin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611032},
  month     = {5},
  pages     = {8305-8311},
  title     = {A model for multi-agent autonomy that uses opinion dynamics and multi-objective behavior optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Standardization of cloth objects and its relevance in
robotic manipulation. <em>ICRA</em>, 8298–8304. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The field of robotics faces inherent challenges in manipulating deformable objects, particularly in understanding and standardising fabric properties like elasticity, stiffness, and friction. While the significance of these properties is evident in the realm of cloth manipulation, accurately categorising and comprehending them in real-world applications remains elusive. This study sets out to address two primary objectives: (1) to provide a framework suitable for robotics applications to characterise cloth objects, and (2) to study how these properties influence robotic manipulation tasks. Our preliminary results validate the framework’s ability to characterise cloth properties and compare cloth sets, and reveal the influence that different properties have on the outcome of five manipulation primitives. We believe that, in general, results on the manipulation of clothes should be reported along with a better description of the garments used in the evaluation. This paper proposes a set of these measures.},
  archive   = {C_ICRA},
  author    = {Irene Garcia-Camacho and Alberta Longhini and Michael Welle and Guillem Alenyà and Danica Kragic and Júlia Borràs},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610630},
  month     = {5},
  pages     = {8298-8304},
  title     = {Standardization of cloth objects and its relevance in robotic manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RobotPerf: An open-source, vendor-agnostic, benchmarking
suite for evaluating robotics computing system performance.
<em>ICRA</em>, 8288–8297. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce RobotPerf, a vendor-agnostic bench-marking suite designed to evaluate robotics computing performance across a diverse range of hardware platforms using ROS 2 as its common baseline. The suite encompasses ROS 2 packages covering the full robotics pipeline and integrates two distinct benchmarking approaches: black-box testing, which measures performance by eliminating upper layers and replacing them with a test application, and grey-box testing, an application-specific measure that observes internal system states with minimal interference. Our benchmarking framework provides ready-to-use tools and is easily adaptable for the assessment of custom ROS 2 computational graphs. Drawing from the knowledge of leading robot architects and system architecture experts, RobotPerf establishes a standardized approach to robotics benchmarking. As an open-source initiative, RobotPerf remains committed to evolving with community input to advance the future of hardware-accelerated robotics.},
  archive   = {C_ICRA},
  author    = {Víctor Mayoral-Vilches and Jason Jabbour and Yu-Shun Hsiao and Zishen Wan and Martiño Crespo-Álvarez and Matthew Stewart and Juan Manuel Reina-Muñoz and Prateek Nagras and Gaurav Vikhe and Mohammad Bakhshalipour and Martin Pinzger and Stefan Rass and Smruti Panigrahi and Giulio Corradi and Niladri Roy and Phillip B. Gibbons and Sabrina M. Neuman and Brian Plancher and Vijay Janapa Reddi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610841},
  month     = {5},
  pages     = {8288-8297},
  title     = {RobotPerf: An open-source, vendor-agnostic, benchmarking suite for evaluating robotics computing system performance},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LET-3D-AP: Longitudinal error tolerant 3D average precision
for camera-only 3D detection. <em>ICRA</em>, 8272–8279. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The 3D Average Precision (3DAP) relies on the intersection over union between predictions and ground truth objects. However, camera-only detectors have limited depth accuracy, which may cause otherwise reasonable predictions that suffer from such longitudinal localization errors to be treated as false positives. We therefore propose variants of the 3DAP metric to be more permissive with respect to depth estimation errors. Specifically, our novel longitudinal error tolerant metrics, LET-3D-AP and LET-3D-APL, allow longitudinal localization errors of the prediction boxes up to a given tolerance. To evaluate the proposed metrics, we also construct a new test set for the Waymo Open Dataset, tailored to camera-only 3D detection methods. Surprisingly, we find that state-of-the-art camera-based detectors can outperform popular LiDAR-based detectors with our new metrics past at 10% depth error tolerance, suggesting that existing camera-based detectors already have the potential to surpass LiDAR-based detectors in downstream applications. We believe the proposed metrics and the new benchmark dataset will facilitate advances in the field of camera-only 3D detection by providing more informative signals that can better indicate the system-level performance.},
  archive   = {C_ICRA},
  author    = {Wei-Chih Hung and Vincent Casser and Henrik Kretzschmar and Jyh-Jing Hwang and Dragomir Anguelov},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609986},
  month     = {5},
  pages     = {8272-8279},
  title     = {LET-3D-AP: Longitudinal error tolerant 3D average precision for camera-only 3D detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CRITERIA: A new benchmarking paradigm for evaluating
trajectory prediction models for autonomous driving. <em>ICRA</em>,
8265–8271. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Benchmarking is a common method for evaluating trajectory prediction models for autonomous driving. Existing benchmarks rely on datasets, which are biased towards more common scenarios, such as cruising, and distance-based metrics that are computed by averaging over all scenarios. Following such a regiment provides a little insight into the properties of the models both in terms of how well they can handle different scenarios and how admissible and diverse their outputs are. There exist a number of complementary metrics designed to measure the admissibility and diversity of trajectories, however, they suffer from biases, such as length of trajectories.In this paper, we propose a new benChmarking paRadIgm for evaluaTing trajEctoRy predIction Approaches (CRITE-RIA). Particularly, we propose 1) a method for extracting driving scenarios at varying levels of specificity according to the structure of the roads, models’ performance, and data properties for fine-grained ranking of prediction models; 2) A set of new bias-free metrics for measuring diversity, by incorporating the characteristics of a given scenario, and admissibility, by considering the structure of roads and kinematic compliancy, motivated by real-world driving constraints; 3) Using the proposed benchmark, we conduct extensive experimentation on a representative set of the prediction models using the large scale Argoverse dataset. We show that the proposed benchmark can produce a more accurate ranking of the models and serve as a means of characterizing their behavior. We further present ablation studies to highlight contributions of different elements that are used to compute the proposed metrics 1 .},
  archive   = {C_ICRA},
  author    = {Changhe Chen and Mozhgan Pourkeshavarz and Amir Rasouli},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610911},
  month     = {5},
  pages     = {8265-8271},
  title     = {CRITERIA: A new benchmarking paradigm for evaluating trajectory prediction models for autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SceneReplica: Benchmarking real-world robot manipulation by
creating replicable scenes. <em>ICRA</em>, 8258–8264. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a new reproducible benchmark for evaluating robot manipulation in the real world, specifically focusing on a pick-and-place task. Our benchmark uses the YCB object set, a commonly used dataset in the robotics community, to ensure that our results are comparable to other studies. Additionally, the benchmark is designed to be easily reproducible in the real world, making it accessible to researchers and practitioners. We also provide our experimental results and analyzes for model-based and model-free 6D robotic grasping on the benchmark, where representative algorithms are evaluated for object perception, grasping planning, and motion planning. We believe that our benchmark will be a valuable tool for advancing the field of robot manipulation. By providing a standardized evaluation framework, researchers can more easily compare different techniques and algorithms, leading to faster progress in developing robot manipulation methods. 1},
  archive   = {C_ICRA},
  author    = {Ninad Khargonkar and Sai Haneesh Allu and Yangxiao Lu and Jishnu Jaykumar P and Balakrishnan Prabhakaran and Yu Xiang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610180},
  month     = {5},
  pages     = {8258-8264},
  title     = {SceneReplica: Benchmarking real-world robot manipulation by creating replicable scenes},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AD4RL: Autonomous driving benchmarks for offline
reinforcement learning with value-based dataset. <em>ICRA</em>,
8239–8245. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Offline reinforcement learning has emerged as a promising technology by enhancing its practicality through the use of pre-collected large datasets. Despite its practical benefits, most algorithm development research in offline reinforcement learning still relies on game tasks with synthetic datasets. To address such limitations, this paper provides autonomous driving datasets and benchmarks for offline reinforcement learning research. We provide 19 datasets, including real-world human driver’s datasets, and seven popular offline reinforcement learning algorithms in three realistic driving scenarios. We also provide a unified decision-making process model that can operate effectively across different scenarios, serving as a reference framework in algorithm design. Our research lays the groundwork for further collaborations in the community to explore practical aspects of existing reinforcement learning methods. Dataset and codes can be found in https://sites.google.com/view/ad4rl.},
  archive   = {C_ICRA},
  author    = {Dongsu Lee and Chanin Eom and Minhae Kwon},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610308},
  month     = {5},
  pages     = {8239-8245},
  title     = {AD4RL: Autonomous driving benchmarks for offline reinforcement learning with value-based dataset},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A group theoretic metric for robot state estimation
leveraging chebyshev interpolation. <em>ICRA</em>, 8230–8238. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a new metric for robot state estimation based on the recently introduced SE 2 (3) Lie group definition. Our metric is related to prior metrics for SLAM but explicitly takes into account the linear velocity of the state estimate, improving over current pose-based trajectory analysis. This has the benefit of providing a single, quantitative metric to evaluate state estimation algorithms against, while being compatible with existing tools and libraries. Since ground truth data generally consists of pose data from motion capture systems, we also propose an approach to compute the ground truth linear velocity based on polynomial interpolation. Using Chebyshev interpolation and a pseudospectral parameterization, we can accurately estimate the ground truth linear velocity of the trajectory in an optimal fashion with best approximation error. We demonstrate how this approach performs on multiple robotic platforms where accurate state estimation is vital, and compare it to alternative approaches such as finite differences. The pseudospectral parameterization also provides a means of trajectory data compression as an additional benefit. Experimental results show our method provides a valid and accurate means of comparing state estimation systems, which is also easy to interpret and report.},
  archive   = {C_ICRA},
  author    = {Varun Agrawal and Frank Dellaert},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611072},
  month     = {5},
  pages     = {8230-8238},
  title     = {A group theoretic metric for robot state estimation leveraging chebyshev interpolation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Chained flexible capsule endoscope: Unraveling the conundrum
of size limitations and functional integration for gastrointestinal
transitivity. <em>ICRA</em>, 8223–8229. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Capsule endoscopes, predominantly serving diagnostic functions, provide lucid internal imagery but are devoid of surgical or therapeutic capabilities. Consequently, despite lesion detection, physicians frequently resort to traditional endoscopic or open surgical procedures for treatment, resulting in more complex, potentially risky interventions. To surmount these limitations, this study introduces a chained flexible capsule endoscope (FCE) design concept, specifically conceived to navigate the inherent volume constraints of capsule endoscopes whilst augmenting their therapeutic functionalities. The FCE’s distinctive flexibility originates from a conventional rotating joint design and the incision pattern in the flexible material. In vitro experiments validated the passive navigation ability of the FCE in rugged intestinal tracts. Further, the FCE demonstrates consistent reptile-like peristalsis under the influence of an external magnetic field, and possesses the capability for film expansion and disintegration under high-frequency electromagnetic stimulation. These findings illuminate a promising path toward amplifying the therapeutic capacities of capsule endoscopes without necessitating a size compromise.},
  archive   = {C_ICRA},
  author    = {Sishen Yuan and Guang Li and Baijia Liang and Lailu Li and Qingzhuo Zheng and Shuang Song and Zhen Li and Hongliang Ren},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611010},
  month     = {5},
  pages     = {8223-8229},
  title     = {Chained flexible capsule endoscope: Unraveling the conundrum of size limitations and functional integration for gastrointestinal transitivity},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive motion scaling for robot-assisted microsurgery
based on hybrid offline reinforcement learning and damping control.
<em>ICRA</em>, 8216–8222. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion scaling is essential to empower users to conduct precise manipulation during teleoperation for robot-assisted microsurgery (RAMS). A constant, small motion scaling ratio can enhance the precision of teleoperation but hinder the operator from quickly reaching distant targets. The concept of self-adaptive motion scaling has been proposed in previous work. However, previous frameworks required extensive manual tuning of core parameters, which significantly depends on prior knowledge and may potentially lead to non-optimal solutions. This paper presents a hybrid offline reinforcement learning and damping control approach to regulate the motion scaling ratio for different operations during offline training. This method can take user-specific characteristics into consideration and help them achieve better teleoperation performance. Comparisons are made with and without using the adaptive motion-scaling algorithm. Detailed user studies indicate that a suitable motion-scaling ratio can be obtained and adjusted online. The overall performance of the operators in terms of time cost for task completion is significantly improved, while the variance of average speed and the total distance for robot operation is reduced.},
  archive   = {C_ICRA},
  author    = {Peiyang Jiang and Wei Li and Yifan Li and Dandan Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611009},
  month     = {5},
  pages     = {8216-8222},
  title     = {Adaptive motion scaling for robot-assisted microsurgery based on hybrid offline reinforcement learning and damping control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cooperative vs. Teleoperation control of the steady hand eye
robot with adaptive sclera force control: A comparative study.
<em>ICRA</em>, 8209–8215. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A surgeon’s physiological hand tremor can significantly impact the outcome of delicate and precise retinal surgery, such as retinal vein cannulation (RVC) and epiretinal membrane peeling. Robot-assisted eye surgery technology provides ophthalmologists with advanced capabilities such as hand tremor cancellation, hand motion scaling, and safety constraints that enable them to perform these otherwise challenging and high-risk surgeries with high precision and safety. Steady-Hand Eye Robot (SHER) with cooperative control mode can filter out surgeon’s hand tremor, yet another important safety feature, that is, minimizing the contact force between the surgical instrument and sclera surface for avoiding tissue damage cannot be met in this control mode. Also, other capabilities, such as hand motion scaling and haptic feedback, require a teleoperation control framework. In this work, for the first time, we implemented a teleoperation control mode incorporated with an adaptive sclera force control algorithm using a PHANTOM Omni haptic device and a force-sensing surgical instrument equipped with Fiber Bragg Grating (FBG) sensors attached to the SHER 2.1 end-effector. This adaptive sclera force control algorithm allows the robot to dynamically minimize the toolsclera contact force. Moreover, for the first time, we compared the performance of the proposed adaptive teleoperation mode with the cooperative mode by conducting a vessel-following experiment inside an eye phantom under a microscope.},
  archive   = {C_ICRA},
  author    = {Mojtaba Esfandiari and Ji Woong Kim and Botao Zhao and Golchehr Amirkhani and Muhammad Hadi and Peter Gehlbach and Russell H. Taylor and Iulian Iordachita},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611084},
  month     = {5},
  pages     = {8209-8215},
  title     = {Cooperative vs. teleoperation control of the steady hand eye robot with adaptive sclera force control: A comparative study},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Envibroscope: Real-time monitoring and prediction of
environmental motion for enhancing safety in robot-assisted
microsurgery. <em>ICRA</em>, 8202–8208. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Several robotic systems have emerged in the recent past to enhance the precision of micro-surgeries such as retinal procedures. Significant advancements have recently been achieved to increase the precision of such systems beyond surgeon capabilities. However, little attention has been paid to the impact of non-predicted and sudden movements of the patient and the environment. Therefore, analyzing environmental motion and vibrations is crucial to ensuring the optimal performance and reliability of medical systems that require micron-level precision, especially in real-life scenarios.To address this challenge, this paper introduces a novel environmental motion analysis system that employs a grid layout with distributed sensing nodes throughout the environment. This system effectively tracks undesired movements (motions) at designated locations and predicts upcoming motions using neural network-based approaches. The outcomes of our experiments exhibit promising prospects for real-time motion monitoring and prediction, which has the potential to form a solid basis for enhancing the automation, safety, integration, and overall efficiency of robot-assisted micro-surgeries.},
  archive   = {C_ICRA},
  author    = {Alireza Alikhani and Satoshi Inagaki and Shervin Dehghani and Mathias Maier and Nassir Navab and M. Ali Nasseri},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611207},
  month     = {5},
  pages     = {8202-8208},
  title     = {Envibroscope: Real-time monitoring and prediction of environmental motion for enhancing safety in robot-assisted microsurgery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid robot for percutaneous needle intervention
procedures: Mechanism design and experiment verification. <em>ICRA</em>,
8195–8201. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a 6-DOF hybrid robot for percutaneous needle intervention procedures. The new robot combines the advantages of both serial robots and parallel robots, featuring compactness, high accuracy, and small footprint while overcoming the problems of the high cost of serial robots and the small workspace and singularity issue of parallel robots. Besides, by analyzing the workspace of the robot, the equation is derived between the structure parameter and workspace to adjust the parameters of the robot to satisfy different working scenes. According to the experiment, the accuracy of the robot is related to the position, distance, and insertion angle. The result shows that the performance is better when working near the center workspace and away from the servos and the average error of the robot is 1.39mm. The phantom experiment of lumbar puncture validates its feasibility.},
  archive   = {C_ICRA},
  author    = {Hanyi Zhang and Guocai Yao and Feifan Zhang and Fanchuan Lin and Fuchun Sun},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611222},
  month     = {5},
  pages     = {8195-8201},
  title     = {Hybrid robot for percutaneous needle intervention procedures: Mechanism design and experiment verification},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AiAReSeg: Catheter detection and segmentation in
interventional ultrasound using transformers. <em>ICRA</em>, 8187–8194.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work proposes a state-of-the-art transformer architecture to detect and segment catheters in axial interventional Ultrasound image sequences. The network architecture was inspired by the Attention in Attention mechanism, temporal tracking networks, and introduced a novel 3D segmentation head that performs 3D deconvolution across time. To train the network, we introduce a new data synthesis pipeline that uses physics-based catheter insertion simulations, along with a convolutional ray-casting ultrasound simulator to produce synthetic ultrasound images of endovascular interventions. The proposed method is validated on a hold-out validation dataset, thus demonstrated robustness to ultrasound noise and a wide range of scanning angles. It was also tested on data collected from silicon aorta phantoms, thus demonstrated its potential for translation from sim-to-real. This work represents a significant step towards safer and more efficient endovascular surgery using interventional ultrasound.},
  archive   = {C_ICRA},
  author    = {Alex Ranne and Yordanka Velikova and Nassir Navab and Ferdinando Rodriguez y Baena},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611539},
  month     = {5},
  pages     = {8187-8194},
  title     = {AiAReSeg: Catheter detection and segmentation in interventional ultrasound using transformers},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An intelligent robotic endoscope control system based on
fusing natural language processing and vision models. <em>ICRA</em>,
8180–8186. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, the area of Robot-Assisted Minimally Invasive Surgery (RAMIS) is standing on the the verge of a new wave of innovations. However, autonomy in RAMIS is still in a primitive stage. Therefore, most surgeries still require manual control of the endoscope and the robotic instruments, resulting in surgeons needing to switch attention between performing surgical procedures and moving endoscope camera. Automation may reduce the complexity of surgical operations and consequently reduce the cognitive load on the surgeon while speeding up the surgical process. In this paper, a hybrid robotic endoscope control system based on fusion model of natural language processing (NLP) and modified YOLO-V8 vision model is proposed. This proposed system can analyze the current surgical workflow and generate logs to summarize the procedure for teaching and providing feedback to junior surgeons. The user study of this system indicated a significant reduction of the number of clutching actions and mean task time, which effectively enhanced the surgical training.},
  archive   = {C_ICRA},
  author    = {Beili Dong and Junhong Chen and Zeyu Wang and Kaizhong Deng and Yiping Li and Benny Lo and George Mylonas},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611534},
  month     = {5},
  pages     = {8180-8186},
  title     = {An intelligent robotic endoscope control system based on fusing natural language processing and vision models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3D navigation of a magnetic swimmer using a 2D
ultrasonography probe manipulated by a robotic arm for position
feedback. <em>ICRA</em>, 8173–8179. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Millimeter-scale magnetic rotating swimmers have multiple potential medical applications. They could, for example, navigate inside the bloodstream of a patient toward an occlusion and remove it. Magnetic rotating swimmers have internal magnets and propeller fins with a helical shape. A rotating magnetic field applies torque on the swimmer and makes it rotate. The shape of the swimmer, combined with the rotational movement, generates a propulsive force. Visual feedback is suitable for in-vitro closed-loop control. However, in-vivo procedures will require different feedback modalities due to the opacity of the human body. In this paper, we provide new methods and tools that enable the 3D control of a magnetic swimmer using a 2D ultrasonography device attached to a robotic arm to sense the swimmer’s position. We also provide an algorithm that computes the placement of the robotic arm and a controller that keeps the swimmer within the ultrasound imaging slice. The position measurement and closed-loop control were tested experimentally.},
  archive   = {C_ICRA},
  author    = {Premal Gorroochurn and Charles P. Hong and Carter M. Klebuc and Yitong Lu and Khue Phan and Javier Garcia and Aaron T. Becker and Julien Leclerc},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611538},
  month     = {5},
  pages     = {8173-8179},
  title     = {3D navigation of a magnetic swimmer using a 2D ultrasonography probe manipulated by a robotic arm for position feedback},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the disentanglement of tube inequalities in concentric
tube continuum robots. <em>ICRA</em>, 8166–8172. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Concentric tube continuum robots utilize nested tubes, which are subject to a set of inequalities. Current approaches to account for inequalities rely on branching methods such as if-else statements. It can introduce discontinuities, may result in a complicated decision tree, has a high wall-clock time, and cannot be vectorized. This affects the behavior and result of downstream methods in control, learning, workspace estimation, and path planning, among others.In this paper, we investigate a mapping to mitigate branching methods. We derive a lower triangular transformation matrix to disentangle the inequalities and provide proof for the unique existence. It transforms the interdependent inequalities into independent box constraints. Further investigations are made for sampling, control, and workspace estimation. Approaches utilizing the proposed mapping are at least 14 times faster (up to 176 times faster), generate always valid joint configurations, are more interpretable, and are easier to extend.},
  archive   = {C_ICRA},
  author    = {Reinhard M. Grassmann and Anastasiia Senyk and Jessica Burgner-Kahrs},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610322},
  month     = {5},
  pages     = {8166-8172},
  title     = {On the disentanglement of tube inequalities in concentric tube continuum robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Degenerate motions of multisensor fusion-based navigation.
<em>ICRA</em>, 8113–8119. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The system observability analysis is of practical importance, for example, due to its ability to identify the unobservable directions of the estimated state which can influence estimation accuracy and help develop consistent and robust estimators. Recent studies focused on analyzing the observability of the state of various multisensor systems with a particular interest in unobservable directions induced by degenerate motions. However, those studies mostly stay in the specific sensor domain without aiding to extend the understanding to other heterogeneous systems. To this end, in this work, we provide degenerate motion analysis on general local and global sensor-paired systems, offering insights applicable to a wide range of existing navigation systems. Our analysis includes 9 degenerate motion identification including 5 already identified in literature and 4 new motions with both synchronous and asynchronous sensor-pair cases. Comprehensive numerical studies are conducted to verify those identified motions, show the effect of degenerate motion on state estimation, and demonstrate the generalizability of our analysis on various multisensor systems.},
  archive   = {C_ICRA},
  author    = {Woosik Lee and Chuchu Chen and Guoquan Huang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610255},
  month     = {5},
  pages     = {8113-8119},
  title     = {Degenerate motions of multisensor fusion-based navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling and analysis of combined rimless wheel with
tensegrity spine. <em>ICRA</em>, 8088–8093. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the natural world, benefited from the advantages of the spine, quadrupeds exhibiting extraordinary flexibility which allowing them to move efficiently on variable terrains. The previous researches have indicated the legged robots which efficiently utilizing their spine can achieve rapid and stable locomotion. However, within the field of legged robot dynamics, the design of the spine and understanding how it positively influences locomotion is unclear, which is significant for quadruped robot to achieve efficient and stable walking. In this study, we proposed a model formed by tensegrity spine and rimless wheel to represent quadrupeds, using passive dynamic walking as a method, which has been well-demonstrated for observing the inherent characteristics, exhibited the locomotion characteristic of the model proposed. By numerical simulation, we observed change trend of locomotion performance with the configurations of spine’s shape, and found direction of spine design that have a positive impact on walking. These findings contribute to the design of spine structures in quadruped robots.},
  archive   = {C_ICRA},
  author    = {Yuxuan Xiang and Yanqiu Zheng and Fumihiko Asano},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611065},
  month     = {5},
  pages     = {8088-8093},
  title     = {Modeling and analysis of combined rimless wheel with tensegrity spine},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforcement learning for blind stair climbing with legged
and wheeled-legged robots. <em>ICRA</em>, 8081–8087. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, legged and wheeled-legged robots have gained prominence for tasks in environments predominantly created for humans across various domains. One significant challenge faced by many of these robots is their limited capability to navigate stairs, which hampers their functionality in multi-story environments. This study proposes a method aimed at addressing this limitation, employing reinforcement learning to develop a versatile controller applicable to a wide range of robots. In contrast to the conventional velocity-based controllers, our approach builds upon a position-based formulation of the RL task, which we show to be vital for stair climbing. Furthermore, the methodology leverages an asymmetric actor-critic structure, enabling the utilization of privileged information from simulated environments during training while eliminating the reliance on exteroceptive sensors during real-world deployment. Another key feature of the proposed approach is the incorporation of a boolean observation within the controller, enabling the activation or deactivation of a stair-climbing mode. We present our results on different quadrupeds and bipedal robots in simulation and showcase how our method allows the balancing robot Ascento to climb 15cm stairs in the real world, a task that was previously impossible for this robot.},
  archive   = {C_ICRA},
  author    = {Simon Chamorro and Victor Klemm and Miguel de La Iglesia Valls and Christopher Pal and Roland Siegwart},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610069},
  month     = {5},
  pages     = {8081-8087},
  title     = {Reinforcement learning for blind stair climbing with legged and wheeled-legged robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical experience-informed navigation for multi-modal
quadrupedal rebar grid traversal. <em>ICRA</em>, 8065–8072. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study focuses on a layered, experience-based, multi-modal contact planning framework for agile quadrupedal locomotion over a constrained rebar environment. To this end, our hierarchical planner incorporates locomotion-specific modules into the high-level contact sequence planner and performs kinodynamically-aware trajectory optimization as the low-level motion planner. Through quantitative analysis of the experience accumulation process and experimental validation of the kinodynamic feasibility of the generated locomotion trajectories, we demonstrate that the planning heuristic of experience offers an effective way of providing candidate footholds for a legged contact planner. Additionally, we introduce a guiding torso path heuristic at the global planning level to enhance the navigation success rate in the presence of environmental obstacles. Our results indicate that the torso-path guided experience accumulation requires significantly fewer offline trials to successfully reach the goal compared to regular experience accumulation. Finally, our planning framework is validated in both dynamics simulations and real hardware implementations on a quadrupedal robot provided by Skymul Inc.},
  archive   = {C_ICRA},
  author    = {Max Asselmeier and Jane Ivanova and Ziyi Zhou and Patricio A. Vela and Ye Zhao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610248},
  month     = {5},
  pages     = {8065-8072},
  title     = {Hierarchical experience-informed navigation for multi-modal quadrupedal rebar grid traversal},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimization based dynamic skateboarding of quadrupedal
robot. <em>ICRA</em>, 8058–8064. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot skateboarding is a novel and challenging task for legged robots. Accurately modeling the dynamics of dual floating bases and developing effective planning and control methods present significant complexities in accomplishing skateboarding behavior. This paper focuses on enabling the quadrupedal platform CyberDog2 to achieve dynamic balancing and acceleration on a skateboard. An optimization-based control pipeline is developed through careful derivation of the system’s equations of motion, considering both the robot and skateboard dynamics. By accounting for system physical constraints, an advanced offline trajectory optimization method is employed to generate various acceleration trajectories, creating a motion library for the system. An online linear model predictive control with whole body control framework is used to track the generated trajectories and stablize the system in real-time. To validate its effectiveness, we conducted experiments in various scenarios. The quadrupedal robot successfully performed acceleration from a static state to various velocities and demonstrated the ability to balance and steer the skateboard.},
  archive   = {C_ICRA},
  author    = {Zhe Xu and Mohamed Al-Khulaqui and Hanxin Ma and Jiajun Wang and Quanbin Xin and Yangwei You and Mingliang Zhou and Diyun Xiang and Shiwu Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611075},
  month     = {5},
  pages     = {8058-8064},
  title     = {Optimization based dynamic skateboarding of quadrupedal robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Convergent iLQR for safe trajectory planning and control of
legged robots. <em>ICRA</em>, 8051–8057. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In order to perform highly dynamic and agile maneuvers, legged robots typically spend time in underactuated domains (e.g. with feet off the ground) where the system has limited command of its acceleration and a constrained amount of time before transitioning to a new domain (e.g. foot touchdown). Meanwhile, these transitions can instantaneously change the system’s state, possibly causing perturbations to be mapped arbitrarily far away from the target trajectory. These properties make it difficult for local feedback controllers to effectively recover from disturbances as the system evolves through underactuated domains and hybrid impact events. To address this, we utilize the fundamental solution matrix that characterizes the evolution of perturbations through a hybrid trajectory and its 2-norm, which represents the worst-case growth of perturbations. In this paper, the worst-case perturbation analysis is used to explicitly reason about the tracking performance of a hybrid trajectory and is incorporated in an iLQR framework to optimize a trajectory while taking into account the closed-loop convergence of the trajectory under an LQR tracking controller. The generated convergent trajectories recover more effectively from perturbations, are more robust to large disturbances, and use less feedback control effort than trajectories generated with traditional methods.},
  archive   = {C_ICRA},
  author    = {James Zhu and J. Joe Payne and Aaron M. Johnson},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611641},
  month     = {5},
  pages     = {8051-8057},
  title     = {Convergent iLQR for safe trajectory planning and control of legged robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient terrain map using planar regions for footstep
planning on humanoid robots. <em>ICRA</em>, 8044–8050. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humanoid robots possess the ability to perform complex tasks in challenging environments. However, they require a model of the surroundings in a representation that is sufficient enough for downstream tasks such as footstep planning. The maps generated by existing mapping algorithms are either sparse, insufficient for footstep planning, memory intensive, or too slow for dynamic humanoid behaviors. In this work, we develop a mapping algorithm that combines planar region measurements along with kinematic-inertial state estimates to build a dense but efficient map of bounded planar surfaces. We present novel algorithms for plane feature matching, tracking and registration for mapping within a factor graph framework. The generated map is not only memory efficient, but also offers higher reliability and speed in bipedal footstep planning, than was possible earlier. The complete algorithm is also demonstrated using a full-scale humanoid robot, Nadia, walking over both flat ground and rough terrain utilizing the generated terrain map.},
  archive   = {C_ICRA},
  author    = {Bhavyansh Mishra and Duncan Calvert and Sylvain Bertrand and Jerry Pratt and Hakki Erhan Sevil and Robert Griffin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610879},
  month     = {5},
  pages     = {8044-8050},
  title     = {Efficient terrain map using planar regions for footstep planning on humanoid robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unlocking versatile locomotion: A novel quadrupedal robot
with 4-DoFs legs for roller skating. <em>ICRA</em>, 8037–8043. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Roller skating with passive wheels on a quadrupedal robot is more efficient than traditional walking. However, the typical mammalian quadruped robot with 3-DoFs legs can only perform one dynamic roller skating gait and has difficulty achieving turning motion. To address this limitation, we designed a novel quadrupedal robot with each leg having 4-DoFs to enable various roller skating locomotion including Swizzling, Stroking, and trot-like gaits while easily achieving turning motions. We considered the geometrical characteristics of the passive wheel and used the Levenberg-Marquardt method in robot kinematics to improve precision for both roller skating kinematics and contact point position for the dynamics controller. The position of the robot foot and the yaw angle of the passive wheel are decoupled for motion planning of all proposed gaits. Our proposed kinematics with wheeled geometry was verified through experiments to have higher precision, while the feasibility of all proposed roller-skating gaits was confirmed during straight motion and turning motion with a small radius on our prototype robot. Finally, we discussed the mobility efficiency of different roller skating gaits which were found to be more efficient than walking.},
  archive   = {C_ICRA},
  author    = {Jiawei Chen and Ripeng Qin and Longfei Huang and Zongbo He and Kun Xu and Xilun Ding},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610706},
  month     = {5},
  pages     = {8037-8043},
  title     = {Unlocking versatile locomotion: A novel quadrupedal robot with 4-DoFs legs for roller skating},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hierarchical framework for robot safety using whole-body
tactile sensors. <em>ICRA</em>, 8021–8028. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Using tactile signal is a natural way to perceive potential dangers and safeguard robots. One possible method is to use full-body tactile sensors on the robot and perform safety maneuvers when dangerous stimuli are detected. In this work, we proposed a method based on full-body tactile sensors that operates at three different levels of granularity to ensure that robot interacts with the environment safely. The results showed that our system dramatically reduced the overall collision chance compared with several baselines, and intelligently handled current collisions. Our proposed framework is generalizable to a wide variety of robots, enabling them to predict and avoid dangerous collisions and reactively handle accidental tactile stimuli.},
  archive   = {C_ICRA},
  author    = {Shuo Jiang and Lawson L.S. Wong},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610834},
  month     = {5},
  pages     = {8021-8028},
  title     = {A hierarchical framework for robot safety using whole-body tactile sensors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multimodal visual-tactile representation learning through
self-supervised contrastive pre-training. <em>ICRA</em>, 8013–8020. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The rapidly evolving field of robotics necessitates methods that can facilitate the fusion of multiple modalities. Specifically, when it comes to interacting with tangible objects, effectively combining visual and tactile sensory data is key to understanding and navigating the complex dynamics of the physical world, enabling a more nuanced and adaptable response to changing environments. Nevertheless, much of the earlier work in merging these two sensory modalities has relied on supervised methods utilizing datasets labeled by humans. This paper introduces MViTac, a novel methodology that leverages contrastive learning to integrate vision and touch sensations in a self-supervised fashion. By availing both sensory inputs, MViTac leverages intra and inter-modality losses for learning representations, resulting in enhanced material property classification and more adept grasping prediction. Through a series of experiments, we showcase the effectiveness of our method and its superiority over existing state-of-the-art self-supervised and supervised techniques. In evaluating our methodology, we focus on two distinct tasks: material classification and grasping success prediction. Our results indicate that MViTac facilitates the development of improved modality encoders, yielding more robust representations as evidenced by linear probing assessments. https://sites.google.com/view/mvitac/home},
  archive   = {C_ICRA},
  author    = {Vedant Dave and Fotios Lygerakis and Elmar Rueckert},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610228},
  month     = {5},
  pages     = {8013-8020},
  title     = {Multimodal visual-tactile representation learning through self-supervised contrastive pre-training},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Crosstalk-free impedance-separating array measurement for
iontronic tactile sensors*. <em>ICRA</em>, 7998–8004. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Iontronic tactile sensors are promising to measure spatial-temporal contact information with high performance. However, no suitable measuring method has been presented, due to issues with crosstalk and non-negligible equivalent resistance. Hence, this study presents an impedance-separating method, which does not require complex analog components. A general Quadri-Terminal Impedance Network (QTIN) model is introduced to reduce crosstalk, which has specific compatibility with the impedance-separating method. The precise ranges are measured, showing non-rectangle shapes suitable for the response of iontronic tactile sensors. A simple denoising method is provided to reduce initial array noise obviously. This work could benefit various scenarios, such as human-robot interaction and physiological information monitoring.},
  archive   = {C_ICRA},
  author    = {Funing Hou and Gang Li and Chenxing Mu and Mengqi Shi and Jixiao Liu and Shijie Guo},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610535},
  month     = {5},
  pages     = {7998-8004},
  title     = {Crosstalk-free impedance-separating array measurement for iontronic tactile sensors*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tactile-informed action primitives mitigate jamming in dense
clutter. <em>ICRA</em>, 7991–7997. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It is difficult for robots to retrieve objects in densely cluttered lateral access scenes with movable objects as jamming against adjacent objects and walls can inhibit progress. We propose the use of two action primitives— burrowing and excavating—that can fluidize the scene to unjam obstacles and enable continued progress. Even when these primitives are implemented in an open loop manner at clockdriven intervals, we observe a decrease in the final distance to the target location. Furthermore, we combine the primitives into a closed loop hybrid control strategy using tactile and proprioceptive information to leverage the advantages of both primitives without being overly disruptive. In doing so, we achieve a 10-fold increase in success rate above the baseline control strategy and significantly improve completion times as compared to the primitives alone or a naive combination of them.},
  archive   = {C_ICRA},
  author    = {Dane Brouwer and Joshua Citron and Hojung Choi and Marion Lepert and Michael Lin and Jeannette Bohg and Mark Cutkosky},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610224},
  month     = {5},
  pages     = {7991-7997},
  title     = {Tactile-informed action primitives mitigate jamming in dense clutter},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimization of flexible bronchoscopy shape sensing using
fiber optic sensors. <em>ICRA</em>, 7984–7990. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents a novel shape evaluation and optimization approach for shape sensing, specifically targeting the constrained, irregular, and intricate spatial shapes of flexible bronchoscopes (FB) in human bronchial tree. The proposed evaluation criteria and optimization methods combine clinical significance related to bronchial anatomical structures and address issues related to singular points and discontinuities in traditional shape reconstruction models. Three-dimensional experiments were conducted within eight spatial complex configurations printed from a proportional bronchial model. The 3D experiment results demonstrate an average reduction of approximately 34.1% in shape reconstruction errors across all eight airway models compared to the traditional model, validating the effectiveness and feasibility.},
  archive   = {C_ICRA},
  author    = {Xinran Liu and Hao Chen and Hongbin Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611502},
  month     = {5},
  pages     = {7984-7990},
  title     = {Optimization of flexible bronchoscopy shape sensing using fiber optic sensors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TEXterity: Tactile extrinsic deXterity. <em>ICRA</em>,
7976–7983. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a novel approach that combines tactile estimation and control for in-hand object manipulation. By integrating measurements from robot kinematics and an image-based tactile sensor, our framework estimates and tracks object pose while simultaneously generating motion plans in a receding horizon fashion to control the pose of a grasped object. This approach consists of a discrete pose estimator that tracks the most likely sequence of object poses in a coarsely discretized grid, and a continuous pose estimator-controller to refine the pose estimate and accurately manipulate the pose of the grasped object. Our method is tested on diverse objects and configurations, achieving desired manipulation objectives and outperforming single-shot methods in estimation accuracy. The proposed approach holds potential for tasks requiring precise manipulation and limited intrinsic in-hand dexterity under visual occlusion, laying the foundation for closed-loop behavior in applications such as regrasping, insertion, and tool use. Please see this url for videos of real-world demonstrations.},
  archive   = {C_ICRA},
  author    = {Antonia Bronars and Sangwoon Kim and Parag Patre and Alberto Rodriguez},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610622},
  month     = {5},
  pages     = {7976-7983},
  title     = {TEXterity: Tactile extrinsic deXterity},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GelRoller: A rolling vision-based tactile sensor for large
surface reconstruction using self-supervised photometric stereo method.
<em>ICRA</em>, 7961–7967. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate perception of the surrounding environment stands as a primary objective for robots. Through tactile interaction, vision-based tactile sensors provide the capability to capture high-resolution and multi-modal surface information of objects, thereby facilitating robots in achieving more dexterous manipulations. However, the prevailing GelSight sensors entail intricate calibration procedures, posing challenges in their application on curved surfaces and requiring the maintenance of stable lighting conditions throughout experimentation. Additionally, constrained by shape and structure, current vision-based tactile sensors are predominantly applied to measurements within a limited area. In this study, we design a novel cylindrical vision-based tactile sensor that enables continuous and swift perception of large-scale object surfaces through rolling. To tackle the challenges posed by laborious calibration processes, we propose a self-supervised photometric stereo method based on deep learning, which eliminates pre-calibration requirements and enables the derivation of surface normals from a single image without relying on stable lighting conditions. Finally, we perform surface reconstruction from normal and point cloud registration on the multiple frames of images obtained by rolling the cylindrical sensor, resulting in large surface reconstruction. We compare our method with the representative lookup table method in the GelSight sensors. The results show that the proposed method enhances both reconstruction accuracy and robustness, thereby demonstrating the potential of the proposed sensor in large-scale surface reconstruction. Codes and mechanical structures are available at: https://github.com/ZhangZhiyuanZhang/GelRoller},
  archive   = {C_ICRA},
  author    = {Zhiyuan Zhang and Huan Ma and Yulin Zhou and Jingjing Ji and Hua Yang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610417},
  month     = {5},
  pages     = {7961-7967},
  title     = {GelRoller: A rolling vision-based tactile sensor for large surface reconstruction using self-supervised photometric stereo method},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Building user proficiency in piloting small unmanned aerial
vehicles (sUAV). <em>ICRA</em>, 7946–7952. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Assessing proficiency in small unmanned aerial vehicles (sUAVs) pilots is complex and not well understood, but increasingly important to employ these vehicles in serious jobs such as wildland firefighting and infrastructure inspection. The limited prior work with UAVs has focused on user training using modalities like simulators and VR and no performance assessments with line-of-sight UAVs. This paper presents a training methodology for novice pilots of sUAVs. We presented two studies: the Baseline study (21 participants) and the Training study (16 participants). Our work is of interest to sUAV operators, regulators, and companies developing this technologies to produce a more capable workforce capable of consistent, safe operations. We successfully utilized the method developed in [1] to assess user proficiency in flying UAVs. We presented a UAV pilot training schedule for novice users (in the Training study), and were able to determine the minimum training time necessary to observe performance gains and mitigate damage. Results indicate that task completions noticeably improved and crashes minimized by day 10 of training, with a training plateau observed by day 15.},
  archive   = {C_ICRA},
  author    = {Siya Kunde and Brittany Duncan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610439},
  month     = {5},
  pages     = {7946-7952},
  title     = {Building user proficiency in piloting small unmanned aerial vehicles (sUAV)},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decision making for human-in-the-loop robotic agents via
uncertainty-aware reinforcement learning. <em>ICRA</em>, 7939–7945. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In a Human-in-the-Loop paradigm, a robotic agent is able to act mostly autonomously in solving a task, but can request help from an external expert when needed. However, knowing when to request such assistance is critical: too few requests can lead to the robot making mistakes, but too many requests can overload the expert. In this paper, we present a Reinforcement Learning based approach to this problem, where a semi-autonomous agent asks for external assistance when it has low confidence in the eventual success of the task. The confidence level is computed by estimating the variance of the return from the current state. We show that this estimate can be iteratively improved during training using a Bellman-like recursion. On discrete navigation problems with both fully-and partially-observable state information, we show that our method makes effective use of a limited budget of expert calls at run-time, despite having no access to the expert at training time.},
  archive   = {C_ICRA},
  author    = {Siddharth Singi and Zhanpeng He and Alvin Pan and Sandip Patel and Gunnar A. Sigurdsson and Robinson Piramuthu and Shuran Song and Matei Ciocarlie},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611425},
  month     = {5},
  pages     = {7939-7945},
  title     = {Decision making for human-in-the-loop robotic agents via uncertainty-aware reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Jacquard v2: Refining datasets using the human in the loop
data correction method. <em>ICRA</em>, 7932–7938. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the context of rapid advancements in industrial automation, vision-based robotic grasping plays an increasingly crucial role. In order to enhance visual recognition accuracy, the utilization of large-scale datasets is imperative for training models to acquire implicit knowledge related to the handling of various objects. Creating datasets from scratch is a time and labor-intensive process. Moreover, existing datasets often contain errors due to automated annotations aimed at expediency, making the improvement of these datasets a substantial research challenge. Consequently, several issues have been identified in the annotation of grasp bounding boxes within the popular Jacquard Grasp Dataset [1]. We propose utilizing a Human-In-The-Loop(HIL) method to enhance dataset quality. This approach relies on backbone deep learning networks to predict object positions and orientations for robotic grasping. Predictions with Intersection over Union (IOU) values below 0.2 undergo an assessment by human operators. After their evaluation, the data is categorized into False Negatives(FN) and True Negatives(TN). FN are then subcategorized into either missing annotations or catastrophic labeling errors. Images lacking labels are augmented with valid grasp bounding box information, whereas images afflicted by catastrophic labeling errors are completely removed. The open-source tool Labelbee was employed for 53,026 iterations of HIL dataset enhancement, leading to the removal of 2,884 images and the incorporation of ground truth information for 30,292 images. The enhanced dataset, named the Jacquard V2 Grasping Dataset, served as the training data for a range of neural networks. We have empirically demonstrated that these dataset improvements significantly enhance the training and prediction performance of the same network, resulting in an increase of 7.1% across most popular detection architectures for ten iterations. This refined dataset will be accessible on One Drive and Baidu Netdisk, while the associated tools, source code, and benchmarks will be made available on GitHub (https://github.com/lqh12345/Jacquard_V2).},
  archive   = {C_ICRA},
  author    = {Qiuhao Li and Shenghai Yuan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611652},
  month     = {5},
  pages     = {7932-7938},
  title     = {Jacquard v2: Refining datasets using the human in the loop data correction method},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards enhanced human activity recognition for real-world
human-robot collaboration. <em>ICRA</em>, 7909–7915. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This research contributes to the field of Human-Robot Collaboration (HRC) within dynamic and unstructured environments by extending the previously proposed Fuzzy State-Long Short-Term Memory (FS-LSTM) architecture to handle the uncertainty and irregularity inherent in real-world sensor data. Recognising the challenges posed by low-cost sensors, which are highly susceptible to environmental conditions and often fail to provide regular periodic readings, this paper introduces additional pre-processing blocks. These include two indirect Kalman filters and an additional LSTM network, which together enhance the input variables for the fuzzification process. The enhanced FS-LSTM approach is evaluated using real-world data, demonstrating its effectiveness in extracting meaningful information and accurately recognising human activities. This work underscores the potential of robotics in addressing global challenges, particularly in labour-intensive and hazardous tasks. By improving the integration of humans and robots in unstructured environments, this research contributes to the broader exploration of robotics in new societal applications, fostering connections and collaborations across diverse fields.},
  archive   = {C_ICRA},
  author    = {Beril Yalcinkaya and Micael S. Couceiro and Lucas Pina and Salviano Soares and António Valente and Fabio Remondino},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610664},
  month     = {5},
  pages     = {7909-7915},
  title     = {Towards enhanced human activity recognition for real-world human-robot collaboration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transparency control of a 1-DoF knee exoskeleton via
human-in-the-loop velocity optimisation. <em>ICRA</em>, 7902–7908. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Rehabilitative robotics, particularly lower-limb exoskeletons (LLEs), have gained increasing importance in aiding patients regain ambulatory functions. One of the challenges in making these systems effective is the implementation of an assist-as-needed (AAN) control strategy that intervenes only when the patient deviates from the correct movement pattern. Equally crucial is the need for the LLE to exhibit &quot;transparency&quot; — minimising its interaction forces with the wearer to feel as natural as possible. This paper introduces a novel approach to transparency control based on a human-in-the-loop velocity optimisation framework. The proposed method employs torque data captured from past steps through a Series Elastic Actuator (SEA) to approximate the wearer’s intended future movements and computes a corresponding transparent velocity trajectory. The velocity commands are complemented by an Adaptive Frequency Oscillator (AFO) based position controller that leverages the periodic nature of human gait and is modified with a force sensor for increased reactiveness to human gait variations. This approach is experimentally evaluated against a standard zero-torque controller with a stationary single-degree-of-freedom knee exoskeleton test platform in a proof-of-concept study. Preliminary results indicate that combining adaptive oscillators with interaction force sensing can improve transparency compared to the conventional zero-torque controller, using force readings for position control and torque measurements for velocity optimisation and control.},
  archive   = {C_ICRA},
  author    = {Lukas Cha and Annika Guez and Chih-Yu Chen and Sion Kim and Zhenhua Yu and Bo Xiao and Ravi Vaidyanathan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611632},
  month     = {5},
  pages     = {7902-7908},
  title     = {Transparency control of a 1-DoF knee exoskeleton via human-in-the-loop velocity optimisation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VIDAR: Data quality improvement for monocular 3D
reconstruction through in-situ visual interaction. <em>ICRA</em>,
7895–7901. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D reconstruction based on monocular videos has attracted wide attention, and existing reconstruction methods usually work in a reconstruction-after-scanning manner. However, these methods suffer from insufficient data collection problems due to the lack of effective guidance for users during the scanning process, which affects reconstruction quality. We propose VIDAR, which visually guides users with the streaming incremental reconstructed mesh in data collection for monocular 3D reconstruction. We propose an incremental mesh extraction algorithm to achieve lossless fusion of streaming incremental mesh data via slice-style management for guidance quality. We also design an incremental mesh rendering algorithm to achieve precise memory reallocation by updating the buffer in a fill-in-the-blank pattern for guidance efficiency. Besides, we introduce several optimizations on data transmission and human-computer interaction to improve the overall system performance. The experiment results on real-world scenes show that VIDAR efficiently delivers high-quality visual guidance and outperforms the non-interactive data collection methods for scene reconstruction.},
  archive   = {C_ICRA},
  author    = {Han Gao and Yating Liu and Fang Cao and Hao Wu and Fengyuan Xu and Sheng Zhong},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610260},
  month     = {5},
  pages     = {7895-7901},
  title     = {VIDAR: Data quality improvement for monocular 3D reconstruction through in-situ visual interaction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ToP-ToM: Trust-aware robot policy with theory of mind.
<em>ICRA</em>, 7888–7894. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Theory of Mind (ToM) is a fundamental cognitive architecture that endows humans with the ability to attribute mental states to others. Humans infer the desires, beliefs, and intentions of others by observing their behavior and, in turn, adjust their actions to facilitate better interpersonal communication and team collaboration. In this paper, we investigated trust-aware robot policy with the theory of mind in a multi-agent setting where a human collaborates with a robot against another human opponent. We show that by only focusing on team performance, the robot may resort to the reverse psychology trick, which poses a significant threat to trust maintenance. The human’s trust in the robot will collapse when they discover deceptive behavior by the robot. To mitigate this problem, we adopt the robot theory of mind model to infer the human’s trust beliefs, including true belief and false belief (an essential element of ToM). We designed a dynamic trust-aware reward function based on different trust beliefs to guide the robot policy learning, which aims to balance between avoiding human trust collapse due to robot reverse psychology and leveraging its potential to boost team performance. The experimental results demonstrate the importance of the ToM-based robot policy for human-robot trust and the effectiveness of our robot ToM-based robot policy in multiagent interaction settings.},
  archive   = {C_ICRA},
  author    = {Chuang Yu and Baris Serhan and Angelo Cangelosi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610127},
  month     = {5},
  pages     = {7888-7894},
  title     = {ToP-ToM: Trust-aware robot policy with theory of mind},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhanced human-robot collaboration with intent prediction
using deep inverse reinforcement learning. <em>ICRA</em>, 7880–7887. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In shared autonomy, human-robot handover for object delivery is crucial. Accurate robot predictions of human hand motion and intentions enhance collaboration efficiency. However, low prediction accuracy increases mental and physical demands on the user. In this work, we propose a system for predicting hand motion and intended target during human-robot handover using Inverse Reinforcement Learning (IRL). A set of feature functions were designed to explicitly capture users’ preferences during the task. The proposed approach was experimentally validated through user studies. Results indicate that the proposed method outperformed other state-of-the-art methods (PI-IRL, BP-HMT, RNNIK-MKF and CM k=5 ) with users feeling comfortable reaching upto 60% of the total distance to the target for handover with 90% target prediction accuracy. The target prediction accuracy reaches 99.9% when less than 20% of the task remains.},
  archive   = {C_ICRA},
  author    = {Mukund Mitra and Gyanig Kumar and Partha Pratim Chakrabarti and Pradipta Biswas},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610595},
  month     = {5},
  pages     = {7880-7887},
  title     = {Enhanced human-robot collaboration with intent prediction using deep inverse reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From unstable electrode contacts to reliable control: A deep
learning approach for HD-sEMG in neurorobotics. <em>ICRA</em>,
7874–7879. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the past decade, there has been significant advancement in designing wearable neural interfaces for controlling neurorobotic systems, particularly bionic limbs. These interfaces function by decoding signals captured noninvasively from the skin’s surface. Portable high-density surface electromyography (HD-sEMG) modules combined with deep learning decoding have attracted interest by achieving excellent gesture prediction and myoelectric control of prosthetic systems and neurorobots. However, factors like small electrode size and unstable electrode-skin contacts make HD-sEMG susceptible to pixel electrode drops. The sparse electrode-skin disconnections rooted in issues such as low adhesion, sweating, hair blockage, and skin stretch challenge the reliability and scalability of these modules as the perception unit for neurorobotic systems. This paper proposes a novel deep-learning model providing resiliency for HD-sEMG modules, which can be used in the wearable interfaces of neurorobots. The proposed 3D Dilated Efficient CapsNet model trains on an augmented input space to computationally ‘force’ the network to learn channel dropout variations and thus learn robustness to channel dropout. The proposed framework maintained high performance under a sensor dropout reliability study conducted. Results show conventional models’ performance significantly degrades with dropout and is recovered using the proposed architecture and the training paradigm.},
  archive   = {C_ICRA},
  author    = {Eion Tyacke and Kunal Gupta and Jay Patel and Raghav Katoch and S. Farokh Atashzar},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610638},
  month     = {5},
  pages     = {7874-7879},
  title     = {From unstable electrode contacts to reliable control: A deep learning approach for HD-sEMG in neurorobotics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interactive navigation in environments with traversable
obstacles using large language and vision-language models.
<em>ICRA</em>, 7867–7873. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes an interactive navigation framework by using large language and vision-language models, allowing robots to navigate in environments with traversable obstacles. We utilize the large language model (GPT-3.5) and the open-set Vision-language Model (Grounding DINO) to create an action-aware costmap to perform effective path planning without fine-tuning. With the large models, we can achieve an end-to-end system from textual instructions like &quot;Can you pass through the curtains to deliver medicines to me?&quot;, to bounding boxes (e.g., curtains) with action-aware attributes. They can be used to segment LiDAR point clouds into two parts: traversable and untraversable parts, and then an action-aware costmap is constructed for generating a feasible path. The pre-trained large models have great generalization ability and do not require additional annotated data for training, allowing fast deployment in the interactive navigation tasks. We choose to use multiple traversable objects such as curtains and grasses for verification by instructing the robot to traverse them. Besides, traversing curtains in a medical scenario was tested. All experimental results demonstrated the proposed framework’s effectiveness and adaptability to diverse environments.},
  archive   = {C_ICRA},
  author    = {Zhen Zhang and Anran Lin and Chun Wai Wong and Xiangyu Chu and Qi Dou and K. W. Samuel Au},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610751},
  month     = {5},
  pages     = {7867-7873},
  title     = {Interactive navigation in environments with traversable obstacles using large language and vision-language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning self-confidence from semantic action embeddings for
improved trust in human-robot interaction. <em>ICRA</em>, 7859–7866. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In Human-Robot Interaction (HRI) scenarios, human factors like trust can greatly impact task performance and interaction quality. Recent research has confirmed that perceived robot proficiency is a major antecedent of trust. By making robots aware of their capabilities, we can allow them to choose when to perform low-confidence actions, thus actively controlling the risk of trust reduction. In this paper, we propose Self-Confidence through Observed Novel Experiences (SCONE), a policy to learn self-confidence from experience using semantic action embeddings. Using an assistive cooking setting, we show that the semantic aspect allows SCONE to learn self-confidence faster than existing approaches, while also achieving promising performance in simple instructions following. Finally, we share results from a pilot study with 31 participants, showing that such a self-confidence-aware policy increases capability-based human trust.},
  archive   = {C_ICRA},
  author    = {Cedric Goubard and Yiannis Demiris},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611445},
  month     = {5},
  pages     = {7859-7866},
  title     = {Learning self-confidence from semantic action embeddings for improved trust in human-robot interaction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trust recognition in human-robot cooperation using EEG.
<em>ICRA</em>, 7827–7833. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collaboration between humans and robots is becoming increasingly crucial in our daily life. In order to accomplish efficient cooperation, trust recognition is vital, empowering robots to predict human behaviors and make trust-aware decisions. Consequently, there is an urgent need for a generalized approach to recognize human-robot trust. This study addresses this need by introducing an EEG-based method for trust recognition during human-robot cooperation. A human-robot cooperation game scenario is used to stimulate various human trust levels when working with robots. To enhance recognition performance, the study proposes an EEG Vision Transformer model coupled with a 3-D spatial representation to capture the spatial information of EEG, taking into account the topological relationship among electrodes. To validate this approach, a public EEG-based human trust dataset called EEGTrust is constructed. Experimental results indicate the effectiveness of the proposed approach, achieving an accuracy of 74.99% in slice-wise cross-validation and 62.00% in trial-wise cross-validation. This outperforms baseline models in both recognition accuracy and generalization. Furthermore, an ablation study demonstrates a significant improvement in trust recognition performance of the spatial representation. The source code and EEGTrust dataset are available at https://github.com/CaiyueXu/EEGTrust.},
  archive   = {C_ICRA},
  author    = {Caiyue Xu and Changming Zhang and Yanmin Zhou and Zhipeng Wang and Ping Lu and Bin He},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610156},
  month     = {5},
  pages     = {7827-7833},
  title     = {Trust recognition in human-robot cooperation using EEG},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised 6-DoF robot grasping by demonstration via
augmented reality teleoperation system. <em>ICRA</em>, 7819–7826. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most existing 6-DoF robot grasping solutions depend on strong supervision on grasp pose to ensure satisfactory performance, which could be laborious and impractical when the robot works in some restricted area. To this end, we propose a self-supervised 6-DoF grasp pose detection framework via an Augmented Reality (AR) teleoperation system that can efficiently learn human demonstrations and provide 6-DoF grasp poses without grasp pose annotations. Specifically, the system collects the human demonstration from the AR environment and contrastively learns the grasping strategy from the demonstration. For the real-world experiment, the proposed system leads to satisfactory grasping abilities and learning to grasp unknown objects within three demonstrations.},
  archive   = {C_ICRA},
  author    = {Xiwen Dengxiong and Xueting Wang and Shi Bai and Yunbo Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611721},
  month     = {5},
  pages     = {7819-7826},
  title     = {Self-supervised 6-DoF robot grasping by demonstration via augmented reality teleoperation system},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 6-DoF closed-loop grasping with reinforcement learning.
<em>ICRA</em>, 7812–7818. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel vision-based, 6-DoF grasping framework based on Deep Reinforcement Learning (DRL) that is capable of directly synthesizing continuous 6-DoF actions in cartesian space. Our proposed approach uses visual observations from an eye-in-hand RGB-D camera, and we mitigate the sim-to-real gap with a combination of domain randomization, image augmentation, and segmentation tools. Our method consists of an off-policy, maximum-entropy, Actor-Critic algorithm that learns a policy from a binary reward and a few simulated example grasps. It does not need any real-world grasping examples, is trained completely in simulation, and is deployed directly to the real world without any fine-tuning. The efficacy of our approach is demonstrated in simulation and experimentally validated in the real world on 6-DoF grasping tasks, achieving state-of-the-art results of an 86% mean zero-shot success rate on previously unseen objects, an 85% mean zero-shot success rate on a class of previously unseen adversarial objects, and a 74.3% mean zero-shot success rate on a class of previously unseen, challenging &quot;6-DoF&quot; objects.Raw footage of real-world validation can be found at https://youtu.be/bwPf8Imvook},
  archive   = {C_ICRA},
  author    = {Sverre Herland and Kerstin Bach and Ekrem Misimi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610080},
  month     = {5},
  pages     = {7812-7818},
  title     = {6-DoF closed-loop grasping with reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Information-driven affordance discovery for efficient
robotic manipulation. <em>ICRA</em>, 7780–7787. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic affordances, providing information about what actions can be taken in a given situation, can aid robotic manipulation. However, learning about affordances requires expensive large annotated datasets of interactions or demonstrations. In this work, we argue that well-directed interactions with the environment can mitigate this problem and propose an information-based measure to augment the agent&#39;s objective and accelerate the affordance discovery process. We provide a theoretical justification of our approach and we empirically validate the approach both in simulation and real-world tasks. Our method, which we dub IDA, enables the efficient discovery of visual affordances for several action primitives, such as grasping, stacking objects, or opening drawers, strongly improving data efficiency in simulation, and it allows us to learn grasping affordances in a small number of interactions, on a real-world setup with a UFACTORY xArm 6 robot arm.},
  archive   = {C_ICRA},
  author    = {Pietro Mazzaglia and Taco Cohen and Daniel Dijkman},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611170},
  month     = {5},
  pages     = {7780-7787},
  title     = {Information-driven affordance discovery for efficient robotic manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mastering stacking of diverse shapes with large-scale
iterative reinforcement learning on real robots. <em>ICRA</em>,
7772–7779. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning solely from an agent’s self-generated data is often believed to be infeasible for learning on real robots, due to the amount of data needed. However, if done right, agents learning from real data can be surprisingly efficient through re-using previously collected sub-optimal data. In this paper we demonstrate how the increased understanding of off-policy learning methods and their embedding in an iterative online/offline scheme (&quot;collect and infer&quot;) can drastically improve data-efficiency by using all the collected experience, which empowers learning from real robot experience only. Moreover, the resulting policy improves significantly over the state of the art on a recently proposed real robot manipulation benchmark. Our approach learns end-to-end, directly from pixels, and does not rely on additional human domain knowledge such as a simulator or demonstrations.},
  archive   = {C_ICRA},
  author    = {Thomas Lampe and Abbas Abdolmaleki and Sarah Bechtle and Sandy H. Huang and Jost Tobias Springenberg and Michael Bloesch and Oliver Groth and Roland Hafner and Tim Hertweck and Michael Neunert and Markus Wulfmeier and Jingwei Zhang and Francesco Nori and Nicolas Heess and Martin Riedmiller},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610297},
  month     = {5},
  pages     = {7772-7779},
  title     = {Mastering stacking of diverse shapes with large-scale iterative reinforcement learning on real robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MORPH: Design co-optimization with reinforcement learning
via a differentiable hardware model proxy. <em>ICRA</em>, 7764–7771. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce MORPH, a method for co-optimization of hardware design parameters and control policies in simulation using reinforcement learning. Like most co-optimization methods, MORPH relies on a model of the hardware being optimized, usually simulated based on the laws of physics. However, such a model is often difficult to integrate into an effective optimization routine. To address this, we introduce a proxy hardware model, which is always differentiable and enables efficient co-optimization alongside a long-horizon control policy using RL. MORPH is designed to ensure that the optimized hardware proxy remains as close as possible to its realistic counterpart, while still enabling task completion. We demonstrate our approach on simulated 2D reaching and 3D multi-fingered manipulation tasks.},
  archive   = {C_ICRA},
  author    = {Zhanpeng He and Matei Ciocarlie},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610732},
  month     = {5},
  pages     = {7764-7771},
  title     = {MORPH: Design co-optimization with reinforcement learning via a differentiable hardware model proxy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RGBManip: Monocular image-based robotic manipulation through
active object pose estimation. <em>ICRA</em>, 7748–7755. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic manipulation requires accurate perception of the environment, which poses a significant challenge due to its inherent complexity and constantly changing nature. In this context, RGB image and point-cloud observations are two commonly used modalities in visual-based robotic manipulation, but each of these modalities have their own limitations. Commercial point-cloud observations often suffer from issues like sparse sampling and noisy output due to the limits of the emission-reception imaging principle. On the other hand, RGB images, while rich in texture information, lack essential depth and 3D information crucial for robotic manipulation. To mitigate these challenges, we propose an image-only robotic manipulation framework that leverages an eye-on-hand monocular camera installed on the robot’s parallel gripper. By moving with the robot gripper, this camera gains the ability to actively perceive the object from multiple perspectives during the manipulation process. This enables the estimation of 6D object poses, which can be utilized for manipulation. While, obtaining images from more and diverse viewpoints typically improves pose estimation, it also increases the manipulation time. To address this trade-off, we employ a reinforcement learning policy to synchronize the manipulation strategy with active perception, achieving a balance between 6D pose accuracy and manipulation efficiency. Our experimental results in both simulated and real-world environments showcase the state-of-the-art effectiveness of our approach. We believe that our method will inspire further research on real-world-oriented robotic manipulation. See https://rgbmanip.github.io/ for more details.},
  archive   = {C_ICRA},
  author    = {Boshi An and Yiran Geng and Kai Chen and Xiaoqi Li and Qi Dou and Hao Dong},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610690},
  month     = {5},
  pages     = {7748-7755},
  title     = {RGBManip: Monocular image-based robotic manipulation through active object pose estimation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lifelong LERF: Local 3D semantic inventory monitoring using
FogROS2. <em>ICRA</em>, 7740–7747. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inventory monitoring in homes, factories, and retail stores relies on maintaining data despite objects being swapped, added, removed, or moved. We introduce Lifelong LERF, a method that allows a mobile robot with minimal compute to jointly optimize a dense language and geometric representation of its surroundings. Lifelong LERF maintains this representation over time by detecting semantic changes and selectively updating these regions of the environment, avoiding the need to exhaustively remap. Human users can query inventory by providing natural language queries and receiving a 3D heatmap of potential object locations. To manage the computational load, we use Fog-ROS2, a cloud robotics platform, to offload resource-intensive tasks. Lifelong LERF obtains poses from a monocular RGBD SLAM backend, and uses these poses to progressively optimize a Language Embedded Radiance Field (LERF) for semantic monitoring. Experiments with 3-5 objects arranged on a tabletop and a Turtlebot with a RealSense camera suggest that Lifelong LERF can persistently adapt to changes in objects with up to 91% accuracy.},
  archive   = {C_ICRA},
  author    = {Adam Rashid and Chung Min Kim and Justin Kerr and Letian Fu and Kush Hari and Ayah Ahmad and Kaiyuan Chen and Huang Huang and Marcus Gualtieri and Michael Wang and Christian Juette and Nan Tian and Liu Ren and Ken Goldberg},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611174},
  month     = {5},
  pages     = {7740-7747},
  title     = {Lifelong LERF: Local 3D semantic inventory monitoring using FogROS2},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DefFusion: Deformable multimodal representation fusion for
3D semantic segmentation. <em>ICRA</em>, 7732–7739. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The complementarity between camera and LiDAR data makes fusion methods a promising approach to improve 3D semantic segmentation performance. Recent transformer-based methods have also demonstrated superiority in segmentation. However, multimodal solutions incorporating transformers are underexplored and face two key inherent difficulties: over-attention and noise from different modal data. To overcome these challenges, we propose a Deformable Multimodal Representation Fusion (DefFusion) framework consisting mainly of a Deformable Representation Fusion Transformer and Dynamic Representation Augmentation Modules. The Deformable Representation Fusion Transformer introduces the deformable mechanism in multimodal fusion, avoiding over-attention and improving efficiency by adaptively modeling a 2D key/value set for a given 3D query, thus enabling multimodal fusion with higher flexibility. To enhance the 2D representation and 3D representation, the Dynamic Representation Enhancement Module is proposed to dynamically remove noise in the input representation via Dynamic Grouped Representation Generation and Dynamic Mask Generation. Extensive experiments validate that our model achieves the best 3D semantic segmentation performance on SemanticKITTI and NuScenes benchmarks.},
  archive   = {C_ICRA},
  author    = {Rongtao Xu and Changwei Wang and Duzhen Zhang and Man Zhang and Shibiao Xu and Weiliang Meng and Xiaopeng Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610465},
  month     = {5},
  pages     = {7732-7739},
  title     = {DefFusion: Deformable multimodal representation fusion for 3D semantic segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). End-to-end semantic segmentation network for low-light
scenes. <em>ICRA</em>, 7725–7731. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the fields of robotic perception and computer vision, achieving accurate semantic segmentation of low-light or nighttime scenes is challenging. This is primarily due to the limited visibility of objects and the reduced texture and color contrasts among them. To address the issue of limited visibility, we propose a hierarchical gated convolution unit, which simultaneously expands the receptive field and restores edge texture. To address the issue of reduced texture among objects, we propose a dual closed-loop bipartite matching algorithm to establish a total loss function consisting of the unsupervised illumination enhancement loss and supervised intersection-over-union loss, thus enabling the joint minimization of both losses via the Hungarian algorithm. We thus achieve end-to-end training for a semantic segmentation network especially suitable for handling low-light scenes. Experimental results demonstrate that the proposed network surpasses existing methods on the Cityscapes dataset and notably outperforms state-of-the-art methods on both Dark Zurich and Nighttime Driving datasets.},
  archive   = {C_ICRA},
  author    = {Hongmin Mu and Gang Zhang and MengChu Zhou and Zhengcai Cao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611148},
  month     = {5},
  pages     = {7725-7731},
  title     = {End-to-end semantic segmentation network for low-light scenes},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Few-shot panoptic segmentation with foundation models.
<em>ICRA</em>, 7718–7724. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current state-of-the-art methods for panoptic segmentation require an immense amount of annotated training data that is both arduous and expensive to obtain posing a significant challenge for their widespread adoption. Concurrently, recent breakthroughs in visual representation learning have sparked a paradigm shift leading to the advent of large foundation models that can be trained with completely unlabeled images. In this work, we propose to leverage such task-agnostic image features to enable few-shot panoptic segmentation by presenting Segmenting Panoptic Information with Nearly 0 labels (SPINO). In detail, our method combines a DINOv2 backbone with lightweight network heads for semantic segmentation and boundary estimation. We show that our approach, albeit being trained with only ten annotated images, predicts high-quality pseudo-labels that can be used with any existing panoptic segmentation method. Notably, we demonstrate that SPINO achieves competitive results compared to fully supervised baselines while using less than 0.3% of the ground truth labels, paving the way for learning complex visual recognition tasks leveraging foundation models. To illustrate its general applicability, we further deploy SPINO on real-world robotic vision systems for both outdoor and indoor environments. To foster future research, we make the code and trained models publicly available at http://spino.cs.uni-freiburg.de.},
  archive   = {C_ICRA},
  author    = {Markus Käppeler and Kürsat Petek and Niclas Vödisch and Wolfram Burgard and Abhinav Valada},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611624},
  month     = {5},
  pages     = {7718-7724},
  title     = {Few-shot panoptic segmentation with foundation models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving radial imbalances with hybrid voxelization and
RadialMix for LiDAR 3D semantic segmentation. <em>ICRA</em>, 7710–7717.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Huge progress has been made in LiDAR 3D semantic segmentation, but there are two under-explored imbalances on the radial axis: points are unevenly concentrated on the near side, and the distribution of foreground object instances is skewed to the near side. This leads the training of the model to favor semantics at the near side with the majority of points and object instances. Both the cylindrical and the spherical voxelizations aim to address the problem of imbalanced point distribution by increasing the volume of voxels along the radial distance to include fewer near-side points in a smaller voxel and more far-side points in a bigger voxel. However, this causes a problem of the receptive field enlarging along the radial distance, which is not desirable in LiDAR 3D segmentation. This can be addressed in cubic voxelization which has a fixed volume of voxels. Thus, we propose a new LiDAR 3D semantic segmentation network (Hi-VoxelNet) with Hybrid Voxelization that leverages the advantages of cubic, cylindrical, and spherical voxelizations for hybrid voxel feature learning. To address the radial imbalance of object instances, we propose a novel data augmentation technique termed as RadialMix that uses radial sample duplication to increase the number of distant foreground object instances and mixes the radial duplication with another point cloud for enriching the training samples. With the joint improvements of the radial imbalances, our method archives state-of-the-art performance on nuScenes and SemanticKITTI datasets, and it shows significant improvements along the radial axis. Our code is publicly available at https://github.com/jialeli1/lidarseg3d.},
  archive   = {C_ICRA},
  author    = {Jiale Li and Hang Dai and Yu Wang and Guangzhi Cao and Chun Luo and Yong Ding},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610604},
  month     = {5},
  pages     = {7710-7717},
  title     = {Improving radial imbalances with hybrid voxelization and RadialMix for LiDAR 3D semantic segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LLM-grounder: Open-vocabulary 3D visual grounding with large
language model as an agent. <em>ICRA</em>, 7694–7701. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D visual grounding is a critical skill for household robots, enabling them to navigate, manipulate objects, and answer questions based on their environment. While existing approaches often rely on extensive labeled data or exhibit limitations in handling complex language queries, we propose LLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model (LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to decompose complex natural language queries into semantic constituents and employs a visual grounding tool, such as OpenScene or LERF, to identify objects in a 3D scene. The LLM then evaluates the spatial and commonsense relations among the proposed objects to make a final grounding decision. Our method does not require any labeled training data and can generalize to novel 3D scenes and arbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and demonstrate state-of-the-art zero-shot grounding accuracy. Our findings indicate that LLMs significantly improve the grounding capability, especially for complex language queries, making LLM-Grounder an effective approach for 3D vision-language tasks in robotics.},
  archive   = {C_ICRA},
  author    = {Jianing Yang and Xuweiyi Chen and Shengyi Qian and Nikhil Madaan and Madhavan Iyengar and David F. Fouhey and Joyce Chai},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610443},
  month     = {5},
  pages     = {7694-7701},
  title     = {LLM-grounder: Open-vocabulary 3D visual grounding with large language model as an agent},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mapping high-level semantic regions in indoor environments
without object recognition. <em>ICRA</em>, 7686–7693. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots require a semantic understanding of their surroundings to operate in an efficient and explainable way in human environments. In the literature, there has been an extensive focus on object labeling and exhaustive scene graph generation; less effort has been focused on the task of purely identifying and mapping large semantic regions. The present work proposes a method for semantic region mapping via embodied navigation in indoor environments, generating a high-level representation of the knowledge of the agent. To enable region identification, the method uses a vision-to-language model to provide scene information for mapping. By projecting egocentric scene understanding into the global frame, the proposed method generates a semantic map as a distribution over possible region labels at each location. This mapping procedure is paired with a trained navigation policy to enable autonomous map generation. The proposed method significantly outperforms a variety of baselines, including an object-based system and a pretrained scene classifier, in experiments in a photorealistic simulator.},
  archive   = {C_ICRA},
  author    = {Roberto Bigazzi and Lorenzo Baraldi and Shreyas Kousik and Rita Cucchiara and Marco Pavone},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610897},
  month     = {5},
  pages     = {7686-7693},
  title     = {Mapping high-level semantic regions in indoor environments without object recognition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Perception through cognitive emulation: “A second iteration
of NaivPhys4RP for learningless and safe recognition and 6D-pose
estimation of (transparent) objects”. <em>ICRA</em>, 7679–7685. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In our previous work, we designed a human-like white-box and causal generative model of perception NaivPhys4RP, essentially based on cognitive emulation to understand the past, the present and the future of the state of complex worlds from poor observations. In this paper, as recommended in that previous work, we first refine the theoretical model of NaivPhys4RP in terms of integration of variables as well as perceptual inference tasks to solve. Intuitively, the system is closed under the injection, update and dependency of variables. Then, we present a first implementation of NaivPhys4RP that demonstrates the learningless and safe recognition and 6D-Pose estimation of objects from poor sensor data (e.g., occlusion, transparency, poor-depth, in-hand). This does not only make a substantial step forward comparatively to classical perception systems in perceiving objects in these scenarios, but escape the burden of data-intensive learning and operate safely (transparency and causality — we fit sensor data into mentally constructed meaningful worlds). With respect to ChatGPT’s ambitions, it can imagine physico-realistic socio-physical scenes from texts, demonstrate understanding of these texts, and all these with no data- and resource-intensive learning.},
  archive   = {C_ICRA},
  author    = {Franklin Kenghagho K. and Michael Neumann and Patrick Mania and Michael Beetz},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610037},
  month     = {5},
  pages     = {7679-7685},
  title     = {Perception through cognitive emulation: &quot;A second iteration of NaivPhys4RP for learningless and safe recognition and 6D-pose estimation of (Transparent) objects&quot;},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pulsating fluidic sensor for sensing of location, pressure
and contact area. <em>ICRA</em>, 7672–7678. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Designing information-rich and space-efficient sensors is a key challenge for soft robotics, and crucial for the development of safe soft robots. Sensing and understanding the environmental interactions with a minimal footprint is especially important in the medical context, where portability and unhindered patient/user movement is a priority, to move towards personalized and decentralized healthcare solutions. In this work, a pulsating fluidic soft sensor (PFS) capable of determining location, pressure and contact area of press events is shown. The sensor relies on spatio-temporal resistance changes driven by a pulsating conductive fluid. The sensor demonstrates good repeatability and distinction of single and multiple press events, detecting single indents of sizes greater than 1 cm, forces larger than 2 N, and various locations across the sensor, as well as multiple indents spaced 2 cm apart. Furthermore, the sensor is demonstrated in two applications to detect foot placement and grip location. Overall, the sensor represents an improvement towards minimizing electronic hardware, and cost of the sensing solution, without sacrificing the richness of the sensing information in the field of soft fluidic sensors.},
  archive   = {C_ICRA},
  author    = {Joanna Jones and Marco Pontin and Dana D. Damian},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610816},
  month     = {5},
  pages     = {7672-7678},
  title     = {Pulsating fluidic sensor for sensing of location, pressure and contact area},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Thermally-activated biochemically-sustained reactor for soft
fluidic actuation. <em>ICRA</em>, 7665–7671. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft robots have shown remarkable distinct capabilities due to their high deformation. Recently increasing attention has been dedicated to developing fully soft robots to exploit their full potential, with a recognition that electronic powering may limit this achievement. Alternative powering sources compatible with soft robots have been identified such as combustion and chemical reactions. A further milestone to such systems would be to increase the controllability and responsiveness of their underlying reactions in order to achieve more complex behaviors for soft robots. In this paper, we present a thermally-activated reactor incorporating a biocompatible hydrogel valve that enables control of the biochemical reaction of sugar and yeast. The biochemical reaction is utilized to generate contained pressure, which in turn powers a fluidic soft actuator. Experiments were conducted to evaluate the response time of the hydrogel valves with three different crosslinker concentrations. Among the tested concentrations, we found that the lowest crosslinker concentration yielded the fastest response time of the valve at an ambient temperature of 50°C. We also evaluated the pressure generation capacity of the reactor, which can reach up to 0.22 bar, and demonstrated the thermoresponsive behavior of the reactor to trigger a biochemical reaction for powering a fluidic soft actuator. This work opens up the possibility to power and control tetherless and fully soft robots.},
  archive   = {C_ICRA},
  author    = {Jialun Liu and MennaAllah Soliman and Dana D. Damian},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610060},
  month     = {5},
  pages     = {7665-7671},
  title     = {Thermally-activated biochemically-sustained reactor for soft fluidic actuation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vision-based tip force estimation on a soft continuum robot.
<em>ICRA</em>, 7621–7627. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft continuum robots, fabricated from elastomeric materials, offer unparalleled flexibility and adaptability, making them ideal for applications such as minimally invasive surgery and inspections in constrained environments. With the miniaturization of imaging technologies and the development of novel control algorithms, these devices provide exceptional opportunities to visualize the internal structures of the human body. However, there are still challenges in accurately estimating external forces applied to these systems using current technologies. Adding additional sensors is challenging without compromising the softness of the device. This work presents a visual deformation-based force sensing framework for soft continuum robots. The core idea behind this work is that point loads lead to unique deformation profiles in an actuated soft-bodied robot. We introduce a Convolutional Neural Network-based tip force estimation method that utilizes arbitrarily placed camera images and actuation inputs to predict applied tip forces. Experimental validation was performed using the STIFF-FLOP robot, a pneumatically actuated soft robot developed for minimally invasive surgery. Our vision-based force estimation model demonstrated a sensing precision of 0.05 N in the XY plane during testing, with data collection and training taking only 70 minutes.},
  archive   = {C_ICRA},
  author    = {Xingyu Chen and Jialei Shi and Helge Wurdemann and Thomas George Thuruthel},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611353},
  month     = {5},
  pages     = {7621-7627},
  title     = {Vision-based tip force estimation on a soft continuum robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning motion reconstruction from demonstration via
multi-modal soft tactile sensing. <em>ICRA</em>, 7598–7604. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning manipulation from demonstration is a key way for humans to teach complex tasks. However, this domain mainly focuses on kinetic teaching, and does not consider imitation of interaction forces which is essential for more contact rich tasks. We propose a framework that enables robotic imitation of contact from human demonstration using a wearable finger-tip sensor. By developing a multi-modal sensor (providing both force and contact location) and robotic collection of simple training data of different motion primitives (tapping, rotation and translation), an LSTM-based model can be used to replicate motion from tactile demonstration only. To evaluate this approach, we explore the performance on increasingly complex testing data generated by a robot, and also demonstrate the full pipeline from human demonstration via the sensor used as a wearable device. This approach of using tactile sensing as a means of inferring the required robot motion paves the way for imitation of more contact-rich tasks, and enables imitation of tasks where the demonstration and imitation is performed with different body-schema.},
  archive   = {C_ICRA},
  author    = {Cheng Pan and Kieran Gilday and Emily Sologuren and Kai Junge and Josie Hughes},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610135},
  month     = {5},
  pages     = {7598-7604},
  title     = {Learning motion reconstruction from demonstration via multi-modal soft tactile sensing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). History-aware planning for risk-free autonomous navigation
on unknown uneven terrain. <em>ICRA</em>, 7583–7589. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It is challenging for the mobile robot to achieve autonomous and mapless navigation in the unknown environment with uneven terrain. In this study, we present a layered and systematic pipeline. At the local level, we maintain a tree structure that is dynamically extended with the navigation. This structure unifies the planning with the terrain identification. Besides, it contributes to explicitly identifying the hazardous areas on uneven terrain. In particular, certain nodes of the tree are consistently kept to form a sparse graph at the global level, which records the history of the exploration. A series of subgoals that can be obtained in the tree and the graph are utilized for leading the navigation. To determine a subgoal, we develop an evaluation method whose input elements can be efficiently obtained on the layered structure. We conduct both simulation and real-world experiments to evaluate the developed method and its key modules. The experimental results demonstrate the effectiveness and efficiency of our method. The robot can travel through the unknown uneven region safely and reach the target rapidly without a preconstructed map.},
  archive   = {C_ICRA},
  author    = {Yinchuan Wang and Nianfei Du and Yongsen Qin and Xiang Zhang and Rui Song and Chaoqun Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610488},
  month     = {5},
  pages     = {7583-7589},
  title     = {History-aware planning for risk-free autonomous navigation on unknown uneven terrain},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning terminal state of the trajectory planner:
Application for collision scenarios of autonomous vehicles.
<em>ICRA</em>, 7576–7582. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collision Avoidance/Mitigation System (CAMS) for autonomous vehicles is a crucial technology that ensures the safety and reliability of autonomous driving systems. Conventional collision avoidance approaches struggle in complex and various scenarios by avoiding collisions based on rules for specific collision scenarios. This has led to learning-based methods using neural networks for adaptive collision avoidance. However, the approaches directly outputting control inputs through neural networks have drawbacks in interpretability and stability. To address these limitations, we propose a trajectory planning method for CAMS that combines deep reinforcement learning (DRL) and quintic polynomial (QP) trajectory planning. The proposed method determines the terminal state and confidence of the trajectory using DRL and plans a QP trajectory based on them. By utilizing the terminal state and confidence of the trajectory rather than direct control inputs as the output of the neural network, it generates a more realistic and continuous path. Moreover, this approach considers collision avoidance and mitigation in an integrated manner through the reward function of RL. Our experimental results demonstrate that the proposed method not only improves interpretability and stability compared to existing learning-based methods but also upholds performance in complex and various collision scenarios.},
  archive   = {C_ICRA},
  author    = {Joonhee Lim and Kibeom Lee and Jangho Shin and Dongsuk Kum},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611155},
  month     = {5},
  pages     = {7576-7582},
  title     = {Learning terminal state of the trajectory planner: Application for collision scenarios of autonomous vehicles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learn to navigate in dynamic environments with normalized
LiDAR scans. <em>ICRA</em>, 7568–7575. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The latest robot navigation methods for dynamic environments assume that the states of obstacles, including their geometries and trajectories, are fully observable. While it’s easy to obtain these states accurately in simulations, it’s exceedingly challenging in the real world. Therefore, a viable alternative is to directly map raw sensor observations into robot actions. However, acquiring skills from high-dimensional raw observations demands massive neural networks and extended training periods. Furthermore, there are discrepancies between simulated and real environments that impede real-world implementations. To overcome these limitations, we propose a Learning framework for robot Navigation in Dynamic environments that uses sequential Normalized LiDAR (LNDNL) scans. We employ long-short-term memory (LSTM) to propagate historical environmental information from the sequential LiDAR observations. Additionally, we customize a LiDAR-integrated simulator to speed up sampling and normalize the geometry of real-world obstacles to match that of simulated objects, thereby bridging the sim-to-real gap. Our extensive comparisons with state-of-the-art baselines and real-world implementations demonstrate the potentials of learning to navigate in dynamic environments using raw sensor observations and sim-to-real transfer.},
  archive   = {C_ICRA},
  author    = {Wei Zhu and Mitsuhiro Hayashibe},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611247},
  month     = {5},
  pages     = {7568-7575},
  title     = {Learn to navigate in dynamic environments with normalized LiDAR scans},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GPU-accelerated optimization-based collision avoidance.
<em>ICRA</em>, 7561–7567. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a GPU-accelerated optimization framework for collision avoidance problems where the controlled objects and the obstacles can be modeled as the finite union of convex polyhedra. A novel collision avoidance constraint is proposed based on scale-based collision detection and the strong duality of convex optimization. Under this constraint, the high-dimensional non-convex optimization problems of collision avoidance can be decomposed into several low-dimensional quadratic programmings (QPs) following the paradigm of alternating direction method of multipliers (ADMM). Furthermore, these low-dimensional QPs can be solved parallel with GPUs, significantly reducing computational time. High-fidelity simulations are conducted to validate the proposed method’s effectiveness and practicality.},
  archive   = {C_ICRA},
  author    = {Zeming Wu and Zhuping Wang and Hao Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610871},
  month     = {5},
  pages     = {7561-7567},
  title     = {GPU-accelerated optimization-based collision avoidance},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Probabilistic motion planning and prediction via partitioned
scenario replay. <em>ICRA</em>, 7546–7552. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous mobile robots require predictions of human motion to plan a safe trajectory that avoids them. Because human motion cannot be predicted exactly, future trajectories are typically inferred from real-world data via learning-based approximations. These approximations provide useful information on the pedestrian’s behavior, but may deviate from the data, which can lead to collisions during planning. In this work, we introduce a joint prediction and planning framework, Partitioned Scenario Replay (PSR), that stores and partitions previously observed human trajectories, referred to as scenarios. During planning, scenarios observed in similar situations are reintroduced (or replayed) as motion predictions. By sampling real data and by building on scenario optimization and predictive control, the planner provides probabilistic collision avoidance guarantees in the real-world. Relying on this guarantee to remain safe, PSR can incrementally improve its prediction and planning performance online. We demonstrate our approach on a mobile robot navigating around pedestrians.},
  archive   = {C_ICRA},
  author    = {Oscar de Groot and Anish Sridharan and Javier Alonso-Mora and Laura Ferranti},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611014},
  month     = {5},
  pages     = {7546-7552},
  title     = {Probabilistic motion planning and prediction via partitioned scenario replay},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MoDem-v2: Visuo-motor world models for real-world robot
manipulation. <em>ICRA</em>, 7530–7537. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic systems that aspire to operate in uninstrumented real-world environments must perceive the world directly via onboard sensing. Vision-based learning systems aim to eliminate the need for environment instrumentation by building an implicit understanding of the world based on raw pixels, but navigating the contact-rich high-dimensional search space from solely sparse visual reward signals significantly exacerbates the challenge of exploration. The applicability of such systems is thus typically restricted to simulated or heavily engineered environments since agent exploration in the real-world without the guidance of explicit state estimation and dense rewards can lead to unsafe behavior and safety faults that are catastrophic. In this study, we isolate the root causes behind these limitations to develop a system, called MoDem-V2, capable of learning contact-rich manipulation directly in the uninstrumented real world. Building on the latest algorithmic advancements in model-based reinforcement learning (MBRL), demo-bootstrapping, and effective exploration, MoDem-V2 can acquire contact-rich dexterous manipulation skills directly in the real world. We identify key ingredients for leveraging demonstrations in model learning while respecting real-world safety considerations – exploration centering, agency handover, and actor-critic ensembles. We empirically demonstrate the contribution of these ingredients in four complex visuo-motor manipulation problems in both simulation and the real world. To the best of our knowledge, our work presents the first successful system for demonstration-augmented visual MBRL trained directly in the real world. Visit sites.google.com/view/modem-v2 for videos and more details.},
  archive   = {C_ICRA},
  author    = {Patrick Lancaster and Nicklas Hansen and Aravind Rajeswaran and Vikash Kumar},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611121},
  month     = {5},
  pages     = {7530-7537},
  title     = {MoDem-v2: Visuo-motor world models for real-world robot manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Composable interaction primitives: A structured policy class
for efficiently learning sustained-contact manipulation skills.
<em>ICRA</em>, 7522–7529. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a new policy class, Composable Interaction Primitives (CIPs), specialized for learning sustained-contact manipulation skills like opening a drawer, pulling a lever, turning a wheel, or shifting gears. CIPs have two primary design goals: to minimize what must be learned by exploiting structure present in the world and the robot, and to support sequential composition by construction, so that learned skills can be used by a task-level planner. Using an ablation experiment in four simulated manipulation tasks, we show that the structure included in CIPs substantially improves the efficiency of motor skill learning. We then show that CIPs can be used for plan execution in a zero-shot fashion by sequencing learned skills. We validate our approach on real robot hardware by learning and sequencing two manipulation skills.},
  archive   = {C_ICRA},
  author    = {Ben Abbatematteo and Eric Rosen and Skye Thompson and Tuluhan Akbulut and Sreehari Rammohan and George Konidaris},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610846},
  month     = {5},
  pages     = {7522-7529},
  title     = {Composable interaction primitives: A structured policy class for efficiently learning sustained-contact manipulation skills},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Grasp anything: Combining teacher-augmented policy gradient
learning with instance segmentation to grasp arbitrary objects.
<em>ICRA</em>, 7515–7521. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Interactive grasping from clutter, akin to human dexterity, is one of the longest-standing problems in robot learning. Challenges stem from the intricacies of visual perception, the demand for precise motor skills, and the complex interplay between the two. In this work, we present Teacher-Augmented Policy Gradient (TAPG), a novel two-stage learning framework that synergizes reinforcement learning and policy distillation. After training a teacher policy to master the motor control based on object pose information, TAPG facilitates guided, yet adaptive, learning of a sensorimotor policy, based on object segmentation. We zero-shot transfer from simulation to a real robot by using Segment Anything Model for promptable object segmentation. Our trained policies adeptly grasp a wide variety of objects from cluttered scenarios in simulation and the real world based on human-understandable prompts. Furthermore, we show robust zero-shot transfer to novel objects. Videos of our experiments are available at https://maltemosbach.github.io/grasp_anything.},
  archive   = {C_ICRA},
  author    = {Malte Mosbach and Sven Behnke},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610700},
  month     = {5},
  pages     = {7515-7521},
  title     = {Grasp anything: Combining teacher-augmented policy gradient learning with instance segmentation to grasp arbitrary objects},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning language-conditioned deformable object manipulation
with graph dynamics. <em>ICRA</em>, 7508–7514. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-task learning of deformable object manipulation is a challenging problem in robot manipulation. Most previous works address this problem in a goal-conditioned way and adapt goal images to specify different tasks, which limits the multi-task learning performance and can not generalize to new tasks. Thus, we adapt language instruction to specify deformable object manipulation tasks and propose a learning framework. We first design a unified Transformer-based architecture to understand multi-modal data and output picking and placing action. Besides, we have applied the visible connectivity graph to tackle nonlinear dynamics and complex configuration of the deformable object. Both simulated and real experiments have demonstrated that the proposed method is effective and can generalize to unseen instructions and tasks. Compared with the state-of-the-art method, our method achieves higher success rates (87.2% on average) and has a 75.6% shorter inference time. We also demonstrate that our method performs well in real-world experiments. Supplementary videos can be found at https://sites.google.com/view/language-deformable.},
  archive   = {C_ICRA},
  author    = {Yuhong Deng and Kai Mo and Chongkun Xia and Xueqian Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610890},
  month     = {5},
  pages     = {7508-7514},
  title     = {Learning language-conditioned deformable object manipulation with graph dynamics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Touch-based manipulation with multi-fingered robot using
off-policy RL and temporal contrastive learning. <em>ICRA</em>,
7501–7507. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tactile information holds promise for enhancing the manipulation capabilities of multi-fingered robots. In tasks such as in-hand manipulation, where robots frequently switch between contact and non-contact states, it is important to address the partial observability of tactile sensors and to properly consider the history of observations and actions. Previous studies have shown that Recurrent Neural Network (RNN) can be used to learn latent representations for handling observation and action histories. However, this approach is usually combined with on-policy reinforcement learning (RL) and suffers from low sample efficiency. Integrating RNN with off-policy RL could enhance sample efficiency, but this often compromises stability and robustness, especially as the dimensions of observation and action increase. This paper presents a time-contrastive learning approach tailored for off-policy RL. Our method incorporates a temporal contrastive model and introduces a surrogate loss to extract task-related latent representations, enhancing the pursuit of the optimal policy. Simulations and real robot experiments demonstrate that our proposed method outperforms RNN-based approaches.},
  archive   = {C_ICRA},
  author    = {Naoki Morihira and Pranav Deo and Manoj Bhadu and Akinobu Hayashi and Tadaaki Hasegawa and Satoshi Otsubo and Takayuki Osa},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610239},
  month     = {5},
  pages     = {7501-7507},
  title     = {Touch-based manipulation with multi-fingered robot using off-policy RL and temporal contrastive learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intrinsic language-guided exploration for complex
long-horizon robotic manipulation tasks. <em>ICRA</em>, 7493–7500. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current reinforcement learning algorithms struggle in sparse and complex environments, most notably in long-horizon manipulation tasks entailing a plethora of different sequences. In this work, we propose the Intrinsically Guided Exploration from Large Language Models (IGE-LLMs) framework. By leveraging LLMs as an assistive intrinsic reward, IGE-LLMs guides the exploratory process in reinforcement learning to address intricate long-horizon with sparse rewards robotic manipulation tasks. We evaluate our framework and related intrinsic learning methods in an environment challenged with exploration, and a complex robotic manipulation task challenged by both exploration and long-horizons. Results show IGE-LLMs (i) exhibit notably higher performance over related intrinsic methods and the direct use of LLMs in decision-making, (ii) can be combined and complement existing learning methods highlighting its modularity, (iii) are fairly insensitive to different intrinsic scaling parameters, and (iv) maintain robustness against increased levels of uncertainty and horizons.},
  archive   = {C_ICRA},
  author    = {Eleftherios Triantafyllidis and Filippos Christianos and Zhibin Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611483},
  month     = {5},
  pages     = {7493-7500},
  title     = {Intrinsic language-guided exploration for complex long-horizon robotic manipulation tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sample-efficient learning to solve a real-world labyrinth
game using data-augmented model-based reinforcement learning.
<em>ICRA</em>, 7455–7460. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motivated by the challenge of achieving rapid learning in physical environments, this paper presents the development and training of a robotic system designed to navigate and solve a labyrinth game using model-based reinforcement learning techniques. The method involves extracting low-dimensional observations from camera images, along with a cropped and rectified image patch centered on the current position within the labyrinth, providing valuable information about the labyrinth layout. The learning of a control policy is performed purely on the physical system using model-based reinforcement learning, where the progress along the labyrinth’s path serves as a reward signal. Additionally, we exploit the system’s inherent symmetries to augment the training data. Consequently, our approach learns to successfully solve a popular real-world labyrinth game in record time, with only 5 hours of real-world training data.},
  archive   = {C_ICRA},
  author    = {Thomas Bi and Raffaello D’Andrea},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610577},
  month     = {5},
  pages     = {7455-7460},
  title     = {Sample-efficient learning to solve a real-world labyrinth game using data-augmented model-based reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Guided online distillation: Promoting safe reinforcement
learning by offline demonstration. <em>ICRA</em>, 7447–7454. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safe Reinforcement Learning (RL) aims to find a policy that achieves high rewards while satisfying cost constraints. When learning from scratch, safe RL agents tend to be overly conservative, which impedes exploration and restrains the overall performance. In many realistic tasks, e.g. autonomous driving, large-scale expert demonstration data are available. We argue that extracting expert policy from offline data to guide online exploration is a promising solution to mitigate the conserveness issue. Large-capacity models, e.g. decision transformers (DT), have been proven to be competent in offline policy learning. However, data collected in realworld scenarios rarely contain dangerous cases (e.g., collisions), which makes it prohibitive for the policies to learn safety concepts. Besides, these bulk policy networks cannot meet the computation speed requirements at inference time on real-world tasks such as autonomous driving. To this end, we propose Guided Online Distillation (GOLD), an offline-to-online safe RL framework. GOLD distills an offline DT policy into a lightweight policy network through guided online safe RL training, which outperforms both the offline DT policy and online safe RL algorithms. Experiments in both benchmark safe RL tasks and real-world driving tasks based on the Waymo Open Motion Dataset (WOMD) [1] demonstrate that GOLD can successfully distill lightweight policies and solve decision-making problems in challenging safety-critical scenarios.},
  archive   = {C_ICRA},
  author    = {Jinning Li and Xinyi Liu and Banghua Zhu and Jiantao Jiao and Masayoshi Tomizuka and Chen Tang and Wei Zhan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611123},
  month     = {5},
  pages     = {7447-7454},
  title     = {Guided online distillation: Promoting safe reinforcement learning by offline demonstration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning dual-arm object rearrangement for cartesian robots.
<em>ICRA</em>, 7440–7446. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work focuses on the dual-arm object rearrangement problem abstracted from a realistic industrial scenario of Cartesian robots. The goal of this problem is to transfer all the objects from sources to targets with the minimum total completion time. To achieve the goal, the core idea is to develop an effective object-to-arm task assignment strategy for minimizing the cumulative task execution time and maximizing the dual-arm cooperation efficiency. One of the difficulties in the task assignment is the scalability problem. As the number of objects increases, the computation time of traditional offline-search-based methods grows strongly for computational complexity. Encouraged by the adaptability of reinforcement learning (RL) in long-sequence task decisions, we propose an online task assignment decision method based on RL, and the computation time of our method only increases linearly with the number of objects. Further, we design an attention-based network to model the dependencies between the input states during the whole task execution process to help find the most reasonable object-to-arm correspondence in each task assignment round. In the experimental part, we adapt some search-based methods to this specific setting and compare our method with them. Experimental result shows that our approach achieves outperformance over search-based methods in total execution time and computational efficiency, and also verifies the generalization of our method to different numbers of objects. In addition, we show the effectiveness of our method deployed on the real robot in the supplementary video.},
  archive   = {C_ICRA},
  author    = {Shishun Zhang and Qijin She and Wenhao Li and Chenyang Zhu and Yongjun Wang and Ruizhen Hu and Kai Xu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610573},
  month     = {5},
  pages     = {7440-7446},
  title     = {Learning dual-arm object rearrangement for cartesian robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Symmetry considerations for learning task symmetric robot
policies. <em>ICRA</em>, 7433–7439. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Symmetry is a fundamental aspect of many real-world robotic tasks. However, current deep reinforcement learning (DRL) approaches can seldom harness and exploit symmetry effectively. Often, the learned behaviors fail to achieve the desired transformation invariances and suffer from motion artifacts. For instance, a quadruped may exhibit different gaits when commanded to move forward or backward, even though it is symmetrical about its torso. This issue becomes further pronounced in high-dimensional or complex environments, where DRL methods are prone to local optima and fail to explore regions of the state space equally. Past methods on encouraging symmetry for robotic tasks have studied this topic mainly in a single-task setting, where symmetry usually refers to symmetry in the motion, such as the gait patterns. In this paper, we revisit this topic for goal-conditioned tasks in robotics, where symmetry lies mainly in task execution and not necessarily in the learned motions themselves. In particular, we investigate two approaches to incorporate symmetry invariance into DRL -– data augmentation and mirror loss function. We provide a theoretical foundation for using augmented samples in an on-policy setting. Based on this, we show that the corresponding approach achieves faster convergence and improves the learned behaviors in various challenging robotic tasks, from climbing boxes with a quadruped to dexterous manipulation.},
  archive   = {C_ICRA},
  author    = {Mayank Mittal and Nikita Rudin and Victor Klemm and Arthur Allshire and Marco Hutter},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611493},
  month     = {5},
  pages     = {7433-7439},
  title     = {Symmetry considerations for learning task symmetric robot policies},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Projection-based fast and safe policy optimization for
reinforcement learning. <em>ICRA</em>, 7426–7432. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While reinforcement learning (RL) attracts increasing research attention, maximizing the return while keeping the agent safe at the same time remains an open problem. Motivated to address this challenge, this work proposes a new Fast and Safe Policy Optimization (FSPO) algorithm, which consists of three steps: the first step involves reward improvement update, the second step projects the policy to the neighborhood of the baseline policy to accelerate the optimization process, and the third step addresses the constraint violation by projecting the policy back onto the constraint set. Such a projection-based optimization can improve the convergence and learning performance. Unlike many existing works that require convex approximations for the objectives and constraints, this work exploits a first-order method to avoid expensive computations and high dimensional issues, enabling fast and safe policy optimization, especially for challenging tasks. Numerical simulation and physical experiments demonstrate that FSPO outperforms existing methods in terms of safety guarantees and task completion rate.},
  archive   = {C_ICRA},
  author    = {Shijun Lin and Hao Wang and Ziyang Chen and Zhen Kan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611554},
  month     = {5},
  pages     = {7426-7432},
  title     = {Projection-based fast and safe policy optimization for reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human-aligned longitudinal control for occluded pedestrian
crossing with visual attention. <em>ICRA</em>, 7419–7425. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement Learning (RL) has been widely used to create generalizable autonomous vehicles. However, they rely on fixed reward functions that struggle to balance values like safety and efficiency. How can autonomous vehicles balance different driving objectives and human values in a constantly changing environment? To bridge this gap, we propose an adaptive reward function that utilizes visual attention maps to detect pedestrians in the driving scene and dynamically switch between prioritizing safety or efficiency depending on the current observation. The visual attention map is used to provide spatial attention to the RL agent to boost the training efficiency of the pipeline. We evaluate the pipeline against variants of an occluded pedestrian crossing scenario in the CARLA Urban Driving simulator. Specifically, the proposed pipeline is compared against a modular setup that combines the well-established object detection model, YOLO, with a Proximal Policy Optimization (PPO) agent. The results indicate that the proposed approach can compete with the modular setup while yielding greater training efficiency. The trajectories collected with the approach confirm the effectiveness of the proposed adaptive reward function.},
  archive   = {C_ICRA},
  author    = {Vinal Asodia and Zhenhua Feng and Saber Fallah},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611046},
  month     = {5},
  pages     = {7419-7425},
  title     = {Human-aligned longitudinal control for occluded pedestrian crossing with visual attention},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving the generalization of unseen crowd behaviors for
reinforcement learning based local motion planners. <em>ICRA</em>,
7412–7418. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deploying a safe mobile robot policy in scenarios with human pedestrians is challenging due to their unpredictable movements. Current Reinforcement Learningbased motion planners rely on a single policy to simulate pedestrian movements and could suffer from the over-fitting issue. Alternatively, framing the collision avoidance problem as a multi-agent framework, where agents generate dynamic movements while learning to reach their goals, can lead to conflicts with human pedestrians due to their homogeneity.To tackle this problem, we introduce an efficient method that enhances agent diversity within a single policy by maximizing an information-theoretic objective. This diversity enriches each agent’s experiences, improving its adaptability to unseen crowd behaviors. In assessing an agent’s robustness against unseen crowds, we propose diverse scenarios inspired by pedestrian crowd behaviors. Our behavior-conditioned policies outperform existing works in these challenging scenes, reducing potential collisions without additional time or travel.},
  archive   = {C_ICRA},
  author    = {Wen Zheng Terence Ng and Jianda Chen and Sinno Jialin Pan and Tianwei Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610641},
  month     = {5},
  pages     = {7412-7418},
  title     = {Improving the generalization of unseen crowd behaviors for reinforcement learning based local motion planners},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human-robot gym: Benchmarking reinforcement learning in
human-robot collaboration. <em>ICRA</em>, 7405–7411. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep reinforcement learning (RL) has shown promising results in robot motion planning with first attempts in human-robot collaboration (HRC). However, a fair comparison of RL approaches in HRC under the constraint of guaranteed safety is yet to be made. We, therefore, present human-robot gym, a benchmark suite for safe RL in HRC. We provide challenging, realistic HRC tasks in a modular simulation framework. Most importantly, human-robot gym is the first benchmark suite that includes a safety shield to provably guarantee human safety. This bridges a critical gap between theoretic RL research and its real-world deployment. Our evaluation of six tasks led to three key results: (a) the diverse nature of the tasks offered by human-robot gym creates a challenging benchmark for state-of-the-art RL methods, (b) by leveraging expert knowledge in form of an action imitation reward, the RL agent can outperform the expert, and (c) our agents negligibly overfit to training data.},
  archive   = {C_ICRA},
  author    = {Jakob Thumm and Felix Trost and Matthias Althoff},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610705},
  month     = {5},
  pages     = {7405-7411},
  title     = {Human-robot gym: Benchmarking reinforcement learning in human-robot collaboration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). DroneMOT: Drone-based multi-object tracking considering
detection difficulties and simultaneous moving of drones and objects.
<em>ICRA</em>, 7397–7404. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-object tracking (MOT) on static platforms, such as by surveillance cameras, has achieved significant progress, with various paradigms providing attractive performances. However, the effectiveness of traditional MOT methods is significantly reduced when it comes to dynamic platforms like drones. This decrease is attributed to the distinctive challenges in the MOT-on-drone scenario: (1) objects are generally small in the image plane, blurred, and frequently occluded, making them challenging to detect and recognize; (2) drones move and see objects from different angles, causing the unreliability of the predicted positions and feature embeddings of the objects. This paper proposes DroneMOT, which firstly proposes a Dual-domain Integrated Attention (DIA) module that considers the fast movements of drones to enhance the drone-based object detection and feature embedding for small-sized, blurred, and occluded objects. Then, an innovative Motion-Driven Association (MDA) scheme is introduced, considering the concurrent movements of both the drone and the objects. Within MDA, an Adaptive Feature Synchronization (AFS) technique is presented to update the object features seen from different angles. Additionally, a Dual Motion-based Prediction (DMP) method is employed to forecast the object positions. Finally, both the refined feature embeddings and the predicted positions are integrated to enhance the object association. Comprehensive evaluations on VisDrone2019-MOT and UAVDT datasets show that DroneMOT provides substantial performance improvements over the state-of-the-art in the domain of MOT on drones. The code will be available at https://github.com/PenK1nG/DroneMOT.},
  archive   = {C_ICRA},
  author    = {Peng Wang and Yongcai Wang and Deying Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610941},
  month     = {5},
  pages     = {7397-7404},
  title     = {DroneMOT: Drone-based multi-object tracking considering detection difficulties and simultaneous moving of drones and objects},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unifying foundation models with quadrotor control for visual
tracking beyond object categories. <em>ICRA</em>, 7389–7396. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual control enables quadrotors to adaptively navigate using real-time sensory data, bridging perception with action. Yet, challenges persist, including generalization across scenarios, maintaining reliability, and ensuring real-time responsiveness. This paper introduces a perception framework grounded in foundation models for universal object detection and tracking, moving beyond specific training categories. Integral to our approach is a multi-layered tracker integrated with the foundation detector, ensuring continuous target visibility, even when faced with motion blur, abrupt light shifts, and occlusions. Complementing this, we introduce a model-free controller tailored for resilient quadrotor visual tracking. Our system operates efficiently on limited hardware, relying solely on an onboard camera and an inertial measurement unit. Through extensive validation in diverse challenging indoor and outdoor environments, we demonstrate our system’s effectiveness and adaptability. In conclusion, our research represents a step forward in quadrotor visual tracking, moving from task-specific methods to more versatile and adaptable operations.},
  archive   = {C_ICRA},
  author    = {Alessandro Saviolo and Pratyaksh Rao and Vivek Radhakrishnan and Jiuhong Xiao and Giuseppe Loianno},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610111},
  month     = {5},
  pages     = {7389-7396},
  title     = {Unifying foundation models with quadrotor control for visual tracking beyond object categories},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DCPT: Darkness clue-prompted tracking in nighttime UAVs.
<em>ICRA</em>, 7381–7388. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing nighttime unmanned aerial vehicle (UAV) trackers follow an &quot;Enhance-then-Track&quot; architecture - first using a light enhancer to brighten the nighttime video, then employing a daytime tracker to locate the object. This separate enhancement and tracking fails to build an end-to-end trainable vision system. To address this, we propose a novel architecture called Darkness Clue-Prompted Tracking (DCPT) that achieves robust UAV tracking at night by efficiently learning to generate darkness clue prompts. Without a separate enhancer, DCPT directly encodes anti-dark capabilities into prompts using a darkness clue prompter (DCP). Specifically, DCP iteratively learns emphasizing and undermining projections for darkness clues. It then injects these learned visual prompts into a daytime tracker with fixed parameters across transformer layers. Moreover, a gated feature aggregation mechanism enables adaptive fusion between prompts and between prompts and the base model. Extensive experiments show state-of-the-art performance for DCPT on multiple dark scenario benchmarks. The unified end-to-end learning of enhancement and tracking in DCPT enables a more trainable system. The darkness clue prompting efficiently injects anti-dark knowledge without extra modules. Code is available at https://github.com/bearyi26/DCPT.},
  archive   = {C_ICRA},
  author    = {Jiawen Zhu and Huayi Tang and Zhi-Qi Cheng and Jun-Yan He and Bin Luo and Shihao Qiu and Shengming Li and Huchuan Lu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610544},
  month     = {5},
  pages     = {7381-7388},
  title     = {DCPT: Darkness clue-prompted tracking in nighttime UAVs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Compositional servoing by recombining demonstrations.
<em>ICRA</em>, 7339–7346. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning-based manipulation policies from image inputs often show weak task transfer capabilities. In contrast, visual servoing methods allow efficient task transfer in high-precision scenarios while requiring only a few demonstrations. In this work, we present a framework that formulates the visual servoing task as graph traversal. Our method not only extends the robustness of visual servoing, but also enables multitask capability based on a few task-specific demonstrations. We construct demonstration graphs by splitting existing demonstrations and recombining them. In order to traverse the demonstration graph in the inference case, we utilize a similarity function that helps select the best demonstration for a specific task. This enables us to compute the shortest path through the graph. Ultimately, we show that recombining demonstrations leads to higher task-respective success. We present extensive simulation and real-world experimental results that demonstrate the efficacy of our approach.},
  archive   = {C_ICRA},
  author    = {Max Argus and Abhijeet Nayak and Martin Büchner and Silvio Galesso and Abhinav Valada and Thomas Brox},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609978},
  month     = {5},
  pages     = {7339-7346},
  title     = {Compositional servoing by recombining demonstrations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual feedback control of an underactuated hand for
grasping brittle and soft foods. <em>ICRA</em>, 7332–7338. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel method to control an underactuated hand by using only a monocular camera, not using any internal sensors. In food factories, robots are required to handle a wide variety of foods without damaging them. To accomplish this, the use of underactuated hands is effective because they can adapt to various food shapes. However, if internal sensors such as tactile sensors and force sensors are used in the underactuated hands, it may cause a problem with hygiene and require complicated calibration. Moreover, if external sensors such as cameras are used, it is necessary to grasp foods without damaging them by using external information such as images. In our method, to tackle these problems, a camera is used as an external sensor. First, contact between the hand and the object is detected by using the contours of both, obtained from a camera image. Then, to avoid damaging the object, the following information is extracted from camera images and observed: the centroid of both the hand and object, the deformation of the object, and the occlusion rate of the hand. Furthermore, to prevent the object from dropping while the robotic arm is in motion, the distance between the centroid of the hand and the object is calculated. The experiments were conducted using twelve different food items.},
  archive   = {C_ICRA},
  author    = {Ryogo Kai and Yuzuka Isobe and Sarthak Pathak and Kazunori Umeda},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610649},
  month     = {5},
  pages     = {7332-7338},
  title     = {Visual feedback control of an underactuated hand for grasping brittle and soft foods},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stereo image-based visual servoing towards feature-based
grasping. <em>ICRA</em>, 7325–7331. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an image-based visual servoing scheme that can control robotic manipulators in 3D space using 2D stereo images without needing to perform stereo reconstruction. We use a stereo camera in an eye-to-hand configuration for controlling the robot to reach target positions by directly mapping image space errors to joint space actuation. We achieve convergence without a-priori knowledge of the target object, a reference 2D image, or 3D data. By doing so, we can reach targets in unstructured environments using high-resolution RGB images instead of utilizing relatively noisy depth data. We conduct several experiments on two different physical robots. The Panda 7DOF arm grasps a static target in 3D space, grasps a pitcher handle, and picks and places a box by determining the approach angle using 2D image features, demonstrating that this algorithm can be used for grasping practical objects in 3D space using only 2D image features for feedback. Our second platform, the Atlas humanoid robot, reaches a target from an unknown starting configuration, demonstrating that this controller achieves convergence to a target, even with the uncertainties introduced by walking to a new location. We believe that this algorithm is a step towards enabling intuitive interfaces that allow a user to initiate a grasp on an object by specifying a grasping point in a 2D image.},
  archive   = {C_ICRA},
  author    = {Albert Enyedy and Ashay Aswale and Berk Calli and Michael Gennert},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611604},
  month     = {5},
  pages     = {7325-7331},
  title     = {Stereo image-based visual servoing towards feature-based grasping},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MAL: Motion-aware loss with temporal and distillation hints
for self-supervised depth estimation. <em>ICRA</em>, 7318–7324. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Depth perception is crucial for a wide range of robotic applications. Multi-frame self-supervised depth estimation methods have gained research interest due to their ability to leverage large-scale, unlabeled real-world data. However, the self-supervised methods often rely on the assumption of a static scene and their performance tends to degrade in dynamic environments. To address this issue, we present Motion-Aware Loss, which leverages the temporal relation among consecutive input frames and a novel distillation scheme between the teacher and student networks in the multi-frame self-supervised depth estimation methods. Specifically, we associate the spatial locations of moving objects with the temporal order of input frames to eliminate errors induced by object motion. Meanwhile, we enhance the original distillation scheme in multi-frame methods to better exploit the knowledge from a teacher network. MAL is a novel, plug-and-play module designed for seamless integration into multi-frame self-supervised monocular depth estimation methods. Adding MAL into previous state-of-the-art methods leads to a reduction in depth estimation errors by up to 4.2% and 10.8% on KITTI and CityScapes benchmarks, respectively.},
  archive   = {C_ICRA},
  author    = {Yue-Jiang Dong and Fang-Lue Zhang and Song-Hai Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610688},
  month     = {5},
  pages     = {7318-7324},
  title     = {MAL: Motion-aware loss with temporal and distillation hints for self-supervised depth estimation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Occluded part-aware graph convolutional networks for
skeleton-based action recognition. <em>ICRA</em>, 7310–7317. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recognizing human action is one of the most critical factors in the visual perception of robots. Specifically, skeletonbased action recognition has been actively researched to enhance recognition performance at a lower cost. However, action recognition in occlusion situations, where body parts are not visible, is still challenging.We propose an occluded part-aware graph convolutional network (OP-GCN) to address this challenge using the optimal occluded body parts. The proposed model uses an occluded part detector to identify occluded body parts within a human skeleton. It is based on an autoencoder trained on a nonoccluded human skeleton and exploits the symmetry and angular information of the skeleton. Then, we select an optimal group constructed considering the occluded body parts. Each group comprises five sets of joint nodes, focusing on the body parts, excluding the occluded ones. Finally, to enhance interaction within the selected groups, we apply an interpart association module, considering the fusion of global and local elements. The experimental results reveal that the proposed model outperforms others on the occluded datasets. These comparative experiments demonstrate the effectiveness of the study in addressing the challenge of action recognition in occlusion situations. Our code is publicly available at https://github.com/MJ-Kor/OP-GCN.},
  archive   = {C_ICRA},
  author    = {Min Hyuk Kim and Min Ju Kim and Seok Bong Yoo},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610972},
  month     = {5},
  pages     = {7310-7317},
  title     = {Occluded part-aware graph convolutional networks for skeleton-based action recognition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Marrying NeRF with feature matching for one-step pose
estimation. <em>ICRA</em>, 7302–7309. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Given the image collection of an object, we aim at building a real-time image-based pose estimation method, which requires neither its CAD model nor hours of object-specific training. Recent NeRF-based methods provide a promising solution by directly optimizing the pose from pixel loss between rendered and target images. However, during inference, they require long converging time, and suffer from local minima, making them impractical for real-time robot applications. We aim at solving this problem by marrying image matching with NeRF. With 2D matches and depth rendered by NeRF, we directly solve the pose in one step by building 2D-3D correspondences between target and initial view, thus allowing for real-time prediction. Moreover, to improve the accuracy of 2D-3D correspondences, we propose a 3D consistent point mining strategy, which effectively discards unfaithful points reconstruted by NeRF. Moreover, current NeRF-based methods naively optimizing pixel loss fail at occluded images. Thus, we further propose a 2D matches based sampling strategy to preclude the occluded area. Experimental results on representative datasets prove that our method outperforms state-of-the-art methods, and improves inference efficiency by 90×, achieving real-time prediction at 6 FPS.},
  archive   = {C_ICRA},
  author    = {Ronghan Chen and Yang Cong and Yu Ren},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610766},
  month     = {5},
  pages     = {7302-7309},
  title     = {Marrying NeRF with feature matching for one-step pose estimation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust collaborative perception without external
localization and clock devices. <em>ICRA</em>, 7280–7286. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A consistent spatial-temporal coordination across multiple agents is fundamental for collaborative perception, which seeks to improve perception abilities through information exchange among agents. To achieve this spatial-temporal alignment, traditional methods depend on external devices to provide localization and clock signals. However, hardware-generated signals could be vulnerable to noise and potentially malicious attack, jeopardizing the precision of spatial-temporal alignment. Rather than relying on external hardwares, this work proposes a novel approach: aligning by recognizing the inherent geometric patterns within the perceptual data of various agents. Following this spirit, we propose a robust collaborative perception system that operates independently of external localization and clock devices. The key module of our system, FreeAlign, constructs a salient object graph for each agent based on its detected boxes and uses a graph neural network to identify common subgraphs between agents, leading to accurate relative pose and time. We validate FreeAlign on both real-world and simulated datasets. The results show that, the FreeAlign empowered robust collaborative perception system perform comparably to systems relying on precise localization and clock devices. ${\mathbf{Code}}$ will be released.},
  archive   = {C_ICRA},
  author    = {Zixing Lei and Zhenyang Ni and Ruize Han and Shuo Tang and Chen Feng and Siheng Chen and Yanfeng Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610635},
  month     = {5},
  pages     = {7280-7286},
  title     = {Robust collaborative perception without external localization and clock devices},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CopperTag: A real-time occlusion-resilient fiducial marker.
<em>ICRA</em>, 7273–7279. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fiducial markers, like AprilTag and ArUco, are extensively utilized in robotics applications within industrial environments, encompassing navigation, docking, and object grasping tasks. However, in contrast to controlled laboratory conditions, markers installed in factory grounds or equipment surfaces, often face challenges like damage or contamination. These issues can lead to compromised marker integrity, resulting in reduced detection reliability. To address this challenge, we propose a novel fiducial marker called CopperTag, which incorporates circular and square elements to create a robust occlusion-resistant pattern. The CopperTag detection process relies on three fundamental steps: firstly, extracting all lines from the image; secondly, identifying corners; and lastly, searching for quadrilateral candidate regions using ellipses and nearby corners. The Reed-Solomon (RS) algorithm is utilized for both encoding and decoding the information content. This algorithm possesses the ability to recover corrupted messages in situations where CopperTag data is incomplete. The experimental results illustrate that CopperTag exhibits superior robustness and accuracy in detection when compared to other state-of-the-art fiducial markers, even in scenarios with heavy occlusion. Moreover, CopperTag maintains an average processing time of 10ms per frame on a standard laptop, effectively meeting the real-time demands of robotics applications.},
  archive   = {C_ICRA},
  author    = {Xu Bian and Wenzhao Chen and Xiaoyu Tian and Donglai Ran},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611260},
  month     = {5},
  pages     = {7273-7279},
  title     = {CopperTag: A real-time occlusion-resilient fiducial marker},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Through the real world haze scenes: Navigating the
synthetic-to-real gap in challenging image dehazing. <em>ICRA</em>,
7265–7272. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dehazing real-world hazy images is challenging due to the complexity of natural haze, varying haze conditions, details preservation, and the risk of overexposure. Existing methods excel in synthetic hazy scenarios but struggle in the real world because they don’t use all available features. Classical dehazing techniques primarily focus on low-level dehazing enhancements, whereas deep learning-based methods extract more intricate weather-related features. However, both of these approaches exhibit limitations in effectively addressing the real-world dehazing. To address these challenges, we introduce an innovative approach that combines the strengths of both modalities to dehaze and enhance the visibility of real-world hazy scenes. Firstly, we extract both low-level and deep features and then employ a pre-trained vector quantization GAN to create well-detailed data patches. A decoder, with a normalized module, effectively utilizes these high-quality features. Additionally, we introduce a controllable operation to improve feature matching. To further enhance dehazing and generalizability, the decoder’s output undergoes a sequence of gamma-correction operations and generates a series of multi-exposure images that are combined to create a haze-free and higher-quality image. Our method effectively reduces haziness, enhances sharpness, preserves natural colors, and minimizes artifacts in challenging real-world scenarios. The approach surpasses five SOTA methods in both qualitative and quantitative evaluations across three key metrics, utilizing three synthetic and two real-world hazy datasets. Notably, it achieves a substantial improvement in real-world datasets over the second-best method, with 0.5702 and 0.129 in FADE metrics for the RTTS and Fattal datasets, respectively.},
  archive   = {C_ICRA},
  author    = {Shijie Chen and Mohammad Mahdizadeh and Chong Yu and Jiayuan Fan and Tao Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611709},
  month     = {5},
  pages     = {7265-7272},
  title     = {Through the real world haze scenes: Navigating the synthetic-to-real gap in challenging image dehazing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bigraph matching weighted with learnt incentive function for
multi-robot task allocation. <em>ICRA</em>, 7250–7256. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most real-world Multi-Robot Task Allocation (MRTA) problems require fast and efficient decision-making, which is often achieved using heuristics-aided methods such as genetic algorithms, auction-based methods, and bipartite graph matching methods. These methods often assume a form that lends better explainability compared to an end-to-end (learnt) neural network based policy for MRTA. However, deriving suitable heuristics can be tedious, risky and in some cases impractical if problems are too complex. This raises the question: can these heuristics be learned? To this end, this paper particularly develops a Graph Reinforcement Learning (GRL) framework to learn the heuristics or incentives for a bipartite graph matching approach to MRTA. Specifically a Capsule Attention policy model is used to learn how to weight task/robot pairings (edges) in the bipartite graph that connects the set of tasks to the set of robots. The original capsule attention network architecture is fundamentally modified by adding encoding of robots’ state graph, and two Multihead Attention based decoders whose output are used to construct a LogNormal distribution matrix from which positive bigraph weights can be drawn. The performance of this new bigraph matching approach augmented with a GRL-derived incentive is found to be at par with the original bigraph matching approach that used expert-specified heuristics, with the former offering notable robustness benefits. During training, the learned incentive policy is found to get initially closer to the expert-specified incentive and then slightly deviate from its trend.},
  archive   = {C_ICRA},
  author    = {Steve Paul and Nathan Maurer and Souma Chowdhury},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611094},
  month     = {5},
  pages     = {7250-7256},
  title     = {Bigraph matching weighted with learnt incentive function for multi-robot task allocation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal task allocation for heterogeneous multi-robot teams
with battery constraints. <em>ICRA</em>, 7243–7249. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel approach to optimal multi-robot task allocation in heterogeneous teams of robots. When robots have heterogeneous capabilities and there are diverse objectives and constraints to comply with, computing optimal plans can become especially hard. Moreover, we increase the problem complexity by: 1) considering battery-limited robots that need to schedule recharges; 2) tasks that can be decomposed into multiple fragments; and 3) multi-robot tasks that need to be executed by a coalition synchronously. We define a new problem for heterogeneous multi-robot task allocation and formulate it as a Mixed-Integer Linear Program that includes all the aforementioned features. Then we use an off-the-shelf solver to show the type of optimal solutions that our planner can produce and assess its performance in random scenarios. Our method, which is released as open-source code, represents a first step to formalize and analyze a complex problem that has not been solved in the state of the art.},
  archive   = {C_ICRA},
  author    = {Álvaro Calvo and Jesús Capitán},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611147},
  month     = {5},
  pages     = {7243-7249},
  title     = {Optimal task allocation for heterogeneous multi-robot teams with battery constraints},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Multi-robot autonomous exploration and mapping under
localization uncertainty with expectation-maximization. <em>ICRA</em>,
7236–7242. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose an autonomous exploration algorithm designed for decentralized multi-robot teams, which takes into account map and localization uncertainties of range-sensing mobile robots. Virtual landmarks are used to quantify the combined impact of process noise and sensor noise on map uncertainty. Additionally, we employ an iterative expectation-maximization inspired algorithm to assess the potential out-comes of both a local robot’s and its neighbors’ next-step actions. To evaluate the effectiveness of our framework, we conduct a comparative analysis with state-of-the-art algorithms. The results of our experiments show the proposed algorithm’s capacity to strike a balance between curbing map uncertainty and achieving efficient task allocation among robots.},
  archive   = {C_ICRA},
  author    = {Yewei Huang and Xi Lin and Brendan Englot},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611495},
  month     = {5},
  pages     = {7236-7242},
  title     = {Multi-robot autonomous exploration and mapping under localization uncertainty with expectation-maximization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decentralized multi-agent active search and tracking when
targets outnumber agents. <em>ICRA</em>, 7229–7235. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent multi-target tracking has a wide range of applications, including wildlife patrolling, security surveillance or environment monitoring. Such algorithms often make restrictive assumptions: the number of targets and/or their initial locations may be assumed known, or agents may be pre-assigned to monitor disjoint partitions of the environment, reducing the burden of exploration. This also limits applicability when there are fewer agents than targets, since agents are unable to continuously follow the targets in their fields of view. Multi-agent tracking algorithms additionally assume inter-agent synchronization of observations, or the presence of a central controller to coordinate joint actions. Instead, we focus on the setting of decentralized multi-agent, multi-target, simultaneous active search-and-tracking with asynchronous interagent communication. Our proposed algorithm DecSTER uses a sequential monte carlo implementation of the Probability Hypothesis Density filter for posterior inference combined with Thompson sampling for decentralized multi-agent decision making. We compare different action selection policies, focusing on scenarios where targets outnumber agents. In simulation, we demonstrate that DecSTER is robust to unreliable inter-agent communication and outperforms information-greedy baselines in terms of the Optimal Sub-Pattern Assignment (OSPA) metric for different numbers of targets and varying teamsizes.},
  archive   = {C_ICRA},
  author    = {Arundhati Banerjee and Jeff Schneider},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609977},
  month     = {5},
  pages     = {7229-7235},
  title     = {Decentralized multi-agent active search and tracking when targets outnumber agents},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Partial belief space planning for scaling stochastic dynamic
games. <em>ICRA</em>, 7222–7228. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a method to reduce computations for stochastic dynamic games with game-theoretic belief space planning through partially propagating beliefs. Complex interactions in scenarios such as surveillance, herding, and racing can be modeled using game-theoretic frameworks in the belief space. Stochastic dynamic games can be solved to a local Nash Equilibrium using a game-theoretic belief space variant of an iterative Linear Quadratic Gaussian (iLQG). However, the scalability of this method suffers due to the large dimensionality of beliefs which the iLQG must propagate. We examine the utility of partial belief space propagation, which allows polynomial runtime to decrease. We validate our findings through simulations and hardware implementation.},
  archive   = {C_ICRA},
  author    = {Kamran Vakil and Mela Coffey and Alyssa Pierson},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610219},
  month     = {5},
  pages     = {7222-7228},
  title     = {Partial belief space planning for scaling stochastic dynamic games},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Communicating intent as behaviour trees for decentralised
multi-robot coordination. <em>ICRA</em>, 7215–7221. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a decentralised multi-robot coordination algorithm that features a rich representation for encoding and communicating each robot’s intent. This representation for “intent messages” enables improved coordination behaviour and communication efficiency in difficult scenarios, such as those where there are unknown points of contention that require negotiation between robots. Each intent message is an adaptive policy that conditions on identified points of contention that conflict with the intentions of other robots. These policies are concisely expressed as behaviour trees via algebraic logic simplification, and are interpretable by robot teammates and human operators. We propose this intent representation in the context of the Dec-MCTS online planning algorithm for decentralised coordination. We present results for a generalised multi-robot orienteering domain that show improved plan convergence and coordination performance over standard Dec-MCTS enabled by the intent representation’s ability to encode and facilitate negotiation over points of contention.},
  archive   = {C_ICRA},
  author    = {Rhett Hull and Diluka Moratuwage and Emily Scheide and Robert Fitch and Graeme Best},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610441},
  month     = {5},
  pages     = {7215-7221},
  title     = {Communicating intent as behaviour trees for decentralised multi-robot coordination},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decentralized multi-agent trajectory planning in dynamic
environments with spatiotemporal occupancy grid maps. <em>ICRA</em>,
7208–7214. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a decentralized trajectory planning framework for the collision avoidance problem of multiple micro aerial vehicles (MAVs) in environments with static and dynamic obstacles. The framework utilizes spatiotemporal occupancy grid maps (SOGM), which forecast the occupancy status of neighboring space in the near future, as the environment representation. Based on this representation, we extend the kinodynamic A* and the corridor-constrained trajectory optimization algorithms to efficiently tackle static and dynamic obstacles with arbitrary shapes. Collision avoidance between communicating robots is integrated by sharing planned trajectories and projecting them onto the SOGM. The simulation results show that our method achieves competitive performance against state-of-the-art methods in dynamic environments with different numbers and shapes of obstacles. Finally, the proposed method is validated in real experiments.},
  archive   = {C_ICRA},
  author    = {Siyuan Wu and Gang Chen and Moji Shi and Javier Alonso-Mora},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610670},
  month     = {5},
  pages     = {7208-7214},
  title     = {Decentralized multi-agent trajectory planning in dynamic environments with spatiotemporal occupancy grid maps},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Meta-reinforcement learning based cooperative surface
inspection of 3D uncertain structures using multi-robot systems.
<em>ICRA</em>, 7201–7207. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a decentralized cooperative motion planning approach for surface inspection of 3D structures which includes uncertainties like size, number, shape, position, using multi-robot systems (MRS). Given that most of existing works mainly focus on surface inspection of single and fully known 3D structures, our motivation is two-fold: first, 3D structures separately distributed in 3D environments are complex, therefore the use of MRS intuitively can facilitate an inspection by fully taking advantage of sensors with different capabilities. Second, performing the aforementioned tasks when considering uncertainties is a complicated and time-consuming process because we need to explore, figure out the size and shape of 3D structures and then plan surface-inspection path. To overcome these challenges, we present a meta-learning approach that provides a decentralized planner for each robot to improve the exploration and surface inspection capabilities. The experimental results demonstrate our method can outperform other methods by approximately 10.5%-27% on success rate and 70%-75% on inspection speed.},
  archive   = {C_ICRA},
  author    = {Junfeng Chen and Yuan Gao and Junjie Hu and Fuqin Deng and Tin Lun Lam},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610420},
  month     = {5},
  pages     = {7201-7207},
  title     = {Meta-reinforcement learning based cooperative surface inspection of 3D uncertain structures using multi-robot systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatic configuration of multi-agent model predictive
controllers based on semantic graph world models. <em>ICRA</em>,
7194–7200. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a shared semantic map architecture to construct and configure Model Predictive Controllers (MPC) dynamically, that solve navigation problems for multiple robotic agents sharing parts of the same environment. The navigation task is represented as a sequence of semantically labeled areas in the map, that must be traversed sequentially, i.e. a route. Each semantic label represents one or more constraints on the robots’ motion behaviour in that area. The advantages of this approach are: (i) an MPC-based motion controller in each individual robot can be (re-)configured, at runtime, with the locally and temporally relevant parameters; (ii) the application can influence, also at runtime, the navigation behaviour of the robots, just by adapting the semantic labels; and (iii) the robots can reason about their need for coordination, through analyzing over which horizon in time and space their routes overlap. The paper provides simulations of various representative situations, showing that the approach of runtime configuration of the MPC drastically decreases computation time, while retaining task execution performance similar to an approach in which each robot always includes all other robots in its MPC computations.},
  archive   = {C_ICRA},
  author    = {K. de Vos and E. Torta and H. Bruyninckx and C. A. López Martínez and M. J. G. van de Molengraft},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610708},
  month     = {5},
  pages     = {7194-7200},
  title     = {Automatic configuration of multi-agent model predictive controllers based on semantic graph world models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing motion trajectory segmentation of rigid bodies
using a novel screw-based trajectory-shape representation.
<em>ICRA</em>, 7179–7185. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trajectory segmentation refers to dividing a trajectory into meaningful consecutive sub-trajectories. This paper focuses on trajectory segmentation for 3D rigid-body motions. Most segmentation approaches in the literature represent the body’s trajectory as a point trajectory, considering only its translation and neglecting its rotation. We propose a novel trajectory representation for rigid-body motions that incorporates both translation and rotation, and additionally exhibits several invariant properties. This representation consists of a geometric progress rate and a third-order trajectory-shape descriptor. Concepts from screw theory were used to make this representation time-invariant and also invariant to the choice of body reference point. This new representation is validated for a self-supervised segmentation approach, both in simulation and using real recordings of human-demonstrated pouring motions. The results show a more robust detection of consecutive sub-motions with distinct features and a more consistent segmentation compared to conventional representations. We believe that other existing segmentation methods may benefit from using this trajectory representation to improve their invariance.},
  archive   = {C_ICRA},
  author    = {Arno Verduyn and Maxim Vochten and Joris De Schutter},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610030},
  month     = {5},
  pages     = {7179-7185},
  title     = {Enhancing motion trajectory segmentation of rigid bodies using a novel screw-based trajectory-shape representation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kinematic optimization of a robotic arm for automation tasks
with human demonstration. <em>ICRA</em>, 7172–7178. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic arms are highly common in various automation processes such as manufacturing lines. However, these highly capable robots are usually degraded to simple repetitive tasks such as pick-and-place. On the other hand, designing an optimal robot for one specific task consumes large resources of engineering time and costs. In this paper, we propose a novel concept for optimizing the fitness of a robotic arm to perform a specific task based on human demonstration. Fitness of a robot arm is a measure of its ability to follow recorded human arm and hand paths. The optimization is conducted using a modified variant of the Particle Swarm Optimization for the robot design problem. In the proposed approach, we generate an optimal robot design along with the required path to complete the task. The approach could reduce the time-to-market of robotic arms and enable the standardization of modular robotic parts. Novice users could easily apply a minimal robot arm to various tasks. Two test cases of common manufacturing tasks are presented yielding optimal designs and reduced computational effort by up to 92%.},
  archive   = {C_ICRA},
  author    = {Inbar Meir and Avital Bechar and Avishai Sintov},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610924},
  month     = {5},
  pages     = {7172-7178},
  title     = {Kinematic optimization of a robotic arm for automation tasks with human demonstration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An analytic solution to the 3D CSC dubins path problem.
<em>ICRA</em>, 7157–7163. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an analytic solution to the 3D Dubins path problem for paths composed of an initial circular arc, a straight component, and a final circular arc. These are commonly called CSC paths. By modeling the start and goal configurations of the path as the base frame and final frame of an RRPRR manipulator, we treat this as an inverse kinematics problem. The kinematic features of the 3D Dubins path are built into the constraints of our manipulator model. Furthermore, we show that the number of solutions is not constant, with up to seven valid CSC path solutions even in non-singular regions. An implementation of solution is available at https: //github.com/aabecker/dubins3D.},
  archive   = {C_ICRA},
  author    = {Victor M. Baez and Nikhil Navkar and Aaron T. Becker},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611360},
  month     = {5},
  pages     = {7157-7163},
  title     = {An analytic solution to the 3D CSC dubins path problem},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Jerk-limited traversal of one-dimensional paths and its
application to multi-dimensional path tracking. <em>ICRA</em>,
7142–7148. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present an iterative method to quickly traverse multi-dimensional paths considering jerk constraints. As a first step, we analyze the traversal of each individual path dimension. We derive a range of feasible target accelerations for each intermediate waypoint of a one-dimensional path using a binary search algorithm. Computing a trajectory from waypoint to waypoint leads to the fastest progress on the path when selecting the highest feasible target acceleration. Similarly, it is possible to calculate a trajectory that leads to minimum progress along the path. This insight allows us to control the traversal of a one-dimensional path in such a way that a reference path length of a multi-dimensional path is approximately tracked over time. In order to improve the tracking accuracy, we propose an iterative scheme to adjust the temporal course of the selected reference path length. More precisely, the temporal region causing the largest position deviation is identified and updated at each iteration. In our evaluation, we thoroughly analyze the performance of our method using seven-dimensional reference paths with different path characteristics. We show that our method manages to quickly traverse the reference paths and compare the required traversing time and the resulting path accuracy with other state-of-the-art approaches.},
  archive   = {C_ICRA},
  author    = {Jonas C. Kiemel and Torsten Kröger},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611388},
  month     = {5},
  pages     = {7142-7148},
  title     = {Jerk-limited traversal of one-dimensional paths and its application to multi-dimensional path tracking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accurate kinematic modeling using autoencoders on
differentiable joints. <em>ICRA</em>, 7122–7128. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In robotics and biomechanics, accurately determining joint parameters and computing the corresponding forward and inverse kinematics are critical yet often challenging tasks, especially when dealing with highly individualized and partly unknown systems. This paper unveils a cutting-edge kinematic optimizer, underpinned by an autoencoder-based architecture, to address these challenges. Utilizing a neural network, our approach simulates inverse kinematics, converting measurement data into joint-specific parameters during encoding, enabling a stable optimization process. These parameters are subsequently processed through a predefined, differentiable forward kinematics model, resulting in a decoded representation of the original data. Beyond offering a comprehensive solution to kinematics challenges, our method also unveils previously unidentified joint parameters. Real experimental data from knee and hand joints validate the optimizer’s efficacy. Additionally, our optimizer is multifunctional: it streamlines the modeling and automation of kinematics and enables a nuanced evaluation of diverse modeling techniques. By assessing the differences in reconstruction losses, we illuminate the merits of each approach. Collectively, this preliminary study signifies advancements in kinematic optimization, with potential applications spanning both biomechanics and robotics.},
  archive   = {C_ICRA},
  author    = {Nikolas Wilhelm and Sami Haddadin and Rainer Burgkart and Patrick Van Der Smagt and Maximilian Karl},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611062},
  month     = {5},
  pages     = {7122-7128},
  title     = {Accurate kinematic modeling using autoencoders on differentiable joints},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A non-magnetic dual-mode linear pneumatic actuator: Initial
design and assessment. <em>ICRA</em>, 7107–7113. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A pneumatic linear actuator is presented and evaluated. Designed to operate in demanding environments such as MRI, it is developed to be used with two motion control modes: 1) a step-by-step mode with tooth-based gripping to ensure precision, 2) a continuous mode available locally for fine positioning. The actuator can also be disengaged to enable direct handling by an operator, for example for comanipulation. The design is presented. A prototype, developed in the medical context, is implemented and characterized. A specific step-by-step control sequence is then elaborated based on its characterization. Testing of the dual-mode actuation is finally described. The complementarity between the two motion modes and possible adaptations of the original design are discussed.},
  archive   = {C_ICRA},
  author    = {Timothée Portha and Laurent Barbé and François Geiskopf and Jonathan Vappou and Pierre Renaud},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611707},
  month     = {5},
  pages     = {7107-7113},
  title     = {A non-magnetic dual-mode linear pneumatic actuator: Initial design and assessment},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Experimental comparison of pinwheel and non-pinwheel designs
of 3D-printed cycloidal gearing for robotics. <em>ICRA</em>, 7091–7098.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent trends in robotic actuation have highlighted the need for low cost, high performance, and efficient gearing. We present an experimental study comparing pinwheel and non-pinwheel designs of cycloidal gearing. The open source designs are 3D-printable, combined with off-the-shelf components, achieving a high performance-to-cost ratio. Extensive experimental data is presented, that compares two prototypes on run-in behaviour and a number of quantitative metrics including transmission error, play, friction, and stiffness. Furthermore, we assess overall actuator performance through position control experiments, and a 10-hour endurance test. The results show strong performance characteristics, and crucially, suggest that non-pinwheel designs of cycloidal gearing can be a lower complexity and cost alternative to classical pinwheel designs, while offering similar performance.},
  archive   = {C_ICRA},
  author    = {Wesley Roozing and Glenn Roozing},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610250},
  month     = {5},
  pages     = {7091-7098},
  title     = {Experimental comparison of pinwheel and non-pinwheel designs of 3D-printed cycloidal gearing for robotics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and modeling of a compact serial variable stiffness
actuator (SVSA-III) with linear stiffness profile. <em>ICRA</em>,
7077–7082. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Variable stiffness actuator (VSA) can imitate natural muscles in their compliance capbility, which can provide flexible adaptability for robots, improving the safety of robots interacting with the environment or human. This paper presents a new compact serial variable stiffness actuator ((SVSA-III)) with linear stiffness profile based on symmetrical variable lever arm mechanism. The stiffness motor is used to regulate the position of the pivot located on the Archimedean Spiral Relocation Mechanism (ASRM), so that the stiffness of the actuator can be adjusted (softening or hardening). By designing the lever length, the range of stiffness adjustment can change from 0.3Nm/degree to therotical infinity. Moreover, the continuous linear stiffness profile of the actuator can be customized by solving the transcendental equation of the relationship between the actuator stiffness and the rotation angle of the stiffness motor. SVSA-III has the advantages of compact structure, wide-range stiffness regulation, reduced control difficulty, and linear stiffness profile. Two experiments of step response and stiffness tracking have proved the high accuracy and fast response for both theoretical stiffness and position adjustment.},
  archive   = {C_ICRA},
  author    = {Shuowen Yi and Siyu Liu and Junbei Liao and Zhao Guo},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610331},
  month     = {5},
  pages     = {7077-7082},
  title     = {Design and modeling of a compact serial variable stiffness actuator (SVSA-III) with linear stiffness profile},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel compact design of a lever-cam based variable
stiffness actuator: LC-VSA. <em>ICRA</em>, 7070–7076. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ensuring safe interaction between humans and robots is an important challenge in robotics. In recent years, researchers have developed many different soft robots. One possibility to reach this goal is to integrate mechanical springs into their joints. The forthcoming generation of soft robots will be adaptable for joint stiffness to accommodate various tasks. Consequently, the development of variable stiffness joints (VSA) has become crucial. Among the prevalent approaches for stiffness adjustment, lever mechanisms have been implemented in numerous variable stiffness joints. Nonetheless, the integration of the lever technology into VSA often faces challenges in achieving a compact design. This paper introduces a new mechanically compact design for a novel lever-cam based variable stiffness joint, which has been patent under the grand by the german Patentamt.},
  archive   = {C_ICRA},
  author    = {Hongxi Zhu and Ulrike Thomas},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610124},
  month     = {5},
  pages     = {7070-7076},
  title     = {A novel compact design of a lever-cam based variable stiffness actuator: LC-VSA},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Development of variable transmission series elastic actuator
for hip exoskeletons. <em>ICRA</em>, 7055–7061. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Series Elastic Actuator-based exoskeleton can offer precise torque control and transparency when interacting with human wearers. Accurate control of SEA-produced torques ensures the wearer’s voluntary motion and supports the implementation of multiple assistive paradigms. In this paper, a novel variable transmission series elastic actuator (VTSEA) is developed to meet torque-speed requirements in different exoskeleton-assisted locomotion modes, such as running, walking, sit-to-stand, and stand-to-sit. The VTSEA features a SEA-coupled variable transmission ratio adjusting mechanism and works between three discrete levels of transmission ratio depending on the user’s initiative. The proposed prototype can also improve transparency in human-robot interaction. Also, an accurate torque controller with inertial compensation is developed for the VTSEA via the singular perturbation theory, and its stability is proved. The feasibility of the proposed VTSEA prototype and the precise output torque performance of VTSEA are verified by experiments.},
  archive   = {C_ICRA},
  author    = {Tianci Wang and Hao Wen and Zaixin Song and Zhiping Dong and Chunhua Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611435},
  month     = {5},
  pages     = {7055-7061},
  title     = {Development of variable transmission series elastic actuator for hip exoskeletons},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Energy-aware ergodic search: Continuous exploration for
multi-agent systems with battery constraints. <em>ICRA</em>, 7048–7054.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10609871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Continuous exploration without interruption is important in scenarios such as search and rescue and precision agriculture, where consistent presence is needed to detect events over large areas. Ergodic search already derives continuous trajectories in these scenarios so that a robot spends more time in areas with high information density. However, existing literature on ergodic search does not consider the robot&#39;s energy constraints, limiting how long a robot can explore. In fact, if the robots are battery-powered, it is physically not possible to continuously explore on a single battery charge. Our paper tackles this challenge, integrating ergodic search methods with energy-aware coverage. We trade off battery usage and coverage quality, maintaining uninterrupted exploration by at least one agent. Our approach derives an abstract battery model for future state-of-charge estimation and extends canonical ergodic search to ergodic search under battery constraints. Empirical data from simulations and real-world experiments demonstrate the effectiveness of our energy-aware ergodic search, which ensures continuous exploration and guarantees spatial coverage.},
  archive   = {C_ICRA},
  author    = {Adam Seewald and Cameron J. Lerch and Marvin Chancán and Aaron M. Dollar and Ian Abraham},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609871},
  month     = {5},
  pages     = {7048-7054},
  title     = {Energy-aware ergodic search: Continuous exploration for multi-agent systems with battery constraints},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Decentralized lifelong path planning for multiple ackerman
car-like robots. <em>ICRA</em>, 7041–7047. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Path planning for multiple non-holonomic robots in continuous domains constitutes a difficult robotics challenge with many applications. Despite significant recent progress on the topic, computationally efficient and high-quality solutions are lacking, especially in lifelong settings where robots must continuously take on new tasks. In this work, we make it possible to extend key ideas enabling state-of-the-art (SOTA) methods for multi-robot planning in discrete domains to the motion planning of multiple Ackerman (car-like) robots in lifelong settings, yielding high-performance centralized and decentralized planners. Our planners compute trajectories that allow the robots to reach precise SE(2) goal poses. The effectiveness of our methods is thoroughly evaluated and confirmed using both simulation and real-world experiments.},
  archive   = {C_ICRA},
  author    = {Teng Guo and Jingjin Yu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610330},
  month     = {5},
  pages     = {7041-7047},
  title     = {Decentralized lifelong path planning for multiple ackerman car-like robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient clothoid tree-based local path planning for
self-driving robots. <em>ICRA</em>, 7034–7040. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a real-time clothoid tree-based path planning for self-driving robots. Clothoids, curves that exhibit linear curvature profiles, play an important role in road design and path planning due to their appealing properties. Nevertheless, their real-time applications face considerable challenges, primarily stemming from the lack of a closed-form clothoid expression. To address these challenges, we introduce two innovative techniques: 1) an efficient and precise clothoid approximation using the Gauss-Legendre quadrature; and 2) a data-efficient decoder for interpolating clothoid splines that leverages the symmetry and similarity of clothoids. These techniques are demonstrated with numerical examples. The clothoid approximation ensures an accurate and smooth representation of the curve, and the clothoid spline decoder effectively accelerates the clothoid tree exploration by relaxing the problem constraints and reducing the problem size. Both techniques are integrated into our path planning algorithm and evaluated in various driving scenarios.},
  archive   = {C_ICRA},
  author    = {Minhyeong Lee and Dongjun Lee},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610306},
  month     = {5},
  pages     = {7034-7040},
  title     = {Efficient clothoid tree-based local path planning for self-driving robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient solution to the 2D visibility problem in
cartesian grid maps and its application in heuristic path planning.
<em>ICRA</em>, 7027–7033. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a novel, lightweight method to solve the visibility problem for 2D grids. The proposed method evaluates the existence of lines-of-sight from a source point to all other grid cells in a single pass with no preprocessing and independently of the number and shape of obstacles. It has a compute and memory complexity of $\mathcal{O}(n)$, where n = n x ×n y is the size of the grid, and requires at most ten arithmetic operations per grid cell. In the proposed approach, we use a linear first-order hyperbolic partial differential equation to transport the visibility quantity in all directions. In order to accomplish that, we use an entropy-satisfying upwind scheme that converges to the true visibility polygon as the step size goes to zero. This dynamic-programming approach allows the evaluation of visibility for an entire grid orders of magnitude faster than typical ray-casting algorithms. We provide a practical application of our proposed algorithm by posing the visibility quantity as a heuristic and implementing a deterministic, local-minima-free path planner, setting apart the proposed planner from traditional methods. Lastly, we provide necessary algorithms and an open-source implementation of the proposed methods.},
  archive   = {C_ICRA},
  author    = {Ibrahim Ibrahim and Joris Gillis and Wilm Decré and Jan Swevers},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611529},
  month     = {5},
  pages     = {7027-7033},
  title     = {An efficient solution to the 2D visibility problem in cartesian grid maps and its application in heuristic path planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stein variational guided model predictive path integral
control: Proposal and experiments with fast maneuvering vehicles.
<em>ICRA</em>, 7020–7026. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel Stochastic Optimal Control (SOC) method based on Model Predictive Path Integral control (MPPI), named Stein Variational Guided MPPI (SVG-MPPI), designed to handle rapidly shifting multimodal optimal action distributions. While MPPI can find a Gaussian-approximated optimal action distribution in closed form, i.e., without iterative solution updates, it struggles with the mul-timodality of the optimal distributions. This is due to the less representative nature of the Gaussian. To overcome this limitation, our method aims to identify a target mode of the optimal distribution and guide the solution to converge to fit it. In the proposed method, the target mode is roughly estimated using a modified Stein Variational Gradient Descent (SVGD) method and embedded into the MPPI algorithm to find a closed-form &quot;mode-seeking&quot; solution that covers only the target mode, thus preserving the fast convergence property of MPPI. Our simulation and real-world experimental results demonstrate that SVG-MPPI outperforms both the original MPPI and other state-of-the-art sampling-based SOC algorithms in terms of path-tracking and obstacle-avoidance capabilities. https://github.com/kohonda/proj-svg_mppi},
  archive   = {C_ICRA},
  author    = {Kohei Honda and Naoki Akai and Kosuke Suzuki and Mizuho Aoki and Hirotaka Hosogaya and Hiroyuki Okuda and Tatsuya Suzuki},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611021},
  month     = {5},
  pages     = {7020-7026},
  title     = {Stein variational guided model predictive path integral control: Proposal and experiments with fast maneuvering vehicles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-query TDSP for path planning in time-varying flow
fields. <em>ICRA</em>, 7005–7011. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many applications of path planning in time-varying flow fields, particularly in areas such as marine robotics and ship routing, can be modelled as instances of the time-varying shortest path (TDSP) problem. Although there are no known polynomial-time solutions to TDSP in general, our recent work has identified a tractable case where the flow is modelled as piecewise constant. Extending this method to allow for computational reuse in larger multi-query problems, however, requires additional thought. This paper shows that the piecewise-linear form of the cost function employed in previously work can be used to build an analogy of a shortest path tree, thereby enabling optimal concatenation of sub-problem solutions in the absence of an optimal substructure, and without uniform time discretisation. We present a framework for multi-query TDSP that finds an optimal path that passes through a defined sequence of waypoints and is computationally efficient. Performance comparison is provided in simulation that shows large (up to 100x) speedup compared to a naive approach. This result is significant for applications such as ship routing, where route evaluation is a desirable capability.},
  archive   = {C_ICRA},
  author    = {James Ju Heon Lee and Chanyeol Yoo and Stuart Anstee and Robert Fitch},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611501},
  month     = {5},
  pages     = {7005-7011},
  title     = {Multi-query TDSP for path planning in time-varying flow fields},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HEGN: Hierarchical equivariant graph neural network for 9DoF
point cloud registration. <em>ICRA</em>, 6981–6988. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Given its wide application in robotics, point cloud registration is a widely researched topic. Conventional methods aim to find a rotation and translation that align two point clouds in 6 degrees of freedom (DoF). However, certain tasks in robotics, such as category-level pose estimation, involve non-uniformly scaled point clouds, requiring a 9DoF transform for accurate alignment. We propose HEGN, a novel equivariant graph neural network for 9DoF point cloud registration. HEGN utilizes equivariance to rotation, translation, and scaling to estimate the transformation without relying on point correspondences. Based on graph representations for both point clouds, we extract equivariant node features aggregated in their local, cross-, and global context. In addition, we introduce a novel node pooling mechanism that leverages the cross-context importance of nodes to pool the graph representation. By repeating the feature extraction and node pooling, we obtain a graph hierarchy. Finally, we determine rotation and translation by aligning equivariant features aggregated over the graph hierarchy. To estimate scaling, we leverage scale information in the vector norm of the equivariant features. We evaluate the effectiveness of HEGN through experiments with the synthetic ModelNet40 dataset and the real-world ScanObjectNN dataset. The results show the superior performance of HEGN in 9DoF point cloud registration and its competitive performance in conventional 6DoF point cloud registration.},
  archive   = {C_ICRA},
  author    = {Adam Misik and Driton Salihu and Xin Su and Heike Brock and Eckehard Steinbach},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610562},
  month     = {5},
  pages     = {6981-6988},
  title     = {HEGN: Hierarchical equivariant graph neural network for 9DoF point cloud registration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Universal visual decomposer: Long-horizon manipulation made
easy. <em>ICRA</em>, 6973–6980. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-world robotic tasks stretch over extended horizons and encompass multiple stages. Learning long-horizon manipulation tasks, however, is a long-standing challenge, and demands decomposing the overarching task into several manageable subtasks to facilitate policy learning and generalization to unseen tasks. Prior task decomposition methods require task-specific knowledge, are computationally intensive, and cannot readily be applied to new tasks. To address these shortcomings, we propose Universal Visual Decomposer (UVD), an off-the-shelf task decomposition method for visual long-horizon manipulation using pre-trained visual representations designed for robotic control. At a high level, UVD discovers subgoals by detecting phase shifts in the embedding space of the pre-trained representation. Operating purely on visual demonstrations without auxiliary information, UVD can effectively extract visual subgoals embedded in the videos, while incurring zero additional training cost on top of standard visuomotor policy training. Goal-conditioned policies learned with UVD-discovered subgoals exhibit significantly improved compositional generalization at test time to unseen tasks. Furthermore, UVD-discovered subgoals can be used to construct goal-based reward shaping that jump-starts temporally extended exploration for reinforcement learning. We extensively evaluate UVD on both simulation and real-world tasks, and in all cases, UVD substantially outperforms baselines across imitation and reinforcement learning settings on in-domain and out-of-domain task sequences alike, validating the clear advantage of automated visual task decomposition within the simple, compact UVD framework.},
  archive   = {C_ICRA},
  author    = {Zichen Zhang and Yunshuang Li and Osbert Bastani and Abhishek Gupta and Dinesh Jayaraman and Yecheng Jason Ma and Luca Weihs},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611125},
  month     = {5},
  pages     = {6973-6980},
  title     = {Universal visual decomposer: Long-horizon manipulation made easy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Ultrafast square-root filter-based VINS. <em>ICRA</em>,
6966–6972. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we strongly advocate square-root covariance (instead of information) filtering for Visual-Inertial Navigation Systems (VINS), in particular on resource-constrained edge devices, because of its superior efficiency and numerical stability. Although VINS have made tremendous progress in recent years, they still face resource stringency and numerical instability on embedded systems when imposing limited word length. To overcome these challenges, we develop an ultrafast and numerically-stable square-root filter (SRF)-based VINS algorithm (i.e., SR-VINS). The numerical stability of the proposed SR-VINS is inherited from the adoption of square-root covariance while the remarkable efficiency is largely enabled by the novel SRF update method that is based on our new permuted-QR (P-QR), which fully utilizes and properly maintains the upper triangular structure of the square-root covariance matrix. Furthermore, we choose a special ordering of the state variables which is amenable for (P-)QR operations in the SRF propagation and update and prevents unnecessary computation. The proposed SR-VINS is validated extensively through numerical studies, demonstrating that when the state-of-the-art (SOTA) filters have numerical difficulties, our SR-VINS has superior numerical stability, and remarkably, achieves efficient and robust performance on 32-bit single-precision float at a speed nearly twice as fast as the SOTA methods. We also conduct comprehensive real-world experiments to validate the efficiency, accuracy, and robustness of the proposed SR-VINS.},
  archive   = {C_ICRA},
  author    = {Yuxiang Peng and Chuchu Chen and Guoquan Huang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610916},
  month     = {5},
  pages     = {6966-6972},
  title     = {Ultrafast square-root filter-based VINS},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SeqTrack3D: Exploring sequence information for robust 3D
point cloud tracking. <em>ICRA</em>, 6959–6965. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D single object tracking (SOT) is an important and challenging task for the autonomous driving and mobile robotics. Most existing methods perform tracking between two consecutive frames while ignoring the motion patterns of the target over a series of frames, which would cause performance degradation in the scenes with sparse points. To break through this limitation, we introduce &quot;Sequence-to-Sequence&quot; tracking paradigm and a tracker named SeqTrack3D to capture target motion across continuous frames. Unlike previous methods that primarily adopted three strategies: matching two consecutive point clouds, predicting relative motion, or utilizing sequential point clouds to address feature degradation, our SeqTrack3D combines both historical point clouds and bounding box sequences. This novel method ensures robust tracking by leveraging location priors from historical boxes, even in scenes with sparse points. Extensive experiments conducted on large-scale datasets show that SeqTrack3D achieves new state-of-the-art performances, improving by 6.00% on NuScenes and 14.13% on Waymo dataset. The code will be made public at https://github.com/aron-lin/seqtrack3d.},
  archive   = {C_ICRA},
  author    = {Yu Lin and Zhiheng Li and Yubo Cui and Zheng Fang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611238},
  month     = {5},
  pages     = {6959-6965},
  title     = {SeqTrack3D: Exploring sequence information for robust 3D point cloud tracking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NGEL-SLAM: Neural implicit representation-based global
consistent low-latency SLAM system. <em>ICRA</em>, 6952–6958. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Neural implicit representations have emerged as a promising solution for providing dense geometry in Simultaneous Localization and Mapping (SLAM). However, existing methods in this direction fall short in terms of global consistency and low latency. This paper presents NGEL-SLAM to tackle the above challenges. To ensure global consistency, our system leverages a traditional feature-based tracking module that incorporates loop closure. Additionally, we maintain a global consistent map by representing the scene using multiple neural implicit fields, enabling quick adjustment to the loop closure. Moreover, our system allows for fast convergence through the use of octree-based implicit representations. The combination of rapid response to loop closure and fast convergence makes our system a truly low-latency system that achieves global consistency. Our system enables rendering high-fidelity RGB-D images, along with extracting dense and complete surfaces. Experiments on both synthetic and real-world datasets suggest that our system achieves state-of-the-art tracking and mapping accuracy while maintaining low latency.},
  archive   = {C_ICRA},
  author    = {Yunxuan Mao and Xuan Yu and Zhuqing Zhang and Kai Wang and Yue Wang and Rong Xiong and Yiyi Liao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611269},
  month     = {5},
  pages     = {6952-6958},
  title     = {NGEL-SLAM: Neural implicit representation-based global consistent low-latency SLAM system},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep evidential uncertainty estimation for semantic
segmentation under out-of-distribution obstacles. <em>ICRA</em>,
6943–6951. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In order to navigate safely and reliably in novel environments, robots must estimate perceptual uncertainty when confronted with out-of-distribution (OOD) obstacles not seen in training data. We present a method to accurately estimate pixel-wise uncertainty in semantic segmentation without requiring real or synthetic OOD examples at training time. From a shared per-pixel latent feature representation, a classification network predicts a categorical distribution over semantic labels, while a normalizing flow estimates the probability density of features under the training distribution. The label distribution and density estimates are combined in a Dirichlet-based evidential uncertainty framework that efficiently computes epistemic and aleatoric uncertainty in a single neural network forward pass. Our method is enabled by three key contributions. First, we simplify the problem of learning a transformation to the training data density by starting from a fitted Gaussian mixture model instead of the conventional standard normal distribution. Second, we learn a richer and more expressive latent pixel representation to aid OOD detection by training a decoder to reconstruct input image patches. Third, we perform theoretical analysis of the loss function used in the evidential uncertainty framework and propose a principled objective that more accurately balances training the classification and density estimation networks. We demonstrate the accuracy of our uncertainty estimation approach under long-tail OOD obstacle classes for semantic segmentation in both off-road and urban driving environments.},
  archive   = {C_ICRA},
  author    = {Siddharth Ancha and Philip R. Osteen and Nicholas Roy},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611342},
  month     = {5},
  pages     = {6943-6951},
  title     = {Deep evidential uncertainty estimation for semantic segmentation under out-of-distribution obstacles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Constrained bimanual planning with analytic inverse
kinematics. <em>ICRA</em>, 6935–6942. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In order for a bimanual robot to manipulate an object that is held by both hands, it must construct motion plans such that the transformation between its end effectors remains fixed. This amounts to complicated nonlinear equality constraints in the configuration space, which are difficult for trajectory optimizers. In addition, the set of feasible configurations becomes a measure zero set, which presents a challenge to sampling-based motion planners. We leverage an analytic solution to the inverse kinematics problem to parametrize the configuration space, resulting in a lower-dimensional representation where the set of valid configurations has positive measure. We describe how to use this parametrization with existing motion planning algorithms, including sampling-based approaches, trajectory optimizers, and techniques that plan through convex inner-approximations of collision-free space.},
  archive   = {C_ICRA},
  author    = {Thomas Cohn and Seiji Shaw and Max Simchowitz and Russ Tedrake},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610675},
  month     = {5},
  pages     = {6935-6942},
  title     = {Constrained bimanual planning with analytic inverse kinematics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DenseTact-mini: An optical tactile sensor for grasping
multi-scale objects from flat surfaces. <em>ICRA</em>, 6928–6934. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dexterous manipulation, especially of small daily objects, continues to pose complex challenges in robotics. This paper introduces the DenseTact-Mini, an optical tactile sensor with a soft, rounded, smooth gel surface and compact design equipped with a synthetic fingernail. We propose three distinct grasping strategies: tap grasping using adhesion forces such as electrostatic and van der Waals, fingernail grasping leveraging rolling/sliding contact between the object and fingernail, and fingertip grasping with two soft fingertips. Through comprehensive evaluations, the DenseTact-Mini demonstrates a lifting success rate exceeding 90.2% when grasping various objects, including items such as 1mm basil seeds, thin paperclips, and items larger than 15mm such as bearings. This work demonstrates the potential of soft optical tactile sensors for dexterous manipulation and grasping.},
  archive   = {C_ICRA},
  author    = {Won Kyung Do and Ankush Kundan Dhawan and Mathilda Kitzmann and Monroe Kennedy},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610583},
  month     = {5},
  pages     = {6928-6934},
  title     = {DenseTact-mini: An optical tactile sensor for grasping multi-scale objects from flat surfaces},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SARA-RT: Scaling up robotics transformers with self-adaptive
robust attention. <em>ICRA</em>, 6920–6927. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present Self-Adaptive Robust Attention for Robotics Transformers (SARA-RT): a new paradigm for addressing the emerging challenge of scaling up Robotics Transformers (RT) for on-robot deployment. SARA-RT relies on the new method of fine-tuning proposed by us, called up-training. It converts pre-trained or already fine-tuned Transformer-based robotic policies of quadratic time complexity (including massive billion-parameter vision-language-action models or VLAs), into their efficient linear-attention counterparts maintaining high quality. We demonstrate the effectiveness of SARA-RT by speeding up: (a) the class of recently introduced RT-2 models [1], the first VLA robotic policies pre-trained on internet-scale data, as well as (b) Point Cloud Transformer (PCT) robotic policies operating on large point clouds. We complement our results with the rigorous mathematical analysis providing deeper insight into the phenomenon of SARA.},
  archive   = {C_ICRA},
  author    = {Isabel Leal and Krzysztof Choromanski and Deepali Jain and Avinava Dubey and Jake Varley and Michael Ryoo and Yao Lu and Frederick Liu and Vikas Sindhwani and Quan Vuong and Tamas Sarlos and Ken Oslund and Karol Hausman and Kanishka Rao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611597},
  month     = {5},
  pages     = {6920-6927},
  title     = {SARA-RT: Scaling up robotics transformers with self-adaptive robust attention},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hearing touch: Audio-visual pretraining for contact-rich
manipulation. <em>ICRA</em>, 6912–6919. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although pre-training on a large amount of data is beneficial for robot learning, current paradigms only perform large-scale pretraining for visual representations, whereas representations for other modalities are trained from scratch. In contrast to the abundance of visual data, it is unclear what relevant internet-scale data may be used for pretraining other modalities such as tactile sensing. Such pretraining becomes increasingly crucial in the low-data regimes common in robotics applications. In this paper, we address this gap by using contact microphones as an alternative tactile sensor. Our key insight is that contact microphones capture inherently audio-based information, allowing us to leverage large-scale audio-visual pretraining to obtain representations that boost the performance of robotic manipulation. To the best of our knowledge, our method is the first approach leveraging large-scale multisensory pre-training for robotic manipulation. For supplementary information including videos of real robot experiments, please see https://sites.google.com/view/hearing-touch.},
  archive   = {C_ICRA},
  author    = {Jared Mejia and Victoria Dean and Tess Hellebrekers and Abhinav Gupta},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611305},
  month     = {5},
  pages     = {6912-6919},
  title     = {Hearing touch: Audio-visual pretraining for contact-rich manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards generalizable zero-shot manipulation via translating
human interaction plans. <em>ICRA</em>, 6904–6911. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We pursue the goal of developing robots that can interact zero-shot with generic unseen objects via a diverse repertoire of manipulation skills and show how passive human videos can serve as a rich source of data for learning such generalist robots. Unlike typical robot learning approaches which directly learn how a robot should act from interaction data, we adopt a factorized approach that can leverage large-scale human videos to learn how a human would accomplish a desired task (a human ‘plan’), followed by ‘translating’ this plan to the robot’s embodiment. Specifically, we learn a human ‘plan predictor’ that, given a current image of a scene and a goal image, predicts the future hand and object configurations. We combine this with a ‘translation’ module that learns a plan-conditioned robot manipulation policy, and allows following humans plans for generic manipulation tasks in a zero-shot manner with no deployment-time training. Importantly, while the plan predictor can leverage large-scale human videos for learning, the translation module only requires a small amount of in-domain data, and can generalize to tasks not seen during training. We show that our learned system can perform over 16 manipulation skills that generalize to 40 objects, encompassing 100 real-world tasks for table-top manipulation and diverse in-the-wild manipulation. https://homangab.github.io/hopman/},
  archive   = {C_ICRA},
  author    = {Homanga Bharadhwaj and Abhinav Gupta and Vikash Kumar and Shubham Tulsiani},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610288},
  month     = {5},
  pages     = {6904-6911},
  title     = {Towards generalizable zero-shot manipulation via translating human interaction plans},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Open x-embodiment: Robotic learning datasets and RT-x models
: Open x-embodiment collaboration0. <em>ICRA</em>, 6892–6903. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train &quot;generalist&quot; X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms. The project website is robotics-transformer-x.github.io.},
  archive   = {C_ICRA},
  author    = {Abby O’Neill and Abdul Rehman and Abhiram Maddukuri and Abhishek Gupta and Abhishek Padalkar and Abraham Lee and Acorn Pooley and Agrim Gupta and Ajay Mandlekar and Ajinkya Jain and Albert Tung and Alex Bewley and Alex Herzog and Alex Irpan and Alexander Khazatsky and Anant Rai and Anchit Gupta and Andrew Wang and Anikait Singh and Animesh Garg and Aniruddha Kembhavi and Annie Xie and Anthony Brohan and Antonin Raffin and Archit Sharma and Arefeh Yavary and Arhan Jain and Ashwin Balakrishna and Ayzaan Wahid and Ben Burgess-Limerick and Beomjoon Kim and Bernhard Schölkopf and Blake Wulfe and Brian Ichter and Cewu Lu and Charles Xu and Charlotte Le and Chelsea Finn and Chen Wang and Chenfeng Xu and Cheng Chi and Chenguang Huang and Christine Chan and Christopher Agia and Chuer Pan and Chuyuan Fu and Coline Devin and Danfei Xu and Daniel Morton and Danny Driess and Daphne Chen and Deepak Pathak and Dhruv Shah and Dieter Büchler and Dinesh Jayaraman and Dmitry Kalashnikov and Dorsa Sadigh and Edward Johns and Ethan Foster and Fangchen Liu and Federico Ceola and Fei Xia and Feiyu Zhao and Freek Stulp and Gaoyue Zhou and Gaurav S. Sukhatme and Gautam Salhotra and Ge Yan and Gilbert Feng and Giulio Schiavi and Glen Berseth and Gregory Kahn and Guanzhi Wang and Hao Su and Hao-Shu Fang and Haochen Shi and Henghui Bao and Heni Ben Amor and Henrik I Christensen and Hiroki Furuta and Homer Walke and Hongjie Fang and Huy Ha and Igor Mordatch and Ilija Radosavovic and Isabel Leal and Jacky Liang and Jad Abou-Chakra and Jaehyung Kim and Jaimyn Drake and Jan Peters and Jan Schneider and Jasmine Hsu and Jeannette Bohg and Jeffrey Bingham and Jeffrey Wu and Jensen Gao and Jiaheng Hu and Jiajun Wu and Jialin Wu and Jiankai Sun and Jianlan Luo and Jiayuan Gu and Jie Tan and Jihoon Oh and Jimmy Wu and Jingpei Lu and Jingyun Yang and Jitendra Malik and João Silvério and Joey Hejna and Jonathan Booher and Jonathan Tompson and Jonathan Yang and Jordi Salvador and Joseph J. Lim and Junhyek Han and Kaiyuan Wang and Kanishka Rao and Karl Pertsch and Karol Hausman and Keegan Go and Keerthana Gopalakrishnan and Ken Goldberg and Kendra Byrne and Kenneth Oslund and Kento Kawaharazuka and Kevin Black and Kevin Lin and Kevin Zhang and Kiana Ehsani and Kiran Lekkala and Kirsty Ellis and Krishan Rana and Krishnan Srinivasan and Kuan Fang and Kunal Pratap Singh and Kuo-Hao Zeng and Kyle Hatch and Kyle Hsu and Laurent Itti and Lawrence Yunliang Chen and Lerrel Pinto and Li Fei-Fei and Liam Tan and Linxi Jim Fan and Lionel Ott and Lisa Lee and Luca Weihs and Magnum Chen and Marion Lepert and Marius Memmel and Masayoshi Tomizuka and Masha Itkina and Mateo Guaman Castro and Max Spero and Maximilian Du and Michael Ahn and Michael C. Yip and Mingtong Zhang and Mingyu Ding and Minho Heo and Mohan Kumar Srirama and Mohit Sharma and Moo Jin Kim and Naoaki Kanazawa and Nicklas Hansen and Nicolas Heess and Nikhil J Joshi and Niko Suenderhauf and Ning Liu and Norman Di Palo and Nur Muhammad Mahi Shafiullah and Oier Mees and Oliver Kroemer and Osbert Bastani and Pannag R Sanketi and Patrick Tree Miller and Patrick Yin and Paul Wohlhart and Peng Xu and Peter David Fagan and Peter Mitrano and Pierre Sermanet and Pieter Abbeel and Priya Sundaresan and Qiuyu Chen and Quan Vuong and Rafael Rafailov and Ran Tian and Ria Doshi and Roberto Martín-Martín and Rohan Baijal and Rosario Scalise and Rose Hendrix and Roy Lin and Runjia Qian and Ruohan Zhang and Russell Mendonca and Rutav Shah and Ryan Hoque and Ryan Julian and Samuel Bustamante and Sean Kirmani and Sergey Levine and Shan Lin and Sherry Moore and Shikhar Bahl and Shivin Dass and Shubham Sonawani and Shuran Song and Sichun Xu and Siddhant Haldar and Siddharth Karamcheti and Simeon Adebola and Simon Guist and Soroush Nasiriany and Stefan Schaal and Stefan Welker and Stephen Tian and Subramanian Ramamoorthy and Sudeep Dasari and Suneel Belkhale and Sungjae Park and Suraj Nair and Suvir Mirchandani and Takayuki Osa and Tanmay Gupta and Tatsuya Harada and Tatsuya Matsushima and Ted Xiao and Thomas Kollar and Tianhe Yu and Tianli Ding and Todor Davchev and Tony Z. Zhao and Travis Armstrong and Trevor Darrell and Trinity Chung and Vidhi Jain and Vincent Vanhoucke and Wei Zhan and Wenxuan Zhou and Wolfram Burgard and Xi Chen and Xiaolong Wang and Xinghao Zhu and Xinyang Geng and Xiyuan Liu and Xu Liangwei and Xuanlin Li and Yao Lu and Yecheng Jason Ma and Yejin Kim and Yevgen Chebotar and Yifan Zhou and Yifeng Zhu and Yilin Wu and Ying Xu and Yixuan Wang and Yonatan Bisk and Yoonyoung Cho and Youngwoon Lee and Yuchen Cui and Yue Cao and Yueh-Hua Wu and Yujin Tang and Yuke Zhu and Yunchu Zhang and Yunfan Jiang and Yunshuang Li and Yunzhu Li and Yusuke Iwasawa and Yutaka Matsuo and Zehan Ma and Zhuo Xu and Zichen Jeff Cui and Zichen Zhang and Zipeng Lin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611477},
  month     = {5},
  pages     = {6892-6903},
  title     = {Open X-embodiment: robotic learning datasets and RT-X models : open X-embodiment collaboration0},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Task-driven domain-agnostic learning with information
bottleneck for autonomous steering. <em>ICRA</em>, 6858–6865. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Environments for autonomous driving can vary from place to place, leading to challenges in designing a learning model for a new scene. Transfer learning can leverage knowledge from a learned domain to a new domain with limited data. In this work, we focus on end-to-end autonomous driving as the target task, consisting of both perception and control. We first utilize information bottleneck analysis to build a causal graph that defines our framework and the loss function; then we propose a novel domain-agnostic learning method for autonomous steering based on our analysis of training data, network architecture, and training paradigm. Experiments show that our method outperforms other SOTA methods.},
  archive   = {C_ICRA},
  author    = {Yu Shen and Laura Zheng and Tianyi Zhou and C. Lin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610479},
  month     = {5},
  pages     = {6858-6865},
  title     = {Task-driven domain-agnostic learning with information bottleneck for autonomous steering},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continual driving policy optimization with closed-loop
individualized curricula. <em>ICRA</em>, 6850–6857. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The safety of autonomous vehicles (AV) has been a long-standing top concern, stemming from the absence of rare and safety-critical scenarios in the long-tail naturalistic driving distribution. To tackle this challenge, a surge of research in scenario-based autonomous driving has emerged, with a focus on generating high-risk driving scenarios and applying them to conduct safety-critical testing of AV models. However, limited work has been explored on the reuse of these extensive scenarios to iteratively improve AV models. Moreover, it remains intractable and challenging to filter through gigantic scenario libraries collected from other AV models with distinct behaviors, attempting to extract transferable information for current AV improvement. Therefore, we develop a continual driving policy optimization framework featuring Closed-Loop Individualized Curricula (CLIC), which we factorize into a set of standardized sub-modules for flexible implementation choices: AV Evaluation, Scenario Selection, and AV Training. CLIC frames AV Evaluation as a collision prediction task, where it estimates the chance of AV failures in these scenarios at each iteration. Subsequently, by re-sampling from historical scenarios based on these failure probabilities, CLIC tailors individualized curricula for downstream training, aligning them with the evaluated capability of AV. Accordingly, CLIC not only maximizes the utilization of the vast pre-collected scenario library for closed-loop driving policy optimization but also facilitates AV improvement by individualizing its training with more challenging cases out of those poorly organized scenarios. Experimental results clearly indicate that CLIC surpasses other curriculum-based training strategies, showing substantial improvement in managing risky scenarios, while still maintaining proficiency in handling simpler cases.},
  archive   = {C_ICRA},
  author    = {Haoyi Niu and Yizhou Xu and Xingjian Jiang and Jianming Hu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611578},
  month     = {5},
  pages     = {6850-6857},
  title     = {Continual driving policy optimization with closed-loop individualized curricula},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Approximate multiagent reinforcement learning for on-demand
urban mobility problem on a large map. <em>ICRA</em>, 6843–6849. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we focus on the autonomous multiagent taxi routing problem for a large urban environment where the location and number of future ride requests are unknown a-priori, but can be estimated by an empirical distribution. Recent theory has shown that a rollout algorithm with a stable base policy produces a near-optimal stable policy. In the routing setting, a policy is stable if its execution keeps the number of outstanding requests uniformly bounded over time. Although, rollout-based approaches are well-suited for learning cooperative multiagent policies with considerations for future demand, applying such methods to a large urban environment can be computationally expensive due to the large number of taxis required for stability. In this paper, we aim to address the computational bottleneck of multiagent rollout by proposing an approximate multiagent rollout-based two phase algorithm that reduces computational costs, while still achieving a stable near-optimal policy. Our approach partitions the graph into sectors based on the predicted demand and the maximum number of taxis that can run sequentially given the user’s computational resources. The algorithm then applies instantaneous assignment (IA) for re-balancing taxis across sectors and a sector-wide multiagent rollout algorithm that is executed in parallel for each sector. We provide two main theoretical results: 1) characterize the number of taxis m that is sufficient for IA to be stable; 2) derive a necessary condition on m to maintain stability for IA as time goes to infinity. Our numerical results show that our approach achieves stability for an m that satisfies the theoretical conditions. We also empirically demonstrate that our proposed two phase algorithm has equivalent performance to the one-at-a-time rollout over the entire map, but with significantly lower runtimes.},
  archive   = {C_ICRA},
  author    = {Daniel Garces and Sushmita Bhattacharya and Dimitri Bertsekas and Stephanie Gil},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611063},
  month     = {5},
  pages     = {6843-6849},
  title     = {Approximate multiagent reinforcement learning for on-demand urban mobility problem on a large map},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalizing cooperative eco-driving via multi-residual task
learning. <em>ICRA</em>, 6836–6842. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Conventional control, such as model-based control, is commonly utilized in autonomous driving due to its efficiency and reliability. However, real-world autonomous driving contends with a multitude of diverse traffic scenarios that are challenging for these planning algorithms. Model-free Deep Reinforcement Learning (DRL) presents a promising avenue in this direction, but learning DRL control policies that generalize to multiple traffic scenarios is still a challenge. To address this, we introduce Multi-residual Task Learning (MRTL), a generic learning framework based on multi-task learning that, for a set of task scenarios, decomposes the control into nominal components that are effectively solved by conventional control methods and residual terms which are solved using learning. We employ MRTL for fleet-level emission reduction in mixed traffic using autonomous vehicles as a means of system control. By analyzing the performance of MRTL across nearly 600 signalized intersections and 1200 traffic scenarios, we demonstrate that it emerges as a promising approach to synergize the strengths of DRL and conventional methods in generalizable control.},
  archive   = {C_ICRA},
  author    = {Vindula Jayawardana and Sirui Li and Cathy Wu and Yashar Farid and Kentaro Oguchi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610586},
  month     = {5},
  pages     = {6836-6842},
  title     = {Generalizing cooperative eco-driving via multi-residual task learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Highway-driving with safe velocity bounds on occluded
traffic. <em>ICRA</em>, 6828–6835. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Limited visibility and sensor occlusions pose pressing safety challenges for advanced driver-assistance systems (ADAS) and autonomous vehicles (AVs). In this work, our pursuit was to strike a balance: a method that ensures safety in occluded scenarios while preventing overly cautious behavior. We argue that such approaches are crucial for AVs’ future, particularly when navigating alongside human drivers on highways at high speeds. To this end, we used reachability analysis to find safe velocity bounds on occluded traffic participants. Compared to state-of-the-art methods, we achieved velocity increases in more than 60% of the 230 cut-in scenarios from the highD dataset, without sacrificing safety.},
  archive   = {C_ICRA},
  author    = {Truls Nyberg and Jonne van Haastregt and Jana Tumova},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610904},
  month     = {5},
  pages     = {6828-6835},
  title     = {Highway-driving with safe velocity bounds on occluded traffic},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CausalAgents: A robustness benchmark for motion forecasting.
<em>ICRA</em>, 6820–6827. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As machine learning models become increasingly prevalent in motion forecasting for autonomous vehicles (AVs), it is critical to ensure that model predictions are safe and reliable. In this paper, we examine the robustness of motion forecasting to non-causal perturbations. We construct a new benchmark for evaluating and improving model robustness by applying perturbations to existing data. Specifically, we conduct an extensive labeling effort to identify causal agents, or agents whose presence influences human drivers’ behavior, in the Waymo Open Motion Dataset (WOMD), and we use these labels to perturb the data by deleting non-causal agents from the scene. We evaluate a diverse set of state-of-the-art deep-learning models on our proposed benchmark and find that all evaluated models exhibit large shifts under non-causal perturbation: we observe a surprising 25-38% relative change in minADE as compared to the original. In addition, we investigate techniques to improve model robustness, including increasing the training dataset size and using targeted data augmentations that randomly drop non-causal agents throughout training. Finally, we release the causal agent labels as an extension to WOMD and the robustness benchmarks to aid the community in building more reliable and safe deep-learning models for motion forecasting 1 .},
  archive   = {C_ICRA},
  author    = {Liting Sun and Rebecca Roelofs and Ben Caine and Khaled S. Refaat and Ben Sapp and Scott Ettinger and Wei Chai},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610186},
  month     = {5},
  pages     = {6820-6827},
  title     = {CausalAgents: A robustness benchmark for motion forecasting},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SIMMF: Semantics-aware interactive multiagent motion
forecasting for autonomous vehicle driving. <em>ICRA</em>, 6813–6819.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous vehicles require motion forecasting of their surrounding multiagents (pedestrians and vehicles) to make optimal decisions for navigation. The existing methods focus on techniques to utilize the positions and velocities of these agents and fail to capture semantic information from the scene. Moreover, to mitigate the increase in computational complexity associated with the number of agents in the scene, some works leverage Euclidean distance to prune far-away agents. However, distance-based metric alone is insufficient to select relevant agents and accurately perform their predictions. To resolve these issues, we propose the Semantics-aware Interactive Multiagent Motion Forecasting (SIMMF) method to capture semantics along with spatial information and optimally select relevant agents for motion prediction. Specifically, we achieve this by implementing a semantic-aware selection of relevant agents from the scene and passing them through an attention mechanism to extract global encodings. These encodings along with agents’ local information, are passed through an encoder to obtain time-dependent latent variables for a motion policy predicting the future trajectories. Our results show that the proposed approach outperforms stateof-the-art baselines and provides more accurate and scene-consistent predictions. The demonstration video is available at: https://youtu.be/Wjla071BPBA},
  archive   = {C_ICRA},
  author    = {Vidyaa Krishnan Nivash and Ahmed H. Qureshi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611189},
  month     = {5},
  pages     = {6813-6819},
  title     = {SIMMF: Semantics-aware interactive multiagent motion forecasting for autonomous vehicle driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DTPP: Differentiable joint conditional prediction and cost
evaluation for tree policy planning in autonomous driving.
<em>ICRA</em>, 6806–6812. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion prediction and cost evaluation are vital components in the decision-making system of autonomous vehicles. However, existing methods often ignore the importance of cost learning and treat them as separate modules. In this study, we employ a tree-structured policy planner and propose a differentiable joint training framework for both ego-conditioned prediction and cost models, resulting in a direct improvement of the final planning performance. For conditional prediction, we introduce a query-centric Transformer model that performs efficient ego-conditioned motion prediction. For planning cost, we propose a learnable context-aware cost function with latent interaction features, facilitating differentiable joint learning. We validate our proposed approach using the real-world nuPlan dataset and its associated planning test platform. Our framework not only matches state-of-the-art planning methods but outperforms other learning-based methods in planning quality, while operating more efficiently in terms of runtime. We show that joint training delivers significantly better performance than separate training of the two modules. Additionally, we find that tree-structured policy planning outperforms the conventional single-stage planning approach. Code is available: https://github.com/MCZhi/DTPP.},
  archive   = {C_ICRA},
  author    = {Zhiyu Huang and Peter Karkus and Boris Ivanovic and Yuxiao Chen and Marco Pavone and Chen Lv},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610550},
  month     = {5},
  pages     = {6806-6812},
  title     = {DTPP: Differentiable joint conditional prediction and cost evaluation for tree policy planning in autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prompting multi-modal tokens to enhance end-to-end
autonomous driving imitation learning with LLMs. <em>ICRA</em>,
6798–6805. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The utilization of Large Language Models (LLMs) within the realm of reinforcement learning, particularly as planners, has garnered a significant degree of attention in recent scholarly literature. However, a substantial proportion of existing research predominantly focuses on planning models for robotics that transmute the outputs derived from perception models into linguistic forms, thus adopting a ‘pure-language’ strategy. In this research, we propose a hybrid End-to-End learning framework for autonomous driving by combining basic driving imitation learning with LLMs based on multi-modality prompt tokens. Instead of simply converting perception results from the separated train model into pure language input, our novelty lies in two aspects. 1) The end-to-end integration of visual and LiDAR sensory input into learnable multi-modality tokens, thereby intrinsically alleviating description bias by separated pre-trained perception models. 2) Instead of directly letting LLMs drive, this paper explores a hybrid setting of letting LLMs help the driving model correct mistakes and complicated scenarios. The results of our experiments suggest that the proposed methodology can attain driving scores of 49.21%, coupled with an impressive route completion rate of 91.34% in the offline evaluation conducted via CARLA. These performance metrics are comparable to the most advanced driving models.},
  archive   = {C_ICRA},
  author    = {Yiqun Duan and Qiang Zhang and Renjing Xu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611614},
  month     = {5},
  pages     = {6798-6805},
  title     = {Prompting multi-modal tokens to enhance end-to-end autonomous driving imitation learning with LLMs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards fault-tolerant deployment of mobile robot navigation
in the edge: An experimental study. <em>ICRA</em>, 6791–6797. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern algorithms allow robots to reach a greater level of autonomy and fulfill more challenging tasks. However, on-board limitations regarding computational and battery resources are hindering factors regarding the deployment of such algorithms particularly on mobile robots. Although offloading a majority of the algorithmic components to the edge or even cloud offers an attractive option to leverage massive computing power in robotics applications, safety and reliability remain critical issues. This paper presents a minimalistic safety fallback mechanism when offloading mobile robot navigation to the edge, that ensures safe and collision-free navigation even in the presence of failures in the connection between the on-board and edge-devices. We show the effectiveness of our approach through extensive testing in three different relevant scenarios in a simulated warehouse environment. Our experiments demonstrate the effects of different fallback strategies and show how our proposed approach is able to ensure safety while allowing the robot to continue its mission during an interrupted connection and thus avoiding unnecessary downtime.},
  archive   = {C_ICRA},
  author    = {Florian Mirus and Frederik Pasch and Kay-Ulrich Scholl},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611013},
  month     = {5},
  pages     = {6791-6797},
  title     = {Towards fault-tolerant deployment of mobile robot navigation in the edge: An experimental study},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fixture calibration with guaranteed bounds from a few
correspondence-free surface points. <em>ICRA</em>, 6718–6724. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Calibration of fixtures in robotic work cells is essential but also time consuming and error-prone, and poor calibration can easily lead to wasted debugging time in down-stream tasks. Contact-based calibration methods let the user measure points on the fixture’s surface with a tool tip attached to the robot’s end effector. Most such methods require the user to manually annotate correspondences on the CAD model, however, this is error-prone and a cumbersome user experience. We propose a correspondence-free alternative: The user simply measures a few points from the fixture’s surface, and our method provides a tight superset of the poses which could explain the measured points. This naturally detects ambiguities related to symmetry and uninformative points and conveys this uncertainty to the user. Perhaps more importantly, it provides guaranteed bounds on the pose. The computation of such bounds is made tractable by the use of a hierarchical grid on SE(3). Our method is evaluated both in simulation and on a real collaborative robot, showing great potential for easier and less error-prone fixture calibration. sites.google.com/view/ttpose},
  archive   = {C_ICRA},
  author    = {Rasmus Laurvig Haugaard and Yitaek Kim and Thorbjørn Mosekjær Iversen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610632},
  month     = {5},
  pages     = {6718-6724},
  title     = {Fixture calibration with guaranteed bounds from a few correspondence-free surface points},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Forgetting in robotic episodic long-term memory.
<em>ICRA</em>, 6711–6717. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Artificial cognitive architectures traditionally rely on complex memory models to encode, store, and retrieve information. However, the conventional practice of transferring all data from working memory (WM) to long-term memory (LTM) leads to high data volumes and challenges in efficient information processing and access. Deciding what information to retain or discard within a robot’s LTM is particularly challenging since knowledge about future data utilization is absent. Drawing inspiration from human forgetting this paper implements and evaluates novel forgetting techniques that allow consolidation in the robot’s LTM only when new information is encountered. The proposed approach combines fast filtering during data transfer to the robot’s LTM with slower yet more precise forgetting mechanisms that are periodically evaluated for offline data deletion inside the LTM. We compare different mechanisms, utilizing metrics such as data similarity, data age, and consolidation frequency. The efficacy of forgetting techniques is evaluated by comparing their performance in a task where two ARMAR robots search through their LTM for past object locations in episodic ego-centric images and robot state data. Experimental results show that our forgetting techniques significantly reduce the space requirements of a robot’s LTM while maintaining its capacity to successfully perform tasks relying on LTM information. Notably, similarity-based forgetting methods outperform frequency- and time-based approaches. The combination of online frequency-based, online similarity-based, offline similarity-based, and time-based decay methods shows superior performance compared to using individual forgetting strategies.},
  archive   = {C_ICRA},
  author    = {Joana Plewnia and Fabian Peller-Konrad and Tamim Asfour},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610299},
  month     = {5},
  pages     = {6711-6717},
  title     = {Forgetting in robotic episodic long-term memory},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AutoTAMP: Autoregressive task and motion planning with LLMs
as translators and checkers. <em>ICRA</em>, 6695–6702. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For effective human-robot interaction, robots need to understand, plan, and execute complex, long-horizon tasks described by natural language. Recent advances in large language models (LLMs) have shown promise for translating natural language into robot action sequences for complex tasks. However, existing approaches either translate the natural language directly into robot trajectories or factor the inference process by decomposing language into task sub-goals and relying on a motion planner to execute each sub-goal. When complex environmental and temporal constraints are involved, inference over planning tasks must be performed jointly with motion plans using traditional task-and-motion planning (TAMP) algorithms, making factorization into subgoals untenable. Rather than using LLMs to directly plan task sub-goals, we instead perform few-shot translation from natural language task descriptions to an intermediate task representation that can then be consumed by a TAMP algorithm to jointly solve the task and motion plan. To improve translation, we automatically detect and correct both syntactic and semantic errors via autoregressive re-prompting, resulting in significant improvements in task completion. We show that our approach outperforms several methods using LLMs as planners in complex task domains. See our project website § for prompts, videos, and code.},
  archive   = {C_ICRA},
  author    = {Yongchao Chen and Jacob Arkin and Charles Dawson and Yang Zhang and Nicholas Roy and Chuchu Fan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611163},
  month     = {5},
  pages     = {6695-6702},
  title     = {AutoTAMP: Autoregressive task and motion planning with LLMs as translators and checkers},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Drive anywhere: Generalizable end-to-end autonomous driving
with multi-modal foundation models. <em>ICRA</em>, 6687–6694. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As autonomous driving technology matures, end-to-end methodologies have emerged as a leading strategy, promising seamless integration from perception to control via deep learning. However, existing systems grapple with challenges such as unexpected open set environments and the complexity of black-box models. At the same time, the evolution of deep learning introduces larger, multimodal foundational models, offering multi-modal visual and textual understanding. In this paper, we harness these multimodal foundation models to enhance the robustness and adaptability of autonomous driving systems. We introduce a method to extract nuanced spatial features from transformers and the incorporation of latent space simulation for improved training and policy debugging. We use pixel/patch-aligned feature descriptors to expand foundational model capabilities to create an end-to-end multimodal driving model, demonstrating unparalleled results in diverse tests. Our solution combines language with visual perception and achieves significantly greater robustness on out-of-distribution situations.},
  archive   = {C_ICRA},
  author    = {Tsun-Hsuan Wang and Alaa Maalouf and Wei Xiao and Yutong Ban and Alexander Amini and Guy Rosman and Sertac Karaman and Daniela Rus},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611590},
  month     = {5},
  pages     = {6687-6694},
  title     = {Drive anywhere: Generalizable end-to-end autonomous driving with multi-modal foundation models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FLTRNN: Faithful long-horizon task planning for robotics
with large language models. <em>ICRA</em>, 6680–6686. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent planning methods based on Large Language Models typically employ the In-Context Learning paradigm. Complex long-horizon planning tasks require more context(including instructions and demonstrations) to guarantee that the generated plan can be executed correctly. However, in such conditions, LLMs may overlook(unfaithful) the rules in the given context, resulting in the generated plans being invalid or even leading to dangerous actions. In this paper, we investigate the faithfulness of LLMs for complex long-horizon tasks. Inspired by human intelligence, we introduce a novel framework named FLTRNN. FLTRNN employs a language-based RNN structure to integrate task decomposition and memory management into LLM planning inference, which could effectively improve the faithfulness of LLMs and make the planner more reliable. We conducted experiments in VirtualHome household tasks. Results show that our model significantly improves faithfulness and success rates for complex long-horizon tasks. Website at https://tannl.github.io/FLTRNN.github.io/},
  archive   = {C_ICRA},
  author    = {Jiatao Zhang and Lanling Tang and Yufan Song and Qiwei Meng and Haofu Qian and Jun Shao and Wei Song and Shiqiang Zhu and Jason Gu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611663},
  month     = {5},
  pages     = {6680-6686},
  title     = {FLTRNN: Faithful long-horizon task planning for robotics with large language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gen2Sim: Scaling up robot learning in simulation with
generative models. <em>ICRA</em>, 6672–6679. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generalist robot manipulators need to learn a wide variety of manipulation skills across diverse environments. Current robot training pipelines rely on humans to provide kinesthetic demonstrations or to program simulation environments and to code up reward functions for reinforcement learning. Such human involvement is an important bottleneck towards scaling up robot learning across diverse tasks and environments. We propose Generation to Simulation (Gen2Sim), a method for scaling up robot skill learning in simulation by automating generation of 3D assets, task descriptions, task decompositions and reward functions using large pre-trained generative models of language and vision. We generate 3D assets for simulation by lifting open-world 2D object-centric images to 3D using image diffusion models and querying LLMs to determine plausible physics parameters. Given URDF files of generated and human-developed assets, we chain-of-thought prompt LLMs to map these to relevant task descriptions, temporal decompositions, and corresponding python reward functions for reinforcement learning. We show Gen2Sim succeeds in learning policies for diverse long horizon tasks, where reinforcement learning with non temporally decomposed reward functions fails. Gen2Sim provides a viable path for scaling up reinforcement learning for robot manipulators in simulation, both by diversifying and expanding task and environment development, and by facilitating the discovery of reinforcement-learned behaviors through temporal task decomposition in RL. Our work contributes hundreds of simulated assets, tasks and demonstrations, taking a step towards fully autonomous robotic manipulation skill acquisition in simulation.},
  archive   = {C_ICRA},
  author    = {Pushkal Katara and Zhou Xian and Katerina Fragkiadaki},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610566},
  month     = {5},
  pages     = {6672-6679},
  title     = {Gen2Sim: Scaling up robot learning in simulation with generative models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Resolving loop closure confusion in repetitive environments
for visual SLAM through AI foundation models assistance. <em>ICRA</em>,
6657–6663. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In visual SLAM (VSLAM) systems, loop closure plays a crucial role in reducing accumulated errors. However, VSLAM systems relying on low-level visual features often suffer from the problem of perceptual confusion in repetitive environments, where scenes in different locations are incorrectly identified as the same. Existing work has attempted to introduce object-level features or artificial landmarks. The former approach struggles to distinguish visually similar but different objects, while the latter is both time-consuming and labor-intensive. This paper introduces a novel loop closure detection method that leverages pretrained AI foundation models to extract rich semantic information about specific types of objects (e.g., door numbers), referred to as semantic anchors, that help to distinguish similar scenes better. In settings such as office buildings, hotels, and warehouses, this approach helps to improve the robustness of loop closure detection. We validate the effectiveness of our method through experiments conducted in both simulated and real-world environments.},
  archive   = {C_ICRA},
  author    = {Hongzhou Li and Sijie Yu and Shengkai Zhang and Guang Tan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610083},
  month     = {5},
  pages     = {6657-6663},
  title     = {Resolving loop closure confusion in repetitive environments for visual SLAM through AI foundation models assistance},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). When to replan? An adaptive replanning strategy for
autonomous navigation using deep reinforcement learning. <em>ICRA</em>,
6650–6656. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The hierarchy of global and local planners is one of the most commonly utilized system designs in autonomous robot navigation. While the global planner generates a reference path from the current to goal locations based on the pre-built map, the local planner produces a kinodynamic trajectory to follow the reference path while avoiding perceived obstacles. To account for unforeseen or dynamic obstacles not present on the pre-built map, &quot;when to replan&quot; the reference path is critical for the success of safe and efficient navigation. However, determining the ideal timing to execute replanning in such partially unknown environments still remains an open question. In this work, we first conduct an extensive simulation experiment to compare several common replanning strategies and confirm that effective strategies are highly dependent on the environment as well as the global and local planners. Based on this insight, we then derive a new adaptive replanning strategy based on deep reinforcement learning, which can learn from experience to decide appropriate replanning timings in the given environment and planning setups. Our experimental results show that the proposed replanner can perform on par or even better than the current best-performing strategies in multiple situations regarding navigation robustness and efficiency.},
  archive   = {C_ICRA},
  author    = {Kohei Honda and Ryo Yonetani and Mai Nishimura and Tadashi Kozuno},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611474},
  month     = {5},
  pages     = {6650-6656},
  title     = {When to replan? an adaptive replanning strategy for autonomous navigation using deep reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A metacognitive approach to out-of-distribution detection
for segmentation. <em>ICRA</em>, 6642–6649. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite outstanding semantic scene segmentation in closed-worlds, deep neural networks segment novel instances poorly, which is required for autonomous agents acting in an open world. To improve out-of-distribution (OOD) detection for segmentation, we introduce a metacognitive approach in the form of a lightweight module that leverages entropy measures, segmentation predictions, and spatial context to characterize the segmentation model’s uncertainty and detect pixel-wise OOD data in real-time. Additionally, our approach incorporates a novel method of generating synthetic OOD data in context with in-distribution data, which we use to fine-tune existing segmentation models with maximum entropy training. This further improves the metacognitive module’s performance without requiring access to OOD data while enabling compatibility with established pre-trained models. Our resulting approach can reliably detect OOD instances in a scene, as shown by state-of-the-art performance on OOD detection for semantic segmentation benchmarks.},
  archive   = {C_ICRA},
  author    = {Meghna Gummadi and Cassandra Kent and Karl Schmeckpeper and Eric Eaton},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611287},
  month     = {5},
  pages     = {6642-6649},
  title     = {A metacognitive approach to out-of-distribution detection for segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Better monocular 3D detectors with LiDAR from the past.
<em>ICRA</em>, 6634–6641. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate 3D object detection is crucial to autonomous driving. Though LiDAR-based detectors have achieved impressive performance, the high cost of LiDAR sensors precludes their widespread adoption in affordable vehicles. Camera-based detectors are cheaper alternatives but often suffer inferior performance compared to their LiDAR-based counterparts due to inherent depth ambiguities in images. In this work, we seek to improve monocular 3D detectors by leveraging unlabeled historical LiDAR data. Specifically, at inference time, we assume that the camera-based detectors have access to multiple unlabeled LiDAR scans from past traversals at locations of interest (potentially from other high-end vehicles equipped with LiDAR sensors). Under this setup, we proposed a novel, simple, and end-to-end trainable framework, termed AsyncDepth, to effectively extract relevant features from asynchronous LiDAR traversals of the same location for monocular 3D detectors. We show consistent and significant performance gain (up to 9 AP) across multiple state-of-the-art models and datasets with a negligible additional latency of 9.66 ms and a small storage cost. Our code can be found at https://github.com/YurongYou/AsyncDepth.},
  archive   = {C_ICRA},
  author    = {Yurong You and Cheng Perng Phoo and Carlos Andres Diaz-Ruiz and Katie Z Luo and Wei-Lun Chao and Mark Campbell and Bharath Hariharan and Kilian Q Weinberger},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610444},
  month     = {5},
  pages     = {6634-6641},
  title     = {Better monocular 3D detectors with LiDAR from the past},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). C2FDrone: Coarse-to-fine drone-to-drone detection using
vision transformer networks. <em>ICRA</em>, 6627–6633. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A vision-based drone-to-drone detection system is crucial for various applications like collision avoidance, countering hostile drones, and search-and-rescue operations. However, detecting drones presents unique challenges, including small object sizes, distortion, occlusion, and real-time processing requirements. Current methods integrating multi-scale feature fusion and temporal information have limitations in handling extreme blur and minuscule objects. To address this, we propose a novel coarse-to-fine detection strategy based on vision transformers. We evaluate our approach on three challenging drone- to-drone detection datasets, achieving F1 score enhancements of 7%, 3%, and 1% on the FL-Drones, AOT, and NPS-Drones datasets, respectively. Additionally, we demonstrate real-time processing capabilities by deploying our model on an edge-computing device. Our code will be made publicly available.},
  archive   = {C_ICRA},
  author    = {Sairam VC Rebbapragada and Pranoy Panda and Vineeth N Balasubramanian},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609997},
  month     = {5},
  pages     = {6627-6633},
  title     = {C2FDrone: Coarse-to-fine drone-to-drone detection using vision transformer networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CLIPUNetr: Assisting human-robot interface for uncalibrated
visual servoing control with CLIP-driven referring expression
segmentation. <em>ICRA</em>, 6620–6626. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The classical human-robot interface in uncalibrated image-based visual servoing (UIBVS) relies on either human annotations or semantic segmentation with categorical labels. Both methods fail to match natural human communication and convey rich semantics in manipulation tasks as effectively as natural language expressions. In this paper, we tackle this problem by using referring expression segmentation, which is a prompt-based approach, to provide more in-depth information for robot perception. To generate high-quality segmentation predictions from referring expressions, we propose CLIPUNetr - a new CLIP-driven referring expression segmentation network. CLIPUNetr leverages CLIP’s strong vision-language representations to segment regions from referring expressions, while utilizing its &quot;U-shaped&quot; encoder-decoder architecture to generate predictions with sharper boundaries and finer structures. Furthermore, we propose a new pipeline to integrate CLIPUNetr into UIBVS and apply it to control robots in real-world environments. In experiments, our method improves boundary and structure measurements by an average of 120% and can successfully assist real-world UIBVS control in an unstructured manipulation environment.},
  archive   = {C_ICRA},
  author    = {Chen Jiang and Yuchen Yang and Martin Jagersand},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611647},
  month     = {5},
  pages     = {6620-6626},
  title     = {CLIPUNetr: Assisting human-robot interface for uncalibrated visual servoing control with CLIP-driven referring expression segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HIC-YOLOv5: Improved YOLOv5 for small object detection.
<em>ICRA</em>, 6614–6619. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Small object detection has been a challenging problem in the field of object detection. There has been some works that proposes improvements for this task, such as adding several attention blocks or changing the whole structure of feature fusion networks. However, the computation cost of these models is large, which makes deploying a real-time object detection system unfeasible, while leaving room for improvement. To this end, an improved YOLOv5 model: HIC-YOLOv5 is proposed to address the aforementioned problems. Firstly, an additional prediction head specific to small objects is added to provide a higher-resolution feature map for better prediction. Secondly, an involution block is adopted between the backbone and neck to increase channel information of the feature map. Moreover, an attention mechanism named CBAM is applied at the end of the backbone, thus not only decreasing the computation cost compared with previous works but also emphasizing the important information in both channel and spatial domain. Our result shows that HIC-YOLOv5 has improved mAP@[.5:.95] by 6.42% and mAP@0.5 by 9.38% on VisDrone-2019-DET dataset.},
  archive   = {C_ICRA},
  author    = {Shiyi Tang and Shu Zhang and Yini Fang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610273},
  month     = {5},
  pages     = {6614-6619},
  title     = {HIC-YOLOv5: Improved YOLOv5 for small object detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning temporal cues by predicting objects move for
multi-camera 3D object detection. <em>ICRA</em>, 6607–6613. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In autonomous driving and robotics, there is a growing interest in utilizing short-term historical data to enhance multi-camera 3D object detection, leveraging the continuous and correlated nature of input video streams. Recent work has focused on spatially aligning BEV-based features over timesteps. However, this is often limited as its gain does not scale well with long-term past observations. To address this, we advocate for supervising a model to predict objects’ poses given past observations, thus explicitly guiding to learn objects’ temporal cues. To this end, we propose a model called DAP (Detection After Prediction), consisting of a two-branch network: (i) a branch responsible for forecasting the current objects’ poses given past observations and (ii) another branch that detects objects based on the current and past observations. The features predicting the current objects from branch (i) is fused into branch (ii) to transfer predictive knowledge. We conduct extensive experiments with the large-scale nuScenes datasets, and we observe that utilizing such predictive information significantly improves the overall detection performance. Our model can be used plug-and-play, showing consistent performance gain.},
  archive   = {C_ICRA},
  author    = {Seokha Moon and Hongbeen Park and Jaekoo Lee and Jinkyu Kim},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610934},
  month     = {5},
  pages     = {6607-6613},
  title     = {Learning temporal cues by predicting objects move for multi-camera 3D object detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LSSAttn: Towards dense and accurate view transformation for
multi-modal 3D object detection. <em>ICRA</em>, 6600–6606. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fusing the camera and LiDAR information in the unified BEV representation serves as the elegant paradigm for the 3D detection tasks. Current multi-modal fusion methods in BEV can be categorized into LSS-based and Transformer-based in terms of their view transformation. The former leverages inaccurate depth prediction and massive pseudo points for perspective-to-BEV transformation while the latter only fetches sparse image features to the BEV representation. To overcome their shortcomings, an optimized view transformation is proposed, which can be easily modulated into the LSS-based methods. The proposed module capitalizes on the LSS mechanism to establish dense associations between perspective pixels and BEV grids. It utilizes the attention mechanism to compute similarity scores for each associated pair during feature aggregation. Starting from the BEVFusion baseline, we further introduce (1) cross-attention within the associated subsets to transfer image features into the BEV, and (2) a multi-scale feature fusion mechanism for LSS-based view transformation. Extensive experiments on nuScenes validate the effectiveness and efficiency of our proposed module, which achieves an increase of 1.3% in mAP compared to the baseline model.},
  archive   = {C_ICRA},
  author    = {Qi Jiang and Hao Sun},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610830},
  month     = {5},
  pages     = {6600-6606},
  title     = {LSSAttn: Towards dense and accurate view transformation for multi-modal 3D object detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust 3D object detection from LiDAR-radar point clouds via
cross-modal feature augmentation. <em>ICRA</em>, 6585–6591. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel framework for robust 3D object detection from point clouds via cross-modal hallucination. Our proposed approach is agnostic to either hallucination direction between LiDAR and 4D radar. We introduce multiple alignments on both spatial and feature levels to achieve simultaneous backbone refinement and hallucination generation. Specifically, spatial alignment is proposed to deal with the geometry discrepancy for better instance matching between LiDAR and radar. The feature alignment step further bridges the intrinsic attribute gap between the sensing modalities and stabilizes the training. The trained object detection models can deal with difficult detection cases better, even though only single-modal data is used as the input during the inference stage. Extensive experiments on the View-of-Delft (VoD) dataset show that our proposed method outperforms the state-of-the-art (SOTA) methods for both radar and LiDAR object detection while maintaining competitive efficiency in runtime.},
  archive   = {C_ICRA},
  author    = {Jianning Deng and Gabriel Chan and Hantao Zhong and Chris Xiaoxuan Lu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610775},
  month     = {5},
  pages     = {6585-6591},
  title     = {Robust 3D object detection from LiDAR-radar point clouds via cross-modal feature augmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robot synesthesia: In-hand manipulation with visuotactile
sensing. <em>ICRA</em>, 6558–6565. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Executing contact-rich manipulation tasks necessitates the fusion of tactile and visual feedback. However, the distinct nature of these modalities poses significant challenges. In this paper, we introduce a system that leverages visual and tactile sensory inputs to enable dexterous in-hand manipulation. Specifically, we propose Robot Synesthesia, a novel point cloudbased tactile representation inspired by human tactile-visual synesthesia. This approach allows for the simultaneous and seamless integration of both sensory inputs, offering richer spatial information and facilitating better reasoning about robot actions. Comprehensive ablations are performed on how the integration of vision and touch can improve reinforcement learning and Sim2Real performance. Our project page is available at https://yingyuan0414.github.io/visuotactile/.},
  archive   = {C_ICRA},
  author    = {Ying Yuan and Haichuan Che and Yuzhe Qin and Binghao Huang and Zhao-Heng Yin and Kang-Won Lee and Yi Wu and Soo-Chul Lim and Xiaolong Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610532},
  month     = {5},
  pages     = {6558-6565},
  title     = {Robot synesthesia: In-hand manipulation with visuotactile sensing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dexterous in-hand manipulation by guiding exploration with
simple sub-skill controllers. <em>ICRA</em>, 6551–6557. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, reinforcement learning has led to dexterous manipulation skills of increasing complexity. Nonetheless, learning these skills in simulation still exhibits poor sample-efficiency which stems from the fact these skills are learned from scratch without the benefit of any domain expertise. In this work, we aim to improve the sample efficiency of learning dexterous in-hand manipulation skills using controllers available via domain knowledge. To this end, we design simple sub-skill controllers and demonstrate improved sample efficiency using a framework that guides exploration toward relevant state space by following actions from these controllers. We are the first to demonstrate learning hard-to-explore finger-gaiting in-hand manipulation skills without the use of an exploratory reset distribution.},
  archive   = {C_ICRA},
  author    = {Gagan Khandate and Cameron Paul Mehlman and Xingsheng Wei and Matei Ciocarlie},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611300},
  month     = {5},
  pages     = {6551-6557},
  title     = {Dexterous in-hand manipulation by guiding exploration with simple sub-skill controllers},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust in-hand manipulation with extrinsic contacts.
<em>ICRA</em>, 6544–6550. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present in-hand manipulation tasks where a robot moves an object in grasp, maintains its external contact mode with the environment, and adjusts its in-hand pose simultaneously. The proposed manipulation task leads to complex contact interactions which can be very susceptible to uncertainties in kinematic and physical parameters. Therefore, we propose a robust in-hand manipulation method, which consists of two parts. First, an in-gripper mechanics model that computes a naïve motion cone assuming all parameters are precise. Then, a robust planning method refines the motion cone to maintain desired contact mode regardless of parametric errors. Real-world experiments were conducted to illustrate the accuracy of the mechanics model and the effectiveness of the robust planning framework in the presence of kinematics parameter errors.},
  archive   = {C_ICRA},
  author    = {Boyuan Liang and Kei Ota and Masayoshi Tomizuka and Devesh K. Jha},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611664},
  month     = {5},
  pages     = {6544-6550},
  title     = {Robust in-hand manipulation with extrinsic contacts},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Geometric fabrics: A safe guiding medium for policy
learning. <em>ICRA</em>, 6537–6543. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotics policies are always subjected to complex, second order dynamics that entangle their actions with resulting states. In reinforcement learning (RL) contexts, policies have the burden of deciphering these complicated interactions over massive amounts of experience and complex reward functions to learn how to accomplish tasks. Moreover, policies typically issue actions directly to controllers like Operational Space Control (OSC) or joint PD control, which induces straightline motion towards these action targets in task or joint space. However, straightline motion in these spaces for the most part do not capture the rich, nonlinear behavior our robots need to exhibit, shifting the burden of discovering these behaviors more completely to the agent. Unlike these simpler controllers, geometric fabrics capture a much richer and desirable set of behaviors via artificial, second order dynamics grounded in nonlinear geometry. These artificial dynamics shift the uncontrolled dynamics of a robot via an appropriate control law to form behavioral dynamics. Behavioral dynamics unlock a new action space and safe, guiding behavior over which RL policies are trained. Behavioral dynamics enable bang-bang-like RL policy actions that are still safe for real robots, simplify reward engineering, and help sequence real-world, high-performance policies. We describe the framework more generally and create a specific instantiation for the problem of dexterous, in-hand reorientation of a cube by a highly actuated robot hand.},
  archive   = {C_ICRA},
  author    = {Karl Van Wyk and Ankur Handa and Viktor Makoviychuk and Yijie Guo and Arthur Allshire and Nathan D. Ratliff},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610235},
  month     = {5},
  pages     = {6537-6543},
  title     = {Geometric fabrics: A safe guiding medium for policy learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Curriculum-based sensing reduction in simulation to
real-world transfer for in-hand manipulation. <em>ICRA</em>, 6530–6536.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simulation to Real-World Transfer allows affordable and fast training of learning-based robots for manipulation tasks using Deep Reinforcement Learning methods. Currently, Asymmetric Actor-Critic approaches are used for Sim2Real to reduce the rich idealized features in simulation to the accessible ones in the real world. However, the feature reduction from the simulation to the real world is conducted through an empirically defined one-step curtail. Small feature reduction does not sufficiently remove the actor’s features, which may still cause difficulty setting up the physical system, while large feature reduction may cause difficulty and inefficiency in training. To address this issue, we proposed Curriculum-based Sensing Reduction to enable the actor to start with the same rich feature space as the critic and then get rid of the hard-to-extract features step-by-step for higher training performance and better adaptation for real-world feature space. The reduced features are replaced with random signals from a Deep Random Generator to remove the dependency between the output and the removed features and avoid creating new dependencies. The methods are evaluated on the Allegro robot hand in a real-world in-hand manipulation task. The results show that our methods have faster training and higher task performance than baselines and can solve real-world tasks when selected tactile features are reduced.},
  archive   = {C_ICRA},
  author    = {Lingfeng Tao and Jiucai Zhang and Qiaojie Zheng and Xiaoli Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610328},
  month     = {5},
  pages     = {6530-6536},
  title     = {Curriculum-based sensing reduction in simulation to real-world transfer for in-hand manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). In-hand rolling manipulation based on ball-on-cloth system.
<em>ICRA</em>, 6521–6527. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel in-hand rolling manipulation method in which a ball on a cloth attached to fingertips is controlled using flexible and adaptive deformation of the cloth. First, an analytical model of the ball-on-cloth system is introduced. The shape of the cloth is simplified, and the rolling constraint of the ball on the cloth is defined focusing on the lowest point of the ball. Next, the relationship between the input to the cloth anchor point and the position of the lowest point of the ball is expressed by a linear approximation. Then, the input to generate the desired rolling orbit is designed. Next, as an example of utilizing the rolling orbits, a manipulation method to rotate the ball around a vertical axis is developed. Finally, a multi-fingered hand with a piece of cloth attached to the fingertips is developed, and the effectiveness of the proposed system is experimentally verified.},
  archive   = {C_ICRA},
  author    = {Hinano Ichikura and Mitsuru Higashimori},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609867},
  month     = {5},
  pages     = {6521-6527},
  title     = {In-hand rolling manipulation based on ball-on-cloth system},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quasi-static soft fixture analysis of rigid and deformable
objects. <em>ICRA</em>, 6513–6520. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a sampling-based approach to reasoning about the caging-based manipulation of rigid and a simplified class of deformable 3D objects subject to energy constraints. Towards this end, we propose the notion of soft fixtures extending earlier work on energy-bounded caging to include a broader set of energy function constraints, such as gravitational and elastic potential energy of 3D deformable objects. Previous methods focused on establishing provably correct algorithms to compute lower bounds or analytically exact estimates of escape energy for a very restricted class of known objects with low-dimensional configuration spaces, such as planar polygons. We instead propose a practical sampling-based approach that is applicable in higher-dimensional configuration spaces, but only produces a sequence of upper-bound estimates that, however, appear to converge rapidly to actual escape energy. We present 8 simulation experiments demonstrating the applicability of our approach to various complex quasi-static manipulation scenarios. Quantitative results indicate the effectiveness of our approach in providing upper-bound estimates for escape energy in quasi-static manipulation scenarios. Two real-world experiments also show that the computed normalized escape energy estimates appear to correlate strongly with the probability of escape of an object under randomized pose perturbation 1 .},
  archive   = {C_ICRA},
  author    = {Yifei Dong and Florian T. Pokorny},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611593},
  month     = {5},
  pages     = {6513-6520},
  title     = {Quasi-static soft fixture analysis of rigid and deformable objects},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Regrasping on printed circuit boards with the smart suction
cup. <em>ICRA</em>, 6477–6483. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The disposal of waste electrical and electronic equipment (WEEE) presents a sustainability challenge, particularly for waste printed circuit boards (PCBs). PCBs are challenging to sort out from other waste materials in part because traditional industrial end-effectors struggle to reliably grip these irregularly shaped objects with unmodeled surface-mounted components. Vision-based separators, while effective for object categorization, face challenges with identifying precise grasp points on PCB surfaces. This paper studies regrasping control to enhance suction cup grasping performance on PCBs, addressing issues arising from uneven surfaces and intricate features that interfere with suction sealing. We categorize PCBs into two recycling levels – with large surface features intact or removed – and conduct experiments on both stationary and conveyor belt setups with realistic vision-based grasp planners. Results show that jumping regrasping improves pick-and-place success rate. Haptically driven jumping – using the Smart Suction Cup – is especially useful for unprocessed waste PCBs with large surface mount parts. The proposed method offers a promising solution to enhance the efficiency and reliability of robotic grasping in recycling applications.},
  archive   = {C_ICRA},
  author    = {Jungpyo Lee and Zheng Sun and Zhipeng Dong and Fei Chen and Hannah S. Stuart},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610153},
  month     = {5},
  pages     = {6477-6483},
  title     = {Regrasping on printed circuit boards with the smart suction cup},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GrainGrasp: Dexterous grasp generation with fine-grained
contact guidance. <em>ICRA</em>, 6470–6476. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One goal of dexterous robotic grasping is to allow robots to handle objects with the same level of flexibility and adaptability as humans. However, it remains a challenging task to generate an optimal grasping strategy for dexterous hands, especially when it comes to delicate manipulation and accurate adjustment the desired grasping poses for objects of varying shapes and sizes. In this paper, we propose a novel dexterous grasp generation scheme called GrainGrasp that provides fine-grained contact guidance for each fingertip. In particular, we employ a generative model to predict separate contact maps for each fingertip on the object point cloud, effectively capturing the specifics of finger-object interactions. In addition, we develop a new dexterous grasping optimization algorithm that solely relies on the point cloud as input, eliminating the necessity for complete mesh information of the object. By leveraging the contact maps of different fingertips, the proposed optimization algorithm can generate precise and determinable strategies for human-like object grasping. Experimental results confirm the efficiency of the proposed scheme. Our code is available at https://github.com/wmtlab/GrainGrasp.},
  archive   = {C_ICRA},
  author    = {Fuqiang Zhao and Dzmitry Tsetserukou and Qian Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610035},
  month     = {5},
  pages     = {6470-6476},
  title     = {GrainGrasp: Dexterous grasp generation with fine-grained contact guidance},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A surprisingly efficient representation for multi-finger
grasping. <em>ICRA</em>, 6462–6469. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The problem of grasping objects using a multi-finger hand has received significant attention in recent years. However, it remains challenging to handle a large number of unfamiliar objects in real and cluttered environments. In this work, we propose a representation that can be effectively mapped to the multi-finger grasp space. Based on this representation, we develop a simple decision model that generates accurate grasp quality scores for different multi-finger grasp poses using only hundreds to thousands of training samples. We demonstrate that our representation performs well on a real robot and achieves a success rate of 78.64% after training with only 500 real-world grasp attempts and 87% with 4500 grasp attempts. Additionally, we achieve a success rate of 84.51% in a dynamic human-robot handover scenario using a multi-finger hand.},
  archive   = {C_ICRA},
  author    = {Hengxu Yan and Hao-Shu Fang and Cewu Lu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611424},
  month     = {5},
  pages     = {6462-6469},
  title     = {A surprisingly efficient representation for multi-finger grasping},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards feasible dynamic grasping: Leveraging gaussian
process distance field, SE(3) equivariance, and riemannian mixture
models. <em>ICRA</em>, 6455–6461. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a novel approach to improve robotic grasping in dynamic environments by integrating Gaussian Process Distance Fields (GPDF), SE(3) equivariant networks, and Riemannian Mixture Models. The aim is to enable robots to grasp moving objects effectively. Our approach comprises three main components: object shape reconstruction, grasp sampling, and implicit grasp pose selection. GPDF accurately models the shape of objects, which is essential for precise grasp planning. SE(3) equivariance ensures that the sampled grasp poses are equivariant to the object’s pose changes, enhancing robustness in dynamic scenarios. Riemannian Gaussian Mixture Models are employed to assess reachability, providing a feasible and adaptable grasping strategies. Feasible grasp poses are targeted by novel task or joint space reactive controllers formulated using Gaussian Mixture Models and Gaussian Processes. This method resolves the challenge of discrete grasp pose selection, enabling smoother grasping execution. Experimental validation confirms the effectiveness of our approach in generating feasible grasp poses and achieving successful grasps in dynamic environments. By integrating these advanced techniques, we present a promising solution for enhancing robotic grasping capabilities in real-world scenarios.},
  archive   = {C_ICRA},
  author    = {Ho Jin Choi and Nadia Figueroa},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611601},
  month     = {5},
  pages     = {6455-6461},
  title     = {Towards feasible dynamic grasping: Leveraging gaussian process distance field, SE(3) equivariance, and riemannian mixture models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). InterRep: A visual interaction representation for robotic
grasping. <em>ICRA</em>, 6448–6454. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, pre-trained vision models have gained significant attention in motor control, showcasing impressive performance across diverse robotic learning tasks. While previous works predominantly concentrate on the significance of the pre-training phase, the equally important task of extracting more effective representations based on existing pre-trained visual models remains unexplored. To better leverage the representation capabilities of pre-trained models for robotic grasping, we propose InterRep, a novel interaction representation method that possesses not only the strengths of pre-trained models, known for their robustness in noisy environments and their proficiency in recognizing essential features, but also the capacity of capturing dynamic interaction details and local geometric features during the grasping process. Based on the novel representation, we introduce a deep reinforcement learning method to learn generalizable grasping policies. The experimental results demonstrate that our proposed representation outperforms the baselines in terms of both training speed and generalization. For the generalized grasping tasks with dexterous robotic hands, our method boasts a success rate nearly 20% higher than methods using the global features of the entire image from pre-trained models. In addition, our proposed representation method demonstrates promising performance when applied to a different robotic hand and task. It also exhibits excellent performance on real robots with a success rate of 70%.},
  archive   = {C_ICRA},
  author    = {Yu Cui and Qi Ye and Qingtao Liu and Anjun Chen and Gaofeng Li and Jiming Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610870},
  month     = {5},
  pages     = {6448-6454},
  title     = {InterRep: A visual interaction representation for robotic grasping},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OmniColor: A global camera pose optimization approach of
LiDAR-360Camera fusion for colorizing point clouds. <em>ICRA</em>,
6396–6402. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A Colored point cloud, as a simple and efficient 3D representation, has many advantages in various fields, including robotic navigation and scene reconstruction. This representation is now commonly used in 3D reconstruction tasks relying on cameras and LiDARs. However, fusing data from these two types of sensors is poorly performed in many existing frameworks, leading to unsatisfactory mapping results, mainly due to inaccurate camera poses. This paper presents Omni-Color, a novel and efficient algorithm to colorize point clouds using an independent 360-degree camera. Given a LiDAR-based point cloud and a sequence of panorama images with initial coarse camera poses, our objective is to jointly optimize the poses of all frames for mapping images onto geometric reconstructions. Our pipeline works in an off-the-shelf manner that does not require any feature extraction or matching process. Instead, we find optimal poses by directly maximizing the photometric consistency of LiDAR maps. In experiments, we show that our method can overcome the severe visual distortion of omnidirectional images and greatly benefit from the wide field of view (FOV) of 360-degree cameras to reconstruct various scenarios with accuracy and stability. The code will be released at https://github.com/liubonan123/OmniColor/.},
  archive   = {C_ICRA},
  author    = {Bonan Liu and Guoyang Zhao and Jianhao Jiao and Guang Cai and Chengyang Li and Handi Yin and Yuyang Wang and Ming Liu and Pan Hui},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610292},
  month     = {5},
  pages     = {6396-6402},
  title     = {OmniColor: A global camera pose optimization approach of LiDAR-360Camera fusion for colorizing point clouds},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast and robust normal estimation for sparse LiDAR scans.
<em>ICRA</em>, 6389–6395. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Light Detection and Ranging (LiDAR) technology has proven to be an important part of many robotics systems. Surface normals estimated from LiDAR data are commonly used for a variety of tasks in such systems. As most of the today’s mechanical LiDAR sensors produce sparse data, estimating normals from a single scan in a robust manner poses difficulties.In this paper, we address the problem of estimating normals for sparse LiDAR data avoiding the typical issues of smoothing out the normals in high curvature areas.Mechanical LiDARs rotate a set of rigidly mounted lasers. One firing of such a set of lasers produces an array of points where each point’s neighbor is known due to the known firing pattern of the scanner. We use this knowledge to connect these points to their neighbors and label them using the angles of the lines connecting them. When estimating normals at these points, we only consider points with the same label as neighbors. This allows us to avoid estimating normals in high curvature areas.We evaluate our approach on various data, both self-recorded and publicly available, acquired using various sparse LiDAR sensors. We show that using our method for normal estimation leads to normals that are more robust in areas with high curvature which leads to maps of higher quality. We also show that our method only incurs a constant factor runtime overhead with respect to a lightweight baseline normal estimation procedure and is therefore suited for operation in computationally demanding environments.},
  archive   = {C_ICRA},
  author    = {Igor Bogoslavskyi and Konstantinos Zampogiannis and Raymond Phan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611556},
  month     = {5},
  pages     = {6389-6395},
  title     = {Fast and robust normal estimation for sparse LiDAR scans},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scene action maps: Behavioural maps for navigation without
metric information. <em>ICRA</em>, 6354–6360. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans are remarkable in their ability to navigate without metric information. We can read abstract 2D maps, such as floor-plans or hand-drawn sketches, and use them to navigate in unseen rich 3D environments, without requiring prior traversals to map out these scenes in detail. We posit that this is enabled by the ability to represent the environment abstractly as interconnected navigational behaviours, e.g., &quot;follow the corridor&quot; or &quot;turn right&quot;, while avoiding detailed, accurate spatial information at the metric level. We introduce the Scene Action Map (SAM), a behavioural topological graph, and propose a learnable map-reading method, which parses a variety of 2D maps into SAMs. Map-reading extracts salient information about navigational behaviours from the overlooked wealth of pre-existing, abstract and inaccurate maps, ranging from floor-plans to sketches. We evaluate the performance of SAMs for navigation, by building and deploying a behavioural navigation stack on a quadrupedal robot. Videos and more information is available at: https://scene-action-maps.github.io.},
  archive   = {C_ICRA},
  author    = {Joel Loo and David Hsu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610489},
  month     = {5},
  pages     = {6354-6360},
  title     = {Scene action maps: Behavioural maps for navigation without metric information},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ColonMapper: Topological mapping and localization for
colonoscopy. <em>ICRA</em>, 6329–6336. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a topological mapping and localization system able to operate on real human colonoscopies, despite significant shape and illumination changes. The map is a graph where each node codes a colon location by a set of real images, while edges represent traversability between nodes. For close-in-time images, where scene changes are minor, place recognition can be successfully managed with the recent transformers-based local feature matching algorithms. However, under long-term changes –such as different colonoscopies of the same patient– feature-based matching fails. To address this, we train on real colonoscopies a deep global descriptor achieving high recall with significant changes in the scene. The addition of a Bayesian filter boosts the accuracy of long-term place recognition, enabling relocalization in a previously built map. Our experiments show that ColonMapper is able to autonomously build a map and localize against it in two important use cases: localization within the same colonoscopy or within different colonoscopies of the same patient. Code: github.com/jmorlana/ColonMapper.},
  archive   = {C_ICRA},
  author    = {Javier Morlana and Juan D. Tardós and J. M. M. Montiel},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610426},
  month     = {5},
  pages     = {6329-6336},
  title     = {ColonMapper: Topological mapping and localization for colonoscopy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OptiState: State estimation of legged robots using gated
networks with transformer-based vision and kalman filtering.
<em>ICRA</em>, 6314–6320. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {State estimation for legged robots is challenging due to their highly dynamic motion and limitations imposed by sensor accuracy. By integrating Kalman filtering, optimization, and learning-based modalities, we propose a hybrid solution that combines proprioception and exteroceptive information for estimating the state of the robot’s trunk. Leveraging joint encoder and IMU measurements, our Kalman filter is enhanced through a single-rigid body model that incorporates ground reaction force control outputs from convex Model Predictive Control optimization. The estimation is further refined through Gated Recurrent Units, which also considers semantic insights and robot height from a Vision Transformer autoencoder applied on depth images. This framework not only furnishes accurate robot state estimates, including uncertainty evaluations, but can minimize the nonlinear errors that arise from sensor measurements and model simplifications through learning. The proposed methodology is evaluated in hardware using a quadruped robot on various terrains, yielding a 65% improvement on the Root Mean Squared Error compared to our VIO SLAM baseline. Code example: https://github.com/AlexS28/OptiState},
  archive   = {C_ICRA},
  author    = {Alexander Schperberg and Yusuke Tanaka and Saviz Mowlavi and Feng Xu and Bharathan Balaji and Dennis Hong},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610160},
  month     = {5},
  pages     = {6314-6320},
  title     = {OptiState: State estimation of legged robots using gated networks with transformer-based vision and kalman filtering},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Leveraging neural radiance fields for uncertainty-aware
visual localization. <em>ICRA</em>, 6298–6305. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As a promising fashion for visual localization, scene coordinate regression (SCR) has seen tremendous progress in the past decade. Most recent methods usually adopt neural networks to learn the mapping from image pixels to 3D scene coordinates, which requires a vast amount of annotated training data. We propose to leverage Neural Radiance Fields (NeRF) to generate training samples for SCR. Despite NeRF’s efficiency in rendering, many of the rendered data are polluted by artifacts or only contain minimal information gain, which can hinder the regression accuracy or bring unnecessary computational costs with redundant data. These challenges are addressed in three folds in this paper: (1) A NeRF is designed to separately predict uncertainties for the rendered color and depth images, which reveal data reliability at the pixel level. (2) SCR is formulated as deep evidential learning with epistemic uncertainty, which is used to evaluate information gain and scene coordinate quality. (3) Based on the three arts of uncertainties, a novel view selection policy is formed that significantly improves data efficiency. Experiments on public datasets demonstrate that our method could select the samples that bring the most information gain and promote the performance with the highest efficiency.},
  archive   = {C_ICRA},
  author    = {Le Chen and Weirong Chen and Rui Wang and Marc Pollefeys},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610126},
  month     = {5},
  pages     = {6298-6305},
  title     = {Leveraging neural radiance fields for uncertainty-aware visual localization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Globalizing local features: Image retrieval using shared
local features with pose estimation for faster visual localization.
<em>ICRA</em>, 6290–6297. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual localization is an important sub-task in SfM and visual SLAM that involves estimating a 6-DoF camera pose for an input query image relative to a given 3D model of the environment. The most accurate approach is a hierarchical one that splits the task into two stages: image retrieval and camera pose estimation. Each stage requires different image features, with global features compactly encoding holistic image information for the first stage and local features encoding the appearance around salient image points for the second stage. While existing methods use independent networks to extract these features, one for global and one for local, this strategy is suboptimal in terms of computational efficiency. In this paper, we propose a novel approach that achieves state-of-the-art inference accuracy with significantly improved efficiency. Our approach’s core component is SuperGF, a network that aggregates local features optimized for camera pose estimation to create a global feature that enables precise image retrieval. Through extensive experiments on the standard benchmark tests, we demonstrate that the method offers a better trade-off between accuracy and computational cost.},
  archive   = {C_ICRA},
  author    = {Wenzheng Song and Ran Yan and Boshu Lei and Takayuki Okatani},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610786},
  month     = {5},
  pages     = {6290-6297},
  title     = {Globalizing local features: Image retrieval using shared local features with pose estimation for faster visual localization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WayIL: Image-based indoor localization with wayfinding maps.
<em>ICRA</em>, 6274–6281. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper tackles a localization problem in large-scale indoor environments with wayfinding maps. A wayfinding map abstractly portrays the environment, and humans can localize themselves based on the map. However, when it comes to using it for robot localization, large geometrical discrepancies between the wayfinding map and the real world make it hard to use conventional localization methods. Our objective is to estimate a robot pose within a wayfinding map, utilizing RGB images from perspective cameras. We introduce two different imagination modules which are inspired by how humans can comprehend and interpret their surroundings for localization purposes. These modules jointly learn how to effectively observe the first-person-view (FPV) world to interpret bird-eye-view (BEV) maps. Providing explicit guidance to the two imagination modules significantly improves the precision of the localization system. We demonstrate the effectiveness of the proposed approach using real-world datasets, which are collected from various large-scale crowded indoor environments. The experimental results show that, in 85% of scenarios, the proposed localization system can estimate its pose within 3m in large indoor spaces. Project Site: https://rllab-snu.github.io/projects/WayIL/},
  archive   = {C_ICRA},
  author    = {Obin Kwon and Dongki Jung and Youngji Kim and Soohyun Ryu and Suyong Yeon and Songhwai Oh and Donghwan Lee},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610480},
  month     = {5},
  pages     = {6274-6281},
  title     = {WayIL: Image-based indoor localization with wayfinding maps},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A turning radius prediction scheme for sailing robots under
complex marine environment. <em>ICRA</em>, 6260–6265. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a strategy for predicting the turning radius of a sailing robot with consideration of aerodynamic and hydrodynamic interferences from the marine environment. The turning radius is initially obtained based on three consecutive designated points during the turning process, which is regarded as the baseline method. Subsequently, on the basis of our constructed turning datasets, a model is trained using Gaussian process regression (GPR) to achieve radius prediction. The feasibility and effectiveness of the proposed scheme have been validated in both simulation and experiments (conducted with OceanVoy as shown in Fig. 1). Under experimental circumstances, the Mean Absolute Error (MAE) of the turning radius produced by the trained prediction model is 0.58m. Furthermore, it has been observed that during longterm sailing covering a distance of 1200km, apart from wind speed and robot velocity, the tidal range also has a significant impact on the navigation of sailing robots.},
  archive   = {C_ICRA},
  author    = {Weimin Qi and Qinbo Sun and Huihuan Qian},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611373},
  month     = {5},
  pages     = {6260-6265},
  title     = {A turning radius prediction scheme for sailing robots under complex marine environment},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predicting against the flow: Boosting source localization by
means of field belief modeling using upstream source proximity.
<em>ICRA</em>, 6254–6259. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Time-effective and accurate source localization with mobile robots is crucial in safety-critical scenarios, e.g. leakage detection. This becomes particular challenging in realistic cluttered scenarios, i.e. in the presence of complex current flows or wind. Traditional methods often fall short due to simplifications or limited onboard resources.We propose to combine source localization with a Gaussian Markov Random Field (GMRF). This allows to improve source localization hypotheses by building on the GMRF’s concentration and flow field belief that are continuously updated by gathered measurements. We introduce the upstream source proximity (USP) as a natural metric that exploits the joint knowledge represented in the field belief’s concentration and flow field, i.e. predicting sources upstream. As a result, our method yields a computationally efficient source localization and field belief module providing substantially more stable gradients than conventional concentration gradient-based methods.We demonstrate the suitability of our approach in a series of numerical experiments covering complex source location scenarios. With regard to computational requirements, the method achieves update rates of 10Hz on a RaspberryPi4B.},
  archive   = {C_ICRA},
  author    = {Finn L Busch and Nathalie Bauschmann and Sami Haddadin and Robert Seifried and Daniel A Duecker},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610144},
  month     = {5},
  pages     = {6254-6259},
  title     = {Predicting against the flow: Boosting source localization by means of field belief modeling using upstream source proximity},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Seabed intervention with an underwater legged robot.
<em>ICRA</em>, 6247–6253. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Efficiently performing intervention tasks underwater is crucial in various commercial and scientific sectors; however, propeller-driven vehicles face limitations due to their floating nature. In Remotely Operated Vehicles (ROVs) operations, this can be compensated by the ability of the operator, but they come with high operational costs. Instead, Autonomous Underwater Vehicles (AUVs) have shown promise, but demonstrated intervention tasks are limited to controlled environments or docked. To address these limitations, we focused on the use of Underwater Legged Robots (ULRs), which offer greater stability and agile seabed mobility thanks to their legged propulsion system. This paper presents the field demonstration of teleoperated pick-and-place tasks using the ULR SILVER2 for which a novel stance control, Graphic User Interface (GUI), and tendon-driven gripper have been developed based on the lessons learned through several hours of field use. The methodology is validated through four field trials, including missions in both shallow water and open sea environments. The trials involve picking and placing various objects, such as plastic bottles, bags, and cans. The results demonstrate successful teleoperated object grasping and manipulation in real-world conditions, with collection times ranging from a few minutes to around ten minutes. Overall, this research contributes to advancing the capabilities of ULRs and lays the foundation for future underwater intervention missions in various scientific and industrial applications, aligning with the goals of the Decade of Ocean Science for Sustainable Development.},
  archive   = {C_ICRA},
  author    = {Giacomo Picardi and Anna Astolfi and Marcello Calisti},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611135},
  month     = {5},
  pages     = {6247-6253},
  title     = {Seabed intervention with an underwater legged robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GIRA: Gaussian mixture models for inference and robot
autonomy. <em>ICRA</em>, 6212–6218. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces the open-source framework, GIRA, which implements fundamental robotics algorithms for reconstruction, pose estimation, and occupancy modeling using compact generative models. Compactness enables perception in the large by ensuring that the perceptual models can be communicated through low-bandwidth channels during large-scale mobile robot deployments. The generative property enables perception in the small by providing high-resolution reconstruction capability. These properties address perception needs for diverse robotic applications, including multi-robot exploration and dexterous manipulation. State-of-the-art perception systems construct perceptual models via multiple disparate pipelines that reuse the same underlying sensor data, which leads to increased computation, redundancy, and complexity. GIRA bridges this gap by providing a unified perceptual modeling framework using Gaussian mixture models (GMMs) as well as a novel systems contribution, which consists of GPUaccelerated functions to learn GMMs 10-100x faster compared to existing CPU implementations. Because few GMM-based frameworks are open-sourced, this work seeks to accelerate innovation and broaden adoption of these techniques.},
  archive   = {C_ICRA},
  author    = {Kshitij Goel and Wennie Tabib},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611216},
  month     = {5},
  pages     = {6212-6218},
  title     = {GIRA: Gaussian mixture models for inference and robot autonomy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A powerline inspection UAV equipped with dexterous, lockable
gripping mechanisms for autonomous perching and contact rolling.
<em>ICRA</em>, 6206–6211. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inspection of powerlines is a hard problem that requires humans to operate in remote locations and dangerous conditions. This paper proposes a quadcopter unmanned aerial vehicle (UAV) equipped with rolling-capable perching mechanisms and a depth-vision system for the purpose of autonomous power line inspection. The perching mechanism grips onto the power line, allowing the UAV to withstand external forces such as wind disturbances. Once engaged and applying the desired gripping force, the perching mechanism requires no power through the use of a ratcheting serial elastic transmission, allowing the UAV to perch indefinitely. The depth-vision system automates the perching and unperching procedures by estimating the position and pose of the UAV relative to the powerline. These measurements are sent to a local position controller that guides the UAV to and from the power line. Once perched, rollers in the fingers of the perching mechanism drive the UAV along the powerline, providing a close-up platform for inspection equipment. The proposed system was tested in an outdoor testing environment and shown to autonomously perch and unperch from a steel cable. The grippers force application was analysed and the UAVs powerless robust perch is demonstrated by total disconnect of power while perched. These results suggest that such a system could be a valuable tool for the upkeep of electricity networks.},
  archive   = {C_ICRA},
  author    = {Angus Lynch and Corey Duguid and Joao Buzzatto and Minas Liarokapis},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610783},
  month     = {5},
  pages     = {6206-6211},
  title     = {A powerline inspection UAV equipped with dexterous, lockable gripping mechanisms for autonomous perching and contact rolling},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Topological exploration using segmented map with keyframe
contribution in subterranean environments. <em>ICRA</em>, 6199–6205. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing exploration algorithms mainly generate frontiers using random sampling or motion primitive methods within a specific sensor range or search space. However, frontiers generated within constrained spaces lead to back-and-forth maneuvers in large-scale environments, thereby diminishing exploration efficiency. To address this issue, we propose a method that utilizes a 3D dense map to generate Segmented Exploration Regions (SERs) and generate frontiers from a global-scale perspective. In particular, this paper presents a novel topological map generation approach that fully utilizes Line-of-Sight (LOS) features of LiDAR sensor points to enhance exploration efficiency inside large-scale subterranean environments. Our topological map contains the contributions of keyframes that generate each SER, enabling rapid exploration through a switch between local path planning and global path planning to each frontier. The proposed method achieved higher explored volume generation than the state-of-the-art algorithm in a large-scale simulation environment and demonstrated a 62% improvement in explored volume increment performance. For validation, we conducted field tests using UAVs in real subterranean environments, demonstrating the efficiency and speed of our method.},
  archive   = {C_ICRA},
  author    = {Boseong Kim and Hyunki Seong and D. Hyunchul Shim},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610605},
  month     = {5},
  pages     = {6199-6205},
  title     = {Topological exploration using segmented map with keyframe contribution in subterranean environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-modal hybrid robot with enhanced traversal
performance*. <em>ICRA</em>, 6193–6198. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current multi-modal hybrid robots with flight and wheeled modes have fallen into the dilemma that they can only avoid obstacles by re-taking off when encountering obstacles due to the poor performance of wheeled obstacle-crossing. To tackle this problem, this paper presents a novel multi-modal hybrid robot with the ability to actively adjust the wheel’s size, which is inspired by the behavior of the turtle’s legs when it encounters obstacles, to enhance the traversal performance. In detail, we describe the hardware design that allows the robot to achieve a modal switch between flight and wheeled modes through foldable structures and variable wheel diameters; then, we present the architecture to control these two morphing mechanisms. After that, we establish the theoretical kinematic models for both the foldable arm and variable wheel and carry out extensive experiments to test the performance of the foldable arm, the variable-diameter wheel, as well as the traversal performance of the robot. Experimental results show that the proposed multimodal robot can realize the function of a quadrotor, respond quickly with full-scale folding within 0.9 s, climb a maximum slope of 36°, and traverse narrow passageways, which exhibit superior mobility and environmental adaptability.},
  archive   = {C_ICRA},
  author    = {Zhipeng He and Na Zhao and Yudong Luo and Sian Long and Xi Luo and Hongbin Deng},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609980},
  month     = {5},
  pages     = {6193-6198},
  title     = {A multi-modal hybrid robot with enhanced traversal performance*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust control for bidirectional thrust quadrotors under
instantaneously drastic disturbances. <em>ICRA</em>, 6186–6192. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Quadrotors may crash and cause severe accidents under instantaneously drastic disturbances. To mitigate the effect of such disturbances, these critical issues should be considered: efficient disturbance observation and compensation, full attitude controllability, and instant output power generation of the quadrotor. In this paper, to keep the quadrotor stable even under suddenly drastic disturbances, a novel control framework is presented to by integrating the advantages of active disturbance rejection control (ADRC) as well as geometric control for a quadrotor with bidirectional thrust capabilities. Moreover, to strengthen the adaptability under significant disturbances, a novel switching strategy is introduced into the control framework by virtue of the quadrotor’s bidirectional thrust capabilities. The ADRC scheme is performed when the disturbances are within a range; alternatively, if the disturbances surpass the preset range and the desired control is beyond the ultimate output of the quadrotor, the quadrotor compliantly responds by executing a 180° flip reverse flight to handle such drastic disturbances. Numerical and real-world experiments demonstrate that the proposed robust control strategy has superior performance adapts to instantaneously drastic disturbances.},
  archive   = {C_ICRA},
  author    = {Zujian Chen and Shaolin Mo and Botao Zhang and Jiyu Li and Hui Cheng},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611241},
  month     = {5},
  pages     = {6186-6192},
  title     = {Robust control for bidirectional thrust quadrotors under instantaneously drastic disturbances},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). End-to-end reinforcement learning for time-optimal
quadcopter flight. <em>ICRA</em>, 6172–6177. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Aggressive time-optimal control of quadcopters poses a significant challenge in the field of robotics. The state-of-the-art approach leverages reinforcement learning (RL) to train optimal neural policies. However, a critical hurdle is the sim-to-real gap, often addressed by employing a robust inner loop controller —an abstraction that, in theory, constrains the optimality of the trained controller, necessitating margins to counter potential disturbances. In contrast, our novel approach introduces high-speed quadcopter control using end-to-end RL (E2E) that gives direct motor commands. To bridge the reality gap, we incorporate a learned residual model and an adaptive method that can compensate for modeling errors in thrust and moments. We compare our E2E approach against a state-of-the-art network that commands thrust and body rates to an INDI inner loop controller, both in simulated and real-world flight. E2E showcases a significant 1.39-second advantage in simulation and a 0.17-second edge in real-world testing, highlighting end-to-end reinforcement learning’s potential. The performance drop observed from simulation to reality shows potential for further improvement, including refining strategies to address the reality gap or exploring offline reinforcement learning with real flight data.},
  archive   = {C_ICRA},
  author    = {Robin Ferede and Christophe De Wagter and Dario Izzo and Guido C.H.E. de Croon},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611665},
  month     = {5},
  pages     = {6172-6177},
  title     = {End-to-end reinforcement learning for time-optimal quadcopter flight},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Particle filter with stable embedding for state estimation
of the rigid body attitude system on the set of unit quaternions.
<em>ICRA</em>, 6166–6171. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel method for state estimation of rigid body attitude system evolving on the manifold S 3 , which is crucial in robotics and drone applications. We introduce a particle filter with stable embedding that extends the system into Euclidean space while ensuring stability of the manifold. Our particle filter with stable embedding enables accurate state estimation by maintaining estimated state values in close proximity to the manifold, while requiring significantly fewer computational resources than the standard exponential-map-based method that keeps state estimates on the manifold. Furthermore, our method facilitates the application of usual techniques designed for particle filters in Euclidean spaces, to the manifold system, as is, without any modification. The accuracy and the efficiency of our particle filter are confirmed both by simulation and by real drone experiments.},
  archive   = {C_ICRA},
  author    = {Hee-Deok Jang and Jae-Hyeon Park and Dong Eui Chang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610922},
  month     = {5},
  pages     = {6166-6171},
  title     = {Particle filter with stable embedding for state estimation of the rigid body attitude system on the set of unit quaternions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling and control of PADUAV: A passively articulated dual
UAVs platform for aerial manipulation*. <em>ICRA</em>, 6159–6165. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce PADUAV, a novel 5-DOF aerial platform designed to overcome the limitations of traditional tiltrotor vehicles. PADUAV features a unique mechanical design that incorporates two off-the-shelf quadrotors passively articulated to a rigid frame. This innovation enables free pitch rotation without mechanical constraints like cable winding, significantly enhancing its capabilities for various tasks. To control PADUAV’s 5 degrees of freedom, we propose a versatile and straightforward 5-DOF geometric tracking control strategy that generates 2D force and 3D torque. A decomposition approach is designed to distribute the output to the torque and thrust commands for each subplane, with no need for complex optimization. We validate our approach through three simulation experiments conducted in the Gazebo environment, leveraging the utilities provided by the RotorS simulator. These experiments not only demonstrate the feasibility of our platform but also provide new perspectives for future aerial platform development, particularly in terms of simulation-based approaches.},
  archive   = {C_ICRA},
  author    = {Jiali Sun and Kaidi Wang and Chuanbeibei Shi and Xiujia Li and Xiaojian Yi and Yushu Yu and Fuchun Sun and Yiqun Dong},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610094},
  month     = {5},
  pages     = {6159-6165},
  title     = {Modeling and control of PADUAV: A passively articulated dual UAVs platform for aerial manipulation*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Passive aligning physical interaction of fully-actuated
aerial vehicles for pushing tasks. <em>ICRA</em>, 6152–6158. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, the utilization of aerial manipulators for performing pushing tasks in non-destructive testing (NDT) applications has seen significant growth. Such operations entail physical interactions between the aerial robotic system and the environment. End-effectors with multiple contact points are often used for placing NDT sensors in contact with a surface to be inspected. Aligning the NDT sensor and the work surface while preserving contact, requires that all available contact points at the end-effector tip are in contact with the work surface. With a standard full-pose controller, attitude errors often occur due to perturbations caused by modeling uncertainties, sensor noise, and environmental uncertainties. Even small attitude errors can cause a loss of contact points between the end-effector tip and the work surface. To preserve full alignment amidst these uncertainties, we propose a control strategy which selectively deactivates angular motion control and enables direct force control in specific directions. In particular, we derive two essential conditions to be met, such that the robot can passively align with flat work surfaces achieving full alignment through the rotation along non-actively controlled axes. Additionally, these conditions serve as hardware design and control guidelines for effectively integrating the proposed control method for practical usage. Real world experiments are conducted to validate both the control design and the guidelines.},
  archive   = {C_ICRA},
  author    = {Tong Hui and Eugenio Cuniato and Michael Pantic and Marco Tognon and Matteo Fumagalli and Roland Siegwart},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610711},
  month     = {5},
  pages     = {6152-6158},
  title     = {Passive aligning physical interaction of fully-actuated aerial vehicles for pushing tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Harnessing the differential flatness of monocopter dynamics
for the purpose of trajectory tracking in a stable invertible coaxial
actuated ROtorcraft (SICARO). <em>ICRA</em>, 6145–6151. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, the dynamics of an emerging class of rotating nature-inspired micro aerial vehicles known as the Monocopter is proven and shown to be differentially flat. By exploiting this phenomenon, trajectory tracking can now be implemented on Monocopters via feed-forward terms that are computed per the trajectory. To demonstrate this, a Monocopter in the form of a Stable Invertible Coaxial Actuated ROtorcraft (SICARO) is chosen to harness this approach fully. The SICARO is capable of flying with either side of the wing facing up and this feature determines the craft’s direction of rotation about its body Z axis as well. In addition, it has the unique feature of a coaxial motor configuration that allows for a pitching-up moment regardless of the wing side facing up. The feed-forward terms computed are fused into a cascaded nonlinear controller on the craft to ensure its effectiveness in tracking trajectories. Lastly, the flight experiments extend to both sides of the wing to validate this method as being applicable for trajectory tracking for Monocopters such as the SICARO which has an extended range of flying capabilities.},
  archive   = {C_ICRA},
  author    = {Emmanuel Tang and Wei Jun Ang and Kian Wee Tan and Shaohui Foong},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611251},
  month     = {5},
  pages     = {6145-6151},
  title     = {Harnessing the differential flatness of monocopter dynamics for the purpose of trajectory tracking in a stable invertible coaxial actuated ROtorcraft (SICARO)},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Angler: An autonomy framework for intervention tasks with
lightweight underwater vehicle manipulator systems. <em>ICRA</em>,
6126–6132. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Developing autonomous intervention capabilities for lightweight underwater vehicle manipulator systems (UVMS) has garnered significant attention within recent years because of the opportunity for these systems to reduce intervention operating costs. Developing autonomous UVMS capabilities is challenging, however, because of the lack of available standardized software frameworks and pipelines. Previous works offer simulation environments and deployment pipelines for underwater vehicles, but fall short of providing a complete UVMS software framework. We address this gap by creating Angler: a software framework for developing localization, control, and decision-making algorithms with support for sim-to-real transfer. We validate this framework by implementing a state-of-the-art control architecture and demonstrate the ability to perform station keeping with a mean error below 0.25 m and waypoint tracking with an average final error of 0.398 m.},
  archive   = {C_ICRA},
  author    = {Evan Palmer and Christopher Holm and Geoffrey Hollinger},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610184},
  month     = {5},
  pages     = {6126-6132},
  title     = {Angler: An autonomy framework for intervention tasks with lightweight underwater vehicle manipulator systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Benchmarking classical and learning-based multibeam point
cloud registration. <em>ICRA</em>, 6118–6125. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep learning has shown promising results for multiple 3D point cloud registration datasets. However, in the underwater domain, most registration of multibeam echo-sounder (MBES) point cloud data are still performed using classical methods in the iterative closest point (ICP) family. In this work, we curate and release DotsonEast Dataset, a semi-synthetic MBES registration dataset constructed from an autonomous underwater vehicle in West Antarctica. Using this dataset, we systematically benchmark the performance of 2 classical and 4 learning-based methods. The experimental results show that the learning-based methods work well for coarse alignment, and are better at recovering rough transforms consistently at high overlap (20-50%). In comparison, GICP (a variant of ICP) performs well for fine alignment and is better across all metrics at extremely low overlap (10%). To the best of our knowledge, this is the first work to benchmark both learning-based and classical registration methods on an AUV-based MBES dataset. To facilitate future research, both the code and data are made available online. 1},
  archive   = {C_ICRA},
  author    = {Li Ling and Jun Zhang and Nils Bore and John Folkesson and Anna Wåhlin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610118},
  month     = {5},
  pages     = {6118-6125},
  title     = {Benchmarking classical and learning-based multibeam point cloud registration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Underwater dome-port camera calibration: Modeling of
refraction and offset through n-sphere camera model. <em>ICRA</em>,
6110–6117. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The optical effects that are observed in underwater imagery are more complex than those in-air. This is partially because we enclose most underwater cameras in a watertight enclosure, such as a hemispheric dome window. We then observe optical issues including the distortion effects of the lens, e.g., wide-angle field-of-view (FOV), the refractive effects at the enclosure (water-acrylic and acrylic-air) interfaces, and offset effects of a non-centered camera with respect to the dome. In this paper, we present an N-Sphere (NS) and Shifted N-Sphere (S-NS) camera models, tailored to these cameras and lenses mounted in water-tight dome enclosures. The proposed camera models treat each layer of effects as a ‘sphere’ that a 3D point will project on. Furthermore, the S-NS model includes additional parameters to address the camera offset variability. The versatility of the NS model makes it applicable to various lenses, as validated with fisheye (FOV &gt;120°) and wide-FOV (FOV ≈ 120°). We validated our models with different in-water calibration sequences, lenses, and housing setups, as well as with comparisons with other state-of-the-art camera models. Additionally, we demonstrated the performance of our proposed models in an example stereo-based visual odometry application. The low computational load of the proposed models makes it ideal for integrating in real-time visual navigation and reconstruction frameworks. We provide full math derivations of the proposed models as well as example C++ header files 1 for easy incorporation in independent projects.},
  archive   = {C_ICRA},
  author    = {Monika Roznere and Adithya K. Pediredla and Samuel E. Lensgraf and Yogesh Girdhar and Alberto Quattrini Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611338},
  month     = {5},
  pages     = {6110-6117},
  title     = {Underwater dome-port camera calibration: Modeling of refraction and offset through N-sphere camera model},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust model predictive control with control barrier
functions for autonomous surface vessels. <em>ICRA</em>, 6089–6095. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In autonomous robot navigation, the trajectories from path planners are considered to be safe regions, and deviations could endanger vessels. Model Predictive Control (MPC) stands as a popular choice for trajectory tracking problems as it naturally addresses operational constraints, such as dynamics and control constraints. Nevertheless, achieving robustness in changing environments like oceans and rivers, which are constantly subject to significant external disturbances, remains an ongoing challenge for MPC. It must consistently keep the system within a predefined safe region (such as a reference trajectory) even in the presence of model inaccuracies and perturbations. To address this challenge, we present a robust model predictive control strategy utilizing Control Barrier Functions (CBFs), which increases the disturbance-rejection abilities. We verify our method on an autonomous surface vessel in simulation and natural waters, both with external disturbances. Specifically, compared with the traditional MPC method, our proposed MPC-CBF strategy reduces tracking errors by 17.82% and 40.26% in simulations and field experiments, respectively. Although the control effort slightly increases by 7.78% and 4.20%, respectively, these results clearly demonstrate the enhanced resilience of MPC-CBF to disturbances.},
  archive   = {C_ICRA},
  author    = {Wei Wang and Wei Xiao and Alejandro Gonzalez-Garcia and Jan Swevers and Carlo Ratti and Daniela Rus},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610620},
  month     = {5},
  pages     = {6089-6095},
  title     = {Robust model predictive control with control barrier functions for autonomous surface vessels},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ReefGlider: A highly maneuverable vectored buoyancy engine
based underwater robot. <em>ICRA</em>, 6082–6088. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There exists a capability gap in the design of currently available autonomous underwater vehicles (AUV). Most AUVs use a set of thrusters, and optionally control surfaces, to control their depth and pose. AUVs utilizing thrusters can be highly maneuverable, making them well-suited to operate in complex environments such as in close-proximity to coral reefs. However, they are inherently power-inefficient and produce significant noise and disturbance. Underwater gliders, on the other hand, use changes in buoyancy and center of mass, in combination with a control surface to move around. They are extremely power efficient but not very maneuverable. Gliders are designed for long-range missions that do not require precision maneuvering. Furthermore, since gliders only activate the buoyancy engine for small time intervals, they do not disturb the environment and can also be used for passive acoustic observations. In this paper we present ReefGlider, a novel AUV that uses only buoyancy for control but is still highly maneuverable from additional buoyancy control devices. ReefGlider bridges the gap between the capabilities of thruster-driven AUVs and gliders. These combined characteristics make ReefGlider ideal for tasks such as long-term visual and acoustic monitoring of coral reefs. We present the overall design and implementation of the system, as well as provide analysis of some of its capabilities.},
  archive   = {C_ICRA},
  author    = {Kevin Macauley and Levi Cai and Peter Adamczyk and Yogesh Girdhar},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610140},
  month     = {5},
  pages     = {6082-6088},
  title     = {ReefGlider: A highly maneuverable vectored buoyancy engine based underwater robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hybrid dynamical model for robotic underwater vehicles
when submerged or surfaced: Approach and preliminary evaluation.
<em>ICRA</em>, 6058–6064. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper reports a numerical method for modeling underwater vehicle (UV) interactions with the free surface using a finite-dimensional dynamical plant model. Although finite-dimensional plant models of fully submerged UV behavior are well-established, they are unable to model the ubiquitous condition of a UV operating at or near the free surface. We report a Monte Carlo-based hybrid model approach for calculating the buoyancy and righting moment of a partially or fully submerged UV in order to model interactions with the free surface. We also report a preliminary evaluation of the hybrid model in numerical simulations, comparing the hybrid model’s performance to that of a model for fully submerged UVs and to the experimentally observed behavior of an actual vehicle while fully submerged and while interacting with the free surface. The results of this preliminary study suggest that the proposed hybrid approach may offer a simple and practical method for modeling UV behavior when submerged or interacting with the free surface.},
  archive   = {C_ICRA},
  author    = {James E. Hunt and Louis L. Whitcomb},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610798},
  month     = {5},
  pages     = {6058-6064},
  title     = {A hybrid dynamical model for robotic underwater vehicles when submerged or surfaced: Approach and preliminary evaluation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An augmented catenary model for underwater tethered robots.
<em>ICRA</em>, 6051–6057. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper examines the relevance of using catenary-based curves to model cables in underwater tethered robotic applications in order to take into account the influence of hydrodynamic damping. To this end, an augmented catenary-based model is introduced to deal with the dynamical effects of surge motion, sway motion or a combination of both on a cable. Experimental studies are carried out with eight cables of varying stiffness, weight and buoyancy. One end of the cable is fixed, while the other end is moved by the underwater robot. The obtained results help to determine which cables and which dynamics are compatible with a fair estimation of the cable shape through the proposed models.},
  archive   = {C_ICRA},
  author    = {Martin Filliung and Juliette Drupt and Charly Peraud and Claire Dune and Nicolas Boizot and Andrew Comport and Cedric Anthierens and Vincent Hugel},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611132},
  month     = {5},
  pages     = {6051-6057},
  title     = {An augmented catenary model for underwater tethered robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Microrobotic flight enabled by ultralight ion thrusters with
high thrust-to-weight ratio and low fabrication cost. <em>ICRA</em>,
6036–6042. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Flying microrobots have garnered growing research interest owing to their technological intricacies and suitability for various applications leveraging miniaturized size. Electrohydrodynamic (EHD) thrust offers advantages by generating propulsion without moving parts, but real-world use is limited by insufficient thrust generation, manufacturing challenges, fragility, and cost. This work presents the design and development of an optimized ion-propelled flying microrobot that excels in low weight, high thrust-to-weight ratio, and cost efficiency. Regarding design, multiphysics simulations guided structural optimization to increase thrust while decreasing weight. For materials, metal-coated polyethylene terephthalate (PET) film was selected to leverage the combined merits of metal conductivity and polymer flexibility, light weight, and low cost, enabling further weight reduction, easy assembly, robustness, and cost-effectiveness. Various experiments, including voltage-current measurements, ionic wind speed, thrust quantification, and airflow visualization, directed design refinements and validated performance. Through structural optimization, the maximum wind speed attained 2.25 m/s. Flight demonstrations with payloads evidenced the microrobot can stably fly at an inherent 16 mg weight while carrying an additional 72 mg load, achieving a record 5.5 thrust-to-weight ratio. These results open possibilities to incorporate microelectronics, enabling autonomous flight functionality.},
  archive   = {C_ICRA},
  author    = {Yang Gu and Xianfa Cai and Khadga Thakuri and Wenyu Yang and Yufeng Guo and Wei Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611166},
  month     = {5},
  pages     = {6036-6042},
  title     = {Microrobotic flight enabled by ultralight ion thrusters with high thrust-to-weight ratio and low fabrication cost},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dragonfly-inspired flapping wing robot mimicking force
vector control approach. <em>ICRA</em>, 6029–6035. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dragonflies show impressive flying skills by achieving both high efficiency and agility. They can perform distinctive flight maneuvers, such as flying backwards, which has proven to be achieved through &quot;force vectoring&quot; mechanism recently. In this paper, to explore the agile flight ability of dragonflies on man-made flapping wing systems, we designed, optimized and fabricated a dragonfly-inspired flapping wing robot (DFWR) with inclinable stroke plane control degrees. The proposed platform employs a four-wing configuration, each of which integrates an extra servo motor to enable the rotation of the flapping plane and imitate the &quot;force vectoring&quot; mechanism. Besides, referring to the flapping kinematics of dragonflies, the installation angle and wing pitch angle of the proposed DFWR are optimized considering the total lift and energy consumption through multiobjective optimization based on NSGA-II method. The &quot;force vector&quot; produced by the proposed platform has been illustrated through both theoretical method and experimental method. Moreover, the feasibility of the design is further verified through a series of operation validation experiments. Such a robot has the potential to provide a highly biomimetic platform to validate the flight mechanism studying of Odonata as well as the relative on-board applications such as bio-inspired vision.},
  archive   = {C_ICRA},
  author    = {Fangyuan Liu and Song Li and Jinwu Xiang and Daochun Li and Zhan Tu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610326},
  month     = {5},
  pages     = {6029-6035},
  title     = {A dragonfly-inspired flapping wing robot mimicking force vector control approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Direct learning of home vector direction for insect-inspired
robot navigation. <em>ICRA</em>, 6022–6028. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Insects have long been recognized for their ability to navigate and return home using visual cues from their nest’s environment. However, the precise mechanism underlying this remarkable homing skill remains a subject of ongoing investigation. Drawing inspiration from the learning flights of honey bees and wasps, we propose a robot navigation method that directly learns the home vector direction from visual percepts during a learning flight in the vicinity of the nest. After learning, the robot will travel away from the nest, come back by means of odometry, and eliminate the resultant drift by inferring the home vector orientation from the currently experienced view. Using a compact convolutional neural network, we demonstrate successful learning in both simulated and real forest environments, as well as successful homing control of a simulated quadrotor. The average errors of the inferred home vectors in general stay well below the 90° required for successful homing, and below 24° if all images contain sufficient texture and illumination. Moreover, we show that the trajectory followed during the initial learning flight has a pronounced impact on the network’s performance. A higher density of sample points in proximity to the nest results in a more consistent return. Code and data are available at https://mavlab.tudelft.nl/learning_to_home.},
  archive   = {C_ICRA},
  author    = {Michiel V. M. Firlefyn and Jesse J. Hagenaars and Guido C. H. E. De Croon},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611609},
  month     = {5},
  pages     = {6022-6028},
  title     = {Direct learning of home vector direction for insect-inspired robot navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VLEIBot: A new 45-mg swimming microrobot driven by a
bioinspired anguilliform propulsor. <em>ICRA</em>, 6014–6021. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents the VLEIBot * (Very Little Eel-Inspired roBot), a 45-mg/23-mm 3 microrobotic swimmer that is propelled by a bioinspired anguilliform propulsor. The propulsor is excited by a single 6-mg high-work-density (HWD) microactuator and undulates periodically due to wave propagation phenomena generated by fluid-structure interaction (FSI) during swimming. The microactuator is composed of a carbon-fiber beam, which functions as a leaf spring, and shape-memory alloy (SMA) wires, which deform cyclically when excited periodically using Joule heating. The VLEIBot can swim at speeds as high as 15.1 mm • s −1 (0.33 Bl • s −1 ) when driven with a heuristically-optimized propulsor. To improve maneuverability, we evolved the VLEIBot design into the 90-mg/47-mm 3 VLEIBot + , which is driven by two propulsors and fully controllable in the two-dimensional (2D) space. The VLEIBot + can swim at speeds as high as 16.1 mm • s –1 (0.35 Bl • s –1 ), when driven with heuristically-optimized propulsors, and achieves turning rates as high as 0.28 rad • s –1 , when tracking path references. The measured root-mean-square (RMS) values of the tracking errors are as low as 4 mm.},
  archive   = {C_ICRA},
  author    = {Elijah K. Blankenship and Conor K. Trygstad and Francisco M. F. R. Gonçalves and Néstor O. Pérez-Arancibia},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610895},
  month     = {5},
  pages     = {6014-6021},
  title     = {VLEIBot: A new 45-mg swimming microrobot driven by a bioinspired anguilliform propulsor},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-speed interfacial flight of an insect-scale robot.
<em>ICRA</em>, 6006–6013. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Several insect species are able to locomote across the air-water interface by leveraging surface tension to remain above the water surface. A subset of these insects, such as the stonefly and waterlily beetle, flap their wings to actively move around the two dimensional surface — a locomotion strategy referred to as interfacial flight. Here, we present an insect-scale robot, the γ-bot, inspired by these interfacial fliers. The robot is comprised of a flapping-wing vehicle that generates a thrust force parallel to the water surface, and three passive legs utilize surface tension to support the body mass and maintain contact with the air-water interface. We developed and validated a simple model to characterize the drag forces acting on the vehicle and estimate the robot’s velocity. This 112 mg robot can reach maximum velocities of 0.9 ms −1 (corresponding to 15 BLs −1 ) and can initiate both left and right turns, demonstrating high maneuverability along the air-water interface. In addition, the robot can carry an additional 419 mg, enabling future sensing, control, and power autonomous operation.},
  archive   = {C_ICRA},
  author    = {Hang Gao and Sunghwan Jung and E. Farrell Helbling},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611592},
  month     = {5},
  pages     = {6006-6013},
  title     = {High-speed interfacial flight of an insect-scale robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-modal jumping and crawling in an autonomous,
springtail-inspired microrobot. <em>ICRA</em>, 5999–6005. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Springtails are tiny arthropods that crawl and jump. They jump by temporarily storing elastic energy in resilin elastic cuticular structures and releasing that energy to accelerate a tail, called a furca, propelling them in the air. This paper presents an autonomous, springtail-inspired microrobot that can crawl and jump. The microrobot has a mass of 980mg and stands 13mm tall, and has on-board sensing, computation, and power, enabling autonomy. The microrobot was designed with a super-elastic shape memory alloy (SMA) spring that is manually loaded to store elastic energy. The on-board sensing and computation triggers an actuator at the jump frequency range that unlatches the spring, launching the microrobot into the air at speeds up to 3.171ms −1 . At the same time, the microrobot is capable of crawling, when actuated at frequencies lower or higher than the jump frequency range, demonstrating autonomous multi-modal locomotion. This work opens up new pathways toward autonomy in multi-modal microrobots.},
  archive   = {C_ICRA},
  author    = {Shashwat Singh and Zeynep Temel and Ryan St. Pierre},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610130},
  month     = {5},
  pages     = {5999-6005},
  title     = {Multi-modal jumping and crawling in an autonomous, springtail-inspired microrobot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RB5 low-cost explorer: Implementing autonomous long-term
exploration on low-cost robotic hardware. <em>ICRA</em>, 5977–5983. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This systems paper presents the implementation and design of RB5, a wheeled robot for autonomous long-term exploration with fewer and cheaper sensors. Requiring just an RGB-D camera and low-power computing hardware, the system consists of an experimental platform with rocker-bogie suspension. It operates in unknown and GPS-denied environments and on indoor and outdoor terrains. The exploration consists of a methodology that extends frontier- and sampling-based exploration with a path-following vector field and a state-of-the-art SLAM algorithm. The methodology allows the robot to explore its surroundings at lower update frequencies, enabling the use of lower-performing and lower-cost hardware while still retaining good autonomous performance. The approach further consists of a methodology to interact with a remotely located human operator based on an inexpensive long-range and low-power communication technology from the internet-of-things domain (i.e., LoRa) and a customized communication protocol. The results and the feasibility analysis show the possible applications and limitations of the approach.Code—The open-source software stack is made available on the project repository webpage † .},
  archive   = {C_ICRA},
  author    = {Adam Seewald and Marvin Chancán and Connor M. McCann and Seonghoon Noh and Omeed Fallahi and Hector Castillo and Ian Abraham and Aaron M. Dollar},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610399},
  month     = {5},
  pages     = {5977-5983},
  title     = {RB5 low-cost explorer: Implementing autonomous long-term exploration on low-cost robotic hardware},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Wireless communication infrastructure building for mobile
robot search and inspection missions. <em>ICRA</em>, 5970–5976. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the paper, we address wireless communication infrastructure building by relay placement based on approaches utilized in wireless network sensors. The problem is motivated by search and inspection missions with mobile robots, where known sensing ranges may be exploited. We investigate the relay placement, establishing network connectivity to support robust food-based communication routing. The proposed method decomposes the given area into Open space and Corridor space where specific deployment patterns allow for guaranteed k-connectivity, making the resulting network redundant while keeping channel utilization bounded. In particular, a hexagonal tesselation coverage pattern with 3-connectivity is investigated in Open space and a linear 4-connectivity pattern in Corridor space, respectively. The proposed approach is empirically evaluated in a realistic scenario, and based on the reported results, it is found superior compared to the existing stochastic randomized dual sampling schema.},
  archive   = {C_ICRA},
  author    = {Martin Zoula and Jan Faigl},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611561},
  month     = {5},
  pages     = {5970-5976},
  title     = {Wireless communication infrastructure building for mobile robot search and inspection missions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Multi-robot search in a 3D environment with intersection
system constraints. <em>ICRA</em>, 5963–5969. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Efficient task allocation is a challenge for multirobot search. The multi-robot search problem is reformulated as submodular maximization subject to intersection system constraints. The objective function is submodular and consists of a coverage function to cover environments and a balancing function to efficiently dispatch robots. The intersection system is composed of routing and clustering constraints. The experiment results show that the proposed approach outperforms state-ofthe-art methods in multi-robot search.},
  archive   = {C_ICRA},
  author    = {Yan-Shuo Li and Kuo-Shih Tseng},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610393},
  month     = {5},
  pages     = {5963-5969},
  title     = {Multi-robot search in a 3D environment with intersection system constraints},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Computation-aware multi-object search in 3D space using
submodular tree. <em>ICRA</em>, 5956–5962. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Searching for targets in 3D environments can be formulated as submodular maximization problems with routing constraints. However, it involves solving two NP-hard problems: the maximal coverage problem and the traveling salesman problem. Since the time constraint is critical for search problems, this research proposes a Computation-Aware Search for Multiple Objects (CASMO) algorithm to further consider the computational time in the cost constraints. Due to the submdularity, the greedy algorithm achieves $\frac{1}{2}\left( {1 - \frac{1}{e}} \right)\overline {OPT} $, where $\overline {OPT} $ is the approximate optimum. The experiment results show that the proposed algorithm outperforms state-of-the-art approaches in multi-object search.},
  archive   = {C_ICRA},
  author    = {Yan-Shuo Li and Kuo-Shih Tseng},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610369},
  month     = {5},
  pages     = {5956-5962},
  title     = {Computation-aware multi-object search in 3D space using submodular tree},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). STAGE: Scalable and traversability-aware graph based
exploration planner for dynamically varying environments. <em>ICRA</em>,
5949–5955. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this article, we propose a novel navigation framework that leverages a two layered graph representation of the environment for efficient large-scale exploration, while it integrates a novel uncertainty awareness scheme to handle dynamic scene changes in previously explored areas. The framework is structured around a novel goal oriented graph representation, that consists of, i) the local sub-graph and ii) the global graph layer respectively. The local sub-graphs encode local volumetric gain locations as frontiers, based on the direct pointcloud visibility, allowing fast graph building and path planning. Additionally, the global graph is build in an efficient way, using node-edge information exchange only on overlapping regions of sequential sub-graphs. Different from the state-of-the-art graph based exploration methods, the proposed approach efficiently re-uses sub-graphs built in previous iterations to construct the global navigation layer. Another merit of the proposed scheme is the ability to handle scene changes (e.g. blocked pathways), adaptively updating the obstructed part of the global graph from traversable to not-traversable. This operation involved oriented sample space of a path segment in the global graph layer, while removing the respective edges from connected nodes of the global graph in cases of obstructions. As such, the exploration behavior is directing the robot to follow another route in the global re-positioning phase through path-way updates in the global graph. Finally, we showcase the performance of the method both in simulation runs as well as deployed in real-world scene involving a legged robot carrying camera and lidar sensor.},
  archive   = {C_ICRA},
  author    = {Akash Patel and Mario A. V. Saucedo and Christoforos Kanellakis and George Nikolakopoulos},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610939},
  month     = {5},
  pages     = {5949-5955},
  title     = {STAGE: Scalable and traversability-aware graph based exploration planner for dynamically varying environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cellular-enabled collaborative robots planning and
operations for search-and-rescue scenarios. <em>ICRA</em>, 5942–5948.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mission-critical operations, particularly in the context of Search-and-Rescue (SAR) and emergency response situations, demand optimal performance and efficiency from every component involved to maximize the success probability of such operations. In these settings, cellular-enabled collaborative robotic systems have emerged as invaluable assets, assisting first responders in several tasks, ranging from victim localization to hazardous area exploration. However, a critical limitation in the deployment of cellular-enabled collaborative robots in SAR missions is their energy budget, primarily supplied by batteries, which directly impacts their task execution and mobility. This paper tackles this problem, and proposes a search-and-rescue framework for cellular-enabled collaborative robots use cases that, taking as input the area size to be explored, the robots fleet size, their energy profile, exploration rate required and target response time, finds the minimum number of robots able to meet the SAR mission goals and the path they should follow to explore the area. Our results, i) show that first responders can rely on a SAR cellular-enabled robotics framework when planning mission-critical operations to take informed decisions with limited resources, and, ii) illustrate the number of robots versus explored area and response time trade-off depending on the type of robot: wheeled vs quadruped.},
  archive   = {C_ICRA},
  author    = {Arnau Romero and Carmen Delgado and Lanfranco Zanzi and Raúl Suárez and Xavier Costa-Pérez},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611179},
  month     = {5},
  pages     = {5942-5948},
  title     = {Cellular-enabled collaborative robots planning and operations for search-and-rescue scenarios},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mechanism design for new sensors field deployment by
LineRanger powerline robot. <em>ICRA</em>, 5927–5933. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Powerline robotics is slowly becoming key tools for electric utilities. Contrary to drones that are usually limited to inspection tasks, wheeled robots like LineRanger can perform a broader range of applications. In this paper, a suite of mechanical devices is featured, as several new asset management tasks were recently added to LineRanger’s capabilities. While previous applications focused on non-contact inspection (visual, electro-magnetic, etc.), the new tasks at hand involved reaching adjacent conductors to probe line components with micro-Ohmmeter, installing and retrieving custom build sensors for multi-day line monitoring, and assessing aging conductors surface properties, to refine their thermal model and optimize the line capacity during heat waves. All three applications were recently field validated onto LineRanger, and mechanical design insights shall be presented for each module.},
  archive   = {C_ICRA},
  author    = {Pierre-Luc Richard and Jonathan Bellemare and Philippe Hamelin and Camille Hébert and Ghislain Lambert and Samuel Lavoie and Sébastien Leprohon and Matthieu Montfrond and Marion Nourry and Alex Sartor and Nicolas Pouliot},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610764},
  month     = {5},
  pages     = {5927-5933},
  title     = {Mechanism design for new sensors field deployment by LineRanger powerline robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Autonomous robotic re-alignment for face-to-face underwater
human-robot interaction*. <em>ICRA</em>, 5920–5926. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The use of autonomous underwater vehicles (AUVs) to accomplish traditionally challenging and dangerous tasks has proliferated thanks to advances in sensing, navigation, manipulation, and on-board computing technologies. Utilizing AUVs in underwater human-robot interaction (UHRI) has witnessed comparatively smaller levels of growth due to limitations in bi-directional communication and significant technical hurdles to bridge the gap between analogies with terrestrial interaction strategies and those that are possible in the underwater domain. A necessary component to support UHRI is establishing a system for safe robotic-diver approach to establish face-to-face communication that considers nonstandard human body pose. In this work, we introduce a stereo vision system for enhancing UHRI that utilizes threedimensional reconstruction from stereo image pairs and machine learning for localizing human joint estimates. We then establish a convention for a coordinate system that encodes the direction the human is facing with respect to the camera coordinate frame. This allows automatic setpoint computation that preserves human body scale and can be used as input to an image-based visual servo control scheme. We show that our setpoint computations tend to agree both quantitatively and qualitatively with experimental setpoint baselines. The methodology introduced shows promise for enhancing UHRI by improving robotic perception of human orientation underwater.},
  archive   = {C_ICRA},
  author    = {Demetrious T. Kutzke and Ashwin Wariar and Junaed Sattar},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610809},
  month     = {5},
  pages     = {5920-5926},
  title     = {Autonomous robotic re-alignment for face-to-face underwater human-robot interaction*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uncertainty-aware shape estimation of a surgical continuum
manipulator in constrained environments using fiber bragg grating
sensors. <em>ICRA</em>, 5913–5919. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Continuum Dexterous Manipulators (CDMs) are well-suited tools for minimally invasive surgery due to their inherent dexterity and reachability. Nonetheless, their flexible structure and non-linear curvature pose significant challenges for shape-based feedback control. The use of Fiber Bragg Grating (FBG) sensors for shape sensing has shown great potential in estimating the CDM’s tip position and subsequently reconstructing the shape using optimization algorithms. This optimization, however, is under-constrained and may be ill-posed for complex shapes, falling into local minima. In this work, we introduce a novel method capable of directly estimating a CDM’s shape from FBG sensor wavelengths using a deep neural network. In addition, we propose the integration of uncertainty estimation to address the critical issue of uncertainty in neural network predictions. Neural network predictions are unreliable when the input sample is outside the training distribution or corrupted by noise. Recognizing such deviations is crucial when integrating neural networks within surgical robotics, as inaccurate estimations can pose serious risks to the patient. We present a robust method that not only improves the precision upon existing techniques for FBG-based shape estimation but also incorporates a mechanism to quantify the models’ confidence through uncertainty estimation. We validate the uncertainty estimation through extensive experiments, demonstrating its effectiveness and reliability on out-of-distribution (OOD) data, adding an additional layer of safety and precision to minimally invasive surgical robotics.},
  archive   = {C_ICRA},
  author    = {Alexander Schwarz and Arian Mehrfard and Golchehr Amirkhani and Henry Phalen and Justin H. Ma and Robert B. Grupp and Alejandro Martin Gomez and Mehran Armand},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610024},
  month     = {5},
  pages     = {5913-5919},
  title     = {Uncertainty-aware shape estimation of a surgical continuum manipulator in constrained environments using fiber bragg grating sensors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Design and visual servoing control of a hybrid dual-segment
flexible neurosurgical robot for intraventricular biopsy. <em>ICRA</em>,
5906–5912. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traditional rigid endoscopes have challenges in flexibly treating tumors located deep in the brain, and low operability and fixed viewing angles limit its development. This study introduces a novel dual-segment flexible robotic endoscope MicroNeuro, designed to perform biopsies with dexterous surgical manipulation deep in the brain. Taking into account the uncertainty of the control model, an image-based visual servoing with online robot Jacobian estimation has been implemented to enhance motion accuracy. Furthermore, the application of model predictive control with constraints significantly bolsters the flexible robot’s ability to adaptively track mobile objects and resist external interference. Experimental results underscore that the proposed control system enhances motion stability and precision. Phantom testing substantiates its considerable potential for deployment in neurosurgery.},
  archive   = {C_ICRA},
  author    = {Jian Chen and Mingcong Chen and Qingxiang Zhao and Shuai Wang and Yihe Wang and Ying Xiao and Jian Hu and Danny Tat Ming Chan and Kam Tong Leo Yeung and David Yuen Chung Chan and Hongbin Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610302},
  month     = {5},
  pages     = {5906-5912},
  title     = {Design and visual servoing control of a hybrid dual-segment flexible neurosurgical robot for intraventricular biopsy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A magnetic continuum robot with in-situ magnetic
reprogramming capability. <em>ICRA</em>, 5891–5897. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Magnetic continuum robots (MCR) have shown great potential in minimally invasive interventions because they can be actively and remotely navigated through complex in vivo environments. However, the deformation capability of current MCRs is limited by fixed magnetization congurations, preventing them from accessing hard-to-reach areas. This is due to the fact that under a global magnetic field, fixed magnetization conguration causes the magnets on the MCRs exposed to coupled magnetic forces and torques, resulting in a lack of controllable degrees of freedom. Here, we introduce a reprogrammable magnetic continuum robot (RMCR) enabled by magnetic reprogramming modules (MRM). Actuated by shape memory alloys, the magnetic moment direction of MRMs can be selectively reprogrammed in real-time and in-situ. Magnetic reprogramming capabilities enable the RMCR to achieve complex shape transformations. Results show that the range of motion in the tip direction of the RMCR increases by 193% compared with regular MCR. Besides, MRMs on the RMCR can achieve active attraction and separation under simple magnetic fields. The reprogramming process of the RMCR is theoretically investigated. A design methodology for MRMs is then proposed and the fabrication process of RMCR is described in detail. Furthermore, a kinematic model of the RMCR is established, simulated, and experimentally validated.},
  archive   = {C_ICRA},
  author    = {Junnan Xue and Moqiu Zhang and Xurui Liu and Jiaqi Zhu and Yanfei Cao and Li Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611450},
  month     = {5},
  pages     = {5891-5897},
  title     = {A magnetic continuum robot with in-situ magnetic reprogramming capability},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A three-dimensional compliant bowtie-shaped mechanical
amplifier to magnify coaxial displacement in a confined space.
<em>ICRA</em>, 5884–5890. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a novel form of a three-dimensional coaxial bowtie-shaped mechanical amplifier. The proposed model incorporates a lever mechanism into the Sarrus linkage structure. It allows the target plate to move along one axis with amplified displacement in a parallel manner. The amplifier was assembled after machining the components using a computer numerical control machine. A flexible hinge was incorporated into the amplifier design for simplified fabrication and reduced friction in the actuation mechanism. Castigliano’s theorem is used to build a mathematical model of the proposed mechanical amplifier, and the performance was validated through finite element analysis and prototype fabrication. We achieved the amplification ratio of ×8.44, resulting in the axial displacement up to 86 µm. The demonstrated amplifier is expected to apply to compact microsurgical robots or biomedical imaging apparatus requiring coaxial displacement amplification in confined spaces.},
  archive   = {C_ICRA},
  author    = {Jintaek Im and Eunsil Jang and Cheol Song},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611636},
  month     = {5},
  pages     = {5884-5890},
  title     = {A three-dimensional compliant bowtie-shaped mechanical amplifier to magnify coaxial displacement in a confined space},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A miniature 1R1T precision manipulator with remote center of
motion for minimally invasive surgery. <em>ICRA</em>, 5877–5883. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In robotic-assisted minimally invasive surgery, the remote center of motion (RCM) achieves precision and safe manipulation of surgical devices through the insertion point into the patient’s body. One of the RCM configurations, one-rotation and one-translation (1R1T) RCM based on a closed-loop design, enables two-degrees-of-freedom transmission from the proximal end of the robotic arm to the distal end. This feature offers important advantages, particularly in enhancing safety by minimizing physical contact risks with patients or other surgical tools owing to the simplified layout near the surgical field. However, conventional 1R1T RCM robots typically employ complex structures with numerous joints consisting of pin-and-hole mating mechanisms. This complexity can increase the overall size of the robot and compromise motion precision. This study presents a miniature 1R1T precision manipulator with RCM, ORIGANOID (dimensions: W60 x D120 x H30 mm, weight: 12.6 g). The robotic arm features flexure hinges, eliminates clearance issues, and can be fabricated using an origami-inspired robotic approach. Furthermore, using a novel backlash-free coupling method, the robotic arm could be easily attached and detached from the drive units. A prototype was fabricated and experimentally validated. The results demonstrated that high-resolution motion could be achieved within 10 μm. Furthermore, a demonstration using an eyeball model confirmed the successful implementation of 1R1T RCM.},
  archive   = {C_ICRA},
  author    = {Hiroyuki Suzuki},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610812},
  month     = {5},
  pages     = {5877-5883},
  title     = {A miniature 1R1T precision manipulator with remote center of motion for minimally invasive surgery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel SEA-based haptic interface for robot-assisted
vascular interventional surgery. <em>ICRA</em>, 5871–5876. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot-assisted vascular interventional surgery can isolate interventionists and X-ray radiation, and improve surgical accuracy. However, the leader side outside the operating room still has problems such as incomplete collection of operating information and unrealistic tactile feedback. The main objective of this paper is to design a haptic interface that can simultaneously capture the force-position information of the interventionists and generate force to assist the interventionists in performing surgeries on the leader side. It can capture the interventionists’ delivery displacement, twisting angle, clamping force, and provide real-time force feedback. A leader-follower bidirectional force feedback control strategy was proposed. Based on this strategy, on the one hand, the interventionist perceives the multi-modal information fed back from the follower side, makes judgments, and actively adjusts the surgical operation. On the other hand, the interventionist controls the grasping state of the instruments remotely to control the safety operating force threshold. Finally, the experimental setup was built and a series of evaluation experiments were performed. The experimental results verified the feasibility of the designed haptic interface. It can generate dynamic and accurate force feedback and realize leader-follower grasping force control.},
  archive   = {C_ICRA},
  author    = {Yonggan Yan and Shuxiang Guo and Chuqiao Lyu and Jian Guo and Jian Wang and Pengfei Yang and Yongwei Zhang and Yongxin Zhang and Jianmin Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611034},
  month     = {5},
  pages     = {5871-5876},
  title     = {A novel SEA-based haptic interface for robot-assisted vascular interventional surgery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inconstant curvature kinematics of parallel continuum robot
without static model. <em>ICRA</em>, 5864–5870. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the study of minimally invasive surgical robots, a mini parallel continuum robot has shown motion advantage after passing through a long and winding working channel. However, due to the interaction force between the elastic wires of the parallel robots during motion generation processes, the constant curvature assumption has shown modeling errors. This causes the current geometric kinematic model to become unreliable. Therefore, there is a need for a more accurate kinematic model in the absence of a complicated static model. This paper aims to solve this issue. The simulation in ANSYS is carried out, and the shape of one of the driving wires, when bending, is fitted by a two-segment polynomial curve. Then, the position of the distal wrist tip can be calculated based on the curve shape. To verify the accuracy of the proposed model, bending simulation and experiment are carried out. The accuracy of the proposed model is compared with that of the kinematic model based on constant curvature assumption. The result shows that the proposed model can get more accurate results, especially when the driving wire displacement increases. For a 10 mm parallel robot, when the displacements of the two pairs of wires are both 3.0 mm, the errors of the two models are 0.42 mm and 5.79 mm (4.2% and 57.9%), respectively.},
  archive   = {C_ICRA},
  author    = {Tao Zhang and Huxin Gao and Hongliang Ren},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610251},
  month     = {5},
  pages     = {5864-5870},
  title     = {Inconstant curvature kinematics of parallel continuum robot without static model},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A force-driven and vision-driven hybrid control method of
autonomous laparoscope-holding robot. <em>ICRA</em>, 5857–5863. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Laparoscope-holding robots significantly enhance the stability and precision of visualization in minimally invasive surgeries. Most existing robots of this kind depend on visual servo systems and struggle with efficient, rapid adjustments in the field-of-view (FOV), especially when identifying organs and needles outside the FOV. This paper presents a laparoscope-holding robot system capable of employing both vision-driven and force-driven mechanisms for continuous and large-scale FOV adjustments, respectively. The system features an integrated tactile handle, enabling the reception of human-robot interaction forces during surgical navigation. We propose a hybrid control method that leverages both force and vision inputs for laparoscopic FOV adjustments. This approach integrates a virtual wrench, generated from visual information, and an interaction wrench, obtained from the tactile handle, into the robot&#39;s dynamic model, which complies with remote center of motion constraints. The interaction wrench&#39;s gain is adjusted with the gripping force on the integrated tactile handle, ensuring that unintended movements caused by accidental contacts are prevented, thus safeguarding operational safety. The proposed method eliminates the need to switch control modes, enabling simultaneous visual tracking and tactile interaction guidance. Experimental results demonstrate that the proposed method not only allows for FOV adjustments with surgical instrument guiding but also adapts well to large-scale FOV adjustment tasks.},
  archive   = {C_ICRA},
  author    = {Jin Fang and Ling Li and Xiaojian Li and Hangjie Mo and Pengxin Guo and Xilin Xiao and Yanwei Qu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610509},
  month     = {5},
  pages     = {5857-5863},
  title     = {A force-driven and vision-driven hybrid control method of autonomous laparoscope-holding robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DRIVE: Data-driven robot input vector exploration.
<em>ICRA</em>, 5829–5836. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An accurate motion model is a fundamental component of most autonomous navigation systems. While much work has been done on improving model formulation, no standard protocol exists for gathering empirical data required to train models. In this work, we address this issue by proposing Data-driven Robot Input Vector Exploration (DRIVE), a protocol that enables characterizing uncrewed ground vehicles (UGVs) input limits and gathering empirical model training data. We also propose a novel learned slip approach outperforming similar acceleration learning approaches. Our contributions are validated through an extensive experimental evaluation, cumulating over 7km and 1.8h of driving data over three distinct UGVs and four terrain types. We show that our protocol offers increased predictive performance over common human-driven data-gathering protocols. Furthermore, our protocol converges with 46 s of training data, almost four times less than the shortest human dataset gathering protocol. We show that the operational limit for our model is reached in extreme slip conditions encountered on surfaced ice. DRIVE is an efficient way of characterizing UGV motion in its operational conditions. Our code and dataset are both available online at this link: https://github.com/norlab-ulaval/DRIVE.},
  archive   = {C_ICRA},
  author    = {Dominic Baril and Simon-Pierre Deschênes and Luc Coupal and Cyril Goffin and Julien Lépine and Philippe Giguère and François Pomerleau},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611172},
  month     = {5},
  pages     = {5829-5836},
  title     = {DRIVE: Data-driven robot input vector exploration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive contact-implicit model predictive control with
online residual learning. <em>ICRA</em>, 5822–5828. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The hybrid nature of multi-contact robotic systems, due to making and breaking contact with the environment, creates significant challenges for high-quality control. Existing model-based methods typically rely on either good prior knowledge of the multi-contact model or require significant offline model tuning effort, thus resulting in low adaptability and robustness. In this paper, we propose a real-time adaptive multi-contact model predictive control framework, which enables online adaption of the hybrid multi-contact model and continuous improvement of the control performance for contact-rich tasks. This framework includes an adaption module, which continuously learns a residual of the hybrid model to minimize the gap between the prior model and reality, and a real-time multi-contact MPC controller. We demonstrated the effectiveness of the framework in synthetic examples, and applied it on hardware to solve contact-rich manipulation tasks, where a robot uses its end-effector to roll different unknown objects on a table to track given paths. The hardware experiments show that with a rough prior model, the multi-contact MPC controller adapts itself on-the-fly with an adaption rate around 20 Hz and successfully manipulates previously unknown objects with non-smooth surface geometries. Accompanying media can be found at: https://sites.google.com/view/adaptive-contact-implicit-mpc/home},
  archive   = {C_ICRA},
  author    = {Wei-Cheng Huang and Alp Aydinoglu and Wanxin Jin and Michael Posa},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610416},
  month     = {5},
  pages     = {5822-5828},
  title     = {Adaptive contact-implicit model predictive control with online residual learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). K-BMPC: Derivative-based koopman bilinear model predictive
control for tractor-trailer trajectory tracking with unknown parameters.
<em>ICRA</em>, 5808–5813. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Nonlinear dynamics bring difficulties to controller design for control-affine systems such as tractor-trailer vehicles, especially when the parameters in the dynamics are unknown. To address this constraint, we propose a derivative-based lifting function construction method, show that the corresponding infinite dimensional Koopman bilinear model over the lifting function is equivalent to the original control-affine system. Further, we analyze the propagation and bounds of state prediction errors caused by the truncation in derivative order. The identified finite dimensional Koopman bilinear model would serve as predictive model in the next step. Koopman Bilinear Model Predictive control (K-BMPC) is proposed to solve the trajectory tracking problem. We linearize the bilinear model around the estimation of the lifted state and control input. Then the bilinear Model Predictive Control problem is approximated by a quadratic programming problem. Further, the estimation is updated at each iteration until the convergence is reached. Moreover, we implement our algorithm on a tractor-trailer system, taking into account the longitudinal and side slip effects. The open-loop simulation shows the proposed Koopman bilinear model captures the dynamics with unknown parameters and has good prediction performance. Closed-loop tracking results show the proposed K-BMPC exhibits elevated tracking precision with the commendable computational efficiency. The experimental results demonstrate the feasibility of K-BMPC.},
  archive   = {C_ICRA},
  author    = {Zehao Wang and Han Zhang and Jingchuan Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610320},
  month     = {5},
  pages     = {5808-5813},
  title     = {K-BMPC: Derivative-based koopman bilinear model predictive control for tractor-trailer trajectory tracking with unknown parameters},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforcement learning for reduced-order models of legged
robots. <em>ICRA</em>, 5801–5807. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Model-based approaches for planning and control for bipedal locomotion have a long history of success. It can provide stability and safety guarantees while being effective in accomplishing many locomotion tasks. Model-free reinforcement learning, on the other hand, has gained much popularity in recent years due to computational advancements. It can achieve high performance in specific tasks, but it lacks physical interpretability and flexibility in re-purposing the policy for a different set of tasks. For instance, we can initially train a neural network (NN) policy using velocity commands as inputs. However, to handle new task commands like desired hand or footstep locations at a desired walking velocity, we must retrain a new NN policy. In this work, we attempt to bridge the gap between these two bodies of work on a bipedal platform. We formulate a model-based reinforcement learning problem to learn a reduced-order model (ROM) within a model predictive control (MPC). Results show a 49% improvement in viable task region size and a 21% reduction in motor torque cost. All videos and code are available at https://sites.google.com/view/ymchen/research/rl-for-roms.},
  archive   = {C_ICRA},
  author    = {Yu-Ming Chen and Hien Bui and Michael Posa},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610747},
  month     = {5},
  pages     = {5801-5807},
  title     = {Reinforcement learning for reduced-order models of legged robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Stiffness-based hybrid motion/ force control for
cable-driven serpentine manipulator*. <em>ICRA</em>, 5795–5800. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, there has been a growing demand for robotic manipulators to perform tasks in various unstructured environments and situations requiring precision and force control. However, traditional robotic arms have limitations in fully leveraging their advantages in such scenarios. To address this demand, we have designed a cable-driven serpentine manipulator (CDSM) that combines force and precision motion control. This control method allows for precise manipulation of forces and torques at the end-effector, particularly in applications like electric vehicle charging and narrow-space exploration. It also enables independent control in multiple configurations. We achieve force-position hybrid control in task space, ensuring accurate control of end-effector force while achieving precise position control in other directions. Additionally, we implement joint angle closed-loop control in joint space to reduce the impact of cable elasticity deformation and friction on joint motion accuracy. Finally, servo control is applied at the lowest motor level. This paper investigates the modeling, sensing, and control of CDSM within a unified framework of hybrid motion/force control. Through experiments and simulations, we demonstrate the high accuracy and practicality of this control method in various scenarios.},
  archive   = {C_ICRA},
  author    = {Wenshuo Li and Wenfu Xu and Peisheng Huang and Boyang Lin and Bin Liang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611054},
  month     = {5},
  pages     = {5795-5800},
  title     = {Stiffness-based hybrid motion/ force control for cable-driven serpentine manipulator*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A reinforcement learning-based control strategy for robust
interaction of robotic systems with uncertain environments.
<em>ICRA</em>, 5788–5794. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the context of interaction with unmodelled systems, it becomes imperative for a robot controller to possess the capability to dynamically adjust its actions in real-time, enhancing its resilience in the face of fluctuating environmental conditions. This adaptation process must be performed in a stability-preserving fashion, and resourcefully exploit the knowledge acquired during the interaction process. In this article, we propose a novel control strategy, based on the synergistic usage of state-of-the-art passivity-based control and Deep Reinforcement Learning (DRL). The concept of energy tank is used to provide stability guarantees for the interaction controller with uncertain environments, while an online learning policy allows to properly estimate the requirements of the task and adapt the controller accordingly, thus simultaneously achieving stability and performance. The proposed architecture is successfully validated through simulations and experiments with a collaborative manipulator in a surface polishing task.},
  archive   = {C_ICRA},
  author    = {Diletta Sacerdoti and Federico Benzi and Cristian Secchi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610082},
  month     = {5},
  pages     = {5788-5794},
  title     = {A reinforcement learning-based control strategy for robust interaction of robotic systems with uncertain environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trajectory optimization strategy that considers body
tip-over stability, limb dynamics, and motion continuity in legged
robots. <em>ICRA</em>, 5773–5779. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a limb trajectory planning method that considers both body and limb dynamics in robots, particularly suitable for those with non-trivial limb mass. To simplify the complexity and computation cost of using the full-body dynamics of the limbs, a reduced-order model that can simulate the dynamic characteristics of the original limb is proposed. The performance of the model is experimentally validated using an exemplary single leg-wheel of the leg-wheel transformable robot. The limb trajectory optimization is developed using a genetic algorithm that considers many aspects, including body and limb dynamics, limb workspace, limb motion continuity, body tip-over stability, and power consumption. The performance of the proposed limb trajectory planning strategy is experimentally validated using the same leg-wheel transformable robot, and the results confirm the effectiveness of the strategy.},
  archive   = {C_ICRA},
  author    = {Kuan-Lun Lu and I-Chia Chang and Wei-Shun Yu and Pei-Chun Lin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611365},
  month     = {5},
  pages     = {5773-5779},
  title     = {Trajectory optimization strategy that considers body tip-over stability, limb dynamics, and motion continuity in legged robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generation of steady wheel gait for planar x-shaped walker
with reaction wheel. <em>ICRA</em>, 5766–5772. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the problem of realizing a novel robotic bipedal locomotion called wheel gait, which is achieved by rotating the stance and swing legs in the same direction. First, a model of a planar 3-DOF X-shaped walker with a reaction wheel is introduced, and the mathematical equations are described. Second, the condition for stabilizing zero dynamics is formulated as the time integral value of control input to the reaction wheel for one step becomes zero, and the control system for achieving this is designed based on the method of continuous-time output deadbeat control. Third, a typical steady wheel gait of the linearized model is numerically generated, and its extension to the nonlinear model is discussed. Although the nonlinear model has only one nonlinear term in the gravity term, numerical simulations show that there is a big gap between this and the linearized model. Through analysis of the typical nonlinear wheel gaits, the difficulty of achieving the same walking speed as the linearized model is discussed.},
  archive   = {C_ICRA},
  author    = {Fumihiko Asano and Taiki Sedoguchi and Cong Yan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610478},
  month     = {5},
  pages     = {5766-5772},
  title     = {Generation of steady wheel gait for planar X-shaped walker with reaction wheel},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive model predictive control with data-driven error
model for quadrupedal locomotion. <em>ICRA</em>, 5731–5737. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Model Predictive Control (MPC) relies heavily on the robot model for its control law. However, a gap always exists between the reduced-order control model with uncertainties and the real robot, which degrades its performance. To address this issue, we propose the controller of integrating a data-driven error model into traditional MPC for quadruped robots. Our approach leverages real-world data from sensors to compensate for defects in the control model. Specifically, we employ the Autoregressive Moving Average Vector (ARMAV) model to construct the state error model of the quadruped robot using data. The predicted state errors are then used to adjust the predicted future robot states generated by MPC. By such an approach, our proposed controller can provide more accurate inputs to the system, enabling it to achieve desired states even in the presence of model parameter inaccuracies or disturbances. The proposed controller exhibits the capability to partially eliminate the disparity between the model and the real-world robot, thereby enhancing the locomotion performance of quadruped robots. We validate our proposed method through simulations and real-world experimental trials on a large-size quadruped robot that involves carrying a 20 kg un-modeled payload (84% of body weight).},
  archive   = {C_ICRA},
  author    = {Xuanqi Zeng and Hongbo Zhang and Linzhu Yue and Zhitao Song and Lingwei Zhang and Yun-Hui Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611302},
  month     = {5},
  pages     = {5731-5737},
  title     = {Adaptive model predictive control with data-driven error model for quadrupedal locomotion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient model-based approach on learning agile motor
skills without reinforcement. <em>ICRA</em>, 5724–5730. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning-based methods have improved locomotion skills of quadruped robots through deep reinforcement learning. However, the sim-to-real gap and low sample efficiency still limit the skill transfer. To address this issue, we propose an efficient model-based learning framework that combines a world model with a policy network. We train a differentiable world model to predict future states and use it to directly supervise a Variational Autoencoder (VAE)-based policy network to imitate real animal behaviors. This significantly reduces the need for real interaction data and allows for rapid policy updates. We also develop a high-level network to track diverse commands and trajectories. Our simulated results show a tenfold sample efficiency increase compared to reinforcement learning methods such as PPO. In real-world testing, our policy achieves proficient command-following performance with only a two-minute data collection period and generalizes well to new speeds and paths.},
  archive   = {C_ICRA},
  author    = {Haojie Shi and Tingguang Li and Qingxu Zhu and Jiapeng Sheng and Lei Han and Max Q.-H. Meng},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611560},
  month     = {5},
  pages     = {5724-5730},
  title     = {An efficient model-based approach on learning agile motor skills without reinforcement},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pedipulate: Enabling manipulation skills using a quadruped
robot’s leg. <em>ICRA</em>, 5717–5723. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Legged robots have the potential to become vital in maintenance, home support, and exploration scenarios. In order to interact with and manipulate their environments, most legged robots are equipped with a dedicated robot arm, which means additional mass and mechanical complexity compared to standard legged robots. In this work, we explore pedipulation - using the legs of a legged robot for manipulation. By training a reinforcement learning policy that tracks position targets for one foot, we enable a dedicated pedipulation controller that is robust to disturbances, has a large workspace through whole-body behaviors, and can reach far-away targets with gait emergence, enabling loco-pedipulation. By deploying our controller on a quadrupedal robot using teleoperation, we demonstrate various real-world tasks such as door opening, sample collection, and pushing obstacles. We demonstrate load carrying of more than 2.0 kg at the foot. Additionally, the controller is robust to interaction forces at the foot, disturbances at the base, and slippery contact surfaces. Videos of the experiments are available at https://sites.google.com/leggedrobotics.com/pedipulate.},
  archive   = {C_ICRA},
  author    = {Philip Arm and Mayank Mittal and Hendrik Kolvenbach and Marco Hutter},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611307},
  month     = {5},
  pages     = {5717-5723},
  title     = {Pedipulate: Enabling manipulation skills using a quadruped robot’s leg},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Augmenting tactile simulators with real-like and zero-shot
capabilities. <em>ICRA</em>, 5702–5708. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simulating tactile perception could potentially leverage the learning capabilities of robotic systems in manipulation tasks. However, the reality gap of simulators for high-resolution tactile sensors remains large. Models trained on simulated data often fail in zero-shot inference and require fine-tuning with real data. In addition, work on high-resolution sensors commonly focus on ones with flat surfaces while 3D round sensors are essential for dexterous manipulation. In this paper, we propose a bi-directional Generative Adversarial Network (GAN) termed SightGAN. SightGAN relies on the early CycleGAN while including two additional loss components aimed to accurately reconstruct background and contact patterns including small contact traces. The proposed SightGAN learns real-to-sim and sim-to-real processes over difference images. It is shown to generate real-like synthetic images while maintaining accurate contact positioning. The generated images can be used to train zero-shot models for newly fabricated sensors. Consequently, the resulted sim-to-real generator could be built on top of the tactile simulator to provide a real-world framework. Potentially, the framework can be used to train, for instance, reinforcement learning policies of manipulation tasks. The proposed model is verified in extensive experiments with test data collected from real sensors and also shown to maintain embedded force information within the tactile images.},
  archive   = {C_ICRA},
  author    = {Osher Azulay and Alon Mizrahi and Nimrod Curtis and Avishai Sintov},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610442},
  month     = {5},
  pages     = {5702-5708},
  title     = {Augmenting tactile simulators with real-like and zero-shot capabilities},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CushSense: Soft, stretchable, and comfortable
tactile-sensing skin for physical human-robot interaction.
<em>ICRA</em>, 5694–5701. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Whole-arm tactile feedback is crucial for robots to ensure safe physical interaction with their surroundings. This paper introduces CushSense, a fabric-based soft and stretchable tactile-sensing skin designed for physical human-robot interaction (pHRI) tasks such as robotic caregiving. Using stretchable fabric and hyper-elastic polymer, CushSense identifies contacts by monitoring capacitive changes due to skin deformation. CushSense is cost-effective (∼US$7 per taxel) and easy to fabricate. We detail the sensor design and fabrication process and perform characterization, highlighting its high sensing accuracy (relative error of 0.58%) and durability (0.054% accuracy drop after 1000 interactions). We also present a user study underscoring its perceived safety and comfort for the assistive task of limb manipulation. We open source all sensor-related resources on emprise.cs.cornell.edu/cushsense.},
  archive   = {C_ICRA},
  author    = {Boxin Xu and Luoyan Zhong and Grace Zhang and Xiaoyu Liang and Diego Virtue and Rishabh Madan and Tapomayukh Bhattacharjee},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610014},
  month     = {5},
  pages     = {5694-5701},
  title     = {CushSense: Soft, stretchable, and comfortable tactile-sensing skin for physical human-robot interaction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing multi-touch textile and tactile skin sensing
through circuit parameter estimation. <em>ICRA</em>, 5687–5693. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tactile and textile skin technologies have become increasingly important for enhancing human-robot interaction and allowing robots to adapt to different environments. Despite notable advancements, there are ongoing challenges in skin signal processing, particularly in achieving both accuracy and speed in dynamic touch sensing. This paper introduces a new framework that poses the touch sensing problem as an estimation problem of resistive sensory arrays. Utilizing a Regularized Least Squares objective function—which estimates the resistance distribution of the skin—we enhance the touch sensing accuracy and mitigate the ghosting effects, where false or misleading touches may be registered. Furthermore, our study presents a streamlined skin design that simplifies manufacturing processes without sacrificing performance. Experimental outcomes substantiate the effectiveness of our method, showing 26.9% improvement in multi-touch force-sensing accuracy for the tactile skin.},
  archive   = {C_ICRA},
  author    = {Bo Ying Su and Yuchen Wu and Chengtao Wen and Changliu Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610053},
  month     = {5},
  pages     = {5687-5693},
  title     = {Optimizing multi-touch textile and tactile skin sensing through circuit parameter estimation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SATac: A thermoluminescence enabled tactile sensor for
concurrent perception of temperature, pressure, and shear.
<em>ICRA</em>, 5680–5686. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most vision-based tactile sensors use elastomer deformation to infer tactile information, which can not sense some modalities, like temperature. As an important part of human tactile perception, temperature sensing can help robots better interact with the environment. In this work, we propose a novel multi-modal vision-based tactile sensor, SATac, which can simultaneously perceive information on temperature, pressure, and shear. SATac utilizes the thermoluminescence of strontium aluminate to sense a wide range of temperatures with exceptional resolution. Additionally, the pressure and shear can also be perceived by analyzing the Voronoi diagram. A series of experiments are conducted to verify the performance of our proposed sensor. We also discuss the possible application scenarios and demonstrate how SATac could benefit robot perception capabilities.},
  archive   = {C_ICRA},
  author    = {Ziwu Song and Ran Yu and Xuan Zhang and Kit Wa Sou and Shilong Mu and Dengfeng Peng and Xiao-Ping Zhang and Wenbo Ding},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610373},
  month     = {5},
  pages     = {5680-5686},
  title     = {SATac: A thermoluminescence enabled tactile sensor for concurrent perception of temperature, pressure, and shear},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A detachable FBG-based contact force sensor for capturing
gripper-vegetable interactions. <em>ICRA</em>, 5673–5679. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vertical farming, a sustainable key for urban agriculture, has garnered attention for its land use optimization and enhanced food production capabilities. The adoption of automation in vertical farming is a pivotal response to labor shortages, addressing the need for increased efficiency, particularly in labor-intensive tasks like harvesting. Although soft robotic grippers offer a significant promise for delicately handling fragile objects, the absence of sensors has hindered their full potential to execute precise and secure grasping. To address this challenge, we present a new solution: a detachable Fiber Bragg Grating-based flexible contact force sensor to capture gripper-vegetable interactions. The sensing module was 3D printed using soft material, and the FBG fiber was attached to the module using epoxy. From evaluation tests, this lightweight sensor demonstrated a wide measurement range of up to 9.87 N, with a high sensitivity of 141.7 pm/N, good repeatability, and a hysteresis of 7.96%. Compared to commercial load cells, our sensor achieves a small measurement RMSE of 0.41 N and a percentage error of 4.15%. The sensor was integrated into two robotic 3D-printed soft grippers to enable real-time monitoring of dynamic contact force during vegetable harvesting in vertical farming scenarios. By reflecting contact status, this sensor provides a promising glimpse into the future of agricultural automation, enhancing operational efficiency and strengthening situation awareness and decision-making capabilities in vertical farms. Beyond agriculture, the versatility of this sensor extends to application in areas such as warehousing, logistics, and the food and beverage industry.},
  archive   = {C_ICRA},
  author    = {Wenjie Lai and Jiajun Liu and Bing Rui Sim and Ming Rui Joel Tan and Chidanand Hegde and Shlomo Magdassi and Soo Jay Phee},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611433},
  month     = {5},
  pages     = {5673-5679},
  title     = {A detachable FBG-based contact force sensor for capturing gripper-vegetable interactions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3D force and contact estimation for a soft-bubble
visuotactile sensor using FEM. <em>ICRA</em>, 5666–5672. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft-bubble tactile sensors have the potential to capture dense contact and force information across a large contact surface. However, it is difficult to extract contact forces directly from observing the bubble surface because local contacts change the global surface shape significantly due to membrane mechanics and air pressure. This paper presents a model-based method of reconstructing dense contact forces from the bubble sensor’s internal RGBD camera and air pressure sensor. We present a finite element model of the force response of the bubble sensor that uses a linear plane stress approximation that only requires calibrating 3 variables. Our method is shown to reconstruct normal and shear forces significantly more accurately than the state-of-the-art, with comparable accuracy for detecting the contact patch, and with very little calibration data.},
  archive   = {C_ICRA},
  author    = {Jing-Chen Peng and Shaoxiong Yao and Kris Hauser},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610233},
  month     = {5},
  pages     = {5666-5672},
  title     = {3D force and contact estimation for a soft-bubble visuotactile sensor using FEM},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contrastive learning-based attribute extraction method for
enhanced terrain classification. <em>ICRA</em>, 5644–5650. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The outdoor environment has many uneven surfaces that put the robot at risk of sinking or tipping over. Recognizing the type of terrain can help robot avoid risks and choose an appropriate gait. One of the critical problems is how to extract the terrain-related knowledge from sensor data collected as the robot traversed the ground. Many existing vision-based approaches are limited in directly perceiving the intrinsic properties of various terrains. The intuitive approach entails directly analyzing data recorded by the robot’s proprioceptive sensors. However, it faces challenges in being specific to certain robot leg configurations or in the lack of interpretability of the extracted features. In this paper, a terrain attribute extraction algorithm is proposed based on contrastive learning. It leverages the haptic data generated from the interaction between the robot’s legs and terrain to automatically extract terrain attributes. The results demonstrate that the attributes extracted using this method strongly correlate with the actual softness of the terrain. Furthermore, these attributes played an important role in achieving high accuracy in terrain classification tasks.},
  archive   = {C_ICRA},
  author    = {Xiao Liu and Hongjin Chen and Haoyao Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611271},
  month     = {5},
  pages     = {5644-5650},
  title     = {Contrastive learning-based attribute extraction method for enhanced terrain classification},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A 3D vector field and gaze data fusion framework for hand
motion intention prediction in human-robot collaboration. <em>ICRA</em>,
5637–5643. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In human-robot collaboration (HRC) settings, hand motion intention prediction (HMIP) plays a pivotal role in ensuring prompt decision-making, safety, and an intuitive collaboration experience. Precise and robust HMIP with low computational resources remains a challenge due to the stochastic nature of hand motion and the diversity of HRC tasks. This paper proposes a framework that combines hand trajectories and gaze data to foster robust, real-time HMIP with minimal to no training. A novel 3D vector field method is introduced for hand trajectory representation, leveraging minimum jerk trajectory predictions to discern potential hand motion endpoints. This is statistically combined with gaze fixation data using a weighted Naive Bayes Classifier (NBC). Acknowledging the potential variances in saccadic eye motion due to factors like fatigue or inattentiveness, we incorporate stationary gaze entropy to gauge visual concentration, thereby adjusting the contribution of gaze fixation to the HMIP. Empirical experiments substantiate that the proposed framework robustly predicts intended endpoints of hand motion before at least 50% of the trajectory is completed. It also successfully exploits gaze fixations when the human operator is attentive and mitigates its influence when the operator loses focus. A real-time implementation in a construction HRC scenario (collaborative tiling) showcases the intuitive nature and potential efficiency gains to be leveraged by introducing the proposed HMIP into HRC contexts. The opens-ource implementation of the framework is made available at https://github.com/maleenj/hmip_ros.git.},
  archive   = {C_ICRA},
  author    = {Maleen Jayasuriya and Gibson Hu and Dinh Dang Khoa Le and Karyne Ang and Shankar Sankaran and Dikai Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609996},
  month     = {5},
  pages     = {5637-5643},
  title     = {A 3D vector field and gaze data fusion framework for hand motion intention prediction in human-robot collaboration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel benchmarking paradigm and a scale- and motion-aware
model for egocentric pedestrian trajectory prediction. <em>ICRA</em>,
5630–5636. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a new paradigm for evaluating egocentric pedestrian trajectory prediction algorithms. Based on various contextual information, we extract driving scenarios for a meaningful and systematic approach to identifying challenges for prediction models. In this regard, we also propose a new metric for more effective ranking within the scenario-based evaluation. We conduct extensive empirical studies of existing models on these scenarios to expose shortcomings and strengths of different approaches. The scenario-based analysis highlights the importance of using multimodal sources of information and challenges caused by inadequate modeling of ego-motion and scale of pedestrians. To this end, we propose a novel egocentric trajectory prediction model that benefits from multimodal sources of data fused in an effective and efficient step-wise hierarchical fashion and two auxiliary tasks designed to learn more robust representation of scene dynamics. We conduct empirical evaluation on common benchmark datasets and show that our model not only achieves state-of-the-art performance, but also significantly improves performance by up to 39% in challenging scenarios, such as high ego-speed, compared to the past arts 1 .},
  archive   = {C_ICRA},
  author    = {Amir Rasouli},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610614},
  month     = {5},
  pages     = {5630-5636},
  title     = {A novel benchmarking paradigm and a scale- and motion-aware model for egocentric pedestrian trajectory prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CARTIER: Cartographic lAnguage reasoning targeted at
instruction execution for robots. <em>ICRA</em>, 5615–5621. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work explores the capacity of large language models (LLMs) to address problems at the intersection of spatial planning and natural language interfaces for navigation. We focus on following complex instructions that are more akin to natural conversation than traditional explicit procedural directives typically seen in robotics. Unlike most prior work where navigation directives are provided as simple imperative commands (e.g., &quot;go to the fridge&quot;), we examine implicit directives obtained through conversational interactions.We leverage the 3D simulator AI2Thor to create household query scenarios at scale, and augment it by adding complex language queries for 40 object types. We demonstrate that a robot using our method CARTIER (Cartographic lAnguage Reasoning Targeted at Instruction Execution for Robots) can parse descriptive language queries up to 42% more reliably than existing LLM-enabled methods by exploiting the ability of LLMs to interpret the user interaction in the context of the objects in the scenario.},
  archive   = {C_ICRA},
  author    = {Dmitriy Rivkin and Nikhil Kakodkar and Francois Hogan and Bobak H. Baghi and Gregory Dudek},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610072},
  month     = {5},
  pages     = {5615-5621},
  title     = {CARTIER: Cartographic lAnguage reasoning targeted at instruction execution for robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Looking inside out: Anticipating driver intent from videos.
<em>ICRA</em>, 5608–5614. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Anticipating driver intention is an important task when vehicles of mixed and varying levels of human/machine autonomy share roadways. Driver intention can be leveraged to improve road safety, such as warning surrounding vehicles in the event the driver is attempting a dangerous maneuver. In this work, we propose a novel method of utilizing both in-cabin and external camera data to improve state-of-the-art performance in predicting future driver actions. Compared to existing methods, our approach explicitly extracts object and road-level features from external camera data, which we demonstrate are important features for predicting driver intention. Using our handcrafted features as inputs for both a transformer and a long-short-term-memory-based architecture, we empirically show that jointly utilizing in-cabin and external features improves performance compared to using in-cabin features alone. Furthermore, our models predict driver maneuvers more accurately and sooner than existing approaches, with an accuracy of 87.5% and an average prediction time of 4.35 seconds before the maneuver takes place. We release our model configurations and training scripts on https://github.com/ykung83/Driver-Intent-Prediction.},
  archive   = {C_ICRA},
  author    = {Yung-Chi Kung and Arthur Zhang and Junmin Wang and Joydeep Biswas},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610257},
  month     = {5},
  pages     = {5608-5614},
  title     = {Looking inside out: Anticipating driver intent from videos},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Microexpression to macroexpression: Facial expression
magnification by single input. <em>ICRA</em>, 5600–5607. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Microexpressions are expressions that people inadvertently express, and therefore often represent a person’s true emotion. However, because it has a low intensity and a short duration, it is hard to be recognized correctly. In this paper, we propose a deep learning magnification method to generate macroexpressions from a single microexpression image. In the first stage, we extract the expression information from a single microexpression image. Then, We combine the idea of cyclegan and optical flow consistency to model the extracted expression features as the optical flow field between the neutral face and microexpressions. To extract a reliable optical flow field from the expression information, we design an optical flow refiner. In the second stage, we adopt an encoder-decoder network and let it learn to magnify the optical flow. Finally, the magnified optical flow guided the microexpression images to generate macroexpression images. We compare our single input based network with current two-frames-input based networks. The results show that our method performs better, even in wild images. We fed our magnified images directly into a simple ResNet18 network for recognition, achieving a competitive score under the MEGC2019 standard, compared with recent complex recognition networks.},
  archive   = {C_ICRA},
  author    = {Yaqi Song and Tong Chen and Shigang Li and Jianfeng Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610258},
  month     = {5},
  pages     = {5600-5607},
  title     = {Microexpression to macroexpression: Facial expression magnification by single input},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the feasibility of EEG-based motor intention detection
for real-time robot assistive control. <em>ICRA</em>, 5592–5599. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper explores the feasibility of employing EEG-based intention detection for real-time robot assistive control. We focus on predicting and distinguishing motor intentions of left/right arm movements by presenting: i) an offline data collection and training pipeline, used to train a classifier for left/right motion intention prediction, and ii) an online real-time prediction pipeline leveraging the trained classifier and integrated with an assistive robot. Central to our approach is a rich feature representation composed of the tangent space projection of time-windowed sample covariance matrices from EEG filtered signals and derivatives; allowing for a simple SVM classifier to achieve unprecedented accuracy and real-time performance. In pre-recorded real-time settings (160 Hz), a peak accuracy of 86.88% is achieved, surpassing prior works. In robot-in-the-loop settings, our system successfully detects intended motion solely from EEG data with 70% accuracy, triggering a robot to execute an assistive task. We provide a comprehensive evaluation of the proposed classifier.},
  archive   = {C_ICRA},
  author    = {Ho Jin Choi and Satyajeet Das and Shaoting Peng and Ruzena Bajcsy and Nadia Figueroa},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610321},
  month     = {5},
  pages     = {5592-5599},
  title     = {On the feasibility of EEG-based motor intention detection for real-time robot assistive control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robot trajectron: Trajectory prediction-based shared control
for robot manipulation. <em>ICRA</em>, 5585–5591. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the problem of (a) predicting the trajectory of an arm reaching motion, based on a few seconds of the motion’s onset, and (b) leveraging this predictor to facilitate shared-control manipulation tasks, by reducing the operator’s cognitive load through assistance in their anticipated direction of motion. Our novel intent estimator, dubbed the Robot Trajectron (RT), produces a probabilistic representation of the robot’s anticipated trajectory based on its recent position, velocity and acceleration history. By taking arm dynamics into account, RT can capture the operator’s intent better than other SOTA models that only use the arm’s position, making it particularly well-suited to assist in tasks where the operator’s intent is susceptible to change. We derive a novel shared-control solution that combines RT’s predictive capacity to a representation of the locations of potential reaching targets. Our experiments demonstrate RT’s effectiveness in both intent estimation and shared-control tasks. We will make the code and data supporting our experiments publicly available at https://gitlab.kuleuven.be/detry-lab/public/robot-trajectron},
  archive   = {C_ICRA},
  author    = {Pinhao Song and Pengteng Li and Erwin Aertbeliën and Renaud Detry},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611507},
  month     = {5},
  pages     = {5585-5591},
  title     = {Robot trajectron: Trajectory prediction-based shared control for robot manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stretch with stretch: Physical therapy exercise games led by
a mobile manipulator. <em>ICRA</em>, 5569–5576. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Physical therapy (PT) is a key component of many rehabilitation regimens, such as treatments for Parkinson’s disease (PD). However, there are shortages of physical therapists and adherence to self-guided PT is low. Robots have the potential to support physical therapists and increase adherence to self-guided PT, but prior robotic systems have been large and immobile, which can be a barrier to use in homes and clinics. We present Stretch with Stretch (SWS), a novel robotic system for leading stretching exercise games for older adults with PD. SWS consists of a compact and lightweight mobile manipulator (Hello Robot Stretch RE1) that visually and verbally guides users through PT exercises. The robot’s soft end effector serves as a target that users repetitively reach towards and press with a hand, foot, or knee. For each exercise, target locations are customized for the individual via a visually estimated kinematic model, a haptically estimated range of motion, and the person’s exercise performance. The system includes sound effects and verbal feedback from the robot to keep users engaged throughout a session and augment physical exercise with cognitive exercise. We conducted a user study for which people with PD (n = 10) performed 6 exercises with the system. Participants perceived the SWS to be useful and easy to use. They also reported mild to moderate perceived exertion (RPE).},
  archive   = {C_ICRA},
  author    = {Matthew Lamsey and You Liang Tan and Meredith D. Wells and Madeline Beatty and Zexuan Liu and Arjun Majumdar and Kendra Washington and Jerry Feldman and Naveen Kuppuswamy and Elizabeth Nguyen and Arielle Wallenstein and Madeleine E. Hackney and Charles C. Kemp},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611524},
  month     = {5},
  pages     = {5569-5576},
  title     = {Stretch with stretch: Physical therapy exercise games led by a mobile manipulator},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive control for triadic human-robot-FES collaboration
in gait rehabilitation: A pilot study. <em>ICRA</em>, 5561–5568. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The hybridisation of robot-assisted gait training and functional electrical stimulation (FES) can provide numerous physiological benefits to neurological patients. However, the design of an effective hybrid controller poses significant challenges. In this over-actuated system, it is extremely difficult to find the right balance between robotic assistance and FES that will provide personalised assistance, prevent muscle fatigue and encourage the patient’s active participation in order to accelerate recovery. In this paper, we present an adaptive hybrid robot-FES controller to do this and enable the triadic collaboration between the patient, the robot and FES. A patient-driven controller is designed where the voluntary movement of the patient is prioritised and assistance is provided using FES and the robot in a hierarchical order depending on the patient’s performance and their muscles’ fitness. The performance of this hybrid adaptive controller is tested in simulation and on one healthy subject. Our results indicate an increase in tracking performance with lower overall assistance, and less muscle fatigue when the hybrid adaptive controller is used, compared to its non adaptive equivalent. This suggests that our hybrid adaptive controller may be able to adapt to the behaviour of the user to provide assistance as needed and prevent the early termination of physical therapy due to muscle fatigue.},
  archive   = {C_ICRA},
  author    = {Andreas Christou and Antonio J. Del-Ama and Juan C. Moreno and Sethu Vijayakumar},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611133},
  month     = {5},
  pages     = {5561-5568},
  title     = {Adaptive control for triadic human-robot-FES collaboration in gait rehabilitation: A pilot study},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Controlling FES of arm movements using physics-informed
reinforcement learning via co-kriging adjustment. <em>ICRA</em>,
5555–5560. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Upper limb paralysis affects the quality of life. Functional Electrical Stimulation (FES) offers a solution to restore lost motor functions. Yet, there remain challenges in controlling FES to induce arbitrary arm movements. Reinforcement learning (RL) emerges as a promising method for controlling arm movement with success in simulation. However, challenges remain in translating the successes into real-world settings. One dominant challenge is the sample efficiency of RL. This study presents a practical RL setup to control FES for arm movements. We also present a flexible method, called co-kriging adjustment (CKA), which combines a biomechanical simulator and real data to build an accurate model of the real system. We demonstrate our RL-based control on a 2-DoF planar setting where the subject’s arm, placed on a frictionless supporter, is stimulated to perform point-to-point reaching. By using 90 seconds of real interaction data, our RL-based control can perform the reaching with the average error over the workspace of 5.5 cm. Beyond the application of FES, our method can be extended to other control systems, propelling RL towards general uses in the real world.},
  archive   = {C_ICRA},
  author    = {Nat Wannawas and Clara Diaz-Pintado and Jyotindra Narayan and A. Aldo Faisal},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610521},
  month     = {5},
  pages     = {5555-5560},
  title     = {Controlling FES of arm movements using physics-informed reinforcement learning via co-kriging adjustment},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An adaptable ankle trajectory generation method for
lower-limb exoskeletons by means of safety constraints computation and
minimum jerk planning. <em>ICRA</em>, 5548–5554. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a method to compute smooth ankle trajectories for lower limb exoskeletons with powered ankle joints. The proposed approach defines ankle trajectories using four polynomial functions, each representing one of the four primary phases of gait. These polynomials are computed according to different safety constraints. During the single support phase, ground contact constraints are enforced. In the swing phase, an optimization problem is solved to achieve minimum jerk planning while respecting a set of equality and inequality constraints designed to minimize the risk of stumbling. The used approach focuses on making the ankle joint able to smoothly adapt in real-time to different walking styles defined by user-selected gait parameters such as step length and clearance. The primary aim is to improve the user experience by producing a secure and comfortable walking pattern. To validate the effectiveness of the proposed method, the new ankle trajectories were tested on a group of healthy volunteers using the TWIN lower limb exoskeleton.},
  archive   = {C_ICRA},
  author    = {Raffaele Giannattasio and Stefano Maludrottu and Gaia Zinni and Elena De Momi and Matteo Laffranchi and Lorenzo De Michieli},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610191},
  month     = {5},
  pages     = {5548-5554},
  title     = {An adaptable ankle trajectory generation method for lower-limb exoskeletons by means of safety constraints computation and minimum jerk planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Achieving mechanical transparency using fusion hybrid linear
actuator for shoulder flexion and extension in exoskeleton robot.
<em>ICRA</em>, 5533–5539. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, the importance of mechanical transparency in human-assistive robots has grown. Traditionally, its primary goal was minimizing interaction forces during assistance. However, under this conventional definition, mechanical transparency was not considered when an interaction force was required during assistance. This research focuses on achieving mechanical transparency within the context of shoulder motion in upper extremity exoskeletons for rehabilitation. Our primary goal is maintaining interaction forces at target values, even with motion disturbances. To this end, we developed a shoulder actuation testbed for exoskeletons, incorporating a fusion hybrid linear actuator distinguished by high back-drivability, robust torque generation capability, and safety features. To attain mechanical transparency, we created a model for calculating the required joint torque, accounting for gravitational dynamics, and subsequently determined the necessary actuator output. The system characteristics were evaluated based on the joint torque generated by the actuator. The actuator utilized pneumatic pressure to generate force and compensated for kinetic friction using electromagnetic forces. The results showed that the compensation by the electromagnetic force reduced the root mean square error of the torque to less than 60% in relation to pneumatic pressure alone. This demonstrated the ability to generate consistent torque with high robustness to motion disturbances.},
  archive   = {C_ICRA},
  author    = {Takuma Shimoyama and Tomoyuki Noda and Tatsuya Teramae and Yoshihiro Nakata},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611131},
  month     = {5},
  pages     = {5533-5539},
  title     = {Achieving mechanical transparency using fusion hybrid linear actuator for shoulder flexion and extension in exoskeleton robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design &amp; systematic evaluation of power transmission
efficiency of an ankle exoskeleton for walking post-stroke.
<em>ICRA</em>, 5526–5532. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Community-based locomotor training post-stroke has shown improvements in independent ambulation by increasing dose, intensity, and specificity of walking practice. Robotic ankle exoskeletons hold the potential to facilitate continued rehabilitation at home, but understanding what aspects of the design are most relevant for successful translation to the community presents a challenge. Here, we design a portable rigid ankle exoskeleton to use as a research platform for investigating the effect of assistance on post-stroke gait during overground, community-based walking. We first test our device with stroke survivors and validate its potential for future community use. We then present a systematic method for quantifying power transmission losses at each transmission stage from the battery to the wearer, using data gathered from walking trials with healthy participants. Our evaluation method revealed inefficiencies in power transfer at the interface level, likely resulting from the compliance in the structural components of the system, which motivates future redesign considerations. Overall, our method provides a framework to identify and characterize the components that must be redesigned to lower exoskeleton weight and maximize performance.},
  archive   = {C_ICRA},
  author    = {Myles Cooper and Santiago Canete and Asa Eckert-Erdheim and Aidan Kimberley and Christopher Siviy and Teresa Baker and Terry D. Ellis and Patrick Slade and Conor J. Walsh},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610736},
  month     = {5},
  pages     = {5526-5532},
  title     = {Design &amp; systematic evaluation of power transmission efficiency of an ankle exoskeleton for walking post-stroke},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The impact of evolutionary computation on robotic design: A
case study with an underactuated hand exoskeleton. <em>ICRA</em>,
5519–5525. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic exoskeletons can enhance human strength and aid people with physical disabilities. However, designing them to ensure safety and optimal performance presents significant challenges. Developing exoskeletons should incorporate specific optimization algorithms to find the best design. This study investigates the potential of Evolutionary Computation (EC) methods in robotic design optimization, with an underactuated hand exoskeleton (U-HEx) used as a case study. We propose improving the performance and usability of the U-HEx design, which was initially optimized using a naive brute-force approach, by integrating EC techniques such as Genetic Algorithm and Big Bang-Big Crunch Algorithm. Comparative analysis revealed that EC methods consistently yield more precise and optimal solutions than brute force in a significantly shorter time. This allowed us to improve the optimization by increasing the number of variables in the design, which was impossible with naive methods. The results show significant improvements in terms of the torque magnitude the device transfers to the user, enhancing its efficiency. These findings underline the importance of performing proper optimization while designing exoskeletons, as well as providing a significant improvement to this specific robotic design.},
  archive   = {C_ICRA},
  author    = {Baris Akbas and Huseyin Taner Yuksel and Aleyna Soylemez and Mazhar Eid Zyada and Mine Sarac and Fabio Stroppa},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611070},
  month     = {5},
  pages     = {5519-5525},
  title     = {The impact of evolutionary computation on robotic design: A case study with an underactuated hand exoskeleton},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ChatAdp: ChatGPT-powered adaptation system for human-robot
interaction. <em>ICRA</em>, 5512–5518. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Different people have different preferences when it comes to human-robot interaction. Therefore, it is desirable for the robot to adapt its actions to fit users’ preferences. Human feedback is essential to facilitating robot adaptation. However, when the task is complex or the robot action space is large, it requires a large amount of user feedback. ChatGPT is a powerful generative AI tool based on large language models (LLMs), which possesses a significant corpus of information obtained from human society, and exhibits robust proficiency in the comprehension and acquisition of natural language. Therefore, in this paper, we proposed a ChatGPT-powered adaptation system (ChatAdp) for human-robot interaction which requires less user feedback to achieve a good adaptation result. In the proposed ChatAdp, we use ChatGPT as a user simulator to provide feedback. We evaluated ChatAdp in a case study for context-aware conversation adaptation. The results are very promising. Our proposed method can achieve a mean success rate of 92% on the user’s natural language-described preferences after receiving 33 rounds of feedback from a user on average, which is only 2% of the number of states covered by the user preferences and outperforms the two baseline methods.},
  archive   = {C_ICRA},
  author    = {Zhidong Su and Weihua Sheng},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611520},
  month     = {5},
  pages     = {5512-5518},
  title     = {ChatAdp: ChatGPT-powered adaptation system for human-robot interaction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design of embodied mediator haru for remote cross cultural
communication. <em>ICRA</em>, 5505–5511. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Social robots for children have focused mainly on conventional education domains such as teaching language, science, and math, while applications focusing on the enhancement of cultural competency are quite scarce. In this paper, we present a prototype of a robot-mediation framework for cross-cultural communication. This framework paves the way for a social robot to act as a mediator between groups of schoolchildren from different countries. First, we conducted a participatory design activity by an interdisciplinary team, resulting in the extraction of the design, robot’s roles, and technical requirements. Based on these requirements, we built the robot-mediation system prototype. We conducted a pilot study using the system with groups of high school children in Japan and Australia and our results show the potential of the system to drive children’s interest in communicating, sharing, and discussing cultural themes with their remote peers through the social robot.},
  archive   = {C_ICRA},
  author    = {Randy Gomez and Deborah Szapiro and Sara Cooper and Nabil Bougria and Guillermo Pérez and Eric Nichols and Javier Giménez-Figueroa and Jose M. Perez-Moleron and Matthew Peavy and Daniel Serrano and Luis Merino},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611253},
  month     = {5},
  pages     = {5505-5511},
  title     = {Design of embodied mediator haru for remote cross cultural communication},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring the impact of narrator type on response latency
and utterance length during interactive storytelling. <em>ICRA</em>,
5499–5504. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The inexorable progress of technology brought forth an era where robots increasingly integrate into human life which necessitates the understanding of human-robot interactions (HRI). This study unravels the details of HRI within interactive storytelling contexts. Through a between-subject experiment with 28 participants, we assessed response latency and utterance lengths to interactive story narrations delivered by either a human or a robot. Findings indicated that participants displayed longer response latency interacting with the robot narrator while articulating shorter utterances compared to the human condition where participants displayed longer utterances and shorter response latency. These observations suggest significant differences in cognitive and communicative strategies in human-human versus human-robot interactions. The results underscore the challenges and potential of designing social robots that are time-sensitive in interacting with humans. Future explorations should focus on the cognitive and emotional drivers behind these interactions.},
  archive   = {C_ICRA},
  author    = {Iman Bakhoda and Pourya Shahverdi and Katelyn Rousso and Justin Klotz and Wing-Yue Geoffrey Louie},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610817},
  month     = {5},
  pages     = {5499-5504},
  title     = {Exploring the impact of narrator type on response latency and utterance length during interactive storytelling},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Grounding conversational robots on vision through dense
captioning and large language models. <em>ICRA</em>, 5492–5498. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work explores a novel approach to empowering robots with visual perception capabilities using textual descriptions. Our approach involves the integration of GPT-4 with dense captioning, enabling robots to perceive and interpret the visual world through detailed text-based descriptions. To assess both user experience and the technical feasibility of this approach, experiments were conducted with human participants interacting with a Pepper robot equipped with visual capabilities. The results affirm the viability of the proposed approach, allowing to perform vision-based conversations effectively, despite processing time limitations.},
  archive   = {C_ICRA},
  author    = {Lucrezia Grassi and Zhouyang Hong and Carmine Tommaso Recchiuto and Antonio Sgorbissa},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611232},
  month     = {5},
  pages     = {5492-5498},
  title     = {Grounding conversational robots on vision through dense captioning and large language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning crowd behaviors in navigation with attention-based
spatial-temporal graphs. <em>ICRA</em>, 5485–5491. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safe and efficient navigation in dynamic environments shared with humans remains an open and challenging task for mobile robots. Previous works have shown the efficacy of using reinforcement learning frameworks to train policies for efficient navigation. However, their performance deteriorates when crowd configurations change, i.e. become larger or more complex. Thus, it is crucial to fully understand the complex, dynamic, and sophisticated interactions of the crowd resulting in proactive and foresighted behaviors for robot navigation. In this paper, a novel deep graph learning architecture based on attention mechanisms is proposed, which leverages the spatial-temporal graph to enhance robot navigation. We employ spatial graphs to capture the current spatial interactions, and through the integration with RNN, the temporal graphs utilize past trajectory information to infer the future intentions of each agent. The spatial-temporal graph reasoning ability allows the robot to better understand and interpret the relationships between agents over time and space, thereby making more informed decisions. Compared to previous state-of-the-art methods, our method demonstrates superior robustness in terms of safety, efficiency, and generalization in various challenging scenarios.},
  archive   = {C_ICRA},
  author    = {Yanying Zhou and Jochen Garcke},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610279},
  month     = {5},
  pages     = {5485-5491},
  title     = {Learning crowd behaviors in navigation with attention-based spatial-temporal graphs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Planning of explanations for robot navigation.
<em>ICRA</em>, 5478–5484. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The choices made by autonomous robots in social settings bear consequences for humans and their presumptions of robot behavior. Explanations can serve to alleviate detrimental impacts on humans and amplify their comprehension of robot decisions. We model the process of explanation generation for robot navigation as an automated planning problem considering different possible explanation attributes. Our visual and textual explanations of a robot’s navigation are influenced by the robot’s personality. Moreover, they account for different contextual, environmental, and spatial characteristics. We present the results of a user study demonstrating that users are more satisfied with multimodal than unimodal explanations. Additionally, our findings reveal low user satisfaction with explanations of a robot with extreme personality traits. In conclusion, we deliberate on potential future research directions and the associated constraints. Our work advocates for fostering socially adept and safe autonomous robot navigation.},
  archive   = {C_ICRA},
  author    = {Amar Halilovic and Senka Krivic},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611000},
  month     = {5},
  pages     = {5478-5484},
  title     = {Planning of explanations for robot navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The effect of rejection strategy on trust and shopping
choices in robot-assisted shopping *. <em>ICRA</em>, 5471–5477. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we investigate how a customer-facing service robot can support decision making in shopping interactions. In this role, a robot needs sometimes to reject a customer’s choice. Thus, we investigate different rejection strategies with the goal of changing customer behavior. The implemented strategies have been developed based on an ethnographic study on assisted shopping and tested in a lab experiment with 31 participants. The experiment showed significant differences in trust ratings and decision-making depending on the employed strategy.},
  archive   = {C_ICRA},
  author    = {Matthias Rehm and Antonia L. Krummheuer and Carlos G. Cubero},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611366},
  month     = {5},
  pages     = {5471-5477},
  title     = {The effect of rejection strategy on trust and shopping choices in robot-assisted shopping *},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward grounded commonsense reasoning. <em>ICRA</em>,
5463–5470. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Consider a robot tasked with tidying a desk with a meticulously constructed Lego sports car. A human may recognize that it is not appropriate to disassemble the sports car and put it away as part of the &quot;tidying.&quot; How can a robot reach that conclusion? Although large language models (LLMs) have recently been used to enable commonsense reasoning, grounding this reasoning in the real world has been challenging. To reason in the real world, robots must go beyond passively querying LLMs and actively gather information from the environment that is required to make the right decision. For instance, after detecting that there is an occluded car, the robot may need to actively perceive the car to know whether it is an advanced model car made out of Legos or a toy car built by a toddler. We propose an approach that leverages an LLM and vision language model (VLM) to help a robot actively perceive its environment to perform grounded commonsense reasoning. To evaluate our framework at scale, we release the MessySurfaces dataset which contains images of 70 real-world surfaces that need to be cleaned. We additionally illustrate our approach with a robot on 2 carefully designed surfaces. We find an average 12.9% improvement on the MessySurfaces benchmark and an average 15% improvement on the robot experiments over baselines that do not use active perception. The dataset, code, and videos of our approach can be found at https://minaek.github.io/grounded_commonsense_reasoning/.},
  archive   = {C_ICRA},
  author    = {Minae Kwon and Hengyuan Hu and Vivek Myers and Siddharth Karamcheti and Anca Dragan and Dorsa Sadigh},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611218},
  month     = {5},
  pages     = {5463-5470},
  title     = {Toward grounded commonsense reasoning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An offline learning of behavior correction policy for
vision-based robotic manipulation. <em>ICRA</em>, 5448–5454. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Offline learning usually requires a large dataset for training. In this paper, we focus on vision-based robotic manipulation tasks and utilize certain task properties to achieve offline learning with a small dataset. We propose a two-stage agent consisting of a tentative decision stage and a correction stage, where the tentative decision stage determines a tentative action from the original camera image, and the correction stage determines a correction to the tentative action based on the cropped image according to the tentative action. The correction stage utilizes task properties to obtain the cropped image with task-relevant features, enabling efficient correction. In particular, the training of the two stages can be performed individually, which enables a straightforward application of general offline learning algorithms. We conduct experiments by combining the two-stage agent with conventional offline reinforcement learning and imitation learning algorithms. In both cases, we benchmark the proposed method using RLBench and demonstrate that the task performance is significantly improved by the correction stage.},
  archive   = {C_ICRA},
  author    = {Qingxiuxiong Dong and Toshimitsu Kaneko and Masashi Sugiyama},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610177},
  month     = {5},
  pages     = {5448-5454},
  title     = {An offline learning of behavior correction policy for vision-based robotic manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ASGrasp: Generalizable transparent object reconstruction and
6-DoF grasp detection from RGB-d active stereo camera. <em>ICRA</em>,
5441–5447. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we tackle the problem of grasping transparent and specular objects. This issue holds importance, yet it remains unsolved within the field of robotics due to failure of recover their accurate geometry by depth cameras. For the first time, we propose ASGrasp, a 6-DoF grasp detection network that uses an RGB-D active stereo camera. ASGrasp utilizes a two-layer learning-based stereo network for the purpose of transparent object reconstruction, enabling material-agnostic object grasping in cluttered environments. In contrast to existing RGB-D based grasp detection methods, which heavily depend on depth restoration networks and the quality of depth maps generated by depth cameras, our system distinguishes itself by its ability to directly utilize raw IR and RGB images for transparent object geometry reconstruction. We create an extensive synthetic dataset through domain randomization, which is based on GraspNet-1Billion. Our experiments demonstrate that ASGrasp can achieve over 90% success rate for generalizable transparent object grasping in both simulation and the real via seamless sim-to-real transfer. Our method significantly outperforms SOTA networks and even surpasses the performance upper bound set by perfect visible point cloud inputs. Project page: https://pku-epic.github.io/ASGrasp},
  archive   = {C_ICRA},
  author    = {Jun Shi and Yong A and Yixiang Jin and Dingzhe Li and Haoyu Niu and Zhezhu Jin and He Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611152},
  month     = {5},
  pages     = {5441-5447},
  title     = {ASGrasp: Generalizable transparent object reconstruction and 6-DoF grasp detection from RGB-D active stereo camera},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contact energy based hindsight experience prioritization.
<em>ICRA</em>, 5434–5440. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-goal robot manipulation tasks with sparse rewards are difficult for reinforcement learning (RL) algorithms due to the inefficiency in collecting successful experiences. Recent algorithms such as Hindsight Experience Replay (HER) expedite learning by taking advantage of failed trajectories and replacing the desired goal with one of the achieved states so that any failed trajectory can be utilized as a contribution to learning. However, HER uniformly chooses failed trajectories, without taking into account which ones might be the most valuable for learning. In this paper, we address this problem and propose a novel approach Contact Energy Based Prioritization (CEBP) to select the samples from the replay buffer based on rich information due to contact, leveraging the touch sensors in the gripper of the robot and object displacement. Our prioritization scheme favors sampling of contact-rich experiences, which are arguably the ones providing the largest amount of information. We evaluate our proposed approach on various sparse reward robotic tasks and compare it with the state-of-the-art methods. We show that our method surpasses or performs on par with those methods on robot manipulation tasks. Finally, we deploy the trained policy from our method to a real Franka robot for a pick-and-place task. We observe that the robot can solve the task successfully. The videos and code are publicly available at: https://erdiphd.github.io/HER_force/.},
  archive   = {C_ICRA},
  author    = {Erdi Sayar and Zhenshan Bing and Carlo D’Eramo and Ozgur S. Oguz and Alois Knoll},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610910},
  month     = {5},
  pages     = {5434-5440},
  title     = {Contact energy based hindsight experience prioritization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient end-to-end detection of 6-DoF grasps for robotic
bin picking. <em>ICRA</em>, 5427–5433. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bin picking is an important building block for many robotic systems, in logistics, production or in household use-cases. In recent years, machine learning methods for the prediction of 6-DoF grasps on diverse and unknown objects have shown promising progress. However, existing approaches only consider a single ground truth grasp orientation at a grasp location during training and therefore can only predict limited grasp orientations which leads to a reduced number of feasible grasps in bin picking with restricted reachability. In this paper, we propose a novel approach for learning dense and diverse 6-DoF grasps for parallel-jaw grippers in robotic bin picking. We introduce a parameterized grasp distribution model based on Power-Spherical distributions that enables a training based on all possible ground truth samples. Thereby, we also consider the grasp uncertainty enhancing the model’s robustness to noisy inputs. As a result, given a single top-down view depth image, our model can generate diverse grasps with multiple collision-free grasp orientations. Experimental evaluations in simulation and on a real robotic bin picking setup demonstrate the model’s ability to generalize across various object categories achieving an object clearing rate of around 90% in simulation and real-world experiments. We also outperform state of the art approaches. Moreover, the proposed approach exhibits its usability in real robot experiments without any refinement steps, even when only trained on a synthetic dataset, due to the probabilistic grasp distribution modeling.},
  archive   = {C_ICRA},
  author    = {Yushi Liu and Alexander Qualmann and Zehao Yu and Miroslav Gabriel and Philipp Schillinger and Markus Spies and Ngo Anh Vien and Andreas Geiger},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611417},
  month     = {5},
  pages     = {5427-5433},
  title     = {Efficient end-to-end detection of 6-DoF grasps for robotic bin picking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GAMMA: Generalizable articulation modeling and manipulation
for articulated objects. <em>ICRA</em>, 5419–5426. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Articulated objects like cabinets and doors are widespread in daily life. However, directly manipulating 3D articulated objects is challenging because they have diverse geometrical shapes, semantic categories, and kinetic constraints. Prior works mostly focused on recognizing and manipulating articulated objects with specific joint types. They can either estimate the joint parameters or distinguish suitable grasp poses to facilitate trajectory planning. Although these approaches have succeeded in certain types of articulated objects, they lack generalizability to unseen objects, which significantly impedes their application in broader scenarios. In this paper, we propose a novel framework of Generalizable Articulation Modeling and Manipulating for Articulated Objects (GAMMA), which learns both articulation modeling and grasp pose affordance from diverse articulated objects with different categories. In addition, GAMMA adopts adaptive manipulation to iteratively reduce the modeling errors and enhance manipulation performance. We train GAMMA with the PartNet-Mobility dataset and evaluate with comprehensive experiments in SAPIEN simulation and real-world Franka robot. Results show that GAMMA significantly outperforms SOTA articulation modeling and manipulation algorithms in unseen and cross-category articulated objects. Images, videos and codes are published on the project website at: sites.google.com/view/gamma-articulation.},
  archive   = {C_ICRA},
  author    = {Qiaojun Yu and Junbo Wang and Wenhai Liu and Ce Hao and Liu Liu and Lin Shao and Weiming Wang and Cewu Lu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610652},
  month     = {5},
  pages     = {5419-5426},
  title     = {GAMMA: Generalizable articulation modeling and manipulation for articulated objects},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning active manipulation to target shapes with
model-free, long-horizon deep reinforcement learning. <em>ICRA</em>,
5411–5418. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate the active manipulation of objects using model-free and long-horizon DRL (Deep Reinforcement Learning) to achieve target shapes. Our proposed approach uses visual observations consisting of segmented images, to mitigate the sim-to-real gap. We address a long-horizon manipulation task requiring a sequence of accurate actions to achieve the target shapes using a robot arm with an RGB-D camera in eye-in-hand configuration, and an elongated, volumetric, elastoplastic object. We find similar objects in food, marine, and manufacturing domains. The aim is to actively manipulate the object into an arbitrary target shape using image observations. We trained a DRL agent using PPO (Proximal Policy Optimization) by running 768 parallel actors in simulation, for a total of 1,2M environment interactions, and tested this on 200 unseen target deformations. In three attempts, 82% of the trials achieved a greater than 90% overlap with the 200 target shapes. By relying on segmentation images as a visual observation space, we successfully transferred the agent to the real world without supplementary training. Our approach does not need any real-world manipulation examples nor fine-tuning in the real world. The robustness of our approach was demonstrated in simulation, and experimentally validated in the real world for specific manipulation tasks, achieving a 94.2% mean zero-shot overlap success rate on previously unseen target shapes.},
  archive   = {C_ICRA},
  author    = {Matias Sivertsvik and Kirill Sumskiy and Ekrem Misimi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610033},
  month     = {5},
  pages     = {5411-5418},
  title     = {Learning active manipulation to target shapes with model-free, long-horizon deep reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning extrinsic dexterity with parameterized manipulation
primitives. <em>ICRA</em>, 5404–5410. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many practically relevant robot grasping problems feature a target object for which all grasps are occluded, e.g., by the environment. Single-shot grasp planning invariably fails in such scenarios. Instead, it is necessary to first manipulate the object into a configuration that affords a grasp. We solve this problem by learning a sequence of actions that utilize the environment to change the object’s pose. Concretely, we employ hierarchical reinforcement learning to combine a sequence of learned parameterized manipulation primitives. By learning the low-level manipulation policies, our approach can control the object’s state through exploiting interactions between the object, the gripper, and the environment. Designing such a complex behavior analytically would be infeasible under uncontrolled conditions, as an analytic approach requires accurate physical modeling of the interaction and contact dynamics. In contrast, we learn a hierarchical policy model that operates directly on depth perception data, without the need for object detection, pose estimation, or manual design of controllers. We evaluate our approach on picking box-shaped objects of various weight, shape, and friction properties from a constrained table-top workspace. Our method transfers to a real robot and is able to successfully complete the object picking task in 98% of experimental trials.},
  archive   = {C_ICRA},
  author    = {Shih-Min Yang and Martin Magnusson and Johannes A. Stork and Todor Stoyanov},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611431},
  month     = {5},
  pages     = {5404-5410},
  title     = {Learning extrinsic dexterity with parameterized manipulation primitives},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RoboTAP: Tracking arbitrary points for few-shot visual
imitation. <em>ICRA</em>, 5397–5403. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For robots to be useful outside labs and specialized factories we need a way to teach them new useful behaviors quickly. Current approaches lack either the generality to onboard new tasks without task-specific engineering, or else lack the data-efficiency to do so in an amount of time that enables practical use. In this work we explore dense tracking as a representational vehicle to allow faster and more general learning from demonstration. Our approach utilizes Track-Any-Point (TAP) models to isolate the relevant motion in a demonstration, and parameterize a low-level controller to reproduce this motion across changes in the scene configuration. We show this results in robust robot policies that can solve complex object-arrangement tasks such as shape-matching, stacking, and even full path-following tasks such as applying glue and sticking objects together, all from demonstrations that can be collected in minutes.},
  archive   = {C_ICRA},
  author    = {Mel Vecerik and Carl Doersch and Yi Yang and Todor Davchev and Yusuf Aytar and Guangyao Zhou and Raia Hadsell and Lourdes Agapito and Jon Scholz},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611409},
  month     = {5},
  pages     = {5397-5403},
  title     = {RoboTAP: Tracking arbitrary points for few-shot visual imitation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). STOPNet: Multiview-based 6-DoF suction detection for
transparent objects on production lines. <em>ICRA</em>, 5389–5396. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we present STOPNet, a framework for 6-DoF object suction detection on production lines, with a focus on but not limited to transparent objects, which is an important and challenging problem in robotic systems and modern industry. Current methods requiring depth input fail on transparent objects due to depth cameras’ deficiency in sensing their geometry, while we proposed a novel framework to reconstruct the scene on the production line depending only on RGB input, based on multiview stereo. Compared to existing works, our method not only reconstructs the whole 3D scene in order to obtain high-quality 6-DoF suction poses in real time but also generalizes to novel environments, novel arrangements and novel objects, including challenging transparent objects, both in simulation and the real world. Extensive experiments in simulation and the real world show that our method significantly surpasses the baselines and has better generalizability, which caters to practical industrial needs.},
  archive   = {C_ICRA},
  author    = {Yuxuan Kuang and Qin Han and Danshi Li and Qiyu Dai and Lian Ding and Dong Sun and Hanlin Zhao and He Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611585},
  month     = {5},
  pages     = {5389-5396},
  title     = {STOPNet: Multiview-based 6-DoF suction detection for transparent objects on production lines},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advancements in 3D lane detection using LiDAR point clouds:
From data collection to model development. <em>ICRA</em>, 5382–5388. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Advanced Driver-Assistance Systems (ADAS) have successfully integrated learning-based techniques into vehicle perception and decision-making. However, their application in 3D lane detection for effective driving environment perception is hindered by the lack of comprehensive LiDAR datasets. The sparse nature of LiDAR point cloud data prevents an efficient manual annotation process. To solve this problem, we present LiSV-3DLane, a large-scale 3D lane dataset that comprises 20k frames of surround-view LiDAR point clouds with enriched semantic annotation. Unlike existing datasets confined to a frontal perspective, LiSV-3DLane provides a full 360-degree spatial panorama around the ego vehicle, capturing complex lane patterns in both urban and highway environments. We leverage the geometric traits of lane lines and the intrinsic spatial attributes of LiDAR data to design a simple yet effective automatic annotation pipeline for generating finer lane labels. To propel future research, we propose a novel LiDAR-based 3D lane detection model, LiLaDet, incorporating the spatial geometry learning of the LiDAR point cloud into Bird’s Eye View (BEV) based lane identification. Experimental results indicate that LiLaDet outperforms existing camera- and LiDAR-based approaches in the 3D lane detection task on the K-Lane dataset and our LiSV-3DLane. The project code will be available at https://github.com/RunkaiZhao/LiLaDet.},
  archive   = {C_ICRA},
  author    = {Runkai Zhao and Yuwen Heng and Heng Wang and Yuanda Gao and Shilei Liu and Changhao Yao and Jiawen Chen and Weidong Cai},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610087},
  month     = {5},
  pages     = {5382-5388},
  title     = {Advancements in 3D lane detection using LiDAR point clouds: From data collection to model development},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RoboKeyGen: Robot pose and joint angles estimation via
diffusion-based 3D keypoint generation. <em>ICRA</em>, 5375–5381. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Estimating robot pose and joint angles is significant in advanced robotics, enabling applications like robot collaboration and online hand-eye calibration. However, the introduction of unknown joint angles makes prediction more complex than simple robot pose estimation, due to its higher dimensionality. Previous methods either regress 3D keypoints directly or utilise a render&amp;compare strategy. These approaches often falter in terms of performance or efficiency and grapple with the cross-camera gap problem. This paper presents a novel framework that bifurcates the high-dimensional prediction task into two manageable subtasks: 2D keypoints detection and lifting 2D keypoints to 3D. This separation promises enhanced performance without sacrificing the efficiency innate to keypoint-based techniques. A vital component of our method is the lifting of 2D keypoints to 3D keypoints. Common deterministic regression methods may falter when faced with uncertainties from 2D detection errors or self-occlusions. Leveraging the robust modeling potential of diffusion models, we reframe this issue as a conditional 3D keypoints generation task. To bolster cross-camera adaptability, we introduce the Normalised Camera Coordinate Space (NCCS), ensuring alignment of estimated 2D keypoints across varying camera intrinsics. Experimental results demonstrate that the proposed method outperforms the state-of-the-art render&amp;compare method and achieves higher inference speed. Furthermore, the tests accentuate our method’s robust cross-camera generalisation capabilities. We intend to release both the dataset and code in https://nimolty.github.io/Robokeygen/.},
  archive   = {C_ICRA},
  author    = {Yang Tian and Jiyao Zhang and Guowei Huang and Bin Wang and Ping Wang and Jiangmiao Pang and Hao Dong},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611423},
  month     = {5},
  pages     = {5375-5381},
  title     = {RoboKeyGen: Robot pose and joint angles estimation via diffusion-based 3D keypoint generation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GAM-depth: Self-supervised indoor depth estimation
leveraging a gradient-aware mask and semantic constraints.
<em>ICRA</em>, 5367–5374. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-supervised depth estimation has evolved into an image reconstruction task that minimizes a photometric loss. While recent methods have made strides in indoor depth estimation, they often produce inconsistent depth estimation in textureless areas and unsatisfactory depth discrepancies at object boundaries. To address these issues, in this work, we propose GAM-Depth, developed upon two novel components: gradient-aware mask and semantic constraints. The gradient-aware mask enables adaptive and robust supervision for both key areas and textureless regions by allocating weights based on gradient magnitudes. The incorporation of semantic constraints for indoor self-supervised depth estimation improves depth discrepancies at object boundaries, leveraging a co-optimization network and proxy semantic labels derived from a pretrained segmentation model. Experimental studies on three indoor datasets, including NYUv2, ScanNet, and InteriorNet, show that GAM-Depth outperforms existing methods and achieves state-of-the-art performance, signifying a meaningful step forward in indoor depth estimation. Our code will be available at https://github.com/AnqiCheng1234/GAM-Depth.},
  archive   = {C_ICRA},
  author    = {Anqi Cheng and Zhiyuan Yang and Haiyue Zhu and Kezhi Mao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610653},
  month     = {5},
  pages     = {5367-5374},
  title     = {GAM-depth: Self-supervised indoor depth estimation leveraging a gradient-aware mask and semantic constraints},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). End-to-end semi-supervised 3D instance segmentation with
PCTeacher. <em>ICRA</em>, 5352–5358. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D instance segmentation is a fundamental and critical task for enabling robots to operate effectively in unstructured 3D environments. In order to address the challenges posed by the high demand for large-scale annotated data and the limited availability of such data in the context of 3D instance segmentation, we study semi-supervised 3D instance segmentation problem and propose a novel end-to-end framework based on the mean teacher paradigm, named PCTeacher. Our PCTeacher generates both point-level and cluster-level pseudo labels to harness knowledge from unlabeled data. It notably enhances the training stability through end-to-end training and improves pseudo-label quality. Specifically, for point-level pseudo labels, PCTeacher employs a multi-view fusion strategy to achieve higher precision and recall. Regarding cluster-level pseudo labels, it introduces a hybrid grouping strategy to generate more potential proposals and utilizes a point-cluster agreement-based thresholding (PCAT) mechanism to fully exploit cluster-level pseudo labels. By combining and strengthening both point-level and cluster-level pseudo labels, our PCTeacher achieves state-of-the-art performance on two benchmark datasets across multiple labeled data ratios with a more compact network compared to the existing method.},
  archive   = {C_ICRA},
  author    = {Linfeng Li and Na Zhao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610145},
  month     = {5},
  pages     = {5352-5358},
  title     = {End-to-end semi-supervised 3D instance segmentation with PCTeacher},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-throughput visual nano-drone to nano-drone relative
localization using onboard fully convolutional networks. <em>ICRA</em>,
5345–5351. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Relative drone-to-drone localization is a fundamental building block for any swarm operations. We address this task in the context of miniaturized nano-drones, i.e., ∼10cm in diameter, which show an ever-growing interest due to novel use cases enabled by their reduced form factor. The price for their versatility comes with limited onboard resources, i.e., sensors, processing units, and memory, which limits the complexity of the onboard algorithms. A traditional solution to overcome these limitations is represented by lightweight deep learning models directly deployed aboard nano-drones. This work tackles the challenging relative pose estimation between nano-drones using only a gray-scale low-resolution camera and an ultra-low-power System-on-Chip (SoC) hosted onboard. We present a vertically integrated system based on a novel vision-based fully convolutional neural network (FCNN), which runs at 39Hz within 101mW onboard a Crazyflie nano-drone extended with the GWT GAP8 SoC. We compare our FCNN against three State-of-the-Art (SoA) systems. Considering the best-performing SoA approach, our model results in a R 2 improvement from 32 to 47% on the horizontal image coordinate and from 18 to 55% on the vertical image coordinate, on a real-world dataset of ∼30k images. Finally, our in-field tests show a reduction of the average tracking error of 37% compared to a previous SoA work and an endurance performance up to the entire battery lifetime of 4min.},
  archive   = {C_ICRA},
  author    = {Luca Crupi and Alessandro Giusti and Daniele Palossi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611455},
  month     = {5},
  pages     = {5345-5351},
  title     = {High-throughput visual nano-drone to nano-drone relative localization using onboard fully convolutional networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Watching the air rise: Learning-based single-frame schlieren
detection. <em>ICRA</em>, 5338–5344. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Detecting air flows caused by phenomena such as heat convection is valuable in multiple scenarios, including leak identification and locating thermal updrafts for extending UAV flight duration. Unfortunately, the heat signature of these flows is often too subtle to be seen by a thermal camera. While convection also leads to fluctuations in air density and hence causes so-called schlieren – intensity and color variations in images – existing techniques such as Background-oriented schlieren (BOS) allow detecting them only against a known background and from a static camera, making these approaches unsuitable for moving vehicles. In this work we demonstrate the feasibility of visualizing air movement by predicting the corresponding schlieren-induced optical flow from a single greyscale image captured by a moving camera against an unfamiliar background. We first record and label a set of optical flows in an indoor setup using standard BOS techniques. We then train a convolutional neural network (CNN) by applying the previously collected optical flow distortions to a dataset containing a mixture of real and synthetically generated images to predict the two-dimensional optical flow from a single image. Finally, we evaluate our approach on the task of extracting the optical flow caused by schlieren from both a static and moving camera on previously unseen flow patterns and background images.},
  archive   = {C_ICRA},
  author    = {Florian Achermann and Julian Andreas Haug and Tobias Zumsteg and Nicholas Lawrance and Jen Jen Chung and Andrey Kolobov and Roland Siegwart},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611416},
  month     = {5},
  pages     = {5338-5344},
  title     = {Watching the air rise: Learning-based single-frame schlieren detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contrastive learning for enhancing robust scene transfer in
vision-based agile flight. <em>ICRA</em>, 5330–5337. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Scene transfer for vision-based mobile robotics applications is a highly relevant and challenging problem. The utility of a robot greatly depends on its ability to perform a task in the real world, outside of a well-controlled lab environment. Existing scene transfer end-to-end policy learning approaches often suffer from poor sample efficiency or limited generalization capabilities, making them unsuitable for mobile robotics applications. This work proposes an adaptive multi-pair contrastive learning strategy for visual representation learning that enables zero-shot scene transfer and real-world deployment. Control policies relying on the embedding are able to operate in unseen environments without the need for finetuning in the deployment environment. We demonstrate the performance of our approach on the task of agile, vision-based quadrotor flight. Extensive simulation and real-world experiments demonstrate that our approach successfully generalizes beyond the training domain and outperforms all baselines. Video: https://youtu.be/4A4YyPgEWD8},
  archive   = {C_ICRA},
  author    = {Jiaxu Xing and Leonard Bauersfeld and Yunlong Song and Chunwei Xing and Davide Scaramuzza},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610095},
  month     = {5},
  pages     = {5330-5337},
  title     = {Contrastive learning for enhancing robust scene transfer in vision-based agile flight},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UAV-sim: NeRF-based synthetic data generation for UAV-based
perception. <em>ICRA</em>, 5323–5329. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tremendous variations coupled with large degrees of freedom in UAV-based imaging conditions lead to a significant lack of data in adequately learning UAV-based perception models. Using various synthetic renderers in conjunction with perception models is prevalent to create synthetic data to augment the learning in the ground-based imaging domain. However, severe challenges in the austere UAV-based domain require distinctive solutions to image synthesis for data augmentation. In this work, we leverage recent advancements in neural rendering to improve static and dynamic novel-view UAV-based image synthesis, especially from high altitudes, capturing salient scene attributes. Finally, we demonstrate a considerable performance boost is achieved when a state-of-the-art detection model is optimized primarily on hybrid sets of real and synthetic data instead of the real or synthetic data separately.},
  archive   = {C_ICRA},
  author    = {Christopher Maxey and Jaehoon Choi and Hyungtae Lee and Dinesh Manocha and Heesung Kwon},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611523},
  month     = {5},
  pages     = {5323-5329},
  title     = {UAV-sim: NeRF-based synthetic data generation for UAV-based perception},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continuously estimate and control prosthetic grip force by
an optical waveguide sensor. <em>ICRA</em>, 5317–5322. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The emergence of intelligent prostheses has facilitated the life and work of disabled patients. The interaction aspect of prostheses has become a highlight research topic in the field of rehabilitation robotics. However, most of the existing prosthetic interaction methods focus on the use of myoelectricity to classify finite gestures, rather than continuous (infinite) force detection, which greatly limits the use of prosthetic scenarios. In this study, a novel optical waveguide sensor was used to collect muscle deformation information from the human arm for continuous control of the prosthetic grip force. The optical waveguide sensor was embedded with carbon fiber to limit the stretching of the waveguide, which led to the optical waveguide sensor being sensitive to bending deformation. Compared with EMGs, the accuracy of continuous grip force control based on the optical waveguide sensor is higher. The R-Square for prosthetic grip force and hand grip force were 0.867 and 0.9724 in the periodic and sustaining grip force experiments, respectively. The results suggested that the proposed method could provide a new approach to the interaction of prostheses.},
  archive   = {C_ICRA},
  author    = {Linhang Ju and Hanze Jia and Yanjun Shi and Xilun Ding and Yanggang Feng and Wuxiang Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610613},
  month     = {5},
  pages     = {5317-5322},
  title     = {Continuously estimate and control prosthetic grip force by an optical waveguide sensor},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A soft miniaturized continuum robot with 3D shape sensing
via functionalized soft optical waveguides. <em>ICRA</em>, 5309–5316.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a fully soft miniaturized continuum robot that integrates 3D optical shape sensing through functionalized tubing used as soft optical waveguides. The sensor is fabricated by laser patterning an off-the-shelf medical tubing, allowing for bidirectional responses to large curvatures in two bending directions, enabling 3D shape sensing and tip tracking of the continuum robot. The robot is able to bend and sense its own shape up to a curvature of 44.7 m -1 , corresponding to a bending angle of 102°, having high-accuracy tracking capabilities, resulting in an average tracking error of 3.08 mm, that is 7.7 % of the robot length. The robot’s functionality was shown in validation experiments, including a real-time shape prediction through a graphical user interface.},
  archive   = {C_ICRA},
  author    = {Viola Del Bono and Max McCandless and Frank Julia Wise and Sheila Russo},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611549},
  month     = {5},
  pages     = {5309-5316},
  title     = {A soft miniaturized continuum robot with 3D shape sensing via functionalized soft optical waveguides},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Capacitive origami sensing modules for measuring force in a
neurosurgical, soft robotic retractor. <em>ICRA</em>, 5302–5308. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In neurosurgery, soft robots have the potential to introduce significant benefits over traditional metal tools for their ability to safely interact with delicate tissues. In this paper, we introduce a proof-of-concept soft, capacitive origami sensing module (OSM) that can measure forces during neurosurgical retraction. Using origami-inspired design and fabrication principles, the OSM is easily folded and integrated within a soft robotic retractor that interacts with brain tissue to generate a surgical workspace upon actuation. We demonstrate the individual OSM signal response to forces and folding. We further characterize the OSM response within a fully-assembled soft robotic retractor to both folding and the application of forces over 0-5 N showing a 0.38 N average prediction error and resolution of 0.25 N. The sensing capability of the retractor is validated on an in-vitro model to demonstrate prediction errors of 0.06 N and the proposed operation during neurosurgery.},
  archive   = {C_ICRA},
  author    = {Daniel Van Lewen and Catherine Wang and Hun Chan Lee and Anand Devaiah and Urvashi Upadhyay and Sheila Russo},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610863},
  month     = {5},
  pages     = {5302-5308},
  title     = {Capacitive origami sensing modules for measuring force in a neurosurgical, soft robotic retractor},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design of a rigid-soft hybrid robotic glove with force
sensing function. <em>ICRA</em>, 5279–5285. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft robotic gloves can not only provide timely, effective, safe and cheap rehabilitation training for patients with impaired movement function of hand, but also assist in completing daily grasping activities. However, most soft robotic gloves are completely composed of flexible structures. Although they have high flexibility and safety, there are problems such as poor fit and low output force. In order to solve these problems, this paper refers to the structure of the human hand and designs an articulated rigid-soft hybrid robotic glove, which combines the advantages of rigid robotic gloves and soft robotic gloves, and has high flexibility, high output force and good fit. In addition, soft robotic gloves generally lack the ability to sense the force between the human hand and the glove. Therefore, this paper designed an array flexible force sensor, and studied the structure, signal acquisition and preparation process of the sensor. Finally, a complete test platform was built to test the performance of the rigid-soft hybrid robotic glove with force sensing function. The test results show that the robotic glove has good fit and high output force, can effectively assist training and assist grasping, and can perceive the contact force.},
  archive   = {C_ICRA},
  author    = {Hexin Li and Li Jiang and Ruichen Zhen and Ming Cheng and Kehan Ding},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610415},
  month     = {5},
  pages     = {5279-5285},
  title     = {Design of a rigid-soft hybrid robotic glove with force sensing function},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Embedded air channels transform soft lattices into
sensorized grippers. <em>ICRA</em>, 5264–5270. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sensing plays a pivotal role in robotic manipulation, dictating the accuracy and versatility with which objects are handled. Vision-based sensing methods often suffer from fabrication complexity and low durability, while approaches that rely on direct measurements on the gripper often have limited resolution and are difficult to scale. Here, we present a soft robotic gripper made out of two cubic lattices that are sensorized by embedding air channels within the structure. The lattices are 3D printed from a single build material, simplifying the fabrication process. The flexibility of this approach offers significant control over sensor and lattice design, while the pressure-based internal sensing provides measurements with minimal disruption to the grasping surface. With only 12 sensors, 6 per lattice, this gripper can estimate an object’s weight and location and offer new insights into grasp parameters like friction coefficients and grasp force.},
  archive   = {C_ICRA},
  author    = {Annan Zhang and Lillian Chin and Daniel L. Tong and Daniela Rus},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610187},
  month     = {5},
  pages     = {5264-5270},
  title     = {Embedded air channels transform soft lattices into sensorized grippers},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A biomorphic whisker sensor for aerial tactile applications.
<em>ICRA</em>, 5257–5263. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unmanned air vehicles (UAVs) have traditionally been considered as &quot;eyes in the sky&quot;, that can move in three dimensions and need to avoid any contact with their environment. On the contrary, contact should not be considered as a problem, but as an opportunity to expand the range of UAVs applications. In this paper, we designed, fabricated, and characterized a whisker sensor unit based on MEMS barometers suitable for tactile localization on UAVs, featuring lightweight, low stiffness, high sensitivity, a broad sensing range, and scalability. Then, for the challenging task of contact point localization, we propose a Recurrent Multi-output Network (RMN) for predicting 3D contact points under continuous contact conditions to address the problems of non-linearity, hysteresis, and non-injective mapping between signals and contact points by considering time series. In addition, we propose an azimuth prediction loss function which reduces the RMSE by 3.24 ◦ compared to L 1 loss. Finally, we conduct experiments on a linear stage to validate the 3D contact point localization capability of the proposed whisker system and model. The results show that our localization can achieve excellent performance, with an inference time of 1.4 ms and a mean error of only 9.18 mm in Euclidean distance within 3D space, laying a robust foundation for future implementation of tactile localization on UAVs. The design files, dataset, and source code are available on: https://github.com/BioMorphic-Intelligence-Lab/Whisker-3D-Localization.},
  archive   = {C_ICRA},
  author    = {Chaoxiang Ye and Guido De Croon and Salua Hamaza},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610850},
  month     = {5},
  pages     = {5257-5263},
  title     = {A biomorphic whisker sensor for aerial tactile applications},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UIVNAV: Underwater information-driven vision-based
navigation via imitation learning. <em>ICRA</em>, 5250–5256. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous navigation in the underwater environment is challenging due to limited visibility, dynamic changes, and the lack of a cost-efficient, accurate localization system. We introduce UIVN AV , a novel end-to-end underwater navigation solution designed to navigate robots over Objects of Interest (OOI) while avoiding obstacles, all without relying on localization. UIVN AV utilizes imitation learning and draws inspiration from the navigation strategies employed by human divers, who do not rely on localization. UIVN AV consists of the following phases: (1) generating an intermediate representation (IR) and (2) training the navigation policy based on human-labeled IR. By training the navigation policy on IR instead of raw data, the second phase is domain-invariant — the navigation policy does not need to be retrained if the domain or the OOI changes. We demonstrate this within simulation by deploying the same navigation policy to survey two distinct Objects of Interest (OOIs): oyster and rock reefs. We compared our method with complete coverage and random walk methods, showing that our approach is more efficient in gathering information for OOIs while avoiding obstacles. The results show that UIVN AV chooses to visit the areas with larger area sizes of oysters or rocks with no prior information about the environment or localization. Moreover, a robot using UIVN AV compared to complete coverage method surveys on average 36% more oysters when traveling the same distances. We also demonstrate the feasibility of real-time deployment of UIVN AV in pool experiments with BlueROV underwater robot for surveying a bed of oyster shells.},
  archive   = {C_ICRA},
  author    = {Xiaomin Lin and Nare Karapetyan and Kaustubh Joshi and Tianchen Liu and Nikhil Chopra and Miao Yu and Pratap Tokekar and Yiannis Aloimonos},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611203},
  month     = {5},
  pages     = {5250-5256},
  title     = {UIVNAV: Underwater information-driven vision-based navigation via imitation learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ViPlanner: Visual semantic imperative learning for local
navigation. <em>ICRA</em>, 5243–5249. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-time path planning in outdoor environments still challenges modern robotic systems due to differences in terrain traversability, diverse obstacles, and the necessity for fast decision-making. Established approaches have primarily focused on geometric navigation solutions, which work well for structured geometric obstacles but have limitations regarding the semantic interpretation of different terrain types and their affordances. Moreover, these methods fail to identify traversable geometric occurrences, such as stairs. To overcome these issues, we introduce ViPlanner, a learned local path planning approach that generates local plans based on geometric and semantic information. The system is trained using the Imperative Learning paradigm, for which the network weights are optimized end-to-end based on the planning task objective. This optimization uses a differentiable formulation of a semantic costmap, which enables the planner to distinguish between the traversability of different terrains and accurately identify obstacles. The semantic information is represented in 30 classes using an RGB colorspace that can effectively encode the multiple levels of traversability. We show that the planner can adapt to diverse real-world environments without requiring any real-world training. In fact, the planner is trained purely in simulation, enabling a highly scalable training data generation. Experimental results demonstrate resistance to noise, zero-shot sim-to-real transfer, and a decrease of 38.02% in terms of traversability cost compared to purely geometric-based approaches. Code and models are made publicly available: https://github.com/leggedrobotics/viplanner.},
  archive   = {C_ICRA},
  author    = {Pascal Roth and Julian Nubert and Fan Yang and Mayank Mittal and Marco Hutter},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610025},
  month     = {5},
  pages     = {5243-5249},
  title     = {ViPlanner: Visual semantic imperative learning for local navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bridging zero-shot object navigation and foundation models
through pixel-guided navigation skill. <em>ICRA</em>, 5228–5234. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Zero-shot object navigation is a challenging task for home-assistance robots. This task emphasizes visual grounding, commonsense inference and locomotion abilities, where the first two are inherent in foundation models. But for the locomotion part, most works still depend on map-based planning approaches. The gap between RGB space and map space makes it difficult to directly transfer the knowledge from foundation models to navigation tasks. In this work, we propose a Pixel-guided Navigation skill (PixNav), which bridges the gap between the foundation models and the embodied navigation task. It is straightforward for recent foundation models to indicate an object by pixels, and with pixels as the goal specification, our method becomes a versatile navigation policy towards all different kinds of objects. Besides, our PixNav is a pure RGB-based policy that can reduce the cost of homeassistance robots. Experiments demonstrate the robustness of the PixNav which achieves 80+% success rate in the local path-planning task. To perform long-horizon object navigation, we design an LLM-based planner to utilize the commonsense knowledge between objects and rooms to select the best waypoint. Evaluations across both photorealistic indoor simulators and real-world environments validate the effectiveness of our proposed navigation strategy. More details are accessible via our project website https://sites.google.com/view/pixnav/.},
  archive   = {C_ICRA},
  author    = {Wenzhe Cai and Siyuan Huang and Guangran Cheng and Yuxing Long and Peng Gao and Changyin Sun and Hao Dong},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610499},
  month     = {5},
  pages     = {5228-5234},
  title     = {Bridging zero-shot object navigation and foundation models through pixel-guided navigation skill},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Probable object location (POLo) score estimation for
efficient object goal navigation. <em>ICRA</em>, 5221–5227. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we focus on object search tasks within unexplored environments. We introduce a framework centered around the Probable Object Location (POLo) score. Utilizing a 3D object probability map, the POLo score allows the agent to make data-driven decisions for efficient object search. We further enhance the framework’s practicality by introducing POLoNet, a neural network trained to approximate the computationally-intensive POLo score. Our approach addresses critical limitations of both end-to-end reinforcement learning methods, which suffer from memory decay over long-horizon tasks, and traditional map-based methods that neglect visibility constraints. Our experiments, involving the first phase of the Open-Vocabulary Mobile Manipulation (OVMM) 2023 challenge, demonstrate that an agent equipped with POLoNet significantly outperforms a range of baseline methods, including end-to-end RL techniques and prior map-based strategies. To provide a comprehensive evaluation, we introduce new performance metrics that offer insights into the efficiency and effectiveness of various agents in object goal navigation.},
  archive   = {C_ICRA},
  author    = {Jiaming Wang and Harold Soh},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610671},
  month     = {5},
  pages     = {5221-5227},
  title     = {Probable object location (POLo) score estimation for efficient object goal navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Aligning knowledge graph with visual perception for
object-goal navigation. <em>ICRA</em>, 5214–5220. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Object-goal navigation is a challenging task that requires guiding an agent to specific objects based on first-person visual observations. The ability of agent to comprehend its surroundings plays a crucial role in achieving successful object finding. However, existing knowledge-graph-based navigators often rely on discrete categorical one-hot vectors and vote counting strategy to construct graph representation of the scenes, which results in misalignment with visual images. To provide more accurate and coherent scene descriptions and address this misalignment issue, we propose the Aligning Knowledge Graph with Visual Perception (AKGVP) method for object-goal navigation. Technically, our approach introduces continuous modeling of the hierarchical scene architecture and leverages visual-language pre-training to align natural language description with visual perception. The integration of a continuous knowledge graph architecture and multimodal feature alignment empowers the navigator with a remarkable zero-shot navigation capability. We extensively evaluate our method using the AI2-THOR simulator and conduct a series of experiments to demonstrate the effectiveness and efficiency of our navigator.},
  archive   = {C_ICRA},
  author    = {Nuo Xu and Wen Wang and Rong Yang and Mengjie Qin and Zheyuan Lin and Wei Song and Chunlong Zhang and Jason Gu and Chao Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610980},
  month     = {5},
  pages     = {5214-5220},
  title     = {Aligning knowledge graph with visual perception for object-goal navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PlaceNav: Topological navigation through place recognition.
<em>ICRA</em>, 5205–5213. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent results suggest that splitting topological navigation into robot-independent and robot-specific components improves navigation performance by enabling the robot-independent part to be trained with data collected by robots of different types. However, the navigation methods’ performance is still limited by the scarcity of suitable training data and they suffer from poor computational scaling. In this work, we present PlaceNav, subdividing the robot-independent part into navigation-specific and generic computer vision components. We utilize visual place recognition for the subgoal selection of the topological navigation pipeline. This makes subgoal selection more efficient and enables leveraging large-scale datasets from non-robotics sources, increasing training data availability. Bayesian filtering, enabled by place recognition, further improves navigation performance by increasing the temporal consistency of subgoals. Our experimental results verify the design and the new method obtains a 76 % higher success rate in indoor and 23 % higher in outdoor navigation tasks with higher computational efficiency.},
  archive   = {C_ICRA},
  author    = {Lauri Suomela and Jussi Kalliola and Harry Edelman and Joni-Kristian Kämäräinen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610575},
  month     = {5},
  pages     = {5205-5213},
  title     = {PlaceNav: Topological navigation through place recognition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Guided by the way: The role of on-the-route objects and
scene text in enhancing outdoor navigation. <em>ICRA</em>, 5198–5204.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In outdoor environments, Vision-and-Language Navigation (VLN) requires an agent to rely on multi-modal cues from real-world urban environments and natural language instructions. While existing outdoor VLN models predict actions using a combination of panorama and instruction features, this approach ignores objects in the environment and learns data bias to fail navigation. According to our preliminary findings, most instances of navigation failure in previous models were due to turning or stopping at the wrong place. In contrast, humans intuitively frequently use identifiable objects or store names as reference landmarks, ensuring accurate turns and stops, especially in unfamiliar places. To address this insight gap, we propose an Object-Attention VLN (OAVLN) model that helps the agent focus on relevant objects during training and understand the environment better. Our model outperforms previous methods in all evaluation metrics under both seen and unseen scenarios on two existing benchmark datasets, Touchdown and map2seq.},
  archive   = {C_ICRA},
  author    = {Yanjun Sun and Yue Qiu and Yoshimitsu Aoki and Hirokatsu Kataoka},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611727},
  month     = {5},
  pages     = {5198-5204},
  title     = {Guided by the way: The role of on-the-route objects and scene text in enhancing outdoor navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning for deformable linear object insertion leveraging
flexibility estimation from visual cues. <em>ICRA</em>, 5183–5189. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Manipulation of deformable Linear objects (DLOs), including iron wire, rubber, silk, and nylon rope, is ubiquitous in daily life. These objects exhibit diverse physical properties, such as Young’s modulus and bending stiffness. Such diversity poses challenges for developing generalized manipulation policies. However, previous research limited their scope to single-material DLOs and engaged in time-consuming data collection for the state estimation. In this paper, we propose a two-stage manipulation approach consisting of a material property (e.g., flexibility) estimation and policy learning for DLO insertion with reinforcement learning. Firstly, we design a flexibility estimation scheme that characterizes the properties of different types of DLOs. The ground truth flexibility data is collected in simulation to train our flexibility estimation module. During the manipulation, the robot interacts with the DLOs to estimate flexibility by analyzing their visual configurations. Secondly, we train a policy conditioned on the estimated flexibility to perform challenging DLO insertion tasks. Our pipeline trained with diverse insertion scenarios achieves an 85.6% success rate in simulation and 66.67% in real robot experiments. Please refer to our project page: https://lmeee.github.io/DLOInsert/},
  archive   = {C_ICRA},
  author    = {Mingen Li and Changhyun Choi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610419},
  month     = {5},
  pages     = {5183-5189},
  title     = {Learning for deformable linear object insertion leveraging flexibility estimation from visual cues},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Brain-inspired hyperdimensional computing in the wild:
Lightweight symbolic learning for sensorimotor controls of wheeled
robots. <em>ICRA</em>, 5176–5182. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Efficiency and performance are significant challenges in applying Machine Learning (ML) to robotics, especially in energy-constrained real-world scenarios. In this context, Hyperdimensional Computing offers an energy-efficient alternative but has been underexplored in robotics. We introduce ReactHD, an HDC-based framework tailored for perception-action-based learning for sensorimotor controls of robot tasks. ReactHD employs hypervectors to encode sensory inputs and learn the suitable high-dimensional pattern for robot actions. It also integrates two HD-based lightweight symbolic learning techniques: HDC-based supervised learning by demonstration (HDC-IL) and HD-Reinforcement Learning (HDC-RL) to enable precise, reactive robot behaviors in complex environments. Our empirical evaluations show that ReactHD achieves robust and accurate learning outcomes comparable to state-of-the-art deep learning while substantially improving the performance and energy consumption efficiency by 14.2× and 15.3×. To the best of our knowledge, ReactHD is the first HDC-based framework deployed in real-world settings.},
  archive   = {C_ICRA},
  author    = {Hyukjun Kwon and Kangwon Kim and Junyoung Lee and Hyunsei Lee and Jiseung Kim and Jinhyung Kim and Taehyung Kim and Yongnyeon Kim and Yang Ni and Mohsen Imani and Ilhong Suh and Yeseong Kim},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610176},
  month     = {5},
  pages     = {5176-5182},
  title     = {Brain-inspired hyperdimensional computing in the wild: Lightweight symbolic learning for sensorimotor controls of wheeled robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). REFORMA: Robust REinFORceMent learning via adaptive
adversary for drones flying under disturbances. <em>ICRA</em>,
5169–5175. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we introduce REFORMA, a novel robust reinforcement learning (RL) approach to design controllers for unmanned aerial vehicles (UAVs) robust to unknown disturbances during flights. These disturbances, typically due to wind turbulence, electromagnetic interference, temperature extremes and many other external physical interference, are highly dynamic and difficult to model. REFORMA can perform a real-time online adaptation to these disturbances and generate appropriate velocity actions as countermeasures to stabilize the drone. REFORMA consists of two components: a base policy trained completely in simulation using model-free RL and an adaptation module trained via supervised learning with on-policy datasets. By varying the disturbance strength in an adaptation module, i.e., adopting adaptive adversary, the policy is then able to handle extreme cases when the velocity of the drone is immediately affected by disturbances. Finally, we demonstrate the effectiveness of our method through extensive simulated experiments. To the best of our knowledge, REFORMA is the first robust RL approach that uses adaptive adversaries to tackle uncertain disturbances in drone tasks.},
  archive   = {C_ICRA},
  author    = {Hao-Lun Hsu and Haocheng Meng and Shaocheng Luo and Juncheng Dong and Vahid Tarokh and Miroslav Pajic},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611002},
  month     = {5},
  pages     = {5169-5175},
  title     = {REFORMA: Robust REinFORceMent learning via adaptive adversary for drones flying under disturbances},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving offline reinforcement learning with inaccurate
simulators. <em>ICRA</em>, 5162–5168. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Offline reinforcement learning (RL) provides a promising approach to avoid costly online interaction with the real environment. However, the performance of offline RL highly depends on the quality of the datasets, which may cause extrapolation error in the learning process. In many robotic applications, an inaccurate simulator is often available. However, the data directly collected from the inaccurate simulator cannot be directly used in offline RL due to the well-known exploration-exploitation dilemma and the dynamic gap between inaccurate simulation and the real environment. To address these issues, we propose a novel approach to combine the offline dataset and the inaccurate simulation data in a better manner. Specifically, we pre-train a generative adversarial network (GAN) model to fit the state distribution of the offline dataset. Given this, we collect data from the inaccurate simulator starting from the distribution provided by the generator and reweight the simulated data using the discriminator. Our experimental results in the D4RL benchmark and a real-world manipulation task confirm that our method can benefit more from both inaccurate simulator and limited offline datasets to achieve better performance than the state-of-the-art methods.},
  archive   = {C_ICRA},
  author    = {Yiwen Hou and Haoyuan Sun and Jinming Ma and Feng Wu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610833},
  month     = {5},
  pages     = {5162-5168},
  title     = {Improving offline reinforcement learning with inaccurate simulators},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MAexp: A generic platform for RL-based multi-agent
exploration. <em>ICRA</em>, 5155–5161. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The sim-to-real gap poses a significant challenge in RL-based multi-agent exploration due to scene quantization and action discretization. Existing platforms suffer from the inefficiency in sampling and the lack of diversity in Multi-Agent Reinforcement Learning (MARL) algorithms across different scenarios, restraining their widespread applications. To fill these gaps, we propose MAexp, a generic platform for multi-agent exploration that integrates a broad range of state-of-the-art MARL algorithms and representative scenarios. Moreover, we employ point clouds to represent our exploration scenarios, leading to high-fidelity environment mapping and a sampling speed approximately 40 times faster than existing platforms. Furthermore, equipped with an attention-based Multi-Agent Target Generator and a Single-Agent Motion Planner, MAexp can work with arbitrary numbers of agents and accommodate various types of robots. Extensive experiments are conducted to establish the first benchmark featuring several high-performance MARL algorithms across typical scenarios for robots with continuous actions, which highlights the distinct strengths of each algorithm in different scenarios.},
  archive   = {C_ICRA},
  author    = {Shaohao Zhu and Jiacheng Zhou and Anjun Chen and Mingming Bai and Jiming Chen and Jinming Xu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611573},
  month     = {5},
  pages     = {5155-5161},
  title     = {MAexp: A generic platform for RL-based multi-agent exploration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Privacy risks in reinforcement learning for household
robots. <em>ICRA</em>, 5148–5154. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The prominence of embodied Artificial Intelligence (AI), which empowers robots to navigate, perceive, and engage within virtual environments, has attracted significant attention, owing to the remarkable advances in computer vision and large language models. Privacy emerges as a pivotal concern within the realm of embodied AI, as the robot accesses substantial personal information. However, the issue of privacy leakage in embodied AI tasks, particularly concerning reinforcement learning algorithms, has not received adequate consideration in research. This paper aims to address this gap by proposing an attack on the training process of the value-based algorithm and the gradient-based algorithm, utilizing gradient inversion to reconstruct states, actions, and supervisory signals. The choice of using gradients for the attack is motivated by the fact that commonly employed federated learning techniques solely utilize gradients computed based on private user data to optimize models, without storing or transmitting the data to public servers. Nevertheless, these gradients contain sufficient information to potentially expose private data. To validate our approach, we conducted experiments on the AI2THOR simulator and evaluated our algorithm on active perception, a prevalent task in embodied AI. The experimental results demonstrate the effectiveness of our method in successfully reconstructing all information from the data in 120 room layouts. Check our website for videos.},
  archive   = {C_ICRA},
  author    = {Miao Li and Wenhao Ding and Ding Zhao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610832},
  month     = {5},
  pages     = {5148-5154},
  title     = {Privacy risks in reinforcement learning for household robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Extremum-seeking action selection for accelerating policy
optimization. <em>ICRA</em>, 5141–5147. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning for control over continuous spaces typically uses high-entropy stochastic policies, such as Gaussian distributions, for local exploration and estimating policy gradient to optimize performance. Many robotic control problems deal with complex unstable dynamics, where applying actions that are off the feasible control manifolds can quickly lead to undesirable divergence. In such cases, most samples taken from the ambient action space generate low-value trajectories that hardly contribute to policy improvement, resulting in slow or failed learning. We propose to improve action selection in this model-free RL setting by introducing additional adaptive control steps based on Extremum-Seeking Control (ESC). On each action sampled from stochastic policies, we apply sinusoidal perturbations and query for estimated Q-values as the response signal. Based on ESC, we then dynamically improve the sampled actions to be closer to nearby optima before applying them to the environment. Our methods can be easily added in standard policy optimization to improve learning efficiency, which we demonstrate in various control learning environments.},
  archive   = {C_ICRA},
  author    = {Ya-Chien Chang and Sicun Gao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610197},
  month     = {5},
  pages     = {5141-5147},
  title     = {Extremum-seeking action selection for accelerating policy optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active automotive augmented reality displays using
reinforcement learning. <em>ICRA</em>, 5134–5140. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In order to enhance driving convenience and safety, automotive Augmented Reality displays, e.g., head-up displays, have garnered attention and are gradually being deployed. However, when vehicles encounter uneven roads, vertical vibrations lead to mismatches between external physical objects and augmented reality overlay images, adversely affecting the AR display’s visibility. Resolving the problem is quite challenging because the optical system operates on a nanometer scale and is highly sensitive due to its multifunctional nature involving reflection and refraction through an intermediate medium. This paper aims to address the newly emerging problem of vertical mismatches in automotive AR displays. To tackle this issue, we begin by defining the problem and then examine the effectiveness of traditional control methods, on-policy and off-policy reinforcement learning as potential solutions. Finally, we validate our approach through experiments, demonstrating a significant reduction in vertical mismatches and an improvement in the overall visibility of automotive AR displays. Our findings provide valuable insights for enhancing driving convenience and safety in real-world conditions.},
  archive   = {C_ICRA},
  author    = {Ju-Hyeok Ryu and Chan Kim and Seong-Woo Kim},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611683},
  month     = {5},
  pages     = {5134-5140},
  title     = {Active automotive augmented reality displays using reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pre-trained masked image model for mobile robot navigation.
<em>ICRA</em>, 5126–5133. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {2D top-down maps are commonly used for the navigation and exploration of mobile robots through unknown areas. Typically, the robot builds the navigation maps incrementally from local observations using onboard sensors. Recent works have shown that predicting the structural patterns in the environment through learning-based approaches can greatly enhance task efficiency. While many such works build task-specific networks using limited datasets, we show that the existing foundational vision networks can accomplish the same without any fine-tuning. Specifically, we use Masked Autoencoders, pre-trained on street images, to present novel applications for field-of-view expansion, single-agent topological exploration, and multi-agent exploration for indoor mapping, across different input modalities. Our work motivates the use of foundational vision models for generalized structure prediction-driven applications, especially in the dearth of training data. We share more qualitative results at https://raaslab.org/projects/MIM4Robots.},
  archive   = {C_ICRA},
  author    = {Vishnu Dutt Sharma and Anukriti Singh and Pratap Tokekar},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611184},
  month     = {5},
  pages     = {5126-5133},
  title     = {Pre-trained masked image model for mobile robot navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An environmental-complexity-based navigation method based on
hierarchical deep reinforcement learning. <em>ICRA</em>, 5119–5125. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Navigation methods based on deep reinforcement learning (RL) have recently exhibited superior performance, particularly for navigation in dynamic environments. However, most existing methods solely rely on deep neural network feature encoders to extract features from raw LiDAR data, lacking an explicit representation of environmental structure. This limitation hinders effective environmental representation and interpretability, constraining navigation performance improvement. To solve this problem, we propose two quantitative metrics based on laser scans, which explicitly represent environmental complexity and show great interpretability. Furthermore, we propose an environmental-complexity-based navigation method based on hierarchical deep RL with the proposed metrics. Experimental results show that the proposed method achieves better navigation performance than baselines, especially in challenging scenarios with corners and dynamic obstacles.},
  archive   = {C_ICRA},
  author    = {Pengbin Chen and Qi Liu and Yanjie Li and Shuaikang Ma},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610970},
  month     = {5},
  pages     = {5119-5125},
  title     = {An environmental-complexity-based navigation method based on hierarchical deep reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Zero-shot wireless indoor navigation through
physics-informed reinforcement learning. <em>ICRA</em>, 5111–5118. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The growing focus on indoor robot navigation utilizing wireless signals has stemmed from the capability of these signals to capture high-resolution angular and temporal measurements. Prior heuristic-based methods, based on radio frequency (RF) propagation, are intuitive and generalizable across simple scenarios, yet fail to navigate in complex environments. On the other hand, end-to-end (e2e) deep reinforcement learning (RL) can explore a rich class of policies, delivering surprising performance when facing complex wireless environments. However, the price to pay is the astronomical amount of training samples, and the resulting policy, without fine-tuning (zero-shot), is unable to navigate efficiently in new scenarios unseen in the training phase. To equip the navigation agent with sample-efficient learning and zero-shot generalization, this work proposes a novel physics-informed RL (PIRL) where a distance-to-target-based cost (standard in e2e) is augmented with physics-informed reward shaping. The key intuition is that wireless environments vary, but physics laws persist. After learning to utilize the physics information, the agent can transfer this knowledge across different tasks and navigate in an unknown environment without fine-tuning. The proposed PIRL is evaluated using a wireless digital twin (WDT) built upon simulations of a large class of indoor environments from the AI Habitat dataset augmented with electromagnetic radiation simulation for wireless signals. It is shown that the PIRL significantly outperforms both e2e RL and heuristic-based solutions in terms of generalization and performance. Source code is available at https://github.com/Panshark/PIRL-WIN.},
  archive   = {C_ICRA},
  author    = {Mingsheng Yin and Tao Li and Haozhe Lei and Yaqi Hu and Sundeep Rangan and Quanyan Zhu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611229},
  month     = {5},
  pages     = {5111-5118},
  title     = {Zero-shot wireless indoor navigation through physics-informed reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). VPE-SLAM: Neural implicit voxel-permutohedral encoding for
SLAM. <em>ICRA</em>, 5104–5110. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {NeRF can reconstruct incredibly realistic environmental maps in dense simultaneous localization and mapping, providing robots with more comprehensive scene map information. However, NeRF often struggles with geometric distortions in indoor reconstructions. To correct geometric distortions, we develop VPE-SLAM, based on the proposed voxel-permutohedral encoding, which can incrementally reconstruct maps of unknown scenes. Specifically, voxel-permutohedral encoding combines a sparse voxel feature grid created by an octree and multi-resolution permutohedral tetrahedral feature grids to represent the scene effectively. Especially when dealing with object edges, our method can effectively encode the geometry and texture of edges by the hybrid structural grid. We propose a novel local bundle adjustment module that utilizes a sliding window mechanism to manage adjacent keyframes requiring optimization. Furthermore, the proposed method establishes local map consistency by repeatedly optimizing keyframes that were initially under-optimized through a compensation strategy. The consistency of the local map can enhance the adaptability of our method to challenging scenes. Extensive experiments demonstrate that our method can achieve accurate camera tracking and produce high-quality reconstruction results on the Replica and ScanNet datasets. The source code will be available at https://github.com/NeuCV-IRMI/VPE-SLAM.},
  archive   = {C_ICRA},
  author    = {Zhiyao Zhang and Yunzhou Zhang and You Shen and Lei Rong and Sizhan Wang and Xin Ouyang and Yulong Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610865},
  month     = {5},
  pages     = {5104-5110},
  title     = {VPE-SLAM: Neural implicit voxel-permutohedral encoding for SLAM},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RaLF: Flow-based global and metric radar localization in
LiDAR maps. <em>ICRA</em>, 5097–5103. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Localization is paramount for autonomous robots. While camera and LiDAR-based approaches have been extensively investigated, they are affected by adverse illumination and weather conditions. Therefore, radar sensors have recently gained attention due to their intrinsic robustness to such conditions. In this paper, we propose RaLF, a novel deep neural network-based approach for localizing radar scans in a LiDAR map of the environment, by jointly learning to address both place recognition and metric localization. RaLF is composed of radar and LiDAR feature encoders, a place recognition head that generates global descriptors, and a metric localization head that predicts the 3-DoF transformation between the radar scan and the map. We tackle the place recognition task by learning a shared embedding space between the two modalities via cross-modal metric learning. Additionally, we perform metric localization by predicting pixel-level flow vectors that align the query radar scan with the LiDAR map. We extensively evaluate our approach on multiple real-world driving datasets and show that RaLF achieves state-of-the-art performance for both place recognition and metric localization. Moreover, we demonstrate that our approach can effectively generalize to different cities and sensor setups than the ones used during training. We make the code and trained models publicly available at http://ralf.cs.uni-freiburg.de.},
  archive   = {C_ICRA},
  author    = {Abhijeet Nayak and Daniele Cattaneo and Abhinav Valada},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610626},
  month     = {5},
  pages     = {5097-5103},
  title     = {RaLF: Flow-based global and metric radar localization in LiDAR maps},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Snake robot with tactile perception navigates on large-scale
challenging terrain. <em>ICRA</em>, 5090–5096. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Along with the advancement of robot skin technology, there has been notable progress in the development of snake robots featuring body-surface tactile perception. In this study, we proposed a locomotion control framework for snake robots that integrates tactile perception to augment their adaptability to various terrains. Our approach embraces a hierarchical reinforcement learning (HRL) architecture, wherein the high-level orchestrates global navigation strategies while the low-level uses curriculum learning for local navigation maneuvers. Due to the significant computational demands of collision detection in whole-body tactile sensing, the efficiency of the simulator is severely compromised. Thus a distributed training pattern to mitigate the efficiency reduction was adopted. We evaluated the navigation performance of the snake robot in complex large-scale cave exploration with challenging terrains to exhibit improvements in motion efficiency, evidencing the efficacy of tactile perception in terrain-adaptive locomotion.},
  archive   = {C_ICRA},
  author    = {Shuo Jiang and Adarsh Salagame and Alireza Ramezani and Lawson L. S. Wong},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611384},
  month     = {5},
  pages     = {5090-5096},
  title     = {Snake robot with tactile perception navigates on large-scale challenging terrain},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning diverse skills for local navigation under
multi-constraint optimality. <em>ICRA</em>, 5083–5089. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite many successful applications of data-driven control in robotics, extracting meaningful diverse behaviors remains a challenge. Typically, task performance needs to be compromised in order to achieve diversity. In many scenarios, task requirements are specified as a multitude of reward terms, each requiring a different trade-off. In this work, we take a constrained optimization viewpoint on the quality-diversity trade-off and show that we can obtain diverse policies while imposing constraints on their value functions which are defined through distinct rewards. In line with previous work, further control of the diversity level can be achieved through an attract-repel reward term motivated by the Van der Waals force. We demonstrate the effectiveness of our method on a local navigation task where a quadruped robot needs to reach the target within a finite horizon. Finally, our trained policies transfer well to the real 12-DoF quadruped robot, Solo12, and exhibit diverse agile behaviors with successful obstacle traversal.},
  archive   = {C_ICRA},
  author    = {Jin Cheng and Marin Vlastelica and Pavel Kolev and Chenhao Li and Georg Martius},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611629},
  month     = {5},
  pages     = {5083-5089},
  title     = {Learning diverse skills for local navigation under multi-constraint optimality},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Long-tailed 3D semantic segmentation with adaptive weight
constraint and sampling. <em>ICRA</em>, 5037–5044. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing 3D understanding datasets typically provide annotations for a limited number of object classes, with sufficient examples per class. However, real-world object classes are not equally represented in practical settings, leading to poor performance on rarely-occurring categories if the class imbalance is neglected. In this work, we address the challenge of 3D semantic segmentation with a long-tail distribution of classes. Common methods to reduce class imbalance during training include data re-sampling, loss re-weighting, and transfer learning. In contrast, our work proposes to effectively utilize network classifier weights in 3D models to balance the training on long-tail class distributions. While previous work in the 2D domain has studied imposing constraints on the classifier weights to regularize the training, it is sensitive to hyper-parameter choices and has not been yet explored for the 3D domain. To address these challenges, our work proposes adaptive regularization for frequent classes and sampling-based regularization for rare classes that alleviate the need to manually select thresholds and can dynamically focus training on the hard classes. Our experiments on the large-scale Scan-Net200 benchmark show that our method achieves improved performance, surpassing methods that rely on re-sampling, re-weighting, and pre-training.},
  archive   = {C_ICRA},
  author    = {Jean Lahoud and Fahad Shahbaz Khan and Hisham Cholakkal and Rao Muhammad Anwer and Salman Khan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610029},
  month     = {5},
  pages     = {5037-5044},
  title     = {Long-tailed 3D semantic segmentation with adaptive weight constraint and sampling},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ConceptGraphs: Open-vocabulary 3D scene graphs for
perception and planning. <em>ICRA</em>, 5021–5028. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For robots to perform a wide variety of tasks, they require a 3D representation of the world that is semantically rich, yet compact and efficient for task-driven perception and planning. Recent approaches have attempted to leverage features from large vision-language models to encode semantics in 3D representations. However, these approaches tend to produce maps with per-point feature vectors, which do not scale well in larger environments, nor do they contain semantic spatial relationships between entities in the environment, which are useful for downstream planning. In this work, we propose ConceptGraphs, an open-vocabulary graph-structured representation for 3D scenes. ConceptGraphs is built by leveraging 2D foundation models and fusing their output to 3D by multi-view association. The resulting representations generalize to novel semantic classes, without the need to collect large 3D datasets or finetune models. We demonstrate the utility of this representation through a number of downstream planning tasks that are specified through abstract (language) prompts and require complex reasoning over spatial and semantic concepts. To explore the full scope of our experiments and results, we encourage readers to visit our project webpage.},
  archive   = {C_ICRA},
  author    = {Qiao Gu and Ali Kuwajerwala and Sacha Morin and Krishna Murthy Jatavallabhula and Bipasha Sen and Aditya Agarwal and Corban Rivera and William Paul and Kirsty Ellis and Rama Chellappa and Chuang Gan and Celso Miguel de Melo and Joshua B. Tenenbaum and Antonio Torralba and Florian Shkurti and Liam Paull},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610243},
  month     = {5},
  pages     = {5021-5028},
  title     = {ConceptGraphs: Open-vocabulary 3D scene graphs for perception and planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VG4D: Vision-language model goes 4D video recognition.
<em>ICRA</em>, 5014–5020. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Understanding the real world through point cloud video is a crucial aspect of robotics and autonomous driving systems. However, prevailing methods for 4D point cloud recognition have limitations due to sensor resolution, which leads to a lack of detailed information. Recent advances have shown that Vision-Language Models (VLM) pre-trained on web-scale text-image datasets can learn fine-grained visual concepts that can be transferred to various downstream tasks. However, effectively integrating VLM into the domain of 4D point clouds remains an unresolved problem. In this work, we propose the Vision-Language Models Goes 4D (VG4D) framework to transfer VLM knowledge from visual-text pretrained models to a 4D point cloud network. Our approach involves aligning the 4D encoder’s representation with a VLM learning a shared visual and text space from training on large-scale image-text pairs. By transferring the knowledge of the VLM to the 4D encoder and combining the VLM, our VG4D achieves improved recognition performance. To enhance the 4D encoder, we modernize the classic dynamic point cloud backbone and propose an improved version of PSTNet, im-PSTNet, which can efficiently model point cloud videos. Experiments demonstrate that our method achieves state-of-the-art performance for action recognition on both NTU RGB+D 60 dataset and NTU RGB+D 120 dataset.},
  archive   = {C_ICRA},
  author    = {Zhichao Deng and Xiangtai Li and Xia Li and Yunhai Tong and Shen Zhao and Mengyuan Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610217},
  month     = {5},
  pages     = {5014-5020},
  title     = {VG4D: Vision-language model goes 4D video recognition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-model 3D registration: Finding multiple moving objects
in cluttered point clouds. <em>ICRA</em>, 4990–4997. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate a variation of the 3D registration problem, named multi-model 3D registration. In the multi-model registration problem, we are given two point clouds picturing a set of objects at different poses (and possibly including points belonging to the background) and we want to simultaneously reconstruct how all objects moved between the two point clouds. This setup generalizes standard 3D registration where one wants to reconstruct a single pose, e.g., the motion of the sensor picturing a static scene. Moreover, it provides a mathematically grounded formulation for relevant robotics applications, e.g., where a depth sensor onboard a robot perceives a dynamic scene and has the goal of estimating its own motion (from the static portion of the scene) while simultaneously recovering the motion of all dynamic objects. We assume a correspondence-based setup where we have putative matches between the two point clouds and consider the practical case where these correspondences are plagued with outliers. We then propose a simple approach based on Expectation-Maximization (EM) and establish theoretical conditions under which the EM approach converges to the ground truth. We evaluate the approach in simulated and real datasets ranging from table-top scenes to self-driving scenarios and demonstrate its effectiveness when combined with state-of-the-art scene flow methods to establish dense correspondences.},
  archive   = {C_ICRA},
  author    = {David Jin and Sushrut Karmalkar and Harry Zhang and Luca Carlone},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610926},
  month     = {5},
  pages     = {4990-4997},
  title     = {Multi-model 3D registration: Finding multiple moving objects in cluttered point clouds},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Collaborative decision-making using spatiotemporal graphs in
connected autonomy. <em>ICRA</em>, 4983–4989. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collaborative decision-making is an essential capability for multi-robot systems, such as connected vehicles, to collaboratively control autonomous vehicles in accident-prone scenarios. Under limited communication bandwidth, capturing comprehensive situational awareness by integrating connected agents’ observation is very challenging. In this paper, we propose a novel collaborative decision-making method that efficiently and effectively integrates collaborators’ representations to control the ego vehicle in accident-prone scenarios. Our approach formulates collaborative decision-making as a classification problem. We first represent sequences of raw observations as spatiotemporal graphs, which significantly reduce the package size to share among connected vehicles. Then we design a novel spatiotemporal graph neural network based on heterogeneous graph learning, which analyzes spatial and temporal connections of objects in a unified way for collaborative decision-making. We evaluate our approach using a high-fidelity simulator that considers realistic traffic, communication bandwidth, and vehicle sensing among connected autonomous vehicles. The experimental results show that our representation achieves over 100x reduction in the shared data size that meets the requirements of communication bandwidth for connected autonomous driving. In addition, our approach achieves over 30% improvements in driving safety.},
  archive   = {C_ICRA},
  author    = {Peng Gao and Yu Shen and Ming C. Lin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610304},
  month     = {5},
  pages     = {4983-4989},
  title     = {Collaborative decision-making using spatiotemporal graphs in connected autonomy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WeatherDepth: Curriculum contrastive learning for
self-supervised depth estimation under adverse weather conditions.
<em>ICRA</em>, 4976–4982. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Depth estimation models have shown promising performance on clear scenes but fail to generalize to adverse weather conditions due to illumination variations, weather particles, etc. In this paper, we propose WeatherDepth, a self-supervised robust depth estimation model with curriculum contrastive learning, to tackle performance degradation in complex weather conditions. Concretely, we first present a progressive curriculum learning scheme with three simple-to-complex curricula to gradually adapt the model from clear to relative adverse, and then to adverse weather scenes. It encourages the model to gradually grasp beneficial depth cues against the weather effect, yielding smoother and better domain adaption. Meanwhile, to prevent the model from forgetting previous curricula, we integrate contrastive learning into different curricula. By drawing reference knowledge from the previous course, our strategy establishes a depth consistency constraint between different courses toward robust depth estimation in diverse weather. Besides, to reduce manual intervention and better adapt to different models, we designed an adaptive curriculum scheduler to automatically search for the best timing for course switching. In the experiment, the proposed solution is proven to be easily incorporated into various architectures and demonstrates state-of-the-art (SoTA) performance on both synthetic and real weather datasets. Source code and data are available at https://github.com/wangjiyuan9/WeatherDepth.},
  archive   = {C_ICRA},
  author    = {Jiyuan Wang and Chunyu Lin and Lang Nie and Shujun Huang and Yao Zhao and Xing Pan and Rui Ai},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611100},
  month     = {5},
  pages     = {4976-4982},
  title     = {WeatherDepth: Curriculum contrastive learning for self-supervised depth estimation under adverse weather conditions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LiteTrack: Layer pruning with asynchronous feature
extraction for lightweight and efficient visual tracking. <em>ICRA</em>,
4968–4975. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The recent advancements in transformer-based visual trackers have led to significant progress, attributed to their strong modeling capabilities. However, as performance improves, running latency correspondingly increases, presenting a challenge for real-time robotics applications, especially on edge devices with computational constraints. In response to this, we introduce LiteTrack, an efficient transformer-based tracking model optimized for high-speed operations across various devices. It achieves a more favorable trade-off between accuracy and efficiency than the other lightweight trackers. The main innovations of LiteTrack encompass: 1) asynchronous feature extraction and interaction between the template and search region for better feature fushion and cutting redundant computation, and 2) pruning encoder layers from a heavy tracker to refine the balnace between performance and speed. As an example, our fastest variant, LiteTrack-B4, achieves 65.2% AO on the GOT-10k benchmark, surpassing all preceding efficient trackers, while running over 100 fps with ONNX on the Jetson Orin NX edge device. Moreover, our LiteTrack-B9 reaches competitive 72.2% AO on GOT-10k and 82.4% AUC on TrackingNet, and operates at 171 fps on an NVIDIA 2080Ti GPU. The code and demo materials will be available at https://github.com/TsingWei/LiteTrack.},
  archive   = {C_ICRA},
  author    = {Qingmao Wei and Bi Zeng and Jianqi Liu and Li He and Guotian Zeng},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610022},
  month     = {5},
  pages     = {4968-4975},
  title     = {LiteTrack: Layer pruning with asynchronous feature extraction for lightweight and efficient visual tracking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UncertaintyTrack: Exploiting detection and localization
uncertainty in multi-object tracking. <em>ICRA</em>, 4946–4953. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-object tracking (MOT) methods have seen a significant boost in performance recently, due to strong interest from the research community and steadily improving object detection methods. The majority of tracking methods, which follow the tracking-by-detection (TBD) paradigm, blindly trust the incoming detections with no sense of their associated localization uncertainty. This lack of uncertainty awareness poses a problem in safety-critical tasks such as autonomous driving where passengers could be put at risk due to erroneous detections that have propagated to downstream tasks, including MOT. While there are existing works in probabilistic object detection that predict the localization uncertainty around the boxes, no work in 2D MOT for autonomous driving has studied whether these estimates are meaningful enough to be leveraged effectively in object tracking. We introduce UncertaintyTrack, a collection of extensions that can be applied to multiple TBD trackers to account for localization uncertainty estimates from probabilistic object detectors. Experiments on the Berkeley Deep Drive MOT dataset show that the combination of our method and informative uncertainty estimates reduces the number of ID switches by around 19% and improves mMOTA by 2-3%. The source code is available at https://github.com/TRAILab/UncertaintyTrack},
  archive   = {C_ICRA},
  author    = {Chang Won Lee and Steven L. Waslander},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610458},
  month     = {5},
  pages     = {4946-4953},
  title     = {UncertaintyTrack: Exploiting detection and localization uncertainty in multi-object tracking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SWTrack: Multiple hypothesis sliding window 3D multi-object
tracking. <em>ICRA</em>, 4939–4945. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern robotic systems are required to operate in dense dynamic environments, requiring highly accurate real-time track identification and estimation. For 3D multi-object tracking, recent approaches process a single measurement frame recursively with greedy association and are prone to errors in ambiguous association decisions. Our method, Sliding Window Tracker (SWTrack), yields more accurate association and state estimation by batch processing many frames of sensor data while being capable of running online in real-time. The most probable track associations are identified by evaluating all possible track hypotheses across the temporal sliding window. A novel graph optimization approach is formulated to solve the multidimensional assignment problem with lifted graph edges introduced to account for missed detections and graph sparsity enforced to retain real-time efficiency. We evaluate our SWTrack implementation on the NuScenes autonomous driving dataset to demonstrate improved tracking performance.},
  archive   = {C_ICRA},
  author    = {Sandro Papais and Robert Ren and Steven Waslander},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611067},
  month     = {5},
  pages     = {4939-4945},
  title     = {SWTrack: Multiple hypothesis sliding window 3D multi-object tracking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Refining pre-trained motion models. <em>ICRA</em>,
4932–4938. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Given the difficulty of manually annotating motion in video, the current best motion estimation methods are trained with synthetic data, and therefore struggle somewhat due to a train/test gap. Self-supervised methods hold the promise of training directly on real video, but typically perform worse. These include methods trained with warp error (i.e., color constancy) combined with smoothness terms, and methods that encourage cycle-consistency in the estimates (i.e., tracking backwards should yield the opposite trajectory as tracking forwards). In this work, we take on the challenge of improving state-of-the-art supervised models with self-supervised training. We find that when the initialization is supervised weights, most existing self-supervision techniques actually make performance worse instead of better, which suggests that the benefit of seeing the new data is overshadowed by the noise in the training signal. Focusing on obtaining a &quot;clean&quot; training signal from real-world unlabelled video, we propose to separate label-making and training into two distinct stages. In the first stage, we use the pre-trained model to estimate motion in a video, and then select the subset of motion estimates which we can verify with cycle-consistency. This produces a sparse but accurate pseudo-labelling of the video. In the second stage, we fine-tune the model to reproduce these outputs, while also applying augmentations on the input. We complement this boot-strapping method with simple techniques that densify and re-balance the pseudo-labels, ensuring that we do not merely train on &quot;easy&quot; tracks. We show that our method yields reliable gains over fully-supervised methods in real videos, for both short-term (flow-based) and long-range (multiframe) pixel tracking. Our code can be found here: https: //github.com/AlexSunNik/refining-motion-code.},
  archive   = {C_ICRA},
  author    = {Xinglong Sun and Adam W. Harley and Leonidas J. Guibas},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610900},
  month     = {5},
  pages     = {4932-4938},
  title     = {Refining pre-trained motion models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Zero-shot open-vocabulary tracking with large pre-trained
models. <em>ICRA</em>, 4916–4923. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Object tracking is central to robot perception and scene understanding, allowing robots to parse a video stream in terms of moving objects with names. Tracking-by-detection has long been a dominant paradigm for object tracking of specific object categories [1], [2]. Recently, large-scale pre-trained models have shown promising advances in detecting and segmenting objects and parts in 2D static images in the wild. This raises the question: can we re-purpose these large-scale pre-trained static image models for open-vocabulary video tracking? In this paper, we combine an open-vocabulary detector [3], segmenter [4], and dense optical flow estimator [5], into a model that tracks and segments any object in 2D videos. Given a monocular video input, our method predicts object and part mask tracks with associated language descriptions, rebuilding the pipeline of Tractor [6] with modern large pre-trained models for static image detection and segmentation: we detect open-vocabulary object instances and propagate their boxes from frame to frame using a flow-based motion model, refine the propagated boxes with the box regression module of the visual detector, and prompt an open-world segmenter with the refined box to segment the objects. We decide the termination of an object track based on the objectness score of the propagated boxes as well as forward-backward optical flow consistency. We re-identify objects across occlusions using deep feature matching. We show that our model achieves strong performance on multiple established benchmarks [7], [8], [9], [10], and can produce reasonable tracks in manipulation data [11]. In particular, our model outperforms previous state-of-the-art in UVO and BURST, benchmarks for open-world object tracking and segmentation, despite never being explicitly trained for tracking. We hope that our approach can serve as a simple and extensible framework for future research and enable imitation learning from videos with unconventional objects.},
  archive   = {C_ICRA},
  author    = {Wen-Hsuan Chu and Adam W. Harley and Pavel Tokmakov and Achal Dave and Leonidas Guibas and Katerina Fragkiadaki},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611726},
  month     = {5},
  pages     = {4916-4923},
  title     = {Zero-shot open-vocabulary tracking with large pre-trained models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Object permanence filter for robust tracking with
interactive robots. <em>ICRA</em>, 4909–4915. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Object permanence, which refers to the concept that objects continue to exist even when they are no longer perceivable through the senses, is a crucial aspect of human cognitive development. In this work, we seek to incorporate this understanding into interactive robots by proposing a set of assumptions and rules to represent object permanence in multi-object, multi-agent interactive scenarios. We integrate these rules into the particle filter, resulting in the Object Permanence Filter (OPF). For multi-object scenarios, we propose an ensemble of K interconnected OPFs, where each filter predicts plausible object tracks that are resilient to missing, noisy, and kinematically or dynamically infeasible measurements, thus bringing perceptional robustness. Through several interactive scenarios, we demonstrate that the proposed OPF approach provides robust tracking in human-robot interactive tasks agnostic to measurement type, even in the presence of prolonged and complete occlusion. Webpage: https://opfilter.github.io/.},
  archive   = {C_ICRA},
  author    = {Shaoting Peng and Margaret X. Wang and Julie A. Shah and Nadia Figueroa},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611528},
  month     = {5},
  pages     = {4909-4915},
  title     = {Object permanence filter for robust tracking with interactive robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RecNet: An invertible point cloud encoding through range
image embeddings for multi-robot map sharing and reconstruction.
<em>ICRA</em>, 4883–4889. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the field of resource-constrained robots and the need for effective place recognition in multi-robotic systems, this article introduces RecNet, a novel approach that concurrently addresses both challenges. The core of RecNet’s methodology involves a transformative process: it projects 3D point clouds into range images, compresses them using an encoder-decoder framework, and subsequently reconstructs the range image, restoring the original point cloud. Additionally, RecNet utilizes the latent vector extracted from this process for efficient place recognition tasks. This approach not only achieves comparable place recognition results but also maintains a compact representation, suitable for sharing among robots to reconstruct their collective maps. The evaluation of RecNet encompasses an array of metrics, including place recognition performance, the structural similarity of the reconstructed point clouds, and the bandwidth transmission advantages, derived from sharing only the latent vectors. Our proposed approach is assessed using both a publicly available dataset and field experiments 1 confirming its efficacy and potential for real-world applications.},
  archive   = {C_ICRA},
  author    = {Nikolaos Stathoulopoulos and Mario A.V. Saucedo and Anton Koval and George Nikolakopoulos},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611602},
  month     = {5},
  pages     = {4883-4889},
  title     = {RecNet: An invertible point cloud encoding through range image embeddings for multi-robot map sharing and reconstruction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic targeting of satellite observations incorporating
slewing costs and complex observation utility *. <em>ICRA</em>,
4876–4882. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Maximizing the utility of limited Earth observing satellite resources is a difficult ongoing problem. Dynamic Targeting is an approach to this challenge that intelligently plans and executes primary sensor observations based on information from a look-ahead sensor. However, current implementations have failed to account for realistic satellite operational constraints and have used static utility for repeat observations of the same target. To address these limitations, we implement a more general Dynamic Targeting framework that comprises a physics-based slew model, a dynamic model of observation utility, and an algorithm for gathering high-utility observations. To demonstrate this framework, we also supply complex dynamic utility models that are applicable to many missions and new algorithms for intelligently scheduling observations with slewing restrictions and changing utility, including a greedy algorithm and a depth-first search algorithm. To evaluate these algorithms, we test their performance across simulated runs through two datasets and compare to the performance of an algorithm representative of most scheduling algorithms aboard Earth science missions today as well as an intractable upper bound. We show that our algorithms have great potential to improve science return from Earth science missions.},
  archive   = {C_ICRA},
  author    = {Akseli Kangaslahti and Alberto Candela and Jason Swope and Qing Yue and Steve Chien},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610692},
  month     = {5},
  pages     = {4876-4882},
  title     = {Dynamic targeting of satellite observations incorporating slewing costs and complex observation utility *},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Measurement-limited multi-agent, relative pose estimation
for on-orbit inspection. <em>ICRA</em>, 4869–4875. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Relative navigation methods are a critical enabling technology for the next generation of autonomous spacecraft conducting close proximity operations. This is especially true for multi-agent inspection operations in which safety including intra-agent or agent-target collisions are a serious concern. Additionally, in an on-orbit servicing operation various failure modes of the target may result in unreliable a-priori knowledge or cooperation from the target. The main contribution of this work is the demonstration of a method for multi-agent, relative pose estimation that is robust to A) sensor blinding and B) dynamic uncertainty. This objective is accomplished leveraging GTSAM, an existing toolbox for the formulation of factor graphs, along with an algorithm for the efficient, real-time solution of such factor graphs, iSAM2. This estimation method is demonstrated in an example scenario with uncertain dynamics and sensor blinding due to sun position. Results revealed that the iSAM2-based method is capable of handling sensor blinding through leveraging an inter-agent range measurement, despite a dynamically uncertain environment.},
  archive   = {C_ICRA},
  author    = {Mark Mercier and David Curtis and Clark Taylor},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611394},
  month     = {5},
  pages     = {4869-4875},
  title     = {Measurement-limited multi-agent, relative pose estimation for on-orbit inspection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-robot human-in-the-loop control under spatiotemporal
specifications. <em>ICRA</em>, 4841–4847. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we present a coordination strategy tailored for scenarios involving multiple agents and tasks. We devise a range of tasks using signal temporal logic (STL), each earmarked for specific agents. These tasks are then imposed through control barrier function (CBF) constraints to ensure completion. To extend existing methodologies, our framework adeptly manages interactions among multiple agents. This extension is facilitated by leveraging nonlinear model predictive control (NMPC) to compute trajectories that avoid collisions. An integral aspect of our approach is the integration of a human-in-the-loop (HIL) model. This model enables real-time integration of human directives into the coordination process. A novel task allocation protocol is embedded within the frame-work to guide this process. We substantiate our methodology through a series of experiments, which corroborate the viability and relevance of our algorithms.},
  archive   = {C_ICRA},
  author    = {Yixiao Zhang and Victor Nan Fernandez-Ayala and Dimos V. Dimarogonas},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610123},
  month     = {5},
  pages     = {4841-4847},
  title     = {Multi-robot human-in-the-loop control under spatiotemporal specifications},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-robot cooperative navigation in crowds: A
game-theoretic learning-based model predictive control approach.
<em>ICRA</em>, 4834–4840. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we develop a control framework for the coordination of multiple robots as they navigate through crowded environments. Our framework comprises of a local model predictive control (MPC) for each robot and a social long short-term memory model that forecasts pedestrians’ trajectories. We formulate the local MPC formulation for each individual robot that includes both individual and shared objectives, in which the latter encourages the emergence of coordination among robots. Next, we consider the multi-robot navigation and human-robot interaction, respectively, as a potential game and a two-player game, then employ an iterative best response approach to solve the resulting optimization problems in a centralized and distributed fashion. Finally, we demonstrate the effectiveness of coordination among robots in simulated crowd navigation.},
  archive   = {C_ICRA},
  author    = {Viet-Anh Le and Vaishnav Tadiparthi and Behdad Chalaki and Hossein Nourkhiz Mahjoub and Jovin D’Sa and Ehsan Moradi-Pari and Andreas A. Malikopoulos},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611204},
  month     = {5},
  pages     = {4834-4840},
  title     = {Multi-robot cooperative navigation in crowds: A game-theoretic learning-based model predictive control approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Behavior tree capabilities for dynamic multi-robot task
allocation with heterogeneous robot teams. <em>ICRA</em>, 4826–4833. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While individual robots are becoming increasingly capable, the complexity of expected missions increases exponentially in comparison. To cope with this complexity, heterogeneous teams of robots have become a significant research interest in recent years. Making effective use of the robots and their unique skills in a team is challenging. Dynamic runtime conditions often make static task allocations infeasible, requiring a dynamic, capability-aware allocation of tasks to team members. To this end, we propose and implement a system that allows a user to specify missions using Behavior Trees (BTs), which can then, at runtime, be dynamically allocated to the current robot team. The system allows to statically model an individual robot’s capabilities within our ros_bt_py BT framework. It offers a runtime auction system to dynamically allocate tasks to the most capable robot in the current team. The system leverages utility values and pre-conditions to ensure that the allocation improves the overall mission execution quality while preventing faulty assignments. To evaluate the system, we simulated a find-and-decontaminate mission with a team of three heterogeneous robots and analyzed the utilization and overall mission times as metrics. Our results show that our system can improve the overall effectiveness of a team while allowing for intuitive mission specification and flexibility in the team composition.},
  archive   = {C_ICRA},
  author    = {Georg Heppner and David Oberacker and Arne Roennau and Rüdiger Dillmann},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610515},
  month     = {5},
  pages     = {4826-4833},
  title     = {Behavior tree capabilities for dynamic multi-robot task allocation with heterogeneous robot teams},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Is it a bug? Understanding physical unit mismatches in robot
software. <em>ICRA</em>, 4819–4825. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot software is abundant with variables that represent real-world physical units (e.g., meters, seconds). Operations over different units (e.g., adding meters and seconds) may be incorrect and can lead to dangerous system misbehaviors; manually detecting such mistakes is challenging. Current software analysis techniques identify such mismatches using dimensional analysis rules and ROS-specific assumptions to analyze the source code. However, these are ignorant of the fact that physical unit mismatches in robotics code are often intentional (e.g., when operating a differential drive robot), resulting in false positive bug reports that can impede robotics developer trust and productivity. In this work, we study how developers introduce physical unit mismatches by manually inspecting 180 errors detected by the software analysis technique, Phys. We identify three types of physical unit mismatches and present a taxonomy of eight high-level categories of how these errors manifest. We find that developers often make unforced and paradigmatic physical unit mismatches through differential drives, small angle approximations, and controls. We draw insights on current development to inform future research to better detect, categorize, and address meaningful physical unit mismatches.},
  archive   = {C_ICRA},
  author    = {Paulo Canelas and Trenton Tabor and John-Paul Ore and Alcides Fonseca and Claire Le Goues and Christopher S. Timperley},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611413},
  month     = {5},
  pages     = {4819-4825},
  title     = {Is it a bug? understanding physical unit mismatches in robot software},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scaling motion forecasting models with ensemble
distillation. <em>ICRA</em>, 4812–4818. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion forecasting has become an increasingly critical component of autonomous robotic systems. Onboard compute budgets typically limit the accuracy of real-time systems. In this work we propose methods of improving motion forecasting systems subject to limited compute budgets by combining model ensemble and distillation techniques. The use of ensembles of deep neural networks has been shown to improve generalization accuracy in many application domains. We first demonstrate significant performance gains by creating a large ensemble of optimized single models. We then develop a generalized framework to distill motion forecasting model ensembles into small student models which retain high performance with a fraction of the computing cost. For this study we focus on the task of motion forecasting using real world data from autonomous driving systems. We develop ensemble models that are very competitive on the Waymo Open Motion Dataset (WOMD) and Argoverse leaderboards. From these ensembles, we train distilled student models which have high performance at a fraction of the compute costs. These experiments demonstrate distillation from ensembles as an effective method for improving accuracy of predictive models for robotic systems with limited compute budgets.},
  archive   = {C_ICRA},
  author    = {Scott Ettinger and Kratarth Goel and Avikalp Srivastava and Rami Al-Rfou},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611196},
  month     = {5},
  pages     = {4812-4818},
  title     = {Scaling motion forecasting models with ensemble distillation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robot fine-tuning made easy: Pre-training rewards and
policies for autonomous real-world reinforcement learning.
<em>ICRA</em>, 4804–4811. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The pre-train and fine-tune paradigm in machine learning has had dramatic success in a wide range of domains because the use of existing data or pre-trained models on the internet enables quick and easy learning of new tasks. We aim to enable this paradigm in robotic reinforcement learning, allowing a robot to learn a new task with little human effort by leveraging data and models from the Internet. However, reinforcement learning often requires significant human effort in the form of manual reward specification or environment resets, even if the policy is pre-trained. We introduce RoboFuME, a reset-free fine-tuning system that pre-trains a multi-task manipulation policy from diverse datasets of prior experiences and self-improves online to learn a target task with minimal human intervention. Our insights are to utilize calibrated offline reinforcement learning techniques to ensure efficient online fine-tuning of a pre-trained policy in the presence of distribution shifts and leverage pre-trained vision language models (VLMs) to build a robust reward classifier for autonomously providing reward signals during the online fine-tuning process. In a diverse set of five real robot manipulation tasks, we show that our method can incorporate data from an existing robot dataset collected at a different institution and improve on a target task within as little as 3 hours of autonomous real-world experience. We also demonstrate in simulation experiments that our method outperforms prior works that use different RL algorithms or different approaches for predicting rewards. Project website: https://robofume.github.io},
  archive   = {C_ICRA},
  author    = {Jingyun Yang and Max Sobol Mark and Brandon Vu and Archit Sharma and Jeannette Bohg and Chelsea Finn},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610421},
  month     = {5},
  pages     = {4804-4811},
  title     = {Robot fine-tuning made easy: Pre-training rewards and policies for autonomous real-world reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dream2Real: Zero-shot 3D object rearrangement with
vision-language models. <em>ICRA</em>, 4796–4803. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce Dream2Real, a robotics framework which integrates vision-language models (VLMs) trained on 2D data into a 3D object rearrangement pipeline. This is achieved by the robot autonomously constructing a 3D representation of the scene, where objects can be rearranged virtually and an image of the resulting arrangement rendered. These renders are evaluated by a VLM, so that the arrangement which best satisfies the user instruction is selected and recreated in the real world with pick-and-place. This enables language-conditioned rearrangement to be performed zero-shot, without needing to collect a training dataset of example arrangements. Results on a series of real-world tasks show that this framework is robust to distractors, controllable by language, capable of understanding complex multi-object relations, and readily applicable to both tabletop and 6-DoF rearrangement tasks. Videos are available on our webpage at: https://www.robot-learning.uk/dream2real.},
  archive   = {C_ICRA},
  author    = {Ivan Kapelyukh and Yifei Ren and Ignacio Alzugaray and Edward Johns},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611220},
  month     = {5},
  pages     = {4796-4803},
  title     = {Dream2Real: Zero-shot 3D object rearrangement with vision-language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RoboAgent: Generalization and efficiency in robot
manipulation via semantic augmentations and action chunking.
<em>ICRA</em>, 4788–4795. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The grand aim of having a single robot that can manipulate arbitrary objects in diverse settings is at odds with the paucity of robotics datasets. Acquiring and growing such datasets is strenuous due to manual efforts, operational costs, and safety challenges. A path toward such a universal agent requires an efficient framework capable of generalization but within a reasonable data budget. In this paper, we develop an efficient framework (MT-ACT) for training universal agents capable of multi-task manipulation skills using (a) semantic augmentations that can rapidly multiply existing datasets and (b) action representations that can extract performant policies with small yet diverse multi-modal datasets without overfitting. In addition, reliable task conditioning and an expressive policy architecture enables our agent to exhibit a diverse repertoire of skills in novel situations specified using task commands. Using merely 7500 demonstrations, we are able to train a single policy RoboAgent capable of 12 unique skills, and demonstrate its generalization over 38 tasks spread across common daily activities in diverse kitchen scenes. On average, RoboAgent outperforms prior methods by over 40% in unseen situations while being more sample efficient. See https://robopen.github.io/for video results and appendix.},
  archive   = {C_ICRA},
  author    = {Homanga Bharadhwaj and Jay Vakil and Mohit Sharma and Abhinav Gupta and Shubham Tulsiani and Vikash Kumar},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611293},
  month     = {5},
  pages     = {4788-4795},
  title     = {RoboAgent: Generalization and efficiency in robot manipulation via semantic augmentations and action chunking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SpawnNet: Learning generalizable visuomotor skills from
pre-trained network. <em>ICRA</em>, 4781–4787. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The existing internet-scale image and video datasets cover a wide range of everyday objects and tasks, bringing the potential of learning policies that generalize in diverse scenarios. Prior works have explored visual pre-training with different self-supervised objectives. Still, the generalization capabilities of the learned policies and the advantages over well-tuned baselines remain unclear from prior studies. In this work, we present a focused study of the generalization capabilities of the pre-trained visual representations at the categorical level. We identify the key bottleneck in using a frozen pre-trained visual backbone for policy learning and then propose SpawnNet, a novel two-stream architecture that learns to fuse pre-trained multi-layer representations into a separate network to learn a robust policy. Through extensive simulated and real experiments, we show significantly better categorical generalization compared to prior approaches in imitation learning settings. Open-sourced code and videos can be found on our website: https://xingyu-lin.github.io/spawnnet/.},
  archive   = {C_ICRA},
  author    = {Xingyu Lin and John So and Sashwat Mahalingam and Fangchen Liu and Pieter Abbeel},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610356},
  month     = {5},
  pages     = {4781-4787},
  title     = {SpawnNet: Learning generalizable visuomotor skills from pre-trained network},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Increasing the absolute position accuracy of industrial
robots by means of a deep continual evidential regression model *.
<em>ICRA</em>, 4774–4780. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The use of industrial robots represents a key technology for increasing productivity and efficiency in manufacturing. However, their low absolute position accuracy still denies the broad substitution of machine tools by industrial robots. In this paper, a data-driven method for accuracy enhancement of industrial robots under consideration of kinematic, elastic, and thermal effects is presented. A continual learning algorithm is proposed, which allows to train the model in a process-parallel manner without suffering from catastrophic forgetting. Furthermore, the model is able to determine confidence intervals of the prediction values and thus supports further processing in safety-relevant applications. The effectiveness of the model can be demonstrated using a large data stream with about 3,000 real data points. As a result, it can be shown that the absolute position accuracy of the industrial robot can be improved by 96 % with the proposed method.},
  archive   = {C_ICRA},
  author    = {Eckart Uhlmann and Mitchel Polte and Julian Blumberg and Sheng Yin and Gang Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611053},
  month     = {5},
  pages     = {4774-4780},
  title     = {Increasing the absolute position accuracy of industrial robots by means of a deep continual evidential regression model *},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WOMD-LiDAR: Raw sensor dataset benchmark for motion
forecasting. <em>ICRA</em>, 4766–4773. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Widely adopted motion forecasting datasets sub-stitute the observed sensory inputs with higher-level abstractions such as 3D boxes and polylines. These sparse shapes are inferred through annotating the original scenes with perception systems’ predictions. Such intermediate representations tie the quality of the motion forecasting models to the performance of computer vision models. Moreover, the human-designed explicit interfaces between perception and motion forecasting typically pass only a subset of the semantic information present in the original sensory input. To study the effect of these modular approaches, design new paradigms that mitigate these limitations, and accelerate the development of end-to-end motion forecasting models, we augment the Waymo Open Motion Dataset (WOMD) with large-scale, high-quality, diverse LiDAR data for the motion forecasting task.The new augmented dataset (WOMD-LiDAR) 1 consists of over 100,000 scenes that each spans 20 seconds, consisting of well-synchronized and calibrated high quality LiDAR point clouds captured across a range of urban and suburban geographies. Compared to Waymo Open Dataset (WOD), WOMDLiDAR dataset contains 100× more scenes. Furthermore, we integrate the LiDAR data into the motion forecasting model training and provide a strong baseline. Experiments show that the LiDAR data brings improvement in the motion forecasting task. We hope that WOMD-LiDAR will provide new opportunities for boosting end-to-end motion forecasting models.},
  archive   = {C_ICRA},
  author    = {Kan Chen and Runzhou Ge and Hang Qiu and Rami Ai-Rfou and Charles Qi and Xuanyu Zhou and Zoey Yang and Scott Ettinger and Pei Sun and Zhaoqi Leng and Mustafa Baniodeh and Ivan Bogun and Weiyue Wang and Mingxing Tan and Dragomir Anguelov},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610651},
  month     = {5},
  pages     = {4766-4773},
  title     = {WOMD-LiDAR: Raw sensor dataset benchmark for motion forecasting},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OpenBot-fleet: A system for collective learning with real
robots. <em>ICRA</em>, 4758–4765. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce OpenBot-Fleet, a comprehensive open-source cloud robotics system for navigation. OpenBot-Fleet uses smartphones for sensing, local compute and communication, Google Firebase for secure cloud storage and off-board compute, and a robust yet low-cost wheeled robot to act in real-world environments. The robots collect task data and upload it to the cloud where navigation policies can be learned either offline or online and can then be sent back to the robot fleet. In our experiments we distribute 72 robots to a crowd of workers who operate them in homes, and show that OpenBot-Fleet can learn robust navigation policies that generalize to unseen homes with &gt;80% success rate. OpenBot-Fleet represents a significant step forward in cloud robotics, making it possible to deploy large continually learning robot fleets in a cost-effective and scalable manner. All materials can be found at https://www.openbot.org/.},
  archive   = {C_ICRA},
  author    = {Matthias Müller and Samarth Brahmbhatt and Ankur Deka and Quentin Leboutet and David Hafner and Vladlen Koltun},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610960},
  month     = {5},
  pages     = {4758-4765},
  title     = {OpenBot-fleet: A system for collective learning with real robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Torque transmission in double-tendon sheath driven actuators
for application in exoskeletons. <em>ICRA</em>, 4751–4757. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bowden cables serve as essential components in various mechanical systems, facilitating power transmission from remote actuators to specific destinations. The pretension of Bowden cables profoundly influences system performance, notably in terms of friction. This study investigates the effects of cable pretension and shape on friction and torque efficiency. A custom self-designed testbed, comprising integrated actuator units, pulleys, and a novel pretension mechanism connected by Bowden cables, is utilized to conduct experimental tests under varying parameters. This work adopts an integrated approach of experimentation, modeling, and validation, offering preliminary insights into the torque transmission characteristics of tendon driven actuator systems. Additionally, the precise model exhibits excellent conformity across a broad range of shapes and provides initial insights into hysteresis modeling attributable to cable material properties.},
  archive   = {C_ICRA},
  author    = {Daniel Pérez-Suay and Yu Li and Hamid Sadeghian and Abdeldjallil Naceri and Sami Haddadin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610254},
  month     = {5},
  pages     = {4751-4757},
  title     = {Torque transmission in double-tendon sheath driven actuators for application in exoskeletons},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Elasto-static modelling and identification of a deployable
cable-driven parallel robot with compliant masts*. <em>ICRA</em>,
4744–4750. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Some cable-driven parallel robots (CDPRs) can be rapidly deployed on-site. To achieve such deployability, the fixed frame is usually substituted by four masts. However, not having any rigid fixture between the masts reduces the overall stiffness of the CDPR. This paper introduces a CDPR called Rocaspect, that has four compliant masts. The robot behavior and accuracy is evaluated experimentally and three different mast models are proposed.},
  archive   = {C_ICRA},
  author    = {Zane Zane and Stéphane Caro},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610703},
  month     = {5},
  pages     = {4744-4750},
  title     = {Elasto-static modelling and identification of a deployable cable-driven parallel robot with compliant masts*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Investigation on the multi-solution problem of the
kinetostatics of cable-driven continuum manipulators. <em>ICRA</em>,
4700–4705. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cable-driven continuum manipulators have gained considerable attention due to their high dexterity and inherent structural compliance, making them a popular research topic. However, previous studies have overlooked the kinetostatics of these manipulators, which can result in a multi-solution problem. This issue is critical as having multiple equilibrium states can lead to erroneous estimations of the manipulator&#39;s profile. To address this issue, the kinetostatic model is presented and simulations based on both the interval analysis method and the commonly used floating-point optimization algorithm are conducted under the same actuating forces and external loads. Results show that there are multiple solutions to the kinetostatics of cable-driven continuum manipulators with constant cross section or variable cross section. This paper fills a gap in the current literature and offers valuable insights for researchers in the field of cable-driven continuum manipulators.},
  archive   = {C_ICRA},
  author    = {Yicheng Dai and Zuan Li and Xin Wang and Han Yuan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610795},
  month     = {5},
  pages     = {4700-4705},
  title     = {Investigation on the multi-solution problem of the kinetostatics of cable-driven continuum manipulators},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid force-position control of an elastic tendon-driven
scrubbing robot (TEDSR). <em>ICRA</em>, 4693–4699. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There is a lack of cleaning robots dedicated to the scrubbing of contaminated surfaces. Contaminated surfaces in domestic and industrial settings typically require manual scrubbing which can be costly or hazardous. There is growing demand for automated sanitization systems in hospitals, food-processing plants, and other settings where cleanliness of surfaces is important. To address the opportunity to automate the scrubbing of surfaces, this work focuses on the use of series elastic actuators which can apply consistent trajectories of scrubbing force. Consistent force during scrubbing increases the rate of removal for a contaminant. An elastic robot which has rigid links and low-stiffness joints can perform friction-based cleaning of surfaces with complex geometries while maintaining consistent scrubbing force. This study uses a hybrid force-position control scheme and a low-cost elastic robot to perform scrubbing. This study observes the relationship between joint stiffness in the robot and the disturbance rejection for forcebased control during scrubbing.},
  archive   = {C_ICRA},
  author    = {Noah Harmatz and Alina Zahra and Amir Abdelmalak and Shivam Purohit and Trevor Shin and Aaron D. Mazzeo},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610594},
  month     = {5},
  pages     = {4693-4699},
  title     = {Hybrid force-position control of an elastic tendon-driven scrubbing robot (TEDSR)},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and validation of a variable stiffness spiral cam
actuator (VS-SCA). <em>ICRA</em>, 4686–4692. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study presents the design and validation of a variable stiffness actuator incorporating multiple cam mechanisms. The actuator is intended for use in walking assistance, focusing on assisting individuals with diminished ankle function. This study highlights the advantages of variable stiffness actuators over traditional and other modern actuators in mobility assistance. The working principles of the proposed Variable Stiffness Spiral Cam Actuator (VS-SCA) are described, focusing on the cantilever beams with adjustable supports, main cam mechanism, and symmetric support positioning architecture utilizing an Archimedean spiral cam. The design and fabrication process are discussed, considering system design considerations, cantilever beam design, cam design, and spiral cam design. The analytical methodology used for validation is also presented, which connects the subsystems of the actuator and allows for the determination of effective torsional stiffness. The experimental validation showed that the VS-SCA provides a range of stiffness from 20 to 75 Nm/rad for dorsiflexion, necessary for providing ankle assistance during the push-off phase of walking, while maintaining low stiffness (4 - 12 Nm/rad) for plantarflexion not to hinder natural ankle motion in the swing phase.},
  archive   = {C_ICRA},
  author    = {Matthew R. Auer and Suhrud P. Joglekar and Hyunglae Lee},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610506},
  month     = {5},
  pages     = {4686-4692},
  title     = {Design and validation of a variable stiffness spiral cam actuator (VS-SCA)},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-sensing feedback control of an electrohydraulic robotic
shoulder. <em>ICRA</em>, 4679–4685. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The human shoulder, with its glenohumeral joint, tendons, ligaments, and muscles, allows for the execution of complex tasks with precision and efficiency. However, current robotic shoulder designs lack the compliance and compactness inherent in their biological counterparts. A major limitation of these designs is their reliance on external sensors like rotary encoders, which restrict mechanical joint design and introduce bulk to the system. To address this constraint, we present a bio-inspired antagonistic robotic shoulder with two degrees of freedom powered by self-sensing hydraulically amplified self-healing electrostatic actuators. Our artificial muscle design decouples the high-voltage electrostatic actuation from the pair of low-voltage self-sensing electrodes. This approach allows for proprioceptive feedback control of trajectories in the task space while eliminating the necessity for any additional sensors. We assess the platform’s efficacy by comparing it to a feedback control based on position data provided by a motion capture system. The study demonstrates closed-loop controllable robotic manipulators based on an inherent self-sensing capability of electrohydraulic actuators. The proposed architecture can serve as a basis for complex musculoskeletal joint arrangements.},
  archive   = {C_ICRA},
  author    = {Clemens C. Christoph and Amirhossein Kazemipour and Michel R. Vogt and Yu Zhang and Robert K. Katzschmann},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610994},
  month     = {5},
  pages     = {4679-4685},
  title     = {Self-sensing feedback control of an electrohydraulic robotic shoulder},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ASPIRe: An informative trajectory planner with mutual
information approximation for target search and tracking. <em>ICRA</em>,
4626–4632. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes an informative trajectory planning approach, namely, adaptive particle filter tree with sigma point-based mutual information reward approximation (ASPIRe), for mobile target search and tracking (SAT) in cluttered environments with limited sensing field of view. We develop a novel sigma point-based approximation to accurately estimate mutual information (MI) for general, non-Gaussian distributions utilizing particle representation of the belief state, while simultaneously maintaining high computational efficiency. Building upon the MI approximation, we develop the Adaptive Particle Filter Tree (APFT) approach with MI as the reward, which features belief state tree nodes for informative trajectory planning in continuous state and measurement spaces. An adaptive criterion is proposed in APFT to adjust the planning horizon based on the expected information gain. Simulations and physical experiments demonstrate that ASPIRe achieves real-time computation and outperforms benchmark methods in terms of both search efficiency and estimation accuracy.},
  archive   = {C_ICRA},
  author    = {Kangjie Zhou and Pengying Wu and Yao Su and Han Gao and Ji Ma and Hangxin Liu and Chang Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611500},
  month     = {5},
  pages     = {4626-4632},
  title     = {ASPIRe: An informative trajectory planner with mutual information approximation for target search and tracking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Choosing the right tool for the job: Online decision making
over SLAM algorithms. <em>ICRA</em>, 4619–4625. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Nearly all state-of-the-art SLAM algorithms are designed to exploit patterns in data from specific sensing modalities, such as time-of-flight and structured light depth sensors, or RGB cameras. This specialization increases localization accuracy in domains where the given modality detects many high-quality features, but comes at the cost of decreasing performance in other, less favorable environments. For robotic systems that may experience a wide variety of sensing conditions, this difficulty in generalization presents a significant challenge. In this paper, we propose running several computationally cheap SLAM front ends in parallel and choosing the most promising feature set online. This problem is similar to the Algorithm Selection Problem (ASP), but has several complicating factors that preclude application of existing methods. We first provide an extension of the ASP formalism that captures the unique challenges in the SLAM setting, and then, based on this formalism, we propose modeling the SLAM ASP as a partially observable Markov decision process (POMDP). Our experiments show that dynamically selecting SLAM front ends, even myopically, improves localization robustness compared to selecting a static front end, and that using a POMDP policy provides even greater improvement.},
  archive   = {C_ICRA},
  author    = {Samer B. Nashed and Roderic A. Grupen and Shlomo Zilberstein},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610827},
  month     = {5},
  pages     = {4619-4625},
  title     = {Choosing the right tool for the job: Online decision making over SLAM algorithms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weathering ongoing uncertainty: Learning and planning in a
time-varying partially observable environment. <em>ICRA</em>, 4612–4618.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Optimal decision-making presents a significant challenge for autonomous systems operating in uncertain, stochastic and time-varying environments. Environmental variability over time can significantly impact the system’s optimal decision making strategy for mission completion. To model such environments, our work combines the previous notion of Time-Varying Markov Decision Processes (TVMDP) with partial observability and introduces Time-Varying Partially Observable Markov Decision Processes (TV-POMDP). We propose a twopronged approach to accurately estimate and plan within the TV-POMDP: 1) Memory Prioritized State Estimation (MPSE), which leverages weighted memory to provide more accurate time-varying transition estimates; and 2) an MPSE-integrated planning strategy that optimizes long-term rewards while accounting for temporal constraint. We validate the proposed framework and algorithms using simulations and hardware, with robots exploring a partially observable, time-varying environments. Our results demonstrate superior performance over standard methods, highlighting the framework’s effectiveness in stochastic, uncertain, time-varying domains.},
  archive   = {C_ICRA},
  author    = {Gokul Puthumanaillam and Xiangyu Liu and Negar Mehr and Melkior Ornik},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610954},
  month     = {5},
  pages     = {4612-4618},
  title     = {Weathering ongoing uncertainty: Learning and planning in a time-varying partially observable environment},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Perceptual factors for environmental modeling in robotic
active perception. <em>ICRA</em>, 4605–4611. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurately assessing the potential value of new sensor observations is a critical aspect of planning for active perception. This task is particularly challenging when reasoning about high-level scene understanding using measurements from vision-based neural networks. Due to appearance-based reasoning, the measurements are susceptible to several environmental effects such as the presence of occluders, variations in lighting conditions, and redundancy of information due to similarity in appearance between nearby viewpoints. To address this, we propose a new active perception framework incorporating an arbitrary number of perceptual effects in planning and fusion. Our method models the correlation with the environment by a set of general functions termed perceptual factors to construct a perceptual map, which quantifies the aggregated influence of the environment on candidate viewpoints. This information is seamlessly incorporated into the planning and fusion processes by adjusting the uncertainty associated with measurements to weigh their contributions. We evaluate our perceptual maps in a simulated environment that reproduces environmental conditions common in robotics applications. Our results show that, by accounting for environmental effects within our perceptual maps, we improve the state estimation by correctly selecting the viewpoints and considering the measurement noise correctly when affected by environmental factors. We furthermore deploy our approach on a ground robot to showcase its applicability for real-world active perception missions.},
  archive   = {C_ICRA},
  author    = {David Morilla-Cabello and Jonas Westheider and Marija Popović and Eduardo Montijano},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611380},
  month     = {5},
  pages     = {4605-4611},
  title     = {Perceptual factors for environmental modeling in robotic active perception},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-sample long range path planning under sensing
uncertainty for off-road autonomous driving. <em>ICRA</em>, 4598–4604.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We focus on the problem of long-range dynamic replanning for off-road autonomous vehicles, where a robot plans paths through a previously unobserved environment while continuously receiving noisy local observations. An effective approach for planning under sensing uncertainty is determinization, where one converts a stochastic world into a deterministic one and plans under this simplification. This makes the planning problem tractable, but the cost of following the planned path in the real world may be different than in the determinized world. This causes collisions if the determinized world optimistically ignores obstacles, or causes unnecessarily long routes if the determinized world pessimistically imagines more obstacles. We aim to be robust to uncertainty over potential worlds while still achieving the efficiency benefits of determinization. We evaluate algorithms for dynamic replanning on a large real-world dataset of challenging long-range planning problems from the DARPA RACER program. Our method, Dynamic Replanning via Evaluating and Aggregating Multiple Samples (DREAMS), outperforms other determinization-based approaches in terms of combined traversal time and collision cost. https://sites.google.com/cs.washington.edu/dreams/},
  archive   = {C_ICRA},
  author    = {Matt Schmittle and Rohan Baijal and Brian Hou and Siddhartha Srinivasa and Byron Boots},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610476},
  month     = {5},
  pages     = {4598-4604},
  title     = {Multi-sample long range path planning under sensing uncertainty for off-road autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Observer-based distributed MPC for collaborative
quadrotor-quadruped manipulation of a cable-towed load. <em>ICRA</em>,
4591–4597. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a collaborative quadrotor-quadruped robot system for the manipulation of a cable-towed payload. In particular, we aim to solve the challenge from the unknown dynamics of the cable-towed payload. To this end, we first propose novel dynamic models for both the quadrotor and the quadruped robot, taking into account the nonlinear robot dynamics and the uncertainties associated with the cable-towed load. Moreover, we design observers for the hybrid interaction between the robots and the payload. Theoretically, the convergence of these observers is analyzed using Lyapunov functions under mild technical assumptions. Finally, we seamlessly integrate the dynamics models and the observers into a distributed Model Predictive Control (MPC) framework with kinematics limitations and collision avoidance constraints. The proposed system is validated through challenging field experiments in indoor and outdoor environments, involving push disturbances, varying and unknown payloads, uneven terrains, etc.},
  archive   = {C_ICRA},
  author    = {Shaohang Xu and Yian Wang and Wentao Zhang and Chin Pang Ho and Lijun Zhu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610348},
  month     = {5},
  pages     = {4591-4597},
  title     = {Observer-based distributed MPC for collaborative quadrotor-quadruped manipulation of a cable-towed load},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uncertainty-bounded active monitoring of unknown dynamic
targets in road-networks with minimum fleet. <em>ICRA</em>, 4584–4590.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fleets of unmanned robots can be beneficial for the long-term monitoring of large areas, e.g., to monitor wild flocks, detect intruders, search and rescue. Monitoring numerous dynamic targets in a collaborative and efficient way is a challenging problem that requires online coordination and information fusion. The majority of existing works either assume a passive all-to-all observation model to minimize the summed uncertainties over all targets by all robots, or optimize over the jointed discrete actions while neglecting the dynamic constraints of the robots and unknown behaviors of the targets. This work proposes an online task and motion coordination algorithm that ensures an explicitly-bounded estimation uncertainty for the target states, while minimizing the average number of active robots. The robots have a limited-range perception to actively track a limited number of targets simultaneously, of which their future control decisions are all unknown. It includes: (i) the assignment of monitoring tasks, modeled as a flexible size multiple vehicle routing problem with time windows (m-MVRPTW), given the predicted target trajectories with uncertainty measure in the road-networks; (ii) the nonlinear model predictive control (NMPC) for optimizing the robot trajectories under uncertainty and safety constraints. It is shown that the robots can switch between active and inactive roles dynamically online as required by the unknown monitoring task. The proposed methods are validated via large-scale simulations of up to 100 robots and targets.},
  archive   = {C_ICRA},
  author    = {Shuaikang Wang and Yiannis Kantaros and Meng Guo},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610495},
  month     = {5},
  pages     = {4584-4590},
  title     = {Uncertainty-bounded active monitoring of unknown dynamic targets in road-networks with minimum fleet},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Asynchronous distributed smoothing and mapping via
on-manifold consensus ADMM. <em>ICRA</em>, 4577–4583. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we present a fully distributed, asynchronous, and general purpose optimization algorithm for Consensus Simultaneous Localization and Mapping (CSLAM). Multi-robot teams require that agents have timely and accurate solutions to their state as well as the states of the other robots in the team. To optimize this solution we develop a CSLAM back-end based on Consensus ADMM called MESA (Manifold, Edge-based, Separable ADMM). MESA is fully distributed to tolerate failures of individual robots, asynchronous to tolerate communication delays and outages, and general purpose to handle any CSLAM problem formulation. We demonstrate that MESA exhibits superior convergence rates and accuracy compare to existing state-of-the art CSLAM back-end optimizers.},
  archive   = {C_ICRA},
  author    = {Daniel McGann and Kyle Lassak and Michael Kaess},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611193},
  month     = {5},
  pages     = {4577-4583},
  title     = {Asynchronous distributed smoothing and mapping via on-manifold consensus ADMM},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning for dynamic subteaming and voluntary waiting in
heterogeneous multi-robot collaborative scheduling. <em>ICRA</em>,
4569–4576. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Coordinating heterogeneous robots is essential for autonomous multi-robot teaming. To execute a set of dependent tasks as quickly as possible, and to complete tasks that cannot be addressed by individual robots, it is necessary to form subteams that can collaboratively finish the tasks. It is also advantageous for robots to wait for teammates and tasks to become available in order to form better subteams or reduce the overall completion time. To enable both abilities, we introduce a new graph learning approach that formulates heterogeneous collaborative scheduling as a bipartite matching problem that maximizes a reward matrix learned via imitation learning. We design a novel graph attention transformer network (GATN) that represents the problem of collaborative scheduling as a bipartite graph, and integrates both local and global graph information to estimate the reward matrix using graph attention networks and transformers. By relaxing the constraint of one-to-one correspondence in bipartite matching, our approach allows multiple robots to address the same task as a subteam. Our approach also enables voluntary waiting by introducing an idle task that the robots can select to wait. Experimental results have shown that our approach well addresses heterogeneous collaborative scheduling with dynamic subteam formation and voluntary waiting, and outperforms the previous and baseline methods.},
  archive   = {C_ICRA},
  author    = {Williard Joshua Jose and Hao Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610342},
  month     = {5},
  pages     = {4569-4576},
  title     = {Learning for dynamic subteaming and voluntary waiting in heterogeneous multi-robot collaborative scheduling},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Do we run large-scale multi-robot systems on the edge? More
evidence for two-phase performance in system size scaling.
<em>ICRA</em>, 4562–4568. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With increasing numbers of mobile robots arriving in real-world applications, more robots coexist in the same space, interact, and possibly collaborate. Methods to provide such systems with system size scalability are known, for example, from swarm robotics. Example strategies are self-organizing behavior, a strict decentralized approach, and limiting the robot-robot communication. Despite applying such strategies, any multi-robot system breaks above a certain critical system size (i.e., number of robots) as too many robots share a resource (e.g., space, communication channel). We provide additional evidence based on simulations, that at these critical system sizes, the system performance separates into two phases: nearly optimal and minimal performance. We speculate that in real-world applications that are configured for optimal system size, the supposedly high-performing system may actually live on borrowed time as it is on a transient to breakdown. We provide two modeling options (based on queueing theory and a population model) that may help to support this reasoning.},
  archive   = {C_ICRA},
  author    = {Jonas Kuckling and Robin Luckey and Viktor Avrutin and Andrew Vardy and Andreagiovanni Reina and Heiko Hamann},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610771},
  month     = {5},
  pages     = {4562-4568},
  title     = {Do we run large-scale multi-robot systems on the edge? more evidence for two-phase performance in system size scaling},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid volitional control of a robotic transtibial
prosthesis using a phase variable impedance controller. <em>ICRA</em>,
4555–4561. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For robotic transtibial prosthesis control, the global tibia kinematics can be used to monitor gait cycle progression and command smooth and continuous actuation. In this work, these global tibia kinematics define a phase variable impedance controller (PVIC), which is implemented as the nonvolitional base controller within a hybrid volitional control framework (PVI-HVC). The gait progression estimation and biomechanic performance of one able-bodied individual walking on a robotic ankle prosthesis via a bypass adapter are compared for three control schemes: benchmark passive controller, PVIC, and PVI-HVC. The different actuation of each had a direct effect on the global tibia kinematics, but the average deviation between the estimated and ground truth gait percentages were 1.6%, 1.8%, and 2.1%, respectively, for each controller. Both PVIC and PVI-HVC produced good agreement with able-bodied kinematic and kinetic references. As designed, PVI-HVC results were similar to those of PVIC when the user used low volitional intent, but yielded higher peak plantarflexion, peak torque, and peak power when the user commanded high volitional input in late stance. This additional torque and power also allowed the user to volitionally and continuously achieve activities beyond level walking, such as ascending ramps, avoiding obstacles, standing on tip-toes, and tapping the foot. In this way, PVI-HVC offers the kinetic and kinematic performance of the PVIC during level ground walking, along with the freedom to volitionally pursue alternative activities.},
  archive   = {C_ICRA},
  author    = {Ryan R. Posh and Jonathan A. Tittle and David J. Kelly and James P. Schmiedeler and Patrick M. Wensing},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610930},
  month     = {5},
  pages     = {4555-4561},
  title     = {Hybrid volitional control of a robotic transtibial prosthesis using a phase variable impedance controller},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Colibri5: Real-time monocular 5-DoF trocar pose tracking for
robot-assisted vitreoretinal surgery. <em>ICRA</em>, 4547–4554. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Retinal surgery is a complex medical procedure that requires high precision dexterity to perform delicate instrument maneuvers with sub-millimeter accuracy. Minimizing the manual tremor and achieving precise and repeatable execution of surgical tasks has motivated the development of robotic platforms to overcome the limitations of manual surgery. However, specific tasks, such as instrument insertion through the trocar, are more challenging in robotic surgery than in conventional manual procedures since the robot control is often optimized for navigation inside the eye. This challenges the integration of robotic systems, creating a high cognitive load on the operator and prolonging the surgery time. Moreover, misalignment of the robot’s remote center of motion (RCM) and trocar position during the procedure can lead to excessive forces between the instrument and the trocar, potentially causing patient trauma. Precise and rapid localization of the trocars enables the automation of the insertion procedure and dynamic compensation of eye motion.In this work, we present a real-time marker-less method for 3D pose tracking of trocar, achieved with only a single monocular camera. Our experiments show promising results towards real-time trocar pose estimation and tracking, achieving an average error of 3 ◦ in trocar orientation estimation, with an average processing time of 15 fps. This could serve as a foundation to improve robotic systems’ automation, integration, and efficiency of robotic systems for retinal surgery. The dataset created for this work is made publicly available.},
  archive   = {C_ICRA},
  author    = {Shervin Dehghani and Michael Sommersperger and Mahdi Saleh and Alireza Alikhani and Benjamin Busam and Peter Gehlbach and Iulian Iordachita and Nassir Navab and M. Ali Nasseri},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610576},
  month     = {5},
  pages     = {4547-4554},
  title     = {Colibri5: Real-time monocular 5-DoF trocar pose tracking for robot-assisted vitreoretinal surgery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and implementation of a robotized hand-held dissector
for endoscopic pulmonary endarterectomy. <em>ICRA</em>, 4541–4546. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Severe chronic pulmonary endarterectomy needs a dissector to delicately remove proliferative intima located in the depth of the pulmonary artery. This work proposed a novel endoscopic robotized steerable dissector for this surgery, enabling easier access to curved deep artery branches. The handheld surgical dissector also provides suction and visualization for surgeons to enhance effectiveness. The steerable section is a cable-driven hinged structure, and through an antagonistic mechanism regulating the cable tension, the overall stiffness is adjusted to adapt various surroundings. The mapping between actuation space and shape configuration and tip force estimation model are respectively established for further closed-loop control scheme, achieving adaptive positioning and safe surgery. Experiments first demonstrate the feasibility of the proposed models and ex vitro trials validated the usage and effectiveness of the robotized dissector.},
  archive   = {C_ICRA},
  author    = {Runfeng Zhu and Xilong Hou and Wei Huang and Lei Du and Zhong Wu and Hongbin Liu and Henry K. Chu and Qingxiang Zhao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610104},
  month     = {5},
  pages     = {4541-4546},
  title     = {Design and implementation of a robotized hand-held dissector for endoscopic pulmonary endarterectomy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient model learning and adaptive tracking control of
magnetic micro-robots for non-contact manipulation. <em>ICRA</em>,
4534–4540. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Magnetic microrobots can be navigated by an external magnetic field to autonomously move within living organisms with complex and unstructured environments. Potential applications include drug delivery, diagnostics, and therapeutic interventions. Existing techniques commonly impart magnetic properties to the target object, or drive the robot to contact and then manipulate the object, both probably inducing physical damage. This paper considers a non-contact formulation, where the robot spins to generate a repulsive field to push the object without physical contact. Under such a formulation, the main challenge is that the motion model between the input of the magnetic field and the output velocity of the target object is commonly unknown and difficult to analyze. To deal with it, this paper proposes a data-driven-based solution. A neural network is constructed to efficiently estimate the motion model. Then, an approximate model-based optimal control scheme is developed to push the object to track a time-varying trajectory, maintaining the non-contact with distance constraints. Furthermore, a straightforward planner is introduced to assess the adaptability of non-contact manipulation in a cluttered unstructured environment. Experimental results are presented to show the tracking and navigation performance of the proposed scheme.},
  archive   = {C_ICRA},
  author    = {Yongyi Jia and Shu Miao and Junjian Zhou and Niandong Jiao and Lianqing Liu and Xiang Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610098},
  month     = {5},
  pages     = {4534-4540},
  title     = {Efficient model learning and adaptive tracking control of magnetic micro-robots for non-contact manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intraoperatively iterative hough transform based in-plane
hybrid control of arterial robotic ultrasound for magnetic
catheterization. <em>ICRA</em>, 4528–4533. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an intraoperatively iterative Hough transform (IHT) based in-plane hybrid control of extracorporeal ultrasound (US) guided magnetic catheterization for arterial intervention. One uniqueness lies in that both control and tracking of the arterial robotic ultrasound end-effector have been implemented to improve performance. Firstly, the magnetic catheter model and hybrid visual/force servoing control scheme of the extracorporeal ultrasound-integrated tracking arm (EUTA) are derived based on the interaction Jacobian matrix and impedance modeling. Meanwhile, we implement a tracking method of in-plane ultrasound catheter’s tip and detection of vascular boundaries utilizing intensity-level iterative Hough-transform with Iterative End-Ponit Fitting (IEPF). The effectiveness of the proposed control and tracking method has been verified by conducting in vitro experimental studies for catheter steering of a soft tissue-imitating phantom. Results show that an average steering error of 0.56 mm and signal-to-noise-ratio (SNR) of 12.2 are obtained for the ultrasound imaging at high synchronization along with a low target lost rate (15.8%) and constant-force tracking (2.50±1.02 N).},
  archive   = {C_ICRA},
  author    = {Zhengyang Li and Magejiang Yeerbulati and Qingsong Xu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611682},
  month     = {5},
  pages     = {4528-4533},
  title     = {Intraoperatively iterative hough transform based in-plane hybrid control of arterial robotic ultrasound for magnetic catheterization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exoskeleton-mediated physical human-human interaction for a
sit-to-stand rehabilitation task. <em>ICRA</em>, 4521–4527. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sit-to-Stand (StS) is a fundamental daily activity that can be challenging for stroke survivors due to strength, motor control, and proprioception deficits in their lower limbs. Existing therapies involve repetitive StS exercises, but these can be physically demanding for therapists while assistive devices may limit patient participation and hinder motor learning. To address these challenges, this work proposes the use of two lower-limb exoskeletons to mediate physical interaction between therapists and patients during a StS rehabilitative task. This approach offers several advantages, including improved therapist-patient interaction, safety enforcement, and performance quantification. The whole body control of the two exoskeletons transmits online feedback between the two users, but at the same time assists in movement and ensures balance, and thus helping subjects with greater difficulty. In this study we present the architecture of the framework, presenting and discussing some technical choices made in the design.},
  archive   = {C_ICRA},
  author    = {Lorenzo Vianello and Emek Barış Küçüktabak and Matthew Short and Clément Lhoste and Lorenzo Amato and Kevin Lynch and Jose Pons},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610796},
  month     = {5},
  pages     = {4521-4527},
  title     = {Exoskeleton-mediated physical human-human interaction for a sit-to-stand rehabilitation task},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mixed traffic control and coordination from pixels.
<em>ICRA</em>, 4488–4494. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traffic congestion is a persistent problem in our society. Previous methods for traffic control have proven futile in alleviating current congestion levels leading researchers to explore ideas with robot vehicles given the increased emergence of vehicles with different levels of autonomy on our roads. This gives rise to mixed traffic control, where robot vehicles regulate human-driven vehicles through reinforcement learning (RL). However, most existing studies use precise observations that require domain expertise and hand engineering for each road network’s observation space. Additionally, precise observations use global information, such as environment outflow, and local information, i.e., vehicle positions and velocities. Obtaining this information requires updating existing road infrastructure with vast sensor environments and communication to potentially unwilling human drivers. We consider image observations, a modality that has not been extensively explored for mixed traffic control via RL, as the alternative: 1) images do not require a complete re-imagination of the observation space from environment to environment; 2) images are ubiquitous through satellite imagery, in-car camera systems, and traffic monitoring systems; and 3) images only require communication to equipment. In this work, we show robot vehicles using image observations can achieve competitive performance to using precise information on environments, including ring, figure eight, intersection, merge, and bottleneck. In certain scenarios, our approach even outperforms using precision observations, e.g., up to 8% increase in average vehicle velocity in the merge environment, despite only using local traffic information as opposed to global traffic information.},
  archive   = {C_ICRA},
  author    = {Michael Villarreal and Bibek Poudel and Jia Pan and Weizi Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610517},
  month     = {5},
  pages     = {4488-4494},
  title     = {Mixed traffic control and coordination from pixels},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RaTrack: Moving object detection and tracking with 4D radar
point cloud. <em>ICRA</em>, 4480–4487. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile autonomy relies on the precise perception of dynamic environments. Robustly tracking moving objects in 3D world thus plays a pivotal role for applications like trajectory prediction, obstacle avoidance, and path planning. While most current methods utilize LiDARs or cameras for Multiple Object Tracking (MOT), the capabilities of 4D imaging radars remain largely unexplored. Recognizing the challenges posed by radar noise and point sparsity in 4D radar data, we introduce RaTrack, an innovative solution tailored for radar-based tracking. Bypassing the typical reliance on specific object types and 3D bounding boxes, our method focuses on motion segmentation and clustering, enriched by a motion estimation module. Evaluated on the View-of-Delft dataset, RaTrack showcases superior tracking precision of moving objects, largely surpassing the performance of the state of the art. We release our code and model at https://github.com/LJacksonPan/RaTrack.},
  archive   = {C_ICRA},
  author    = {Zhijun Pan and Fangqiang Ding and Hantao Zhong and Chris Xiaoxuan Lu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610368},
  month     = {5},
  pages     = {4480-4487},
  title     = {RaTrack: Moving object detection and tracking with 4D radar point cloud},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Are you a robot? Detecting autonomous vehicles from behavior
analysis. <em>ICRA</em>, 4473–4479. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The tremendous hype around autonomous driving is eagerly calling for emerging and novel technologies to support advanced mobility use cases. As car manufactures keep developing SAE level 3+ systems to improve the safety and comfort of passengers, traffic authorities need to establish new procedures to manage the transition from human-driven to fully-autonomous vehicles while providing a feedback-loop mechanism to fine-tune envisioned autonomous systems. Thus, a way to automatically profile autonomous vehicles and differentiate those from human-driven ones is a must.In this paper, we present a fully-fledged framework that monitors active vehicles using camera images and state information in order to determine whether vehicles are autonomous, without requiring any active notification from the vehicles themselves. Essentially, it builds on the cooperation among vehicles, which share their data acquired on the road feeding a machine learning model to identify autonomous cars. We extensively tested our solution and created the NexusStreet dataset, by means of the CARLA simulator, employing an autonomous driving control agent and a steering wheel maneuvered by licensed drivers. Experiments show it is possible to discriminate the two behaviors by analyzing video clips with an accuracy of ~ 80%, which improves up to ~ 93% when the target’s state information is available. Lastly, we deliberately degraded the state to observe how the framework performs under non-ideal data collection conditions.},
  archive   = {C_ICRA},
  author    = {Fabio Maresca and Filippo Grazioli and Antonio Albanese and Vincenzo Sciancalepore and Gianpiero Negri and Xavier Costa-Perez},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610658},
  month     = {5},
  pages     = {4473-4479},
  title     = {Are you a robot? detecting autonomous vehicles from behavior analysis},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A neural-evolutionary algorithm for autonomous transit
network design. <em>ICRA</em>, 4457–4464. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Planning a public transit network is a challenging optimization problem, but essential in order to realize the benefits of autonomous buses. We propose a novel algorithm for planning networks of routes for autonomous buses. We first train a graph neural net model as a policy for constructing route networks, and then use the policy as one of several mutation operators in a evolutionary algorithm. We evaluate this algorithm on a standard set of benchmarks for transit network design, and find that it outperforms the learned policy alone by up to 20% and a plain evolutionary algorithm approach by up to 53% on realistic benchmark instances.},
  archive   = {C_ICRA},
  author    = {Andrew Holliday and Gregory Dudek},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611313},
  month     = {5},
  pages     = {4457-4464},
  title     = {A neural-evolutionary algorithm for autonomous transit network design},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SmartCooper: Vehicular collaborative perception with
adaptive fusion and judger mechanism. <em>ICRA</em>, 4450–4456. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, autonomous driving has garnered significant attention due to its potential for improving road safety through collaborative perception among connected and autonomous vehicles (CAVs). However, time-varying channel variations in vehicular transmission environments demand dynamic allocation of communication resources. Moreover, in the context of collaborative perception, it is important to recognize that not all CAVs contribute valuable data, and some CAV data even have detrimental effects on collaborative perception. In this paper, we introduce SmartCooper, an adaptive collaborative perception framework that incorporates communication optimization and a judger mechanism to facilitate CAV data fusion. Our approach begins with optimizing the connectivity of vehicles while considering communication constraints. We then train a learnable encoder to dynamically adjust the compression ratio based on the channel state information (CSI). Subsequently, we devise a judger mechanism to filter the detrimental image data reconstructed by adaptive decoders. We evaluate the effectiveness of our proposed algorithm on the OpenCOOD platform. Our results demonstrate a substantial reduction in communication costs by 23.10% compared to the non-judger scheme. Additionally, we achieve a significant improvement on the average precision of Intersection over Union (AP@IoU) by 7.15% compared with state-of-the-art schemes.},
  archive   = {C_ICRA},
  author    = {Yuang Zhang and Haonan An and Zhengru Fang and Guowen Xu and Yuan Zhou and Xianhao Chen and Yuguang Fang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610199},
  month     = {5},
  pages     = {4450-4456},
  title     = {SmartCooper: Vehicular collaborative perception with adaptive fusion and judger mechanism},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). STT: Stateful tracking with transformers for autonomous
driving. <em>ICRA</em>, 4442–4449. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tracking objects in three-dimensional space is critical for autonomous driving. To ensure safety while driving, the tracker must be able to reliably track objects across frames and accurately estimate their states such as velocity and acceleration in the present. Existing works frequently focus on the association task while either neglecting the model’s performance on state estimation or deploying complex heuristics to predict the states. In this paper, we propose STT, a Stateful Tracking model built with Transformers, that can consistently track objects in the scenes while also predicting their states accurately. STT consumes rich appearance, geometry, and motion signals through long term history of detections and is jointly optimized for both data association and state estimation tasks. Since the standard tracking metrics like MOTA and MOTP do not capture the combined performance of the two tasks in the wider spectrum of object states, we extend them with new metrics called S-MOTA and MOTP S that address this limitation. STT achieves competitive real-time performance on the Waymo Open Dataset.},
  archive   = {C_ICRA},
  author    = {Longlong Jing and Ruichi Yu and Xu Chen and Zhengli Zhao and Shiwei Sheng and Colin Graber and Qi Chen and Qinru Li and Shangxuan Wu and Han Deng and Sangjin Lee and Chris Sweeney and Qiurui He and Wei-Chih Hung and Tong He and Xingyi Zhou and Farshid Moussavi and James Guo and Yin Zhou and Mingxing Tan and Weilong Yang and Congcong Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610802},
  month     = {5},
  pages     = {4442-4449},
  title     = {STT: Stateful tracking with transformers for autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PCB-RandNet: Rethinking random sampling for LiDAR semantic
segmentation in autonomous driving scene. <em>ICRA</em>, 4435–4441. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fast and efficient semantic segmentation of large-scale LiDAR point clouds is a fundamental problem in autonomous driving. To achieve this goal, the existing point-based methods mainly choose to adopt Random Sampling strategy to process large-scale point clouds. However, our quantative and qualitative studies have found that Random Sampling may be less suitable for the autonomous driving scenario, since the LiDAR points follow an uneven or even long-tailed distribution across the space, which prevents the model from capturing sufficient information from points in different distance ranges and reduces the model’s learning capability. To alleviate this problem, we propose a new Polar Cylinder Balanced Random Sampling method that enables the downsampled point clouds to maintain a more balanced distribution and improve the segmentation performance under different spatial distributions. In addition, a sampling consistency loss is introduced to further improve the segmentation performance and reduce the model’s variance under different sampling methods. Extensive experiments confirm that our approach produces excellent performance on both SemanticKITTI and SemanticPOSS benchmarks, achieving a 2.8% and 4.0% improvement, respectively. The source code is available at PCB-RandNet.},
  archive   = {C_ICRA},
  author    = {Xian-Feng Han and Huixian Cheng and Hang Jiang and Dehong He and Guoqiang Xiao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610105},
  month     = {5},
  pages     = {4435-4441},
  title     = {PCB-RandNet: Rethinking random sampling for LiDAR semantic segmentation in autonomous driving scene},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A safety-adapted loss for pedestrian detection in autonomous
driving. <em>ICRA</em>, 4428–4434. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In safety-critical domains like autonomous driving (AD), errors by the object detector may endanger pedestrians and other vulnerable road users (VRU). As raw evaluation metrics are not an adequate safety indicator, recent works leverage domain knowledge to identify safety-relevant VRU, and to back-annotate the criticality of the interaction to the object detector. However, those approaches do not consider the safety factor in the deep neural network (DNN) training process. Thus, state-of-the-art DNN penalize all misdetections equally irrespective of their importance for the safe driving task. Hence, to mitigate the occurrence of safety-critical failure cases like false negatives, a safety-aware training strategy is needed to enhance the detection performance for critical pedestrians. In this paper, we propose a novel, safety-adapted loss variation that leverages the estimated per-pedestrian criticality during training. Therefore, we exploit the reachable set-based time-to-collision (TTC RSB ) metric from the motion domain along with distance information to account for the worst-case threat. Our evaluation results using RetinaNet and FCOS on the nuScenes dataset demonstrate that training the models with our safety-adapted loss function mitigates the misdetection of safety-critical pedestrians with robust performance for the general case, i.e., safety-irrelevant pedestrians.},
  archive   = {C_ICRA},
  author    = {Maria Lyssenko and Piyush Pimplikar and Maarten Bieshaar and Farzad Nozarian and Rudolph Triebel},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610038},
  month     = {5},
  pages     = {4428-4434},
  title     = {A safety-adapted loss for pedestrian detection in autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Longitudinal control volumes: A novel centralized estimation
and control framework for distributed multi-agent sorting systems.
<em>ICRA</em>, 4420–4427. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Centralized control of a multi-agent system improves upon distributed control especially when multiple agents share a common task e.g., sorting different materials in a recycling facility. Traditionally, each agent in a sorting facility is tuned individually which leads to suboptimal performance if one agent is less efficient than the others. Centralized control overcomes this bottleneck by leveraging global system state information, but it can be computationally expensive. In this work, we propose a novel framework called Longitudinal Control Volumes (LCV) to model the flow of material in a recycling facility. We then employ a Kalman Filter that incorporates local measurements of materials into a global estimation of the material flow in the system. We utilize a model predictive control algorithm that optimizes the rate of material flow using the global state estimate in real-time. We show that our proposed framework outperforms distributed control methods by 40-100% in simulation and physical experiments.},
  archive   = {C_ICRA},
  author    = {James Maier and Prasanna Sriganesh and Matthew Travers},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611225},
  month     = {5},
  pages     = {4420-4427},
  title     = {Longitudinal control volumes: A novel centralized estimation and control framework for distributed multi-agent sorting systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Combining coordination and independent coverage in
MultiRobot graph patrolling. <em>ICRA</em>, 4413–4419. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph patrolling algorithms provide effective strategies for coordinating mobile robots in the context of autonomously surveilling valuable assets. Optimizing patrolling strategies often aims to minimize the time between subsequent visits to a vertex, a measure known in the literature as idleness. In the domain of multi-robot patrolling, two approaches have received the most attention so far. The first involves coordinating all robots to follow a shared patrolling strategy covering the entire graph, while the second approach partitions the environment into disjoint areas that are then assigned to individual robots. Starting from these existing solutions, this paper introduces a new method that bridges these two complementary approaches. Our technique splits the vertices of the graph into a partition that includes a shared portion of the environment patrolled collectively by all robots, along with disjoint areas allocated exclusively to individual robots. This problem is formulated in terms of minimizing the maximum weighted idleness of the graph and is shown to be NP-hard. We then describe an exact solution for the problem and propose various heuristics to efficiently compute solutions for large problem instances. We evaluate and compare the proposed techniques in simulation and demonstrate that, in most cases, our methods produce better patrolling strategies when compared to classic solutions. Moreover, for small problem instances where the exact solution can be found, we show that our proposed heuristic has a competitive performance ratio.},
  archive   = {C_ICRA},
  author    = {Carlos Diaz Alvarenga and Nicola Basilico and Stefano Carpin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611463},
  month     = {5},
  pages     = {4413-4419},
  title     = {Combining coordination and independent coverage in MultiRobot graph patrolling},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Learning generalizable patrolling strategies through domain
randomization of attacker behaviors. <em>ICRA</em>, 4406–4412. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph-patrolling problems in the adversarial domain typically embed models and assumptions about how hostile events, from which an environment must be protected, are generated at a specific time and location. Relying upon such attacker models prevents algorithms from synthesizing strategies that can generalize in different settings, providing good performance under different and uncertain scenarios. In this paper, we propose a first method to deal with adversarial patrolling using a data driven approach. We cast the problem in an RL setting where the reward function is based on the ability to neutralize attacks that can follow an unknown strategy and that, hence, can be viewed as a black box component. We apply a policy gradient framework for optimizing action probabilities under such a reward model showing how effective patrolling strategies can be obtained from repeated attack-defense interactions between a patrolling agent and an attacker. Our results show that the data driven patroller can effectively provide protection against multiple, diverse attacker behaviors.},
  archive   = {C_ICRA},
  author    = {Carlos Diaz Alvarenga and Nicola Basilico and Stefano Carpin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610052},
  month     = {5},
  pages     = {4406-4412},
  title     = {Learning generalizable patrolling strategies through domain randomization of attacker behaviors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MM4MM: Map matching framework for multi-session mapping in
ambiguous and perceptually-degraded environments. <em>ICRA</em>,
4399–4405. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-session mapping serves as the pre-requisite for autonomous robots to fulfill various long-term tasks (e.g., map updating, navigation, collaboration). However, it is challenging to implement multi-session mapping in enclosed or partially enclosed ambiguous environments (e.g., long corridors, industrial warehouses). Existing solutions either depend heavily on the matching of elementary geometric features (e.g., points, lines, and planes), which tends to fail in environments with ambiguous geometric features; or depend on the given guess of the initial transformation matrix of multiple single-session maps, which is not always obtainable and accurate enough. The ambient magnetic field has exhibited ubiquity and high distinctiveness at different location, which makes it suitable for estimating the initial transformation matrix. Thus, this paper proposes a novel probabilistic magnetic-aware Map Matching framework for Multi-session Mapping, namely MM4MM, to estimate the relative transformation of multiple single-session maps and to build the globally consistent maps in ambiguous and perceptually-degraded environments. The key novelties of this work are the designing of the hierarchical probabilistic map matching framework and the Particle Swarm Optimization strategy to associate the magnetic data of multiple sessions. Evaluations on both simulated and real world experiments demonstrate the greatly improved utility, accuracy, and robustness of multi-session mapping over the comparative methods.},
  archive   = {C_ICRA},
  author    = {Zhenyu Wu and Wei Wang and Chunyang Zhao and Yufeng Yue and Jun Zhang and Hongming Shen and Danwei Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611566},
  month     = {5},
  pages     = {4399-4405},
  title     = {MM4MM: Map matching framework for multi-session mapping in ambiguous and perceptually-degraded environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ASAP: Automated sequence planning for complex robotic
assembly with physical feasibility. <em>ICRA</em>, 4380–4386. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The automated assembly of complex products requires a system that can automatically plan a physically feasible sequence of actions for assembling many parts together. In this paper, we present ASAP, a physics-based planning approach for automatically generating such a sequence for general-shaped assemblies. ASAP accounts for gravity to design a sequence where each sub-assembly is physically stable with a limited number of parts being held and a support surface. We apply efficient tree search algorithms to reduce the combinatorial complexity of determining such an assembly sequence. The search can be guided by either geometric heuristics or graph neural networks trained on data with simulation labels. Finally, we show the superior performance of ASAP at generating physically realistic assembly sequence plans on a large dataset of hundreds of complex product assemblies. We further demonstrate the applicability of ASAP on both simulation and real-world robotic setups. Project website: asap.csail.mit.edu},
  archive   = {C_ICRA},
  author    = {Yunsheng Tian and Karl D. D. Willis and Bassel Al Omari and Jieliang Luo and Pingchuan Ma and Yichen Li and Farhad Javid and Edward Gu and Joshua Jacob and Shinjiro Sueda and Hui Li and Sachin Chitta and Wojciech Matusik},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611595},
  month     = {5},
  pages     = {4380-4386},
  title     = {ASAP: Automated sequence planning for complex robotic assembly with physical feasibility},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bridging the sim-to-real gap with dynamic compliance tuning
for industrial insertion. <em>ICRA</em>, 4356–4363. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Contact-rich manipulation tasks often exhibit a large sim-to-real gap. For instance, industrial assembly tasks frequently involve tight insertions where the clearance is less than 0.1 mm and can even be negative when dealing with a deformable receptacle. This narrow clearance leads to complex contact dynamics that are difficult to model accurately in simulation, making it challenging to transfer simulation-learned policies to real-world robots. In this paper, we propose a novel framework for robustly learning manipulation skills for real-world tasks using simulated data only. Our framework consists of two main components: the &quot;Force Planner&quot; and the &quot;Gain Tuner&quot;. The Force Planner plans both the robot motion and desired contact force, while the Gain Tuner dynamically adjusts the compliance control gains to track the desired contact force during task execution. The key insight is that by dynamically adjusting the robot’s compliance control gains during task execution, we can modulate contact force in the new environment, thereby generating trajectories similar to those trained in simulation and narrowing the sim-to-real gap. Experimental results show that our method, trained in simulation on a generic square peg-and-hole task, can generalize to a variety of real-world insertion tasks involving narrow and negative clearances, all without requiring any fine-tuning. Videos are available at https://dynamic-compliance.github.io},
  archive   = {C_ICRA},
  author    = {Xiang Zhang and Masayoshi Tomizuka and Hui Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610707},
  month     = {5},
  pages     = {4356-4363},
  title     = {Bridging the sim-to-real gap with dynamic compliance tuning for industrial insertion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multifidelity sim-to-real pipeline for verifiable and
compositional reinforcement learning. <em>ICRA</em>, 4349–4355. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose and demonstrate a compositional framework for training and verifying reinforcement learning (RL) systems within a multifidelity sim-to-real pipeline, in order to deploy reliable and adaptable RL policies on physical hardware. By decomposing complex robotic tasks into component subtasks and defining mathematical interfaces between them, the framework allows for the independent training and testing of the corresponding subtask policies, while simultaneously providing guarantees on the overall behavior that results from their composition. By verifying the performance of these subtask policies using a multifidelity simulation pipeline, the framework not only allows for efficient RL training, but also for a refinement of the subtasks and their interfaces in response to challenges arising from discrepancies between simulation and reality. In an experimental case study, we apply the framework to train and deploy a compositional RL system that successfully pilots a Warthog unmanned ground robot.},
  archive   = {C_ICRA},
  author    = {Cyrus Neary and Christian Ellis and Aryaman Singh Samyal and Craig Lennon and Ufuk Topcu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610735},
  month     = {5},
  pages     = {4349-4355},
  title     = {A multifidelity sim-to-real pipeline for verifiable and compositional reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How to prompt your robot: A PromptBook for manipulation
skills with code as policies. <em>ICRA</em>, 4340–4348. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large Language Models (LLMs) have demonstrated the ability to perform semantic reasoning, planning and write code for robotics tasks. However, most methods rely on pre-existing primitives (i.e. pick, open drawer) or similar examples of robot code alone, which heavily limits their scalability to new scenarios. We present PromptBook, a collection of different prompting paradigms to generate code for successfully executing new manipulation skills. We demonstrate example-based, instruction-based and chain-of-thought to write robot code; as well as a method to build the prompt leveraging LLMs and human feedback. We show PromptBook enables LLMs to write code for new low-level manipulation skills in a zero-shot manner: from picking diverse objects, opening/closing drawers, to whisking, and waving hello. We evaluate the new skills on a mobile manipulator with 83% success rate at picking, 50-71% at opening drawers and 100% at closing them. Notably, the LLM is able to infer gripper orientation for grasping a drawer handle (z-axis aligned) vs. a top-down grasp (x-axis aligned).},
  archive   = {C_ICRA},
  author    = {Montserrat Gonzalez Arenas and Ted Xiao and Sumeet Singh and Vidhi Jain and Allen Ren and Quan Vuong and Jake Varley and Alexander Herzog and Isabel Leal and Sean Kirmani and Mario Prats and Dorsa Sadigh and Vikas Sindhwani and Kanishka Rao and Jacky Liang and Andy Zeng},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610784},
  month     = {5},
  pages     = {4340-4348},
  title     = {How to prompt your robot: A PromptBook for manipulation skills with code as policies},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Language-conditioned robotic manipulation with fast and slow
thinking. <em>ICRA</em>, 4333–4339. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The language-conditioned robotic manipulation aims to transfer natural language instructions into executable actions, from simple &quot;pick-and-place&quot; to tasks requiring intent recognition and visual reasoning. Inspired by the dual-process theory in cognitive science—which suggests two parallel systems of fast and slow thinking in human decision-making—we introduce Robotics with Fast and Slow Thinking (RFST), a framework that mimics human cognitive architecture to classify tasks and makes decisions on two systems based on instruction types. Our RFST consists of two key components: 1) an instruction discriminator to determine which system should be activated based on the current user’s instruction, and 2) a slow-thinking system that is comprised of a fine-tuned vision-language model aligned with the policy networks, which allow the robot to recognize user’s intention or perform reasoning tasks. To assess our methodology, we built a dataset featuring real-world trajectories, capturing actions ranging from spontaneous impulses to tasks requiring deliberate contemplation. Our results, both in simulation and real-world scenarios, confirm that our approach adeptly manages intricate tasks that demand intent recognition and reasoning.},
  archive   = {C_ICRA},
  author    = {Minjie Zhu and Yichen Zhu and Jinming Li and Junjie Wen and Zhiyuan Xu and Zhengping Che and Chaomin Shen and Yaxin Peng and Dong Liu and Feifei Feng and Jian Tang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611525},
  month     = {5},
  pages     = {4333-4339},
  title     = {Language-conditioned robotic manipulation with fast and slow thinking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning to play foosball: System and baselines.
<em>ICRA</em>, 4326–4332. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work stages Foosball as a versatile platform for advancing scientific research, particularly in the realm of robot learning. We present an automated Foosball table along with its corresponding simulated counterpart, showcasing a diverse range of challenges through example tasks within the Foosball environment. Initial findings are shared using a simple baseline approach. Foosball constitutes a versatile learning environment with the potential to yield cutting-edge research in various fields of artificial intelligence and machine learning, notably robust learning, while also extending its applicability to industrial robotics and automation setups. To transform our physical Foosball table into a research-friendly system, we augmented it with a 2 degrees of freedom kinematic chain to control the goalkeeper rod as an initial setup with the intention to be extended to the full game as soon as possible. Our experiments reveal that a realistic simulation is essential for mastering complex robotic tasks, yet translating these accomplishments to the real system remains challenging, often accompanied by a performance decline. This emphasizes the critical importance of research in this direction. In this concern, we spotlight the automated Foosball table as an invaluable tool, possessing numerous desirable attributes, to serve as a demanding learning environment for advancing robotics and automation research.},
  archive   = {C_ICRA},
  author    = {Janosch Moos and Cedric Derstroff and Niklas Schröder and Debora Clever},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611321},
  month     = {5},
  pages     = {4326-4332},
  title     = {Learning to play foosball: System and baselines},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Object-centric instruction augmentation for robotic
manipulation. <em>ICRA</em>, 4318–4325. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans interpret scenes by recognizing both the identities and positions of objects in their observations. For a robot to perform tasks such as &quot;pick and place&quot;, understanding both what the objects are and where they are located is crucial. While the former has been extensively discussed in the literature that uses the large language model to enrich the text descriptions, the latter remains underexplored. In this work, we introduce the Object-Centric Instruction Augmentation (OCI) framework to augment highly semantic and information-dense language instruction with position cues. We utilize a Multi-modal Large Language Model (MLLM) to weave knowledge of object locations into natural language instruction, thus aiding the policy network in mastering actions for versatile manipulation. Additionally, we present a feature reuse mechanism to integrate the vision-language features from off-the-shelf pre-trained MLLM into policy networks. Through a series of simulated and real-world robotic tasks, we demonstrate that robotic manipulator imitation policies trained with our enhanced instructions outperform those relying solely on traditional language instructions.},
  archive   = {C_ICRA},
  author    = {Junjie Wen and Yichen Zhu and Minjie Zhu and Jinming Li and Zhiyuan Xu and Zhengping Che and Chaomin Shen and Yaxin Peng and Dong Liu and Feifei Feng and Jian Tang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609992},
  month     = {5},
  pages     = {4318-4325},
  title     = {Object-centric instruction augmentation for robotic manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable multi-robot collaboration with large language
models: Centralized or decentralized systems? <em>ICRA</em>, 4311–4317.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A flurry of recent work has demonstrated that pre-trained large language models (LLMs) can be effective task planners for a variety of single-robot tasks. The planning performance of LLMs is significantly improved via prompting techniques, such as in-context learning or re-prompting with state feedback, placing new importance on the token budget for the context window. An under-explored but natural next direction is to investigate LLMs as multi-robot task planners. However, long-horizon, heterogeneous multi-robot planning introduces new challenges of coordination while also pushing up against the limits of context window length. It is therefore critical to find token-efficient LLM planning frameworks that are also able to reason about the complexities of multi-robot coordination. In this work, we compare the task success rate and token efficiency of four multi-agent communication frameworks (centralized, decentralized, and two hybrid) as applied to four coordination-dependent multi-agent 2D task scenarios for increasing numbers of agents. We find that a hybrid framework achieves better task success rates across all four tasks and scales better to more agents. We further demonstrate the hybrid frameworks in 3D simulations where the vision-to-text problem and dynamical errors are considered. See our project website 4 for prompts, videos, and code.},
  archive   = {C_ICRA},
  author    = {Yongchao Chen and Jacob Arkin and Yang Zhang and Nicholas Roy and Chuchu Fan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610676},
  month     = {5},
  pages     = {4311-4317},
  title     = {Scalable multi-robot collaboration with large language models: Centralized or decentralized systems?},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SG-bot: Object rearrangement via coarse-to-fine robotic
imagination on scene graphs. <em>ICRA</em>, 4303–4310. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Object rearrangement is pivotal in robotic-environment interactions, representing a significant capability in embodied AI. In this paper, we present SG-Bot, a novel rearrangement framework that utilizes a coarse-to-fine scheme with a scene graph as the scene representation. Unlike previous methods that rely on either known goal priors or zero-shot large models, SG-Bot exemplifies lightweight, real-time, and user-controllable characteristics, seamlessly blending the consideration of commonsense knowledge with automatic generation capabilities. SG-Bot employs a three-fold procedure– observation, imagination, and execution–to adeptly address the task. Initially, objects are discerned and extracted from a cluttered scene during the observation. These objects are first coarsely organized and depicted within a scene graph, guided by either commonsense or user-defined criteria. Then, this scene graph subsequently informs a generative model, which forms a fine-grained goal scene considering the shape information from the initial scene and object semantics. Finally, for execution, the initial and envisioned goal scenes are matched to formulate robotic action policies. Experimental results demonstrate that SG-Bot outperforms competitors by a large margin.},
  archive   = {C_ICRA},
  author    = {Guangyao Zhai and Xiaoni Cai and Dianye Huang and Yan Di and Fabian Manhardt and Federico Tombari and Nassir Navab and Benjamin Busam},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610792},
  month     = {5},
  pages     = {4303-4310},
  title     = {SG-bot: Object rearrangement via coarse-to-fine robotic imagination on scene graphs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SliceIt! - a dual simulator framework for learning robot
food slicing. <em>ICRA</em>, 4296–4302. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cooking robots can enhance the home experience by reducing the burden of daily chores. However, these robots must perform their tasks dexterously and safely in shared human environments, especially when handling dangerous tools such as kitchen knives. This study focuses on enabling a robot to autonomously and safely learn food-cutting tasks. More specifically, our goal is to enable a collaborative robot or industrial robot arm to perform food-slicing tasks by adapting to varying material properties using compliance control. Our approach involves using Reinforcement Learning (RL) to train a robot to compliantly manipulate a knife, by reducing the contact forces exerted by the food items and by the cutting board. However, training the robot in the real world can be inefficient, and dangerous, and result in a lot of food waste. Therefore, we proposed SliceIt!, a framework for safely and efficiently learning robot food-slicing tasks in simulation. Following a real2sim2real approach, our framework consists of collecting a few real food slicing data, calibrating our dual simulation environment (a high-fidelity cutting simulator and a robotic simulator), learning compliant control policies on the calibrated simulation environment, and finally, deploying the policies on the real robot.},
  archive   = {C_ICRA},
  author    = {Cristian C. Beltran-Hernandez and Nicolas Erbetti and Masashi Hamaya},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611596},
  month     = {5},
  pages     = {4296-4302},
  title     = {SliceIt! - a dual simulator framework for learning robot food slicing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). BEE-net: Bridging semantic and instance with gated encoding
and edge constraint for efficient panoptic segmentation. <em>ICRA</em>,
4281–4287. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Panoptic segmentation is a challenging perception task, which can help robots to comprehensively perceive the surrounding environment. In the task, we notice that semantic, instance, and panoptic have rich relations, however, which are rarely explored. In this work, we propose a novel panoptic, instance, and semantic bridged network to delve into the reciprocal relation. To make semantic and instance benefit from each other, we design a novel Gated Encoding (GE) module, incorporating complementary cues between semantic and instance heads through the gated mechanism. In addition, a novel edge-aware consistency constraint among edges of each task is presented, which exhaustedly exploits geometric constraints, to boost the segmentation quality of challenging edges. Experimental results on the Cityscapes and MS-COCO datasets demonstrate that our approach achieves state-of-the-art performance in an efficient CNN-based paradigm, attaining a balance between accuracy and efficiency.},
  archive   = {C_ICRA},
  author    = {Xinyang Huang and Guanghui Zhang and Dongchen Zhu and Yunpeng Sun and Wenjun Shi and Gang Ye and Yang Xiao and Lei Wang and Xiaolin Zhang and Bo Li and Jiamao Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610497},
  month     = {5},
  pages     = {4281-4287},
  title     = {BEE-net: Bridging semantic and instance with gated encoding and edge constraint for efficient panoptic segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-cluster shifting for efficient and effective 3D object
detection in autonomous driving. <em>ICRA</em>, 4273–4280. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a new 3D point-based detector model, named Shift-SSD, for precise 3D object detection in autonomous driving. Traditional point-based 3D object detectors often employ architectures that rely on a progressive downsampling of points. While this method effectively reduces computational demands and increases receptive fields, it will compromise the preservation of crucial non-local information for accurate 3D object detection, especially in the complex driving scenarios. To address this, we introduce an intriguing Cross-Cluster Shifting operation to unleash the representation capacity of the point-based detector by efficiently modeling longer-range inter-dependency while including only a negligible overhead. Concretely, the Cross-Cluster Shifting operation enhances the conventional design by shifting partial channels from neighboring clusters, which enables richer interaction with non-local regions and thus enlarges the receptive field of clusters. We conduct extensive experiments on the KITTI, Waymo, and nuScenes datasets, and the results demonstrate the state-of-the-art performance of Shift-SSD in both detection accuracy and runtime efficiency.},
  archive   = {C_ICRA},
  author    = {Zhili Chen and Kien T. Pham and Maosheng Ye and Zhiqiang Shen and Qifeng Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611345},
  month     = {5},
  pages     = {4273-4280},
  title     = {Cross-cluster shifting for efficient and effective 3D object detection in autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient semantic segmentation for compressed video.
<em>ICRA</em>, 4266–4272. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots, constrained by limited onboard computing resources, often encounter situations wherein high-resolution and high-bit-rate videos captured by their cameras necessitate compression before further analysis. In this paper, we propose a novel video semantic segmentation paradigm for compressed video. Specifically, our framework draws the inspiration from the principle of Wavelet Transform, and thus we design the network structure, WTDecomNet, approximating the decomposition of high-resolution image into its low-resolution counterpart and axial details. The aim is to well preserve the image content through decomposition and maintain model efficiency by obtaining semantics from low-resolution image. To facilitate this purpose, we propose an efficient axial subband approximation module for extracting axial details and a lightweight temporal alignment module for associating keyframes and non-keyframes of compressed video. Through comprehensive experiments, we show that our model can achieve the state-of-the-art performance on public benchmarks. Especially on CamVid, comparing to baseline, our proposed model reduces the computational overhead by ∼70% while improving mIoU by ∼4%.},
  archive   = {C_ICRA},
  author    = {Jiaxin Cai and Qi Li and Yulin Shen and Jia Pan and Wenxi Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610435},
  month     = {5},
  pages     = {4266-4272},
  title     = {Efficient semantic segmentation for compressed video},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fine-grained pillar feature encoding via spatio-temporal
virtual grid for 3D object detection. <em>ICRA</em>, 4259–4265. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Developing high-performance, real-time architectures for LiDAR-based 3D object detectors is essential for the successful commercialization of autonomous vehicles. Pillar-based methods stand out as a practical choice for onboard deployment due to their computational efficiency. However, despite their efficiency, these methods can sometimes underperform compared to alternative point encoding techniques such as Voxel-encoding or PointNet++. We argue that current pillar-based methods have not sufficiently captured the fine-grained distributions of LiDAR points within each pillar structure. Consequently, there exists considerable room for improvement in pillar feature encoding. In this paper, we introduce a novel pillar encoding architecture referred to as Fine-Grained Pillar Feature Encoding (FG-PFE). FG-PFE utilizes Spatio-Temporal Virtual (STV) grids to capture the distribution of point clouds within each pillar across vertical, temporal, and horizontal dimensions. Through STV grids, points within each pillar are individually encoded using Vertical PFE (V-PFE), Temporal PFE (T-PFE), and Horizontal PFE (H-PFE). These encoded features are then aggregated through an Attentive Pillar Aggregation method. Our experiments conducted on the nuScenes dataset demonstrate that FG-PFE achieves significant performance improvements over baseline models such as PointPillar, CenterPoint-Pillar, and PillarNet, with only a minor increase in computational overhead.},
  archive   = {C_ICRA},
  author    = {Konyul Park and Yecheol Kim and Junho Koh and Byungwoo Park and Jun Won Choi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611414},
  month     = {5},
  pages     = {4259-4265},
  title     = {Fine-grained pillar feature encoding via spatio-temporal virtual grid for 3D object detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Frame fusion with vehicle motion prediction for 3D object
detection. <em>ICRA</em>, 4252–4258. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In LiDAR-based 3D detection, history point clouds contain rich temporal information helpful for future prediction. In the same way, history detections should contribute to future detections. In this paper, we propose a detection enhancement method, namely FrameFusion, which improves 3D object detection results by fusing history detection frames. In FrameFusion, we &quot;forward&quot; history frames to the current frame and apply weighted Non-Maximum-Suppression on dense bounding boxes to obtain a fused frame with merged boxes. To &quot;forward&quot; frames, we use vehicle motion models to estimate the future pose of the bounding boxes. Our method is flexible in motion model selection. We explore three motion models in our work and show how the unicycle model and the bicycle model improve turning cases. On Waymo Open Dataset, our FrameFusion method consistently improves the performance of various 3D detectors by about 2.0 vehicle LEVEL 2 APH with negligible latency and slightly enhances the performance of the temporal fusion method MPPNet. We also conduct extensive experiments on motion model selection.},
  archive   = {C_ICRA},
  author    = {Xirui Li and Feng Wang and Naiyan Wang and Chao Ma},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610204},
  month     = {5},
  pages     = {4252-4258},
  title     = {Frame fusion with vehicle motion prediction for 3D object detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical point attention for indoor 3D object detection.
<em>ICRA</em>, 4245–4251. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D object detection is an essential vision technique for various robotic systems, such as augmented reality and domestic robots. Transformers as versatile network architectures have recently seen great success in 3D point cloud object detection. However, the lack of hierarchy in a plain transformer restrains its ability to learn features at different scales. Such limitation makes transformer detectors perform worse on smaller objects and affects their reliability in indoor environments where small objects are the majority. This work proposes two novel attention operations as generic hierarchical designs for point-based transformer detectors. First, we propose Aggregated Multi-Scale Attention (MS-A) that builds multi-scale tokens from a single-scale input feature to enable more fine-grained feature learning. Second, we propose Size-Adaptive Local Attention (Local-A) with adaptive attention regions for localized feature aggregation within bounding box proposals. Both attention operations are model-agnostic network modules that can be plugged into existing point cloud transformers for end-to-end training. We evaluate our method on two widely used indoor detection benchmarks. By plugging our proposed modules into the state-of-the-art transformer-based 3D detectors, we improve the previous best results on both benchmarks, with more significant improvements on smaller objects.},
  archive   = {C_ICRA},
  author    = {Manli Shu and Le Xue and Ning Yu and Roberto Martín-Martín and Caiming Xiong and Tom Goldstein and Juan Carlos Niebles and Ran Xu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610108},
  month     = {5},
  pages     = {4245-4251},
  title     = {Hierarchical point attention for indoor 3D object detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PVTransformer: Point-to-voxel transformer for scalable 3D
object detection. <em>ICRA</em>, 4238–4244. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D object detectors for point clouds often rely on a pooling-based PointNet [20] to encode sparse points into grid-like voxels or pillars. In this paper, we identify that the common PointNet design introduces an information bottleneck that limits 3D object detection accuracy and scalability. To address this limitation, we propose PVTransformer: a transformer-based point-to-voxel architecture for 3D detection. Our key idea is to replace the PointNet pooling operation with an attention module, leading to a better point-to-voxel aggregation function. Our design respects the permutation invariance of sparse 3D points while being more expressive than the pooling-based PointNet. Experimental results show our PVTransformer achieves much better performance compared to the latest 3D object detectors. On the widely used Waymo Open Dataset, our PVTransformer achieves state-of-the-art 76.5 mAPH L2, outperforming the prior art of SWFormer [27] by +1.7 mAPH L2.},
  archive   = {C_ICRA},
  author    = {Zhaoqi Leng and Pei Sun and Tong He and Dragomir Anguelov and Mingxing Tan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610346},
  month     = {5},
  pages     = {4238-4244},
  title     = {PVTransformer: Point-to-voxel transformer for scalable 3D object detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Road obstacle detection based on unknown objectness scores.
<em>ICRA</em>, 4231–4237. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The detection of unknown traffic obstacles is vital to ensure safe autonomous driving. The standard object-detection methods cannot identify unknown objects that are not included under predefined categories. This is because object-detection methods are trained to assign a background label to pixels corresponding to the presence of unknown objects. To address this problem, the pixel-wise anomaly-detection approach has attracted increased research attention. Anomaly-detection techniques, such as uncertainty estimation and perceptual difference from reconstructed images, make it possible to identify pixels of unknown objects as out-of-distribution (OoD) samples. However, when applied to images with many unknowns and complex components, such as driving scenes, these methods often exhibit unstable performance. The purpose of this study is to achieve stable performance for detecting unknown objects by incorporating the object-detection fashions into the pixel-wise anomaly detection methods. To achieve this goal, we adopt a semantic-segmentation network with a sigmoid head that simultaneously provides pixel-wise anomaly scores and objectness scores. Our experimental results show that the objectness scores play an important role in improving the detection performance. Based on these results, we propose a novel anomaly score by integrating these two scores, which we term as unknown objectness score. Quantitative evaluations show that the proposed method outperforms state-of-the-art methods when applied to the publicly available datasets.},
  archive   = {C_ICRA},
  author    = {Chihiro Noguchi and Toshiaki Ohgushi and Masao Yamanaka},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610249},
  month     = {5},
  pages     = {4231-4237},
  title     = {Road obstacle detection based on unknown objectness scores},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HASHI: Highly adaptable seafood handling instrument for
manipulation in industrial settings. <em>ICRA</em>, 4191–4197. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The seafood processing industry provides fertile ground for robotics to impact the future-of-work from multiple perspectives including productivity, worker safety, and quality of work life. The robotics research challenge in this domain is the realization of flexible and reliable manipulation of soft, deformable, slippery, spiky and scaly objects. In this paper, we propose a novel robot end effector, called HASHI, that employs chopstick-like appendages for precise and dexterous manipulation. This gripper is capable of in-hand manipulation by rotating its two constituent sticks relative to each other and offers control of objects in all three axes of rotation by imitating human use of chopsticks. HASHI delicately positions and orients food through embedded 6-axis force-torque sensors. We derive and validate the kinematic model for HASHI, as well as demonstrate grip force and torque readings from the sensorization of each chopstick. We also evaluate the versatility of HASHI through grasping trials of a variety of real and simulated food items with varying geometry, weight, and firmness.},
  archive   = {C_ICRA},
  author    = {Austin Allison and Nathaniel Hanson and Sebastian Wicke and Taşkın Padır},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611022},
  month     = {5},
  pages     = {4191-4197},
  title     = {HASHI: Highly adaptable seafood handling instrument for manipulation in industrial settings},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Squirrel-inspired tendon-driven passive gripper for agile
landing. <em>ICRA</em>, 4184–4190. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Squirrels exhibit agile leaping between tree branches, often using non-prehensile gripping with compliant and passively adaptive fingers. We aim to test the utility of such gripping in agile robotic maneuvering. In the present study, we first examine the parametric design of a squirrel-inspired underactuated gripper for passive landing on impact. We fix the geometry of the gripper and vary the joint stiffness and contact conditions. We find that stiffer fingers with soft foam pads enlarge the landing sufficiency region. Specifically, friction appears to enlarge horizontal error tolerance, while joint stiffness and pad damping allow for higher impact speeds. Thus, these features should be considered in the design of future agile robot hands and feet that include high impact landings on rods with pose inaccuracy.},
  archive   = {C_ICRA},
  author    = {Stanley J. Wang and Duyi Kuang and Sebastian D. Lee and Robert J. Full and Hannah S. Stuart},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610730},
  month     = {5},
  pages     = {4184-4190},
  title     = {Squirrel-inspired tendon-driven passive gripper for agile landing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Co-designing manipulation systems using task-relevant
constraints. <em>ICRA</em>, 4177–4183. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A robotic system’s hardware and control policy must be co-optimized to ensure they complement each other to interact robustly with the environment. However, this combined search is extremely high-dimensional and intractable without a suitable underlying representation. This paper uses environmental constraints to structure the co-design space for manipulation. We show that task-relevant constraints encode regions of the search space containing reasonable co-design solutions. Furthermore, this underlying representation renders a co-design space amenable to gradient-based optimization. For efficient search, we present the co-design Jacobian that describes how the robot’s motion varies with control as well as hardware design changes. This Jacobian exploits the structure induced by environmental constraints for iterative design updates in the co-design space. Using these two conceptual tools, we co-design manipulators, grippers, and multi-fingered hands, showing that environmental constraints are an effective representation for co-designing diverse manipulation systems. Our methodology also scales well with increased co-design parameters, rendering the co-design of complex, high-dimensional manipulation systems feasible.},
  archive   = {C_ICRA},
  author    = {Apoorv Vaish and Oliver Brock},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611686},
  month     = {5},
  pages     = {4177-4183},
  title     = {Co-designing manipulation systems using task-relevant constraints},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The double-scoop gripper: A tendon-driven soft-rigid
end-effector for food handling exploiting constraints in narrow spaces.
<em>ICRA</em>, 4170–4176. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Food handling is a challenging task for robotic grippers, as it requires to manipulate highly deformable and fragile items, that can be easily damaged. Moreover, ingredients for the preparation of the different dishes are usually stored in small containers that are often not easily accessible. This paper introduces an innovative soft-rigid, tendon-driven gripper: the Double-Scoop Gripper (DSG). Its two-fingered design exploits a specialized structure to cope with constrained spaces (e.g., containers in narrow shelves). The DSG can delicately grasp objects of various shapes by employing two scoop-shaped fingertips that can form a single plate when fingers are flexed. Data obtained from an on-board camera are used to detect the food item features and plan the grasping strategy that better exploits the possible environmental constraints regulating the opening of the two fingers and the approaching direction of the gripper. DSG capabilities are verified with experiments conducted using real food ingredients within a pick-and-place setup to evaluate both the grasping and the releasing capability of the gripper. Obtained results are promising and suggest that this approach could be particularly advantageous in the context of automated food serving.},
  archive   = {C_ICRA},
  author    = {Leonardo Franco and Enrico Turco and Valerio Bo and Maria Pozzi and Monica Malvezzi and Domenico Prattichizzo and Gionata Salvietti},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611693},
  month     = {5},
  pages     = {4170-4176},
  title     = {The double-scoop gripper: A tendon-driven soft-rigid end-effector for food handling exploiting constraints in narrow spaces},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The fractal hand–i: A non-anthropomorphic, but synergistic,
adaptable gripper. <em>ICRA</em>, 4162–4169. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a novel Fractal Hand robotic gripper. The hand has only 1 actuator, but (2 n+1 −1) joints, where a design parameter n defines the depth of the fingers’ tree structures. The hand is synergistic in its operation (because its joint movements are coupled through the hand’s interaction with the grasped object), but it is not anthropomorphic. The basic finger and hand geometry, governing kinematics, and quasi-statics mechanics of a rigid version of the hand are developed. These analyses remarkably show that under mild constraints, the grasped object is compliantly stable at an equilibrium grasp configuration. Thus, the Fractal Hand adapts to a very wide range of planar objects with a single design. Grasp planning is thus simplified. A companion paper [33] introduces a design methodology for this new class of robot hands, and multiple prototypes.},
  archive   = {C_ICRA},
  author    = {Joel W. Burdick and Malcolm Tisdale},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610687},
  month     = {5},
  pages     = {4162-4169},
  title     = {The fractal Hand–I: A non-anthropomorphic, but synergistic, adaptable gripper},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model-based runtime monitoring with interactive imitation
learning. <em>ICRA</em>, 4154–4161. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot learning methods have recently made great strides, but generalization and robustness challenges still hinder their widespread deployment. Failing to detect and address potential failures renders state-of-the-art learning systems not combat-ready for high-stakes tasks. Recent advances in interactive imitation learning have presented a promising framework for human-robot teaming, enabling the robots to operate safely and continually improve their performances over long-term deployments. Nonetheless, existing methods typically require constant human supervision and preemptive feedback, limiting their practicality in realistic domains. This work aims to endow a robot with the ability to monitor and detect errors during task execution. We introduce a model-based runtime monitoring algorithm that learns from deployment data to detect system anomalies and anticipate failures. Unlike prior work that cannot foresee future failures or requires failure experiences for training, our method learns a latent-space dynamics model and a failure classifier, enabling our method to simulate future action outcomes and detect out-of-distribution and high-risk states preemptively. We train our method within an interactive imitation learning framework, where it continually updates the model from the experiences of the human-robot team collected using trustworthy deployments. Consequently, our method reduces the human workload needed over time while ensuring reliable task execution. Our method outperforms the baselines across system-level and unit-test metrics, with 23% and 40% higher success rates in simulation and on physical hardware, respectively. More information at https://ut-austin-rpl.github.io/sirius-runtime-monitor/},
  archive   = {C_ICRA},
  author    = {Huihan Liu and Shivin Dass and Roberto Martín-Martín and Yuke Zhu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611038},
  month     = {5},
  pages     = {4154-4161},
  title     = {Model-based runtime monitoring with interactive imitation learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The grasp reset mechanism: An automated apparatus for
conducting grasping trials. <em>ICRA</em>, 4147–4153. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Advancing robotic grasping and manipulation requires the ability to test algorithms and/or train learning models on large numbers of grasps. Towards the goal of more advanced grasping, we present the Grasp Reset Mechanism (GRM), a fully automated apparatus for conducting large-scale grasping trials. The GRM automates the process of resetting a grasping environment, repeatably placing an object in a fixed location and controllable 1-D orientation. It also collects data and swaps between multiple objects enabling robust dataset collection with no human intervention. We also present a standardized state machine interface for control, which allows for integration of most manipulators with minimal effort. In addition to the physical design and corresponding software, we include a dataset of 1,020 grasps. The grasps were created with a Kinova Gen3 robot arm and Robotiq 2F-85 Adaptive Gripper to enable training of learning models and to demonstrate the capabilities of the GRM. The dataset includes ranges of grasps conducted across four objects and a variety of orientations. Manipulator states, object pose, video, and grasp success data are provided for every trial.},
  archive   = {C_ICRA},
  author    = {Kyle DuFrene and Keegan Nave and Joshua Campbell and Ravi Balasubramanian and Cindy Grimm},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610892},
  month     = {5},
  pages     = {4147-4153},
  title     = {The grasp reset mechanism: An automated apparatus for conducting grasping trials},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ICGNet: A unified approach for instance-centric grasping.
<em>ICRA</em>, 4140–4146. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate grasping is the key to several robotic tasks including assembly and household robotics. Executing a successful grasp in a cluttered environment requires multiple levels of scene understanding: First, the robot needs to analyze the geometric properties of individual objects to find feasible grasps. These grasps need to be compliant with the local object geometry. Second, for each proposed grasp, the robot needs to reason about the interactions with other objects in the scene. Finally, the robot must compute a collision-free grasp trajectory while taking into account the geometry of the target object. Most grasp detection algorithms directly predict grasp poses in a monolithic fashion, which does not capture the composability of the environment. In this paper, we introduce an end-to-end architecture for object-centric grasping. The method uses pointcloud data from a single arbitrary viewing direction as an input and generates an instance-centric representation for each partially observed object in the scene. This representation is further used for object reconstruction and grasp detection in cluttered table-top scenes. We show the effectiveness of the proposed method by extensively evaluating it against state-of-the-art methods on synthetic datasets, indicating superior performance for grasping and reconstruction. Additionally, we demonstrate real-world applicability by decluttering scenes with varying numbers of objects. Videos and Code icgraspnet.github.io.},
  archive   = {C_ICRA},
  author    = {René Zurbrügg and Yifan Liu and Francis Engelmann and Suryansh Kumar and Marco Hutter and Vaishakh Patil and Fisher Yu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611725},
  month     = {5},
  pages     = {4140-4146},
  title     = {ICGNet: A unified approach for instance-centric grasping},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The fractal hand-II: Reviving a classic mechanism for
contemporary grasping challenges. <em>ICRA</em>, 4133–4139. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper and its companion propose a new fractal robotic gripper, drawing inspiration from the centuryold Fractal Vise. The unusual synergistic properties allow it to passively conform to diverse objects using only one actuator. Designed to be easily integrated with prevailing parallel jaw grippers, it alleviates the complexities tied to perception and grasp planning, especially when dealing with unpredictable object poses and geometries. We build on the foundational principles of the Fractal Vise to a broader class of gripping mechanisms and address the limitations that had led to its obscurity. Two Fractal Fingers, coupled with a closing actuator, can form an adaptive and synergistic Fractal Hand. We articulate a design methodology for low-cost, easy-to-fabricate, large workspace, and compliant Fractal Fingers. The companion paper delves into the kinematics and grasping properties of a specific class of Fractal Fingers and Hands.},
  archive   = {C_ICRA},
  author    = {Malcolm Tisdale and Joel W. Burdick},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611267},
  month     = {5},
  pages     = {4133-4139},
  title     = {The fractal hand-II: Reviving a classic mechanism for contemporary grasping challenges},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VFAS-grasp: Closed loop grasping with visual feedback and
adaptive sampling. <em>ICRA</em>, 4126–4132. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problem of closed-loop robotic grasping and present a novel planner which uses Visual Feedback and an uncertainty-aware Adaptive Sampling strategy (VFAS) to close the loop. At each iteration, our method VFAS-Grasp builds a set of candidate grasps by generating random perturbations of a seed grasp. The candidates are then scored using a novel metric which combines a learned grasp-quality estimator, the uncertainty in the estimate and the distance from the seed proposal to promote temporal consistency. Additionally, we present two mechanisms to improve the efficiency of our sampling strategy: We dynamically scale the sampling region size and number of samples in it based on past grasp scores. We also leverage a motion vector field estimator to shift the center of our sampling region. We demonstrate that our algorithm can run in real time (20 Hz) and is capable of improving grasp performance for static scenes by refining the initial grasp proposal. We also show that it can enable grasping of slow moving objects, such as those encountered during human to robot handover. Video: https://youtu.be/8DRe2OFlf7o},
  archive   = {C_ICRA},
  author    = {Pedro Piacenza and Jiacheng Yuan and Jinwook Huh and Volkan Isler},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611183},
  month     = {5},
  pages     = {4126-4132},
  title     = {VFAS-grasp: Closed loop grasping with visual feedback and adaptive sampling},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kinematic synergy primitives for human-like grasp motion
generation. <em>ICRA</em>, 4119–4125. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Grasping with five-fingered humanoid hands is a complex control problem. Throughout the entire grasping motion, all finger joints need to be coordinated to achieve a stable grasp. Grasp synergies provide a simplified, low-dimensional representation of grasp postures and motions, that can be used for the description of human grasps as well as the generation of novel, human-like grasps. However, the abstract synergy representation complicates the association of relevant high-level grasp parameters, as for example the grasp type and final posture or the grasp speed. Therefore, it is difficult to control these grasp characteristics in the synergy space. This paper presents an adaptable representation for kinematic grasping motions in synergy space, that allows the generation of novel, human-like grasps under direct control of high-level grasp parameters. It is based on via-point movement primitives trained on synergy trajectories of human grasping motions. The representation using synergy primitives allows for a straightforward adaptation of grasp characteristics while preserving the essential grasping motion learned from human demonstration. The kinematic synergy primitives have a low reproduction error of 3.9% of the maximum finger joint angle and are able to generate successful grasps on a simulated human hand and a real prosthetic hand.},
  archive   = {C_ICRA},
  author    = {Julia Starke and Tamim Asfour},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611490},
  month     = {5},
  pages     = {4119-4125},
  title     = {Kinematic synergy primitives for human-like grasp motion generation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Domain randomization for sim2real transfer of automatically
generated grasping datasets. <em>ICRA</em>, 4112–4118. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic grasping refers to making a robotic system pick an object by applying forces and torques on its surface. Many recent studies use data-driven approaches to address grasping, but the sparse reward nature of this task made the learning process challenging to bootstrap. To avoid constraining the operational space, an increasing number of works propose grasping datasets to learn from. But most of them are limited to simulations. The present paper investigates how automatically generated grasps can be exploited in the real world. More than 7000 reach-and-grasp trajectories have been generated with Quality-Diversity (QD) methods on 3 different arms and grippers, including parallel fingers and a dexterous hand, and tested in the real world. Conducted analysis on the collected measure shows correlations between several Domain Randomization-based quality criteria and sim-to-real transferability. Key challenges regarding the reality gap for grasping have been identified, stressing matters on which researchers on grasping should focus in the future. A QD approach has finally been proposed for making grasps more robust to domain randomization, resulting in a transfer ratio of 84% on the Franka Research 3 arm.},
  archive   = {C_ICRA},
  author    = {Johann Huber and François Hélénon and Hippolyte Watrelot and Faïz Ben Amar and Stéphane Doncieux},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610677},
  month     = {5},
  pages     = {4112-4118},
  title     = {Domain randomization for sim2real transfer of automatically generated grasping datasets},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Acoustic soft tactile skin (AST skin). <em>ICRA</em>,
4105–4111. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel acoustic soft tactile (AST) skin technology operating with sound waves. In this innovative approach, the sound waves generated by a speaker travel in channels embedded in a soft membrane and get modulated due to a deformation of the channel when pressed by an external force and received by a microphone at the end of the channel. The sensor leverages regression and classification methods for estimating the normal force and its contact location. Our sensor can be affixed to any robot part, e.g., end effectors or arm. We tested several regression and classifier methods to learn the relation between sound wave modulation, the applied force, and its location, respectively and picked the best-performing models for force and location predictions. The best skin configurations yield more than 93% of the force estimation within ±1.5 N tolerances for a range of 0-30 +1 N and contact locations with over 96% accuracy. We also demonstrated the performance of AST Skin technology for a real-time gripping force control application.},
  archive   = {C_ICRA},
  author    = {Vishnu Rajendran S and Willow Mandil and Kiyanoush Nazari and Simon Parsons and Amir Ghalamzan E},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610768},
  month     = {5},
  pages     = {4105-4111},
  title     = {Acoustic soft tactile skin (AST skin)},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Grasp manipulation relationship detection based on graph
sample and aggregation. <em>ICRA</em>, 4098–4104. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In multi-object stacking scenarios, exploring the relationships among objects and determining the correct sequence of operations are crucial for robotic manipulation. However, previous algorithms inefficiently combine global and local information, often focusing solely on the local features of objects or the interactions of object features at a global level. This approach leads to imbalanced distribution of features and the generation of redundant or missing relationships in complex scenes, such as multi-object stacking and partial occlusion. To address this issue, we have developed a grasp manipulation relationship detection algorithm called Graph Sampling Aggregation Network for Visual Manipulation Relationship Detection (GSAGED). This algorithm assists robots in detecting targets in complex scenes and determining the appropriate grasping order. Firstly, the Positional Encoding Module in GSAGED enhances object feature information by considering global contexts. Secondly, the Graph Sampling Aggregation method effectively integrates global and local information, relieving imbalanced distribution of features. Finally, we applied the developed algorithm to a physical robot for grasping. Experimental results on the Visual Manipulation Relationship Dataset (VMRD) and the large-scale relational grasp dataset named REGRAD demonstrate that our method significantly improves the accuracy of relationship detection in complex scenes and exhibits robust generalization capabilities in real-world applications.},
  archive   = {C_ICRA},
  author    = {Jiayuan Luo and Yaxin Liu and Han Wang and Mengyuan Ding and Xuguang Lan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611627},
  month     = {5},
  pages     = {4098-4104},
  title     = {Grasp manipulation relationship detection based on graph sample and aggregation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RoboHop: Segment-based topological map representation for
open-world visual navigation. <em>ICRA</em>, 4090–4097. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mapping is crucial for spatial reasoning, planning and robot navigation. Existing approaches range from metric, which require precise geometry-based optimization, to purely topological, where image-as-node based graphs lack explicit object-level reasoning and interconnectivity. In this paper, we propose a novel topological representation of an environment based on , which are semantically meaningful and open-vocabulary queryable, conferring several advantages over previous works based on pixel-level features. Unlike 3D scene graphs, we create a purely topological graph with segments as nodes, where edges are formed by a) associating segment-level descriptors between pairs of consecutive images and b) connecting neighboring segments within an image using their pixel centroids. This unveils a continuous sense of a place, defined by inter-image persistence of segments along with their intra-image neighbours. It further enables us to represent and update segment-level descriptors through neighborhood aggregation using graph convolution layers, which improves robot localization based on segment-level retrieval. Using real-world data, we show how our proposed map representation can be used to i) generate navigation plans in the form of hops over segments and ii) search for target objects using natural language queries describing spatial relations of objects. Furthermore, we quantitatively analyze data association at the segment level, which underpins inter-image connectivity during mapping and segment-level localization when revisiting the same place. Finally, we show preliminary trials on segment-level ‘hopping’ based zero-shot real-world navigation. Project page with supplementary details: oravus.github.io/RoboHop/.},
  archive   = {C_ICRA},
  author    = {Sourav Garg and Krishan Rana and Mehdi Hosseinzadeh and Lachlan Mares and Niko Sünderhauf and Feras Dayoub and Ian Reid},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610234},
  month     = {5},
  pages     = {4090-4097},
  title     = {RoboHop: Segment-based topological map representation for open-world visual navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uncertainty-aware 3D object-level mapping with deep shape
priors. <em>ICRA</em>, 4082–4089. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D object-level mapping is a fundamental problem in robotics, which is especially challenging when object CAD models are unavailable during inference. We propose a framework that can reconstruct high-quality object-level maps for unknown objects. Our approach takes multiple RGB-D images as input and outputs dense 3D shapes and 9-DoF poses (including 3 scale parameters) for detected objects. The core idea is to leverage a learnt generative model for a category of object shapes as priors and to formulate a probabilistic, uncertainty-aware optimization framework for 3D reconstruction. We derive a probabilistic formulation that propagates shape and pose uncertainty through two novel loss functions. Unlike current state-of-the-art approaches, we explicitly model the uncertainty of the object shapes and poses during our optimization, resulting in a high-quality object-level mapping system. Moreover, the estimated shape and pose uncertainties, which we demonstrate can accurately reflect the true errors of our object maps, can be useful for downstream robotics tasks such as active vision. We perform extensive evaluations on indoor and outdoor real-world datasets, achieving substantial improvements over state-of-the-art methods. Our code is available at https://github.com/TRAILab/UncertainShapePose.},
  archive   = {C_ICRA},
  author    = {Ziwei Liao and Jun Yang and Jingxing Qian and Angela P. Schoellig and Steven L. Waslander},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611206},
  month     = {5},
  pages     = {4082-4089},
  title     = {Uncertainty-aware 3D object-level mapping with deep shape priors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ERASOR++: Height coding plus egocentric ratio based dynamic
object removal for static point cloud mapping. <em>ICRA</em>, 4067–4073.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mapping plays a crucial role in location and navigation within automatic systems. However, the presence of dynamic objects in 3D point cloud maps generated from scan sensors can introduce map distortion and long traces, thereby posing challenges for accurate mapping and navigation. To address this issue, we propose ERASOR++, an enhanced approach based on the Egocentric Ratio of Pseudo Occupancy for effective dynamic object removal. To begin, we introduce the Height Coding Descriptor, which combines height difference and height layer information to encode the point cloud. Subsequently, we propose the Height Stack Test, Ground Layer Test, and Surrounding Point Test methods to precisely and efficiently identify the dynamic bins within point cloud bins, thus overcoming the limitations of prior approaches. Through extensive evaluation on open-source datasets, our approach demonstrates superior performance in terms of precision and efficiency compared to existing methods. Furthermore, the techniques described in our work hold promise for addressing various challenging tasks or aspects through subsequent migration.},
  archive   = {C_ICRA},
  author    = {Jiabao Zhang and Yu Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610396},
  month     = {5},
  pages     = {4067-4073},
  title     = {ERASOR++: Height coding plus egocentric ratio based dynamic object removal for static point cloud mapping},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). QuadricsNet: Learning concise representation for geometric
primitives in point clouds. <em>ICRA</em>, 4060–4066. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel framework to learn a concise geometric primitive representation for 3D point clouds. Different from representing each type of primitive individually, we focus on the challenging problem of how to achieve a concise and uniform representation robustly. We employ quadrics to represent diverse primitives with only 10 parameters and propose the first end-to-end learning-based framework, namely QuadricsNet, to parse quadrics in point clouds. The relationships between quadrics mathematical formulation and geometric attributes, including the type, scale and pose, are insightfully integrated for effective supervision of QuaidricsNet. Besides, a novel pattern-comprehensive dataset with quadrics segments and objects is collected for training and evaluation. Experiments demonstrate the effectiveness of our concise representation and the robustness of QuadricsNet. Our code is available at https://github.com/MichaelWu99-lab/QuadricsNet.},
  archive   = {C_ICRA},
  author    = {Ji Wu and Huai Yu and Wen Yang and Gui-Song Xia},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610722},
  month     = {5},
  pages     = {4060-4066},
  title     = {QuadricsNet: Learning concise representation for geometric primitives in point clouds},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Camera relocalization in shadow-free neural radiance fields.
<em>ICRA</em>, 4052–4059. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Camera relocalization is a crucial problem in computer vision and robotics. Recent advancements in neural radiance fields (NeRFs) have shown promise in synthesizing photo-realistic images. Several works have utilized NeRFs for refining camera poses, but they do not account for lighting changes that can affect scene appearance and shadow regions, causing a degraded pose optimization process. In this paper, we propose a two-staged pipeline that normalizes images with varying lighting and shadow conditions to improve camera relocalization. We implement our scene representation upon a hash-encoded NeRF which significantly boosts up the pose optimization process. To account for the noisy image gradient computing problem in grid-based NeRFs, we further propose a re-devised truncated dynamic low-pass filter (TDLF) and a numerical gradient averaging technique to smoothen the process. Experimental results on several datasets with varying lighting conditions demonstrate that our method achieves state-of-the-art results in camera relocalization under varying lighting conditions. Code and data will be made publicly available.},
  archive   = {C_ICRA},
  author    = {Shiyao Xu and Caiyun Liu and Yuantao Chen and Zhenxin Zhu and Zike Yan and Yongliang Shi and Hao Zhao and Guyue Zhou},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611228},
  month     = {5},
  pages     = {4052-4059},
  title     = {Camera relocalization in shadow-free neural radiance fields},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards large-scale incremental dense mapping using
robot-centric implicit neural representation. <em>ICRA</em>, 4045–4051.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large-scale dense mapping is vital in robotics, digital twins, and virtual reality. Recently, implicit neural mapping has shown remarkable reconstruction quality. However, incremental large-scale mapping with implicit neural representations remains problematic due to low efficiency, limited video memory, and the catastrophic forgetting phenomenon. To counter these challenges, we introduce the Robot-centric Implicit Mapping (RIM) technique for large-scale incremental dense mapping. This method employs a hybrid representation, encoding shapes with implicit features via a multi-resolution voxel map and decoding signed distance fields through a shallow MLP. We advocate for a robot-centric local map to boost model training efficiency and curb the catastrophic forgetting issue. A decoupled scalable global map is further developed to archive learned features for reuse and maintain constant video memory consumption. Validation experiments demonstrate our method’s exceptional quality, efficiency, and adaptability across diverse scales and scenes over advanced dense mapping methods using range sensors. Our system’s code will be accessible at https://github.com/HITSZ-NRSL/RIM.git.},
  archive   = {C_ICRA},
  author    = {Jianheng Liu and Haoyao Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611564},
  month     = {5},
  pages     = {4045-4051},
  title     = {Towards large-scale incremental dense mapping using robot-centric implicit neural representation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3QFP: Efficient neural implicit surface reconstruction using
tri-quadtrees and fourier feature positional encoding. <em>ICRA</em>,
4036–4044. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Neural implicit surface representations are currently receiving a lot of interest as a means to achieve high-fidelity surface reconstruction at a low memory cost, compared to traditional explicit representations. However, state-of-the-art methods still struggle with excessive memory usage and non-smooth surfaces. This is particularly problematic in large-scale applications with sparse inputs, as is common in robotics use cases. To address these issues, we first introduce a sparse structure, tri-quadtrees, which represents the environment using learnable features stored in three planar quadtree projections. Secondly, we concatenate the learnable features with a Fourier feature positional encoding. The combined features are then decoded into signed distance values through a small multilayer perceptron. We demonstrate that this approach facilitates smoother reconstruction with a higher completion ratio with fewer holes. Compared to two recent baselines, one implicit and one explicit, our approach requires only 10%–50% as much memory, while achieving competitive quality. The code is released on https://github.com/ljjTYJR/3QFP.},
  archive   = {C_ICRA},
  author    = {Shuo Sun and Malcolm Mielle and Achim J. Lilienthal and Martin Magnusson},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610338},
  month     = {5},
  pages     = {4036-4044},
  title     = {3QFP: Efficient neural implicit surface reconstruction using tri-quadtrees and fourier feature positional encoding},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Augmenting lane perception and topology understanding with
standard definition navigation maps. <em>ICRA</em>, 4029–4035. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous driving has traditionally relied heavily on costly and labor-intensive High Definition (HD) maps, hindering scalability. In contrast, Standard Definition (SD) maps are more affordable and have worldwide coverage, offering a scalable alternative. In this work, we systematically explore the effect of SD maps for real-time lane-topology understanding. We propose a novel framework to integrate SD maps into online map prediction and propose a Transformer-based encoder, SD Map Encoder Representations from transFormers, to leverage priors in SD maps for the lane-topology prediction task. This enhancement consistently and significantly boosts (by up to 60%) lane detection and topology prediction on current state-of-the-art online map prediction methods without bells and whistles and can be immediately incorporated into any Transformer-based lane-topology method. Code is available at https://github.com/NVlabs/SMERF.},
  archive   = {C_ICRA},
  author    = {Katie Z Luo and Xinshuo Weng and Yan Wang and Shuang Wu and Jie Li and Kilian Q Weinberger and Yue Wang and Marco Pavone},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610276},
  month     = {5},
  pages     = {4029-4035},
  title     = {Augmenting lane perception and topology understanding with standard definition navigation maps},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VioLA: Aligning videos to 2D LiDAR scans. <em>ICRA</em>,
4021–4028. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of aligning a video that captures a local portion of an environment to the 2D LiDAR scan of the entire environment. We introduce a method (VioLA) that starts with building a semantic map of the local scene from the image sequence, then extracts points at a fixed height for registering to the LiDAR map. Due to reconstruction errors or partial coverage of the camera scan, the reconstructed semantic map may not contain sufficient information for registration. To address this problem, VioLA makes use of a pre-trained text-to-image inpainting model paired with a depth completion model for filling in the missing scene content in a geometrically consistent fashion to support pose registration. We evaluate VioLA on two real-world RGB-D benchmarks, as well as a self-captured dataset of a large office scene. Notably, our proposed scene completion module improves the pose registration performance by up to 20%.},
  archive   = {C_ICRA},
  author    = {Jun-Jee Chao and Selim Engin and Nikhil Chavan-Dafle and Bhoram Lee and Volkan Isler},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610757},
  month     = {5},
  pages     = {4021-4028},
  title     = {VioLA: Aligning videos to 2D LiDAR scans},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantic-focused patch tokenizer with multi-branch mixer for
visual place recognition. <em>ICRA</em>, 4006–4012. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual Place Recognition (VPR) is critical for navigation and loop closure in autonomous driving tasks, mitigating the impact of shift errors caused by dynamic changes in the environment. Due to the limited ability of backbone networks and extreme environmental changes, current methods fail to capture foundational semantic details that include the distinctive attributes for unique place identification. To address this problem, we propose a new visual token-guided VPR framework that contains a semantic-focused patch tokenizer and a multi-branch Mixer. To mitigate the inference from place-unrelated objects, the semantic-focused patch tokenizer exploits attention-based channel selection and spatial partition, which efficiently captures important semantic information within the channels and preserve spatial relationships among the backbone features. To extract abstract features with spatial structure information, the multi-branch Mixer utilizes a multi-branch structure to aggregate local and global position information, improving the robustness of global representations to environmental changes. Experimental results demonstrate that our method outperforms state-of-the-art methods, achieving 85.3% Recall@1 on the MSLS val dataset and 59.1% Recall@1 on the Nordland dataset when using ResNet18 as the backbone.},
  archive   = {C_ICRA},
  author    = {Zhenyu Xu and Ziliang Ren and Qieshi Zhang and Jie Lou and Dacheng Tao and Jun Cheng},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610372},
  month     = {5},
  pages     = {4006-4012},
  title     = {Semantic-focused patch tokenizer with multi-branch mixer for visual place recognition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024e). RELEAD: Resilient localization with enhanced LiDAR odometry
in adverse environments. <em>ICRA</em>, 3999–4005. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {LiDAR-based localization is valuable for applications like mining surveys and underground facility maintenance. However, existing methods can struggle when dealing with uninformative geometric structures in challenging scenarios. This paper presents RELEAD, a LiDAR-centric solution designed to address scan-matching degradation. Our method enables degeneracy-free point cloud registration by solving constrained ESIKF updates in the front end and incorporates multisensor constraints, even when dealing with outlier measurements, through graph optimization based on Graduated Non-Convexity (GNC). Additionally, we propose a robust Incremental Fixed Lag Smoother (rIFL) for efficient GNC-based optimization. RELEAD has undergone extensive evaluation in degenerate scenarios and has outperformed existing state-of-the-art LiDAR-Inertial odometry and LiDAR-Visual-Inertial odometry methods.},
  archive   = {C_ICRA},
  author    = {Zhiqiang Chen and Hongbo Chen and Yuhua Qi and Shipeng Zhong and Dapeng Feng and Jin Wu and Weisong Wen and Ming Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611074},
  month     = {5},
  pages     = {3999-4005},
  title     = {RELEAD: Resilient localization with enhanced LiDAR odometry in adverse environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unifying local and global multimodal features for place
recognition in aliased and low-texture environments. <em>ICRA</em>,
3991–3998. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Perceptual aliasing and weak textures pose significant challenges to the task of place recognition, hindering the performance of Simultaneous Localization and Mapping (SLAM) systems. This paper presents a novel model, called UMF (standing for Unifying Local and Global Multimodal Features) that 1) leverages multi-modality by cross-attention blocks between vision and LiDAR features, and 2) includes a re-ranking stage that re-orders based on local feature matching the top-k candidates retrieved using a global representation. Our experiments, particularly on sequences captured on a planetary-analogous environment, show that UMF outperforms significantly previous baselines in those challenging aliased environments. Since our work aims to enhance the reliability of SLAM in all situations, we also explore its performance on the widely used RobotCar dataset, for broader applicability. Code and models are available at https://github.com/DLR-RM/UMF.},
  archive   = {C_ICRA},
  author    = {Alberto García-Hernández and Riccardo Giubilato and Klaus H. Strobl and Javier Civera and Rudolph Triebel},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611563},
  month     = {5},
  pages     = {3991-3998},
  title     = {Unifying local and global multimodal features for place recognition in aliased and low-texture environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Self-supervised learning of monocular visual odometry and
depth with uncertainty-aware scale consistency. <em>ICRA</em>,
3984–3990. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The inherent scale ambiguity issue greatly limits the performance of monocular visual odometry. In recent years, a variety of methods have been proposed for self-supervised learning of ego-motion and depth estimation, incorporating specifically designed scale-consistency constraints that utilize estimated depth as a reference. However, these existing methods neglect the influence of the depth uncertainty introduced by the dominant photometric loss, which leads to unreliable depth estimation in difficult regions and detrimentally affects scale alignment. To solve these problems, we introduces a feature-based visual odometry learning system with an effective scale recovery strategy in this paper. Additionally, we propose a learning method to estimate the photometric-sensitive depth uncertainty for guiding the scale recovery. The proposed method is evaluated on KITTI odometry, and the experimental results demonstrate that our system can predict scale-consistent trajectories from monocular videos and achieves state-of-the-art performance. Moreover, the proposed method achieves competitive performance on KITTI depth estimation.},
  archive   = {C_ICRA},
  author    = {Changhao Wang and Guanwen Zhang and Wei Zhou},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610075},
  month     = {5},
  pages     = {3984-3990},
  title     = {Self-supervised learning of monocular visual odometry and depth with uncertainty-aware scale consistency},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From satellite to ground: Satellite assisted visual
localization with cross-view semantic matching. <em>ICRA</em>,
3977–3983. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One of the key challenges of visual Simultaneous Localization and Mapping (SLAM) in large-scale environments is how to effectively use global localization to correct the cumulative errors from long-term tracking. This challenge presents itself in two main aspects: first, the difficulty for robots in revisiting previous locations to perform loop closure, and second, the considerable memory resources required to maintain point-cloud-based global maps. Recent solutions have resorted into neural networks, using satellite images as the references for ground-level localization. However, most of these methods merely provide cross-view patch-matching results, which leads to unfeasible in integration with the SLAM system. To address these issues, we present a semantic-based cross-view localization method. This approach combines semantic information with a reward and penalty mechanism, enabling us to obtain a global probability map and achieve precise 3-degree-of-freedom (3-DoF) localization. Based on that, we develop a SLAM system that capitalizes on satellite imagery for global localization. This strategy effectively bridges the gap between SLAM and real-world coordinates while also substantially reducing accumulated errors. Our experimental results demonstrate that our global localization method significantly outperforms existing satellite-based systems. Moreover, in scenarios where the robot struggles to find loop closures, employing our localization method improves the SLAM accuracy.},
  archive   = {C_ICRA},
  author    = {Xiyue Guo and Haocheng Peng and Junjie Hu and Hujun Bao and Guofeng Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611658},
  month     = {5},
  pages     = {3977-3983},
  title     = {From satellite to ground: Satellite assisted visual localization with cross-view semantic matching},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive outlier thresholding for bundle adjustment in
visual SLAM. <em>ICRA</em>, 3969–3976. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {State-of-the-art V-SLAM pipelines utilize robust cost functions and outlier rejection techniques to remove incorrect correspondences. However, these methods are typically fine-tuned to overfit certain benchmarks and struggle to adapt effectively to changes in the application domain or environmental conditions. This renders them impractical for many robotic applications in which robustness in a wide variety of conditions is essential. In this paper we introduce a novel distribution-based approach for online outlier rejection that reduces the necessity for scene-specific fine-tuning while simultaneously improving the overall SLAM performance. Through experiments across 3 different public datasets, we show that our approach consistently outperforms state-of-the-art methods in various real-world settings. Our code is available at https://github.com/alejandrofontan/ORB_SLAM2_Distribution},
  archive   = {C_ICRA},
  author    = {Alejandro Fontan and Javier Civera and Michael Milford},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610433},
  month     = {5},
  pages     = {3969-3976},
  title     = {Adaptive outlier thresholding for bundle adjustment in visual SLAM},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TP3M: Transformer-based pseudo 3D image matching with
reference image. <em>ICRA</em>, 3962–3968. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image matching is still challenging in such scenes with large viewpoints or illumination changes or with low textures. In this paper, we propose a Transformer-based pseudo 3D image matching method. It upgrades the 2D features extracted from the source image to 3D features with the help of a reference image and matches to the 2D features extracted from the destination image by the coarse-to-fine 3D matching. Our key discovery is that by introducing the reference image, the source image’s fine points are screened and furtherly their feature descriptors are enriched from 2D to 3D, which improves the match performance with the destination image. Experimental results on multiple datasets show that the proposed method achieves the state-of-the-art on the tasks of homography estimation, pose estimation and visual localization especially in challenging scenes.},
  archive   = {C_ICRA},
  author    = {Liming Han and Zhaoxiang Liu and Shiguo Lian},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610556},
  month     = {5},
  pages     = {3962-3968},
  title     = {TP3M: Transformer-based pseudo 3D image matching with reference image},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CoLRIO: LiDAR-ranging-inertial centralized state estimation
for robotic swarms. <em>ICRA</em>, 3920–3926. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collaborative state estimation using different heterogeneous sensors is a fundamental prerequisite for robotic swarms operating in GPS-denied environments, posing a significant research challenge. In this paper, we introduce a centralized system to facilitate collaborative LiDAR-ranging-inertial state estimation, enabling robotic swarms to operate without the need for anchor deployment. The system efficiently distributes computationally intensive tasks to a central server, thereby reducing the computational burden on individual robots for local odometry calculations. The server back-end establishes a global reference by leveraging shared data and refining joint pose graph optimization through place recognition, global optimization techniques, and removal of outlier data to ensure precise and robust collaborative state estimation. Extensive evaluations of our system, utilizing both publicly available datasets and our custom datasets, demonstrate significant enhancements in the accuracy of collaborative SLAM estimates. Moreover, our system exhibits remarkable proficiency in large-scale missions, seamlessly enabling ten robots to collaborate effectively in performing SLAM tasks. In order to contribute to the research community, we will make our code open-source and accessible at https://github.com/PengYu-team/Co-LRIO.},
  archive   = {C_ICRA},
  author    = {Shipeng Zhong and Hongbo Chen and Yuhua Qi and Dapeng Feng and Zhiqiang Chen and Jin Wu and Weisong Wen and Ming Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611672},
  month     = {5},
  pages     = {3920-3926},
  title     = {CoLRIO: LiDAR-ranging-inertial centralized state estimation for robotic swarms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AutoFusion: Autonomous visual geolocation and online dense
reconstruction for UAV cluster. <em>ICRA</em>, 3913–3919. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-time dense reconstruction using Unmanned Aerial Vehicle (UAV) is becoming increasingly popular in large-scale rescue and environmental monitoring tasks. However, due to the energy constraints of a single UAV, the efficiency can be greatly improved through the collaboration of multi-UAVs. Nevertheless, when faced with unknown environments or the loss of Global Navigation Satellite System (GNSS) signal, most multi-UAV SLAM systems can’t work, making it hard to construct a global consistent map. In this paper, we propose a real-time dense reconstruction system called AutoFusion for multiple UAVs, which robustly supports scenarios with lost global positioning and weak co-visibility. A method for Visual Geolocation and Matching Network (VGMN) is suggested by constructing a graph convolutional neural network as a feature extractor. It can acquire geographical location information solely through images. We also present a real-time dense reconstruction framework for multi-UAV with autonomous visual geolocation. UAV agents send images and relative positions to the ground server, which processes the data using VGMN for multi-agent geolocation optimization, including initialization, pose graph optimization, and map fusion. Extensive experiments demonstrate that our system can efficiently and stably construct large-scale dense maps in real-time with high accuracy and robustness.},
  archive   = {C_ICRA},
  author    = {Yizhu Zhang and Shuhui Bu and Yifei Dong and Yu Zhang and Kun Li and Lin Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611315},
  month     = {5},
  pages     = {3913-3919},
  title     = {AutoFusion: Autonomous visual geolocation and online dense reconstruction for UAV cluster},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust multi-robot global localization with unknown initial
pose based on neighbor constraints. <em>ICRA</em>, 3898–3904. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-robot global localization (MR-GL) with unknown initial positions in a large scale environment is a challenging task. The key point is the data association between different robots’ viewpoints. It also makes traditional Appearance-based localization methods unusable. Recently, researchers have utilized the object’s semantic invariance to generate a semantic graph to address this issue. However, previous works lack robustness and are sensitive to overlap rate of maps, resulting in unpredictable performance in real-world environments. In this paper, we propose a data association algorithm based on neighbor constraints to improve the robustness of the system. We demonstrate the effectiveness of our method on three different datasets, indicating a significant improvement in robustness compared to previous works.},
  archive   = {C_ICRA},
  author    = {Yaojie Zhang and Haowen Luo and Weijun Wang and Wei Feng},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610066},
  month     = {5},
  pages     = {3898-3904},
  title     = {Robust multi-robot global localization with unknown initial pose based on neighbor constraints},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tight fusion of odometry and kinematic constraints for
multiple aerial vehicles in physical interconnection. <em>ICRA</em>,
3891–3897. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Integrated aerial Platforms (IAPs), comprising multiple aircrafts, are typically fully actuated and hold significant potential for aerial manipulation tasks. Differing from a multiple aerial swarm, the aircrafts within the IAP are interconnected, presenting promising opportunities for enhancing localization. Incorporating the physical constraints of these multiple aircrafts to improve the accuracy and reliability of integrated aircraft positioning and navigation systems is a challenging yet highly significant problem. In this paper, we introduce a distributed multi-aircraft visual-inertial-range odometry system that analyzes the position, velocity, and attitude constraints within the IAP. Leveraging constraint relationships in the IAP, we propose corresponding methods that tightly fuse visual-inertial-range odometry and kinematic constraints to optimize odometry accuracy. Our system’s performance is validated using a collected dataset, resulting in a notable 28.7% reduction in drift compared to the baseline.},
  archive   = {C_ICRA},
  author    = {Yingjun Fan and Chuanbeibei Shi and Ganghua Lai and Ruiheng Zhang and Yushu Yu and Fuchun Sun and Yiqun Dong},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610282},
  month     = {5},
  pages     = {3891-3897},
  title     = {Tight fusion of odometry and kinematic constraints for multiple aerial vehicles in physical interconnection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The price of a safe flight: Risk cost based path planning.
<em>ICRA</em>, 3884–3890. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A risk aware UAS path planning methodology is proposed using monetary value as the sole cost metric. A third party ground risk model is used to generate a non-uniform costmap for a modified A* heuristic search. The Value of a Prevented Fatality provides a basis to convert fatality risk to monetary value terms as a Human Value at Risk (HVaR) measure. Additional operating and UAS Capital Value at Risk (CVaR) costs are modelled to provide a holistic monetary cost model for path cost minimisation. A number of future cost variants are investigated based upon prior work for a realistic urban-rural mix logistics case study in Southern England. Results show increasingly risk averse paths with decreasing future UAS operating costs.},
  archive   = {C_ICRA},
  author    = {Aliaksei Pilko and Andy Oakey and Mario Ferraro and James Scanlan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610752},
  month     = {5},
  pages     = {3884-3890},
  title     = {The price of a safe flight: Risk cost based path planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Empirical study of ground proximity effects for small-scale
electroaerodynamic thrusters. <em>ICRA</em>, 3868–3875. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Electroaerodynamic (EAD) propulsion, where thrust is produced by collisions between electrostatically-accelerated ions and neutral air, is a potentially transformative method for indoor flight owing to its silent and solid-state nature. Like rotors, EAD thrusters exhibit changes in performance based on proximity to surfaces. Unlike rotors, they have no fragile and quickly spinning parts that have to avoid those surfaces; taking advantage of the efficiency benefits from proximity effects may be a route towards longer-duration indoor operation of ion-propelled fliers. This work presents the first empirical study of ground proximity effects for EAD propulsors, both individually and as quad-thruster arrays. It focuses on multi-stage ducted centimeter-scale actuators suitable for use on small robots envisioned for deployment in human-proximal and indoor environments. Three specific effects (ground, suckdown, and fountain lift), each occurring with a different magnitude at a different spacing from the ground plane, are investigated and shown to have strong dependencies on geometric parameters including thruster-to-thruster spacing, thruster protrusion from the fuselage, and inclusion of flanges or strakes. Peak thrust enhancement ranging from 300 to 600% is found for certain configurations operated in close proximity (0.2 mm) to the ground plane and as much as a 20% increase is measured even when operated centimeters away.},
  archive   = {C_ICRA},
  author    = {Grant Nations and C. Luke Nelson and Daniel S. Drew},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610481},
  month     = {5},
  pages     = {3868-3875},
  title     = {Empirical study of ground proximity effects for small-scale electroaerodynamic thrusters},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flight validation of a global singularity-free aerodynamic
model for flight control of tail sitters. <em>ICRA</em>, 3861–3867. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work validates through flight tests a previously developed wide-envelope singularity-free aerodynamic framework, called ϕ-theory, for modeling dual-engine tail-sitting flying-wing vehicles for optimization-based control. The ϕ-theory methodology imposes a specific geometry on aerodynamic coefficients that leads to polynomial differential equations of motion amenable to semidefinite programming optimization. Through ϕ-theory, we illustrate a typical predicted longitudinal and lateral flight envelope of a tail-sitting vehicle, which, while commonplace for fixed-wing aircraft in performance textbooks, is a novel figure that generalizes fixed-wing doghouse plots to tail-sitting vehicles. This flight envelope figure suggests a novel, natural and intuitive remote piloting interface that we validate in flight tests. Furthermore, we further validate ϕ-theory through the computation of flight features in simulation and their subsequent observation in flight tests.},
  archive   = {C_ICRA},
  author    = {Krishna Murali and Elena P. Moreno and Leandro R. Lustosa},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610780},
  month     = {5},
  pages     = {3861-3867},
  title     = {Flight validation of a global singularity-free aerodynamic model for flight control of tail sitters},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lumped drag model identification and real-time external
force detection for rotary-wing micro aerial vehicles. <em>ICRA</em>,
3853–3860. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work focuses on understanding and identifying the drag forces applied to a rotary-wing Micro Aerial Vehicle (MAV). We propose a lumped drag model that concisely describes the aerodynamical forces the MAV is subject to, with a minimal set of parameters. We only rely on commonly available sensor information onboard a MAV, such as accelerometer data, pose estimate, and throttle commands, which makes our method generally applicable. The identification uses an offline gradient-based method on flight data collected over specially designed trajectories. The identified model allows us to predict the aerodynamical forces experienced by the aircraft due to its own motion in real-time and, therefore, will be useful to distinguish them from external perturbations, such as wind or physical contact with the environment. The results show that we are able to identify the drag coefficients of a rotary-wing MAV through onboard flight data and observe the close correlation between the motion of the MAV, the measured external forces, and the predicted drag forces.},
  archive   = {C_ICRA},
  author    = {Lucas Wälti and Alcherio Martinoli},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610862},
  month     = {5},
  pages     = {3853-3860},
  title     = {Lumped drag model identification and real-time external force detection for rotary-wing micro aerial vehicles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Aerial tensile perching and disentangling mechanism for
long-term environmental monitoring. <em>ICRA</em>, 3827–3833. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Aerial robots show significant potential for forest canopy research and environmental monitoring by providing data collection capabilities at high spatial and temporal resolutions. However, limited flight endurance hinders their application. Inspired by natural perching behaviours, we propose a multi-modal aerial robot system that integrates tensile perching for energy conservation and a suspended actuated pod for data collection. The system consists of a quadrotor drone, a slewing ring mechanism allowing 360° tether rotation, and a streamlined pod with two ducted propellers connected via a tether. Winding and unwinding the tether allows the pod to move within the canopy, and activating the propellers allows the tether to be wrapped around branches for perching or disentangling. We experimentally determined the minimum counterweights required for stable perching under various conditions. Building on this, we devised and evaluated multiple perching and disentangling strategies. Comparisons of perching and disentangling manoeuvres demonstrate energy savings that could be further maximized with the use of the pod or tether winding. These approaches can reduce energy consumption to only 22% and 1.5%, respectively, compared to a drone disentangling manoeuvre. We also calculated the minimum idle time required by the proposed system after the system perching and motor shut down to save energy on a mission, which is 48.9% of the operating time. Overall, the integrated system expands the operational capabilities and enhances the energy efficiency of aerial robots for long-term monitoring tasks.},
  archive   = {C_ICRA},
  author    = {Tian Lan and Luca Romanello and Mirko Kovac and Sophie F. Armanini and Basaran Bahadir Kocer},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609975},
  month     = {5},
  pages     = {3827-3833},
  title     = {Aerial tensile perching and disentangling mechanism for long-term environmental monitoring},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Energy consumption modelling of coaxial-rotor in vortex ring
state for controllable high-speed descending. <em>ICRA</em>, 3803–3809.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to fast climb and descend is crucial for Unmanned Aerial Vehicle (UAV) applications in the mountains. The slower descent speed will affect the UAV’s working efficiency in reaching the rescue area. However, during the fast descent of the rotorcraft, a chaotic flow field rampages as the rotorcraft falls into its wake flow. This is known as the vortex ring. Therefore, the safe descent velocity of consumer UAVs is usually limited to approximately 3m/s. This limitation reduces the potential of UAVs to execute tasks in mountainous and plateau regions. To broaden the task capability constrained by the maximum descending speed, it is necessary to jointly analyze the flow field and the energy consumption during descending. Existing research mainly focused on how to avoid entering the vortex ring instead of offering sufficient power to fly with it. In this paper, in order to achieve an efficient rotorcraft for rescuing in mountainous and plateaus, we break through the maximum-descending-speed of a coaxial rotors UAV. Hence, a power consumption managing pipeline is proposed to extend the power tolerance of the UAV. Specifically, a theoretic model for the coaxial rotors is proposed to analyze the induced velocity and energy consumption during vertical descending. Then, the theoretic model is verified to be consistent with the Computational Fluid Dynamics (CFD) and wind tunnel experiment results. Finally, we optimized the tolerance of the power and dynamic system according to the theoretic model. With this pipeline, our real-time flight achieved 8m/s controlled vertical-descent-speed (CVDS), which is a leading result in both quadrotors and coaxial UAVs.},
  archive   = {C_ICRA},
  author    = {Jiawei Sun and Xiang Zhou and Taoze Ban and Jiannan Zhao and Feng Shuang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610429},
  month     = {5},
  pages     = {3803-3809},
  title     = {Energy consumption modelling of coaxial-rotor in vortex ring state for controllable high-speed descending},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A du-octree based cross-attention model for LiDAR geometry
compression. <em>ICRA</em>, 3796–3802. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Point cloud compression is an essential technology for efficient storage and transmission of 3D data. Previous methods usually use hierarchical tree data structures for encoding the spatial sparseness of point clouds. However, the node context within the tree is not fully discovered since the feature space among nodes varies significantly. To address this problem, we innovatively represent the LiDAR points in a two-octree structure instead of using traditional single-octree coding, and then design the cross-attention model to capture the hierarchical features between different octrees, of which each octree incorporates a transformer-based deep entropy model and an arithmetic encoder. Besides, we introduce the untied cross-aware position encoding with principal component analysis and different projection matrices, which enhances the correlations over two octrees’ attention feature embeddings. Experimental results show that our method outperforms the previous state-of-the-art works, achieving up to 8.2% Bpp savings on point cloud benchmark datasets with different lasers.},
  archive   = {C_ICRA},
  author    = {Mingyue Cui and Mingjian Feng and Junhua Long and Daosong Hu and Shuai Zhao and Kai Huang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610640},
  month     = {5},
  pages     = {3796-3802},
  title     = {A du-octree based cross-attention model for LiDAR geometry compression},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discovering biological hotspots with a passively listening
AUV. <em>ICRA</em>, 3789–3795. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel system which blends multiple distinct sensing modalities in audio-visual surveys to assist marine biologists in collecting datasets for understanding the ecological relationship of fish and other organisms with their habitats on and around coral reefs. Our system, designed for the CUREE AUV, uses four hydrophones to determine the bearing to biological sound sources through beamforming. These observations are merged in a Bayesian Occupancy Grid to produce a 2D map of the acoustic activity of a coral reef. Simultaneously, the AUV uses unsupervised topic modeling to identify different benthic habitats. Combining these maps allows us to determine the level of acoustic activity within each habitat. We demonstrated the system in field trials on reefs in the U.S. Virgin Islands, where it was able to autonomously discover the favored habitats of snapping shrimp (genus Alpheus).},
  archive   = {C_ICRA},
  author    = {Seth McCammon and Stewart Jamieson and T. Aran Mooney and Yogesh Girdhar},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610917},
  month     = {5},
  pages     = {3789-3795},
  title     = {Discovering biological hotspots with a passively listening AUV},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CaveSeg: Deep semantic segmentation and scene parsing for
autonomous underwater cave exploration. <em>ICRA</em>, 3781–3788. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present CaveSeg - the first visual learning pipeline for semantic segmentation and scene parsing for AUV navigation inside underwater caves. We address the problem of scarce annotated training data by preparing a comprehensive dataset for semantic segmentation of underwater cave scenes. It contains pixel annotations for important navigation markers (e.g. caveline, arrows), obstacles (e.g. ground plain and overhead layers), scuba divers, and open areas for servoing. Through comprehensive benchmark analyses on cave systems in USA, Mexico, and Spain locations, we demonstrate that robust deep visual models can be developed based on CaveSeg for fast semantic scene parsing of underwater cave environments. In particular, we formulate a novel transformer-based model that is computationally light and offers near real-time execution in addition to achieving state-of-the-art performance. Finally, we explore the design choices and implications of semantic segmentation for visual servoing by AUVs inside underwater caves. The proposed model and benchmark dataset open up promising opportunities for future research in autonomous underwater cave exploration and mapping.},
  archive   = {C_ICRA},
  author    = {Adnan Abdullah and Titon Barua and Reagan Tibbetts and Zijie Chen and Md Jahidul Islam and Ioannis Rekleitis},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611543},
  month     = {5},
  pages     = {3781-3788},
  title     = {CaveSeg: Deep semantic segmentation and scene parsing for autonomous underwater cave exploration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SONIC: Sonar image correspondence using pose supervised
learning for imaging sonars. <em>ICRA</em>, 3766–3772. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we address the challenging problem of data association for underwater SLAM through a novel method for sonar image correspondence using learned features. We introduce SONIC (SONar Image Correspondence), a pose-supervised network designed to yield robust feature correspondence capable of withstanding viewpoint variations. The inherent complexity of the underwater environment stems from the dynamic and frequently limited visibility conditions, restricting vision to a few meters of often featureless expanses. This makes camera-based systems suboptimal in most open water application scenarios. Consequently, multibeam imaging sonars emerge as the preferred choice for perception sensors. However, they too are not without their limitations. While imaging sonars offer superior long-range visibility compared to cameras, their measurements can appear different from varying viewpoints. This inherent variability presents formidable challenges in data association, particularly for feature-based methods. Our method demonstrates significantly better performance in generating correspondences for sonar images which will pave the way for more accurate loop closure constraints and sonar-based place recognition. Code as well as simulated and real-world datasets are made public on https://github.com/rpl-cmu/sonic to facilitate further development in the field.},
  archive   = {C_ICRA},
  author    = {Samiran Gode and Akshay Hinduja and Michael Kaess},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611678},
  month     = {5},
  pages     = {3766-3772},
  title     = {SONIC: Sonar image correspondence using pose supervised learning for imaging sonars},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Metrically scaled monocular depth estimation through sparse
priors for underwater robots. <em>ICRA</em>, 3751–3757. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we address the problem of real-time dense depth estimation from monocular images for mobile underwater vehicles. We formulate a deep learning model that fuses sparse depth measurements from triangulated features to improve the depth predictions and solve the problem of scale ambiguity. To allow prior inputs of arbitrary sparsity, we apply a dense parameterization method. Our model extends recent state-of-the-art approaches to monocular image based depth estimation, using an efficient encoder-decoder backbone and modern lightweight transformer optimization stage to encode global context. The network is trained in a supervised fashion on the forward-looking underwater dataset, FLSea. Evaluation results on this dataset demonstrate significant improvement in depth prediction accuracy by the fusion of the sparse feature priors. In addition, without any retraining, our method achieves similar depth prediction accuracy on a downward looking dataset we collected with a diver operated camera rig, conducting a survey of a coral reef. The method achieves real-time performance, running at 24 FPS on a NVIDIA Jetson Xavier NX, 160 FPS on a NVIDIA RTX 2080 GPU and 7 FPS on a single Intel i9-9900K CPU core, making it suitable for direct deployment on embedded GPU systems. The implementation of this work is made publicly available at https://github.com/ebnerluca/uw_depth.},
  archive   = {C_ICRA},
  author    = {Luca Ebner and Gideon Billings and Stefan Williams},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611007},
  month     = {5},
  pages     = {3751-3757},
  title     = {Metrically scaled monocular depth estimation through sparse priors for underwater robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RUMP: Robust underwater motion planning in dynamic
environments of fast-moving obstacles. <em>ICRA</em>, 3743–3750. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robust underwater motion planning of autonomous underwater vehicles (AUVs) in dynamic cluttered environments is a problem that has yet to be addressed in depth. Due to advances in technology and computational capacity, AUVs are expected to operate safely and autonomously in increasingly challenging environments, necessitating methods that are able to safely navigate robots in real-time. Though, most solutions remain overly cautious and conservative. This paper proposes RUMP, a novel locally-optimal motion planning framework for robust real-time autonomous underwater navigation in 3D cluttered environments consisting of observed static and dynamic obstacles. The problem is modeled using path optimization and can be solved in real-time with a common nonlinear solver. The constructed objective function allows deciding the local goal during optimization to both maximize safety within a planning horizon and minimize the expected distance to the target position. Furthermore, path safety is considered for the entire transition between consecutive states, utilizing a novel approach for continuous spatiotemporal collision checks. The proposed formulation provides safe performance even in environments with obstacles that may move orders of magnitude faster than the AUV itself. Simulation experiments, in different challenging scenarios of obstacles moving up to 100 times faster than the robot, showcase robustness and efficient real-time performance of more than 15 Hz.},
  archive   = {C_ICRA},
  author    = {Herman Biørn Amundsen and Torben Falleth Olsen and Marios Xanthidis and Martin Føre and Eleni Kelasidi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610406},
  month     = {5},
  pages     = {3743-3750},
  title     = {RUMP: Robust underwater motion planning in dynamic environments of fast-moving obstacles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel fish-inspired self-adaptive approach to collective
escape of swarm robots based on neurodynamic models. <em>ICRA</em>,
3736–3742. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fish schools present high-efficiency group behaviors to collective migration and dynamic escape from the predator through simple individual interactions. The purpose of this research is to infuse swarm robots with &quot;fish-like&quot; intelligence that will enable safe navigation and efficient cooperation, and successful completion of escape tasks in changing environments. In this paper, a novel fish-inspired self-adaptive approach is proposed for the collective escape of swarm robots. A bio-inspired neural network (BINN) is introduced to generate collision-free escape trajectories through the dynamics of neural activity and the combination of attractive and repulsive forces. In addition, a neurodynamics-based self-adaptive mechanism is proposed to improve the self-adaptive performance of the swarm robots in dynamic environments. Similar to fish escape maneuvers, simulations and real-robot experiments show that the swarm robots can collectively leave away from the threat and respond to sudden environmental changes. Several comparison studies demonstrated that the proposed approach can significantly improve the effectiveness, efficiency, and flexibility of swarm robots in complex environments.},
  archive   = {C_ICRA},
  author    = {Junfei Li and Simon X. Yang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610169},
  month     = {5},
  pages     = {3736-3742},
  title     = {A novel fish-inspired self-adaptive approach to collective escape of swarm robots based on neurodynamic models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time estimation for the swimming direction of robotic
fish based on IMU sensors*. <em>ICRA</em>, 3721–3727. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An increasing number of underwater robots inspired by Carangidae are developed, which is characterized by high efficiency and flexibility. However, estimating the swimming direction of these robotic fish is challenging due to the constant swinging of the head during movement, which complicates precise control. In this study, we installed two low-cost inertial measurement unit (IMU) sensors separately on the head and tail parts of a double-joint robotic fish and presented a method for accurately and timely estimating the swimming direction. Firstly, we effectively compensated for the yaw angle drift of the IMU sensors through a fused Kalman Filter. Furthermore, we propose the Anti-Shake Estimation (ASE) algorithm to calculate the real-time swimming direction using filtered yaw angles at a high updating rate of 100Hz. Finally, we applied the method to swimming direction feedback control for evaluation and comparison. The results show that our ASE method performs better than other existing methods in straight-line swimming experiments. The experiment of S-curve swimming also demonstrates the effectiveness of our method in complex missions.},
  archive   = {C_ICRA},
  author    = {Shikun Li and Yufan Zhai and Chen Wang and Guangming Xie},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610815},
  month     = {5},
  pages     = {3721-3727},
  title     = {Real-time estimation for the swimming direction of robotic fish based on IMU sensors*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model-based approach for lateral maneuvers of bird-size
ornithopter. <em>ICRA</em>, 3684–3690. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A model-based approach for lateral maneuvering of flapping wing UAVs in closed spaces is presented. Bird-size ornithopters do not have asymmetric actuation in the wing due to mechanical complexity, so they rely upon the tail for lateral maneuvering. The prototype E-Flap can deflect the vertical tail to make maneuvers out of the longitudinal plane. This work defines simplified equations for the steady turning maneuver based on the body roll angle. The relation between the velocity of the prototype and the turning radius is also stated. Then, an approach to the attitude is proposed, defining the relation between the deflection of the vertical tail and the roll angle. We prove that, even though this deflection causes a yaw moment, the coupling between yaw and roll dynamics generates also a roll rate. To validate this simplified model, a simple control is presented for continuous circular trajectory tracking inside an indoor flight zone. The objective is to track circular trajectories of a radius 2 times greater than the wingspan at a constant height. Results show a very good agreement between the theoretical and experimental turning radius. In addition, the direct relation between the vertical tail deflection and the roll rate of the ornithopter is identified. Even though the desired radius is not reached, the FWUAV is capable of maintaining a closed turning maneuver for several laps. Therefore, the insight provided by the model proves to be an appropriate approach for aggressive lateral maneuvers of bird-size ornithopters.},
  archive   = {C_ICRA},
  author    = {Ernesto Sanchez-Laulhe and Álvaro C. Satué Crespo and Saeed Rafee Nekoo and Anibal Ollero},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611580},
  month     = {5},
  pages     = {3684-3690},
  title     = {Model-based approach for lateral maneuvers of bird-size ornithopter},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Ospreys-inspired self-takeoff strategy of an eagle-scale
flapping-wing robot: System design and flight experiments.
<em>ICRA</em>, 3669–3675. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we achieved a self-takeoff of an eagle-scale flapping-wing robot for the first time. Inspired by the takeoff process of Ospreys, we propose a bio-inspired takeoff strategy, then discuss the dynamic model and the requirements for self-takeoff. Based on the requirements of flight strategy, we designed a system with two parts, including a flapping-wing aircraft with a wingspan of 1.8m and a take-off weight of 870g, and an auxiliary platform with an initial pitch angle adjustment function. In order to explore the differences in the take-off process under different conditions, we conduct the flight experiments under different time-averaged thrust-to-weight ratios (0.745-0.876) and launch angles (45°-90°). The results of flight experiments confirmed the theoretical analysis that the flapping-wing robot can achieve self-takeoff with no potential energy cost and maintain high maneuverability (The video shows a rapid climb immediately after takeoff) even when the time-averaged thrust-to-weight ratio is smaller than 1. This is significantly different from conventional rotary-wing and vertical take-off and landing (VTOL) UAVs. This work solves the challenge of self-takeoff for large-scale flapping-wing robots using a designable method and demonstrates the superior performance potential of flapping-wing robots compared to conventional UAVs.},
  archive   = {C_ICRA},
  author    = {Haoyu Wang and Wenfu Xu and Linpo Hou and Erzhen Pan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610958},
  month     = {5},
  pages     = {3669-3675},
  title     = {Ospreys-inspired self-takeoff strategy of an eagle-scale flapping-wing robot: System design and flight experiments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatic loading of unknown material with a wheel loader
using reinforcement learning. <em>ICRA</em>, 3646–3652. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Loading multiple different materials with wheel loaders is a challenging task because various materials require different loading techniques. It’s, therefore, difficult to find a single controller capable of handling them all. One solution is to use a base controller and fine-tune it for different materials. Reinforcement Learning (RL) automates this process without the need for collecting additional human-annotated data. We investigated the feasibility of this approach using a full-size 24-tonnes wheel loader in the real world and demonstrated that it’s possible to fine-tune a neural network controller that was originally trained with imitation learning on blasted rock for use with an unknown gravel material, requiring 20 bucket fillings. Additionally, we showcased the adaptability of a controller pre-trained on woodchips for an unknown gravel material, requiring 40 bucket fillings. We also proposed a novel reward function for the material loading task. Finally, we examined how the sampling time of the reinforcement learning algorithm affects convergence speed and adaptability. Our results demonstrate that it’s optimal to match the sampling time of the RL algorithm to the delays of the wheel loader’s hydraulic actuators.},
  archive   = {C_ICRA},
  author    = {Daniel Eriksson and Reza Ghabcheloo and Marcus Geimer},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610221},
  month     = {5},
  pages     = {3646-3652},
  title     = {Automatic loading of unknown material with a wheel loader using reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable underwater assembly with reconfigurable visual
fiducials. <em>ICRA</em>, 3639–3645. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a scalable combined localization infrastructure deployment and task planning algorithm for underwater assembly. Infrastructure is autonomously modified to suit the needs of manipulation tasks based on an uncertainty model on the infrastructure’s positional accuracy. Our uncertainty model can be combined with the noise characteristics from multiple sensors. For the task planning problem, we propose a layer-based clustering approach that completes the manipulation tasks one cluster at a time. We employ movable visual fiducial markers as infrastructure and an autonomous underwater vehicle (AUV) for manipulation tasks. The proposed task planning algorithm is computationally simple, and we implement it on AUV without any offline computation requirements. Combined hardware experiments and simulations over large datasets show that the proposed technique is scalable to large areas.},
  archive   = {C_ICRA},
  author    = {Samuel Lensgraf and Ankita Sarkar and Adithya Pediredla and Devin Balkcom and Alberto Quattrini Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611643},
  month     = {5},
  pages     = {3639-3645},
  title     = {Scalable underwater assembly with reconfigurable visual fiducials},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LiSTA: Geometric object-based change detection in cluttered
environments. <em>ICRA</em>, 3632–3638. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present LiSTA (LiDAR Spatio-Temporal Analysis), a system to detect probabilistic object-level change over time using multi-mission SLAM. Many applications require such a system, including construction, robotic navigation, long-term autonomy, and environmental monitoring. We focus on the semi-static scenario where objects are added, subtracted, or changed in position over weeks or months. Our system combines multi-mission LiDAR SLAM, volumetric differencing, object instance description, and correspondence grouping using learned descriptors to keep track of an open set of objects. Object correspondences between missions are determined by clustering the object’s learned descriptors. We demonstrate our approach using datasets collected in a simulated environment and a real-world dataset captured using a LiDAR system mounted on a quadruped robot monitoring an industrial facility containing static, semi-static, and dynamic objects. Our method demonstrates superior performance in detecting changes in semi-static environments compared to existing methods.},
  archive   = {C_ICRA},
  author    = {Joseph Rowell and Lintong Zhang and Maurice Fallon},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610102},
  month     = {5},
  pages     = {3632-3638},
  title     = {LiSTA: Geometric object-based change detection in cluttered environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-reconfigurable robots for collaborative discrete
lattice assembly. <em>ICRA</em>, 3624–3631. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a robotic system for the assembly of 3D discrete lattice structures in which the robots are able to self-reproduce, such that the assembly system may scale its own parallelization. Robots and structures are made from a set of compatible building blocks, or voxels, which can be assembled and reassembled into more complex structures. Robotic modules are made by combining actuators with a functional voxel, which routes electrical power and signals. Robotic modules then assemble into reconfigurable robots via a reversible solder joint. The robot assembles higher performance structures using a set of construction voxels, which do not contain electrical features. This paper describes the design, development, and evaluation of this assembly system, including the robotic hardware, lattice material, and planning and controls methods. We demonstrate the system through a set of fundamental assembly tasks: the robot assembling another robot, and the two robots collaborating to assemble a small structure.},
  archive   = {C_ICRA},
  author    = {Miana Smith and Amira Abdel-Rahman and Neil Gershenfeld},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609866},
  month     = {5},
  pages     = {3624-3631},
  title     = {Self-reconfigurable robots for collaborative discrete lattice assembly},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hyblock: Hardware realization and control of modular
hydraulic robots with dowel connectors. <em>ICRA</em>, 3609–3615. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents the hardware design and development of Hyblock, a modular hydraulic robot for heavy-duty application such as construction. The robot is equipped with a simple docking mechanism called a C-type expansion dowel and a novel hydraulic circuit MHSB that matches the modular structure. In this paper, we first report on the design of the robot hardware including the dowel and hydraulic circuit, then present preliminary experiments on pressure-based torque control and docking control using proximal magnetic sensors. Next, we propose a framework for dynamic reconfiguration and task-space motion control built on the concept of dowel connectors. Simulation results demonstrate that a collective modular robot achieves desired motion tasks while keeping all normal contact forces of the connectors being lower-bound. The results are also explained in the supplementary video.},
  archive   = {C_ICRA},
  author    = {Sang-Ho Hyon and Ryo Ando and Eiji Sono and Shunichi Sugimoto and Yasushi Saitou},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611336},
  month     = {5},
  pages     = {3609-3615},
  title     = {Hyblock: Hardware realization and control of modular hydraulic robots with dowel connectors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward a framework integrating augmented reality and virtual
fixtures for safer robot-assisted lymphadenectomy. <em>ICRA</em>,
3602–3608. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Lymphadenectomy generally accompanies various oncology surgeries to remove infected cancer cells. However, there are two limitations in robot-assisted lymphadenectomy: 1) lymph nodes are not visible during operation since they are hidden by the superficial fat layer; 2) intra-operative bleeding may occur during lymph node removal caused by collisions between surgical instruments and delicate blood vessels (arteries or veins) near the lymph nodes. Therefore, we propose a framework integrating augmented reality and virtual fixtures to address these limitations. Augmented reality intra-operatively visualizes the hidden lymph nodes by projecting the corresponding 3D pre-operative model, and virtual fixtures are used to provide force feedback to surgeons to avoid possible collisions when they operate the surgical instruments to resect the lymph nodes surrounding the blood vessel. Ten human subjects were invited to perform an emulated lymphadenectomy based on the da Vinci robot in a dry lab. Experimental results demonstrated that the proposed framework can keep localizing the hidden lymph nodes, and reduce the number of collisions (21% and 48% reduction rates using two different force models compared to the standard setup, respectively) between the instruments and the delicate blood vessel during lymph node resection. It shows the potential to enhance the safety of robot-assisted lymphadenectomy.},
  archive   = {C_ICRA},
  author    = {Ziyang Chen and Ke Fan and Laura Cruciani and Matteo Fontana and Lorenzo Muraglia and Francesco Ceci and Laura Travaini and Giancarlo Ferrigno and Elena De Momi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610099},
  month     = {5},
  pages     = {3602-3608},
  title     = {Toward a framework integrating augmented reality and virtual fixtures for safer robot-assisted lymphadenectomy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intelligent disinfection robot with high-touch surface
detection and dynamic pedestrian avoidance. <em>ICRA</em>, 3595–3601.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The increasing awareness of public health issues has highlighted the need for effective disinfection of crowded indoor public areas, leading to the development of automated disinfection robots. However, most of the existing robots spray disinfectant in all areas, and they are still immature to navigate in densely populated environments. Hence, in this paper, we design a new disinfection robotic system consisting of a mobile platform, an RGB-D camera, and a robotic arm with a spray disinfection device. To address the above challenges, we propose a vision-based method for accurately detecting high-touch areas in the surroundings, enabling the disinfection robot to achieve superior disinfection efficiency. In addition, we propose a dynamic pedestrian avoidance method, namely Socially Aware APF (SA-APF), which can predict the movement trend of pedestrians and plan the path in real-time. Both simulated and real-world experiments are conducted to demonstrate the effectiveness of our disinfection robot system, especially highlighting the ability to detect high-touch areas and navigate in the environment while avoiding dynamic pedestrians.},
  archive   = {C_ICRA},
  author    = {Yunfei Luan and Muhang He and Yudong Tian and Chengjie Lin and Yunhan Fang and Zihao Zhao and Jianxin Yang and Yao Guo},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610836},
  month     = {5},
  pages     = {3595-3601},
  title     = {Intelligent disinfection robot with high-touch surface detection and dynamic pedestrian avoidance},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Haptic-assisted collaborative robot framework for improved
situational awareness in skull base surgery. <em>ICRA</em>, 3588–3594.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Skull base surgery is a demanding field in which surgeons operate in and around the skull while avoiding critical anatomical structures including nerves and vasculature. While image-guided surgical navigation is the prevailing standard, limitation still exists requiring personalized planning and recognizing the irreplaceable role of a skilled surgeon. This paper presents a collaboratively controlled robotic system tailored for assisted drilling in skull base surgery. Our central hypothesis posits that this collaborative system, enriched with haptic assistive modes to enforce virtual fixtures, holds the potential to significantly enhance surgical safety, streamline efficiency, and alleviate the physical demands on the surgeon. The paper describes the intricate system development work required to enable these virtual fixtures through haptic assistive modes. To validate our system’s performance and effectiveness, we conducted initial feasibility experiments involving a medical student and two experienced surgeons. The experiment focused on drilling around critical structures following cortical mastoidectomy, utilizing dental stone phantom and cadaveric models. Our experimental results demonstrate that our proposed haptic feedback mechanism enhances the safety of drilling around critical structures compared to systems lacking haptic assistance. With the aid of our system, surgeons were able to safely skeletonize the critical structures without breaching any critical structure even under obstructed view of the surgical site.},
  archive   = {C_ICRA},
  author    = {Hisashi Ishida and Manish Sahu and Adnan Munawar and Nimesh Nagururu and Deepa Galaiya and Peter Kazanzides and Francis X. Creighton and Russell H. Taylor},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611187},
  month     = {5},
  pages     = {3588-3594},
  title     = {Haptic-assisted collaborative robot framework for improved situational awareness in skull base surgery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Preliminary study of fingertip and wrist motion based haptic
controller for robotically assisted micro- and supermicrosurgery.
<em>ICRA</em>, 3582–3587. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One issue of robotic microsurgery is that compared to manual surgery, the operation time tends to be longer due to high motion scaling. To address this issue, we developed a new controller that can provide the accuracy required for microsurgery without a high scaling factor by utilizing fingertip and wrist motions. Also, for the better outcome of surgery, the proposed controller has a force feedback function which is not available for the existing controllers for microsurgical robots. A challenge of designing such a controller is associated with the size requirement. For conventional microsurgery, surgeons perform surgical procedures while looking at the eyepieces of a surgical microscope and the same applies for robotic microsurgery. The only space available to manipulate controllers is a narrow space between the patient/surgical bed and the surgeon. To satisfy this constraint, the proposed controller is integrated with a handrest and the controller’s DOFs are strategically allocated. In this work, as the first step of addressing the issue of prolonged operational time, we built a prototype controller and evaluated the accuracy and task space with simulations. The results indicated that by using the fingertip and wrist motions with a scaling factor of 3x, 0.5 mm diameter circles could be traced with a mean bidirectional precision of 0.0485 mm. Also, 10.0 mm diameter circles were traceable with the same scaling factor.},
  archive   = {C_ICRA},
  author    = {Muneaki Miyasaka and Pepijn Van Esch and Atsushi Morikawa and Kotaro Tadano},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610859},
  month     = {5},
  pages     = {3582-3587},
  title     = {Preliminary study of fingertip and wrist motion based haptic controller for robotically assisted micro- and supermicrosurgery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hybrid admittance control algorithm for automatic robotic
cranium-milling. <em>ICRA</em>, 3575–3581. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Prior robot-assisted cranium-milling studies only considered controlling the force in the skull’s vertical direction and neglected the milling cutter’s feed force. Additionally, achieving stable force control in multiple directions is challenging for robots due to the uneven skull surface. Here a hybrid admittance control algorithm incorporating a model-free adaptive nonlinear force control and fuzzy control algorithms is proposed to accomplish effective automatic cranial-milling tasks. First, a pure data-driven model-free adaptive control method based on partial form dynamic linearization is used to control the feed force. Second, fuzzy control minimizes the total error of both the vertical and feed force by adaptively adjusting the milling cutter’s velocity and position. 42 ex vivo animal skull-milling experiments conducted by the automatic robotic cranium-milling system indicate that when using the proposed control algorithm, the force error percentage can be maintained below 5.0% within 3 s and the maximal root mean square error percentages for vertical and feed force are 1.85% and 1.94%, respectively. Moreover, no instances of dura mater damage are observed and the robotic system exhibits a high level of autonomy as it performs the skull milling task with minimal human involvement throughout the entire experiment. The results suggest the potential for advancing the intelligence level of neurosurgery in the future.},
  archive   = {C_ICRA},
  author    = {Chen Qian and Zhen Li and Qiang Ye and Peicong Ge and Jizong Zhao and Gui-Bin Bian},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610672},
  month     = {5},
  pages     = {3575-3581},
  title     = {A hybrid admittance control algorithm for automatic robotic cranium-milling},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Elliptical torus-based six-axis FBG force sensor with
in-situ calibration for condition monitoring of orthopedic surgical
robot*. <em>ICRA</em>, 3561–3566. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Six-axis force/moment (6-A F/M) sensors make surgical robots effectively sense intraoperative force feedback and drilling status information, reducing the operating challenges and psychological burden of doctors, which also improves the quality and safety of surgery. However, it is difficult for current commercial electrical 6-A F/M sensors to adapt the electromagnetic environment in the operating room, and status changes after installation can also reduce accuracy. At the same time, there is a strong vibration coupling of low-frequency force information, leading to low identification accuracy and slow response speed in the drilling and milling status. Aiming at these problems, an elliptical torus-based 6-A fiber optic F/M sensor and its in-situ calibration method for orthopedic surgical robot force sensing are proposed. Furthermore, combined with the multichannel one-dimensional convolutional gated recurrent unit (M1-DCGRU), a fast and accurate identification of seven drilling stages was realized. The final force sensing error is less than 7.1%, and the drilling state identification accuracy is at least 93.9%. The designed sensor has higher accuracy, is compatible with magnetic resonance imaging (MRI), and accurately identifies finer drilling stages without relying on other sensors.},
  archive   = {C_ICRA},
  author    = {Tianliang Li and Chen Zhao and Yuhang Wen and Fayin Chen and Yuegang Tan and Zude Zhou},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611093},
  month     = {5},
  pages     = {3561-3566},
  title     = {Elliptical torus-based six-axis FBG force sensor with in-situ calibration for condition monitoring of orthopedic surgical robot*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robotic craniomaxillofacial osteotomy system using acoustic
3D registration *. <em>ICRA</em>, 3554–3560. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Osteotomy holds a pivotal position among the fundamental procedures in craniomaxillofacial (CMF) surgery. However, there are inherent challenges and risks associated with ensuring the recuperation of occlusion, safeguarding the facial nerves and blood vessels, as well as preserving facial aesthetics. In this study, a non-invasive image-to-patient registration method for navigation/robotic CMF surgery based on intraoperative freehand ultrasound (US) 3D reconstruction is proposed. Building upon this, a CMF osteotomy robotic system with compliant human-robot interaction and osteotomy trajectory planning was devised. In the freehand US 3D reconstruction and registration experiments, the registration errors for human volunteers and phantoms were consistently less than 1 mm. In robot osteotomy experiments based on the resulting registration, the average osteotomy error was below 1.5 mm. The proposed US 3D reconstruction based registration method is non-invasive and radiation-free, and shows the promising accuracy which is suitable for CMF robotic or navigation systems.},
  archive   = {C_ICRA},
  author    = {Jiayu Zhu and Runzhe Han and Mengning Yuan and Bimeng Jie and Shanshan Du and Yang He and Runshi Zhang and Junchen Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610887},
  month     = {5},
  pages     = {3554-3560},
  title     = {Robotic craniomaxillofacial osteotomy system using acoustic 3D registration *},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An integrated position-velocity-force method for
safety-enhanced shared control in robot-assisted surgical cutting.
<em>ICRA</em>, 3533–3539. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Numerous studies have emphasized the application of autonomous intelligence in human-robot shared control to enhance surgical convenience and efficiency. However, the neglect of human dominance may reduce surgical safety. This paper developed a safety-enhanced human-robot shared control method by intelligently allocating control authority, with the surgeon remaining the leader during the surgical procedure. Three controllers are designed initially, including a master hand position (MP) controller and a master hand velocity (MV) controller related to the surgeon&#39;s manipulation, and a planned trajectory tracking (PT) controller related to the robot. In precision surgical manipulation scenarios, precise tracking of the human&#39;s operation is achieved by combining MP and MV controllers, while a combination of MV and PT controllers is developed in high-efficiency surgical scenarios, which relaxes the requirement for precise tracking of hand position and enables precise robot assistance guided by the velocity of human hand. The autonomous scenarios and controllers switching are accomplished through a motion fusion mechanism, which is achieved via optimizing evaluation functions that are reliant on future states. Furthermore, a force feedback mechanism is proposed to help human understand the intent of autonomous control to improve safety. The feasibility and effectiveness of this method have been validated through simulations and experiments.},
  archive   = {C_ICRA},
  author    = {Xilin Xiao and Xiaojian Li and Yudong Shi and Jin Fang and Lin Li and Pengfei He and Hangjie Mo},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610466},
  month     = {5},
  pages     = {3533-3539},
  title     = {An integrated position-velocity-force method for safety-enhanced shared control in robot-assisted surgical cutting},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Constrained nonlinear disturbance observer for robotic
systems. <em>ICRA</em>, 3526–3532. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Disturbance observer (DOB) is a well-known two-loop control structure that imparts robustness to a controller with a simple implementation. As a nonlinear DOB for the robotic systems, we proposed so-called nonlinear robust internal-loop compensator (NRIC) framework in our previous work. In this paper, we further extend the NRIC in such a way that an optimization scheme can be embedded in the control structure. The proposed method is called constrained NRIC (C-NRIC), because the optimization allows us to impose constraints, by which a controller acquires additional properties. As a particular use case of the C-NRIC framework, we design contact-responsive motion controllers that enables a robot to react to unknown interactions while accurately tracking the desired trajectory in free motion. The effectiveness of such designs is validated through the real-world experiments.},
  archive   = {C_ICRA},
  author    = {Ji Wan Han and Daehyung Park and Min Jun Kim},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611091},
  month     = {5},
  pages     = {3526-3532},
  title     = {Constrained nonlinear disturbance observer for robotic systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model-free control of a class of high-precision scanning
motion systems with piezoceramic actuators. <em>ICRA</em>, 3520–3525.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To enhance the precision of coarse long-stroke motion axes, complementary short-stroke fine positioning stages are usually introduced. Being mechanically attached, the motion of the combined positioning stages needs to be controlled and synchronized. Therefore, typically suitable model-based controllers of fine stages are designed according to the sophisticated models and identification techniques used. Due to their appealing features, Piezocermamic-based fine positioning stages were successfully utilized in many applications, which recently sparked their use in high-acceleration motion found in wafer scanners, for example, where high-precision motion is required despite the resulting high inertial forces involved. Unfortunately, hard nonlinear behavior is associated with piezoelectric actuators, which adds to the complexity of modeling, control, and synchronization processes. To overcome such a burden, in this study, the design procedure of a model-free control and synchronization technique of piezocermamic-based fine positioning stages is introduced and verified experimentally using a representative precision motion system comprising a planner stage and a uni-axial fine stage under step-and-scan trajectories commonly used in wafer scanners. Despite its simplicity, the herein proposed design procedure can be seamlessly extended to other robotics and automation applications.},
  archive   = {C_ICRA},
  author    = {Yazan M. Al-Rawashdeh and Mohammad Al Saaideh and Marcel F. Heertjes and Mohammad Al Janaideh},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611589},
  month     = {5},
  pages     = {3520-3525},
  title     = {Model-free control of a class of high-precision scanning motion systems with piezoceramic actuators},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhanced robust motion control based on unknown system
dynamics estimator for robot manipulators. <em>ICRA</em>, 3514–3519. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To achieve high-accuracy manipulation in the presence of unknown disturbances, we propose two novel efficient and robust motion control schemes for high-dimensional robot manipulators. Both controllers incorporate an unknown system dynamics estimator (USDE) to estimate disturbances without requiring acceleration signals and the inverse of inertia matrix. Then, based on the USDE framework, an adaptive-gain controller and a super-twisting sliding mode controller are designed to speed up the convergence of tracking errors and strengthen anti-perturbation ability. The former aims to enhance feedback portions through error-driven control gains, while the latter exploits finite-time convergence of discontinuous switching terms. We analyze the boundedness of control signals and the stability of the closed-loop system in theory, and conduct real hardware experiments on a robot manipulator with seven degrees of freedom (DoF). Experimental results verify the effectiveness and improved performance of the proposed controllers, and also show the feasibility of implementation on high-dimensional robots.},
  archive   = {C_ICRA},
  author    = {Xinyu Jia and Jun Yang and Kaixin Lu and Yongping Pan and Haoyong Yu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611460},
  month     = {5},
  pages     = {3514-3519},
  title     = {Enhanced robust motion control based on unknown system dynamics estimator for robot manipulators},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Phase synthesis for spatial locomotion control of
retractable worm robots. <em>ICRA</em>, 3507–3513. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Retractable worm robots possess hyper-flexibility, allowing them to work in confined spaces that are difficult for humans. However, the spatial locomotion control of these robots remains challenging due to the robots’ large degrees of freedom. To address this challenge, we propose a phase synthesis (PS) scheme for retractable worm robots. The scheme combines an undulating gait inspired by caterpillars with three-dimensional movement commands. We first introduce the kinematics model and real-world prototype of our retractable worm robot, called RW-Robot, and then we introduce footstep phases to express the timing of segments’ spatial movement. According to the length of movement periods, we classify the movement into short-term movements and long-term movements and compress their patterns in the frequency domain. Our PS scheme aligns the patterns according to the footstep phases to generate new gaits of spatial locomotion. We evaluate the scheme in real-world experiments, including steering and climbing a slope. The experimental results indicate that our scheme allows the RW-Robot to perform flexible spatial locomotion from simple user input.},
  archive   = {C_ICRA},
  author    = {Zhongcheng Wang and Shiwei Yuan and Manfeng Dou and Jianhua Yang and Bin Liang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610882},
  month     = {5},
  pages     = {3507-3513},
  title     = {Phase synthesis for spatial locomotion control of retractable worm robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust and remote center of cyclic motion control for
redundant robots with partially unknown structure. <em>ICRA</em>,
3492–3498. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Remote center of motion (RCM) describes a robot with a rod-like end-effector operating through a hole in the interface separating the internal space from the external space. Considering that the control of RCM may be influenced by perturbations (noises) and that the end-effector is frequently replaced to complete different tasks, the structural information related to the robot manipulator and its rod-like end-effector may contain errors. This paper proposes an acceleration-level remote center of cyclic motion (ARC 2 M) control scheme, which takes into account the cyclic motion index and the physical limitations of robot manipulators to achieve repetitive motion planning and RCM control at the acceleration level. Additionally, a parameter calculation method is proposed to compute unknown parameters of the end-effector under the influence of noise. Kalman filter and a neural dynamics-based method are employed to address noises effects, and related theoretical analyses are given. To validate the proposed ARC 2 M scheme, simulations and physical experiments are carried out. The source code is available at https://github.com/LongJin-lab/ARCM.},
  archive   = {C_ICRA},
  author    = {Long Jin and Kun Liu and Mei Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611145},
  month     = {5},
  pages     = {3492-3498},
  title     = {Robust and remote center of cyclic motion control for redundant robots with partially unknown structure},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Safety-critical control of quadrupedal robots with rolling
arms for autonomous inspection of complex environments. <em>ICRA</em>,
3485–3491. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a safety-critical control framework tailored for quadruped robots equipped with a roller arm, particularly when performing locomotive tasks such as autonomous robotic inspection in complex, multi-tiered environments. In this study, we consider the problem of operating a quadrupedal robot in distillation columns, locomoting on column trays and transitioning between these trays with a roller arm. To address this problem, our framework encompasses the following key elements: 1) Trajectory generation for seamless transitions between columns, 2) Foothold re-planning in regions deemed unsafe, 3) Safety-critical control incorporating control barrier functions, 4) Gait transitions based on safety levels, and 5) A low-level controller. Our comprehensive framework, comprising these components, enables autonomous and safe locomotion across multiple layers. We incorporate reduced-order and full-body models to ensure safety, integrating safety-critical control and footstep re-planning approaches. We validate the effectiveness of our proposed framework through practical experiments involving a quadruped robot equipped with a roller arm, successfully navigating and transitioning between different levels within the column tray structure.},
  archive   = {C_ICRA},
  author    = {Jaemin Lee and Jeeseop Kim and Wyatt Ubellacker and Tamas G. Molnar and Aaron D. Ames},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610504},
  month     = {5},
  pages     = {3485-3491},
  title     = {Safety-critical control of quadrupedal robots with rolling arms for autonomous inspection of complex environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Safety-critical coordination of legged robots via layered
controllers and forward reachable set based control barrier functions.
<em>ICRA</em>, 3478–3484. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a safety-critical approach to the coordination of robots in dynamic environments. To this end, we leverage control barrier functions (CBFs) with the forward reachable set to guarantee the safe coordination of the robots while preserving a desired trajectory via a layered controller. The top-level planner generates a safety-ensured trajectory for each agent, accounting for the dynamic constraints in the environment. This planner leverages high-order CBFs based on the forward reachable set to ensure safety-critical coordination control, i.e., guarantee the safe coordination of the robots during locomotion. The middle-level trajectory planner employs single rigid body (SRB) dynamics to generate optimal ground reaction forces (GRFs) to track the safety-ensured trajectories from the top-level planner. The whole-body motions to adhere to the optimal GRFs while ensuring the friction cone condition at the end of each stance leg are generated from the low-level controller. The effectiveness of the approach is demonstrated through simulation and hardware experiments.},
  archive   = {C_ICRA},
  author    = {Jeeseop Kim and Jaemin Lee and Aaron D. Ames},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610589},
  month     = {5},
  pages     = {3478-3484},
  title     = {Safety-critical coordination of legged robots via layered controllers and forward reachable set based control barrier functions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ManyQuadrupeds: Learning a single locomotion policy for
diverse quadruped robots. <em>ICRA</em>, 3471–3477. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning a locomotion policy for quadruped robots has traditionally been constrained to a specific robot morphology, mass, and size. The learning process must usually be repeated for every new robot, where hyperparameters and reward function weights must be re-tuned to maximize performance for each new system. Alternatively, attempting to train a single policy to accommodate different robot sizes, while maintaining the same degrees of freedom (DoF) and morphology, requires either complex learning frameworks, or mass, inertia, and dimension randomization, which leads to prolonged training periods. In our study, we show that drawing inspiration from animal motor control allows us to effectively train a single locomotion policy capable of controlling a diverse range of quadruped robots. The robot differences encompass: a variable number of DoFs, (i.e. 12 or 16 joints), three distinct morphologies, a broad mass range spanning from 2 kg to 200 kg, and nominal standing heights ranging from 18 cm to 100 cm. Our policy modulates a representation of the Central Pattern Generator (CPG) in the spinal cord, effectively coordinating both frequencies and amplitudes of the CPG to produce rhythmic output (Rhythm Generation), which is then mapped to a Pattern Formation (PF) layer. Across different robots, the only varying component is the PF layer, which adjusts the scaling parameters for the stride height and length. Subsequently, we evaluate the sim-to-real transfer by testing the single policy on both the Unitree Go1 and A1 robots. Remarkably, we observe robust performance, even when adding a 15 kg load, equivalent to 125% of the A1 robot’s nominal mass.},
  archive   = {C_ICRA},
  author    = {Milad Shafiee and Guillaume Bellegarda and Auke Ijspeert},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610155},
  month     = {5},
  pages     = {3471-3477},
  title     = {ManyQuadrupeds: Learning a single locomotion policy for diverse quadruped robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SpaceHopper: A small-scale legged robot for exploring
low-gravity celestial bodies. <em>ICRA</em>, 3464–3470. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present SpaceHopper, a three-legged, small-scale robot designed for future mobile exploration of asteroids and moons. The robot weighs 5.2 kg and has a body size of 245 mm while using space-qualifiable components. Furthermore, SpaceHopper’s design and controls make it well-adapted for investigating dynamic locomotion modes with extended flight-phases. Instead of gyroscopes or fly-wheels, the system uses its three legs to reorient the body during flight in preparation for landing. We control the leg motion for reorientation using Deep Reinforcement Learning policies. In a simulation of Ceres’ gravity (0.029 g), the robot can reliably jump to commanded positions up to 6 m away. Our real-world experiments show that SpaceHopper can successfully reorient to a safe landing orientation within 9.7 deg inside a rotational gimbal and jump in a counterweight setup in Earth’s gravity. Overall, we consider SpaceHopper an important step towards controlled jumping locomotion in low-gravity environments.},
  archive   = {C_ICRA},
  author    = {Alexander Spiridonov and Fabio Buehler and Moriz Berclaz and Valerio Schelbert and Jorit Geurts and Elena Krasnova and Emma Steinke and Jonas Toma and Joschua Wuethrich and Recep Polat and Wim Zimmermann and Philip Arm and Nikita Rudin and Hendrik Kolvenbach and Marco Hutter},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610057},
  month     = {5},
  pages     = {3464-3470},
  title     = {SpaceHopper: A small-scale legged robot for exploring low-gravity celestial bodies},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Introducing the carpal-claw: A mechanism to enhance
high-obstacle negotiation for quadruped robots. <em>ICRA</em>,
3457–3463. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The capability of a quadruped robot to negotiate obstacles is tightly connected to its leg workspace and joint torque limits. When facing terrain where the height of obstacles is close to the leg length, the locomotion robustness and safety are reduced since more dynamic motions are required to traverse it. In this paper, we introduce a new mechanism called the Carpal-Claw, which enables quadruped robots to negotiate higher obstacles and adds safety to the locomotion by allowing the robot to negotiate obstacles under static and quasi-static locomotion and regular joint torque demands. The design of the mechanism is detailed, as well as the methodology to exploit the mechanism in the locomotion control framework. The Carpal-Claw functionality is validated through various experiments on a very high obstacle and stairs-like terrains using an Aliengo robot. We demonstrate how Aliengo can safely descend a step height of 40cm, which is 80% of its leg length. To the best knowledge of the authors, this is the first time a mechanism like the C-Claw is proposed for improving quadruped robot locomotion over high obstacles.},
  archive   = {C_ICRA},
  author    = {Victor Barasuol and Sinan Emre and Vivian Suzano Medeiros and Angelo Bratta and Claudio Semini},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611337},
  month     = {5},
  pages     = {3457-3463},
  title     = {Introducing the carpal-claw: A mechanism to enhance high-obstacle negotiation for quadruped robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing dynamic balance in a rat robot via the lateral
flexion of a soft actuated spine. <em>ICRA</em>, 3442–3448. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Balancing oneself using the spine is a physiological alignment of the body posture in the most efficient manner by the muscular forces for mammals. For this reason, we can see many disabled quadruped animals can still stand or walk even with three limbs. This paper investigates the optimization of dynamic balance during trot gait based on the spatial relationship between the center of mass (CoM) and support area influenced by spinal flexion. During trotting, the robot balance is significantly influenced by the distance of the CoM to the support area formed by diagonal footholds. In this context, lateral spinal flexion, which is able to modify the position of footholds, holds promise for optimizing balance during trotting. This paper explores this phenomenon using a rat robot equipped with a soft actuated spine. Based on the lateral flexion of the spine, we establish a kinematic model to quantify the impact of spinal flexion on robot balance during trot gait. Subsequently, we develop an optimized controller for spinal flexion, designed to enhance balance without altering the leg locomotion. The effectiveness of our proposed controller is evaluated through extensive simulations and physical experiments conducted on a rat robot. Compared to both a non-spine based trot gait controller and a trot gait controller with lateral spinal flexion, our proposed optimized controller effectively improves the dynamic balance of the robot and retains the desired locomotion during trotting.},
  archive   = {C_ICRA},
  author    = {Yuhong Huang and Zhenshan Bing and Zitao Zhang and Genghang Zhuang and Kai Huang and Alois Knoll},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611626},
  month     = {5},
  pages     = {3442-3448},
  title     = {Optimizing dynamic balance in a rat robot via the lateral flexion of a soft actuated spine},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning emergent gaits with decentralized phase
oscillators: On the role of observations, rewards, and feedback.
<em>ICRA</em>, 3426–3433. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a minimal phase oscillator model for learning quadrupedal locomotion. Each of the four oscillators is coupled only to itself and its corresponding leg through local feedback of the ground reaction force, which can be interpreted as an observer feedback gain. We interpret the oscillator itself as a latent contact state-estimator. Through a systematic ablation study, we show that the combination of phase observations, simple phase-based rewards, and the local feedback dynamics induces policies that exhibit emergent gait preferences, while using a reduced set of simple rewards, and without prescribing a specific gait. The code is open-source, and a video synopsis available at https://youtu.be/1NKQ0rSV3jU.},
  archive   = {C_ICRA},
  author    = {Jenny Zhang and Steve Heim and Se Hwan Jeon and Sangbae Kim},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611045},
  month     = {5},
  pages     = {3426-3433},
  title     = {Learning emergent gaits with decentralized phase oscillators: On the role of observations, rewards, and feedback},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Terrestrial locomotion of PogoX: From hardware design to
energy shaping and step-to-step dynamics based control. <em>ICRA</em>,
3419–3425. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel controller design on a robotic locomotor that combines an aerial vehicle with a spring-loaded leg. The main motivation is to enable the terrestrial locomotion capability on aerial vehicles so that they can carry heavy loads: heavy enough that flying is no longer possible, e.g., when the thrust-to-weight ratio (TWR) is small. The robot is designed with a pogo-stick leg and a quadrotor, and thus it is named as PogoX. We show that with a simple and lightweight spring-loaded leg, the robot is capable of hopping with TWR &lt;1. The control of hopping is realized via two components: a vertical height control via control Lyapunov function-based energy shaping, and a step-to-step (S2S) dynamics based horizontal velocity control that is inspired by the hopping of the Spring-Loaded Inverted Pendulum (SLIP). The controller is successfully realized on the physical robot, showing dynamic terrestrial locomotion of PogoX which can hop at variable heights and different horizontal velocities with robustness to ground height variations and external pushes.},
  archive   = {C_ICRA},
  author    = {Yi Wang and Jiarong Kang and Zhiheng Chen and Xiaobin Xiong},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611545},
  month     = {5},
  pages     = {3419-3425},
  title     = {Terrestrial locomotion of PogoX: From hardware design to energy shaping and step-to-step dynamics based control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AcTExplore: Active tactile exploration on unknown objects.
<em>ICRA</em>, 3411–3418. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tactile exploration plays a crucial role in understanding object structures for fundamental robotics tasks such as grasping and manipulation. However, efficiently exploring such objects using tactile sensors is challenging, primarily due to the large-scale unknown environments and limited sensing coverage of these sensors. To this end, we present AcTExplore, an active tactile exploration method driven by reinforcement learning for object reconstruction at scales that automatically explores the object surfaces in a limited number of steps. Through sufficient exploration, our algorithm incrementally collects tactile data and reconstructs 3D shapes of the objects as well, which can serve as a representation for higher-level downstream tasks. Our method achieves an average of 95.97% IoU coverage on unseen YCB objects while just being trained on primitive shapes.},
  archive   = {C_ICRA},
  author    = {Amir-Hossein Shahidzadeh and Seong Jong Yoo and Pavan Mantripragada and Chahat Deep Singh and Cornelia Fermüller and Yiannis Aloimonos},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611667},
  month     = {5},
  pages     = {3411-3418},
  title     = {AcTExplore: Active tactile exploration on unknown objects},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An investigation of multi-feature extraction and
super-resolution with fast microphone arrays. <em>ICRA</em>, 3388–3394.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we use MEMS microphones as vibration sensors to simultaneously classify texture and estimate contact position and velocity. Vibration sensors are an important facet of both human and robotic tactile sensing, providing fast detection of contact and onset of slip. Microphones are an attractive option for implementing vibration sensing as they offer a fast response and can be sampled quickly, are affordable, and occupy a very small footprint. Our prototype sensor uses only a sparse array (8-9 mm spacing) of distributed MEMS microphones (&lt;$1, 3.76×2.95×1.10 mm) embedded under an elastomer. We use transformer-based architectures for data analysis, taking advantage of the microphones’ high sampling rate to run our models on time-series data as opposed to individual snapshots. This approach allows us to obtain 77.3% average accuracy on 4-class texture classification (84.2% when excluding the slowest drag velocity), 1.8 mm mean error on contact localization, and 5.6 mm/s mean error on contact velocity. We show that the learned texture and localization models are robust to varying velocity and generalize to unseen velocities. We also report that our sensor provides fast contact detection, an important advantage of fast transducers. This investigation illustrates the capabilities one can achieve with a MEMS microphone array alone, leaving valuable sensor real estate available for integration with complementary tactile sensing modalities.},
  archive   = {C_ICRA},
  author    = {Eric T. Chang and Runsheng Wang and Peter Ballentine and Jingxi Xu and Trey Smith and Brian Coltin and Ioannis Kymissis and Matei Ciocarlie},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611599},
  month     = {5},
  pages     = {3388-3394},
  title     = {An investigation of multi-feature extraction and super-resolution with fast microphone arrays},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tactile embeddings for multi-task learning. <em>ICRA</em>,
3348–3355. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tactile sensing plays a pivotal role in human perception and manipulation tasks, allowing us to intuitively understand task dynamics and adapt our actions in real time. Transferring such tactile intelligence to robotic systems would help intelligent agents understand task constraints and accurately interpret the dynamics of both the objects they are interacting with and their own operations. While significant progress has been made in imbuing robots with this tactile intelligence, challenges persist in effectively utilizing tactile information due to the diversity of tactile sensor form factors, manipulation tasks, and learning objectives involved. To address this challenge, we present a unified tactile embedding space capable of predicting a variety of task-centric qualities over multiple manipulation tasks. We collect tactile data from human demonstrations across various tasks and leverage this data to construct a shared latent space for task stage classification, object dynamics estimation, and tactile dynamics prediction. Through experiments and ablation studies, we demonstrate the effectiveness of our shared tactile latent space for more accurate and adaptable tactile networks, showing an improvement of up to 84% over the single-task training.},
  archive   = {C_ICRA},
  author    = {Yiyue Luo and Murphy Wonsick and Jessica Hodgins and Brian Okorn},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611419},
  month     = {5},
  pages     = {3348-3355},
  title     = {Tactile embeddings for multi-task learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advancing virtual reality interaction: A ring-shaped
controller and pose tracking. <em>ICRA</em>, 3341–3347. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ensuring robust tracking of controllers’ movement is critical for human-robot interaction in virtual reality (VR) scenarios. This paper proposes a robust tracking algorithm based on a novel wearable ring-shaped controller equipped with an inertial measurement unit (IMU) and a light-emitting diode (LED). This novel controller design allows users to free up their hands for more immersive experiences. To track the controller’s motion accurately and robustly, we resort to various forms of visual measurements, including 6 DoF and 5 DoF pose measurements from hand gesture detection, as well as 3 DoF position measurement and 2 DoF image measurement derived from the LED. We theoretically analyze the performances of these observation models and propose an optimal observation model combination scheme. Moreover, the necessity and rationale of online estimating system gravity are illustrated. The effectiveness of our tracking method is validated through extensive experiments.},
  archive   = {C_ICRA},
  author    = {Zhuqing Zhang and Dongxuan Li and Jiayao Ma and Yijia He and Pan Ji and Rong Xiong and Hongdong Li and Yue Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610313},
  month     = {5},
  pages     = {3341-3347},
  title     = {Advancing virtual reality interaction: A ring-shaped controller and pose tracking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assessment and benchmarking of XoNLI: A natural language
processing interface for industrial exoskeletons. <em>ICRA</em>,
3333–3340. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Industrial exoskeletons are a potential solution for reducing work-related musculoskeletal disorders during carrying or lifting tasks. Having sensors, electrical/pneumatic actuators, and control systems, active exoskeletons present a more versatile control system because it is possible to select different assistive strategies based on the performed task. From this perspective, human-machine interaction is required to safely open basic exoskeleton domains to the user and provide an adaptable setup system. This article presents the assessment and benchmarking of the novel XoLab Natural Language Interface, a voice user interface for interaction and configuration of industrial active exoskeletons. The evaluation of the novel interface was performed by 17 participants who completed the setup and operational activities while wearing the XoTrunk exoskeleton. The benchmark consisted of a comparison of the presented device with previous adaptable interfaces for the exoskeleton: the user command interface and the monitor system interface. The results showed that although the novel interface demonstrated a considerable lag in the time response, it was more attractive, stimulating and novel than the standard one. However, the standard interface obtained favourable results over the user command interface and the voice interface perspicuity and efficiency.},
  archive   = {C_ICRA},
  author    = {Olmo A. Moreno F. and Raajshekhar Parameswari and Christian Di Natali and Darwin G. Caldwell and Jesus Ortiz},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610451},
  month     = {5},
  pages     = {3333-3340},
  title     = {Assessment and benchmarking of XoNLI: A natural language processing interface for industrial exoskeletons},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assisting group discussions using desktop robot haru.
<em>ICRA</em>, 3326–3332. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Socially assistive robots are potentially to be integrated with human daily lives in the near future, and expected to be able to improve group dynamics when interacting with groups of people in social settings. In this paper, we developed a system with desktop robot Haru to assist group discussions. The system consists of three modules: a dialogue assistance module which facilitates Haru to speak to users and answer questions in a free way; a dialogue balance module to encourage participation of users in the discussion with verbal behaviors; an autonomous gazing behavior module trained via deep reinforcement learning in simulation and deployed on physical Haru in reality, which can show politeness during group discussion, e.g., gazing to the speaking member, looking to the middle when both members are talking or silent, looking at the least spoken person when encouraging her. Results of user study with 40 subjects show the significant effectiveness of our system in assisting group discussion.},
  archive   = {C_ICRA},
  author    = {Fei Tang and Chuanxiong Zheng and Hongqi Yu and Lei Zhang and Eric Nichols and Randy Gomez and Guangliang Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611098},
  month     = {5},
  pages     = {3326-3332},
  title     = {Assisting group discussions using desktop robot haru},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing tactile sensing in robotics: Dual-modal force and
shape perception with EIT-based sensors and MM-CNN. <em>ICRA</em>,
3311–3317. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Electrical Impedance Tomography (EIT)-based tactile sensors offer durability, scalability, and cost-effective manufacturing. However, simultaneously reconstructing force and shape from boundary measurements remains challenging due to EIT’s inherent location dependencies and image artifacts. This study presents a model-driven multimodal convolutional neural network (MM-CNN) for joint EIT-based force and shape sensing. The hybrid approach combines physics-inspired voltage preprocessing with an attention-based network to overcome EIT’s limitations. The preprocessing network applies a linearized one-step inverse solution with Tikhonov regularization to convert raw boundary voltage into a noise-reduced 2D image. The image reconstruction network uses an attention mechanism to focus on salient features, addressing location dependency issues. Quantitative metrics show that MM-CNN outperforms traditional EIT algorithms like NOSER and TV, reducing location dependency and improving shape discrimination. MM-CNN enables unified force and shape modalities, validated through real-contact experiments, enhancing EIT tactile systems for human-robot interaction by incorporating physical knowledge with deep learning.},
  archive   = {C_ICRA},
  author    = {Haofeng Chen and Xuanxuan Yang and Gang Ma and Yucheng Wang and Xiaojie Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610215},
  month     = {5},
  pages     = {3311-3317},
  title     = {Enhancing tactile sensing in robotics: Dual-modal force and shape perception with EIT-based sensors and MM-CNN},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PROGrasp: Pragmatic human-robot communication for object
grasping. <em>ICRA</em>, 3304–3310. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Interactive Object Grasping (IOG) is the task of identifying and grasping the desired object via human-robot natural language interaction. Current IOG systems assume that a human user initially specifies the target object’s category (e.g., bottle). Inspired by pragmatics, where humans often convey their intentions by relying on context to achieve goals, we introduce a new IOG task, Pragmatic-IOG, and the corresponding dataset, Intention-oriented Multi-modal Dialogue (IM-Dial). In our proposed task scenario, an intention-oriented utterance (e.g., &quot;I am thirsty&quot;) is initially given to the robot. The robot should then identify the target object by interacting with a human user. Based on the task setup, we propose a new robotic system that can interpret the user’s intention and pick up the target object, Pragmatic Object Grasping (PROGrasp). PROGrasp performs Pragmatic-IOG by incorporating modules for visual grounding, question asking, object grasping, and most importantly, answer interpretation for pragmatic inference. Experimental results show that PROGrasp is effective in offline (i.e., target object discovery) and online (i.e., IOG with a physical robot arm) settings. Code and data are available at https://github.com/gicheonkang/prograsp.},
  archive   = {C_ICRA},
  author    = {Gi-Cheon Kang and Junghyun Kim and Jaein Kim and Byoung-Tak Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610543},
  month     = {5},
  pages     = {3304-3310},
  title     = {PROGrasp: Pragmatic human-robot communication for object grasping},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Think, act, and ask: Open-world interactive personalized
robot navigation. <em>ICRA</em>, 3296–3303. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Zero-Shot Object Navigation (ZSON) enables agents to navigate towards open-vocabulary objects in unknown environments. The existing works of ZSON mainly focus on following individual instructions to find generic object classes, neglecting the utilization of natural language interaction and the complexities of identifying user-specific objects. To address these limitations, we introduce Zero-shot Interactive Personalized Object Navigation (ZIPON), where robots need to navigate to personalized goal objects while engaging in conversations with users. To solve ZIPON, we propose a new framework termed Open-woRld Interactive persOnalized Navigation (ORION) 1 , which uses Large Language Models (LLMs) to make sequential decisions to manipulate different modules for perception, navigation and communication. Experimental results show that the performance of interactive agents that can leverage user feedback exhibits significant improvement. However, obtaining a good balance between task completion and the efficiency of navigation and interaction remains challenging for all methods. We further provide more findings on the impact of diverse user feedback forms on the agents’ performance.},
  archive   = {C_ICRA},
  author    = {Yinpei Dai and Run Peng and Sikai Li and Joyce Chai},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610178},
  month     = {5},
  pages     = {3296-3303},
  title     = {Think, act, and ask: Open-world interactive personalized robot navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards unified interactive visual grounding in the wild.
<em>ICRA</em>, 3288–3295. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Interactive visual grounding in Human-Robot Interaction (HRI) is challenging yet practical due to the inevitable ambiguity in natural languages. It requires robots to disambiguate the user’s input by active information gathering. Previous approaches often rely on predefined templates to ask disambiguation questions, resulting in performance reduction in realistic interactive scenarios. In this paper, we propose TiO, an end-to-end system for interactive visual grounding in human-robot interaction. Benefiting from a unified formulation of visual dialog and grounding, our method can be trained on a joint of extensive public data, and show superior generality to diversified and challenging open-world scenarios. In the experiments, we validate TiO on GuessWhat?! and InViG benchmarks, setting new state-of-the-art performance by a clear margin. Moreover, we conduct HRI experiments on the carefully selected 150 challenging scenes as well as real-robot platforms. Results show that our method demonstrates superior generality to diversified visual and language inputs with a high success rate. Codes and demos are available on https://jxu124.github.io/TiO/.},
  archive   = {C_ICRA},
  author    = {Jie Xu and Hanbo Zhang and Qingyi Si and Yifeng Li and Xuguang Lan and Tao Kong},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611354},
  month     = {5},
  pages     = {3288-3295},
  title     = {Towards unified interactive visual grounding in the wild},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Task-space control of a powered ankle prosthesis.
<em>ICRA</em>, 3262–3268. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Powered lower-limb prostheses have shown promise in helping individuals with amputation regain functionality that passive prostheses cannot provide. However, the best method for controlling these devices in coordination with their users is still an open research topic. While powered devices can replicate normative joint kinematics and kinetics, active control also holds the potential to shape system-level characteristics such as the center of mass (CoM) that play an important role in balance. Controlling the prosthesis based on these system-level, or task-space, variables would further represent a new way of coordinating the user and their device.This paper explores the initial implementation of task-space control for a powered ankle prosthesis, characterizing the emergent outcomes of this new coordination strategy. One able-bodied subject walked using a bypass adapter while prosthesis torques were commanded based on reference ground reaction force (GRF) and CoM trajectories. The subject could walk comfortably and continuously at their preferred walking speed, achieving normative ankle torques and joint trajectories despite not tracking explicit joint-level references in stance.},
  archive   = {C_ICRA},
  author    = {David J. Kelly and Ryan R. Posh and Patrick M. Wensing},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611270},
  month     = {5},
  pages     = {3262-3268},
  title     = {Task-space control of a powered ankle prosthesis},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pilot comparison of customized and generalized
hip-knee-ankle exoskeleton torque profiles. <em>ICRA</em>, 3256–3261.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Optimized assistance patterns have produced the greatest exoskeleton benefits to energy expenditure of any strategy to date. This strategy may be effective due to the customization of the applied torque profiles to the user as well as the locomotion condition; however, it is currently unclear how sensitive participants are to their unique torque profile. To investigate, we applied previously optimized hip-knee-ankle torque profiles to expert users (N=3; 1.25 m/s; 0 deg incline). The participants walked with the profile optimized to them, the two profiles optimized to the other two participants, and the average of the three torque profiles while we measured their energy expenditure. Relative to walking with the device turned off, on average, participants experienced a 47.5% (range 12%) metabolic reduction when walking with the torque profile optimized to them and a 46% (range 15%) reduction when walking with the other profiles. Interestingly, within-subject performance was more consistent than across subjects (P1: 52% range 5%, P2: 49% range 6%, P3: 39% range 3%) suggesting that, for expert users of some devices, there may be a range of nearly equally effective torque profiles to reduce the metabolic cost of walking. The torque timing was remarkably similar across the four torque profiles while the torque magnitude varied; participants may be much more sensitive to torque timing than torque magnitude, and there may be a set of torque timing parameters that are generally effective.},
  archive   = {C_ICRA},
  author    = {Gwendolyn M. Bryan and Patrick W. Franks and Seungmoon Song and Steven H. Collins},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611676},
  month     = {5},
  pages     = {3256-3261},
  title     = {Pilot comparison of customized and generalized hip-knee-ankle exoskeleton torque profiles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ExoRecovery: Push recovery with a lower-limb exoskeleton
based on stepping strategy. <em>ICRA</em>, 3248–3255. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Balance loss is a significant challenge in lower-limb exoskeleton applications, as it can lead to potential falls, thereby impacting user safety and confidence. We introduce a control framework for omnidirectional recovery step planning by online optimization of step duration and position in response to external forces. We map the step duration and position to a human-like foot trajectory, which is then translated into joint trajectories using inverse kinematics. These trajectories are executed via an impedance controller, promoting cooperation between the exoskeleton and the user. Moreover, our framework is based on the concept of the divergent component of motion, also known as the Extrapolated Center of Mass, which has been established as a consistent dynamic for describing human movement. This real-time online optimization framework enhances the adaptability of exoskeleton users under unforeseen forces thereby improving the overall user stability and safety. To validate the effectiveness of our approach, simulations, and experiments were conducted. Our push recovery experiments employing the exoskeleton in zero-torque mode (without assistance) exhibit an alignment with the exoskeleton’s recovery assistance mode, that shows the consistency of the control framework with human intention. To the best of our knowledge, this is the first cooperative push recovery framework for the lower-limb human exoskeleton that relies on the simultaneous adaptation of intra-stride parameters in both frontal and sagittal directions. The proposed control scheme has been validated with human subject experiments.},
  archive   = {C_ICRA},
  author    = {Zeynep Özge Orhan and Milad Shafiee and Vincent Juillard and Joel Coelho Oliveira and Auke Ijspeert and Mohamed Bouri},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610027},
  month     = {5},
  pages     = {3248-3255},
  title     = {ExoRecovery: Push recovery with a lower-limb exoskeleton based on stepping strategy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time locomotion transitions detection: Maximizing
performances with minimal resources. <em>ICRA</em>, 3241–3247. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Assistive devices, such as exoskeletons and prostheses, have revolutionized the field of rehabilitation and mobility assistance. Efficiently detecting transitions between different activities, such as walking, stair ascending and descending, and sitting, is crucial for ensuring adaptive control and enhancing user experience. We present an approach for real-time transition detection, aimed at optimizing the processing-time performance. By establishing activity-specific threshold values through trained machine learning models, we effectively distinguish motion patterns and we identify transition moments between locomotion modes. This threshold-based method improves real-time embedded processing time performance by up to 11 times compared to machine learning approaches. The efficacy of the developed finite-state machine is validated using data collected from three different measurement systems. Moreover, experiments with healthy participants were conducted on an active pelvis orthosis to validate the robustness and reliability of our approach. The proposed algorithm achieved high accuracy in detecting transitions between activities. These promising results show the robustness and reliability of the method, reinforcing its potential for integration into practical applications.},
  archive   = {C_ICRA},
  author    = {Zeynep Özge Orhan and Andrea Dal Prete and Anastasia Bolotnikova and Marta Gandolla and Auke Ijspeert and Mohamed Bouri},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611651},
  month     = {5},
  pages     = {3241-3247},
  title     = {Real-time locomotion transitions detection: Maximizing performances with minimal resources},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design of a front-enveloping powered exoskeleton considering
optimal distribution of actuating torques and center of mass.
<em>ICRA</em>, 3234–3240. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traditionally, powered exoskeletons have predominantly featured a back-enveloping design due to its simplicity in both implementation and user donning. However, this design results in a backward shift of the center of mass (CoM) in the sagittal plane. This paper identifies the limitations of existing design approaches and determines the optimal anterior-posterior (A/P) CoM position considering factors like actuating power, balance in the neutral posture, and user’s hand workspace. Our optimization analysis recommends placing the CoM in front of the user. We address historical constraints on front-enveloping designs and propose solutions. Furthermore, we validate the usability of our designed exoskeleton through testing with a complete paraplegic user.},
  archive   = {C_ICRA},
  author    = {Jeongsu Park and Kyeongsu Shi and Gunhee Lee and Hyojun An and Seunghwan Kim and Chanyoung Ko and Taeyeon Kim and Hyeongjun Kim and Kyoungchul Kong},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610165},
  month     = {5},
  pages     = {3234-3240},
  title     = {Design of a front-enveloping powered exoskeleton considering optimal distribution of actuating torques and center of mass},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ankle exoskeleton with a symmetric 3 DoF structure for
plantarflexion assistance. <em>ICRA</em>, 3227–3233. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ankle exoskeletons can assist the ankle joint and reduce the metabolic cost of walking. However, many existing ankle exoskeletons constrain the natural 3 degrees of freedom (DoF) of the ankle to limit the exoskeleton’s weight and mechanical complexity, thereby compromising comfort and kinematic compatibility with the user.This paper presents a novel ankle exoskeleton frame design that allows for 3 DoF ankle motion using a symmetric parallel frame design principle resulting in a strong frame while weighing 1.8 kg. Furthermore, a cable routing method is proposed to actuate the plantarflexion of the ankle. The kinematic compatibility of the proposed exoskeleton frame is evaluated in straight- and curve-walking scenarios with four users. The study demonstrates that the exoskeleton frame adapts to the natural 3 DoF ankle motion and the range of motion (RoM) during walking. The actuation in plantarflexion is evaluated in a stationary torque experiment demonstrating the ability of the frame to transfer large torque loads of up to 57.4 Nm. This work contributes to the design and development of more flexible and adaptable ankle exoskeletons for walking assistance.},
  archive   = {C_ICRA},
  author    = {Miha Dežman and Charlotte Marquardt and Tamim Asfour},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609991},
  month     = {5},
  pages     = {3227-3233},
  title     = {Ankle exoskeleton with a symmetric 3 DoF structure for plantarflexion assistance},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A personalizable controller for the walking assistive
omNi-directional exo-robot (WANDER). <em>ICRA</em>, 3212–3218. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Preserving and encouraging mobility in the elderly and adults with chronic conditions is of paramount importance. However, existing walking aids are either inadequate to provide sufficient support to users’ stability or too bulky and poorly maneuverable to be used outside hospital environments. In addition, they all lack adaptability to individual requirements. To address these challenges, this paper introduces WANDER, a novel Walking Assistive omNi-Directional Exo-Robot. It consists of an omnidirectional platform and a robust aluminum structure mounted on top of it, which provides partial body weight support. A comfortable and minimally restrictive coupling interface embedded with a force/torque sensor allows to detect users’ intentions, which are translated into command velocities by means of a variable admittance controller. An optimization technique based on users’ preferences, i.e., Preference-Based Optimization (PBO) guides the choice of the admittance parameters (i.e., virtual mass and damping) to better fit subject-specific needs and characteristics. Experiments with twelve healthy subjects exhibited a significant decrease in energy consumption and jerk when using WANDER with PBO parameters as well as improved user performance and comfort. The great interpersonal variability in the optimized parameters highlights the importance of personalized control settings when walking with an assistive device, aiming to enhance users’ comfort and mobility while ensuring reliable physical support.},
  archive   = {C_ICRA},
  author    = {Andrea Fortuna and Marta Lorenzini and Mattia Leonori and Juan M. Gandarias and Pietro Balatti and Younggeol Cho and Elena De Momi and Arash Ajoudani},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611368},
  month     = {5},
  pages     = {3212-3218},
  title     = {A personalizable controller for the walking assistive omNi-directional exo-robot (WANDER)},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring the effect of base compliance on physical
human-robot collaboration. <em>ICRA</em>, 3198–3204. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile physical human-robot collaboration (pHRC) using collaborative robots (cobots) and mobile robots has attracted much research attention. Many researchers have focused on improving the control performance to comply with human intentions. However, a problem that generally exists with mobile pHRC but often gets neglected is the impact of non-rigid components e.g. deformable tyres, suspension systems and uneven terrain on human interaction experience and task performance. To fullfil this current research gap, we carried out an investigation on the above-mentioned problem by altering a cobot’s base rigidity level (also referred to as base compliance level or BCL) during pHRC experiments. We explored how the task performance is affected by base compliance as well as human operator’s experience and cobot control parameters. Measurements include the human operator’s physical effort, task velocity, and task error. From the experimental results, it is discovered that base compliance has a significant impact on task accuracy as it can easily excite the system if an inadequate control strategy is deployed. Furthermore, through ANOVA, it is discovered that the influence of base compliance can be minimized and system excitation can be avoided by sufficient human operator training and the appropriate selection of cobot’s control parameters.},
  archive   = {C_ICRA},
  author    = {Ziqi Wang and Marc G Carmichael},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611559},
  month     = {5},
  pages     = {3198-3204},
  title     = {Exploring the effect of base compliance on physical human-robot collaboration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring transformers and visual transformers for force
prediction in human-robot collaborative transportation tasks.
<em>ICRA</em>, 3191–3197. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we analyze the possibilities offered by Deep Learning State-of-the-Art architectures such as Transformers and Visual Transformers in generating a prediction of the human’s force in a Human-Robot collaborative object transportation task at a middle distance. We outperform our previous predictor by achieving a success rate of 93.8% in testset and 90.9% in real experiments with 21 volunteers predicting in both cases the force that the human will exert during the next 1 s. A modification in the architecture allows us to obtain a second output from the model with a velocity prediction, which allows us to improve the capabilities of our predictor if it is used to estimate the trajectory that the human-robot pair will follow. An ablation test is also performed to verify the relative contribution to performance of each input.},
  archive   = {C_ICRA},
  author    = {J. E. Domínguez-Vidal and Alberto Sanfeliu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611205},
  month     = {5},
  pages     = {3191-3197},
  title     = {Exploring transformers and visual transformers for force prediction in human-robot collaborative transportation tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Proactive robot control for collaborative manipulation using
human intent. <em>ICRA</em>, 3176–3182. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collaborative manipulation task often requires negotiation using explicit or implicit communication. An important example is determining where to move when the goal destination is not uniquely specified, and who should lead the motion. This work is motivated by the ability of humans to communicate the desired destination of motion through back-and-forth force exchanges. Inherent to these exchanges is also the ability to dynamically assign a role to each participant, either taking the initiative or deferring to the partner’s lead. In this paper, we propose a hierarchical robot control framework that emulates human behavior in communicating a motion destination to a human collaborator and in responding to their actions. At the top level, the controller consists of a set of finite-state machines corresponding to different levels of commitment of the robot to its desired goal configuration. The control architecture is loosely based on the human strategy observed in the human-human experiments, and the key component is a real-time intent recognizer that helps the robot respond to human actions. We describe the details of the control framework, feature engineering and training process of the intent recognition. The proposed controller was implemented on a UR10e robot (Universal Robots) and evaluated through human studies. The experiments show that the robot correctly recognizes and responds to human input, communicates its intent clearly, and resolves conflict. We report success rates and draw comparisons with human-human experiments to demonstrate the effectiveness of the approach.},
  archive   = {C_ICRA},
  author    = {Zhanibek Rysbek and Siyu Li and Afagh Mehri Shervedani and Miloš Žefran},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610909},
  month     = {5},
  pages     = {3176-3182},
  title     = {Proactive robot control for collaborative manipulation using human intent},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SynH2R: Synthesizing hand-object motions for learning
human-to-robot handovers. <em>ICRA</em>, 3168–3175. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision-based human-to-robot handover is an important and challenging task in human-robot interaction. Recent work has attempted to train robot policies by interacting with dynamic virtual humans in simulated environments, where the policies can later be transferred to the real world. However, a major bottleneck is the reliance on human motion capture data, which is expensive to acquire and difficult to scale to arbitrary objects and human grasping motions. In this paper, we introduce a framework that can generate plausible human grasping motions suitable for training the robot. To achieve this, we propose a hand-object synthesis method that is designed to generate handover-friendly motions similar to humans. This allows us to generate synthetic training and testing data with 100x more objects than previous work. In our experiments, we show that our method trained purely with synthetic data is competitive with state-of-the-art methods that rely on real human motion data both in simulation and on a real system. In addition, we can perform evaluations on a larger scale compared to prior work. With our newly introduced test set, we show that our model can better scale to a large variety of unseen objects and human motions compared to the baselines.},
  archive   = {C_ICRA},
  author    = {Sammy Christen and Lan Feng and Wei Yang and Yu-Wei Chao and Otmar Hilliges and Jie Song},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610694},
  month     = {5},
  pages     = {3168-3175},
  title     = {SynH2R: Synthesizing hand-object motions for learning human-to-robot handovers},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transformer-based prediction of human motions and contact
forces for physical human-robot interaction. <em>ICRA</em>, 3161–3167.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a transformer-based architecture for predicting contact forces during a physical human-robot interaction. Our Neural Network is composed of two main parts: a Multi-Layer Perceptron called Transducer and a Transformer. The former estimates, based on the kinematic data from a motion capture suit, the current contact forces. The latter predicts – taking as input the same kinematic data and the output of the Transducer – the human motions and the contact forces over a time window in the future. We validated our approach by testing the network on directions of motions that were not provided in the training set. We also compared our approach to a purely Transformer-based network, showing a better prediction accuracy of the contact forces.},
  archive   = {C_ICRA},
  author    = {Alessia Fusco and Valerio Modugno and Dimitrios Kanoulas and Alessandro Rizzo and Marco Cognetti},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611211},
  month     = {5},
  pages     = {3161-3167},
  title     = {Transformer-based prediction of human motions and contact forces for physical human-robot interaction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decomposing the generalization gap in imitation learning for
visual robotic manipulation. <em>ICRA</em>, 3153–3160. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {What makes generalization hard for imitation learning in visual robotic manipulation? This question is difficult to approach at face value, but the environment from the perspective of a robot can often be decomposed into enumerable factors of variation, such as the lighting conditions or the placement of the camera. Empirically, generalization to some of these factors have presented a greater obstacle than others, but existing work sheds little light on precisely how much each factor contributes to the generalization gap. Towards an answer to this question, we study imitation learning policies in simulation and on a real robot language-conditioned manipulation task to quantify the difficulty of generalization to different (sets of) factors. We design a simulated benchmark of 19 tasks with 11 factors of variation to facilitate more controlled evaluations of generalization. From our study, we determine an ordering of factors based on generalization difficulty, that is consistent across simulation and our real robot setup. 1},
  archive   = {C_ICRA},
  author    = {Annie Xie and Lisa Lee and Ted Xiao and Chelsea Finn},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611331},
  month     = {5},
  pages     = {3153-3160},
  title     = {Decomposing the generalization gap in imitation learning for visual robotic manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DefGoalNet: Contextual goal learning from demonstrations for
deformable object manipulation. <em>ICRA</em>, 3145–3152. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Shape servoing, a robotic task dedicated to controlling objects to desired goal shapes, is a promising approach to deformable object manipulation. An issue arises, however, with the reliance on the specification of a goal shape. This goal has been obtained either by a laborious domain knowledge engineering process or by manually manipulating the object into the desired shape and capturing the goal shape at that specific moment, both of which are impractical in various robotic applications. In this paper, we solve this problem by developing a novel neural network DefGoalNet, which learns deformable object goal shapes directly from a small number of human demonstrations. We demonstrate our method’s effectiveness on various robotic tasks, both in simulation and on a physical robot. Notably, in the surgical retraction task, even when trained with as few as 10 demonstrations, our method achieves a median success percentage of nearly 90%. These results mark a substantial advancement in enabling shape servoing methods to bring deformable object manipulation closer to practical real-world applications.},
  archive   = {C_ICRA},
  author    = {Bao Thach and Tanner Watts and Shing-Hei Ho and Tucker Hermans and Alan Kuntz},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610109},
  month     = {5},
  pages     = {3145-3152},
  title     = {DefGoalNet: Contextual goal learning from demonstrations for deformable object manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual-critic deep reinforcement learning for push-grasping
synergy in cluttered environment. <em>ICRA</em>, 3138–3144. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic push-grasping in densely cluttered environments presents significant challenges due to unbalanced synergy and redundancy between both actions, leading to decreased grasp efficiency. In this paper, a novel double-critic deep reinforcement learning framework is introduced to optimize the push-grasping synergy for robotic manipulation in such environments, aiming to significantly reduce pre-grasping redundancy. This framework incorporates two distinct Deep Q-learning critics: Critic I selects the best course of actions based on the current state derived from visual interpretation, whereas Critic II evaluates the success rate of the current state-action pairing. To further refine the push-grasping synergy, an active double-step learning mechanism is introduced to optimize the training reward function for the pushing action, thereby enhancing its effectiveness through increased intentionality. Simulations show that the proposed framework outperforms contemporary counterparts, notably in grasping success rate and action efficiency. Finally, the framework’s generalization and adaptability are demonstrated by conducting real-world experiments using novel objects without the need of retraining.},
  archive   = {C_ICRA},
  author    = {Jiakang Zhong and Yew Wee Wong and Jiong Jin and Yong Song and Xianfeng Yuan and Xiaoqi Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610121},
  month     = {5},
  pages     = {3138-3144},
  title     = {Dual-critic deep reinforcement learning for push-grasping synergy in cluttered environment},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HAGrasp: Hybrid action grasp control in cluttered scenes
using deep reinforcement learning. <em>ICRA</em>, 3131–3137. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic autonomous grasp requires the system to perform multiple functions such as gripper and robot control, making it a task with hybrid output nature. Existing methods based on closed-loop deep reinforcement learning rely on external models for termination evaluation. To achieve more effective grasp for novel objects, we propose a new autonomous grasp control scheme termed HAGrasp that considers the complete point cloud of the workspace. It integrates grasp pose estimation, end-effector pose evaluation, and motion planning of the robotic arm into a single model, enhancing the success rate while reducing computational load. We present a closed-loop grasp control system based on deep reinforcement learning. This control system can perform grasp tasks while dynamically adjusting to avoid end-effector collisions. The design of hybrid-action reinforcement learning module is trained with unified latent action space and further improve generalization, achieving real-time autonomous grasp control. Real robot experiments show that our method has 74.2% success rate for grasping 7 unseen objects. Comparative experiments show that the proposed HAGrasp outperforms open-loop baseline Contact-Graspnet in both success rate and inference time. It is demonstrated that with integrated multi-view input and sim-to-real training design, our method improves real-world applications of autonomous grasp.},
  archive   = {C_ICRA},
  author    = {Kai-Tai Song and Hsiang-Hsi Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610852},
  month     = {5},
  pages     = {3131-3137},
  title     = {HAGrasp: Hybrid action grasp control in cluttered scenes using deep reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning fabric manipulation in the real world with human
videos. <em>ICRA</em>, 3124–3130. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fabric manipulation is a long-standing challenge in robotics due to the enormous state space and complex dynamics. Learning approaches stand out as promising for this domain as they allow us to learn behaviours directly from data. Most prior methods however rely heavily on simulation, which is still limited by the large sim-to-real gap of deformable objects or rely on large datasets. A promising alternative is to learn fabric manipulation directly from watching humans perform the task. In this work, we explore how demonstrations for fabric manipulation tasks can be collected directly by humans, providing an extremely natural and fast data collection pipeline. Then, using only a handful of such demonstrations, we show how a pick-and-place policy can be learned and deployed on a real robot, without any robot data collection at all. We demonstrate our approach on a fabric smoothing and folding task, showing that our policy can reliably reach folded states from crumpled initial configurations. Code, video and data are available on the project website: https://sites.google.com/view/foldingbyhand},
  archive   = {C_ICRA},
  author    = {Robert Lee and Jad Abou-Chakra and Fangyi Zhang and Peter Corke},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610062},
  month     = {5},
  pages     = {3124-3130},
  title     = {Learning fabric manipulation in the real world with human videos},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Out of sight, still in mind: Reasoning and planning about
unobserved objects with video tracking enabled memory models.
<em>ICRA</em>, 3108–3115. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots need to have a memory of previously observed, but currently occluded objects to work reliably in realistic environments. We investigate the problem of encoding object-oriented memory into a multi-object manipulation reasoning and planning framework. We propose DOOM and LOOM, which leverage transformer relational dynamics to encode the history of trajectories given partial-view point clouds and an object discovery and tracking engine. Our approaches can perform multiple challenging tasks including reasoning with occluded objects, novel objects appearance, and object reappearance. Throughout our extensive simulation and real-world experiments, we find that our approaches perform well in terms of different numbers of objects and different numbers of distractor actions. Furthermore, we show our approaches outperform an implicit memory baseline.},
  archive   = {C_ICRA},
  author    = {Yixuan Huang and Jialin Yuan and Chanho Kim and Pupul Pradhan and Bryan Chen and Li Fuxin and Tucker Hermans},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610240},
  month     = {5},
  pages     = {3108-3115},
  title     = {Out of sight, still in mind: Reasoning and planning about unobserved objects with video tracking enabled memory models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Symmetric models for visual force policy learning.
<em>ICRA</em>, 3101–3107. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While it is generally acknowledged that force feedback is beneficial to robotic control, applications of policy learning to robotic manipulation typically only leverage visual feedback. Recently, symmetric neural models have been used to significantly improve the sample efficiency and performance of policy learning across a variety of robotic manipulation domains. This paper explores an application of symmetric policy learning to visual-force problems. We present Symmetric Visual Force Learning (SVFL), a novel method for robotic control which leverages visual and force feedback. We demonstrate that SVFL can significantly outperform state of the art baselines for visual force learning and report several interesting empirical findings related to the utility of learning force feedback control policies in both general manipulation tasks and scenarios with low visual acuity.},
  archive   = {C_ICRA},
  author    = {Colin Kohler and Anuj Shrivatsav Srikanth and Eshan Arora and Robert Platt},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610728},
  month     = {5},
  pages     = {3101-3107},
  title     = {Symmetric models for visual force policy learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Utilizing inpainting for training keypoint detection
algorithms towards markerless visual servoing. <em>ICRA</em>, 3086–3092.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel strategy to train keypoint detection models for robotics applications. Our goal is to develop methods that can robustly detect and track natural features on robotic manipulators. Such features can be used for vision-based control and pose estimation purposes, when placing artificial markers (e.g. ArUco) on the robot’s body is not possible or practical in runtime. Prior methods require accurate camera calibration and robot kinematic models in order to label training images for the keypoint locations. In this paper, we remove these dependencies by utilizing inpainting methods: In the training phase, we attach ArUco markers along the robot’s body and then label the keypoint locations as the center of those markers. We, then, use an inpainting method to reconstruct the parts of the robot occluded by the ArUco markers. As such, the markers are artificially removed from the training images, and labeled data is obtained to train markerless keypoint detection algorithms without the need for camera calibration or robot models. Using this approach, we trained a model for realtime keypoint detection and used the inferred keypoints as control features for an adaptive visual servoing scheme. We obtained successful control results with this fully model-free control strategy, utilizing natural robot features in the runtime and not requiring camera calibration or robot models in any stage of this process.},
  archive   = {C_ICRA},
  author    = {Sreejani Chatterjee and Duc Doan and Berk Calli},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610006},
  month     = {5},
  pages     = {3086-3092},
  title     = {Utilizing inpainting for training keypoint detection algorithms towards markerless visual servoing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-object RANSAC: Efficient plane clustering method in a
clutter. <em>ICRA</em>, 3079–3085. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel method for plane clustering specialized in cluttered scenes using an RGB-D camera and validate its effectiveness through robot grasping experiments. Unlike existing methods, which focus on large- scale indoor structures, our approach—Multi-Object RANSAC emphasizes cluttered environments that contain a wide range of objects with different scales. It enhances plane segmentation by generating subplanes in Deep Plane Clustering (DPC) module, which are then merged with the final planes by postprocessing. DPC rearranges the point cloud by voting layers to make subplane clusters, trained in a self-supervised manner using pseudo-labels generated from RANSAC. Multi-Object RANSAC demonstrates superior plane instance segmentation performances over other recent RANSAC applications. We conducted an experiment on robot suction-based grasping, comparing our method with vision-based grasping network and RANSAC applications. The results from this real-world scenario showed its remarkable performance surpassing the baseline methods, highlighting its potential for advanced scene understanding and manipulation.},
  archive   = {C_ICRA},
  author    = {Seunghyeon Lim and Youngjae Yoo and Jun Ki Lee and Byoung-Tak Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611029},
  month     = {5},
  pages     = {3079-3085},
  title     = {Multi-object RANSAC: Efficient plane clustering method in a clutter},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Language-conditioned affordance-pose detection in 3D point
clouds. <em>ICRA</em>, 3071–3078. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Affordance detection and pose estimation are of great importance in many robotic applications. Their combination helps the robot gain an enhanced manipulation capability, in which the generated pose can facilitate the corresponding affordance task. Previous methods for affodance-pose joint learning are limited to a predefined set of affordances, thus limiting the adaptability of robots in real-world environments. In this paper, we propose a new method for language-conditioned affordance-pose joint learning in 3D point clouds. Given a 3D point cloud object, our method detects the affordance region and generates appropriate 6-DoF poses for any unconstrained affordance label. Our method consists of an open-vocabulary affordance detection branch and a language-guided diffusion model that generates 6-DoF poses based on the affordance text. We also introduce a new high-quality dataset for the task of language-driven affordance-pose joint learning. Intensive experimental results demonstrate that our proposed method works effectively on a wide range of open-vocabulary affordances and outperforms other baselines by a large margin. In addition, we illustrate the usefulness of our method in real-world robotic applications. Our code and dataset are publicly available at https://3DAPNet.github.io.},
  archive   = {C_ICRA},
  author    = {Toan Nguyen and Minh Nhat Vu and Baoru Huang and Tuan Van Vo and Vy Truong and Ngan Le and Thieu Vo and Bac Le and Anh Nguyen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610008},
  month     = {5},
  pages     = {3071-3078},
  title     = {Language-conditioned affordance-pose detection in 3D point clouds},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-degrees-of-freedom dynamic neural fields for robot
self-modeling and motion planning. <em>ICRA</em>, 3064–3070. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A robot self-model is a task-agnostic representation of the robot’s physical morphology that can be used for motion planning tasks in the absence of a classical geometric kinematic model. In particular, when the latter is hard to engineer or the robot’s kinematics change unexpectedly, human-free self-modeling is a necessary feature of truly autonomous agents. In this work, we leverage neural fields to allow a robot to self-model its kinematics as a neural-implicit query model learned only from 2D images annotated with camera poses and configurations. This enables significantly greater applicability than existing approaches which have been dependent on depth images or geometry knowledge. To this end, alongside a curricular data sampling strategy, we propose a new encoder-based neural density field architecture for dynamic object-centric scenes conditioned on high numbers of degrees of freedom (DOFs). In a 7-DOF robot test setup, the learned self-model achieves a Chamfer-L2 distance of 2% of the robot’s workspace dimension. We demonstrate the capabilities of this model on motion planning tasks as an exemplary downstream application.},
  archive   = {C_ICRA},
  author    = {Lennart Schulze and Hod Lipson},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611047},
  month     = {5},
  pages     = {3064-3070},
  title     = {High-degrees-of-freedom dynamic neural fields for robot self-modeling and motion planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Commonsense spatial knowledge-aware 3-d human motion and
object interaction prediction. <em>ICRA</em>, 3057–3063. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel 3-D human motion and object interaction prediction model that is aware of commonsense knowledge about human–object interaction. We jointly predict human joint motion and human–object interactions. The two prediction results are combined to enforce commonsense knowledge, such as &quot;if the human right hand is predicted to be in contact with an object after 1 second, the distance between the right hand and an object should also be predicted to be small,&quot; explicit to the model. Our model uses the raw point cloud representation of the surrounding objects in the environment as input. Using raw point cloud representation allows us to model commonsense knowledge easily and improve accuracy. In particular, it does not require a separate perception system (e.g., object classification, object pose estimation, and so on), as in previous studies, and thus is robust to perception errors. Our model applies a cross-attention mechanism to fuse the environmental point cloud and past human joint poses. The surrounding environment context and past human joint poses are two heterogeneous inputs and cross-attention can be a powerful approach to fuse them. Our model is validated on the KIT Whole-Body Human Motion (WBHM) dataset.},
  archive   = {C_ICRA},
  author    = {Sang Uk Lee},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610161},
  month     = {5},
  pages     = {3057-3063},
  title     = {Commonsense spatial knowledge-aware 3-D human motion and object interaction prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). STNet: Spatio-temporal fusion-based SelfAttention for slip
detection in visuo-tactile sensors. <em>ICRA</em>, 3051–3056. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Slip detection plays a pivotal role in the dexterity of robotics, improving the reliability and precision of manipulations but also contributing to safety, efficiency, and adaptability. Deep learning-based slip detection algorithms commonly difficult to concentrate on key features when faced with dense 3D shape data obtained by visuo-tactile sensors. Data from noncontact locations can interfere with slip judgements and the ignorance of interframe linkage can also lead to slip detection failure. In this paper, a new spatio-temporal sequences fusion-based self-attention, STNet, is proposed to perform slip detection by allocating more attention to the object-sensor contact area when processing complex 3D shape data. A binocular visuo-tactile system (BVTS) is designed and fabricated for dataset construction. The entire 3D shape dataset containing 4 motion patterns, including stationary, pressing, rolling and slipping. Self-attention architecture with and without spatio-temporal sequences fusion mechanism (denoted as STNet and TemNet, respectively) are trained based on the same dataset. The experiments show the validity of STNet, which can reach 98.91% slip detection accuracy. Meanwhile, the ablation studies confirm the effectiveness of the spatio-temporal sequences fusion mechanism.},
  archive   = {C_ICRA},
  author    = {Jin Lu and Bangyan Niu and Huan Ma and Jiafeng Zhu and Jingjing Ji},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610734},
  month     = {5},
  pages     = {3051-3056},
  title     = {STNet: Spatio-temporal fusion-based SelfAttention for slip detection in visuo-tactile sensors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distribution-aware continual test-time adaptation for
semantic segmentation. <em>ICRA</em>, 3044–3050. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Since autonomous driving systems usually face dynamic and ever-changing environments, continual test-time adaptation (CTTA) has been proposed as a strategy for transferring deployed models to continually changing target domains. However, the pursuit of long-term adaptation often introduces catastrophic forgetting and error accumulation problems, which impede the practical implementation of CTTA in the real world. Recently, existing CTTA methods mainly focus on utilizing a majority of parameters to fit target domain knowledge through self-training. Unfortunately, these approaches often amplify the challenge of error accumulation due to noisy pseudo-labels, and pose practical limitations stemming from the heavy computational costs associated with entire model updates. In this paper, we propose a distribution-aware tuning (DAT) method to make the semantic segmentation CTTA efficient and practical in real-world applications. DAT adaptively selects and updates two small groups of trainable parameters based on data distribution during the continual adaptation process, including domain-specific parameters (DSP) and task-relevant parameters (TRP). Specifically, DSP exhibits sensitivity to outputs with substantial distribution shifts, effectively mitigating the problem of error accumulation. In contrast, TRP are allocated to positions that are responsive to outputs with minor distribution shifts, which are fine-tuned to avoid the catastrophic forgetting problem. In addition, since CTTA is a temporal task, we introduce the Parameter Accumulation Update (PAU) strategy to collect the updated DSP and TRP in target domain sequences. We conducted extensive experiments on two widely-used semantic segmentation CTTA benchmarks, achieving competitive performance and efficiency compared to previous state-of-the-art methods.},
  archive   = {C_ICRA},
  author    = {Jiayi Ni and Senqiao Yang and Ran Xu and Jiaming Liu and Xiaoqi Li and Wenyu Jiao and Zehui Chen and Yi Liu and Shanghang Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610045},
  month     = {5},
  pages     = {3044-3050},
  title     = {Distribution-aware continual test-time adaptation for semantic segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EgoPAT3Dv2: Predicting 3D action target from 2D egocentric
vision for human-robot interaction. <em>ICRA</em>, 3036–3043. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A robot’s ability to anticipate the 3D action target location of a hand’s movement from egocentric videos can greatly improve safety and efficiency in human-robot interaction (HRI). While previous research predominantly focused on semantic action classification or 2D target region prediction, we argue that predicting the action target’s 3D coordinate could pave the way for more versatile downstream robotics tasks, especially given the increasing prevalence of headset devices. This study expands EgoPAT3D, the sole dataset dedicated to egocentric 3D action target prediction. We augment both its size and diversity, enhancing its potential for generalization. Moreover, we substantially enhance the baseline algorithm by introducing a large pre-trained model and human prior knowledge. Remarkably, our novel algorithm can now achieve superior prediction outcomes using solely RGB images, eliminating the previous need for 3D point clouds and IMU input. Furthermore, we deploy our enhanced baseline algorithm on a real-world robotic platform to illustrate its practical utility in straightforward HRI tasks. The demonstrations showcase the real-world applicability of our advancements and may inspire more HRI use cases involving egocentric vision. All code and data are open-sourced and can be found on the project website.},
  archive   = {C_ICRA},
  author    = {Irving Fang and Yuzhong Chen and Yifan Wang and Jianghan Zhang and Qiushi Zhang and Jiali Xu and Xibo He and Weibo Gao and Hao Su and Yiming Li and Chen Feng},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610283},
  month     = {5},
  pages     = {3036-3043},
  title     = {EgoPAT3Dv2: Predicting 3D action target from 2D egocentric vision for human-robot interaction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A modular, tendon driven variable stiffness manipulator with
internal routing for improved stability and increased payload capacity.
<em>ICRA</em>, 3030–3035. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Stability and reliable operation under a spectrum of environmental conditions is still an open challenge for soft and continuum style manipulators. The inability to carry sufficient load and effectively reject external disturbances are two drawbacks which limit the scale of continuum designs, preventing widespread adoption of this technology. To tackle these problems, this work details the design and experimental testing of a modular, tendon driven bead-style continuum manipulator with tunable stiffness. By embedding the ability to independently control the stiffness of distinct sections of the structure, the manipulator can regulate it’s posture under greater loads of up to 1kg at the end-effector, with reference to the flexible state. Likewise, an internal routing scheme vastly improves the stability of the proximal segment when operating the distal segment, reducing deviations by at least 70.11%. Operation is validated when gravity is both tangential and perpendicular to the manipulator backbone, a feature uncommon in previous designs. The findings presented in this work are key to the development of larger scale continuum designs, demonstrating that flexibility and tip stability under loading can co-exist without compromise.},
  archive   = {C_ICRA},
  author    = {Kyle L. Walker and Alix J. Partridge and Hsing-Yu Chen and Rahul R. Ramachandran and Adam A. Stokes and Kenjiro Tadakuma and Lucas Cruz Da Silva and Francesco Giorgio-Serchi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611527},
  month     = {5},
  pages     = {3030-3035},
  title     = {A modular, tendon driven variable stiffness manipulator with internal routing for improved stability and increased payload capacity},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-curvature, high-force, vine robot for inspection.
<em>ICRA</em>, 3014–3021. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot performance has advanced considerably both in and out of the factory, however in tightly constrained, unknown environments such as inside a jet engine or the human heart, current robots are less adept. In such cases where a borescope or endoscope can’t reach, disassembly or surgery are costly. One promising inspection device inspired by plant growth are &quot;vine robots&quot; that can navigate cluttered environments by extending from their tip. Yet, these vine robots are currently limited in their ability to simultaneously steer into tight curvatures and apply substantial forces to the environment. Here, we propose a plant-inspired method of steering by asymmetrically lengthening one side of the vine robot to enable high curvature and large force application. Our key development is the introduction of an extremely anisotropic, composite, wrinkled film with elastic moduli 400x different in orthogonal directions. The film is used as the vine robot body, oriented such that it can stretch over 120% axially, but only 3% circumferentially. With the addition of controlled layer jamming, this film enables a steering method inspired by plants in which the circumference of the robot is inextensible, but the sides can stretch to allow turns. This steering method and body pressure do not work against each other, allowing the robot to exhibit higher forces and tighter curvatures than previous vine robot architectures. This work advances the abilities of vine robots–and robots more generally–to not only access tightly constrained environments, but perform useful work once accessed.},
  archive   = {C_ICRA},
  author    = {Mijaíl Jaén Mendoza and Nicholas D. Naclerio and Elliot W. Hawkes},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610845},
  month     = {5},
  pages     = {3014-3021},
  title     = {High-curvature, high-force, vine robot for inspection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and characterization of a soft flat tube twisting
actuator. <em>ICRA</em>, 3000–3005. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft actuators have shown advantages of adaptiveness, large deformation, and safe human-robot interaction, making them suitable for various applications. Herein, a novel soft flat tube twisting actuator (SFTTA) is proposed. The SFTTA is composed of a folded flat tube sandwiched between two silicone rubber laminates. When inflated by compressed air, the folded corners of the flat tube tend to unfold, resulting in the twist of the actuator to a helical structure. The SFTTA has great scalability. It can be fabricated through simple processes with low-cost materials. For a sample SFTTA with the size of a human finger, it can twist 540 0 at an air pressure of 300 kPa. In general, SFTTA based actuators can twist 9.6 degree per millimeter in length, which is significantly larger than previously reported soft twisting actuators. Additionally, the composite-like SFTTA allows mechanical property programming through the alteration of folding patterns of the flat tube and the material structure of the elastomer laminates. Finally, an extensible soft gripper based on flat tube actuators and a robotic wrist module are developed, and their rotation is realized by the proposed SFTTA actuator.},
  archive   = {C_ICRA},
  author    = {Hao Liu and Changchun Wu and Senyuan Lin and Yonghua Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609872},
  month     = {5},
  pages     = {3000-3005},
  title     = {Design and characterization of a soft flat tube twisting actuator},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Soft hand extension glove with thumb abduction and extension
assistance. <em>ICRA</em>, 2993–2999. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hand extension is crucial for stroke survivors with spasticity, where their fingers become rigid and their thumb remains curled within the palm. Due to the underactuated nature of the hand, the dominance of flexor muscles over extensors, and the limited surface area available, developing an extension glove with thumb assistance poses a challenge for researchers. This paper introduces a fully wearable soft hand extension glove based on the X-pouch and strap system, addressing the above challenges. The glove enables adequate finger extension, thumb abduction, and extension for high MAS score patients. Modelling and testing revealed extension torques of up to 2.7 Nm at the MCP joint and 0.67 Nm at the PIP and DIP joints. Performance evaluation, including comparison with existing methods, demonstrated the glove’s superior extension capabilities using a model hand with realistic stiffness. Furthermore, the glove’s effectiveness was confirmed through testing on a stroke patient with MAS = 2, validating its on-body functionality.},
  archive   = {C_ICRA},
  author    = {Disheng Xie and Yujie Su and Xiangqian Shi and Zheng Li and Raymond Kai-yu Tong},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610770},
  month     = {5},
  pages     = {2993-2999},
  title     = {Soft hand extension glove with thumb abduction and extension assistance},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Directly 3D printed, pneumatically actuated multi-material
robotic hand. <em>ICRA</em>, 2986–2992. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft robotic manipulators with many degrees of freedom can carry out complex tasks safely around humans. However, manufacturing of soft robotic hands with several degrees of freedom requires a complex multi-step manual process, which significantly increases their cost. We present a design of a multi-material 15 DoF robotic hand with five fingers including an opposable thumb. Our design has 15 pneumatic actuators based on a series of hollow chambers that are driven by an external pressure system. The thumb utilizes rigid joints and the palm features internal rigid structure and soft skin. The design can be directly 3D printed using a multi-material additive manufacturing process without any assembly process and therefore our hand can be manufactured for less than 300 dollars. We test the hand in conjunction with a low-cost vision-based teleoperation system on different tasks.},
  archive   = {C_ICRA},
  author    = {Hanna Matusik and Chao Liu and Daniela Rus},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610016},
  month     = {5},
  pages     = {2986-2992},
  title     = {Directly 3D printed, pneumatically actuated multi-material robotic hand},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and analysis of soft hybrid-driven manipulator with
variable stiffness and multiple motion patterns. <em>ICRA</em>,
2979–2985. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft manipulators offer the advantages of safety and adaptability. However, due to insufficient stiffness and single motion mode limitations, existing soft manipulators usually exhibit low load capacity and small working space. To address this problem, we propose a novel soft hybrid-driven manipulator with continuous stiffness control capability and multiple motion patterns (omnidirectional bending and extension). Furthermore, we develop kinematic and stiffness models based on the constant curvature assumption. The soft robot consists of a soft bellows actuator and inextensible rigid skeletons, which exhibit a high extension ratio and low drive pressure. With the antagonistic actuation of tendon-pulling and air-pushing, the robot can achieve independent control over stiffness and position in three-dimensional space. The performance associated with the designed soft hybrid-driven manipulator is experimentally verified. The robot can achieve an elongation of 198% and a maximum bending angle of up to 240°. The robot can also increase stiffness by increasing internal air pressure to resist deformation caused by external loads. Additionally, tracking experiments with various trajectories in space verify the accuracy of the kinematic model, which indicates that the soft manipulator can stabilize motion within a broad workspace.},
  archive   = {C_ICRA},
  author    = {Xin Fu and Daohui Zhang and Liyan Mo and Kai Li and Xingang Zhao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610612},
  month     = {5},
  pages     = {2979-2985},
  title     = {Design and analysis of soft hybrid-driven manipulator with variable stiffness and multiple motion patterns},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RAPIDFlow: Recurrent adaptable pyramids with iterative
decoding for efficient optical flow estimation. <em>ICRA</em>,
2946–2952. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Extracting motion information from videos with optical flow estimation is vital in multiple practical robot applications. Current optical flow approaches show remarkable accuracy, but top-performing methods have high computational costs and are unsuitable for embedded devices. Although some previous works have focused on developing low-cost optical flow strategies, their estimation quality has a noticeable gap with more robust methods. In this paper, we develop a novel method to efficiently estimate high-quality optical flow in embedded devices. Our proposed RAPIDFlow model combines efficient NeXt1D convolution blocks with a fully recurrent structure based on feature pyramids to decrease computational costs without significantly impacting estimation accuracy. The adaptable recurrent encoder produces multi-scale features with a single shared block, which allows us to adjust the pyramid length at inference time and make it more robust to changes in input size. Also, it enables our model to offer multiple tradeoffs between accuracy and speed to suit different applications. Experiments using a Jetson Orin NX embedded system on the MPI-Sintel and KITTI public benchmarks show that RAPIDFlow outperforms previous approaches by significant margins at faster speeds. Our code is available at https://github.com/hmorimitsu/ptlflow/tree/main/ptlflow/models/rapidflow.},
  archive   = {C_ICRA},
  author    = {Henrique Morimitsu and Xiaobin Zhu and Roberto M. Cesar and Xiangyang Ji and Xu-Cheng Yin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610277},
  month     = {5},
  pages     = {2946-2952},
  title     = {RAPIDFlow: Recurrent adaptable pyramids with iterative decoding for efficient optical flow estimation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bicode: A hybrid blinking marker system for event cameras.
<em>ICRA</em>, 2939–2945. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the field of robotics, tag systems play an important role in various applications, such as object identification and robot control in real-world environments. While typical visual markers use two-dimensional (2D) patterns and RGB cameras for recognizing object IDs and poses, achieving long-distance recognition necessitates increasing marker size and camera magnification to ensure the required resolution. Furthermore, the growing adoption of event cameras in robotics captures rapid changes in pixel brightness but faces limitations in recognizing stationary 2D markers. Although compact blinker markers using blinking light-emitting diodes (LEDs) achieve long-distance recognition, they are constrained by the number of IDs or recognition speed when used with standard RGB cameras. In addition, recognizing object pose using only a single blinking LED presents challenges. To address these challenges, we introduce ‘Bicode,’ an indoor visual marker designed for event cameras. Bicode seamlessly integrates 2D and blinker markers within a single marker unit. We have developed prototypes of 2.5, 5, and 10 cm square acrylic 2D markers, each equipped with a single LED blinking at 1 kHz, enabling recognition with an event camera. Our experiments revealed the effects of marker size, LED light quantity, recognition distance, and angle, external lighting conditions, and camera or marker movement on accuracy. Notably, using the 5 cm marker, we confirmed its compatibility to recognize IDs at distances exceeding 20 m, and pose recognition at 2.5 m was confirmed.},
  archive   = {C_ICRA},
  author    = {Takuya Kitade and Wataru Yamada and Keiichi Ochiai and Michita Imai},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611033},
  month     = {5},
  pages     = {2939-2945},
  title     = {Bicode: A hybrid blinking marker system for event cameras},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robot navigation in unseen environments using coarse maps.
<em>ICRA</em>, 2932–2938. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Metric occupancy maps are widely used in autonomous robot navigation systems. However, when a robot is deployed in an unseen environment, building an accurate metric map is time-consuming. Can an autonomous robot directly navigate in previously unseen environments using coarse maps? In this work, we propose the Coarse Map Navigator (CMN), a navigation framework that can perform robot navigation in unseen environments using different coarse maps. To do so, CMN addresses two challenges: (1) novel and realistic visual observations; (2) error and misalignment on coarse maps. To tackle novel visual observations in unseen environments, CMN learns a deep perception model that maps the visual input from various pixel spaces to the local occupancy grid space. To tackle the error and misalignment on coarse maps, CMN extends the Bayesian filter and maintains a belief directly on coarse maps using the predicted local occupancy grids as observations. Using the latest belief, CMN extracts a global heuristic vector that guides the planner to find a local navigation action. Empirical results demonstrate that CMN achieves high navigation success rates in unseen environments, significantly outperforming baselines, and is robust to different coarse maps.},
  archive   = {C_ICRA},
  author    = {Chengguang Xu and Christopher Amato and Lawson L.S. Wong},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611256},
  month     = {5},
  pages     = {2932-2938},
  title     = {Robot navigation in unseen environments using coarse maps},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uncertainty-aware hybrid paradigm of nonlinear MPC and
model-based RL for offroad navigation: Exploration of transformers in
the predictive model. <em>ICRA</em>, 2925–2931. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we investigate a hybrid scheme that combines nonlinear model predictive control (MPC) and model-based reinforcement learning (RL) for navigation planning of an autonomous model car across offroad, unstructured terrains without relying on predefined maps. Our innovative approach takes inspiration from BADGR, an LSTM-based network that primarily concentrates on environment modeling, but distinguishes itself by substituting LSTM modules with transformers to greatly elevate the performance of our model. Addressing uncertainty within the system, we train an ensemble of predictive models and estimate the mutual information between model weights and outputs, facilitating dynamic horizon planning through the introduction of variable speeds. Further enhancing our methodology, we incorporate a nonlinear MPC controller that accounts for the intricacies of the vehicle’s model and states. The model-based RL facet produces steering angles and quantifies inherent uncertainty. At the same time, the nonlinear MPC suggests optimal throttle settings, striking a balance between goal attainment speed and managing model uncertainty influenced by velocity. In the conducted studies, our approach excels over the existing baseline by consistently achieving higher metric values in predicting future events and seamlessly integrating the vehicle’s kinematic model for enhanced decision-making. The code and the evaluation data are available at (Github-repo).},
  archive   = {C_ICRA},
  author    = {Faraz Lotfi and Khalil Virji and Farnoosh Faraji and Lucas Berry and Andrew Holliday and David Meger and Gregory Dudek},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610452},
  month     = {5},
  pages     = {2925-2931},
  title     = {Uncertainty-aware hybrid paradigm of nonlinear MPC and model-based RL for offroad navigation: Exploration of transformers in the predictive model},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Teach and repeat navigation: A robust control approach.
<em>ICRA</em>, 2909–2916. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot navigation requires an autonomy pipeline that is robust to environmental changes and effective in varying conditions. Teach and Repeat (T&amp;R) navigation has shown high performance in autonomous repeated tasks under challenging circumstances, but research within T&amp;R has predominantly focused on motion planning as opposed to motion control. In this paper, we propose a novel T&amp;R system based on a robust motion control technique for a skid-steering mobile robot using sliding-mode control that effectively handles uncertainties that are particularly pronounced in the T&amp;R task, where sensor noises, parametric uncertainties, and wheel-terrain interaction are common challenges. We first theoretically demonstrate that the proposed T&amp;R system is globally stable and robust while considering the uncertainties of the closed-loop system. When deployed on a Clearpath Jackal robot, we then show the global stability of the proposed system in both indoor and outdoor environments covering different terrains, outperforming previous state-of-the-art methods in terms of mean average trajectory error and stability in these challenging environments. This paper makes an important step towards long-term autonomous T&amp;R navigation with ensured safety guarantees.},
  archive   = {C_ICRA},
  author    = {Payam Nourizadeh and Michael Milford and Tobias Fischer},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611662},
  month     = {5},
  pages     = {2909-2916},
  title     = {Teach and repeat navigation: A robust control approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploitation-guided exploration for semantic embodied
navigation. <em>ICRA</em>, 2901–2908. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the recent progress in embodied navigation and sim-to-robot transfer, modular policies have emerged as a de facto framework. However, there is more to compositionality beyond the decomposition of the learning load into modular components. In this work, we investigate a principled way to syntactically combine these components. Particularly, we propose Exploitation-Guided Exploration (XgX) where separate modules for exploration and exploitation come together in a novel and intuitive manner. We configure the exploitation module to take over in the deterministic final steps of navigation i.e. when the goal becomes visible. Crucially, an exploitation module teacher-forces the exploration module and continues driving an overridden policy optimization. XgX, with effective decomposition and novel guidance, improves the state-of-the-art performance on the challenging object navigation task from 70% to 73%. Along with better accuracy, through targeted analysis, we show that XgX is also more efficient at goal-conditioned exploration. Finally, we show sim-to-real transfer to robot hardware and XgX performs over two-fold better than the best baseline from simulation benchmarking. Project page: xgxvisnav.github.io},
  archive   = {C_ICRA},
  author    = {Justin Wasserman and Girish Chowdhary and Abhinav Gupta and Unnat Jain},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610117},
  month     = {5},
  pages     = {2901-2908},
  title     = {Exploitation-guided exploration for semantic embodied navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Bi2Lane: Bi-directional temporal refinement with bi-level
feature aggregation for 3D lane detection. <em>ICRA</em>, 2894–2900. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Monocular 3D lane detection has recently received increasing research attention in autonomous driving due to its application effectiveness and simplicity. However, depending solely on the limited semantic information from a single image makes current monocular detection methods unable to deal with complex scenarios, such as occluded, blurred, and unaligned scenes. In this study, we introduce an end-to-end framework named Bi 2 Lane which models temporal dependency in a continuous sequence. It recurrently utilizes detected lanes within historical frames as prior information to achieve robust lane detection. Additionally, Bi 2 Lane employs temporal reverse refinement together with temporal forward refinement to achieve bi-directional temporal refinement (BDTR) while maintaining a robust temporal dependency. For the refined features of different frames, we design a bi-level feature aggregation module (BLFA) to fuse them in both point-level and line-level manners, enabling a comprehensive feature representation to deal with complicated road scenes. Extensive experiments conducted on the OpenLane dataset demonstrate the superiority of Bi 2 Lane, achieving a notable F1 score of 63.8% using a simple ResNet50 backbone, surpassing the performance of existing state-of-the-art methods.},
  archive   = {C_ICRA},
  author    = {Chengxin Li and Yihui Hu and Zewen Zheng and Xiang Gao and Yongqiang Mou and Peng Nie and Jun Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610794},
  month     = {5},
  pages     = {2894-2900},
  title     = {Bi2Lane: Bi-directional temporal refinement with bi-level feature aggregation for 3D lane detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Projected task-specific layers for multi-task reinforcement
learning. <em>ICRA</em>, 2887–2893. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-task reinforcement learning could enable robots to scale across a wide variety of manipulation tasks in homes and workplaces. However, generalizing from one task to another and mitigating negative task interference still remains a challenge. Addressing this challenge by successfully sharing information across tasks will depend on how well the structure underlying the tasks is captured. In this work, we introduce our new architecture, Projected Task-Specific Layers (PTSL), that leverages a common policy with dense task-specific corrections through task-specific layers to better express shared and variable task information. We then show that our model outperforms the state of the art on the MT10 and MT50 benchmarks of Meta-World consisting of 10 and 50 goal-conditioned tasks for a Sawyer arm.},
  archive   = {C_ICRA},
  author    = {Josselin Somerville Roberts and Julia Di},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610483},
  month     = {5},
  pages     = {2887-2893},
  title     = {Projected task-specific layers for multi-task reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Differentially encoded observation spaces for perceptive
reinforcement learning. <em>ICRA</em>, 2880–2886. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Perceptive deep reinforcement learning (DRL) has lead to many recent breakthroughs for complex AI systems leveraging image-based input data. Applications of these results range from super-human level video game agents to dexterous, physically intelligent robots. However, training these perceptive DRL-enabled systems remains incredibly compute and memory intensive, often requiring huge training datasets and large experience replay buffers. This poses a challenge for the next generation of field robots that will need to be able to learn on the edge in order to adapt to their environments. In this paper, we begin to address this issue through differentially encoded observation spaces. By reinterpreting stored imagebased observations as a video, we leverage lossless differential video encoding schemes to compress the replay buffer without impacting training performance. We evaluate our approach with three state-of-the-art DRL algorithms and find that differential image encoding reduces the memory footprint by as much as 14.2× and 16.7× across tasks from the Atari 2600 benchmark and the DeepMind Control Suite (DMC) respectively. These savings also enable large-scale perceptive DRL that previously required paging between flash and RAM to be run entirely in RAM, improving the latency of DMC tasks by as much as 32%.},
  archive   = {C_ICRA},
  author    = {Lev Grossman and Brian Plancher},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611215},
  month     = {5},
  pages     = {2880-2886},
  title     = {Differentially encoded observation spaces for perceptive reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Safety optimized reinforcement learning via multi-objective
policy optimization. <em>ICRA</em>, 2873–2879. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safe reinforcement learning (Safe RL) refers to a class of techniques that aim to prevent RL algorithms from violating constraints in the process of decision-making and exploration during trial and error. In this paper, a novel model-free Safe RL algorithm, formulated based on the multi-objective policy optimization framework is introduced where the policy is optimized towards optimality and safety, simultaneously. The optimality is achieved by the environment reward function that is subsequently shaped using a safety critic. The advantage of the Safety Optimized RL (SORL) algorithm compared to the traditional Safe RL algorithms is that it omits the need to constrain the policy search space. This allows SORL to find a natural tradeoff between safety and optimality without compromising the performance in terms of either safety or optimality due to strict search space constraints. Through our theoretical analysis of SORL, we propose a condition for SORL’s converged policy to guarantee safety and then use it to introduce an aggressiveness parameter that allows for fine-tuning the mentioned tradeoff. The experimental results obtained in seven different robotic environments indicate a considerable reduction in the number of safety violations along with higher, or competitive, policy returns, in comparison to six different state-of-the-art Safe RL methods. The results demonstrate the significant superiority of the proposed SORL algorithm in safety-critical applications.},
  archive   = {C_ICRA},
  author    = {Homayoun Honari and Mehran Ghafarian Tamizi and Homayoun Najjaran},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611316},
  month     = {5},
  pages     = {2873-2879},
  title     = {Safety optimized reinforcement learning via multi-objective policy optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contrastive initial state buffer for reinforcement learning.
<em>ICRA</em>, 2866–2872. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In Reinforcement Learning, the trade-off between exploration and exploitation poses a complex challenge for achieving efficient learning from limited samples. While recent works have been effective in leveraging past experiences for policy updates, they often overlook the potential of reusing past experiences for data collection. Independent of the underlying RL algorithm, we introduce the concept of a Contrastive Initial State Buffer, which strategically selects states from past experiences and uses them to initialize the agent in the environment in order to guide it toward more informative states. We validate our approach on two complex robotic tasks without relying on any prior information about the environment: (i) locomotion of a quadruped robot traversing challenging terrains and (ii) a quadcopter drone racing through a track. The experimental results show that our initial state buffer achieves higher task performance than the nominal baseline while also speeding up training convergence.},
  archive   = {C_ICRA},
  author    = {Nico Messikommer and Yunlong Song and Davide Scaramuzza},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610528},
  month     = {5},
  pages     = {2866-2872},
  title     = {Contrastive initial state buffer for reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning adaptive safety for multi-agent systems.
<em>ICRA</em>, 2859–2865. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ensuring safety in dynamic multi-agent systems is challenging due to limited information about the other agents. Control Barrier Functions (CBFs) are showing promise for safety assurance but current methods make strong assumptions about other agents and often rely on manual tuning to balance safety, feasibility, and performance. In this work, we delve into the problem of adaptive safe learning for multi-agent systems with CBF. We show how emergent behaviour can be profoundly influenced by the CBF configuration, highlighting the necessity for a responsive and dynamic approach to CBF design. We present ASRL, a novel adaptive safe RL framework, to fully automate the optimization of policy and CBF coefficients, to enhance safety and long-term performance through reinforcement learning. By directly interacting with the other agents, ASRL learns to cope with diverse agent behaviours and maintains the cost violations below a desired limit. We evaluate ASRL in a multi-robot system and competitive multi-agent racing, against learning-based and control-theoretic approaches. We empirically demonstrate the efficacy of ASRL, and assess generalization and scalability to out-of-distribution scenarios.},
  archive   = {C_ICRA},
  author    = {Luigi Berducci and Shuo Yang and Rahul Mangharam and Radu Grosu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611037},
  month     = {5},
  pages     = {2859-2865},
  title     = {Learning adaptive safety for multi-agent systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributional reinforcement learning with sample-set
bellman update. <em>ICRA</em>, 2852–2858. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Distributional Reinforcement Learning (DRL) not only endeavors to optimize expected returns, but also strives to accurately characterize the full distribution of these returns, a key aspect in enhancing risk-aware decision-making. Previous DRL implementations often inappropriately treat statistical estimations as concrete samples, which undermines the integrity of learning. While several studies have addressed this issue, they frequently give rise to new complications, including computational burdens and diminished stochastic behavior. In our work, we present a novel DRL framework that leverages the Gaussian mixture model to adeptly depict the distribution of returns. This approach ensures precise, authentic sampling critical for robust learning, while also preserving computational tractability. Through extensive evaluation on a diverse array of 59 Atari games, our method not only surpasses the efficacy of prior DRL algorithms but also presents formidable competition to contemporary top-tier RL algorithms, signifying a substantial advancement in the field.},
  archive   = {C_ICRA},
  author    = {Weijian Zhang and Jianshu Wang and Yang Yu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610740},
  month     = {5},
  pages     = {2852-2858},
  title     = {Distributional reinforcement learning with sample-set bellman update},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforcement learning in a safety-embedded MDP with
trajectory optimization. <em>ICRA</em>, 2845–2851. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safe Reinforcement Learning (RL) plays an important role in applying RL algorithms to safety-critical real-world applications, addressing the trade-off between maximizing rewards and adhering to safety constraints. This work introduces a novel approach that combines RL with trajectory optimization to manage this trade-off effectively. Our approach embeds safety constraints within the action space of a modified Markov Decision Process (MDP). The RL agent produces a sequence of actions that are transformed into safe trajectories by a trajectory optimizer, thereby effectively ensuring safety and increasing training stability. This novel approach excels in its performance on challenging Safety Gym tasks, achieving significantly higher rewards and near-zero safety violations during inference. The method’s real-world applicability is demonstrated through a safe and effective deployment in a real robot task of box-pushing around obstacles. Further insights are available from the videos and appendix on our website: https://sites.google.com/view/safemdp.},
  archive   = {C_ICRA},
  author    = {Fan Yang and Wenxuan Zhou and Zuxin Liu and Ding Zhao and David Held},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610047},
  month     = {5},
  pages     = {2845-2851},
  title     = {Reinforcement learning in a safety-embedded MDP with trajectory optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Offline goal-conditioned reinforcement learning for
safety-critical tasks with recovery policy. <em>ICRA</em>, 2838–2844.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Offline goal-conditioned reinforcement learning (GCRL) aims at solving goal-reaching tasks with sparse rewards from an offline dataset. While prior work has demonstrated various approaches for agents to learn near-optimal policies, these methods encounter limitations when dealing with diverse constraints in complex environments, such as safety constraints. Some of these approaches prioritize goal attainment without considering safety, while others excessively focus on safety at the expense of training efficiency. In this paper, we study the problem of constrained offline GCRL and propose a new method called Recovery-based Supervised Learning (RbSL) to accomplish safety-critical tasks with various goals. To evaluate the method performance, we build a benchmark based on the robot-fetching environment with a randomly positioned obstacle and use expert or random policies to generate an offline dataset. We compare RbSL with three offline GCRL algorithms and one offline safe RL algorithm. As a result, our method outperforms the existing state-of-the-art methods to a large extent. Furthermore, we validate the practicality and effectiveness of RbSL by deploying it on a real Panda manipulator. Code is available at https://github.com/Sunlighted/RbSL.git.},
  archive   = {C_ICRA},
  author    = {Chenyang Cao and Zichen Yan and Renhao Lu and Junbo Tan and Xueqian Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610856},
  month     = {5},
  pages     = {2838-2844},
  title     = {Offline goal-conditioned reinforcement learning for safety-critical tasks with recovery policy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A generalized acquisition function for preference-based
reward learning. <em>ICRA</em>, 2814–2821. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Preference-based reward learning is a popular technique for teaching robots and autonomous systems how a human user wants them to perform a task. Previous works have shown that actively synthesizing preference queries to maximize information gain about the reward function parameters improves data efficiency. The information gain criterion focuses on precisely identifying all parameters of the reward function. This can potentially be wasteful as many parameters may result in the same reward, and many rewards may result in the same behavior in the downstream tasks. Instead, we show that it is possible to optimize for learning the reward function up to a behavioral equivalence class, such as inducing the same ranking over behaviors, distribution over choices, or other related definitions of what makes two rewards similar. We introduce a tractable framework that can capture such definitions of similarity. Our experiments in a synthetic environment, an assistive robotics environment with domain transfer, and a natural language processing problem with real datasets demonstrate the superior performance of our querying method over the state-of-the-art information gain method.},
  archive   = {C_ICRA},
  author    = {Evan Ellis and Gaurav R. Ghosal and Stuart J. Russell and Anca Dragan and Erdem Bıyık},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611472},
  month     = {5},
  pages     = {2814-2821},
  title     = {A generalized acquisition function for preference-based reward learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rank2Reward: Learning shaped reward functions from passive
video. <em>ICRA</em>, 2806–2813. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Teaching robots novel skills with demonstrations via human-in-the-loop data collection techniques like kinesthetic teaching or teleoperation puts a heavy burden on human supervisors. In contrast to this paradigm, it is often significantly easier to provide raw, action-free visual data of tasks being performed. Moreover, this data can even be mined from video datasets or the web. Ideally, this data can serve to guide robot learning for new tasks in novel environments, informing both &quot;what&quot; to do and &quot;how&quot; to do it. A powerful way to encode both the &quot;what&quot; and the &quot;how&quot; is to infer a well-shaped reward function for reinforcement learning. The challenge is determining how to ground visual demonstration inputs into a well-shaped and informative reward function. We propose a technique Rank2Reward for learning behaviors from videos of tasks being performed without access to any low-level states and actions. We do so by leveraging the videos to learn a reward function that measures incremental &quot;progress&quot; through a task by learning how to temporally rank the video frames in a demonstration. By inferring an appropriate ranking, the reward function is able to guide reinforcement learning by indicating when task progress is being made. This ranking function can be integrated into an adversarial imitation learning scheme resulting in an algorithm that can learn behaviors without exploiting the learned reward function. We demonstrate the effectiveness of Rank2Reward at learning behaviors from raw video on a number of tabletop manipulation tasks in both simulations and on a real-world robotic arm. We also demonstrate how Rank2Reward can be easily extended to be applicable to web-scale video datasets. Code and videos are available at https://rank2reward.github.io},
  archive   = {C_ICRA},
  author    = {Daniel Yang and Davin Tjia and Jacob Berg and Dima Damen and Pulkit Agrawal and Abhishek Gupta},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610873},
  month     = {5},
  pages     = {2806-2813},
  title     = {Rank2Reward: Learning shaped reward functions from passive video},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DINOBot: Robot manipulation via retrieval and alignment with
vision foundation models. <em>ICRA</em>, 2798–2805. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose DINOBot, a novel imitation learning framework for robot manipulation, which leverages the image-level and pixel-level capabilities of features extracted from Vision Transformers trained with DINO. When interacting with a novel object, DINOBot first uses these features to retrieve the most visually similar object experienced during human demonstrations, and then uses this object to align its endeffector with the novel object to enable effective interaction. Through a series of real-world experiments on everyday tasks, we show that exploiting both the image-level and pixel-level properties of vision foundation models enables unprecedented learning efficiency and generalisation. Videos and code are available at https://www.robot-learning.uk/dinobot.},
  archive   = {C_ICRA},
  author    = {Norman Di Palo and Edward Johns},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610923},
  month     = {5},
  pages     = {2798-2805},
  title     = {DINOBot: Robot manipulation via retrieval and alignment with vision foundation models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Policy optimization by looking ahead for model-based offline
reinforcement learning. <em>ICRA</em>, 2791–2797. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Offline reinforcement learning (RL) aims to optimize a policy, based on pre-collected data, to maximize the cumulative rewards after performing a sequence of actions. Existing approaches learn a value function from historical data and then guide the updating of the policy parameters by maximizing the value function at a single time. Driven by the gap between maximizing the cumulative rewards of RL and the greedy strategy of existing methods, we propose an approach of policy optimization by looking ahead (POLA) to mitigate the gap. Concretely, we optimize the policy on both current and future states where the future states are predicted by a transition model. A trajectory contains numerous actions before the task is done. Performing the best action at each time does not mean an optimal trajectory in the end. We need to allow sub-optimal or negative actions occasionally. But existing methods focus on generating the optimal action at each time according to the maximizing Q-value principle. This motivates our looking ahead approach. Besides, hidden confounding factors may affect the decision making process. To that end, we incorporate the correlations among dimensions of the state into the policy, providing more information about the environment for the policy to make decisions. Empirical results on the Mujoco dataset show the effectiveness of the proposed approach.},
  archive   = {C_ICRA},
  author    = {Yang Liu and Marius Hofert},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610966},
  month     = {5},
  pages     = {2791-2797},
  title     = {Policy optimization by looking ahead for model-based offline reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). DerainNeRF: 3D scene estimation with adhesive waterdrop
removal. <em>ICRA</em>, 2787–2793. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When capturing images through the glass during rainy or snowy weather conditions, the resulting images often contain waterdrops adhered on the glass surface, and these waterdrops significantly degrade the image quality and performance of many computer vision algorithms. To tackle these limitations, we propose a method to reconstruct the clear 3D scene implicitly from multi-view images degraded by water-drops. Our method exploits an attention network to predict the location of waterdrops and then train a Neural Radiance Fields to recover the 3D scene implicitly. By leveraging the strong scene representation capabilities of NeRF, our method can render high-quality novel-view images with waterdrops removed. Extensive experimental results on both synthetic and real datasets show that our method is able to generate clear 3D scenes and outperforms existing state-of-the-art (SOTA) image adhesive waterdrop removal methods.},
  archive   = {C_ICRA},
  author    = {Yunhao Li and Jing Wu and Lingzhe Zhao and Peidong Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609981},
  month     = {5},
  pages     = {2787-2793},
  title     = {DerainNeRF: 3D scene estimation with adhesive waterdrop removal},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Hierarchical human-to-robot imitation learning for
long-horizon tasks via cross-domain skill alignment. <em>ICRA</em>,
2783–2790. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For a general-purpose robot, it is desirable to imitate human demonstration videos that can effectively solve long-horizon tasks and perform novel ones. Recent advances in skill-based imitation learning have shown that extracting skill embedding from raw human videos is a promising paradigm to enable robots to cope with long-horizon tasks. However, generalization to unseen tasks in a different domain with a human prompt video poses a significant challenge due to the big embodiment and environment difference. To this end, we present Hierarchical Human-to-Robot Imitation Learning (H2RIL) that learns the mapping of cross-domain sensorimotor skills and utilizes it to generalize to unseen tasks given a human video in a different environment. To allow for generalizing zero-shot across environments and embodiments, H2RIL leverages task-agnostic play data for low-level policy training and paired human-robot data for both semantic and temporal skill embedding alignment. Extensive experiments in a simulated kitchen environment demonstrate that H2RIL significantly outperforms other prior baselines and is capable of generalizing to composable new tasks and adapting to Out-of-Distribution (OOD) tasks.},
  archive   = {C_ICRA},
  author    = {Zhenyang Lin and Yurou Chen and Zhiyong Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610084},
  month     = {5},
  pages     = {2783-2790},
  title     = {Hierarchical human-to-robot imitation learning for long-horizon tasks via cross-domain skill alignment},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Overparametrization helps offline-to-online generalization
of closed-loop control from pixels. <em>ICRA</em>, 2774–2782. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There is an ever-growing zoo of modern neural network models that can efficiently learn end-to-end control from visual observations. These advanced deep models, ranging from convolutional to Vision Transformers, from small to gigantic networks, have been extensively tested on offline image classification tasks. In this paper, we study these vision models with respect to the open-loop training to closed-loop generalization abilities, i.e., deployment realizes a causal feedback loop that is not present during training. This causality gap typically emerges in robotics applications such as autonomous driving, where a network is trained to imitate the control commands of a human. In this setting, two situations arise: 1) Closed-loop testing in-distribution, where the test environment shares properties with those of offline training data. 2) Closed-loop testing under distribution shifts and out-of-distribution. Contrary to recently reported results, we show that under proper training guidelines, all vision architectures perform indistinguishably well on in-distribution deployment, resolving the causality gap. In situation 2, We observe that scale is the strongest factor in improving closed-loop generalization regardless of the choice of the model architecture. Our results predict the trend that in the future we will see larger and larger models being used in offline-training-online-deployment imitation learning tasks in robotic applications.},
  archive   = {C_ICRA},
  author    = {Mathias Lechner and Ramin Hasani and Alexander Amini and Tsun-Hsuan Wang and Thomas A. Henzinger and Daniela Rus},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610284},
  month     = {5},
  pages     = {2774-2782},
  title     = {Overparametrization helps offline-to-online generalization of closed-loop control from pixels},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SLCF-net: Sequential LiDAR-camera fusion for semantic scene
completion using a 3D recurrent u-net. <em>ICRA</em>, 2767–2773. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce SLCF-Net, a novel approach for the Semantic Scene Completion (SSC) task that sequentially fuses LiDAR and camera data. It jointly estimates missing geometry and semantics in a scene from sequences of RGB images and sparse LiDAR measurements. The images are semantically segmented by a pre-trained 2D U-Net and a dense depth prior is estimated from a depth-conditioned pipeline fueled by Depth Anything. To associate the 2D image features with the 3D scene volume, we introduce Gaussian-decay Depth-prior Projection (GDP). This module projects the 2D features into the 3D volume along the line of sight with a Gaussian-decay function, centered around the depth prior. Volumetric semantics is computed by a 3D U-Net. We propagate the hidden 3D U-Net state using the sensor motion and design a novel loss to ensure temporal consistency. We evaluate our approach on the SemanticKITTI dataset and compare it with leading SSC approaches. The SLCF-Net excels in all SSC metrics and shows great temporal consistency.},
  archive   = {C_ICRA},
  author    = {Helin Cao and Sven Behnke},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610602},
  month     = {5},
  pages     = {2767-2773},
  title     = {SLCF-net: Sequential LiDAR-camera fusion for semantic scene completion using a 3D recurrent U-net},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mean shift mask transformer for unseen object instance
segmentation. <em>ICRA</em>, 2760–2766. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Segmenting unseen objects from images is a critical perception skill that a robot needs to acquire. In robot manipulation, it can facilitate a robot to grasp and manipulate unseen objects. Mean shift clustering is a widely used method for image segmentation tasks. However, the traditional mean shift clustering algorithm is not differentiable, making it difficult to integrate it into an end-to-end neural network training framework. In this work, we propose the Mean Shift Mask Transformer (MSMFormer), a new transformer architecture that simulates the von Mises-Fisher (vMF) mean shift clustering algorithm, allowing for the joint training and inference of both the feature extractor and the clustering. Its central component is a hypersphere attention mechanism, which updates object queries on a hypersphere. To illustrate the effectiveness of our method, we apply MSMFormer to unseen object instance segmentation. Our experiments show that MSMFormer achieves competitive performance compared to state-of-the-art methods for unseen object instance segmentation 1 .},
  archive   = {C_ICRA},
  author    = {Yangxiao Lu and Yuqiao Chen and Nicholas Ruozzi and Yu Xiang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610943},
  month     = {5},
  pages     = {2760-2766},
  title     = {Mean shift mask transformer for unseen object instance segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MMAUD: A comprehensive multi-modal anti-UAV dataset for
modern miniature drone threats. <em>ICRA</em>, 2745–2751. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In response to the evolving challenges posed by small unmanned aerial vehicles (UAVs), which possess the potential to transport harmful payloads or independently cause damage, we introduce MMAUD: a comprehensive Multi-Modal Anti-UAV Dataset. MMAUD addresses a critical gap in contemporary threat detection methodologies by focusing on drone detection, UAV-type classification, and trajectory estimation. MMAUD stands out by combining diverse sensory inputs, including stereo vision, various Lidars, Radars, and audio arrays. It offers a unique overhead aerial detection vital for addressing real-world scenarios with higher fidelity than datasets captured on specific vantage points using thermal and RGB. Additionally, MMAUD provides accurate Leica-generated ground truth data, enhancing credibility and enabling confident refinement of algorithms and models, which has never been seen in other datasets. Most existing works do not disclose their datasets, making MMAUD an invaluable resource for developing accurate and efficient solutions. Our proposed modalities are cost-effective and highly adaptable, allowing users to experiment and implement new UAV threat detection tools. Our dataset closely simulates real-world scenarios by incorporating ambient heavy machinery sounds. This approach enhances the dataset’s applicability, capturing the exact challenges faced during proximate vehicular operations. It is expected that MMAUD can play a pivotal role in advancing UAV threat detection, classification, trajectory estimation capabilities, and beyond. Our dataset, codes, and designs will be available in https://ntu-aris.github.io/MMAUD.},
  archive   = {C_ICRA},
  author    = {Shenghai Yuan and Yizhuo Yang and Thien Hoang Nguyen and Thien-Minh Nguyen and Jianfei Yang and Fen Liu and Jianping Li and Han Wang and Lihua Xie},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610957},
  month     = {5},
  pages     = {2745-2751},
  title     = {MMAUD: A comprehensive multi-modal anti-UAV dataset for modern miniature drone threats},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging cycle-consistent anchor points for
self-supervised RGB-d registration. <em>ICRA</em>, 2737–2744. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the rise in consumer depth cameras, a wealth of unlabeled RGB-D data has become available. This prompts the question of how to utilize this data for geometric reasoning of scenes. While many RGB-D registration methods rely on geometric and feature-based similarity, we take a different approach. We use cycle-consistent keypoints as salient points to enforce spatial coherence constraints during matching, improving correspondence accuracy. Additionally, we introduce a novel pose block that combines a GRU recurrent unit with transformation synchronization, blending historical and multi-view data. Our approach surpasses previous self-supervised registration methods on ScanNet and 3DMatch, even outperforming some older supervised methods. We also integrate our components into existing methods, showing their effectiveness.},
  archive   = {C_ICRA},
  author    = {Siddharth Tourani and Jayaram Reddy and Sarvesh Thakur and K Madhava Krishna and Muhammad Haris Khan and N Dinesh Reddy},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610738},
  month     = {5},
  pages     = {2737-2744},
  title     = {Leveraging cycle-consistent anchor points for self-supervised RGB-D registration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stereo-LiDAR depth estimation with deformable propagation
and learned disparity-depth conversion. <em>ICRA</em>, 2729–2736. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate and dense depth estimation with stereo cameras and LiDAR is an important task for automatic driving and robotic perception. While sparse hints from LiDAR points have improved cost aggregation in stereo matching, their effectiveness is limited by the low density and non-uniform distribution. To address this issue, we propose a novel stereo-LiDAR depth estimation network with Semi-Dense hint Guidance, named SDG-Depth. Our network includes a deformable propagation module for generating a semi-dense hint map and a confidence map by propagating sparse hints using a learned deformable window. These maps then guide cost aggregation in stereo matching. To reduce the triangulation error in depth recovery from disparity, especially in distant regions, we introduce a disparity-depth conversion module. Our method is both accurate and efficient. The experimental results on benchmark tests show its superior performance. Our code is available at https://github.com/SJTU-ViSYS/SDG-Depth.},
  archive   = {C_ICRA},
  author    = {Ang Li and Anning Hu and Wei Xi and Wenxian Yu and Danping Zou},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611533},
  month     = {5},
  pages     = {2729-2736},
  title     = {Stereo-LiDAR depth estimation with deformable propagation and learned disparity-depth conversion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RIC: Rotate-inpaint-complete for generalizable scene
reconstruction. <em>ICRA</em>, 2713–2720. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {General scene reconstruction refers to the task of estimating the full 3D geometry and texture of a scene containing previously unseen objects. In many practical applications such as AR/VR, autonomous navigation, and robotics, only a single view of the scene may be available, making the scene reconstruction task challenging. In this paper, we present a method for scene reconstruction by structurally breaking the problem into two steps: rendering novel views via inpainting and 2D to 3D scene lifting. Specifically, we leverage the generalization capability of large visual language models (DALL•E 2) to inpaint the missing areas of scene color images rendered from different views. Next, we lift these inpainted images to 3D by predicting normals of the inpainted image and solving for the missing depth values. By predicting for normals instead of depth directly, our method allows for robustness to changes in depth distributions and scale. With rigorous quantitative evaluation, we show that our method outperforms multiple baselines while providing generalization to novel objects and scenes. Code and data is available at https://samsunglabs.github.io/RIC-project-page/.},
  archive   = {C_ICRA},
  author    = {Isaac Kasahara and Shubham Agrawal and Selim Engin and Nikhil Chavan-Dafle and Shuran Song and Volkan Isler},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611694},
  month     = {5},
  pages     = {2713-2720},
  title     = {RIC: Rotate-inpaint-complete for generalizable scene reconstruction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-resolution planar region extraction for uneven
terrains. <em>ICRA</em>, 2706–2712. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies the problem of extracting planar regions in uneven terrains from unordered point cloud measurements. Such a problem is critical in various robotic applications such as robotic perceptive locomotion. While existing approaches have shown promising results in effectively extracting planar regions from the environment, they often suffer from issues such as low computational efficiency or loss of resolution. To address these issues, we propose a multi-resolution planar region extraction strategy in this paper that balances the accuracy in boundaries and computational efficiency. Our method begins with a pointwise classification preprocessing module, which categorizes all sampled points according to their local geometric properties to facilitate multi-resolution segmentation. Subsequently, we arrange the categorized points using an octree, followed by an in-depth analysis of nodes to finish multi-resolution plane segmentation. The efficiency and robustness of the proposed approach are verified via synthetic and real-world experiments, demonstrating our method’s ability to generalize effectively across various uneven terrains while maintaining real-time performance, achieving frame rates exceeding 35 FPS.},
  archive   = {C_ICRA},
  author    = {Yinghan Sun and Linfang Zheng and Hua Chen and Wei Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610269},
  month     = {5},
  pages     = {2706-2712},
  title     = {Multi-resolution planar region extraction for uneven terrains},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nvblox: GPU-accelerated incremental signed distance field
mapping. <em>ICRA</em>, 2698–2705. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dense, volumetric maps are essential to enable robot navigation and interaction with the environment. To achieve low latency, dense maps are typically computed onboard the robot, often on computationally constrained hardware. Previous works leave a gap between CPU-based systems for robotic mapping which, due to computation constraints, limit map resolution or scale, and GPU-based reconstruction systems which omit features that are critical to robotic path planning, such as computation of the Euclidean Signed Distance Field (ESDF). We introduce a library, nvblox, that aims to fill this gap, by GPU-accelerating robotic volumetric mapping. Nvblox delivers a significant performance improvement over the state of the art, achieving up to a 177× speed-up in surface reconstruction, and up to a 31× improvement in distance field computation, and is available open-source 1 .},
  archive   = {C_ICRA},
  author    = {Alexander Millane and Helen Oleynikova and Emilie Wirbel and Remo Steiner and Vikram Ramasamy and David Tingdahl and Roland Siegwart},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611532},
  month     = {5},
  pages     = {2698-2705},
  title     = {Nvblox: GPU-accelerated incremental signed distance field mapping},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stereo-NEC: Enhancing stereo visual-inertial SLAM
initialization with normal epipolar constraints. <em>ICRA</em>,
2691–2697. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose an accurate and robust initialization approach for stereo visual-inertial SLAM systems. Unlike the current state-of-the-art method, which heavily relies on the accuracy of a pure visual SLAM system to estimate inertial variables without updating camera poses, potentially compromising accuracy and robustness, our approach offers a different solution. We realize the crucial impact of precise gyroscope bias estimation on rotation accuracy. This, in turn, affects trajectory accuracy due to the accumulation of translation errors. To address this, we first independently estimate the gyroscope bias and use it to formulate a maximum a posteriori problem for further refinement. After this refinement, we proceed to update the rotation estimation by performing IMU integration with gyroscope bias removed from gyroscope measurements. We then leverage robust and accurate rotation estimates to enhance translation estimation via 3-DoF bundle adjustment. Moreover, we introduce a novel approach for determining the success of the initialization by evaluating the residual of the normal epipolar constraint. Extensive evaluations on the EuRoC dataset illustrate that our method excels in accuracy and robustness. It outperforms ORB-SLAM3, the current leading stereo visual-inertial initialization method, in terms of absolute trajectory error and relative rotation error, while maintaining competitive computational speed. Notably, even with 5 keyframes for initialization, our method consistently surpasses the state-of-the-art approach using 10 keyframes in rotation accuracy. The open source code is available at https://github.com/ApdowJN/Stereo-NEC.git.},
  archive   = {C_ICRA},
  author    = {Weihan Wang and Chieh Chou and Ganesh Sevagamoorthy and Kevin Chen and Zheng Chen and Ziyue Feng and Youjie Xia and Feiyang Cai and Yi Xu and Philippos Mordohai},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611458},
  month     = {5},
  pages     = {2691-2697},
  title     = {Stereo-NEC: Enhancing stereo visual-inertial SLAM initialization with normal epipolar constraints},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AgriSORT: A simple online real-time tracking-by-detection
framework for robotics in precision agriculture. <em>ICRA</em>,
2675–2682. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The problem of multi-object tracking (MOT) consists in detecting and tracking all the objects in a video sequence while keeping a unique identifier for each object. It is a challenging and fundamental problem for robotics. In precision agriculture the challenge of achieving a satisfactory solution is amplified by extreme camera motion, sudden illumination changes, and strong occlusions. Most modern trackers rely on the appearance of objects rather than motion for association, which can be ineffective when most targets are static objects with the same appearance, as in the agricultural case. To this end, on the trail of SORT [5], we propose AgriSORT, a simple, online, real-time tracking-by-detection pipeline for precision agriculture based only on motion information that allows for accurate and fast propagation of tracks between frames. The main focuses of AgriSORT are efficiency, flexibility, minimal dependencies, and ease of deployment on robotic platforms. We test the proposed pipeline on a novel MOT benchmark specifically tailored for the agricultural context, based on video sequences taken in a table grape vineyard, particularly challenging due to strong self-similarity and density of the instances. Both the code and the dataset are available for future comparisons at: https://github.com/Lio320/AgriSORT},
  archive   = {C_ICRA},
  author    = {Leonardo Saraceni and Ionut M. Motoi and Daniele Nardi and Thomas A. Ciarfuglia},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610231},
  month     = {5},
  pages     = {2675-2682},
  title     = {AgriSORT: A simple online real-time tracking-by-detection framework for robotics in precision agriculture},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-object tracking by hierarchical visual
representations. <em>ICRA</em>, 2667–2674. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a new visual hierarchical representation paradigm for multi-object tracking. It is more effective to discriminate between objects by attending to objects’ compositional visual regions and contrasting with the background contextual information instead of sticking to only the semantic visual cue such as bounding boxes. This compositional-semantic-contextual hierarchy is flexible to be integrated in different appearance-based multi-object tracking methods. We also propose an attention-based visual feature module to fuse the hierarchical visual representations. The proposed method achieves state-of-the-art accuracy and time efficiency among query-based methods on multiple multi-object tracking benchmarks.},
  archive   = {C_ICRA},
  author    = {Jinkun Cao and Jiangmiao Pang and Kris Kitani},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611201},
  month     = {5},
  pages     = {2667-2674},
  title     = {Multi-object tracking by hierarchical visual representations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tracking snake-like robots in the wild using only a single
camera. <em>ICRA</em>, 2660–2666. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot navigation within complex environments requires precise state estimation and localization to ensure robust and safe operations. For ambulating mobile robots like robot snakes, traditional methods for sensing require multiple embedded sensors or markers, leading to increased complexity, cost, and increased points of failure. Alternatively, deploying an external camera in the environment is very easy to do, and marker-less state estimation of the robot from this camera’s images is an ideal solution: both simple and cost-effective. However, the challenge in this process is in tracking the robot under larger environments where the cameras may be moved around without extrinsic calibration, or maybe when in motion (e.g., a drone following the robot). The scenario itself presents a complex challenge: single-image reconstruction of robot poses under noisy observations. In this paper, we address the problem of tracking ambulatory mobile robots from a single camera. The method combines differentiable rendering with the Kalman filter. This synergy allows for simultaneous estimation of the robot’s joint angle and pose while also providing state uncertainty which could be used later on for robust control. We demonstrate the efficacy of our approach on a snake-like robot in both stationary and non-stationary (moving) cameras, validating its performance in both structured and unstructured scenarios. The results achieved show an average error of 0.05 m in localizing the robot’s base position and 6 degrees in joint state estimation. We believe this novel technique opens up possibilities for enhanced robot mobility and navigation in future exploratory and search-and-rescue missions.},
  archive   = {C_ICRA},
  author    = {Jingpei Lu and Florian Richter and Shan Lin and Michael C. Yip},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611438},
  month     = {5},
  pages     = {2660-2666},
  title     = {Tracking snake-like robots in the wild using only a single camera},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Using specularities to boost non-rigid
structure-from-motion. <em>ICRA</em>, 2652–2659. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Non-Rigid Structure-from-Motion (NRSfM) reconstructs the time-varying 3D shape of a deforming object from 2D point correspondences in monocular images. Despite promising use-cases such as the grasping of deformable objects and visual navigation in a non-rigid environment, NRSfM has had limited applications in robotics due to a lack of accuracy. To remedy this, we propose a new method which boosts the accuracy of NRSfM using sparse surface normals. Surface normal information is available from many sources, including structured lighting, homography decomposition of infinitesimal planes and shape priors. However, these sources are not always available. We thus propose a widely available new source of surface normals: the specularities. Our first technical contribution is a method which detects specular highlights and reconstructs the surface normals from it. It assumes that the light source is approximately localised, which is widely applicable in robotics applications such as endoscopy. Our second technical contribution is an NRSfM method which exploits a sparse surface normal set. For that, we propose a novel convex formulation and a globally optimal solution method. Experiments on photo-realistic synthetic data and real household and medical data show that the proposed method outperforms existing NRSfM methods. 1 2 3},
  archive   = {C_ICRA},
  author    = {Agniva Sengupta and Karim Makki and Adrien Bartoli},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610803},
  month     = {5},
  pages     = {2652-2659},
  title     = {Using specularities to boost non-rigid structure-from-motion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Direct 3D model-based object tracking with event camera by
motion interpolation. <em>ICRA</em>, 2645–2651. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Event cameras are recent sensors that measure intensity changes in each pixel asynchronously. It is being used due to lower latency and higher temporal resolution compared to traditional frame-based camera. We propose a method of 3D model-based object tracking directly from events captured by event camera. To enable reliable and accurate tracking of objects, we use a new event representation and predict brightness increment images with motion interpolation. Results of object tracking show the new methods significantly improves tracking duration and robustness, both for perspective and fisheye cameras. Our implementation succeeds in tracking objects when the camera speed is reaching 2 m/s.},
  archive   = {C_ICRA},
  author    = {Y. Kang and G. Caron and R. Ishikawa and A. Escande and K. Chappellet and R. Sagawa and T. Oishi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611576},
  month     = {5},
  pages     = {2645-2651},
  title     = {Direct 3D model-based object tracking with event camera by motion interpolation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ensemble latent space roadmap for improved robustness in
visual action planning. <em>ICRA</em>, 2638–2644. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Planning in learned latent spaces helps to decrease the dimensionality of raw observations. In this work, we propose to leverage the ensemble paradigm to enhance the robustness of latent planning systems. We rely on our Latent Space Roadmap (LSR) framework, which builds a graph in a learned structured latent space to perform planning. Given multiple LSR framework instances, that differ either on their latent spaces or on the parameters for constructing the graph, we use the action information as well as the embedded nodes of the produced plans to define similarity measures. These are then utilized to select the most promising plans. We validate the performance of our Ensemble LSR (ENS-LSR) on simulated box stacking and grape harvesting tasks as well as on a real-world robotic T-shirt folding experiment.},
  archive   = {C_ICRA},
  author    = {Martina Lippi and Michael C. Welle and Andrea Gasparri and Danica Kragic},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611385},
  month     = {5},
  pages     = {2638-2644},
  title     = {Ensemble latent space roadmap for improved robustness in visual action planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal containment control of multiple quadrotors via
reinforcement learning*. <em>ICRA</em>, 2632–2637. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper explores the optimal containment control problem for nonlinear and underactuated quadrotors with multiple team leaders governed by nonlinear dynamics, employing the reinforcement learning. A cascade controller is formulated, comprising a position control component to ensure containment achievement and an attitude control component to govern rotational channel. The proposed optimal control protocols derived from historical data collected from quadrotor systems without requirement for exact knowledge of vehicle dynamics. The simulation illustrates the effectiveness of the proposed controller in managing a quadrotor team with multiple leaders.},
  archive   = {C_ICRA},
  author    = {Ming Cheng and Hao Liu and Deyuan Liu and Haibo Gu and Xiangke Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611262},
  month     = {5},
  pages     = {2632-2637},
  title     = {Optimal containment control of multiple quadrotors via reinforcement learning*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A non-cubic space-filling modular robot. <em>ICRA</em>,
2624–2631. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Space-filling building blocks of diverse shape permeate nature at all levels of organization, from atoms to honeycombs, and have proven useful in artificial systems, from molecular containers to clay bricks. But, despite the wide variety of space-filling polyhedra known to mathematics, only the cube has been explored in robotics. Thus, here we roboticize a non-cubic space-filling shape: the rhombic dodecahedron. This geometry offers an appealing alternative to cubes as it greatly simplifies rotational motion of one cell about the edge of another, and increases the number of neighbors each cell can communicate with and hold on to. To better understand the challenges and opportunities of these and other space-filling machines, we manufactured 48 rhombic dodecahedral cells and used them to build various superstructures. We report locomotive ability of some of the structures we built, and discuss the dis/advantages of the different designs we tested. We also introduce a strategy for genderless passive docking of cells that generalizes to any polyhedra with radially symmetrical faces. Future work will allow the cells to freely roll/rotate about one another so that they may realize the full potential of their unique shape.},
  archive   = {C_ICRA},
  author    = {Tyler Hummer and Sam Kriegman},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611176},
  month     = {5},
  pages     = {2624-2631},
  title     = {A non-cubic space-filling modular robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AG-cvg: Coverage planning with a mobile recharging UGV and
an energy-constrained UAV. <em>ICRA</em>, 2617–2623. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present an approach for coverage path planning for a team of an energy-constrained Unmanned Aerial Vehicle (UAV) and an Unmanned Ground Vehicle (UGV). Both the UAV and the UGV have predefined areas that they have to cover. The goal is to perform complete coverage by both robots while minimizing the coverage time. The UGV can also serve as a mobile recharging station. The UAV and UGV need to occasionally rendezvous for recharging. We propose a heuristic method to address this NP-Hard planning problem. Our approach involves initially determining coverage paths without factoring in energy constraints. Subsequently, we cluster segments of these paths and employ graph matching to assign UAV clusters to UGV clusters for efficient recharging management. We perform numerical analysis on real-world coverage applications and show that compared with a greedy approach our method reduces rendezvous overhead on average by 11.33%. We demonstrate proof-of-concept with a team of a VOXL m500 drone and a Clearpath Jackal ground vehicle, providing a complete system from the offline algorithm to the field execution.},
  archive   = {C_ICRA},
  author    = {Nare Karapetyan and Ahmad Bilal Asghar and Amisha Bhaskar and Guangyao Shi and Dinesh Manocha and Pratap Tokekar},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610339},
  month     = {5},
  pages     = {2617-2623},
  title     = {AG-cvg: Coverage planning with a mobile recharging UGV and an energy-constrained UAV},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enabling large-scale heterogeneous collaboration with
opportunistic communications. <em>ICRA</em>, 2610–2616. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-robot collaboration in large-scale environments with limited-sized teams and without external infrastructure is challenging, since the software framework required to support complex tasks must be robust to unreliable and intermittent communication links. In this work, we present MOCHA (Multi-robot Opportunistic Communication for Heterogeneous Collaboration), a framework for resilient multi-robot collaboration that enables large-scale exploration in the absence of continuous communications. MOCHA is based on a gossip communication protocol that allows robots to interact opportunistically whenever communication links are available, propagating information on a peer-to-peer basis. We demonstrate the performance of MOCHA through real-world experiments with commercial-off-the-shelf (COTS) communication hardware. We further explore the system’s scalability in simulation, evaluating the performance of our approach as the number of robots increases and communication ranges vary. Finally, we demonstrate how MOCHA can be tightly integrated with the planning stack of autonomous robots. We show a communication-aware planning algorithm for a high-altitude aerial robot executing a collaborative task while maximizing the amount of information shared with ground robots.The source code for MOCHA and the high-altitude UAV planning system is available open source 1 .},
  archive   = {C_ICRA},
  author    = {Fernando Cladera and Zachary Ravichandran and Ian D. Miller and M. Ani Hsieh and C. J. Taylor and Vijay Kumar},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611469},
  month     = {5},
  pages     = {2610-2616},
  title     = {Enabling large-scale heterogeneous collaboration with opportunistic communications},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simultaneous time synchronization and mutual localization
for multi-robot system. <em>ICRA</em>, 2603–2609. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mutual localization stands as a foundational component within various domains of multi-robot systems. Nevertheless, in relative pose estimation, time synchronization is usually underappreciated and rarely addressed, although it significantly influences estimation accuracy. In this paper, we introduce time synchronization into mutual localization to recover the time offset and relative poses between robots simultaneously. Under a constant velocity assumption in a short time, we fuse time offset estimation with our previous bearing-based mutual localization by a novel error representation. Based on the error model, we formulate a joint optimization problem and utilize semi-definite relaxation (SDR) to furnish a lossless relaxation. By solving the relaxed problem, time synchronization and relative pose estimation can be achieved when time drift between robots is limited. To enhance the application range of time offset estimation, we further propose an iterative method to recover the time offset from coarse to fine. Comparisons between the proposed method and the existing ones through extensive simulation tests present prominent benefits of time synchronization on mutual localization. Moreover, real-world experiments are conducted to show the practicality and robustness.},
  archive   = {C_ICRA},
  author    = {Xiangyong Wen and Yingjian Wang and Xi Zheng and Kaiwei Wang and Chao Xu and Fei Gao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610915},
  month     = {5},
  pages     = {2603-2609},
  title     = {Simultaneous time synchronization and mutual localization for multi-robot system},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Learning decentralized flocking controllers with
spatio-temporal graph neural network. <em>ICRA</em>, 2596–2602. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently a line of research has delved into the use of graph neural networks (GNNs) for decentralized control in swarm robotics. However, it has been observed that relying solely on the states of immediate neighbors is insufficient to imitate a centralized control policy. To address this limitation, prior studies proposed incorporating L-hop delayed states into the computation. While this approach shows promise, it can lead to a lack of consensus among distant flock members and the formation of small clusters, consequently failing cohesive flocking behaviors. Instead, our approach leverages spatiotemporal GNN, named STGNN that encompasses both spatial and temporal expansions. The spatial expansion collects delayed states from distant neighbors, while the temporal expansion incorporates previous states from immediate neighbors. The broader information gathered from both expansions results in more effective and accurate predictions. We develop an expert algorithm for controlling a swarm of robots and employ imitation learning to train our decentralized STGNN model based on the expert algorithm. We simulate the proposed STGNN approach in various settings, demonstrating its decentralized capacity to emulate the global expert algorithm. Further, we implemented our approach to achieve cohesive flocking, leader following, and obstacle avoidance by a group of Crazyflie drones. The performance of STGNN underscores its potential as an effective and reliable approach for achieving cohesive flocking, leader following, and obstacle avoidance tasks.},
  archive   = {C_ICRA},
  author    = {Siji Chen and Yanshen Sun and Peihan Li and Lifeng Zhou and Chang-Tien Lu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610627},
  month     = {5},
  pages     = {2596-2602},
  title     = {Learning decentralized flocking controllers with spatio-temporal graph neural network},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assessing reputation to improve team performance in
heterogeneous multi-robot coverage. <em>ICRA</em>, 2571–2577. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When agents in a multi-robot team have limited knowledge about their relative performance, their teammates, or the environment, robots must observe individual performance variations and adapt accordingly. We propose robot reputation to assess the historical performance of agents and make future adaptations in a persistent coverage task. We consider a heterogeneous multi-robot team, where robots are equipped with different capabilities to serve discrete events in an environment. We utilize a heterogeneous coverage control approach to partition the space according to robot capabilities and the estimated probability density, such that the robot is responsible for serving the events in its assigned region. As the team serves events, we assign each robot a reputation, which is then used to adjust the size of a robot’s region, thus adjusting the amount of space a robot is responsible for serving. Our simulations show that using reputation to weigh the size of the Voronoi cells outperforms the case where we neglect reputation.},
  archive   = {C_ICRA},
  author    = {Mela Coffey and Alyssa Pierson},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611134},
  month     = {5},
  pages     = {2571-2577},
  title     = {Assessing reputation to improve team performance in heterogeneous multi-robot coverage},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Knowledge acquisition plans: Generation, combination, and
execution. <em>ICRA</em>, 2556–2562. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper contemplates the possibility of asking robots questions and having them use their ability to go out into the environment and probe it, in combination with what they already know of the world, to provide answers. We describe a method whereby a robot system efficiently answers such questions on the basis of reasoning about observations as they are made, interrelationships between multiple pieces of evidence, and what they imply.A central idea in the approach is to maintain a separation of concerns so that managing ‘what is known’ is decoupled from ‘how it is learned’. This idea is realized in a graph-based representation well-suited to algorithmic manipulation and composition, exposing synergies rife for optimization. We show how to use this representation to leverage both informational overlap between multiple simultaneous queries and availability of multiple robots working in concert to answer those queries. We demonstrate these ideas in a simple case study and present data illustrating how plan quality (in terms of cost to execute) can be improved through an optimization operation that is robot agnostic.},
  archive   = {C_ICRA},
  author    = {Dylan A. Shell and Jason M. O’Kane},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610628},
  month     = {5},
  pages     = {2556-2562},
  title     = {Knowledge acquisition plans: Generation, combination, and execution},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Verifiable learned behaviors via motion primitive
composition: Applications to scooping of granular media. <em>ICRA</em>,
2549–2555. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A robotic behavior model that can reliably generate behaviors from natural language inputs in real time would substantially expedite the adoption of industrial robots due to enhanced system flexibility. To facilitate these efforts, we construct a framework in which learned behaviors, created by a natural language abstractor, are verifiable by construction. Leveraging recent advancements in motion primitives and probabilistic verification, we construct a natural-language behavior abstractor that generates behaviors by synthesizing a directed graph over the provided motion primitives. If these component motion primitives are constructed according to the criteria we specify, the resulting behaviors are probabilistically verifiable. We demonstrate this verifiable behavior generation capacity in both simulation on an exploration task and on hardware with a robot scooping granular media.},
  archive   = {C_ICRA},
  author    = {Andrew Benton and Eugen Solowjow and Prithvi Akella},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611279},
  month     = {5},
  pages     = {2549-2555},
  title     = {Verifiable learned behaviors via motion primitive composition: Applications to scooping of granular media},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High precision paint deposition modeling considering
variable posture of spray painting robot. <em>ICRA</em>, 2542–2548. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study developed a high-precision paint deposition model that considers the position and direction of a spray-painting gun. Our angle-specific paint deposition model focused on the change in paint deposition due to the change in the painting angle; however, there was a problem with its versatility. We analyzed this problem, and the solution was achieved by separately modeling changes in the film thickness distribution using impact angle and spray distance, which were previously modeled together. For higher accuracy, a special function was proposed to convert the three-dimensional vector into two-dimensional coordinate values in the distribution function upper plane. To confirm the validity of our model, a painting test on an L-shaped surface was conducted, and the measured and predicted values were compared. The L-shaped surface is a typical shape in which the film thickness distribution changes with the angle; a complex path with varying distances and angles was employed. The results confirmed that the predicted values agreed well with the measured values in the L-shaped surface painting test, validating the developed model.},
  archive   = {C_ICRA},
  author    = {Genichiro Tanaka and Yoshinobu Takahashi and Hiroyasu Iwata},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610968},
  month     = {5},
  pages     = {2542-2548},
  title     = {High precision paint deposition modeling considering variable posture of spray painting robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Skill transfer for temporal task specification.
<em>ICRA</em>, 2535–2541. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deploying robots in real-world environments, such as households and manufacturing lines, requires generalization across novel task specifications without violating safety constraints. Linear temporal logic (LTL) is a widely used task specification language with a compositional grammar that naturally induces commonalities among tasks while preserving safety guarantees. However, most prior work on reinforcement learning with LTL specifications treats every new task independently, thus requiring large amounts of training data to generalize. We propose LTL-Transfer, a zero-shot transfer algorithm that composes task-agnostic skills learned during training to safely satisfy a wide variety of novel LTL task specifications. Experiments in Minecraft-inspired domains show that after training on only 50 tasks, LTL-Transfer can solve over 90% of 100 challenging unseen tasks and 100% of 300 commonly used novel tasks without violating any safety constraints. We deployed LTL-Transfer at the task-planning level of a quadruped mobile manipulator to demonstrate its zero-shot transfer ability for fetch-and-deliver and navigation tasks.},
  archive   = {C_ICRA},
  author    = {Jason Xinyu Liu and Ankit Shah and Eric Rosen and Mingxi Jia and George Konidaris and Stefanie Tellex},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611432},
  month     = {5},
  pages     = {2535-2541},
  title     = {Skill transfer for temporal task specification},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active inference for reactive temporal logic motion
planning. <em>ICRA</em>, 2520–2526. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reactive planning enables the robots to deal with dynamic events in uncertain environments. However, existing methods heavily rely on the predefined hard-coded robot behaviors, e.g, a pre-coded temporal logic formula that specifies how robot should react. Little attention has been paid for autonomous generation of reactive tasks specifications during the runtime. As a first attempt towards this goal, this work develops a real-time decision-making and motion planning framework. It allows the robot to follow a global task planned offline while taking proactive decisions and generating temporal logic specifications for local reactive tasks when encountering dynamic events. Specifically, inspired by the causal knowledge graph, a proposition graph is developed, based on which the decision module encode the environment and the task as the Boolean logic and linear temporal logic (LTL), respectively. Based on the established proposition graph and perceived environment, the agent can autonomously generate an LTL formula to realize the local temporary task. A joint sampling algorithm is then developed, in which the automaton states of local and global task are jointly considered to generate a feasible planning that satisfies both global and local tasks. Experiments demonstrate the effectiveness of the proposed decision-making and motion planning.},
  archive   = {C_ICRA},
  author    = {Ziyang Chen and Zhangli Zhou and Lin Li and Zhen Kan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611484},
  month     = {5},
  pages     = {2520-2526},
  title     = {Active inference for reactive temporal logic motion planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stochastic games for interactive manipulation domains.
<em>ICRA</em>, 2513–2519. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As robots become more prevalent, the complexity of robot-robot, robot-human, and robot-environment interactions increases. In these interactions, a robot needs to consider not only the effects of its own actions, but also the effects of other agents’ actions and the possible interactions between agents. Previous works have considered reactive synthesis, where the human/environment is modeled as a deterministic, adversarial agent; as well as probabilistic synthesis, where the human/environment is modeled via a Markov chain. While they provide strong theoretical frameworks, there are still many aspects of human-robot interaction that cannot be fully expressed and many assumptions that must be made in each model. In this work, we propose stochastic games as a general model for human-robot interaction, which subsumes the expressivity of all previous representations. In addition, it allows us to make fewer modeling assumptions and leads to more natural and powerful models of interaction. We introduce the semantics of this abstraction and show how existing tools can be utilized to synthesize strategies to achieve complex tasks with guarantees. Further, we discuss the current computational limitations and improve the scalability by two orders of magnitude by a new way of constructing models for PRISM-games.},
  archive   = {C_ICRA},
  author    = {Karan Muvvala and Andrew M. Wells and Morteza Lahijanian and Lydia E. Kavraki and Moshe Y. Vardi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611623},
  month     = {5},
  pages     = {2513-2519},
  title     = {Stochastic games for interactive manipulation domains},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust MITL planning under uncertain navigation times.
<em>ICRA</em>, 2498–2504. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In environments like offices, the duration of a robot’s navigation between two locations may vary over time. For instance, reaching a kitchen may take more time during lunchtime since the corridors are crowded with people heading the same way. In this work, we address the problem of routing in such environments with tasks expressed in Metric Interval Temporal Logic (MITL) – a rich robot task specification language that allows us to capture explicit time requirements. Our objective is to find a strategy that maximizes the temporal robustness of the robot’s MITL task. As the first step towards a solution, we define a Mixed-integer linear programming approach to solving the task planning problem over a Varying Weighted Transition System, where navigation durations are deterministic but vary depending on the time of day. Then, we apply this planner to optimize for MITL temporal robustness in Markov Decision Processes, where the navigation durations between physical locations are uncertain, but the time-dependent distribution over possible delays is known. Finally, we develop a receding horizon planner for Markov Decision Processes that preserves guarantees over MITL temporal robustness. We show the scalability of our planning algorithms in simulations of robotic tasks.},
  archive   = {C_ICRA},
  author    = {Alexis Linard and Anna Gautier and Daniel Duberg and Jana Tumova},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611704},
  month     = {5},
  pages     = {2498-2504},
  title     = {Robust MITL planning under uncertain navigation times},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OSCaR: An origami-inspired shape-changing robot for ground
coverage tasks. <em>ICRA</em>, 2491–2497. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a novel origami-inspired shape-changing robot OSCaR. The objective is to enhance the adaptability of vehicles engaged in ground coverage tasks, such as floor cleaning. The robot exhibits two distinct configurations: it can fold itself for agile navigation through tight spaces, and unfold to cover larger areas efficiently. The folding pattern has a deploy-to-stow ratio of 3 in the width dimension, and a kinematic model is established to simulate the deployment process for the pattern. The hinge design employs rolling contact elements to mitigate collision among the panels, particularly in regions with multiple colinear crease lines. Furthermore, the design exhibits one degree of freedom and features pivots, making it easy to actuate with motors. The system design of the prototype is also presented, including its structure, an embedded hardware system, and upper computer software. The results show that the robot has great adaptability in complex environments.},
  archive   = {C_ICRA},
  author    = {Zirui Fan and Hongying Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610212},
  month     = {5},
  pages     = {2491-2497},
  title     = {OSCaR: An origami-inspired shape-changing robot for ground coverage tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Non-intrusive LiDAR protection module emulating bio-inspired
wiping motion for outdoor unmanned vehicles. <em>ICRA</em>, 2470–2476.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we have developed a protection module for Light Detection and Ranging (LiDAR) sensors used in outdoor unmanned vehicles. Bio-inspired wiping motion was figured to have more efficient and excellent wiping performance than conventional cleaning methods for LiDAR sensors. An water wiping experiment confirmed that the finger wiping motion removed 35% more water than the translational wiping motion. Also, the theoretical analysis for the existence of an optimal rotational speed at maximum wiping performance was verified to be consistent with the experiment. The LiDAR distortion experiment results demonstrated no data distortion, showing an average error of up to 0.40% for detecting obstacles even when the acrylic cover rotates. Finally, a contamination protection experiment was conducted for water, powder, soil, and mud. As a result, although there was a change in the number of pointcloud and a decrease in the intensity of the sensor data after contamination, it was validated that the number of pointclouds and average intensity of data could be restored to at least 97% and 67% after being cleaned.},
  archive   = {C_ICRA},
  author    = {Youngrae Kim and Seunghyun Lim and Lee Hanmin and Seokchan Kim and Ji-Chul Kim and Dongwon Yun},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610438},
  month     = {5},
  pages     = {2470-2476},
  title     = {Non-intrusive LiDAR protection module emulating bio-inspired wiping motion for outdoor unmanned vehicles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design of a towing system by multi autonomous sailboats*.
<em>ICRA</em>, 2463–2469. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For researchers or administrators of relevant institutions who need to collect hydrological data of a certain water area, using autonomous sailboats to tow floating detection equipment is an energy-saving and convenient scheme for deploying detectors. However, due to the limited pulling force provided by a single autonomous sailboat, this scheme is not suitable for floating equipment with large masses. This paper proposes a new approach for multiple autonomous sailboats to tow floating objects. A system of linear arrangement and connection of two autonomous sailboats is considered an appropriate solution for towing heavy floating objects because of its ability to provide greater pulling force. The main part of the article introduces a new design of multi sailboat towing system that can tow floating objects to sail with or against wind. Repetitive experiments have been conducted at the test site equipped with a motion capture system to find the best strategy to control the sails and rudder, in order to increase the towing system’s pulling force and tacking success rate. Three connection modes are proposed, compared, and tested. The best one is applied to the sailboat’s towing system and improves its performance.},
  archive   = {C_ICRA},
  author    = {Cheng Liang and Bairun Lin and Huihuan Qian},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611188},
  month     = {5},
  pages     = {2463-2469},
  title     = {Design of a towing system by multi autonomous sailboats*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HyperLeg: Biomechanics-inspired high-DOF leg and toe
mechanism for highly dynamic motions. <em>ICRA</em>, 2456–2462. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A human foot with high degrees of freedom (DOF) that has multi-DOF toe joints and a two-DOF ankle provides multiple benefits, such as increased stride length and walking speed, impact mitigation, and enhanced balancing. However, creating such mechanisms for legged robots has been challenging due to increased complexity, heavy weight, and vulnerability to impact. In this paper, a novel leg and toe mechanism inspired by human biomechanics, featuring a one-DOF knee joint, two-DOF ankle joint, and one-DOF toe joint, is developed. All actuators are located at the proximal part of the thigh frame to minimize the distal mass. High payload timing belts and unique linkage mechanisms are utilized in the transmission to achieve high backdrivability and high joint stiffness. Actuation torques are intentionally coupled inspired by human anatomy, enduring the high propulsive force to the ground for dynamic movements, such as jumping. The implemented leg and toe mechanisms weigh 8.16 kg, and the height from the ground to the hip center is 786 mm. The proposed mechanism has been proven to be effective through force test and distance jump experiments.},
  archive   = {C_ICRA},
  author    = {Do-Yun Kim and Seong-Ho Yun and Joong-Kyung Lee and Jongjun Yoon and Dongyun Nam and Chan-Young Maeng and Yong-Jae Kim},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610527},
  month     = {5},
  pages     = {2456-2462},
  title     = {HyperLeg: Biomechanics-inspired high-DOF leg and toe mechanism for highly dynamic motions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Development of the assembling system for structure
transformable humanoid with attach-lock-detachable magnetic coupling.
<em>ICRA</em>, 2433–2439. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose the method to adapt humanoids the ability to change the body structures that modular robots have by using Attach-Lock-Detachable Magnetic Couplings(ALDMag) to give the ability to detach and attach the robot body with an arm-type robot, and the system to manage the connection state of modularized body elements. Robots and we can use the ALDMag to attach and detach mechanical and electrical connections without actuators. Using xacro for writing the file of the robot model description of each module, we can construct a system that allows the robot to attach and detach modules during task operation. We demonstrated the effectiveness of the proposed method by achieving assembly experiments of a small robot with a life-size arm and experiments with environmental contacts by the small robot.},
  archive   = {C_ICRA},
  author    = {Tasuku Makabe and Kei Okada and Masayuki Inaba},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611574},
  month     = {5},
  pages     = {2433-2439},
  title     = {Development of the assembling system for structure transformable humanoid with attach-lock-detachable magnetic coupling},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A GP-based robust motion planning framework for agile
autonomous robot navigation and recovery in unknown environments.
<em>ICRA</em>, 2418–2424. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For autonomous mobile robots, uncertainties in the environment and system model can lead to failure in the motion planning pipeline, resulting in potential collisions. In order to achieve a high level of robust autonomy, these robots should be able to proactively predict and recover from such failures. To this end, we propose a Gaussian Process (GP) based model for proactively detecting the risk of future motion planning failure. When this risk exceeds a certain threshold, a recovery behavior is triggered that leverages the same GP model to find a safe state from which the robot may continue towards the goal. The proposed approach is trained in simulation only and can generalize to real world environments on different robotic platforms. Simulations and physical experiments demonstrate that our framework is capable of both predicting planner failures and recovering the robot to states where planner success is likely, all while producing agile motion.},
  archive   = {C_ICRA},
  author    = {Nicholas Mohammad and Jacob Higgins and Nicola Bezzo},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610382},
  month     = {5},
  pages     = {2418-2424},
  title     = {A GP-based robust motion planning framework for agile autonomous robot navigation and recovery in unknown environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrating predictive motion uncertainties with
distributionally robust risk-aware control for safe robot navigation in
crowds. <em>ICRA</em>, 2410–2417. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ensuring safe navigation in human-populated environments is crucial for autonomous mobile robots. Although recent advances in machine learning offer promising methods to predict human trajectories in crowded areas, it remains unclear how one can safely incorporate these learned models into a control loop due to the uncertain nature of human motion, which can make predictions of these models imprecise. In this work, we address this challenge and introduce a distributionally robust chance-constrained model predictive control (DRCC-MPC) which: (i) adopts a probability of collision as a pre-specified, interpretable risk metric, and (ii) offers robustness against discrepancies between actual human trajectories and their predictions. We consider the risk of collision in the form of a chance constraint, providing an interpretable measure of robot safety. To enable real-time evaluation of chance constraints, we consider conservative approximations of chance constraints in the form of distributionally robust Conditional Value at Risk constraints. The resulting formulation offers computational efficiency as well as robustness with respect to out-of-distribution human motion. With the parallelization of a sampling-based optimization technique, our method operates in real-time, demonstrating successful and safe navigation in a number of case studies with real-world pedestrian data.},
  archive   = {C_ICRA},
  author    = {Kanghyun Ryu and Negar Mehr},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610404},
  month     = {5},
  pages     = {2410-2417},
  title     = {Integrating predictive motion uncertainties with distributionally robust risk-aware control for safe robot navigation in crowds},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IBBT: Informed batch belief trees for motion planning under
uncertainty. <em>ICRA</em>, 2403–2409. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we propose the Informed Batch Belief Trees (IBBT) algorithm for motion planning under motion and sensing uncertainties. The original stochastic motion planning problem is divided into a deterministic motion planning problem and a graph search problem. First, we solve the deterministic planning problem using Rapidly-exploring Random Graph (RRG) to construct a nominal trajectory graph. Then, an informed cost-to-go heuristic for the original problem is computed based on the nominal trajectory graph. Finally, we grow a belief tree by searching the graph using the proposed heuristic. IBBT interleaves batch state sampling, nominal trajectory graph construction, heuristic computing, and searching over the graph to find belief space motion plans. IBBT is an anytime, incremental algorithm. With an increasing number of batches of samples added to the graph, the algorithm finds improved plans. IBBT is efficient by reusing results between sequential iterations. The belief tree search is an ordered search guided by an informed heuristic. We test IBBT in different planning environments. Our numerical investigation confirms that IBBT finds non-trivial motion plans and is faster compared with previous similar methods.},
  archive   = {C_ICRA},
  author    = {Dongliang Zheng and Panagiotis Tsiotras},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610244},
  month     = {5},
  pages     = {2403-2409},
  title     = {IBBT: Informed batch belief trees for motion planning under uncertainty},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MTG: Mapless trajectory generator with traversability
coverage for outdoor navigation. <em>ICRA</em>, 2396–2402. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel learning-based trajectory generation algorithm for outdoor robot navigation. Our goal is to compute collision-free paths that also satisfy the environment-specific traversability constraints. Our approach is designed for global planning using limited onboard robot perception in mapless environments while ensuring comprehensive coverage of all traversable directions. Our formulation uses a Conditional Variational Autoencoder (CVAE) generative model that is enhanced with traversability constraints and an optimization formulation used for the coverage. We highlight the benefits of our approach over state-of-the-art trajectory generation approaches and demonstrate its performance in challenging and large outdoor environments, including around buildings, across intersections, along trails, and off-road terrain, using a Clearpath Husky and a Boston Dynamics Spot robot. In practice, our approach results in a 6% improvement in coverage of traversable areas and an 89% reduction in trajectory portions residing in non-traversable regions. Our video is here: https://youtu.be/3eJ2soAzXnU},
  archive   = {C_ICRA},
  author    = {Jing Liang and Peng Gao and Xuesu Xiao and Adarsh Jagan Sathyamoorthy and Mohamed Elnoor and Ming C. Lin and Dinesh Manocha},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611319},
  month     = {5},
  pages     = {2396-2402},
  title     = {MTG: Mapless trajectory generator with traversability coverage for outdoor navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Autonomous 3D exploration in large-scale environments with
dynamic obstacles. <em>ICRA</em>, 2389–2395. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Exploration in dynamic and uncertain real-world environments is an open problem in robotics and it constitutes a foundational capability of autonomous systems operating in most of the real-world. While 3D exploration planning has been extensively studied, the environments are assumed static or only reactive collision avoidance is carried out. We propose a novel approach to not only avoid dynamic obstacles but also include them in the plan itself, to deliberately exploit the dynamic environment in the agent’s favor. The proposed planner, Dynamic Autonomous Exploration Planner (DAEP), extends AEP to explicitly plan with respect to dynamic obstacles. Furthermore, addressing prior errors within AEP in DAEP has resulted in enhanced exploration within static environments. To thoroughly evaluate exploration planners in such settings we propose a new enhanced benchmark suite with several dynamic environments, including large-scale outdoor environments. DAEP outperforms state-of-the-art planners in dynamic and large-scale environments and is shown to be more effective at both exploration and collision avoidance.},
  archive   = {C_ICRA},
  author    = {Emil Wiman and Ludvig Widén and Mattias Tiger and Fredrik Heintz},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610996},
  month     = {5},
  pages     = {2389-2395},
  title     = {Autonomous 3D exploration in large-scale environments with dynamic obstacles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online adaptation of sampling-based motion planning with
inaccurate models. <em>ICRA</em>, 2382–2388. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic manipulation relies on analytical or learned models to simulate the system dynamics. These models are often inaccurate and based on offline information, so that the robot planner is unable to cope with mismatches between the expected and the actual behavior of the system (e.g., the presence of an unexpected obstacle). In these situations, the robot should use information gathered online to correct its planning strategy and adapt to the actual system response. We propose a sampling-based motion planning approach that uses an estimate of the model error and online observations to correct the planning strategy at each new replanning. Our approach adapts the cost function and the sampling bias of a kinodynamic motion planner when the outcome of the executed transitions is different from the expected one (e.g., when the robot unexpectedly collides with an obstacle) so that future trajectories will avoid unreliable motions. To infer the properties of a new transition, we introduce the notion of context-awareness, i.e., we store local environment information for each executed transition and avoid new transitions with context similar to previous unreliable ones. This is helpful for leveraging online information even if the simulated transitions are far (in the state-and-action space) from the executed ones. Simulation and experimental results show that the proposed approach increases the success rate in execution and reduces the number of replannings needed to reach the goal.},
  archive   = {C_ICRA},
  author    = {Marco Faroni and Dmitry Berenson},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610323},
  month     = {5},
  pages     = {2382-2388},
  title     = {Online adaptation of sampling-based motion planning with inaccurate models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating 3D uncertainty field: Quantifying uncertainty for
neural radiance fields. <em>ICRA</em>, 2375–2381. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current methods based on Neural Radiance Fields (NeRF) significantly lack the capacity to quantify uncertainty in their predictions, particularly on the unseen space including the occluded and outside scene content. This limitation hinders their extensive applications in robotics, where the reliability of model predictions has to be considered for tasks such as robotic exploration and planning in unknown environments. To address this, we propose a novel approach to estimate a 3D Uncertainty Field based on the learned incomplete scene geometry, which explicitly identifies these unseen regions. By considering the accumulated transmittance along each camera ray, our Uncertainty Field infers 2D pixel-wise uncertainty, exhibiting high values for rays directly casting towards occluded or outside the scene content. To quantify the uncertainty on the learned surface, we model a stochastic radiance field. Our experiments demonstrate that our approach is the only one that can explicitly reason about high uncertainty both on 3D unseen regions and its involved 2D rendered pixels, compared with recent methods. Furthermore, we illustrate that our designed uncertainty field is ideally suited for real-world robotics tasks, such as next-best-view selection.},
  archive   = {C_ICRA},
  author    = {Jianxiong Shen and Ruijie Ren and Adria Ruiz and Francesc Moreno-Noguer},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611116},
  month     = {5},
  pages     = {2375-2381},
  title     = {Estimating 3D uncertainty field: Quantifying uncertainty for neural radiance fields},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Constrained hierarchical monte carlo belief-state planning.
<em>ICRA</em>, 2368–2374. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Optimal plans in Constrained Partially Observable Markov Decision Processes (CPOMDPs) maximize reward objectives while satisfying hard cost constraints, generalizing safe planning under state and transition uncertainty. Unfortunately, online CPOMDP planning is extremely difficult in large or continuous problem domains. In many large robotic domains, hierarchical decomposition can simplify planning by using tools for low-level control given high-level action primitives (options). We introduce Constrained Options Belief Tree Search (COBeTS) to leverage this hierarchy and scale online search-based CPOMDP planning to large robotic problems. We show that if primitive option controllers are defined to satisfy assigned constraint budgets, then COBeTS will satisfy constraints anytime. Otherwise, COBeTS will guide the search towards a safe sequence of option primitives, and hierarchical monitoring can be used to achieve runtime safety. We demonstrate COBeTS in several safety-critical, constrained partially observable robotic domains, showing that it can plan successfully in continuous CPOMDPs while non-hierarchical baselines cannot.},
  archive   = {C_ICRA},
  author    = {Arec Jamgochian and Hugo Buurmeijer and Kyle H. Wray and Anthony Corso and Mykel J. Kochenderfer},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611223},
  month     = {5},
  pages     = {2368-2374},
  title     = {Constrained hierarchical monte carlo belief-state planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stochastic implicit neural signed distance functions for
safe motion planning under sensing uncertainty. <em>ICRA</em>,
2360–2367. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion planning under sensing uncertainty is critical for robots in unstructured environments, to guarantee safety for both the robot and any nearby humans. Most work on planning under uncertainty does not scale to high-dimensional robots such as manipulators, assumes simplified geometry of the robot or environment, or requires per-object knowledge of noise. Instead, we propose a method that directly models sensor-specific aleatoric uncertainty to find safe motions for high-dimensional systems in complex environments, without exact knowledge of environment geometry. We combine a novel implicit neural model of stochastic signed distance functions with a hierarchical optimization-based motion planner to plan low- risk motions without sacrificing path quality. Our method also explicitly bounds the risk of the path, offering trustworthiness. We empirically validate that our method produces safe motions and accurate risk bounds and is safer than baseline approaches.},
  archive   = {C_ICRA},
  author    = {Carlos Quintero-Peña and Wil Thomason and Zachary Kingston and Anastasios Kyrillidis and Lydia E. Kavraki},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610773},
  month     = {5},
  pages     = {2360-2367},
  title     = {Stochastic implicit neural signed distance functions for safe motion planning under sensing uncertainty},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimized design and fabrication of skeletal muscle
actuators for bio-syncretic robots. <em>ICRA</em>, 2353–2359. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, bio-syncretic robots actuated by living materials have received widespread attention. Among the common living materials, engineered skeletal muscle tissue (eSKT) has been the focus of researchers due to its high contraction force and good controllability. However, the current performance of eSKT is far from that of natural skeletal muscle tissue. In this paper, an optimized design method for eSKTs has been proposed. By combining simulation analysis with experiments, the eSKTs with multiple strips have been developed. The results show that under a specific volume (250 μL), the optimized strip structures can enhance the stability of eSKT and facilitate the penetration of nutrients and oxygen, leading to improved fusion of myoblasts and the directional arrangement of myotubes, thus improving the performance of eSKT. The eSKT with multiple strips exhibits a significant contraction force and has been successfully utilized in a bio-syncretic robot to demonstrate its actuation capability. This work may provide insights into the development of the field of bio-syncretic robots and even tissue engineering.},
  archive   = {C_ICRA},
  author    = {Lianchao Yang and Chuang Zhang and Ruiqian Wang and Yiwei Zhang and Lianqing Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611728},
  month     = {5},
  pages     = {2353-2359},
  title     = {Optimized design and fabrication of skeletal muscle actuators for bio-syncretic robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and modeling of a nested bi-cavity-based soft growing
robot for grasping in constrained environments. <em>ICRA</em>,
2346–2352. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft growing robots with unique navigation (tip extension by eversion) hold great promise in rescue, medical, and industrial applications. Equipping them with grasping capability would enhance their usefulness in constrained environments for various applications. However, in traditional designs, the tip’s eversion naturally conflicts with grasping, and the addition of grippers at the tip would limit navigation inevitably in constrained environments. To realize grasping in such scenes without extra devices, we propose a nested bi-cavity-based growing soft robot (BIBOT). The new design consists of two coaxially nested cavities, where the inner and outer cavities extend synchronously by inversion and eversion of the film rolls. Such a bi-cavity design enables the BIBOT to navigate and grasp without relative movements between the body and environment, and avoids contact between the object and its surroundings as well. Further, a kinematics model is established and verified to precisely control its lengthening and steering by a feed mechanism. Finally, its capability in a constrained environment is demonstrated by navigating and grasping an object in a curved pipe with a variable internal diameter.},
  archive   = {C_ICRA},
  author    = {Haochen Yong and Fukang Xu and Chenfei Li and Han Ding and Zhigang Wu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610986},
  month     = {5},
  pages     = {2346-2352},
  title     = {Design and modeling of a nested bi-cavity-based soft growing robot for grasping in constrained environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic modeling of wing-assisted inclined running with a
morphing multi-modal robot. <em>ICRA</em>, 2339–2345. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot designs can take many inspirations from nature, where there are many examples of highly resilient and fault-tolerant locomotion strategies to navigate complex terrains by using multi-functional appendages. For example, Chukar and Hoatzin birds can repurpose their wings for quadrupedal walking and wing-assisted incline running (WAIR) to climb steep surfaces. We took inspiration from nature and designed a morphing robot with multi-functional thruster-wheel appendages that allows the robot to change its mode of locomotion by transforming into a rover, quad-rotor, mobile inverted pendulum (MIP), and other modes. In this work, we derive a dynamic model and formulate a nonlinear model predictive controller to perform WAIR to showcase the unique capabilities of our robot. We implemented the model and controller in a numerical simulation and experiments to show their feasibility and the capabilities of our transforming multimodal robot.},
  archive   = {C_ICRA},
  author    = {Eric Sihite and Alireza Ramezani and Morteza Gharib},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610678},
  month     = {5},
  pages     = {2339-2345},
  title     = {Dynamic modeling of wing-assisted inclined running with a morphing multi-modal robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lissajous curve-based vibrational orbit control of a
flexible vibrational actuator with a structural anisotropy.
<em>ICRA</em>, 2332–2338. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a novel flexible vibrational actuator with a structural anisotropy and its control method to diversify the vibrational behavior. First, the analytical model of the proposed actuator, which comprises a rectangular cross-sectional flexible beam and a rotational-type motor, is introduced. Regarding the structural anisotropy, the rotational axis of the motor is nonparallel to both principal axes of bending stiffness of the beam. Then, the vibrational phenomenon of the actuator is theoretically revealed. It is shown that using the synthetic wave input constituting two sine waves based on the resonance frequencies for the principal axes of the beam, the vibrational orbit of the tip of the beam can be controlled in the same manner as the Lissajous curve. Finally, the proposed method is experimentally validated. The Lissajous curve-based vibrational orbit control is performed using a prototype actuator. Furthermore, an application to underactuated-type locomotor is demonstrated.},
  archive   = {C_ICRA},
  author    = {Yuto Miyazaki and Mitsuru Higashimori},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610781},
  month     = {5},
  pages     = {2332-2338},
  title     = {Lissajous curve-based vibrational orbit control of a flexible vibrational actuator with a structural anisotropy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Supernumerary robotic limbs to support post-fall recoveries
for astronauts. <em>ICRA</em>, 2324–2331. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes the utilization of Supernumerary Robotic Limbs (SuperLimbs) for augmenting astronauts during an Extra-Vehicular Activity (EVA) in a partial-gravity environment. We investigate the effectiveness of SuperLimbs in assisting astronauts to their feet following a fall. Based on preliminary observations from a pilot human study, we categorized post-fall recoveries into a sequence of statically stable poses called &quot;waypoints&quot;. The paths between the waypoints can be modeled with a simplified kinetic motion applied about a specific point on the body. Following the characterization of post-fall recoveries, we designed a task-space impedance control with high damping and low stiffness, where the SuperLimbs provide an astronaut with assistance in post-fall recovery while keeping the human-in-the-loop scheme. In order to validate this control scheme, a full-scale wearable analog space suit was constructed and tested with a SuperLimbs prototype. Results from the experimentation found that without assistance, astronauts would impulsively exert themselves to perform a post-fall recovery, which resulted in high energy consumption and instabilities maintaining an upright posture, concurring with prior NASA studies. When the SuperLimbs provided assistance, the astronaut’s energy consumption and deviation in their tracking as they performed a post-fall recovery was reduced considerably.},
  archive   = {C_ICRA},
  author    = {Erik Ballesteros and Sang-Yoep Lee and Kalind C. Carpenter and H. Harry Asada},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610849},
  month     = {5},
  pages     = {2324-2331},
  title     = {Supernumerary robotic limbs to support post-fall recoveries for astronauts},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incremental learning of full-pose via-point movement
primitives on riemannian manifolds. <em>ICRA</em>, 2317–2323. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Movement primitives (MPs) are compact representations of robot skills that can be learned from demonstrations and combined into complex behaviors. However, merely equipping robots with a fixed set of innate MPs is insufficient to deploy them in dynamic and unpredictable environments. Instead, the full potential of MPs remains to be attained via adaptable, large-scale MP libraries. In this paper, we propose a set of seven fundamental operations to incrementally learn, improve, and re-organize MP libraries. To showcase their applicability, we provide explicit formulations of the five spatial operations for libraries composed of Via-Point Movement Primitives (VMPs). By building on Riemannian manifold theory, our approach enables the incremental learning of all parameters of position and orientation VMPs within a library. Moreover, our approach stores a fixed number of parameters, thus complying with the essential principles of incremental learning. We evaluate our approach to incrementally learn a VMP library from sequentially-provided motion capture data.},
  archive   = {C_ICRA},
  author    = {Tilman Daab and Noémie Jaquier and Christian Dreher and Andre Meixner and Franziska Krebs and Tamim Asfour},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610275},
  month     = {5},
  pages     = {2317-2323},
  title     = {Incremental learning of full-pose via-point movement primitives on riemannian manifolds},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robot-assisted navigation for visually impaired through
adaptive impedance and path planning. <em>ICRA</em>, 2310–2316. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a framework to navigate visually impaired people through unfamiliar environments by means of a mobile manipulator. The Human-Robot system consists of three key components: a mobile base, a robotic arm, and the human subject who gets guided by the robotic arm via physically coupling their hand with the cobot’s end-effector. These components, receiving a goal from the user, traverse a collision-free set of waypoints in a coordinated manner, while avoiding static and dynamic obstacles through an obstacle avoidance unit and a novel human guidance planner. With this aim, we also present a legs tracking algorithm that utilizes 2D LiDAR sensors integrated into the mobile base to monitor the human pose. Additionally, we introduce an adaptive pulling planner responsible for guiding the individual back to the intended path if they veer off course. This is achieved by establishing a target arm end-effector position and dynamically adjusting the impedance parameters in real-time through a impedance tuning unit. To validate the framework we present a set of experiments both in laboratory settings with 12 healthy blindfolded subjects and a proof-of-concept demonstration in a real-world scenario.},
  archive   = {C_ICRA},
  author    = {Pietro Balatti and Idil Ozdamar and Doganay Sirintuna and Luca Fortini and Mattia Leonori and Juan M. Gandarias and Arash Ajoudani},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611071},
  month     = {5},
  pages     = {2310-2316},
  title     = {Robot-assisted navigation for visually impaired through adaptive impedance and path planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MateRobot: Material recognition in wearable robotics for
people with visual impairments. <em>ICRA</em>, 2303–2309. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {People with Visual Impairments (PVI) typically recognize objects through haptic perception. Knowing objects and materials before touching is desired by the target users but under-explored in the field of human-centered robotics. To fill this gap, in this work, a wearable vision-based robotic system, MATERobot, is established for PVI to recognize materials and object categories beforehand. To address the computational constraints of mobile platforms, we propose a lightweight yet accurate model MATEViT to perform pixel-wise semantic segmentation, simultaneously recognizing both objects and materials. Our methods achieve respective 40.2% and 51.1% of mIoU on COCOStuff-10K and DMS datasets, surpassing the previous method with +5.7% and +7.0% gains. Moreover, on the field test with participants, our wearable system reaches a score of 28 in the NASA-Task Load Index, indicating low cognitive demands and ease of use. Our MATERobot demonstrates the feasibility of recognizing material property through visual cues and offers a promising step towards improving the functionality of wearable robots for PVI. The source code has been made publicly available at MATERobot.},
  archive   = {C_ICRA},
  author    = {Junwei Zheng and Jiaming Zhang and Kailun Yang and Kunyu Peng and Rainer Stiefelhagen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610333},
  month     = {5},
  pages     = {2303-2309},
  title     = {MateRobot: Material recognition in wearable robotics for people with visual impairments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CoFRIDA: Self-supervised fine-tuning for human-robot
co-painting. <em>ICRA</em>, 2296–2302. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Prior robot painting and drawing work, such as FRIDA, has focused on decreasing the sim-to-real gap and expanding input modalities for users, but the interaction with these systems generally exists only in the input stages. To support interactive, human-robot collaborative painting, we introduce the Collaborative FRIDA (CoFRIDA) robot painting framework, which can co-paint by modifying and engaging with content already painted by a human collaborator. To improve text-image alignment–FRIDA’s major weakness–our system uses pre-trained text-to-image models; however, pre-trained models in the context of real-world co-painting do not perform well because they (1) do not understand the constraints and abilities of the robot and (2) cannot perform co-painting without making unrealistic edits to the canvas and overwriting content. We propose a self-supervised fine-tuning procedure that can tackle both issues, allowing the use of pre-trained state-of-the-art text-image alignment models with robots to enable co-painting in the physical world. Our open-source approach, CoFRIDA, creates paintings and drawings that match the input text prompt more clearly than FRIDA, both from a blank canvas and one with human created work. More generally, our fine-tuning procedure successfully encodes the robot’s constraints and abilities into a foundation model, showcasing promising results as an effective method for reducing sim-to-real gaps. https://pschaldenbrand.github.io/cofrida/},
  archive   = {C_ICRA},
  author    = {Peter Schaldenbrand and Gaurav Parmar and Jun-Yan Zhu and James McCann and Jean Oh},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610618},
  month     = {5},
  pages     = {2296-2302},
  title     = {CoFRIDA: Self-supervised fine-tuning for human-robot co-painting},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). POLITE: Preferences combined with highlights in
reinforcement learning. <em>ICRA</em>, 2288–2295. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many solutions to address the challenge of robot learning have been devised, namely through exploring novel ways for humans to communicate complex goals and tasks in reinforcement learning (RL) setups. One way that experienced recent research interest directly addresses the problem by considering human feedback as preferences between pairs of trajectories (sequences of state-action pairs). However, when simply attributing a single preference to a pair of trajectories that contain many agglomerated steps, key pieces of information are lost in the process. We amplify the initial definition of preferences to account for highlights: state-action pairs of relatively high information (high/low reward) within a preferred trajectory. To include the additional information, we design novel regularization methods within a preference learning framework. To this extent, we present our method which is able to greatly reduce the necessary amount of preferences, by permitting the highlighting of favoured trajectories, in order to reduce the entropy of the credit assignment. We show the effectiveness of our work in both simulation and a user study, which analyzes the feedback given and its implications. We also use the total collected feedback to train a robot policy for socially compliant trajectories in a simulated social navigation environment. We release code and video examples at https://sites.google.com/view/rl-polite},
  archive   = {C_ICRA},
  author    = {Simon Holk and Daniel Marta and Iolanda Leite},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610505},
  month     = {5},
  pages     = {2288-2295},
  title     = {POLITE: Preferences combined with highlights in reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Parallel optimization with hard safety constraints for
cooperative planning of connected autonomous vehicles. <em>ICRA</em>,
2238–2244. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The development of connected autonomous vehicles (CAVs) facilitates the enhancement of traffic efficiency in complicated scenarios. Difficulties remain unsolved in developing an effective and efficient coordination strategy for CAVs. In this paper, we formulate the cooperative autonomous driving task of CAVs as an optimal control problem with safety conditions enforced as hard constraints, and propose a computationally-efficient parallel optimization framework to generate strategies for CAVs with the travel efficiency improved and the hard safety constraints satisfied. Specifically, all constraints involved are addressed appropriately with convex approximation, such that the convexity property of the reformulated optimization problem is exhibited. Then, a parallel optimization algorithm is presented to solve the reformulated optimization problem, with an embodied iterative nearest neighbor search strategy to determine the optimal passing sequence. It is noteworthy that the travel efficiency is enhanced and the computation burden is considerably alleviated with the proposed innovation development. We also examine the proposed method in CARLA simulator and perform thorough comparisons to demonstrate the effectiveness and efficiency of the proposed approach.},
  archive   = {C_ICRA},
  author    = {Zhenmin Huang and Haichao Liu and Shaojie Shen and Jun Ma},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611158},
  month     = {5},
  pages     = {2238-2244},
  title     = {Parallel optimization with hard safety constraints for cooperative planning of connected autonomous vehicles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DESTINE: Dynamic goal queries with temporal transductive
alignment for trajectory prediction. <em>ICRA</em>, 2230–2237. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Predicting temporally consistent road users’ trajectories in a multi-agent setting is a challenging task due to the unknown characteristics of agents and their varying intentions. Besides using semantic map information and modeling interactions, it is important to build an effective mechanism capable of reasoning about behaviors at different levels of granularity.To this end, we propose Dynamic goal quErieS with temporal Transductive alIgNmEnt (DESTINE) method. Unlike prior approaches, our approach 1) dynamically predicts agents’ goals irrespective of particular road structures, such as lanes, allowing the method to produce a more accurate estimation of destinations; 2) achieves map-compliant predictions by generating future trajectories in a coarse-to-fine fashion, where the coarser predictions at a lower frame rate serve as intermediate goals; and 3) uses an attention module designed to temporally align predicted trajectories via a masked attention operation.Using the common Argoverse benchmark dataset, we show that our method achieves state-of-the-art performance on various metrics, and further investigate the contributions of proposed modules via comprehensive ablation studies.},
  archive   = {C_ICRA},
  author    = {Rezaul Karim and Soheil Mohamad Alizadeh Shabestary and Amir Rasouli},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611124},
  month     = {5},
  pages     = {2230-2237},
  title     = {DESTINE: Dynamic goal queries with temporal transductive alignment for trajectory prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical learned risk-aware planning framework for human
driving modeling. <em>ICRA</em>, 2223–2229. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel approach to modeling human driving behavior, designed for use in evaluating autonomous vehicle control systems in a simulation environments. Our methodology leverages a hierarchical forward-looking, risk-aware estimation framework with learned parameters to generate human-like driving trajectories, accommodating multiple driver levels determined by model parameters. This approach is grounded in multimodal trajectory prediction, using a deep neural network with LSTM-based social pooling to predict the trajectories of surrounding vehicles. These trajectories are used to compute forward-looking risk assessments along the ego vehicle’s path, guiding its navigation. Our method aims to replicate human driving behaviors by learning parameters that emulate human decision-making during driving. We ensure that our model exhibits robust generalization capabilities by conducting simulations, employing real-world driving data to validate the accuracy of our approach in modeling human behavior. The results reveal that our model effectively captures human behavior, showcasing its versatility in modeling human drivers in diverse highway scenarios.},
  archive   = {C_ICRA},
  author    = {Nathan Ludlow and Yiwei Lyu and John Dolan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610354},
  month     = {5},
  pages     = {2223-2229},
  title     = {Hierarchical learned risk-aware planning framework for human driving modeling},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ODD-based query-time scenario mutation framework for
autonomous driving scenario databases. <em>ICRA</em>, 2197–2203. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large-scale scenario databases may contain hundreds of thousands of scenarios for the verification and validation (V&amp;V) of autonomous vehicles (AV). Scenarios in the database are often labelled with semantic Operational Design Domain (ODD) tags (e.g., WeatherRainy, RoadTypeHighway and ActorTypeTruck) to be queried via exact tag matching. Such a scenario database design has two major limitations, i.e. combinatorial scenario generation inevitably leads to many redundant scenarios, and each ODD query matches only a small number of scenarios in the database (0.2% in our case study), rendering most of the database wealth wasted. We propose a novel scenario database design and the first ODD-based query-time scenario mutation framework to address the limitations. Our case study results show that the proposed framework has the potential to fully utilize all the database scenarios at query time while eliminating scenario redundancy in the database (in our case study, given the same ODD query, the number of final matched scenarios increased by 36 times, diversity increased by 99 times, and scenario database utilization rate increased from 0.2% to 36%).},
  archive   = {C_ICRA},
  author    = {Yun Tang and Dhanush Raj and Xingyu Zhao and Xizhe Zhang and Antonio A. Bruto da Costa and Siddartha Khastgir and Paul Jennings},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610412},
  month     = {5},
  pages     = {2197-2203},
  title     = {ODD-based query-time scenario mutation framework for autonomous driving scenario databases},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reducing non-IID effects in federated autonomous driving
with contrastive divergence loss. <em>ICRA</em>, 2190–2196. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Federated learning has been widely applied in autonomous driving since it enables training a learning model among vehicles without sharing users’ data. However, data from autonomous vehicles usually suffer from the non-independent-and-identically-distributed (non-IID) problem, which may cause negative effects on the convergence of the learning process. In this paper, we propose a new contrastive divergence loss to address the non-IID problem in autonomous driving by reducing the impact of divergence factors from transmitted models during the local learning process of each silo. We also analyze the effects of contrastive divergence in various autonomous driving scenarios, under multiple network infrastructures, and with different centralized/distributed learning schemes. Our intensive experiments on three datasets demonstrate that our proposed contrastive divergence loss significantly improves the performance over current state-of-the-art approaches. Our source code is available at https://github.com/aioz-ai/CDL.},
  archive   = {C_ICRA},
  author    = {Tuong Do and Binh X. Nguyen and Quang D. Tran and Hien Nguyen and Erman Tjiputra and Te-Chuan Chiu and Anh Nguyen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611202},
  month     = {5},
  pages     = {2190-2196},
  title     = {Reducing non-IID effects in federated autonomous driving with contrastive divergence loss},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards optimal lane-changing coordination of CAVs in
multi-lane mixed traffic scenarios. <em>ICRA</em>, 2183–2189. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Lane changing is a fundamental but challenging operation for moving vehicles. Connected and Automated Vehicles(CAVs) enable autonomous vehicles to cooperate with each other to accomplish the lane changing tasks, profiting from their communication ability. However, dispatching CAVs in mixed traffic remains difficult due to the stochastic behaviors and uncertain intentions of Human-Driven Vehicles(HDVs). To tackle this issue, this paper devises a coordination approach based on Conflict-Based Search(CBS) theory. Firstly, HDVs are accurately modeled as constraints to enable usage of CBS in the mixed traffic. Additionally, virtual goals are introduced to search CAVs’ priority and outlets along with path finding. Furthermore, we optimize the performance of CBS in dense traffic by defining the concept of following vehicles. Experiments show that performance is improved by utilizing new conflict prioritizing rules and a heuristic value calculation method that derived from following vehicles. Finally, we introduce grouping vehicles to extend the proposed method for solving extremely dense and large instances at a scale of more than one hundred without significant loss in efficiency.},
  archive   = {C_ICRA},
  author    = {Yan Ding and Yijun Mao and Chongshan Jiao and Pengju Ren},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611720},
  month     = {5},
  pages     = {2183-2189},
  title     = {Towards optimal lane-changing coordination of CAVs in multi-lane mixed traffic scenarios},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semi-autonomous surface-tracking tasks using omnidirectional
mobile manipulators. <em>ICRA</em>, 2176–2182. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite the potential of mobile manipulators and applications where robots require a force-controlled physical interaction with the environment, the majority of robot automation nowadays is still based on fixed manipulators for free-motion tasks (e.g. welding, pick and place, or painting). In this work, we propose a control solution for omnidirectional mobile manipulators in force-tracking tasks, interacting with unknown surface geometries and with a human teleoperator in the control loop. Keeping a teleoperator in the loop makes the system widely applicable to unstructured environments. With little effort, a human can take care of the mobile base navigation, self-collisions, and collisions with the environment, as well as selecting the area of the asset surface to process. The teleoperator interfaces with the robot platform by commanding motion in the mobile base to increase the arm’s workspace and manoeuvrability. The operator can also command the movement of the end-effector, sliding on the surface geometry to process a specific area. Alternatively, he can let the controller execute a parametric trajectory (spiral or raster) for an autonomous area coverage and meanwhile telecommand the base in order to keep the arm in configurations with good dexterity. The autonomous controller, on the other hand, takes responsibility for following the unknown contour on the manipulated surface by only taking observations from a force/torque sensor attached to the arm’s wrist, exerting a prescribed force, and handling the motion control in the base and the arm so that both can follow their respective task requests. Overall, we have developed a user-friendly control scheme, where an operator with little training and using a joystick, can guide the robot system to perform a physically interactive task on the surface of an asset.},
  archive   = {C_ICRA},
  author    = {Carlos Suarez Zapico and Yvan Petillot and Mustafa Suphi Erden},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611518},
  month     = {5},
  pages     = {2176-2182},
  title     = {Semi-autonomous surface-tracking tasks using omnidirectional mobile manipulators},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RoSSO: A high-performance python package for robotic
surveillance strategy optimization using JAX. <em>ICRA</em>, 2169–2175.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To enable the computation of effective randomized patrol routes for single- or multi-robot teams, we present RoSSO, a Python package designed for solving Markov chain optimization problems. We exploit machine-learning techniques such as reverse-mode automatic differentiation and constraint parametrization to achieve superior efficiency compared to general-purpose nonlinear programming solvers. Additionally, we supplement a game-theoretic stochastic surveillance formulation in the literature with a novel greedy algorithm and multi-robot extension. We close with numerical results for a police district in downtown San Francisco that demonstrate RoSSO’s capabilities on our new formulations and the prior work.},
  archive   = {C_ICRA},
  author    = {Yohan John and Connor Hughes and Gilberto Díaz-García and Jason R. Marden and Francesco Bullo},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610477},
  month     = {5},
  pages     = {2169-2175},
  title     = {RoSSO: A high-performance python package for robotic surveillance strategy optimization using JAX},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Autonomous field-of-view adjustment using adaptive kinematic
constrained control with robot-held microscopic camera feedback.
<em>ICRA</em>, 2162–2168. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic systems for manipulation in millimeter scale often use a camera with high magnification for visual feedback of the target region. However, the limited field-of-view (FoV) of the microscopic camera necessitates camera motion to capture a broader workspace environment. In this work, we propose an autonomous robotic control method to constrain a robot-held camera within a designated FoV. Furthermore, we model the camera extrinsics as part of the kinematic model and use camera measurements coupled with a U-Net based tool tracking to adapt the complete robotic model during task execution. As a proof-of-concept demonstration, the proposed framework was evaluated in a bi-manual setup, where the microscopic camera was controlled to view a tool moving in a pre-defined trajectory. The proposed method allowed the camera to stay 94.1% of the time within the real FoV, compared to 54.4% without the proposed adaptive control.},
  archive   = {C_ICRA},
  author    = {Hung-Ching Lin and Murilo Marques Marinho and Kanako Harada},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610663},
  month     = {5},
  pages     = {2162-2168},
  title     = {Autonomous field-of-view adjustment using adaptive kinematic constrained control with robot-held microscopic camera feedback},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A retinex structure-based low-light enhancement model guided
by spatial consistency. <em>ICRA</em>, 2154–2161. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Images captured by robotics under low-light conditions are often plagued by several challenges, including diminished contrast, increased noise, loss of fine details, and unnatural color reproduction. These factors can significantly hinder the performance of computer vision tasks such as object detection and image segmentation. As a result, improving the quality of low-light images is of paramount importance for practical applications in the computer vision domain. To effectively address these challenges, we present a novel low-light image enhancement model, termed Spatial Consistency Retinex Network (SCRNet), which leverages the Retinex-based structure and is guided by the principle of spatial consistency. Specifically, our proposed model incorporates three levels of consistency: channel level, semantic level, and texture level, inspired by the principle of spatial consistency. These levels of consistency enable our model to adaptively enhance image features, ensuring more accurate and visually pleasing results. Extensive experimental evaluations on various low-light image datasets demonstrate that our proposed SCRNet outshines existing state-of-the-art methods, highlighting the potential of SCRNet as an effective solution for enhancing low-light images.},
  archive   = {C_ICRA},
  author    = {Miao Zhang and Yiqing Shen and Zhuowei Li and Guofeng Pan and Shuai Lu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610021},
  month     = {5},
  pages     = {2154-2161},
  title     = {A retinex structure-based low-light enhancement model guided by spatial consistency},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). An LLM-driven framework for multiple-vehicle dispatching
and navigation in smart city landscapes. <em>ICRA</em>, 2147–2153. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the context of smart cities, autonomous vehicles, such as unmanned delivery vehicles and taxis are gradually gaining acceptance. However, their application scenarios remain significantly fragmented. Typically, an Autonomous Multi-Functional Vehicle (AMFV) is not engaged in other scenarios when idle in a specific one. Currently, a unified system capable of coordinating and using these resources efficiently is lacking. Moreover, there is an absence of an advanced navigation algorithm for facilitating coordinated navigation among Heterogeneous Vehicles (HVs). To address these issues, we propose the LLM-driven Multi-vehicle Dispatching and navigation (LiMeda) framework. It comprises an LLM-driven scheduling module that facilitates efficient allocation considering task scenarios and vehicle information, which addresses the issue of incompatible vehicle resources across various smart city scenarios. And the other is a navigation module, founded on the Heterogeneous Agent Reinforcement Learning (HARL) framework we previously proposed, which can effectively perform cooperative navigation tasks among heterogeneous agents, assisting the cooperative task completion by HVs in a smart city. Experimental results show our method outperforms both traditional scheduling algorithms and Reinforcement Learning navigation algorithms in metric terms. Additionally, it shows remarkable scalability and generalization under varying city scales, vehicle numbers, and task numbers.},
  archive   = {C_ICRA},
  author    = {Ruiqing Chen and Wenbin Song and Weiqin Zu and ZiXin Dong and Ze Guo and Fanglei Sun and Zheng Tian and Jun Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610578},
  month     = {5},
  pages     = {2147-2153},
  title     = {An LLM-driven framework for multiple-vehicle dispatching and navigation in smart city landscapes},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RoboGrind: Intuitive and interactive surface treatment with
industrial robots. <em>ICRA</em>, 2140–2146. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Surface treatment tasks such as grinding, sanding or polishing are a vital step of the value chain in many industries, but are notoriously challenging to automate. We present RoboGrind, an integrated system for the intuitive, interactive automation of surface treatment tasks with industrial robots. It combines a sophisticated 3D perception pipeline for surface scanning and automatic defect identification, an interactive voice-controlled wizard system for the AI-assisted bootstrapping and parameterization of robot programs, and an automatic planning and execution pipeline for force-controlled robotic surface treatment. RoboGrind is evaluated both under laboratory and real-world conditions in the context of refabricating fiberglass wind turbine blades.},
  archive   = {C_ICRA},
  author    = {Benjamin Alt and Florian Stöckl and Silvan Müller and Christopher Braun and Julian Raible and Saad Alhasan and Oliver Rettig and Lukas Ringle and Darko Katic and Rainer Jäkel and Michael Beetz and Marcus Strand and Marco F. Huber},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611143},
  month     = {5},
  pages     = {2140-2146},
  title     = {RoboGrind: Intuitive and interactive surface treatment with industrial robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A simple computationally efficient path ILC for industrial
robotic manipulators. <em>ICRA</em>, 2133–2139. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, a numerically efficient flexible control scheme for the absolute accuracy of industrial robots is presented and experimentally validated. A model-based controller that leverages all typically available parameters is combined with an online path iterative learning controller (ILC). The ILC law is employed to compensate for the unknown residual error dynamics caused by elastic and transmission effects. The proposed approach combines several benefits, including the possibility of a continuous execution of trials, a straightforward generalization of the learned data to different execution speeds, and learning from partial trials. The experimental validations on a 6-axis industrial robot with a laser tracker absolute measurement system show a 95% improvement in absolute accuracy after two trials. When the laser tracker is removed, the learned feedforward controller can sustain the accuracy achieved even without trial-by-trial learning.},
  archive   = {C_ICRA},
  author    = {Michael Schwegel and Andreas Kugi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610623},
  month     = {5},
  pages     = {2133-2139},
  title     = {A simple computationally efficient path ILC for industrial robotic manipulators},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A collision-aware cable grasping method in cluttered
environment. <em>ICRA</em>, 2126–2132. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a Cable Grasping-Convolutional Neural Network (CG-CNN) designed to facilitate robust cable grasping in cluttered environments. Utilizing physics simulations, we generate an extensive dataset that mimics the intricacies of cable grasping, factoring in potential collisions between cables and robotic grippers. We employ the Approximate Convex Decomposition technique to dissect the non-convex cable model, with grasp quality autonomously labeled based on simulated grasping attempts. The CG-CNN is refined using this simulated dataset and enhanced through domain randomization techniques. Subsequently, the trained model predicts grasp quality, guiding the optimal grasp pose to the robot’s controller for execution. Grasping efficacy is assessed across both synthetic and real-world settings. Given our model’s implicit collision sensitivity, we achieved commendable success rates of 92.3% for known cables and 88.4% for unknown cables, surpassing contemporary state-of-the-art approaches. Supplementary materials can be found at https://leizhang-public.github.io/cg-cnn/.},
  archive   = {C_ICRA},
  author    = {Lei Zhang and Kaixin Bai and Qiang Li and Zhaopeng Chen and Jianwei Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610559},
  month     = {5},
  pages     = {2126-2132},
  title     = {A collision-aware cable grasping method in cluttered environment},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A large-scale suction-based climbing parallel robot for wall
painting application. <em>ICRA</em>, 2119–2125. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a large-scale climbing robot that employs a parallel mechanism with three translational degrees of freedom as its locomotion method. Using a robot frame having a triangular pyramid shape, the robot provides a good stability during the locomotion and task execution. Three suction cups, called the perimeter cups, are attached to the vertices of the robot’s pyramid base, whereas three other suction cups called the middle cups, are attached to the end-effector of the parallel mechanism. The climbing motion is made by attaching and releasing the perimeter and middle cups one after another. The synchronization between the parallel mechanism’s motion and the suction cups during locomotion, as well as the improved gait trajectory, was established to ensure successful climbing. The control scheme of the robot integrates the servo control, the suction control, and the application control in a modular fashion. The successful climbing of the robot proves the scalability of the proposed climbing robot using active suction cups with an optimized design. Finally, a painting application was presented to demonstrate the robot’s capability to perform a wall painting task.},
  archive   = {C_ICRA},
  author    = {Abdur Rosyid and Bashar El-Khasawneh},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610067},
  month     = {5},
  pages     = {2119-2125},
  title     = {A large-scale suction-based climbing parallel robot for wall painting application},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Subequivariant reinforcement learning framework for
coordinated motion control. <em>ICRA</em>, 2112–2118. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Effective coordination is crucial for motion control with reinforcement learning, especially as the complexity of agents and their motions increases. However, many existing methods struggle to account for the intricate dependencies between joints. We introduce CoordiGraph, a novel architecture that leverages subequivariant principles from physics to enhance coordination of motion control with reinforcement learning. This method embeds the principles of equivariance as inherent patterns in the learning process under gravity influence, which aids in modeling the nuanced relationships between joints vital for motion control. Through extensive experimentation with sophisticated agents in diverse environments, we highlight the merits of our approach. Compared to current leading methods, CoordiGraph notably enhances generalization and sample efficiency.},
  archive   = {C_ICRA},
  author    = {Haoyu Wang and Xiaoyu Tan and Xihe Qiu and Chao Qu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610563},
  month     = {5},
  pages     = {2112-2118},
  title     = {Subequivariant reinforcement learning framework for coordinated motion control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DeFlow: Decoder of scene flow network in autonomous driving.
<em>ICRA</em>, 2105–2111. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Scene flow estimation determines a scene’s 3D motion field, by predicting the motion of points in the scene, especially for aiding tasks in autonomous driving. Many networks with large-scale point clouds as input use voxelization to create a pseudo-image for real-time running. However, the voxelization process often results in the loss of point-specific features. This gives rise to a challenge in recovering those features for scene flow tasks. Our paper introduces DeFlow which enables a transition from voxel-based features to point features using Gated Recurrent Unit (GRU) refinement. To further enhance scene flow estimation performance, we formulate a novel loss function that accounts for the data imbalance between static and dynamic points. Evaluations on the Argoverse 2 scene flow task reveal that DeFlow achieves state-of-the-art results on large-scale point cloud data, demonstrating that our network has better performance and efficiency compared to others. The code is available at https://github.com/KTH-RPL/deflow.},
  archive   = {C_ICRA},
  author    = {Qingwen Zhang and Yi Yang and Heng Fang and Ruoyu Geng and Patric Jensfelt},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610278},
  month     = {5},
  pages     = {2105-2111},
  title     = {DeFlow: Decoder of scene flow network in autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient hybrid neuromorphic-bayesian model for olfaction
sensing: Detection and classification. <em>ICRA</em>, 2089–2095. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Olfaction sensing in autonomous robotics faces challenges in dynamic operations, energy efficiency, and edge processing. It necessitates a machine learning algorithm capable of managing real-world odor interference, ensuring resource efficiency for mobile robotics, and accurately estimating gas features for critical tasks such as odor mapping, localization, and alarm generation. This paper introduces a hybrid approach that exploits neuromorphic computing in combination with probabilistic inference to address these demanding requirements. Our approach implements a combination of a convolutional spiking neural network for feature extraction and a Bayesian spiking neural network for odor detection and identification. The developed algorithm is rigorously tested on a dataset for sensor drift compensation for robustness evaluation. Additionally, for efficiency evaluation, we compare the energy consumption of our model with a non-spiking machine learning algorithm under identical dataset and operating conditions. Our approach demonstrates superior efficiency alongside comparable accuracy outcomes.},
  archive   = {C_ICRA},
  author    = {Rizwana Kausar and Fakhreddine Zayer and Jaime Viegas and Jorge Dias},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611648},
  month     = {5},
  pages     = {2089-2095},
  title     = {Efficient hybrid neuromorphic-bayesian model for olfaction sensing: Detection and classification},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ISR-LLM: Iterative self-refined large language model for
long-horizon sequential task planning. <em>ICRA</em>, 2081–2088. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motivated by the substantial achievements of Large Language Models (LLMs) in the field of natural language processing, recent research has commenced investigations into the application of LLMs for complex, long-horizon sequential task planning challenges in robotics. LLMs are advantageous in offering the potential to enhance the generalizability as task-agnostic planners and facilitate flexible interaction between human instructors and planning systems. However, task plans generated by LLMs often lack feasibility and correctness. To address this challenge, we introduce ISR-LLM, a novel framework that improves LLM-based planning through an iterative self-refinement process. The framework operates through three sequential steps: preprocessing, planning, and iterative self-refinement. During preprocessing, an LLM translator is employed to convert natural language input into a Planning Domain Definition Language (PDDL) formulation. In the planning phase, an LLM planner formulates an initial plan, which is then assessed and refined in the iterative self-refinement step by a validator. We examine the performance of ISR-LLM across three distinct planning domains. Our experimental results show that ISR-LLM is able to achieve markedly higher success rates in sequential task planning compared to state-of-the-art LLM-based planners. Moreover, it also preserves the broad applicability and generalizability of working with natural language instructions.},
  archive   = {C_ICRA},
  author    = {Zhehua Zhou and Jiayang Song and Kunpeng Yao and Zhan Shu and Lei Ma},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610065},
  month     = {5},
  pages     = {2081-2088},
  title     = {ISR-LLM: Iterative self-refined large language model for long-horizon sequential task planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kinematic-aware prompting for generalizable articulated
object manipulation with LLMs. <em>ICRA</em>, 2073–2080. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generalizable articulated object manipulation is essential for home-assistant robots. Recent efforts focus on imitation learning from demonstrations or reinforcement learning in simulation, however, due to the prohibitive costs of real-world data collection and precise object simulation, it still remains challenging for these works to achieve broad adaptability across diverse articulated objects. Recently, many works have tried to utilize the strong in-context learning ability of Large Language Models (LLMs) to achieve generalizable robotic manipulation, but most of these researches focus on high-level task planning, sidelining low-level robotic control. In this work, building on the idea that the kinematic structure of the object determines how we can manipulate it, we propose a kinematic-aware prompting framework that prompts LLMs with kinematic knowledge of objects to generate low-level motion trajectory waypoints, supporting various object manipulation. To effectively prompt LLMs with the kinematic structure of different objects, we design a unified kinematic knowledge parser, which represents various articulated objects as a unified textual description containing kinematic joints and contact location. Building upon this unified description, a kinematic-aware planner model is proposed to generate precise 3D manipulation waypoints via a designed kinematic-aware chain-of-thoughts prompting method. Our evaluation spanned 48 instances across 16 distinct categories, revealing that our framework not only outperforms traditional methods on 8 seen categories but also shows a powerful zero-shot capability for 8 unseen articulated object categories with only 17 demonstrations. Moreover, the real-world experiments on 7 different object categories prove our framework’s adaptability in practical scenarios. Code is released at https://github.com/GeWu-Lab/LLM_articulated_object_manipulation.},
  archive   = {C_ICRA},
  author    = {Wenke Xia and Dong Wang and Xincheng Pang and Zhigang Wang and Bin Zhao and Di Hu and Xuelong Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610744},
  month     = {5},
  pages     = {2073-2080},
  title     = {Kinematic-aware prompting for generalizable articulated object manipulation with LLMs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Trust-region neural moving horizon estimation for robots.
<em>ICRA</em>, 2059–2065. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate disturbance estimation is essential for safe robot operations. The recently proposed neural moving horizon estimation (NeuroMHE), which uses a portable neural network to model the MHE’s weightings, has shown promise in further pushing the accuracy and efficiency boundary. Currently, NeuroMHE is trained through gradient descent, with its gradient computed recursively using a Kalman filter. This paper proposes a trust-region policy optimization method for training NeuroMHE. We achieve this by providing the second-order derivatives of MHE, referred to as the MHE Hessian. Remarkably, we show that many of the intermediate results used to obtain the gradient, especially the Kalman filter, can be efficiently reused to compute the MHE Hessian. This offers linear computational complexity with respect to the MHE horizon. As a case study, we evaluate the proposed trust region NeuroMHE on real quadrotor flight data for disturbance estimation. Our approach demonstrates highly efficient training in under 5 min using only 100 data points. It outperforms a state-of-the-art neural estimator by up to 68.1% in force estimation accuracy, utilizing only 1.4% of its network parameters. Furthermore, our method showcases enhanced robustness to network initialization compared to the gradient descent counterpart.},
  archive   = {C_ICRA},
  author    = {Bingheng Wang and Xuyang Chen and Lin Zhao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611703},
  month     = {5},
  pages     = {2059-2065},
  title     = {Trust-region neural moving horizon estimation for robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vision-language interpreter for robot task planning.
<em>ICRA</em>, 2051–2058. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large language models (LLMs) are accelerating the development of language-guided robot planners. Meanwhile, symbolic planners offer the advantage of interpretability. This paper proposes a new task that bridges these two trends, namely, multimodal planning problem specification. The aim is to generate a problem description (PD), a machine-readable file used by the planners to find a plan. By generating PDs from language instruction and scene observation, we can drive symbolic planners in a language-guided framework. We propose a Vision-Language Interpreter (ViLaIn), a new framework that generates PDs using state-of-the-art LLM and vision-language models. ViLaIn can refine generated PDs via error message feedback from the symbolic planner. Our aim is to answer the question: How accurately can ViLaIn and the symbolic planner generate valid robot plans? To evaluate ViLaIn, we introduce a novel dataset called the problem description generation (ProDG) dataset. The framework is evaluated with four new evaluation metrics. Experimental results show that ViLaIn can generate syntactically correct problems with more than 99% accuracy and valid plans with more than 58% accuracy. Our code and dataset are available at https://github.com/omron-sinicx/ViLaIn.},
  archive   = {C_ICRA},
  author    = {Keisuke Shirai and Cristian C. Beltran-Hernandez and Masashi Hamaya and Atsushi Hashimoto and Shohei Tanaka and Kento Kawaharazuka and Kazutoshi Tanaka and Yoshitaka Ushiku and Shinsuke Mori},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611112},
  month     = {5},
  pages     = {2051-2058},
  title     = {Vision-language interpreter for robot task planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Implicit coarse-to-fine 3D perception for category-level
object pose estimation from monocular RGB image. <em>ICRA</em>,
2043–2050. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Category-level object pose estimation demonstrates robust generalization capabilities that benefit robotics applications. However, exclusive reliance on RGB images without leveraging any 3D information introduces ambiguity in the translation and size of objects, leading to suboptimal performance. In this paper, we propose a framework for category-level pose estimation from a single RGB image in an end-to-end manner, i.e., Feature Auxiliary Perception Network (FAP-Net). To address inaccurate pose estimation caused by the inherent ambiguity of RGB images, we design a coarse-to-fine approach that first harnesses geometry supervision to facilitate coarse 3D feature perception and subsequently refines the features based on pose and size constraints. Experimental results on REAL275 and CAMERA25 demonstrate that FAP-Net achieves significant improvements (14.7% on 10°10cm and 11.4% on IoU50 on the real-scene REAL275 dataset) over the state-of-the-art and real-time inference (42 FPS).},
  archive   = {C_ICRA},
  author    = {Jia Li and Li Jin and Xibin Song and Yeheng Chen and Nan Li and Xueying Qin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610570},
  month     = {5},
  pages     = {2043-2050},
  title     = {Implicit coarse-to-fine 3D perception for category-level object pose estimation from monocular RGB image},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RGB-based category-level object pose estimation via
decoupled metric scale recovery. <em>ICRA</em>, 2036–2042. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While showing promising results, recent RGB-D camera-based category-level object pose estimation methods have restricted applications due to the heavy reliance on depth sensors. RGB-only methods provide an alternative to this problem yet suffer from inherent scale ambiguity stemming from monocular observations. In this paper, we propose a novel pipeline that decouples the 6D pose and size estimation to mitigate the influence of imperfect scales on rigid transformations. Specifically, we leverage a pre-trained monocular estimator to extract local geometric information, mainly facilitating the search for inlier 2D-3D correspondence. Meanwhile, a separate branch is designed to directly recover the metric scale of the object based on category-level statistics. Finally, we advocate using the RANSAC-PnP algorithm to robustly solve for 6D object pose. Extensive experiments have been conducted on both synthetic and real datasets, demonstrating the superior performance of our method over previous state-of-the-art RGB-based approaches, especially in terms of rotation accuracy. Code: https://github.com/goldoak/DMSR.},
  archive   = {C_ICRA},
  author    = {Jiaxin Wei and Xibin Song and Weizhe Liu and Laurent Kneip and Hongdong Li and Pan Ji},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611723},
  month     = {5},
  pages     = {2036-2042},
  title     = {RGB-based category-level object pose estimation via decoupled metric scale recovery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mutual information-calibrated conformal feature fusion for
uncertainty-aware multimodal 3D object detection at the edge.
<em>ICRA</em>, 2029–2035. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the expanding landscape of AI-enabled robotics, robust quantification of predictive uncertainties is of great importance. Three-dimensional (3D) object detection, a critical robotics operation, has seen significant advancements; however, the majority of current works focus only on accuracy and ignore uncertainty quantification. Addressing this gap, our novel study integrates the principles of conformal inference (CI) with information theoretic measures to perform lightweight, Monte Carlo-free uncertainty estimation within a multimodal framework. Through a multivariate Gaussian product of the latent variables in a Variational Autoencoder (VAE), features from RGB camera and LiDAR sensor data are fused to improve the prediction accuracy. Normalized mutual information (NMI) is leveraged as a modulator for calibrating uncertainty bounds derived from CI based on a weighted loss function. Our simulation results show an inverse correlation between inherent predictive uncertainty and NMI throughout the model’s training. The framework demonstrates comparable or better performance in KITTI 3D object detection benchmarks to similar methods that are not uncertainty-aware, making it suitable for real-time edge robotics.},
  archive   = {C_ICRA},
  author    = {Alex C. Stutts and Danilo Erricolo and Sathya Ravi and Theja Tulabandhula and Amit Ranjan Trivedi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609987},
  month     = {5},
  pages     = {2029-2035},
  title     = {Mutual information-calibrated conformal feature fusion for uncertainty-aware multimodal 3D object detection at the edge},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DA-RAW: Domain adaptive object detection for real-world
adverse weather conditions. <em>ICRA</em>, 2013–2020. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite the success of deep learning-based object detection methods in recent years, it is still challenging to make the object detector reliable in adverse weather conditions such as rain and snow. For the robust performance of object detectors, unsupervised domain adaptation has been utilized to adapt the detection network trained on clear weather images to adverse weather images. While previous methods do not explicitly address weather corruption during adaptation, the domain gap between clear and adverse weather can be decomposed into two factors with distinct characteristics: a style gap and a weather gap. In this paper, we present an unsupervised domain adaptation framework for object detection that can more effectively adapt to real-world environments with adverse weather conditions by addressing these two gaps separately. Our method resolves the style gap by concentrating on style-related information of high-level features using an attention module. Using self-supervised contrastive learning, our framework then reduces the weather gap and acquires instance features that are robust to weather corruption. Extensive experiments demonstrate that our method outperforms other methods for object detection in adverse weather conditions.},
  archive   = {C_ICRA},
  author    = {Minsik Jeon and Junwon Seo and Jihong Min},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611219},
  month     = {5},
  pages     = {2013-2020},
  title     = {DA-RAW: Domain adaptive object detection for real-world adverse weather conditions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward accurate camera-based 3D object detection via cascade
depth estimation and calibration. <em>ICRA</em>, 2006–2012. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent camera-based 3D object detection is limited by the precision of transforming from image to 3D feature spaces, as well as the accuracy of object localization within the 3D space. This paper aims to address such a fundamental problem of camera-based 3D object detection: How to effectively learn depth information for accurate feature lifting and object localization. Different from previous methods which directly predict depth distributions by using a supervised estimation model, we propose a cascade framework consisting of two depth-aware learning paradigms. First, a depth estimation (DE) scheme leverages relative depth information to realize the effective feature lifting from 2D to 3D spaces. Furthermore, a depth calibration (DC) scheme introduces depth reconstruction to further adjust the 3D object localization perturbation along the depth axis. In practice, the DE is explicitly realized by using both the absolute and relative depth optimization loss to promote the precision of depth prediction, while the capability of DC is implicitly embedded into the detection Transformer through a depth denoising mechanism in the training phase. The entire model training is accomplished through an end-to-end manner. We propose a baseline detector and evaluate the effectiveness of our proposal with +2.2%/+2.7% NDS/mAP improvements on NuScenes benchmark, and gain a comparable performance with 55.9%/45.7% NDS/mAP. Furthermore, we conduct extensive experiments to demonstrate its generality based on various detectors with about +2% NDS improvements.},
  archive   = {C_ICRA},
  author    = {Chaoqun Wang and Yiran Qin and Zijian Kang and Ningning Ma and Ruimao Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610281},
  month     = {5},
  pages     = {2006-2012},
  title     = {Toward accurate camera-based 3D object detection via cascade depth estimation and calibration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). MTRadSSD: A multi-task single-stage detector for object
detection and free space analysis in radar point clouds*. <em>ICRA</em>,
1999–2005. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Environmental perception tasks such as object detection and free space detection based on 3+1D radar severely suffer from the disorder and sparsity of point cloud. To tackle this problem, we propose a novel Multi-Task Radar-based Single Stage Detector, termed MTRadSSD, where we adopt instance-aware sampling strategies to discover multi-class road users and propose an occupancy map tool based on kernel density estimation (KDE) to make predictions in bird’s eye view (BEV). The denoised occupancy map also plays key role in generating polygon represented free space in the scene. As a result, our elaborated sampling strategies effectively retained useful semantic information and narrowed the difference of detection performance across object categories. Meanwhile, our MTRadSSD outperforms those state-of-the-art approaches in terms of real-time requirement and detection accuracy. In detail, the proposed method achieves an satisfactory speed of ˜ View-of-Delft (VOD). With IoU thresholds 0.5/0.25/0.25 the average prediction precision (AP) of easy-level objects (cars, pedestrians and cyclists) reaches at competitive 52.2%, 61.1%, 86.3%, respectively, while mean IoU of free space is 87.8%. Especially, the occupancy map also makes difference in improving prediction precision of object orientation dramatically to averaged 64.0%.},
  archive   = {C_ICRA},
  author    = {Yinbao Li and Songshan Yu and Dongfeng Wang and Jingen Jiao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611324},
  month     = {5},
  pages     = {1999-2005},
  title     = {MTRadSSD: A multi-task single-stage detector for object detection and free space analysis in radar point clouds*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VeloVox: A low-cost and accurate 4D object detector with
single-frame point cloud of livox LiDAR. <em>ICRA</em>, 1992–1998. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Combining motion prediction in LiDAR-based 3D object detection is an effective method for improving overall accuracy, especially the downstream autonomous driving tasks. The recent development of low-cost LiDARs (e.g. Livox LiDAR) enables us to explore such 4D perception systems with a lower budget and higher performance. In this paper, we propose a 4D object detector, VeloVox, to establish accurate object detection and velocity estimation with a single-frame point cloud of Livox LiDAR. Based on the non-repetitive scanning pattern and point-level temporal nature, we propose a two-stage module to enhance the spatial-temporal point feature interaction along the time dimension. The aggregated feature also benefits a more accurate proposal refinement. To demonstrate the performance, comparison of VeloVox with several SOTA detector based baselines is evaluated on our in-house dataset and synthesized dataset built under Carla simulation. Code will be released at https://github.com/PJLab-ADG/VeloVox.},
  archive   = {C_ICRA},
  author    = {Tao Ma and Zhiwei Zheng and Hongbin Zhou and Xinyu Cai and Xuemeng Yang and Yikang Li and Botian Shi and Hongsheng Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610427},
  month     = {5},
  pages     = {1992-1998},
  title     = {VeloVox: A low-cost and accurate 4D object detector with single-frame point cloud of livox LiDAR},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IFFNeRF: Initialisation free and fast 6DoF pose estimation
from a single image and a NeRF model. <em>ICRA</em>, 1985–1991. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce IFFNeRF to estimate the six degrees-of-freedom (6DoF) camera pose of a given image, building on the Neural Radiance Fields (NeRF) formulation. IFFNeRF is specifically designed to operate in real-time and eliminates the need for an initial pose guess that is proximate to the sought solution. IFFNeRF utilizes the Metropolis-Hasting algorithm to sample surface points from within the NeRF model. From these sampled points, we cast rays and deduce the color for each ray through pixel-level view synthesis. The camera pose can then be estimated as the solution to a Least Squares problem by selecting correspondences between the query image and the resulting bundle. We facilitate this process through a learned attention mechanism, bridging the query image embedding with the embedding of parameterized rays, thereby matching rays pertinent to the image. Through synthetic and real evaluation settings, we show that our method can improve the angular and translation error accuracy by 80.1% and 67.3%, respectively, compared to iNeRF while performing at 34fps on consumer hardware and not requiring the initial pose guess. Project page: https://mbortolon97.github.io/frenerf/},
  archive   = {C_ICRA},
  author    = {Matteo Bortolon and Theodore Tsesmelis and Stuart James and Fabio Poiesi and Alessio Del Bue},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610425},
  month     = {5},
  pages     = {1985-1991},
  title     = {IFFNeRF: Initialisation free and fast 6DoF pose estimation from a single image and a NeRF model},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalized partially destructive disassembly planning for
robotic disassembly. <em>ICRA</em>, 1978–1984. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While robotic assembly is a well researched topic, recycling and disassembly of products are also becoming ever more important as we transition to a more sustainable economy. In disassembly, we are typically only interested in a subset of product parts, which opens the possibility of using destructive processes such as tearing, cutting, or milling to speed up the disassembly. Currently, such destructive actions are only included as predefined case-specific actions such as milling away a screw head. By contrast, this paper presents a generalized approach to destructive disassembly planning that can automatically derive destructive disassembly actions from a symbolic representation of the disassembly state. Viable destructive actions are identified and verified only based on the underlying geometric model, circumventing the need for their explicit definition. We showcase the performance of this system both virtually on several test parts and physically by destructively and non-destructively disassembling a model of an electric motor using a robot manipulator with a multitool end effector.},
  archive   = {C_ICRA},
  author    = {Malte Hansjosten and Jan Baumgärtner and Jürgen Fleischer},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610546},
  month     = {5},
  pages     = {1978-1984},
  title     = {Generalized partially destructive disassembly planning for robotic disassembly},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design of highly repeatable and multi-functional grippers
for precision handling with articulated robots. <em>ICRA</em>,
1971–1977. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel approach to designing, a low-cost gripper that is highly repeatable and functionally integrated. The gripper is optimized to compensate for gripping errors with particular consideration to potential challenges of articulated robots. The primary design goal is to achieve maximum repeatability during the gripping and releasing stages of a pick-and-place process for a chip-like silicon die. The design is centered around a custom printed circuit board integrates functionality for vision-based error compensation, vacuum level monitoring, part contact detection, and detection of abnormal vibrations. We detail our design requirements and specific design choices for the mechanical and electronic design and provide qualitative and quantitative experimental validation of the achieved repeatability and the integrated functions.},
  archive   = {C_ICRA},
  author    = {Philip Gümbel and Klaus Dröder},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611540},
  month     = {5},
  pages     = {1971-1977},
  title     = {Design of highly repeatable and multi-functional grippers for precision handling with articulated robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Design and fabrication of a novel miniature magnetic
gripper. <em>ICRA</em>, 1964–1970. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Small-scale robots hold significant promise in the field of minimally invasive surgery (MIS). In this paper, we present a miniature magnetic gripper and develop a data-driven kinematic model. The gripper comprises four fingers, wherein each finger has a maximum size not exceeding 3mm, 4mm and 5.5mm in three dimensions. By integrating permanent magnets and elastic ropes as internal actuation elements into the fingers, the gripper is equipped with the capability to open-close under an external magnetic field, facilitating the manipulation of small objects in confined spaces. Modeling and analysis of the magnetic gripper are undertaken, wherein the relationship between the open angle and the external magnetic field is established. The average error between the experimentally observed open angles and the model-predicted values is 2.31°. Subsequent experiments demonstrated the necessity of the magnetic gripper model for precise manipulation, verified its excellent sensitivity to magnetic fields, and demonstrated its potential for future applications in MIS.},
  archive   = {C_ICRA},
  author    = {Mengde Li and Fuqiang Zhao and Xiangli Li and Mingchang Li and Sheng Liu and Miao Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611688},
  month     = {5},
  pages     = {1964-1970},
  title     = {Design and fabrication of a novel miniature magnetic gripper},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Bionic soft fingers with hybrid variable stiffness
mechanisms for multimode grasping. <em>ICRA</em>, 1957–1963. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel Bionic Soft Finger (BSF) that aims to overcome the limitations of conventional rigid manipulators in terms of adaptability and safety, as well as the challenges faced by soft hands regarding carrying capacity and stability. The BSF design uses a hybrid variable stiffness mechanism combining memory alloy actuators with particle jamming to achieve the desired bending angle and actuator stiffness. Our innovative approach utilizes a bionic finger design that incorporates a memory alloy skeleton and a water-cooled recirculation system, leading to a substantial reduction in the time required for each operation. Through the integration of particle jamming, we have enhanced the overall stiffness and performance of the manipulator, enabling load capacities of up to 3N per finger and more than twice the stiffness of a normal condition. Additionally, our design enables multimode grasping and incorporates a liquid metal strain sensor (METT) for real-time monitoring of finger bending angles. Comparative analyses demonstrate that our design exhibits superior stiffness and enables five-mode grasping in comparison to pneumatic actuators. We believe that bionic soft fingers present a promising solution for enhancing adaptability, safety, and performance in human-robot interaction applications.},
  archive   = {C_ICRA},
  author    = {Xiangbo Wang and Tianran Zhang and Hongze Yu and Zhenwei Wen and Lide Fang and Huaping Liu and Fuchun Sun and Lixue Tang and Bin Fang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611680},
  month     = {5},
  pages     = {1957-1963},
  title     = {Bionic soft fingers with hybrid variable stiffness mechanisms for multimode grasping},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vertical vibratory transport of grasped parts using impacts.
<em>ICRA</em>, 1950–1956. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we use impact-induced acceleration in conjunction with periodic stick-slip to successfully and quickly transport parts vertically against gravity. We show analytically that vertical vibratory transport is more difficult than its horizontal counterpart, and provide guidelines for achieving optimal vertical vibratory transport of a part. Namely, such a system must be capable of quickly realizing high accelerations, as well as supply normal forces at least several times that required for static equilibrium. We also show that for a given maximum acceleration, there is an optimal normal force for transport. To test our analytical guidelines, we built a vibrating surface using flexures and a voice coil actuator that can accelerate a magnetic ram into various materials to generate impacts. The surface was used to transport a part against gravity. Experimentally obtained motion tracking data confirmed the theoretical model. A series of grasping tests with a vibrating-surface equipped parallel jaw gripper confirmed the design guidelines.},
  archive   = {C_ICRA},
  author    = {C. L. Yako and Jérôme Nowak and Shenli Yuan and Kenneth Salisbury},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610769},
  month     = {5},
  pages     = {1950-1956},
  title     = {Vertical vibratory transport of grasped parts using impacts},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accelerating robotic picking of rigid objects with a
compliant pneumatic gripper and an impact-aware trajectory plan.
<em>ICRA</em>, 1944–1949. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Industrial robots are capable of moving at high speed. Each time they come into contact with their environment, e.g. to pick up an object, they decelerate to a near standstill. A solution involving a compliant pneumatic gripper and adapted trajectory plan is presented to initiate contact at a higher speed while remaining within hardware limits. By adding overload clutches in either the robot arm or gripper, tolerance to errors is provided. The key parameters such as gripper compliance and maximum allowed initial impact velocity are identified. Results show that by properly optimizing these parameters, robot picking of rigid objects can be accelerated. The complete high-speed picking solution is experimentally verified. A time reduction of 16% was obtained when making contact at 0.65 m/s.},
  archive   = {C_ICRA},
  author    = {Frederik Ostyn and Bram Vanderborght and Guillaume Crevecoeur},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611505},
  month     = {5},
  pages     = {1944-1949},
  title     = {Accelerating robotic picking of rigid objects with a compliant pneumatic gripper and an impact-aware trajectory plan},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A force-controlled gripper capable of measuring mechanical
properties of an object. <em>ICRA</em>, 1929–1935. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Various sensorized grippers have been developed to handle delicate objects safely. These grippers have sensors mounted on their fingers’ surface that provide direct force measurements. However, multiple sensors are often required on one finger, leading to significant sensor placement and wire routing complexity. Finger-based sensors are limited to sensing external gripping force, and fingers cannot be easily replaced to meet the requirements of objects with specific geometries. To overcome the complexity and limitations of finger surface sensors, this paper proposes a force-controlled two-fingered gripper that relies on the deformation sensing of elastic elements in the drivetrain to obtain finger force. By using a minimum number of optical encoders placed in the drivetrain, accurate position and force sensing can be achieved at any location of each finger. When gripping an object, the size and stiffness of the object can thus be accurately measured. Simulation and experimental results demonstrate the proposed gripper’s merits. We expect this new gripper to provide a more competitive solution for robots that need to manipulate objects and check their mechanical qualities at the same time.},
  archive   = {C_ICRA},
  author    = {Yi-Shian Tsai and Pin-Chun Yeh and Chun-Hung Huang and I-Cheng Hsueh and Chao-Chieh Lan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610496},
  month     = {5},
  pages     = {1929-1935},
  title     = {A force-controlled gripper capable of measuring mechanical properties of an object},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scaling object-centric robotic manipulation with multimodal
object identification. <em>ICRA</em>, 1913–1920. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic manipulation is a key enabler for automation in the fulfillment logistics sector. Such robotic systems require perception and manipulation capabilities to handle a wide variety of objects. Existing systems either operate on a closed set of objects or perform object-agnostic manipulation which lacks the capability for deliberate and reliable manipulation at scale. Object identification (ID) unlocks the ability for large-scale, object-centric manipulation by mapping object segments to one of the previously seen objects from a database. Nevertheless, it is often limited by the availability of reference data or coverage for objects in a database. In this work, we propose to perform object identification with multiple reference databases, including images and text references, each with a different coverage and matching challenge. We propose a training strategy that tackles the challenges of learning domain-invariant image embeddings, image-text matching and fusing predictions from different sources. We perform experiments over a recent benchmark with over 190K+ unique objects, extend the dataset with the additional reference sources and propose an evaluation strategy that simulates coverage for different reference sources. Model trained with the proposed learning pipeline shows robust performance over a range of simulation experiments.},
  archive   = {C_ICRA},
  author    = {Chaitanya Mitash and Mostafa Hussein and Jeroen Vanbaar and Vikedo Terhuja and Kapil Katyal},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611181},
  month     = {5},
  pages     = {1913-1920},
  title     = {Scaling object-centric robotic manipulation with multimodal object identification},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Physical and digital adversarial attacks on grasp quality
networks. <em>ICRA</em>, 1907–1902. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Grasp Quality Networks are important components of grasping-capable autonomous robots, as they allow them to evaluate grasp candidates and select the one with highest chance of success. The widespread use of pick-and-place robots and Grasp Quality Networks raises the question of whether such systems are vulnerable to adversarial attacks, as that could lead to large economic damage. In this paper we propose two kinds of attacks on Grasp Quality Networks, one assuming physical access to the workspace (to place or attach a new object) and another assuming digital access to the camera software (to inject a pixel-intensity change on a single pixel). We then use evolutionary optimization to obtain attacks that simultaneously minimize the noticeability of the attacks and the chance that selected grasps are successful. Our experiments show that both kinds of attack lead to drastic drops in algorithm performance, thus making them important attacks to consider in the cybersecurity of grasping robots. Source code can be found at https://github.com/Naif-W-Alharthi/Physical-and-Digital-Attacks-on-Grasping-Networks},
  archive   = {C_ICRA},
  author    = {Naif Wasel Alharthi and Martim Brandão},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610886},
  month     = {5},
  pages     = {1907-1902},
  title     = {Physical and digital adversarial attacks on grasp quality networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FuncGrasp: Learning object-centric neural grasp functions
from single annotated example object. <em>ICRA</em>, 1900–1906. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present FuncGrasp, a framework that can infer dense yet reliable grasp configurations for unseen objects using one annotated object and single-view RGB-D observation via categorical priors. Unlike previous works that only transfer a set of grasp poses, FuncGrasp aims to transfer infinite configurations parameterized by an object-centric continuous grasp function across varying instances. To ease the transfer process, we propose Neural Surface Grasping Fields (NSGF), an effective neural representation defined on the surface to densely encode grasp configurations. Further, we exploit function-to-function transfer using sphere primitives to establish semantically meaningful categorical correspondences, which are learned in an unsupervised fashion without any expert knowledge. We showcase the effectiveness through extensive experiments in both simulators and the real world. Remarkably, our framework significantly outperforms several strong baseline methods in terms of density and reliability for generated grasps.},
  archive   = {C_ICRA},
  author    = {Hanzhi Chen and Binbin Xu and Stefan Leutenegger},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611233},
  month     = {5},
  pages     = {1900-1906},
  title     = {FuncGrasp: Learning object-centric neural grasp functions from single annotated example object},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning realistic and reasonable grasps for anthropomorphic
hand in cluttered scenes. <em>ICRA</em>, 1893–1899. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Grasping is one of the most fundamental skills for humans to interact with objects. However, it remains a challenging problem for anthropomorphic hands, due to the lack of object affordance understanding and high-dimensional grasp planning. In this work, we propose an anthropomorphic hand grasping framework to learn realistic and reasonable grasps in cluttered scenes, which tackles the problem in three items: 1) graspable point segmentation; 2) hand grasp generation and 3) grasp optimization. Specifically, our method generates high-quality hand grasps efficiently without complete object models by learning graspable points, associated grasp configurations from observed point cloud in a parallel manner and optimizing predicted grasps based on hand-object contacts. Simulation experiments show that our model generates physical plausible grasps for the anthropomorphic hand effectively with over 70% success rate. Real-world experiments demonstrate that the model trained in simulation performs satisfactorily in real-world scenarios for unseen objects.},
  archive   = {C_ICRA},
  author    = {Haonan Duan and Yiming Li and Daheng Li and Wei Wei and Yayu Huang and Peng Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610646},
  month     = {5},
  pages     = {1893-1899},
  title     = {Learning realistic and reasonable grasps for anthropomorphic hand in cluttered scenes},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Amortized inference for efficient grasp model adaptation.
<em>ICRA</em>, 1886–1892. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In robotic applications such as bin-picking or block-stacking, learned predictive models have been developed for manipulation of objects with varying but known dynamic properties (e.g., mass distributions and friction coefficients). When a robot encounters a new object, these properties are often difficult to observe and must be inferred through interaction, which can be expensive in both inference time and number of interactions. We propose an encoder/decoder action-feasibility model to efficiently adapt to new objects by estimating their unobserved properties through interaction. The encoder predicts a distribution over the unobserved parameters while the decoder predicts action feasibility, which can be used in an uncertainty-aware planner. An explicit representation of uncertainty in the encoder enables information-gathering heuristics to minimize adaptation interactions. The amortized distributions are efficient to compute and perform comparably to particle-based distributions in a grasping domain. Finally, we deploy our method on a Panda robot to grasp heavy objects.},
  archive   = {C_ICRA},
  author    = {Michael Noseworthy and Seiji Shaw and Chad C. Kessens and Nicholas Roy},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610789},
  month     = {5},
  pages     = {1886-1892},
  title     = {Amortized inference for efficient grasp model adaptation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient 3D instance mapping and localization with neural
fields. <em>ICRA</em>, 1818–1824. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We tackle the problem of learning an implicit scene representation for 3D instance segmentation from a sequence of posed RGB images. Towards this, we introduce 3DIML, a novel framework that efficiently learns a label field that may be rendered from novel viewpoints to produce view-consistent instance segmentation masks. 3DIML significantly improves upon training and inference runtimes of existing implicit scene representation based methods. Opposed to prior art that optimizes a neural field in a self-supervised manner, requiring complicated training procedures and loss function design, 3DIML leverages a two-phase process. The first phase, InstanceMap, takes as input 2D segmentation masks of the image sequence generated by a frontend instance segmentation model, and associates corresponding masks across images to 3D labels. These almost view-consistent pseudolabel masks are then used in the second phase, InstanceLift, to supervise the training of a neural label field, which interpolates regions missed by InstanceMap and resolves ambiguities. Additionally, we introduce InstanceLoc, which enables near realtime localization of instance masks given a trained label field and an off-the-shelf image segmentation model by fusing outputs from both. We evaluate 3DIML on sequences from the Replica and ScanNet datasets and demonstrate 3DIML’s effectiveness under mild assumptions for the image sequences. We achieve a 14-24× speedup over existing implicit scene representation methods with comparable quality, showcasing its potential to facilitate faster and more effective 3D scene understanding.},
  archive   = {C_ICRA},
  author    = {George Tang and Krishna Murthy Jatavallabhula and Antonio Torralba},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611715},
  month     = {5},
  pages     = {1818-1824},
  title     = {Efficient 3D instance mapping and localization with neural fields},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DynaInsRemover: A real-time dynamic instance-aware static 3D
LiDAR mapping framework for dynamic environment. <em>ICRA</em>,
1803–1809. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dynamic objects diversify the distribution of point cloud in the map, degrading the performance of the robotic downstream tasks. To address this problem, we present a novel real-time dynamic instance-aware static mapping framework called DynaInsRemover, which exploits the geometric discrepancies between instances to efficiently remove dynamic objects and preserve more details of static map. It contains the Instance Occupancy Check module for initial dynamic instance proposal and the Instance Belief Update module for reverting false positives. We quantitatively evaluate our approach performance on the SemanticKITTI dataset and validate it in a real-world environment. Experimental evaluations show that our method achieves very promising results in dynamic environments. The implementation of our method is available as open source at: https://github.com/Zhaohuanfeng/DynaInsRemover.git.},
  archive   = {C_ICRA},
  author    = {Huanfeng Zhao and Meibao Yao and Xueming Xiao and Bo Zheng},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610211},
  month     = {5},
  pages     = {1803-1809},
  title     = {DynaInsRemover: A real-time dynamic instance-aware static 3D LiDAR mapping framework for dynamic environment},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3D-BBS: Global localization for 3D point cloud scan matching
using branch-and-bound algorithm. <em>ICRA</em>, 1796–1802. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an accurate and fast 3D global localization method, 3D-BBS, that extends the existing branchand-bound (BnB)-based 2D scan matching (BBS) algorithm. To reduce memory consumption, we utilize a sparse hash table for storing hierarchical 3D voxel maps. To improve the processing cost of BBS in 3D space, we propose an efficient roto-translational space branching. Furthermore, we devise a batched BnB algorithm to fully leverage GPU parallel processing. Through experiments in simulated and real environments, we demonstrated that the 3D-BBS enabled accurate global localization with only a 3D LiDAR scan roughly aligned in the gravity direction and a 3D pre-built map. This method required only 878 msec on average to perform global localization and outperformed state-of-the-art global registration methods in terms of accuracy and processing speed.},
  archive   = {C_ICRA},
  author    = {Koki Aoki and Kenji Koide and Shuji Oishi and Masashi Yokozuka and Atsuhiko Banno and Junichi Meguro},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610810},
  month     = {5},
  pages     = {1796-1802},
  title     = {3D-BBS: Global localization for 3D point cloud scan matching using branch-and-bound algorithm},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TSCM: A teacher-student model for vision place recognition
using cross-metric knowledge distillation. <em>ICRA</em>, 1789–1795. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual place recognition (VPR) plays a pivotal role in autonomous exploration and navigation of mobile robots within complex outdoor environments. While cost-effective and easily deployed, camera sensors are sensitive to lighting and weather changes, and even slight image alterations can greatly affect VPR efficiency and precision. Existing methods overcome this by exploiting powerful yet large networks, leading to significant consumption of computational resources. In this paper, we propose a high-performance teacher and lightweight student distillation framework called TSCM. It exploits our devised cross-metric knowledge distillation to narrow the performance gap between the teacher and student models, maintaining superior performance while enabling minimal computational load during deployment. We conduct comprehensive evaluations on large-scale datasets, namely Pittsburgh30k and Pittsburgh250k. Experimental results demonstrate the superiority of our method over baseline models in terms of recognition accuracy and model parameter efficiency. Moreover, our ablation studies show that the proposed knowledge distillation technique surpasses other counterparts. The code of our method has been released at https://github.com/nubot-nudt/TSCM.},
  archive   = {C_ICRA},
  author    = {Yehui Shen and Mingmin Liu and Huimin Lu and Xieyuanli Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611612},
  month     = {5},
  pages     = {1789-1795},
  title     = {TSCM: A teacher-student model for vision place recognition using cross-metric knowledge distillation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Follow the footprints: Self-supervised traversability
estimation for off-road vehicle navigation based on geometric and visual
cues. <em>ICRA</em>, 1774–1780. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this study, we address the off-road traversability estimation problem, that predicts areas where a robot can navigate in off-road environments. An off-road environment is an unstructured environment comprising a combination of traversable and non-traversable spaces, which presents a challenge for estimating traversability. This study highlights three primary factors that affect a robot’s traversability in an off-road environment: surface slope, semantic information, and robot platform. We present two strategies for estimating traversability, using a guide filter network (GFN) and footprint supervision module (FSM). The first strategy involves building a novel GFN using a newly designed guide filter layer. The GFN interprets the surface and semantic information from the input data and integrates them to extract features optimized for traversability estimation. The second strategy involves developing an FSM, which is a self-supervision module that utilizes the path traversed by the robot in pre-driving, also known as a footprint. This enables the prediction of traversability that reflects the characteristics of the robot platform. Based on these two strategies, the proposed method overcomes the limitations of existing methods, which require laborious human supervision and lack scalability. Extensive experiments in diverse conditions, including automobiles and unmanned ground vehicles, herbfields, woodlands, and farmlands, demonstrate that the proposed method is compatible for various robot platforms and adaptable to a range of terrains. Code is available at https://github.com/yurimjeon1892/FtFoot.},
  archive   = {C_ICRA},
  author    = {Yurim Jeon and E In Son and Seung-Woo Seo},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611198},
  month     = {5},
  pages     = {1774-1780},
  title     = {Follow the footprints: Self-supervised traversability estimation for off-road vehicle navigation based on geometric and visual cues},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). V-STRONG: Visual self-supervised traversability learning for
off-road navigation. <em>ICRA</em>, 1766–1773. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reliable estimation of terrain traversability is critical for the successful deployment of autonomous systems in wild, outdoor environments. Given the lack of large-scale annotated datasets for off-road navigation, strictly-supervised learning approaches remain limited in their generalization ability. To this end, we introduce a novel, image-based self-supervised learning method for traversability prediction, leveraging a state-of-the-art vision foundation model for improved out-of-distribution performance. Our method employs contrastive representation learning using both human driving data and instance-based segmentation masks during training. We show that this simple, yet effective, technique drastically outperforms recent methods in predicting traversability for both on- and off-trail driving scenarios. We compare our method with recent baselines on both a common benchmark as well as our own datasets, covering a diverse range of outdoor environments and varied terrain types. We also demonstrate the compatibility of resulting costmap predictions with a model-predictive controller. Finally, we evaluate our approach on zero- and few-shot tasks, demonstrating unprecedented performance for generalization to new environments. Videos and additional material can be found here: https://sites.google.com/view/visual-traversability-learning.},
  archive   = {C_ICRA},
  author    = {Sanghun Jung and JoonHo Lee and Xiangyun Meng and Byron Boots and Alexander Lambert},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611227},
  month     = {5},
  pages     = {1766-1773},
  title     = {V-STRONG: Visual self-supervised traversability learning for off-road navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An onboard framework for staircases modeling based on point
clouds. <em>ICRA</em>, 1759–1765. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The detection of traversable regions on staircases and the physical modeling constitutes pivotal aspects of the mobility of legged robots. This paper presents an onboard framework tailored to the detection of traversable regions and the modeling of physical attributes of staircases by point cloud data. To mitigate the influence of illumination variations and the overfitting due to the dataset diversity, a series of data augmentations are introduced to enhance the training of the fundamental network. A curvature suppression cross-entropy(CSCE) loss is proposed to reduce the ambiguity of prediction on the boundary between traversable and non-traversable regions. Moreover, a measurement correction based on the pose estimation of stairs is introduced to calibrate the output of raw modeling that is influenced by tilted perspectives. Lastly, we collect a dataset pertaining to staircases and introduce new evaluation criteria. Through a series of rigorous experiments conducted on this dataset, we substantiate the superior accuracy and generalization capabilities of our proposed method. Codes, models, and datasets will be available at https://github.com/szturobotics/Stair-detection-and-modeling-project.},
  archive   = {C_ICRA},
  author    = {Chun Qing and Rongxiang Zeng and Xuan Wu and Yongliang Shi and Gan Ma},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610407},
  month     = {5},
  pages     = {1759-1765},
  title     = {An onboard framework for staircases modeling based on point clouds},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SPOT: Point cloud based stereo visual place recognition for
similar and opposing viewpoints. <em>ICRA</em>, 1752–1758. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recognizing places from an opposing viewpoint during a return trip is a common experience for human drivers. However, the analogous robotics capability, visual place recognition (VPR) with limited field of view cameras under 180 degree rotations, has proven to be challenging to achieve. To address this problem, this paper presents Same Place Opposing Trajectory (SPOT), a technique for opposing viewpoint VPR that relies exclusively on structure estimated through stereo visual odometry (VO). The method extends recent advances in lidar descriptors and utilizes a novel double (similar and opposing) distance matrix sequence matching method. We evaluate SPOT on a publicly available dataset with 6.7-7.6 km routes driven in similar and opposing directions under various lighting conditions. The proposed algorithm demonstrates remarkable improvement over the state-of-the-art, achieving up to 91.7% recall at 100% precision in opposing viewpoint cases, while requiring less storage than all baselines tested and running faster than all but one. Moreover, the proposed method assumes no a priori knowledge of whether the viewpoint is similar or opposing, and also demonstrates competitive performance in similar viewpoint cases.},
  archive   = {C_ICRA},
  author    = {Spencer Carmichael and Rahul Agrawal and Ram Vasudevan and Katherine A. Skinner},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610293},
  month     = {5},
  pages     = {1752-1758},
  title     = {SPOT: Point cloud based stereo visual place recognition for similar and opposing viewpoints},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Tightly coupled range inertial localization on a 3D prior
map based on sliding window factor graph optimization. <em>ICRA</em>,
1745–1751. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a range inertial localization algorithm for a 3D prior map. The proposed algorithm tightly couples scan-to-scan and scan-to-map point cloud registration factors along with IMU factors on a sliding window factor graph. The tight coupling of the scan-to-scan and scan-to-map registration factors enables a smooth fusion of sensor ego-motion estimation and map-based trajectory correction that results in robust tracking of the sensor pose under severe point cloud degeneration and defective regions in a map. We also propose an initial sensor state estimation algorithm that robustly estimates the gravity direction and IMU state and helps perform global localization in 3- or 4-DoF for system initialization without prior position information. Experimental results show that the proposed method outperforms existing state-of-the-art methods in extremely severe situations where the point cloud data becomes degenerate, there are momentary sensor interruptions, or the sensor moves along the map boundary or into unmapped regions.},
  archive   = {C_ICRA},
  author    = {Kenji Koide and Shuji Oishi and Masashi Yokozuka and Atsuhiko Banno},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611195},
  month     = {5},
  pages     = {1745-1751},
  title     = {Tightly coupled range inertial localization on a 3D prior map based on sliding window factor graph optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). MegaParticles: Range-based 6-DoF monte carlo localization
with GPU-accelerated stein particle filter. <em>ICRA</em>, 1738–1744.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a 6-DoF range-based Monte Carlo localization method with a GPU-accelerated Stein particle filter. To update a massive amount of particles, we propose a Gauss-Newton-based Stein variational gradient descent (SVGD) with iterative neighbor particle search. This method uses SVGD to collectively update particle states with gradient and neighborhood information, which provides efficient particle sampling. For an efficient neighbor particle search, it uses locality sensitive hashing and iteratively updates the neighbor list of each particle over time. The neighbor list is then used to propagate the posterior probabilities of particles over the neighbor particle graph. The proposed method is capable of evaluating one million particles in real-time on a single GPU and enables robust pose initialization and re-localization without an initial pose estimate. In experiments, the proposed method showed an extreme robustness to complete sensor occlusion (i.e., kidnapping), and enabled pinpoint sensor localization without any prior information.},
  archive   = {C_ICRA},
  author    = {Kenji Koide and Shuji Oishi and Masashi Yokozuka and Atsuhiko Banno},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610447},
  month     = {5},
  pages     = {1738-1744},
  title     = {MegaParticles: Range-based 6-DoF monte carlo localization with GPU-accelerated stein particle filter},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). COIN-LIO: Complementary intensity-augmented LiDAR inertial
odometry. <em>ICRA</em>, 1730–1737. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present COIN-LIO, a LiDAR Inertial Odometry pipeline that tightly couples information from LiDAR intensity with geometry-based point cloud registration. The focus of our work is to improve the robustness of LiDAR-inertial odometry in geometrically degenerate scenarios, like tunnels or flat fields. We project LiDAR intensity returns into an image, and present a novel image processing pipeline that produces filtered images with improved brightness consistency within the image as well as across different scenes. We effectively leverage intensity as an additional modality, using our new feature selection scheme that detects uninformative directions in the point cloud registration and explicitly selects patches with complementary image information. Photometric error minimization in the image patches is then fused with inertial measurements and point-to-plane registration in an iterated Extended Kalman Filter. The proposed approach improves accuracy and robustness on a public dataset. We additionally publish a new dataset, that captures five real-world environments in challenging, geometrically degenerate scenes. By using the additional photometric information, our approach shows drastically improved robustness against geometric degeneracy in environments where all compared baseline approaches fail.},
  archive   = {C_ICRA},
  author    = {Patrick Pfreundschuh and Helen Oleynikova and Cesar Cadena and Roland Siegwart and Olov Andersson},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610938},
  month     = {5},
  pages     = {1730-1737},
  title     = {COIN-LIO: Complementary intensity-augmented LiDAR inertial odometry},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Colmap-PCD: An open-source tool for fine image-to-point
cloud registration. <em>ICRA</em>, 1723–1729. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {State-of-the-art techniques for monocular camera reconstruction predominantly rely on the Structure from Motion (SfM) pipeline. However, such methods often yield reconstruction outcomes that lack crucial scale information, and over time, accumulation of images leads to inevitable drift issues. In contrast, mapping methods based on LiDAR scans are popular in large-scale urban scene reconstruction due to their precise distance measurements, a capability fundamentally absent in visual-based approaches. Researchers have made attempts to utilize concurrent LiDAR and camera measurements in pursuit of precise scaling and color details within mapping outcomes. However, the outcomes are subject to extrinsic calibration and time synchronization precision. In this paper, we propose a novel cost-effective reconstruction pipeline that utilizes a pre-established LiDAR map as a fixed constraint to effectively address the inherent scale challenges present in monocular camera reconstruction. To our knowledge, our method is the first to register images onto the point cloud map without requiring synchronous capture of camera and LiDAR data, granting us the flexibility to manage reconstruction detail levels across various areas of interest. To facilitate further research in this domain, we have released Colmap-PCD 3 , an open-source tool leveraging the Colmap algorithm, that enables precise fine-scale registration of images to the point cloud map.},
  archive   = {C_ICRA},
  author    = {Chunge Bai and Ruijie Fu and Xiang Gao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611582},
  month     = {5},
  pages     = {1723-1729},
  title     = {Colmap-PCD: An open-source tool for fine image-to-point cloud registration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Subsurface feature-based ground robot/vehicle localization
using a ground penetrating radar. <em>ICRA</em>, 1716–1722. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot localization using subsurface features captured by Ground-Penetrating Radar (GPR) complements and improves robustness over existing common sensor modalities, as subsurface features are less sensitive to weather, season and surface scene changes. Here, we propose a novel subsurface feature-based localization method that uses only GPR measurements with a known subsurface map. An efficient feature descriptor, the dominant energy curve (DEC), is designed to identify different locations in cluttered conditions. Specifically, image processing techniques that involve background segmentation, energy point detection, and energy curve refinement are designed to extract DEC features from a 2D radargram. With DECs features obtained, a metric subsurface feature map is constructed. Finally, we perform robot localization by feature matching under a particle swarm optimization framework. We have implemented our method and tested it with the public CMU-GPR dataset. The results show that our algorithm improves accuracy and robustness with real-time performance for robot localization tasks. Specifically, the mean localization error is 0.50 m for all cases.},
  archive   = {C_ICRA},
  author    = {Haifeng Li and Jiajun Guo and Dezhen Song},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611579},
  month     = {5},
  pages     = {1716-1722},
  title     = {Subsurface feature-based ground Robot/Vehicle localization using a ground penetrating radar},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Block-map-based localization in large-scale environment.
<em>ICRA</em>, 1709–1715. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate localization is an essential technology for the flexible navigation of robots in large-scale environments. Both SLAM-based and map-based localization will increase the computing load due to the increase in map size, which will affect downstream tasks such as robot navigation and services. To this end, we propose a localization system based on Block Maps (BMs) to reduce the computational load caused by maintaining large-scale maps. Firstly, we introduce a method for generating block maps and the corresponding switching strategies, ensuring that the robot can estimate the state in large-scale environments by loading local map information. Secondly, global localization according to Branch-and-Bound Search (BBS) in the 3D map is introduced to provide the initial pose. Finally, a graph-based optimization method is adopted with a dynamic sliding window that determines what factors are being marginalized whether a robot is exposed to a BM or switching to another one, which maintains the accuracy and efficiency of pose tracking. Comparison experiments are performed on publicly available large-scale datasets. Results show that the proposed method can track the robot pose even though the map scale reaches more than 6 kilometers, while efficient and accurate localization is still guaranteed on NCLT [6] and M2DGR [35]. Codes and data will be publicly available on https://github.com/YixFeng/blocklocalization.},
  archive   = {C_ICRA},
  author    = {Yixiao Feng and Zhou Jiang and Yongliang Shi and Yunlong Feng and Xiangyu Chen and Hao Zhao and Guyue Zhou},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610122},
  month     = {5},
  pages     = {1709-1715},
  title     = {Block-map-based localization in large-scale environment},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Salience-guided ground factor for robust localization of
delivery robots in complex urban environments. <em>ICRA</em>, 1701–1708.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In urban environments for delivery robots, particularly in areas such as campuses and towns, many custom features defy standard road semantic categorizations. Addressing this challenge, our paper introduces a method leveraging Salient Object Detection (SOD) to extract these unique features, employing them as pivotal factors for enhanced robot loop closure and localization. Traditional geometric feature-based localization is hampered by fluctuating illumination and appearance changes. Our preference for SOD over semantic segmentation sidesteps the intricacies of classifying a myriad of non-standardized urban features. To achieve consistent ground features, the Motion Compensate IPM (MC-IPM) technique is implemented, capitalizing on motion for distortion compensation and subsequently selecting the most pertinent salient ground features through moment computations. For thorough evaluation, we validated the saliency detection and localization performances to the real urban scenarios. Project page: https://sites.google.com/view/salient-ground-feature/home.},
  archive   = {C_ICRA},
  author    = {Jooyong Park and Jungwoo Lee and Euncheol Choi and Younggun Cho},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611696},
  month     = {5},
  pages     = {1701-1708},
  title     = {Salience-guided ground factor for robust localization of delivery robots in complex urban environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MAVIS: Multi-camera augmented visual-inertial SLAM using
SE2(3) based exact IMU pre-integration. <em>ICRA</em>, 1694–1700. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel optimization-based Visual-Inertial SLAM system designed for multiple partially over-lapped camera systems, named MAVIS. Our framework fully exploits the benefits of wide field-of-view from multi-camera systems, and the metric scale measurements provided by an inertial measurement unit (IMU). We introduce an improved IMU pre-integration formulation based on the exponential function of an automorphism of SE 2 (3), which can effectively enhance tracking performance under fast rotational motion and extended integration time. Furthermore, we extend conventional front-end tracking and back-end optimization module designed for monocular or stereo setup towards multi-camera systems, and introduce implementation details that contribute to the performance of our system in challenging scenarios. The practical validity of our approach is supported by our experiments on public datasets. Our MAVIS won the first place in all the vision-IMU tracks (single and multi-session SLAM) on Hilti SLAM Challenge 2023 with 1.7 times the score compared to the second place 1 .},
  archive   = {C_ICRA},
  author    = {Yifu Wang and Yonhon Ng and Inkyu Sa and Álvaro Parra and Cristian Rodriguez-Opazo and Taojun Lin and Hongdong Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609982},
  month     = {5},
  pages     = {1694-1700},
  title     = {MAVIS: Multi-camera augmented visual-inertial SLAM using SE2(3) based exact IMU pre-integration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). JacobiGPU: GPU-accelerated numerical differentiation for
loop closure in visual SLAM. <em>ICRA</em>, 1687–1693. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce JacobiGPU, a technique that uses a GPU to improve the efficiency of loop closure in visual-inertial SLAM systems, particularly when approximating Jacobians using the Finite Difference Method (FDM). Traditional FDM techniques often face computational overhead due to repeated perturbations in pose graphs. We address this overhead with a novel methodology, leveraging strategic graph partitioning and an optimized approach to Jacobian approximation. By integrating JacobiGPU into ORB-SLAM3’s g2o, we enhance the linearization process. Our evaluation, conducted on 12 sequences of varying lengths from the EuRoC and TUM-VI datasets, demonstrated a speedup of up to 4.23x in the linearization stage and an overall enhancement of up to 2.08x in the overall optimization process.},
  archive   = {C_ICRA},
  author    = {Dhruv Kumar and Shishir Gopinath and Karthik Dantu and Steven Y. Ko},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611512},
  month     = {5},
  pages     = {1687-1693},
  title     = {JacobiGPU: GPU-accelerated numerical differentiation for loop closure in visual SLAM},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual inertial odometry using focal plane binary features
(BIT-VIO). <em>ICRA</em>, 1661–1668. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Focal-Plane Sensor-Processor Arrays (FPSP)s are an emerging technology that can execute vision algorithms directly on the image sensor. Unlike conventional cameras, FPSPs perform computation on the image plane – at individual pixels – enabling high frame rate image processing while consuming low power, making them ideal for mobile robotics. FPSPs, such as the SCAMP-5, use parallel processing and are based on the Single Instruction Multiple Data (SIMD) paradigm. In this paper, we present BIT-VIO, the first Visual Inertial Odometry (VIO) which utilises SCAMP-5. BIT-VIO is a loosely-coupled iterated Extended Kalman Filter (iEKF) which fuses together the visual odometry running fast at 300 FPS with predictions from 400 Hz IMU measurements to provide accurate and smooth trajectories. Project Page: https://sites.google.com/view/bit-vio/home},
  archive   = {C_ICRA},
  author    = {Matthew Lisondra and Junseo Kim and Riku Murai and Kourosh Zareinia and Sajad Saeedi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610838},
  month     = {5},
  pages     = {1661-1668},
  title     = {Visual inertial odometry using focal plane binary features (BIT-VIO)},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Omnidirectional dense SLAM for back-to-back fisheye cameras.
<em>ICRA</em>, 1653–1660. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a real-time visual-inertial dense SLAM system that utilizes the online data streams from back-to-back dual fisheye cameras setup, providing 360 ◦ coverage of the environment. Firstly, we employ a sliding-window-based front-end to estimate real-time poses from the binocular fisheye images and IMU data. Then, we implement a lightweight panoramic depth completion network based on multi-basis depth representation. The network takes panoramic images (obtained by stitching dual-fisheye images with extrinsics and intrinsic parameters) and sparse depths (generated by the front-end local tracking) as input and predicts multiple depth bases along with corresponding confidence as output. The final dense depth is the linear combination of the multiple depth bases. Thanks to the multi-basis depth representation, we can continuously optimize the 360° depth with the traditional optimizer to achieve higher global consistency in depth. We conducted experiments on both simulated and real-world datasets to evaluate our method. The results demonstrate that the proposed method outperforms SoTA methods in terms of depth prediction and 3D reconstruction. In addition, we develop a demo that can run on a mobile to demonstrate the real-time capabilities of our method.},
  archive   = {C_ICRA},
  author    = {Weijian Xie and Guanyi Chu and Quanhao Qian and Yihao Yu and Shangjin Zhai and Danpeng Chen and Nan Wang and Hujun Bao and Guofeng Zhangv},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610351},
  month     = {5},
  pages     = {1653-1660},
  title     = {Omnidirectional dense SLAM for back-to-back fisheye cameras},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Square-root inverse filter-based GNSS-visual-inertial
navigation. <em>ICRA</em>, 1646–1652. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While Global Navigation Satellite System (GNSS) is often used to provide global positioning if available, its intermittency and/or inaccuracy calls for fusion with other sensors. In this paper, we develop a novel GNSS-Visual-Inertial Navigation System (GVINS) that fuses visual, inertial, and raw GNSS measurements within the square-root inverse sliding window filtering (SRI-SWF) framework in a tightly coupled fashion, which thus is termed SRI-GVINS. In particular, for the first time, we deeply fuse the GNSS pseudorange, Doppler shift, single-differenced pseudorange, and double-differenced carrier phase measurements, along with the visual-inertial measurements. Inherited from the SRI-SWF, the proposed SRI-GVINS gains significant numerical stability and computational efficiency over the start-of-the-art methods. Additionally, we propose to use a filter to sequentially initialize the reference frame transformation till converges, rather than collecting measurements for batch optimization. We also perform online calibration of GNSS-IMU extrinsic parameters to mitigate the possible extrinsic parameter degradation. The proposed SRI-GVINS is extensively evaluated on our own collected UAV datasets and the results demonstrate that the proposed method is able to suppress VIO drift in real-time and also show the effectiveness of online GNSS-IMU extrinsic calibration. The experimental validation on the public datasets further reveals that the proposed SRI-GVINS outperforms the state-of-the-art methods in terms of both accuracy and efficiency.},
  archive   = {C_ICRA},
  author    = {Jun Hu and Xiaoming Lang and Feng Zhang and Yinian Mao and Guoquan Huang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611666},
  month     = {5},
  pages     = {1646-1652},
  title     = {Square-root inverse filter-based GNSS-visual-inertial navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online calibration of a single-track ground vehicle dynamics
model by tight fusion with visual-inertial odometry. <em>ICRA</em>,
1631–1637. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Wheeled mobile robots need the ability to estimate their motion and the effect of their control actions for navigation planning. In this paper, we present ST-VIO, a novel approach which tightly fuses a single-track dynamics model for wheeled ground vehicles with visual-inertial odometry (VIO). Our method calibrates and adapts the dynamics model online to improve the accuracy of forward prediction conditioned on future control inputs. The single-track dynamics model approximates wheeled vehicle motion under specific control inputs on flat ground using ordinary differential equations. We use a singularity-free and differentiable variant of the single-track model to enable seamless integration as dynamics factor into VIO and to optimize the model parameters online together with the VIO state variables. We validate our method with real-world data in both indoor and outdoor environments with different terrain types and wheels. In experiments, we demonstrate that ST-VIO can not only adapt to wheel or ground changes and improve the accuracy of prediction under new control inputs, but can even improve tracking accuracy.},
  archive   = {C_ICRA},
  author    = {Haolong Li and Joerg Stueckler},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610157},
  month     = {5},
  pages     = {1631-1637},
  title     = {Online calibration of a single-track ground vehicle dynamics model by tight fusion with visual-inertial odometry},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Field-VIO: Stereo visual-inertial odometry based on
quantitative windows in agricultural open fields. <em>ICRA</em>,
1624–1630. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In agricultural open fields, accurate autonomous localization of robots requires long-term data correlation to reduce cumulative error. Our article presents a Stereo Visual-Inertial Odometry (VIO) system based on ORB-SLAM3 to address the malfunction of the Loop Closure Detection (LCD) methods in this environment. In this method, we first propose a concept of quantitative windows to describe the robot’s trajectory along the crop rows. We design a driving state quantification algorithm and accurately separate the quantitative windows between the crop rows. Our system constructs spatial constraints according to the parallelism between the quantitative windows. We apply an anomaly correction method to maintain the constructed parallel matching relationship and implement holistic pose correction for keyframes within abnormal quantitative windows. Our system demonstrated excellent performance over long distances in experiments on the Rosario dataset, verifying its effectiveness in reducing cumulative positioning error in agricultural open fields.},
  archive   = {C_ICRA},
  author    = {Jianjing Sun and Shuang Wu and Jun Dong and Junming He},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611284},
  month     = {5},
  pages     = {1624-1630},
  title     = {Field-VIO: Stereo visual-inertial odometry based on quantitative windows in agricultural open fields},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simulation and experimental validation of an autonomous
perching and takeoff method for a multirotor UAV on vertical surfaces
using a suction cup. <em>ICRA</em>, 1617–1623. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper details the simulation and experimental validation of an autonomous perching and take-off method for a multirotor unmanned aerial vehicle (UAV) using a suction cup perching mechanism on vertical surfaces. The suction cup interaction with different surface types is characterized with experimental tests to accurately model the perching manoeuvre. The resulting model is used to develop a realistic hardware-in-the-loop (HIL) simulation of the perching and take-off manoeuvre of the UAV in Gazebo. A control method is developed to automate the perching and take-off manoeuvre. The method is tested in simulation and is experimentally validated with flight tests. Comparisons between simulation and experimental data demonstrate that the simulation is accurate and can be used to continue the development of autonomous perching methods.},
  archive   = {C_ICRA},
  author    = {Bruno Chapdelaine and Mathis Celce and Charles Vidal and Lionel Birglen and Bruno Monsarrat},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610925},
  month     = {5},
  pages     = {1617-1623},
  title     = {Simulation and experimental validation of an autonomous perching and takeoff method for a multirotor UAV on vertical surfaces using a suction cup},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Autonomous aerial perching and unperching using
omnidirectional tiltrotor and switching controller. <em>ICRA</em>,
1590–1596. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Aerial unperching of multirotors has received little attention as opposed to perching that has been investigated to elongate operation time. This study presents a new aerial robot capable of both perching and unperching autonomously on/from a ferromagnetic surface during flight, and a switching controller to avoid rotor saturation and mitigate overshoot during transition between free-flight and perching. To enable stable perching and unperching maneuvers on/from a vertical surface, a lightweight (≈ 1 kg), fully actuated tiltrotor that can hover at 90 ◦ pitch angle is first developed. We design a perching/unperching module composed of a single servomotor and a magnet, which is then mounted on the tiltrotor. A switching controller including exclusive control modes for transitions between free-flight and perching is proposed. Lastly, we propose a simple yet effective strategy to ensure robust perching in the presence of measurement and control errors and avoid collisions with the perching site immediately after unperching. We validate the proposed framework in experiments where the tiltrotor successfully performs perching and unperching on/from a vertical surface during flight. We further show effectiveness of the proposed transition mode in the switching controller by ablation studies where large overshoot and even collision with a perching site occur. To the best of the authors’ knowledge, this work presents the first autonomous aerial unperching framework using a fully actuated tiltrotor.},
  archive   = {C_ICRA},
  author    = {Dongjae Lee and Sunwoo Hwang and Jeonghyun Byun and Seung Jae Lee and H. Jin Kim},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610445},
  month     = {5},
  pages     = {1590-1596},
  title     = {Autonomous aerial perching and unperching using omnidirectional tiltrotor and switching controller},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A meter-scale ornithopter capable of jumping take-off.
<em>ICRA</em>, 1583–1589. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Flapping wing air vehicles(FWAV) or ornithopters are bio-inspired aerial robots that mimic the flying principles of insects and birds. Autonomous take-off is an important capability for FWAV to enhance its performance and extend its working time, which is equipped by almost every kind of bird. As a common method of take-off for birds, jumping take-off has a great ability to adapt to different terrain and high energy efficiency compared with running and rotor-based take-off. Despite recent research, there is no FWAV capable of jumping take-off to this day. In this paper, we present a process to realize the jumping take-off of a meter-scale FWAV from flat ground. To lower the mechanical complexity, we eliminate the design of traditional robotic legs. Instead, we realize steady standing through a tripod-like structure that consists of two wings and a jumping mechanism. Two flapping wings are directly driven by two independent servos. Three carbon fiber springs are employed to build a lightweight jumping module with high elastic energy. We build the dynamic model to analyze the aerodynamic effect during the jumping phase and realize a stable transition to flapping flight. This work lays the foundation for outdoor flight without human assistance.},
  archive   = {C_ICRA},
  author    = {Wei Yan and Genliang Chen and Zhuang Zhang and Hao Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610300},
  month     = {5},
  pages     = {1583-1589},
  title     = {A meter-scale ornithopter capable of jumping take-off},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Aerial interaction with tactile sensing. <em>ICRA</em>,
1576–1582. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While the field of autonomous Uncrewed Aerial Vehicles (UAVs) has grown rapidly, most applications only focus on passive visual tasks. Aerial interaction aims to execute tasks involving physical interactions, which offers a way to assist humans in high-altitude and high-risk operations. Tactile sensors, being both cost-effective and lightweight, are capable of sensing contact information including force distribution, as well as recognizing local textures. In this paper, we pioneer the use of vision-based tactile sensors on fully actuated UAVs in dynamic aerial manipulation tasks. We introduce a pipeline utilizing tactile feedback for force tracking via a hybrid motion-force controller and a method for wall texture detection during aerial interactions. Our experiments demonstrate that our system can effectively replace or complement traditional force/torque (F/T) sensors. Compared with only using the F/T sensor, our approach offers two solutions: substitution with tactile sensing, achieving comparable flight performance, or integration of tactile sensing with F/T sensor feedback, leading to around 16% improvement in position tracking accuracy. Our algorithm achieves 93.4% accuracy in real-time texture recognition, which further escalates to 100% in post-contact analysis. To the best of our knowledge, this is the first work to incorporate a vision-based tactile sensor into aerial interaction tasks.},
  archive   = {C_ICRA},
  author    = {Xiaofeng Guo and Guanqi He and Mohammadreza Mousaei and Junyi Geng and Guanya Shi and Sebastian Scherer},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611282},
  month     = {5},
  pages     = {1576-1582},
  title     = {Aerial interaction with tactile sensing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual quaternion control of UAVs with cable-suspended load.
<em>ICRA</em>, 1561–1567. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modeling the kinematics and dynamics of robotics systems with suspended loads using dual quaternions has not been explored so far. This paper introduces a new innovative control strategy using dual quaternions for UAVs with cable-suspended loads, focusing on the sling load lifting and tracking problems. By utilizing the mathematical efficiency and compactness of dual quaternions, a unified representation of the UAV and its suspended load’s dynamics and kinematics is achieved, facilitating the realization of load lifting and trajectory tracking. The simulation results have tested the proposed strategy’s accuracy, efficiency, and robustness. This study makes a substantial contribution to present this novel control strategy that harnesses the benefits of dual quaternions for cargo UAVs. Our work also holds promise for inspiring future innovations in under-actuated systems control using dual quaternions.},
  archive   = {C_ICRA},
  author    = {Yuxia Yuan and Markus Ryll},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610170},
  month     = {5},
  pages     = {1561-1567},
  title     = {Dual quaternion control of UAVs with cable-suspended load},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rapid resistography with passive overhead-perching mechanism
in an unmanned aerial system for wood structure inspection.
<em>ICRA</em>, 1554–1560. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an aerial robotic platform for rapid remote elevated overhead-perching drill operations for wood health inspection. The platform features an innovative passive prismatic-gripper mechanism affixed to the aerial robot’s top, facilitating overhead drilling. The primary aim is to enhance the safety and efficiency of elevated wood structure inspection using the resistography method, which involves drilling into wooden structures to identify internal voids. The research centers on two key enabling technologies: a gripper mechanism for secure attachment to target surfaces and a tethered drill configuration for drilling operations. The novel gripper mechanism enables drilling on large planar surfaces and even small beam-width structures. The paper concludes with discussions on design simulations and drill resistance experiments, highlighting the effectiveness of the proposed approach in detecting internal cavities within wooden structures.},
  archive   = {C_ICRA},
  author    = {Shawndy Michael Lee and Jingmin Liu and Jer Luen Chien and Wei Hien Ng and Milven Lim and Shaohui Foong},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611159},
  month     = {5},
  pages     = {1554-1560},
  title     = {Rapid resistography with passive overhead-perching mechanism in an unmanned aerial system for wood structure inspection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WAVE: An open-source underWater arm-vehicle emulator.
<em>ICRA</em>, 1505–1511. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Underwater vehicle manipulator systems (UVMS) are increasingly popular platforms for performing subsea operations that require precision manipulation. While there is high demand for fully autonomous or even semi-autonomous systems, most UVMS still require human support teams. Developing new hardware and algorithms for autonomous underwater manipulation is challenging. Simulations do not capture the full complexity of the underwater environment, and deploying a UVMS at sea for testing/validation is resource-intensive and expensive. In this paper, we present a physical testbed for underwater manipulation that bridges the gap between simulation and full field trials. The underWater Arm-Vehicle Emulator (WAVE) is a 10-degree of freedom system designed to replicate an inspection-class UVMS. WAVE includes an underwater perception sensor and has 2 operating modes: rigid or passive-mode. In passive-mode, the ROV body can pitch similar to how a dynamically-coupled underactuated UVMS without pitch control would rotate during manipulation tasks. To validate the overall design and passive pitch concept, we evaluated the testbed during underwater experiments in energetic conditions at a wave basin. To support continued research and development in underwater robotics, we make the design open-access and freely available to the community.},
  archive   = {C_ICRA},
  author    = {Marcus Rosette and Hannah Kolano and Chris Holm and Geoffrey A. Hollinger and Aaron Marburg and Madison Pickett and Joseph R. Davidson},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611358},
  month     = {5},
  pages     = {1505-1511},
  title     = {WAVE: An open-source underWater arm-vehicle emulator},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tendon-driven continuum robot for deep-sea application.
<em>ICRA</em>, 1498–1504. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The extreme conditions of the deep sea require the use of large and expensive diving robots designed to withstand the high pressure in these depths. In order to reduce the costs for sediment sampling in the deep sea and thus facilitate the explorations of rare deep-sea ecosystems, the goal of this research is to design an alternative manipulator for deep-sea suction sampling. Instead of relying on heavy hydraulic rigid manipulators that deep-sea diving robots are commonly equipped with, we introduce a new concept for a lightweight actuation system that can be used in combination with a traditional diving robot and a suction sampling system. The proposed concept consists of a series of rigid links connected by angled swivel joints. Each segment is actuated by tendons, which allows for continuous bending. The system can be adapted to various sizes of host systems, and the links and joints are printed in place, simplifying the manufacturing process.},
  archive   = {C_ICRA},
  author    = {Cora Maria Sourkounis and Tom Kwasnitschka and Annika Raatz},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611177},
  month     = {5},
  pages     = {1498-1504},
  title     = {Tendon-driven continuum robot for deep-sea application},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Untethered bimodal robotic fish with tunable bistability.
<em>ICRA</em>, 1491–1497. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In nature, fish are excellent swimmers due to their flexible and precise control of tail, which allows them to freely transform between the smooth flapping and the motion of rapid response so that they can move with dexterity. Here, inspired by the versatile motion abilities of fish, a novel robotic fish has been developed, featuring the capability of adaptable bistability. Through tuning the bistability, the robot can acquire two locomotion modes, namely monostable and bistable modes, and it can also swim at different energy barrier that needs to be overcome to realize the bistable motion. The theoretical models are derived to facilitate the control of the robot and the understanding of its nonlinear behavior. The impact of the tunable bistability on the swimming and turning performance is investigated through extensive experiments. The study effectively demonstrates the robotic fish’s capability to swiftly and efficiently navigate through mode switches, enabled by its tunable bistability. This feature is essential for underwater robots to perform tasks in intricate environments.},
  archive   = {C_ICRA},
  author    = {Xu Chao and Imran Hameed and David Navarro-Alarcon and Xingjian Jing},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610967},
  month     = {5},
  pages     = {1491-1497},
  title     = {Untethered bimodal robotic fish with tunable bistability},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards centimeter-scale underwater mobile robots: An
architecture for capable µAUVs. <em>ICRA</em>, 1484–1490. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Underwater robots are indispensable for aquatic exploration, yet their size and complexity often limit broader application. This research presents a pioneering micro autonomous underwater vehicle (µAUV) design. This robot is distinguished by its utilization of mass-produced drone components, novel jet propulsion mechanisms, and multifunctional spherical shell. Its architecture is modular, appendage-free, and largely seal-free. Preliminary tests highlight its motion capabilities and set new benchmarks for centimeter-scale µAUV advancements.},
  archive   = {C_ICRA},
  author    = {Pascal Spino and Daniela Rus},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610474},
  month     = {5},
  pages     = {1484-1490},
  title     = {Towards centimeter-scale underwater mobile robots: An architecture for capable µAUVs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning-driven burrowing with a snake-like robot.
<em>ICRA</em>, 1477–1483. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Subterranean burrowing is inherently difficult for robots because of the high forces experienced as well as the high amount of uncertainty in this domain. Because of the difficulty in modeling forces in granular media, we propose the use of a novel machine-learning control strategy to obtain optimal techniques for vertical self-burrowing. In this paper, we realize a snake-like bio-inspired robot that is equipped with an IMU and two triple-axis magnetometers. Utilizing magnetic field strength as an analog for depth, a novel deep learning architecture was proposed based on sinusoidal and random data in order to obtain a more efficient strategy for vertical self-burrowing. This strategy was able to outperform many other standard burrowing techniques and was able to automatically reach targeted burrowing depths. We hope these results will serve as a proof of concept for how optimization can be used to unlock the secrets of navigating in the subterranean world more efficiently.},
  archive   = {C_ICRA},
  author    = {Sean Even and Holden Gordon and Hoeseok Yang and Yasemin Ozkan-Aydin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610264},
  month     = {5},
  pages     = {1477-1483},
  title     = {Machine learning-driven burrowing with a snake-like robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LeapRun: A dynamic soft robot with running and jumping
capabilities. <em>ICRA</em>, 1470–1476. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the natural world, insects exhibit remarkable locomotion capabilities through a combination of running and jumping. However, replicating this versatile locomotion in a soft robot poses technical and design complexities. Here, we propose a dynamic soft robot named LeapRun that possesses agile locomotion and the ability to perform continuous jumping. To achieve this, a prototype soft robot (weight of 300 mg, size of 30 mm × 15 mm × 5 mm), composed of piezoelectric thin film, shape memory alloy, magnet-locking mechanism, and corresponding support structures, is fabricated. Experimental results demonstrate a maximum moving speed of 15 cm/s and a maximum jumping height of 8.7 cm. Continuous jumping of steps and crossing of complex rugged surfaces are realized. Besides, integrated with the power source, wireless communication module, and control module, the untethered operation is also presented, showcasing the potential for multiple applications in search and rescue, exploration, and monitoring.},
  archive   = {C_ICRA},
  author    = {J. Lu and J. Liang and D. Zhu and D. Wang and Y. Liu and H. Chen and Y. Bai and H. Zhang and M. Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610431},
  month     = {5},
  pages     = {1470-1476},
  title     = {LeapRun: A dynamic soft robot with running and jumping capabilities},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pegasus: A novel bio-inspired quadruped robot with
underactuated wheeled-legged mechanism *. <em>ICRA</em>, 1464–1469. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents the design and analysis of Pegasus, a quadrupedal wheeled robot grounded in biomimicry principles. Pegasus offers two distinct motion modes, including a wheeled motion and a hybrid wheeled-legged motion, enabling adaptability across various tasks and environmental conditions. The robot draws inspiration from the joint structures of quadruped animals and incorporates biomimetic features. At the robot’s ankle joint, we imitate the articulation of a radiusulna joint to enhance the wheeled motion’s agility. Additionally, we establish comprehensive mathematical models for adaptive dynamics model, providing a robust theoretical foundation for subsequent motion planning and high-precision control. A novel telescopic vehicle mode is also proposed for complex wheel-leg hybrid motion, offering optimized solutions for intricate robot locomotion. Furthermore, we employ parallel underactuated MPC controllers for each leg at the control level, contributing to heightened motion precision and stability. Extensive validation through physical platform experiments highlights the effectiveness and feasibility of the proposed controllers, offering substantial support for real-world applications in robotics.},
  archive   = {C_ICRA},
  author    = {Yuzhen Pan and Rezwan Al Islam Khan and Chenyun Zhang and Anzheng Zhang and Huiliang Shang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611633},
  month     = {5},
  pages     = {1464-1469},
  title     = {Pegasus: A novel bio-inspired quadruped robot with underactuated wheeled-legged mechanism *},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spined torso renders advanced mobility for quadrupedal
locomotion. <em>ICRA</em>, 1457–1463. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Animals possessing spinal columns often exhibit exceptional agility for highly dynamic locomotion. The spine grants the trunk with increased degrees of freedom, thereby endowing diverse postures. This paper presents the development of a robot STRAY for quadrupedal locomotion, featuring a four-degree-of-freedom spine design. Using trajectory based reinforcement learning techniques, STRAY is able to trot and bound dynamically using its spine. Simulation results reveal the positive roles of spinal movement, such as twisting, extension, retraction and rotation, in helping STRAY realize efficient locomotion. Preliminary results from experiments demonstrate that STRAY can achieve a trotting gait of approximately 0.6 m/s and a bounding gait of 0.7 m/s, with desired velocities of 0.8 m/s and 1.0 m/s, respectively. The results also indicate that reinforcement learning is a feasible way to investigate how the spine should be used in dynamic quadrupedal locomotion and achieve more possibilities in the future.},
  archive   = {C_ICRA},
  author    = {Jichao Wang and Jinyu Cheng and Jiangtao Hu and Wei Gao and Shiwu Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610113},
  month     = {5},
  pages     = {1457-1463},
  title     = {Spined torso renders advanced mobility for quadrupedal locomotion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AeroDima: Cheetah-inspired aerodynamic tail design for rapid
maneuverability. <em>ICRA</em>, 1451–1456. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Scientists have long theorized that the cheetah’s tail contributes to its impressive maneuvrability at high speeds by stabilizing its body. This has inspired the design of several agile robots, including Dima - a wheeled platform that used cheetah-inspired inertial tail swings to better execute rapid acceleration and turning motions. Subsequent research suggests that the effectiveness of the cheetah’s tail might be enhanced by aerodynamic effects. In this paper, we introduce AeroDima: a follow-up to the original Dima design that uses aerodynamic drag on the tail as the primary mechanism for generating the stabilizing torque. The resulting sail-like tail is substantially lighter than the original, but still improves the performance of the platform, allowing it to enter turns at a higher speed without toppling. While the yaw rate of the robot was actually higher without the tail, the tail substantially reduced unwanted roll, confirming that this appendage increases maneuvrability by increasing stability, rather than by directly contributing to lateral acceleration.},
  archive   = {C_ICRA},
  author    = {Daryn Bright and Stacey Shield and Amir Patel},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610202},
  month     = {5},
  pages     = {1451-1456},
  title     = {AeroDima: Cheetah-inspired aerodynamic tail design for rapid maneuverability},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quadruped-frog: Rapid online optimization of continuous
quadruped jumping. <em>ICRA</em>, 1443–1450. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Legged robots are becoming increasingly agile in exhibiting dynamic behaviors such as running and jumping. Usually, such behaviors are either optimized and engineered offline (i.e. the behavior is designed for before it is needed), either through model-based trajectory optimization, or through deep learning-based methods involving millions of timesteps of simulation interactions. Notably, such offline-designed locomotion controllers cannot perfectly model the true dynamics of the system, such as the motor dynamics. In contrast, in this paper, we consider a quadruped jumping task that we rapidly optimize online. We design foot force profiles parameterized by only a few parameters which we optimize for directly on hardware with Bayesian Optimization. The force profiles are tracked at the joint level, and added to Cartesian PD impedance control and Virtual Model Control to stabilize the jumping motions. After optimization, which takes only a handful of jumps, we show that this control architecture is capable of diverse and omnidirectional jumps including forward, lateral, and twist (turning) jumps, even on uneven terrain, enabling the Unitree Go1 quadruped to jump 0.5 m high, 0.5 m forward, and jump-turn over 2 rad.},
  archive   = {C_ICRA},
  author    = {Guillaume Bellegarda and Milad Shafiee and Merih Ekin Özberk and Auke Ijspeert},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610141},
  month     = {5},
  pages     = {1443-1450},
  title     = {Quadruped-frog: Rapid online optimization of continuous quadruped jumping},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-righting shell for robotic hexapod. <em>ICRA</em>,
1436–1442. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Decimeter scale robots in human environments are small relative to obstacles they encounter, making them prone to flipping over and needing to self-right. We present a multifaceted shell that by its geometry alone enables the hexapedal robot MediumANT to passively self-right without the need for additional sensory feedback. We designed the shell by specifying the cross-sectional geometry in the yz and xy planes such that the robot returns to an upright position by rolling around the longitudinal (x) axis, and then tweaked the design to reduce the number of faces. We then attached the shell to the robot by modifying some of its chassis structural plates to extend to and support the shell. We evaluated the effectiveness of the shell in two experimental scenarios: passive righting – balancing the robot on each face of the shell before releasing the robot – and an intentional fall – walking the robot off a ledge at various approach angles. As intended by our design, the robot recovered the upright orientation from all starting faces in the passive righting test and righted itself and continued walking in all falling trials. This work presents an example of using biologically inspired simplicity to solve what would otherwise be a technically challenging problem.},
  archive   = {C_ICRA},
  author    = {Katelyn King and Shai Revzen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610914},
  month     = {5},
  pages     = {1436-1442},
  title     = {Self-righting shell for robotic hexapod},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual CPG-RL: Learning central pattern generators for
visually-guided quadruped locomotion. <em>ICRA</em>, 1420–1427. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a framework for learning visually-guided quadruped locomotion by integrating exteroceptive sensing and central pattern generators (CPGs), i.e. systems of coupled oscillators, into the deep reinforcement learning (DRL) framework. Through both exteroceptive and proprioceptive sensing, the agent learns to coordinate rhythmic behavior among different oscillators to track velocity commands, while at the same time override these commands to avoid collisions with the environment. We investigate several open robotics and neuroscience questions: 1) What is the role of explicit interoscillator couplings between oscillators, and can such coupling improve sim-to-real transfer for navigation robustness? 2) What are the effects of using a memory-enabled vs. a memory-free policy network with respect to robustness, energy-efficiency, and tracking performance in sim-to-real navigation tasks? 3) How do animals manage to tolerate high sensorimotor delays, yet still produce smooth and robust gaits? To answer these questions, we train our perceptive locomotion policies in simulation and perform sim-to-real transfers to the Unitree Go1 quadruped, where we observe robust navigation in a variety of scenarios. Our results show that the CPG, explicit interoscillator couplings, and memory-enabled policy representations are all beneficial for energy efficiency, robustness to noise and sensory delays of 90 ms, and tracking performance for successful sim-to-real transfer for navigation tasks.},
  archive   = {C_ICRA},
  author    = {Guillaume Bellegarda and Milad Shafiee and Auke Ijspeert},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611128},
  month     = {5},
  pages     = {1420-1427},
  title     = {Visual CPG-RL: Learning central pattern generators for visually-guided quadruped locomotion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active-perceptive motion generation for mobile manipulation.
<em>ICRA</em>, 1413–1419. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile Manipulation (MoMa) systems incorporate the benefits of mobility and dexterity, due to the enlarged space in which they can move and interact with their environment. However, even when equipped with onboard sensors, e.g., an embodied camera, extracting task-relevant visual information in unstructured and cluttered environments, such as households, remains challenging. In this work, we introduce an active perception pipeline for mobile manipulators to generate motions that are informative toward manipulation tasks, such as grasping in unknown, cluttered scenes. Our proposed approach, ActPerMoMa, generates robot paths in a receding horizon fashion by sampling paths and computing path-wise utilities. These utilities trade-off maximizing the visual Information Gain (IG) for scene reconstruction and the task-oriented objective, e.g., grasp success, by maximizing grasp reachability. We show the efficacy of our method in simulated experiments with a dual-arm TIAGo++ MoMa robot performing mobile grasping in cluttered scenes with obstacles. We empirically analyze the contribution of various utilities and parameters, and compare against representative baselines both with and without active perception objectives. Finally, we demonstrate the transfer of our mobile grasping strategy to the real world, indicating a promising direction for active-perceptive MoMa.},
  archive   = {C_ICRA},
  author    = {Snehal Jauhri and Sophie Lueth and Georgia Chalvatzaki},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610714},
  month     = {5},
  pages     = {1413-1419},
  title     = {Active-perceptive motion generation for mobile manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Dynamic interaction control in legged mobile manipulators:
A decoupled approach. <em>ICRA</em>, 1406–1412. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Legged mobile manipulators are receiving much more attention. Mobile platforms can infinitely expand the workspace of robotic arms, providing more possibilities for robot application scenarios. Compared with wheeled mobile manipulators, legged mobile manipulators have higher requirements for cooperative control of legged robots and robotic arms. This work decouples the control of the robotic arm and the legged robot. On the legged robot side, we explicitly estimate the wrench exerted by the robotic arm on the base and bring it into the legged robot’s dynamics, and then use a nonlinear model predictive controller (NMPC) to control the legged robot. On the robotics arm side, we adopt an impedance controller to realize the end-effector’s force control, and the introduction of impedance control has improved the safety and interactivity of legged mobile manipulators. We conducted experiments on physical robot to compare the differences between decoupled control and independent control, and the results show that the stability and robustness of robot systems have improved using decoupled control.},
  archive   = {C_ICRA},
  author    = {Qikai Li and Qinchen Meng and Yuxing Qin and Jiawei Chen and Xilun Ding and Kun Xu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611344},
  month     = {5},
  pages     = {1406-1412},
  title     = {Dynamic interaction control in legged mobile manipulators: A decoupled approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GAMMA: Graspability-aware mobile MAnipulation policy
learning based on online grasping pose fusion. <em>ICRA</em>, 1399–1405.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile manipulation constitutes a fundamental task for robotic assistants and garners significant attention within the robotics community. A critical challenge inherent in mobile manipulation is the effective observation of the target while approaching it for grasping. In this work, we propose a graspability-aware mobile manipulation approach powered by an online grasping pose fusion framework that enables a temporally consistent grasping observation. Specifically, the predicted grasping poses are online organized to eliminate the redundant, outlier grasping poses, which can be encoded as a grasping pose observation state for reinforcement learning. Moreover, on-the-fly fusing the grasping poses enables a direct assessment of graspability, encompassing both the quantity and quality of grasping poses. This assessment can subsequently serve as an observe-to-grasp reward, motivating the agent to prioritize actions that yield detailed observations while approaching the target object for grasping. Through extensive experiments conducted on the Habitat and Isaac Gym simulators, we find that our method attains a good balance between observation and manipulation, yielding high performance under various grasping metrics. Furthermore, we discover that the incorporation of temporal information from grasping poses aids in mitigating the sim-to-real gap, leading to robust performance in challenging real-world experiments. Project page: https://pku-epic.github.io/GAMMA/},
  archive   = {C_ICRA},
  author    = {Jiazhao Zhang and Nandiraju Gireesh and Jilong Wang and Xiaomeng Fang and Chaoyi Xu and Weiguang Chen and Liu Dai and He Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610125},
  month     = {5},
  pages     = {1399-1405},
  title     = {GAMMA: Graspability-aware mobile MAnipulation policy learning based on online grasping pose fusion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gaussian mixture likelihood-based adaptive MPC for
interactive mobile manipulators. <em>ICRA</em>, 1392–1398. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile robots are nowadays frequently used for interaction tasks in the real world, e.g. for opening doors or for pick-and-place tasks. When used in real-world environments, adapting the robot controllers to uncertain contact dynamics is a significant challenge. Adaptive Model Predictive Control (AMPC) is an approach for controlling robot motions while adapting to uncertain or changing dynamics. However, most of the existing AMPC approaches used in mobile manipulation require either expert tuning or extensive training, making it very difficult to introduce novel or diverse tasks. In addition, the adjustment of several, independent environment parameters is usually not considered in the AMPC formulation. In this work, we introduce a hierarchical approach that uses Gaussian Mixture Models (GMMs) and Gaussian Mixture Regression (GMR) to predict the dynamic model parameters of MPC based on proprioceptive measurements and perform tasks with multiple unknown environmental parameters. The approach is evaluated in simulation and in real experiments on a mobile manipulator and compared to several baseline methods. It is shown that it outperforms standard MPC and an existing AMPC approach on several tasks such as carrying, pushing, and door opening.},
  archive   = {C_ICRA},
  author    = {Dimitrios Rakovitis and Dennis Mronga},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610115},
  month     = {5},
  pages     = {1392-1398},
  title     = {Gaussian mixture likelihood-based adaptive MPC for interactive mobile manipulators},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time whole-body motion planning for mobile manipulators
using environment-adaptive search and spatial-temporal optimization.
<em>ICRA</em>, 1369–1375. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile manipulators have recently gained significant attention in the robotics community due to their superior potential in industrial and service applications. However, the high degree of freedom associated with mobile manipulators poses challenges in achieving real-time whole-body motion planning. To bridge the gap, this paper presents a motion planning method capable of generating high-quality, safe, agile and feasible trajectories for mobile manipulators in real time. First, we present a novel environment-adaptive path searching method, which can generate paths in real-time in various environments by adaptively adjusting searching dimension based on environment complexity. Additionally, we propose a real-time spatial-temporal trajectory optimization method that takes into account the whole-body safety, agility and dynamic feasibility of mobile manipulators. Moreover, task constraints are applied to ensure that the trajectory can fulfill specific task requirements. Simulation and real-world experiments demonstrate that our method is capable of generating whole-body trajectories in real-time in challenging environments. We will release our code to benefit the community.},
  archive   = {C_ICRA},
  author    = {Chengkai Wu and Ruilin Wang and Mianzhi Song and Fei Gao and Jie Mei and Boyu Zhou},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610192},
  month     = {5},
  pages     = {1369-1375},
  title     = {Real-time whole-body motion planning for mobile manipulators using environment-adaptive search and spatial-temporal optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robot task planning under local observability.
<em>ICRA</em>, 1362–1368. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-world robot task planning is intractable in part due to partial observability. A common approach to reducing complexity is introducing additional structure into the decision process, such as mixed-observability, factored states, or temporally-extended actions. We propose the locally observable Markov decision process, a novel formulation that models task-level planning where uncertainty pertains to object-level attributes and where a robot has subroutines for seeking and accurately observing objects. This models sensors that are range-limited and line-of-sight—objects occluded or outside sensor range are unobserved, but the attributes of objects that fall within sensor view can be resolved via repeated observation. Our model results in a three-stage planning process: first, the robot plans using only observed objects; if that fails, it generates a target object that, if observed, could result in a feasible plan; finally, it attempts to locate and observe the target, replanning after each newly observed object. By combining LOMDPs with off-the-shelf Markov planners, we outperform state-of-the-art-solvers for both object-oriented POMDP and MDP analogues with the same task specification. We then apply the formulation to successfully solve a task on a mobile robot.},
  archive   = {C_ICRA},
  author    = {Max Merlin and Shane Parr and Neev Parikh and Sergio Orozco and Vedant Gupta and Eric Rosen and George Konidaris},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610876},
  month     = {5},
  pages     = {1362-1368},
  title     = {Robot task planning under local observability},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Shadow-based 3D pose estimation of intraocular instrument
using only 2D images. <em>ICRA</em>, 1323–1329. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In ophthalmic surgeries, such as vitreoretinal operations, surgeons rely on imaging systems, primarily microscopes, for real-time instrument monitoring and motion planning. However, novice surgeons struggle to extract 3D instrument positions from 2D microscope frames, necessitating extensive trial-and-error experience with the background that additional imaging modalities such as iOCT remain inaccessible in most operating rooms. Targeting intraocular assessment within the current surgical setup, this paper presents an imagebased pose estimation method to obtain real-time instrument tip positions in a standard 12mm-radius spherical eyeball model, which links floating instruments with on-the-retinal objects based on the intraocular shadowing principle. We validate this estimation method in a Unity simulator and verify its depth estimation capability using a specially designed eyeball phantom. Both simulator and phantom experiments demonstrate an average needle-tip estimation error within [1.0, 2.0] mm using only 2D microscope frames.},
  archive   = {C_ICRA},
  author    = {Junjie Yang and Zhihao Zhao and Mathias Maier and Kai Huang and Nassir Navab and M. Ali Nasseri},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611011},
  month     = {5},
  pages     = {1323-1329},
  title     = {Shadow-based 3D pose estimation of intraocular instrument using only 2D images},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Implicit neural representations for breathing-compensated
volume reconstruction in robotic ultrasound. <em>ICRA</em>, 1316–1322.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ultrasound (US) imaging is widely used in diagnosing and staging abdominal diseases due to its lack of non-ionizing radiation and prevalent availability. However, significant inter-operator variability and inconsistent image acquisition hinder the widespread adoption of extensive screening programs. Robotic ultrasound systems have emerged as a promising solution, offering standardized acquisition protocols and the possibility of automated acquisition. Additionally, these systems enable access to 3D data via robotic tracking, enhancing volumetric reconstruction for improved ultrasound interpretation and precise disease diagnosis.However, the interpretability of 3D US reconstruction of abdominal images can be affected by the patient’s breathing motion. This study introduces a method to compensate for breathing motion in 3D US compounding by leveraging implicit neural representations. Our approach employs a robotic ultrasound system for automated screenings. To demonstrate the method’s effectiveness, we evaluate our proposed method for the diagnosis and monitoring of abdominal aorta aneurysms as a representative use case.Our experiments demonstrate that our proposed pipeline facilitates robust automated robotic acquisition, mitigating artifacts from breathing motion, and yields smoother 3D reconstructions for enhanced screening and medical diagnosis.},
  archive   = {C_ICRA},
  author    = {Yordanka Velikova and Mohammad Farid Azampour and Walter Simson and Marco Esposito and Nassir Navab},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611443},
  month     = {5},
  pages     = {1316-1322},
  title     = {Implicit neural representations for breathing-compensated volume reconstruction in robotic ultrasound},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A passive variable impedance control strategy with
viscoelastic parameters estimation of soft tissues for safe
ultrasonography. <em>ICRA</em>, 1298–1304. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the context of telehealth, robotic approaches have proven a valuable solution to in-person visits in remote areas, with decreased costs for patients and infection risks. In particular, in ultrasonography, robots have the potential to reproduce the skills required to acquire high-quality images while reducing the sonographer’s physical efforts. In this paper, we address the control of the interaction of the probe with the patient’s body, a critical aspect of ensuring safe and effective ultrasonography. We introduce a novel approach based on variable impedance control, allowing the real-time optimisation of compliant controller parameters during ultrasound procedures. This optimisation is formulated as a quadratic programming problem and incorporates physical constraints derived from viscoelastic parameter estimations. Safety and passivity constraints, including an energy tank, are also integrated to minimise potential risks during human-robot interaction. The proposed method’s efficacy is demonstrated through experiments on a patient’s dummy torso, highlighting its potential for achieving safe behaviour and accurate force control during ultrasound procedures, even in cases of contact loss.},
  archive   = {C_ICRA},
  author    = {Luca Beber and Edoardo Lamon and Davide Nardi and Daniele Fontanelli and Matteo Saveriano and Luigi Palopoli},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610167},
  month     = {5},
  pages     = {1298-1304},
  title     = {A passive variable impedance control strategy with viscoelastic parameters estimation of soft tissues for safe ultrasonography},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A track-based colon endoscopic robot with depth perception
stereo cameras for haustral fold detection during colonic navigation.
<em>ICRA</em>, 1276–1282. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Colon endoscopic robots represent a promising screening modality for the visualization of colon cancers with high sensitivity. However, current colonoscopy robots are often characterized by intricate and bulky mechanical structures, which pose practical challenges when moving through the complex and narrow environment of the colon. Moreover, these robots are typically equipped with a single camera, limiting their ability to accurately estimate the depth of haustral folds in the colon, which is of great importance for the active colonic navigation of the robots. To address these challenges, we develop a track-based stereoscopic endoscopic robot (TSER) which is equipped with four tracks positioned at the corners of its body. This innovative design maximizes the contact between the tracks and the colon wall, enhancing maneuverability. The tracks are constructed from de-molded polydimethylsiloxane (PDMS) and incorporate micro-patterns on their outer surfaces. We have proposed a straightforward strategy for detecting haustral folds using TSER’s stereo camera, which allows for precise identification of their position and depth. The TSER achieves an average motion speed of 9.8 mm/s in a bellows tube that contains silicone oil and a speed of 5.2 mm/s in an exvivo porcine intestinal segment. Impressively, the TSER boasts an 88.11% accuracy rate in haustral fold depth estimation, surpassing the performance of existing geometric shape fitting methods. These results demonstrate that the TSER holds great potential for effective and efficient movement and inspection within the colon, offering a promising solution for improved colon cancer screening.},
  archive   = {C_ICRA},
  author    = {Shujing He and Yujie Zhang and Baoyi Huang and Jie Lin and Chaoyang Shi and Chengzhi Hu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611617},
  month     = {5},
  pages     = {1276-1282},
  title     = {A track-based colon endoscopic robot with depth perception stereo cameras for haustral fold detection during colonic navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uncertainty-aware contextual visualization for human
supervision of OCT-guided autonomous robotic subretinal injection.
<em>ICRA</em>, 1268–1275. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The injection of therapeutic agents into the sub-retinal space might allow improved treatment of age-related macular degeneration. Various robotic systems have been developed to achieve the required precision and, in combination with intraoperative Optical Coherence Tomography (iOCT) imaging, methods for autonomous robotic guidance have been proposed. In such systems, the robot’s cognition is often governed by machine learning algorithms, such as convolutional neural networks (CNNs), which provide semantic scene information from iOCT images. Although the robot performs a surgical task autonomously, human supervision is critical to monitor the robot’s execution and, if necessary, stop the robot or take control to avoid trauma to the patient. In this paper, we propose a novel visualization concept for improved human supervision of autonomous robotic subretinal injection that integrates uncertainty information of the data provided to the robot. We design a focus and context visualization that renders an automatically identified instrument-aligned B-scan in the context of the 3D OCT volume. Our visualization is enriched by augmenting the uncertainty information on the instrument-aligned B-scan. To dynamically model task-specific uncertainty, we introduce a weighting scheme to assign an importance factor to each pair of classes, controlling the impact of their confusion on the overall uncertainty. We demonstrate our visualization concept on iOCT volumes acquired at different stages during subretinal injection on ex-vivo porcine eyes. We show that our processing pipeline achieves sufficient update rates for surgical display and discuss the impact of our visualization concept on the acceptance of robotic task autonomy for subretinal injection procedures.},
  archive   = {C_ICRA},
  author    = {Michael Sommersperger and Shervin Dehghani and Philipp Matten and Hessam Roodaki and Nassir Navab},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610319},
  month     = {5},
  pages     = {1268-1275},
  title     = {Uncertainty-aware contextual visualization for human supervision of OCT-guided autonomous robotic subretinal injection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A study of force-free control framework for industrial
manipulator tasks based on high-pass filter. <em>ICRA</em>, 1261–1267.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Force-free control (FFC) allows for flexible manipulator motion in response to external forces, making it a vital component of human-robot interaction (HRI). Manual intervention may cause uneven forces on the manipulator or frequencies close to the natural frequency, and mechanical resonance can occur due to the inertia of the manipulator and adjustable equivalent stiffness of the controller. This paper proposes an FFC approach for industrial manipulators using a six-axis force/torque sensor (F/T sensor), implemented through a three-layer control architecture, consisting of motion control layer, Admittance Control layer and force decoupling layer. To mitigate the effects of mechanical resonance, a high-pass filter (HPF) is integrated with the F/T sensor and its impact is investigated. Experimental validation is conducted using both a simulation model and an industrial manipulator. Test results indicate that the proposed FFC architecture enables the manipulator not only to interact smoothly with external forces, but also to distinguish load forces at different frequencies and potentially address the issue of mechanical resonance between the manipulator and the applied load forces.},
  archive   = {C_ICRA},
  author    = {Guanwei He and Guodong Feng and Beichen Ding},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610526},
  month     = {5},
  pages     = {1261-1267},
  title     = {A study of force-free control framework for industrial manipulator tasks based on high-pass filter},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Iterative learning control for deformable open-frame
cable-driven parallel robots. <em>ICRA</em>, 1254–1260. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposed an iterative learning control (ILC) scheme for deformable open-frame cable-driven parallel robots (D-CDPRs). In contrast to the straightforward inverse kinematics of the rigid frame cable-driven parallel robots (CDPRs), accurate modeling of the deformable frame poses challenges due to errors and uncertainties. To address these issues, the authors propose the use of ILC, a control strategy that modifies the control input over iterations based on previous results. ILC has been successfully applied to traditional cable robots, particularly in handling model uncertainty. The paper presents a novel ILC control scheme specifically designed for D-CDPRs, with a focus on reducing tracking errors over repetitive operations. Additionally, hardware experiments are conducted to validate the effectiveness and reliability of the proposed ILC approach. The results demonstrate the efficacy of ILC in mitigating tracking errors, even in scenarios where the dynamic model of the D-CDPRs is unknown.},
  archive   = {C_ICRA},
  author    = {Wuichung Cheng and Arthur Ngo Foon Chan and Darwin Lau},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610529},
  month     = {5},
  pages     = {1254-1260},
  title     = {Iterative learning control for deformable open-frame cable-driven parallel robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prediction of pose errors implied by external forces applied
on robots: Towards a metric for the control of collaborative robots.
<em>ICRA</em>, 1247–1253. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The presented work tackles the question of quantifying the pose deviations of robots subject to external disturbance forces. While this question may not be central for large robots perfectly rejecting disturbances through high controller gains, it is an important factor when considering collaborative settings where smaller robots may be deviated from their task because of unmodeled physical interactions. This is all the more true with human-robot collaboration where human capacities may fluctuate over time and have to be compensated by a proper adaptation of the robot control. To move forward in this direction, this work first derives a deviation prediction methodology and exemplifies it using three largely employed control approaches. The proposed prediction method is then validated using simulated and real robot experiments both in single and multiple robots cases. The obtained results constitute a stepping stone towards a quantitative metric for robots adapting their behaviour to human motor fluctuation.},
  archive   = {C_ICRA},
  author    = {Vincent Fortineau and Vincent Padois and David Daney},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610746},
  month     = {5},
  pages     = {1247-1253},
  title     = {Prediction of pose errors implied by external forces applied on robots: Towards a metric for the control of collaborative robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online-learning-based distributionally robust motion control
with collision avoidance for mobile robots. <em>ICRA</em>, 1241–1246.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collision-free navigation is a critical issue in robotic systems as the environment is often dynamic and uncertain. This paper investigates a data-stream-driven motion control problem for mobile robots to avoid randomly moving obstacles when the probability distribution of the obstacle’s movement is partially observable through data and can be even time-varying. A data-stream-driven ambiguity set is firstly constructed from movement data by leveraging a Dirichlet process mixture model and is updated online using real-time data. Then we propose an Online-Learning-based Distributionally Robust Nonlinear Model Predictive Control (OL-DR-NMPC) approach for limiting the risk of collision through considering the worst-case distribution within the ambiguity set. To facilitate solving the OL-DR-NMPC problem, we reformulate it as a finite-dimensional nonlinear optimization problem. To cope with the bilinear matrix inequality constraints in the nonlinear problem, we develop a parabolic relaxation and a sequential algorithm, by which the problem is further transformed into polynomial-time solvable surrogates. The simulations using a quadrotor model are employed to demonstrate the effectiveness and advantages of the proposed method.},
  archive   = {C_ICRA},
  author    = {Han Wang and Chao Ning and Longyan Li and Weidong Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610765},
  month     = {5},
  pages     = {1241-1246},
  title     = {Online-learning-based distributionally robust motion control with collision avoidance for mobile robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Geometric slosh-free tracking for robotic manipulators.
<em>ICRA</em>, 1226–1232. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work focuses on the agile transportation of liquids with robotic manipulators. In contrast to existing methods that are either computationally heavy, system/container specific or dependant on a singularity-prone pendulum model, we present a real-time slosh-free tracking technique. This method solely requires the reference trajectory and the robot’s kinematic constraints to output kinematically feasible joint space commands. The crucial element underlying this approach consists on mimicking the end-effector’s motion through a virtual quadrotor, which is inherently slosh-free and differentially flat, thereby allowing us to calculate a slosh-free reference orientation. Through the utilization of a cascaded proportional-derivative (PD) controller, this slosh-free reference is transformed into task space acceleration commands, which, following the resolution of a Quadratic Program (QP) based on Resolved Acceleration Control (RAC), are translated into a feasible joint configuration. The validity of the proposed approach is demonstrated by simulated and real-world experiments on a 7 DoF Franka Emika Panda robot.},
  archive   = {C_ICRA},
  author    = {Jon Arrizabalaga and Lukas Pries and Riddhiman Laha and Runkang Li and Sami Haddadin and Markus Ryll},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610813},
  month     = {5},
  pages     = {1226-1232},
  title     = {Geometric slosh-free tracking for robotic manipulators},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LIKO: LiDAR, inertial, and kinematic odometry for bipedal
robots. <em>ICRA</em>, 1180–1185. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {High-frequency and accurate state estimation is crucial for biped robots. This paper presents a tightly-coupled LiDAR-Inertial-Kinematic Odometry (LIKO) for biped robot state estimation based on an iterated extended Kalman filter. Beyond state estimation, the foot contact position is also modeled and estimated. This allows for both position and velocity updates from kinematic measurement. Additionally, the use of kinematic measurement results in an increased output state frequency of about 1kHz. This ensures temporal continuity of the estimated state and makes it practical for control purposes of biped robots. We also announce a biped robot dataset consisting of LiDAR, inertial measurement unit (IMU), joint encoders, force/torque (F/T) sensors, and motion capture ground truth to evaluate the proposed method. The dataset is collected during robot locomotion, and our approach reached the best quantitative result among other LIO-based methods and biped robot state estimation algorithms. The dataset and source code will be available at https://github.com/Mr-Zqr/LIKO.},
  archive   = {C_ICRA},
  author    = {Qingrui Zhao and Mingyuan Li and Yongliang Shi and Xuechao Chen and Zhangguo Yu and Lianqiang Han and Zhenyuan Fu and Jintao Zhang and Chao Li and Yuanxi Zhang and Qiang Huang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610222},
  month     = {5},
  pages     = {1180-1185},
  title     = {LIKO: LiDAR, inertial, and kinematic odometry for bipedal robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data-driven latent space representation for robust bipedal
locomotion learning. <em>ICRA</em>, 1172–1178. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel framework for learning robust bipedal walking by combining a data-driven state representation with a Reinforcement Learning (RL) based locomotion policy. The framework utilizes an autoencoder to learn a low-dimensional latent space that captures the complex dynamics of bipedal locomotion from existing locomotion data. This reduced dimensional state representation is then used as states for training a robust RL-based gait policy, eliminating the need for heuristic state selections or the use of template models for gait planning. The results demonstrate that the learned latent variables are disentangled and directly correspond to different gaits or speeds, such as moving forward, backward, or walking in place. Compared to traditional template model-based approaches, our framework exhibits superior performance and robustness in simulation. The trained policy effectively tracks a wide range of walking speeds and demonstrates good generalization capabilities to unseen scenarios.},
  archive   = {C_ICRA},
  author    = {Guillermo A. Castillo and Bowen Weng and Wei Zhang and Ayonga Hereid},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610978},
  month     = {5},
  pages     = {1172-1178},
  title     = {Data-driven latent space representation for robust bipedal locomotion learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient, dynamic locomotion through step placement with
straight legs and rolling contacts. <em>ICRA</em>, 1143–1150. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For humans, fast, efficient walking over flat ground represents the vast majority of locomotion that an individual experiences on a daily basis, and for an effective, real-world humanoid robot the same will likely be the case. In this work, we propose a locomotion controller for efficient walking over near-flat ground using a relatively simple, model-based controller that utilizes a novel combination of several interesting design features including an ALIP-based step adjustment strategy, stance leg length control as an alternative to center of mass height control, and rolling contact for heel-to-toe motion of the stance foot. We then present the results of this controller on our robot Nadia, both in simulation and on hardware. These results include validation of this controller’s ability to perform fast, reliable forward walking at 0.75 m/s along with backwards walking, side-stepping, turning in place, and push recovery. We also present an efficiency comparison between the proposed control strategy and our baseline walking controller over three steady-state walking speeds. Lastly, we demonstrate some of the benefits of utilizing rolling contact in the stance foot, specifically the reduction of necessary positive and negative work throughout the stride.},
  archive   = {C_ICRA},
  author    = {Stefan Fasano and James Foster and Sylvain Bertrand and Christian DeBuys and Robert Griffin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611056},
  month     = {5},
  pages     = {1143-1150},
  title     = {Efficient, dynamic locomotion through step placement with straight legs and rolling contacts},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Synthesizing robust walking gaits via discrete-time barrier
functions with application to multi-contact exoskeleton locomotion.
<em>ICRA</em>, 1136–1142. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Successfully achieving bipedal locomotion remains challenging due to real-world factors such as model uncertainty, random disturbances, and imperfect state estimation. In this work, we propose a novel metric for locomotive robustness – the estimated size of the hybrid forward invariant set associated with the step-to-step dynamics. Here, the forward invariant set can be loosely interpreted as the region of attraction for the discrete-time dynamics. We illustrate the use of this metric towards synthesizing nominal walking gaits using a simulation-in-the-loop learning approach. Further, we leverage discrete-time barrier functions and a sampling-based approach to approximate sets that are maximally forward invariant. Lastly, we experimentally demonstrate that this approach results in successful locomotion for both flat-foot walking and multi-contact walking on the Atalante lower-body exoskeleton.},
  archive   = {C_ICRA},
  author    = {Maegan Tucker and Kejun Li and Aaron D. Ames},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610537},
  month     = {5},
  pages     = {1136-1142},
  title     = {Synthesizing robust walking gaits via discrete-time barrier functions with application to multi-contact exoskeleton locomotion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Walking-by-logic: Signal temporal logic-guided model
predictive control for bipedal locomotion resilient to external
perturbations. <em>ICRA</em>, 1121–1127. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study proposes a novel planning framework based on a model predictive control formulation that incorporates signal temporal logic (STL) specifications for task completion guarantees and robustness quantification. This marks the first-ever study to apply STL-guided trajectory optimization for bipedal locomotion push recovery, where the robot experiences unexpected disturbances. Existing recovery strategies often struggle with complex task logic reasoning and locomotion robustness evaluation, making them susceptible to failures due to inappropriate recovery strategies or insufficient robustness. To address this issue, the STL-guided framework generates optimal and safe recovery trajectories that simultaneously satisfy the task specification and maximize the locomotion robustness. Our framework outperforms a state-of-the-art locomotion controller in a high-fidelity dynamic simulation, especially in scenarios involving crossed-leg maneuvers. Furthermore, it demonstrates versatility in tasks such as locomotion on stepping stones, where the robot must select from a set of disjointed footholds to maneuver successfully.},
  archive   = {C_ICRA},
  author    = {Zhaoyuan Gu and Rongming Guo and William Yates and Yipu Chen and Yuntian Zhao and Ye Zhao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610811},
  month     = {5},
  pages     = {1121-1127},
  title     = {Walking-by-logic: Signal temporal logic-guided model predictive control for bipedal locomotion resilient to external perturbations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RainbowSight: A family of generalizable, curved,
camera-based tactile sensors for shape reconstruction. <em>ICRA</em>,
1114–1120. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Camera-based tactile sensors can provide high resolution positional and local geometry information for robotic manipulation. Curved and rounded fingers are often advantageous, but it can be difficult to derive illumination systems that work well within curved geometries. To address this issue, we introduce RainbowSight, a family of curved, compact, camera-based tactile sensors which use addressable RGB LEDs illuminated in a novel rainbow spectrum pattern. In addition to being able to scale the illumination scheme to different sensor sizes and shapes to fit on a variety of end effector configurations, the sensors can be easily manufactured and require minimal optical tuning to obtain high resolution depth reconstructions of an object deforming the sensor’s soft elastomer surface. Additionally, we show the advantages of our new hardware design and improvements in calibration methods for accurate depth map generation when compared to alternative lighting methods commonly implemented in previous camera-based tactile sensors. With these advancements, we make the integration of tactile sensors more accessible to roboticists by allowing them the flexibility to easily customize, fabricate, and calibrate camera-based tactile sensors to best fit the needs of their robotic systems.},
  archive   = {C_ICRA},
  author    = {Megha H. Tippur and Edward H. Adelson},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609863},
  month     = {5},
  pages     = {1114-1120},
  title     = {RainbowSight: A family of generalizable, curved, camera-based tactile sensors for shape reconstruction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GelLink: A compact multi-phalanx finger with vision-based
tactile sensing and proprioception. <em>ICRA</em>, 1107–1113. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Compared to fully-actuated robotic end-effectors, underactuated ones are generally more adaptive, robust, and cost-effective. However, state estimation for underactuated hands is usually more challenging. Vision-based tactile sensors, like Gelsight, can mitigate this issue by providing high-resolution tactile sensing and accurate proprioceptive sensing. As such, we present GelLink, a compact, underactuated, linkage-driven robotic finger with low-cost, high-resolution vision-based tactile sensing and proprioceptive sensing capabilities. In order to reduce the amount of embedded hardware, i.e. the cameras and motors, we optimize the linkage transmission with a planar linkage mechanism simulator and develop a planar reflection simulator to simplify the tactile sensing hardware. As a result, GelLink only requires one motor to actuate the three phalanges, and one camera to capture tactile signals along the entire finger. Overall, GelLink is a compact robotic finger that shows adaptability and robustness when performing grasping tasks. The integration of vision- based tactile sensors can significantly enhance the capabilities of underactuated fingers and potentially broaden their future usage.},
  archive   = {C_ICRA},
  author    = {Yuxiang Ma and Jialiang Alan Zhao and Edward Adelson},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610823},
  month     = {5},
  pages     = {1107-1113},
  title     = {GelLink: A compact multi-phalanx finger with vision-based tactile sensing and proprioception},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning contact for haptic feedback: Switching x-lateral
teleoperators. <em>ICRA</em>, 1092–1098. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose X-lateral teleoperation: a novel hybrid unilateral-bilateral teleoperation framework. Bilateral teleoperation enables kinesthetic coupling between the operator and the remote environment with haptic feedback. However, in free motion, unlike unilateral teleoperators, bilateral teleoperators reflect undesirable operational forces to the operator. The proposed X-lateral teleoperation framework benefits from a learning-based contact detection algorithm which triggers switching from unilateral teleoperation in free motion to bilateral teleoperation in contact. We also present a neural network based two-class classification technique to detect contacts even with environments not seen in training. In experiments with linear motors and Phantom Omni devices, using sensorless force estimation, we show that the proposed method can decrease operational forces significantly over transparencyoptimized bilateral architectures.},
  archive   = {C_ICRA},
  author    = {Nural Yilmaz and Ugur Tumerdem},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611571},
  month     = {5},
  pages     = {1092-1098},
  title     = {Learning contact for haptic feedback: Switching X-lateral teleoperators},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An electromagnetism-inspired method for estimating in-grasp
torque from visuotactile sensors. <em>ICRA</em>, 1077–1083. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tactile sensing has become a popular sensing modality for robot manipulators, due to the promise of providing robots with the ability to measure the rich contact information that gets transmitted through its sense of touch. Among the diverse range of information accessible from tactile sensors, torques transmitted from the grasped object to the fingers through extrinsic environmental contact may be particularly important for tasks such as object insertion. However, tactile torque estimation has received relatively little attention when compared to other sensing modalities, such as force, texture, or slip identification. In this work, we introduce the notion of the Tactile Dipole Moment, which we use to estimate tilt torques from gel-based visuotactile sensors. This method does not rely on deep learning, sensor-specific mechanical, or optical modeling, and instead takes inspiration from electromechanics to analyze the vector field produced from 2D marker displacements. Despite the simplicity of our technique, we demonstrate its ability to provide accurate torque readings over two different tactile sensors and three object geometries, and highlight its practicality for the task of USB stick insertion with a compliant robot arm. These results suggest that simple analytical calculations based on dipole moments can sufficiently extract physical quantities from visuotactile sensors.},
  archive   = {C_ICRA},
  author    = {Yuni Fuchioka and Masashi Hamaya},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611429},
  month     = {5},
  pages     = {1077-1083},
  title     = {An electromagnetism-inspired method for estimating in-grasp torque from visuotactile sensors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A neuromorphic system for the real-time classification of
natural textures. <em>ICRA</em>, 1070–1076. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tactile exploration of surfaces is a key component of everyday life, allowing us to make complex inferences about our environments even when vision is occluded. The emergence of biomimetic neuromorphic hardware in recent years has furthered our ability to create biologically plausible sensing solutions. While these platforms continue to improve in regards to latency and power consumption, within recent literature on tactile texture classification there is an emphasis on accuracy at the expense of real-time processing. In order for these tactile sensing systems to find use outside of experimental laboratory environments, it is key to design systems capable of capturing and processing data in real-time. Within this paper we present a system for the real-time classification of texture using a neuromorphic tactile sensor, a spiking neural network and a novel decision making algorithm. Our real-time system achieves classification accuracies of 94% on a dataset of 11 natural textile textures. Furthermore our system is capable of identifying textures at human-level performance in as little as 84ms. Additionally, benchmarking our system across CPU, GPU and Loihi2 hardware platforms resulted in a 96% reduction in power consumption on the neuromorphic platform. This system out-performed previous work by the authors and the state of art, both in terms of accuracy and classification speed.},
  archive   = {C_ICRA},
  author    = {George Brayshaw and Benjamin Ward-Cherrier and Martin J. Pearson},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610401},
  month     = {5},
  pages     = {1070-1076},
  title     = {A neuromorphic system for the real-time classification of natural textures},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A large-area tactile sensor for distributed force sensing
using highly sensitive piezoresistive sponge. <em>ICRA</em>, 1063–1069.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tactile sensing plays a critical role in enabling robots to interact safely with target objects in dynamic and unstructured environments. While various tactile sensors based on different sensing principles or different sensitive materials have been proposed, the development of flexible large-area tactile sensors for robots is still challenging. In this paper, a novel highly sensitive piezoresistive sponge based on multi-walled carbon nanotubes (MWCNTs) and polyurethane (PU) sponge is fabricated for pressure sensing. The sensing behavior of the piezoresistive sponge was experimentally evaluated, showing high sensitivity and fast response. Based on the piezoresistive sponge, a flexible large-area tactile sensor is designed for distributed force detection with electrical resistance tomography technology. The sensing performance of the sensor is validated by touch location, sensitivity analysis, real-time touch discrimination, and touch modality recognition. The experimental results indicate that the sensor performs well in detecting the position and force of contact in a large area. The sensor’s performance shows promise in embodied tactile sensing and human–robot interaction.},
  archive   = {C_ICRA},
  author    = {Wendong Zheng and Kun Liu and Di Guo and Wuqiang Yang and Jun Zhu and Huaping Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610739},
  month     = {5},
  pages     = {1063-1069},
  title     = {A large-area tactile sensor for distributed force sensing using highly sensitive piezoresistive sponge},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ViTacTip: Design and verification of a novel biomimetic
physical vision-tactile fusion sensor. <em>ICRA</em>, 1056–1062. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tactile sensing is significant for robotics since it can obtain physical contact information during manipulation. To capture multimodal contact information within a compact framework, we designed a novel sensor called ViTacTip, which seamlessly integrates both tactile and visual perception capabilities into a single, integrated sensor unit. ViTacTip features a transparent skin to capture fine features of objects during contact, which can be known as the see-through-skin mechanism. In the meantime, the biomimetic tips embedded in ViTacTip can amplify touch motions during tactile perception. For comparative analysis, we also fabricated a ViTac sensor devoid of biomimetic tips, as well as a TacTip sensor with opaque skin. Furthermore, we develop a Generative Adversarial Network (GAN)-based approach for modality switching between different perception modes, effectively alternating the emphasis between vision and tactile perception modes. We conducted a performance evaluation of the proposed sensor across three distinct tasks: i) grating identification, ii) pose regression, iii) contact localization and force estimation. In the grating identification task, ViTacTip demonstrated an accuracy of 99.72%, surpassing TacTip, which achieved 94.60%. It also exhibited superior performance in both pose and force estimation tasks with the minimum error of 0.08 mm and 0.03N, respectively, in contrast to ViTac’s 0.12 mm and 0.15N. Results indicate that ViTacTip outperforms single-modality sensors.},
  archive   = {C_ICRA},
  author    = {Wen Fan and Haoran Li and Weiyong Si and Shan Luo and Nathan Lepora and Dandan Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611186},
  month     = {5},
  pages     = {1056-1062},
  title     = {ViTacTip: Design and verification of a novel biomimetic physical vision-tactile fusion sensor},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Action segmentation using 2D skeleton heatmaps and
multi-modality fusion. <em>ICRA</em>, 1048–1055. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a 2D skeleton-based action segmentation method with applications in fine-grained human activity recognition. In contrast with state-of-the-art methods which directly take sequences of 3D skeleton coordinates as inputs and apply Graph Convolutional Networks (GCNs) for spatiotemporal feature learning, our main idea is to use sequences of 2D skeleton heatmaps as inputs and employ Temporal Convolutional Networks (TCNs) to extract spatiotemporal features. Despite lacking 3D information, our approach yields comparable/superior performances and better robustness against missing keypoints than previous methods on action segmentation datasets. Moreover, we improve the performances further by using both 2D skeleton heatmaps and RGB videos as inputs. To our best knowledge, this is the first work to utilize 2D skeleton heatmap inputs and the first work to explore 2D skeleton+RGB fusion for action segmentation.},
  archive   = {C_ICRA},
  author    = {Syed Waleed Hyder and Muhammad Usama and Anas Zafar and Muhammad Naufil and Fawad Javed Fateh and Andrey Konin and M. Zeeshan Zia and Quoc-Huy Tran},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610644},
  month     = {5},
  pages     = {1048-1055},
  title     = {Action segmentation using 2D skeleton heatmaps and multi-modality fusion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CAMInterHand: Cooperative attention for multi-view
interactive hand pose and mesh reconstruction. <em>ICRA</em>, 1041–1047.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Interactive hand mesh reconstruction from singleview images poses a significant challenge with the severe occlusion and depth ambiguity inherent in interactive hand gestures. Recent approaches that employ probabilistic models and tokenpruned techniques have shown decent results in multi-view human body reconstruction. Nevertheless, these methods have not fully utilized multi-scale semantic information from multiview images and are not applicable in scenarios involving severe occlusion during dual-hand interactions. Simultaneously, current single-view methods independently reconstruct the left and right hands, which are ineffective in enhancing the interaction between both hands. To address these challenges, we propose CAMInterHand, a cooperative attention-based method for multi-view interactive hand pose and mesh reconstruction. Specifically, CAMInterHand extracts local pyramid features and global vertex features from multi-scale feature maps of multi-view images, enabling the exploration of rich local semantic information and facilitating effective feature alignment. Furthermore, CAMInterHand employs the cooperative attention fusion module to fuse all features from multi-view images, enhancing interactions among vertices of dual hands within global and local contexts. We conduct extensive experiments on the large-scale multi-view dataset InterHand2.6M and CAMInterHand achieves a substantial performance improvement over existing methods for multi-view and single-view interactive hand reconstruction.},
  archive   = {C_ICRA},
  author    = {Guwen Han and Qi Ye and Anjun Chen and Jiming Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610469},
  month     = {5},
  pages     = {1041-1047},
  title     = {CAMInterHand: Cooperative attention for multi-view interactive hand pose and mesh reconstruction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual-modal tactile e-skin: Enabling bidirectional
human-robot interaction via integrated tactile perception and feedback.
<em>ICRA</em>, 1026–1032. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To foster an immersive and natural human-robot interaction (HRI), the implementation of tactile perception and feedback becomes imperative, effectively bridging the conventional sensory gap. In this paper, we propose a dual-modal electronic skin (e-skin) that integrates magnetic tactile sensing and vibration feedback for enhanced HRI. The dual-modal tactile e-skin offers multi-functional tactile sensing and programmable haptic feedback, underpinned by a layered structure comprised of flexible magnetic films, soft silicone elastomer, a Hall sensor and actuator array, and a microcontroller unit. The e-skin captures the magnetic field changes caused by subtle deformations through Hall sensors, employing deep learning for accurate tactile perception. Simultaneously, the actuator array generates mechanical vibrations to facilitate haptic feedback, delivering diverse mechanical stimuli. Notably, the dual-modal e-skin is capable of transmitting tactile information bidirectionally, enabling object recognition and fine-weighing operations. This bidirectional tactile interaction framework will enhance the immersion and efficiency of interactions between humans and robots.},
  archive   = {C_ICRA},
  author    = {Shilong Mu and Runze Zhao and Zenan Lin and Yan Huang and Shoujie Li and Chenchang Li and Xiao-Ping Zhang and Wenbo Ding},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610074},
  month     = {5},
  pages     = {1026-1032},
  title     = {Dual-modal tactile E-skin: Enabling bidirectional human-robot interaction via integrated tactile perception and feedback},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Language and sketching: An LLM-driven interactive multimodal
multitask robot navigation framework. <em>ICRA</em>, 1019–1025. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The socially-aware navigation system has evolved to adeptly avoid various obstacles while performing multiple tasks, such as point-to-point navigation, human-following, and -guiding. However, a prominent gap persists: in Human-Robot Interaction (HRI), the procedure of communicating commands to robots demands intricate mathematical formulations. Furthermore, the transition between tasks does not quite possess the intuitive control and user-centric interactivity that one would desire. In this work, we propose an LLM-driven interactive multimodal multitask robot navigation framework, termed LIM2N, to solve the above new challenge in the navigation field. We achieve this by first introducing a multimodal interaction framework where language and hand-drawn inputs can serve as navigation constraints and control objectives. Next, a reinforcement learning agent is built to handle multiple tasks with the received information. Crucially, LIM2N creates smooth cooperation among the reasoning of multimodal input, multitask planning, and adaptation and processing of the intelligent sensing modules in the complicated system. Detailed experiments are conducted in both simulation and the real world demonstrating that LIM2N has solid user needs understanding, alongside an enhanced interactive experience.},
  archive   = {C_ICRA},
  author    = {Weiqin Zu and Wenbin Song and Ruiqing Chen and Ze Guo and Fanglei Sun and Zheng Tian and Wei Pan and Jun Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611462},
  month     = {5},
  pages     = {1019-1025},
  title     = {Language and sketching: An LLM-driven interactive multimodal multitask robot navigation framework},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Non-verbal cues on robot-group persuasion. <em>ICRA</em>,
1000–1006. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When integrating robots into human daily life, persuasive power can be essential. However, there are often group dynamics which can complicate persuasion. This study focuses on how non-verbal cues, specifically gaze and hand gestures, affect the persuasiveness of a social robot. We have designed a protocol to include non-verbal cues in the social robot Vizzy (head and eye gaze, hand gestures) and test them in a series of experiments using the paradigm of the &quot;Desert Survival Challenge&quot;. The goal of the robot is to persuade the participants of the game into changing their answers whilst avoiding negative feelings. It is hypothesized that the nonverbal cues will help avoid psychological reactance without diminishing compliance to the verbal requests issued by the robot. This phenomenon has been verified before for single person persuasion, but it is yet to be tested on groups. Thus, the goal of this project is to verify the effect of non-verbal cues in group persuasion by a robot and comparing it to single person persuasion. The results showed that the robot’s gestures increased compliance by the group and the gaze behaviour decreased psychological reactance.},
  archive   = {C_ICRA},
  author    = {Alexandra Gonçalves and Plinio Moreno and Jodi Forlizzi and Leonel Garcia Marques and Alexandre Bernardino},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611310},
  month     = {5},
  pages     = {1000-1006},
  title     = {Non-verbal cues on robot-group persuasion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predicting the intention to interact with a service robot:
The role of gaze cues. <em>ICRA</em>, 993–999. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For a service robot, it is crucial to perceive as early as possible that an approaching person intends to interact: in this case, it can proactively enact friendly behaviors that lead to an improved user experience. We solve this perception task with a sequence-to-sequence classifier of a potential user intention to interact, which can be trained in a self-supervised way. Our main contribution is a study of the benefit of features representing the person’s gaze in this context. Extensive experiments on a novel dataset show that the inclusion of gaze cues significantly improves the classifier performance (AUROC increases from 84.5 % to 91.2 %); the distance at which an accurate classification can be achieved improves from 2.4 m to 3.2 m. We also quantify the system’s ability to adapt to new environments without external supervision. Qualitative experiments show practical applications with a waiter robot.},
  archive   = {C_ICRA},
  author    = {Simone Arreghini and Gabriele Abbate and Alessandro Giusti and Antonio Paolillo},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610289},
  month     = {5},
  pages     = {993-999},
  title     = {Predicting the intention to interact with a service robot: The role of gaze cues},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The un-kidnappable robot: Acoustic localization of sneaking
people. <em>ICRA</em>, 985–992. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {How easy is it to sneak up on a robot? We examine whether we can detect people using only the incidental sounds they produce as they move, even when they try to be quiet. To do so, we first collect a robotic dataset of high-quality 4-channel audio paired with 360° RGB data of people moving in different indoor settings. Using this dataset, we train models to predict if there is a moving person nearby and then their location using only audio. We implement our method on a robot, allowing it to track a single person moving quietly using only passive audio sensing. For demonstration videos, see our project page.},
  archive   = {C_ICRA},
  author    = {Mengyu Yang and Patrick Grady and Samarth Brahmbhatt and Arun Balajee Vasudevan and Charles C. Kemp and James Hays},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611514},
  month     = {5},
  pages     = {985-992},
  title     = {The un-kidnappable robot: Acoustic localization of sneaking people},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time dexterous prosthesis hand control by decoding
neural information based on EMG decomposition. <em>ICRA</em>, 966–972.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The vague interpretation of myoelectrical signals on the residual limb end makes restoring dexterous hand function in amputees still impossible. Understanding motor control between human motion intention and synaptic inputs to motor neurons also remains a significant challenge. The neural decoding methods of surface EMG signals remains challenging, which limit the application of robot hand in real life. Herein, we propose and substantiate a human-machine interface for motor control that introduces neural information of motor neurons in conjunction with the combination mechanism of muscle contraction. The interface firstly introduces a new concept of motor unit (MU) spike trains, which combines decoupling of the electrical activations on motor neuron axons with extraction of motion patterns from the discharge timings of the motor neuron pools. We realized a real-time implementation of the EMG decomposition algorithm on our developed prosthesis hand control system. The control scheme provides an accurate classification of intuitive hand motions, enabling the amputee to perform versatile finger movements of the prosthesis hand. The concept of motor neuron discharge timings was evaluated through experiments on one amputee participant and six able-bodied participants. The results show that the neuroprosthesis hand control scheme based on MU spike trains has the capacity of generating accurate and intuitive hand movements for amputees in a physical environment.},
  archive   = {C_ICRA},
  author    = {Zhenzhi. Ying and Xianyu. Zhang and Shihao. Li and Koki. Nakashima and Liming. Shu and Naohiko. Sugita},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611356},
  month     = {5},
  pages     = {966-972},
  title     = {Real-time dexterous prosthesis hand control by decoding neural information based on EMG decomposition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Short term after-effects of small force fields applied by an
upper-limb exoskeleton on inter-joint coordination. <em>ICRA</em>,
959–965. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Exoskeleton technologies have numerous potential applications, ranging from improving human motor skills to aiding individuals in their daily activities. While exoskeletons are increasingly viewed, for example, as promising tools in industrial ergonomics, the effect of using them on human motor control, particularly on inter-joint coordination, remains relatively uncharted. This paper investigates the effects of generic low-amplitude force fields applied by an exoskeleton on motor strategies in asymptomatic users. The force fields mimic common perturbations encountered in exoskeletons, such as residual friction, over/under-tuned assistance, or structural elasticity. Fifty-five participants performed reaching tasks while connected to an arm exoskeleton, experiencing one of five tested force fields. Their movements before and after exposure to the exoskeleton force field were compared. The study focuses both on spatial and temporal changes in coordination using specific metrics. The results reveal that even brief exposure to a low- amplitude force field, or to uncompensated residual friction and dynamic forces, applied at the joint level can alter the interjoint coordination, while task performance remains unaffected. The tested force fields induced varying degrees of changes in joint contributions and synchronization. This study highlights the importance of monitoring coordination changes to fully understand the impact of exoskeletons on human motor control and thus enable safe and widespread adoption of those devices.},
  archive   = {C_ICRA},
  author    = {Océane Dubois and Agnès Roby-Brami and Ross Parry and Nathanaël Jarrassé},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610645},
  month     = {5},
  pages     = {959-965},
  title     = {Short term after-effects of small force fields applied by an upper-limb exoskeleton on inter-joint coordination},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards a unified approach for continuously-variable
impedance control of powered prosthetic legs over walking speeds and
inclines. <em>ICRA</em>, 944–950. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Research in powered prosthesis control has explored the use of impedance-based control algorithms due to their biomimetic capabilities and intuitive structure. Modern impedance controllers feature parameters that smoothly vary over gait phase and task according to a data-driven model. However, these recent efforts only use continuous impedance control during stance and instead utilize discrete transition logic to switch to kinematic control during swing, necessitating two separate models for the different parts of the stride. In contrast, this paper presents a controller that uses smooth impedance parameter trajectories throughout the gait, unifying the stance and swing periods under a single, continuous model. Furthermore, this paper proposes a basis model to represent inter-task relationships in the impedance parameters—a strategy that has previously been shown to improve model accuracy over classic linear interpolation methods. In the proposed controller, a weighted sum of Fourier series is used to model the impedance parameters of each joint as continuous functions of gait cycle progression and task. Fourier series coefficients are determined via convex optimization such that the controller best reproduces the joint torques and kinematics in a reference able-bodied dataset. Experiments with a powered knee-ankle prosthesis show that this simpler, unified model produces competitive results when compared to a more complex hybrid impedance-kinematic model over varying walking speeds and inclines.},
  archive   = {C_ICRA},
  author    = {Albert J. Lee and Curt A. Laubscher and T. Kevin Best and Robert D. Gregg},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610071},
  month     = {5},
  pages     = {944-950},
  title     = {Towards a unified approach for continuously-variable impedance control of powered prosthetic legs over walking speeds and inclines},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning based acoustic measurement approach for
robotic applications on orthopedics. <em>ICRA</em>, 921–927. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In Total Knee Replacement Arthroplasty (TKA), surgical robotics can provide image-guided navigation to fit implants with high precision. Its tracking approach highly relies on inserting bone pins into the bones tracked by the optical tracking system. This is normally done by invasive, radiative manners (implantable markers and CT scans), which introduce unnecessary trauma and prolong the preparation time for patients. To tackle this issue, ultrasound-based bone tracking could offer an alternative. In this study, we proposed a novel deep-learning structure to improve the accuracy of bone tracking by an A-mode ultrasound (US). We first obtained a set of ultrasound dataset from the cadaver experiment, where the ground truth locations of bones were calculated using bone pins. These data were used to train the proposed CasAtt-UNet to predict bone location automatically and robustly. The ground truth bone locations and those locations of US were recorded simultaneously. Therefore, we could label bone peaks in the raw US signals. As a result, our method achieved sub-millimeter precision across all eight bone areas with the only exception of one channel in the ankle. This method enables the robust measurement of lower extremity bone positions from 1D raw ultrasound signals. It shows great potential to apply A-mode ultrasound in orthopedic surgery from safe, convenient, and efficient perspectives.},
  archive   = {C_ICRA},
  author    = {Bangyu Lan and Momen Abayazid and Nico Verdonschot and Stefano Stramigioli and Kenan Niu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611713},
  month     = {5},
  pages     = {921-927},
  title     = {Deep learning based acoustic measurement approach for robotic applications on orthopedics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A transtibial prosthesis using a parallel spring mechanism.
<em>ICRA</em>, 914–920. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Prosthetic legs have been used to restore function in the lower limbs lost due to amputation. Early designs including prosthetic legs with a passive joint or without any joint as well as the Energy Storing and Releasing (ESR) feet have shown deficiency in push-off torque, which results in asymmetric gait pattern, slower walking speed, and higher cost of transportation. Although powered prosthetic legs address the aforementioned problems, they suffer from lower energy efficiency, higher volume and weight. In this paper, a powered transtibial prosthesis using a Parallel Elastic Actuator (PEA) is proposed in order to generate the joint torque needed for walking with a lower-powered actuator for lighter and more compact design. A non-linear spring mechanism is proposed to generate the spring torque as needed. The implemented prosthetic leg is evaluated with three intact subjects. The experimental results shows that smaller torque is required for the motor with the spring mechanism. Therefore, less electrical power is consumed when the spring mechanism is used, which implies a lower-powered actuator is sufficient to generate the joint torque needed for walking.},
  archive   = {C_ICRA},
  author    = {Donggyu Jung and Shinsuk Park and Junho Choi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610725},
  month     = {5},
  pages     = {914-920},
  title     = {A transtibial prosthesis using a parallel spring mechanism},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comparison of rating scale and pairwise comparison methods
for measuring human co-worker subjective impression of robot during
physical human-robot collaboration. <em>ICRA</em>, 907–913. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Rating Scale method has been long deemed the standard for measuring subjective perceptions. However, in the field of physical human-robot collaboration (pHRC), its aptness should be put under scrutiny due to inherent challenges such as response bias, between-subject variations, and the granularity nature.Individual variances can introduce significant bias in the rating scale results. A high granularity in the scale could overwhelm participants, leading to unclear and biased responses, while a low granularity may gloss over the fine nuances of human feelings. Additionally, there’s a notable risk of receiving careless responses, which compromise data reliability. Recognizing these challenges, this paper proposes the application of Pairwise Comparison (PC) in pHRC — an alternative survey technique that emphasizes direct comparisons between items on the defined criteria. By using the NASA Task Load Index (NASA-TLX) as a template, RS and PC questionnaires are designed and used in a series of pHRC experiments. Our preliminary findings suggest that PC is more precise and robust than the rating scale method. Compared to RS, PC fosters authentic participant interests in the experiment by intuitive question design and reducing the experimental duration. Besides, the accuracy and reliability of PC are also found to be consistent regardless of the variations in our experimental procedure design.},
  archive   = {C_ICRA},
  author    = {Qiao Wang and Ziqi Wang and Marc G. Carmichael and Dikai Liu and Chin-Teng Lin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611050},
  month     = {5},
  pages     = {907-913},
  title     = {Comparison of rating scale and pairwise comparison methods for measuring human co-worker subjective impression of robot during physical human-robot collaboration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model predictive control with graph dynamics for garment
opening insertion during robot-assisted dressing. <em>ICRA</em>,
883–890. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots have a great potential to help people with movement limitations in activities of daily living, such as dressing. A common problem in almost all dressing tasks is the insertion of a garment’s opening around a part of the human body. The rich contact environment and the deformations of the garment make the task a challenging problem for robots. In this paper, we propose a bi-manual control method for garment opening insertion during robot-assisted dressing. Specifically, we propose a model predictive controller that uses an Attention-based Relational Graph Convolutional Network (ARGCN) for modeling the dynamics of the opening in the presence of the body. We train the model entirely in simulation and validate our method in four real-world dressing scenarios of a medical training manikin. We show that our method generalizes well in the real-world opening insertion tasks achieving an overall success rate of 97.5%, even though the dynamics and the shapes vastly differ from the simulation setup.},
  archive   = {C_ICRA},
  author    = {Stelios Kotsovolis and Yiannis Demiris},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611478},
  month     = {5},
  pages     = {883-890},
  title     = {Model predictive control with graph dynamics for garment opening insertion during robot-assisted dressing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Towards robo-coach: Robot interactive stiffness/position
adaptation for human strength and conditioning training. <em>ICRA</em>,
860–866. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traditional strength and conditioning training relies on the utilization of free weights, such as weighted implements, to elicit external stimuli. However, this approach poses a significant challenge when attempting to modify or adjust the loads within a single training set. This paper introduces an innovative method for achieving adjustable loads during resistance training by leveraging physical Human-Robot Interaction (pHRI). The primary objective is to regulate targeted muscle activation through the use of Robo-Coach (robotic coach system). We first utilize a Task-Parameterized Gaussian Mixture Model (TP-GMM) to learn the motion of coach demonstration, which can be generalized for the trainees. The 3D path extracted from the generated trajectory is then projected onto a 2D plane with respect to the direction of the load. Furthermore, we propose a hybrid stiffness/position generator for online task execution. This generator determines the desired positions in the 2D plane according to the contact point displacements in the stimuli direction and, simultaneously, sets the desired stiffness based on the muscle activation feedback. Finally, the Robo-Coach is implemented with a variable impedance controller to achieve load-adjustable resistance training with the trainee. The biceps curl exercises were conducted and the results showed favorable performance, indicating the effectiveness of this approach.},
  archive   = {C_ICRA},
  author    = {Chenzui Li and Xi Wu and Tao Teng and Sylvain Calinon and Fei Chen},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611028},
  month     = {5},
  pages     = {860-866},
  title     = {Towards robo-coach: Robot interactive Stiffness/Position adaptation for human strength and conditioning training},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards robot to human skill coaching: A ML-powered IoT and
HRI platform for martial arts training. <em>ICRA</em>, 853–859. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Advances in human sensing and machine learning are paving the way for new applications of robotics in sports and fitness, making skill coaching smarter, easier and more accessible. Physical and social human robot interaction in particular has received special attention as a feedback mechanism for human performance augmentation. A core challenge in deploying robots that interact physically with humans in dynamic environments such as sports, relates to modeling human skills and designing appropriate interaction schemes. We present the first ML-based HRI platform for physical robot to human skill coaching in real-time in Martial Arts which can be extended to various sports. Our system comprises of the Sawyer robot, our specially developed IoT katana and a skill-training program for the Martial Art of Iaido. We built and deployed in real-time a ML-based Iaido strike recognition model trained on expert and beginner data, and achieved accuracies ranging between 94.8% and 99.97%. We assessed the system’s effectiveness in coaching skills through robot interaction in a sparring experiment and a survey involving 12 participants practicing key Iaido techniques with guided training from Sawyer. Our results demonstrated improvement in all participants’ Iaido strike skill after training with Sawyer, and they responded positively to robot-assisted skill coaching.},
  archive   = {C_ICRA},
  author    = {Katia Bourahmoune and Karlos Ishac and Marc Carmichael},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610291},
  month     = {5},
  pages     = {853-859},
  title     = {Towards robot to human skill coaching: A ML-powered IoT and HRI platform for martial arts training},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating robustness of visual representations for object
assembly task requiring spatio-geometrical reasoning. <em>ICRA</em>,
831–837. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper primarily focuses on evaluating and benchmarking the robustness of visual representations in the context of object assembly tasks. Specifically, it investigates the alignment and insertion of objects with geometrical extrusions, commonly referred to as a peg-in-hole task. The accuracy required to detect and orient the peg and the hole geometry in SE(3) space for successful assembly poses significant challenges. Addressing this, we employ a general framework in visuomotor policy learning that utilizes visual pretraining models as vision encoders. Our study investigates the robustness of this framework when applied to a dual-arm manipulation setup, specifically to the grasp variations. Our quantitative analysis shows that existing pretrained models fail to capture the essential visual features necessary for this task: a visual encoder trained from scratch consistently outperforms the frozen pre-trained models. Moreover, we discuss rotation representations and associated loss functions that substantially improve policy learning. We present a novel task scenario designed to evaluate the progress in visuomotor policy learning, with a specific focus on improving the robustness of intricate assembly tasks that require both geometrical and spatial reasoning. Videos, additional experiments, dataset, and code are available at https://sites.google.com/view/geometric-peg-in-hole.},
  archive   = {C_ICRA},
  author    = {Chahyon Ku and Carl Winge and Ryan Diaz and Wentao Yuan and Karthik Desingh},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610774},
  month     = {5},
  pages     = {831-837},
  title     = {Evaluating robustness of visual representations for object assembly task requiring spatio-geometrical reasoning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning to design 3D printable adaptations on everyday
objects for robot manipulation. <em>ICRA</em>, 824–830. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Advancements in robot learning for object manipulation have shown promising results, yet certain everyday objects remain challenging for robots to effectively interact with. This discrepancy arises from the fact that human-designed objects are optimized for human use rather than robot manipulation. To address this gap, we propose a framework to automatically design 3D printable adaptations that can be attached to hard-to-use objects, thus improving &quot;robot ergonomics&quot;. Our learning-based framework formulates the adaptation design and control as a dual Markov decision process and is able to improve robot-object interactions for various robot end effectors and objects. We further validate our designs in the real world with a Franka Panda robot. Please see the supplementary video and https://object-adaptation.github.io for additional visualizations.},
  archive   = {C_ICRA},
  author    = {Michelle Guo and Ziang Liu and Stephen Tian and Zhaoming Xie and Jiajun Wu and C. Karen Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610268},
  month     = {5},
  pages     = {824-830},
  title     = {Learning to design 3D printable adaptations on everyday objects for robot manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-level reasoning for robotic assembly: From sequence
inference to contact selection. <em>ICRA</em>, 816–823. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automating the assembly of objects from their parts is a complex problem with innumerable applications in manufacturing, maintenance, and recycling. Unlike existing research, which is limited to target segmentation, pose regression, or using fixed target blueprints, our work presents a holistic multi-level framework for part assembly planning consisting of part assembly sequence inference, part motion planning, and robot contact optimization. We present the Part Assembly Sequence Transformer (PAST) – a sequence-to-sequence neural network – to infer assembly sequences recursively from a target blueprint. We then use a motion planner and optimization to generate part movements and contacts. To train PAST, we introduce D4PAS: a large-scale Dataset for Part Assembly Sequences consisting of physically valid sequences for industrial objects. Experimental results show that our approach generalizes better than prior methods while needing significantly less computational time for inference. Further details on our experiments and results are available in the video.},
  archive   = {C_ICRA},
  author    = {Xinghao Zhu and Devesh K. Jha and Diego Romeres and Lingfeng Sun and Masayoshi Tomizuka and Anoop Cherian},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611259},
  month     = {5},
  pages     = {816-823},
  title     = {Multi-level reasoning for robotic assembly: From sequence inference to contact selection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Online fault detection in manipulation tasks via generative
models. <em>ICRA</em>, 802–808. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a method, Generative Adversarial Networks for Detecting Erroneous Results (GANDER), leveraging Generative Adversarial Networks to provide online error detection in manipulation tasks for autonomous robot systems. GANDER relies on mapping input images of a trained task to a learned manifold that contains only positive task executions and outcomes. When reconstructed through this manifold, the input images from successful task executions will remain largely unchanged, while the images from a failed task will change significantly. Using this insight, GANDER enables inspection and task outcome verification capabilities using a large number of positive examples but only a small set of negative examples, thus increasing the applicability of autonomous robot systems. We detail the design of GANDER and provide results of a proof-of-concept system, establishing its efficacy in an autonomous inspection, maintenance, and repair task. GANDER produces favorable results compared to baseline approaches and is capable of correctly identifying off-nominal behavior with 91.65% accuracy in our test task. Ablation studies were also performed to quantify the amount of data ultimately needed for this approach to succeed.},
  archive   = {C_ICRA},
  author    = {Michael W. Lanighan and Oscar Youngquist},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611418},
  month     = {5},
  pages     = {802-808},
  title     = {Online fault detection in manipulation tasks via generative models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PoseFusion: Multi-scale keypoint correspondence for
monocular camera-to-robot pose estimation in robotic manipulation.
<em>ICRA</em>, 795–801. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual-based robot pose estimation is a fundamental challenge, involving the determination of the camera’s pose with respect to a robot. Conventional methods for camera-to-robot pose calibration rely on fiducial markers to establish keypoint correspondences. However, these approaches exhibit significant variability in accuracy and robustness, particularly in 2D keypoint detection. In this work, we present an end-to-end pose estimation approach that achieves camera-to-robot calibration using monocular images and keypoint information. Our method employs a two-level nested U-shaped architecture, featuring a bottom-level residual U-block to extract richer contextual information from diverse receptive fields to enhance keypoint refinement. By incorporating the perspective-n-point (PnP) algorithm and leveraging 3D robot joint keypoints, we establish correspondence of 3D coordinate points between the robot’s coordinate system and the camera’s coordinate system, facilitating accurate pose estimation. Experimental evaluations encompass real-world and synthetic datasets, demonstrating competitive results across three distinct robot manipulators.},
  archive   = {C_ICRA},
  author    = {Xujun Han and Shaochen Wang and Xiucai Huang and Zhen Kan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610844},
  month     = {5},
  pages     = {795-801},
  title     = {PoseFusion: Multi-scale keypoint correspondence for monocular camera-to-robot pose estimation in robotic manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pseudo labeling and contextual curriculum learning for
online grasp learning in robotic bin picking. <em>ICRA</em>, 788–794.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The prevailing grasp prediction methods predominantly rely on offline learning, overlooking the dynamic grasp learning that occurs during real-time adaptation to novel picking scenarios. These scenarios may involve previously unseen objects, variations in camera perspectives, and bin configurations, among other factors. In this paper, we introduce a novel approach, SSL-ConvSAC, that combines semi-supervised learning and reinforcement learning for online grasp learning. By treating pixels with reward feedback as labeled data and others as unlabeled, it efficiently exploits unlabeled data to enhance learning. In addition, we address the imbalance between labeled and unlabeled data by proposing a contextual curriculum-based method. We ablate the proposed approach on real-world evaluation data and demonstrate promise for improving online grasp learning on bin picking tasks using a physical 7-DoF Franka Emika robot arm with a suction gripper. Video: https://youtu.be/OAro5pg8I9U},
  archive   = {C_ICRA},
  author    = {Huy Le and Philipp Schillinger and Miroslav Gabriel and Alexander Qualmann and Ngo Anh Vien},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611348},
  month     = {5},
  pages     = {788-794},
  title     = {Pseudo labeling and contextual curriculum learning for online grasp learning in robotic bin picking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uncertainty-driven exploration strategies for online grasp
learning. <em>ICRA</em>, 781–787. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing grasp prediction approaches are mostly based on offline learning, while, ignoring the exploratory grasp learning during online adaptation to new picking scenarios, i.e., objects that are unseen or out-of-domain (OOD), camera and bin settings, etc. In this paper, we present an uncertainty-based approach for online learning of grasp predictions for robotic bin picking. Specifically, the online learning algorithm with an effective exploration strategy can significantly improve its adaptation performance to unseen environment settings. To this end, we first propose to formulate online grasp learning as an RL problem that will allow us to adapt both grasp reward prediction and grasp poses. We propose various uncertainty estimation schemes based on Bayesian uncertainty quantification and distributional ensembles. We carry out evaluations on real-world bin picking scenes of varying difficulty. The objects in the bin have various challenging physical and perceptual characteristics that can be characterized by semi- or total transparency, and irregular or curved surfaces. The results of our experiments demonstrate a notable improvement of grasp performance in comparison to conventional online learning methods which incorporate only naive exploration strategies. Video: https://youtu.be/fPKOrjC2QrU},
  archive   = {C_ICRA},
  author    = {Yitian Shi and Philipp Schillinger and Miroslav Gabriel and Alexander Qualmann and Zohar Feldman and Hanna Ziesche and Ngo Anh Vien},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610056},
  month     = {5},
  pages     = {781-787},
  title     = {Uncertainty-driven exploration strategies for online grasp learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Fast and robust point cloud registration with tree-based
transformer. <em>ICRA</em>, 773–780. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Point cloud registration is essential in computer vision and robotics. Recently, transformer-based methods have achieved advanced point cloud registration performance. However, the standard attention mechanism utilized in these methods considers many low-relevance points, and it has difficulty focusing its attention weights on sparse and meaningful points, leading to limited local structure modeling capabilities and quadratic computational complexity. To address these limitations, we present the Tree-based Transformer (TrT), which is able to extract abundant local and global features with linear computational complexity. Specifically, the TrT builds coarse-to-dense feature trees, and a novel Tree-based Attention (TrA) is proposed to guide the progressive convergence of the attended regions toward meaningful points and to structurize point clouds following tree structures. In each layer, the top ${\mathcal{S}}$ key points with the highest attention scores are selected, such that in the next layer, attention is evaluated only within the specified high-relevance regions, corresponding to the child points of these selected ${\mathcal{S}}$ points. Additionally, coarse features containing high-level semantic information are incorporated into the child points to guide the feature extraction process, facilitating local structure modeling and multiscale information integration. Consequently, TrA enables the model to focus on critical local structures and extract rich local information with linear computational complexity. Experiments demonstrate that our method achieves state-of-the-art performance on 3DMatch and KITTI benchmarks. The code for our method is publicly available at https://github.com/CGuangyan-BIT/TrT.},
  archive   = {C_ICRA},
  author    = {Guangyan Chen and Meiling Wang and Yi Yang and Li Yuan and Yufeng Yue},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610004},
  month     = {5},
  pages     = {773-780},
  title     = {Fast and robust point cloud registration with tree-based transformer},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EdgePoint: Efficient point detection and compact description
via distillation. <em>ICRA</em>, 766–772. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Efficient interest point detection and description in images play a crucial role in many tasks such as multi-robot SLAM and collaborative localization. To facilitate fast detection and generate compact descriptions on edge devices, we introduce EdgePoint, a lightweight neural network. We design a new detection loss UnfoldSoftmax to improve inference speed. Futhermore, we propose Ortho-Alignment loss combined with LocalPCA compression to learn compact 32-dimensional descriptors. To enable efficient storage or communication, we also quantize the generated descriptors into integral values. We perform EdgePoint on various datasets, and show that it surpasses SuperPoint in performance while utilizing only 1% of the parameters and achieving up to more than 10 times faster inference speed. By applying descriptor quantization, the requirements for storage and communication can be reduced by up to 97% without performance decreasing.},
  archive   = {C_ICRA},
  author    = {Haodi Yao and Ning Hao and Chen Xie and Fenghua He},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611607},
  month     = {5},
  pages     = {766-772},
  title     = {EdgePoint: Efficient point detection and compact description via distillation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CAPT: Category-level articulation estimation from a single
point cloud using transformer. <em>ICRA</em>, 751–757. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to estimate joint parameters is essential for various applications in robotics and computer vision. In this paper, we propose CAPT: category-level articulation estimation from a point cloud using Transformer. CAPT uses an end-to-end transformer-based architecture for joint parameter and state estimation of articulated objects from a single point cloud. The proposed CAPT methods accurately estimate joint parameters and states for various articulated objects with high precision and robustness. The paper also introduces a motion loss approach, which improves articulation estimation performance by emphasizing the dynamic features of articulated objects. Additionally, the paper presents a double voting strategy to provide the framework with coarse-to-fine parameter estimation. Experimental results on several category datasets demonstrate that our methods outperform existing alternatives for articulation estimation. Our research provides a promising solution for applying Transformer-based architectures in articulated object analysis.},
  archive   = {C_ICRA},
  author    = {Lian Fu and Ryoichi Ishikawa and Yoshihiro Sato and Takeshi Oishi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611073},
  month     = {5},
  pages     = {751-757},
  title     = {CAPT: Category-level articulation estimation from a single point cloud using transformer},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FF-LOGO: Cross-modality point cloud registration with
feature filtering and local to global optimization. <em>ICRA</em>,
744–750. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cross-modality point cloud registration is confronted with significant challenges due to inherent differences in modalities between sensors. To deal with this problem, we propose FF-LOGO: a cross-modality point cloud registration framework with Feature Filtering and LOcal-Global Optimization. The cross-modality feature correlation filtering module extracts geometric transformation-invariant features from cross-modality point clouds and achieves point selection by feature matching. We also introduce a cross-modality optimization process, including a local adaptive key region aggregation module and a global modality consistency fusion optimization module. Experimental results demonstrate that our two-stage optimization significantly improves the registration accuracy of the feature association and selection module. Our method achieves a substantial increase in recall rate compared to the current state-of-the-art methods on the 3DCSR dataset, improving from 40.59% to 75.74%. Our code will be available at https://github.com/wangmohan17/FFLOGO.},
  archive   = {C_ICRA},
  author    = {Nan Ma and Mohan Wang and Yiheng Han and Yong-Jin Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610549},
  month     = {5},
  pages     = {744-750},
  title     = {FF-LOGO: Cross-modality point cloud registration with feature filtering and local to global optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-confidence guided source-free domain adaption method
for point cloud primitive segmentation. <em>ICRA</em>, 737–743. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Point cloud primitive segmentation aims to segment the surface point cloud into various geometric types of primitives, which plays a vital role in robot operation and industrial automation. However, differences in object structures and shapes across industrial datasets create domain shift issues, compounded by privacy concerns preventing dataset sharing. To address these challenges, we propose a novel source-free domain adaptation method for point cloud primitive segmentation, which follows the popular pseudo-label based self-training framework. Unlike previous works using single-model uncertainty to refine pseudo labels, our method leverages multi-confidence, including transformation consistency, task confidence, and geometric saliency to provide more informative guidance. Specifically, the transformation consistency is first utilized to vote pseudo-labels and task confidences. Furthermore, to filter out high-confident noises and obtain more reliable pseudo-labels, we investigate the geometric curvature properties of primitives and propose a geometric saliency guided dynamic prototype matching and label graph aggregation strategies for pseudo-label reassignment with different task confidence. For this novel task, we construct several datasets and verify the effectiveness of the proposed methods through a series of experiments.},
  archive   = {C_ICRA},
  author    = {Shaohu Wang and Yuchuang Tong and Xiuqin Shang and Zhengtao Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611600},
  month     = {5},
  pages     = {737-743},
  title     = {Multi-confidence guided source-free domain adaption method for point cloud primitive segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and fabrication of string-driven origami robots.
<em>ICRA</em>, 713–719. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Origami designs and structures have been widely used in many fields, such as morphing structures, robotics, and metamaterials. However, the design and fabrication of origami structures rely on human experiences and skills, which are both time and labor-consuming. In this paper, we present a rapid design and fabrication method for string-driven origami structures and robots. We developed an origami design software to generate desired crease patterns based on analytical models and Evolution Strategies (ES). Additionally, the software can automatically produce 3D models of origami designs. We then used a dual-material 3D printer to fabricate those wrapping-based origami structures with the required mechanical properties. We utilized Twisted String Actuators (TSAs) to fold the target 3D structures from flat plates. To demonstrate the capability of these techniques, we built and tested an origami crawling robot and an origami robotic arm using 3D-printed origami structures driven by TSAs.},
  archive   = {C_ICRA},
  author    = {Peiwen Yang and Shuguang Li},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610989},
  month     = {5},
  pages     = {713-719},
  title     = {Design and fabrication of string-driven origami robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A phase-change emulsion jamming gripper for manipulation of
micro-scale textured surfaces. <em>ICRA</em>, 706–712. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The inherent elasticity of soft materials can be used to create robotic grippers that deform and comply to a variety of irregular shapes. To date, several soft adaptive grasping strategies have been reported, however, most of them focus on adapting to the overall shape of the structure, while the adaptive grasping of small surface asperities is overlooked. In this paper, we propose a novel method to achieve adaptive grasping on surface asperities with a smart shape-memory silicone sponge. Heating above 60°C makes the sponge soft and deformable to allow it to penetrate within surface asperities via a pressure normal to the surface. Cooling down below 60°C makes the sponge &quot;jam&quot; to retain its deformed shape. The interlocking force between the jammed sponge and the asperities, and the increased area of contact, allows for adaptive grasping on asperities down to 0.4 mm with an adhesive force of up to 27.7 N in a 40 × 40 mm contacting area. We introduce the design, working principle, fabrication, and optimization of a robotic gripper based on this shape-memory silicone sponge. This sponge-jamming gripper shows great potential for developing next-generation robotic grippers for the manipulation of textured and discontinuous surfaces.},
  archive   = {C_ICRA},
  author    = {Alex Keller and Tianqi Yue and Qiukai Qi and Andrew T. Conn and Jonathan Rossiter},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611273},
  month     = {5},
  pages     = {706-712},
  title     = {A phase-change emulsion jamming gripper for manipulation of micro-scale textured surfaces},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A passively bendable, compliant tactile palm with RObotic
modular endoskeleton optical (ROMEO) fingers. <em>ICRA</em>, 691–697.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many robotic hands currently rely on extremely dexterous robotic fingers and a thumb joint to envelop themselves around an object. Few hands focus on the palm even though human hands greatly benefit from their central fold and soft surface. As such, we develop a novel structurally compliant soft palm, which enables more surface area contact for the objects that are pressed into it. Moreover, this design, along with the development of a new low-cost, flexible illumination system, is able to incorporate a high-resolution tactile sensing system inspired by the GelSight sensors. Concurrently, we design RObotic Modular Endoskeleton Optical (ROMEO) fingers, which are underactuated two-segment soft fingers that are able to house the new illumination system, and we integrate them into these various palm configurations. The resulting robotic hand is slightly bigger than a baseball and represents one of the first soft robotic hands with actuated fingers and a passively compliant palm, all of which have high-resolution tactile sensing. This design also potentially helps researchers discover and explore more soft-rigid tactile robotic hand designs with greater capabilities in the future.},
  archive   = {C_ICRA},
  author    = {Sandra Q. Liu and Edward H. Adelson},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611043},
  month     = {5},
  pages     = {691-697},
  title     = {A passively bendable, compliant tactile palm with RObotic modular endoskeleton optical (ROMEO) fingers},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A scalable, light-controlled, individually addressable,
non-metal actuator array. <em>ICRA</em>, 684–690. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Research in the area of photo-actuation is growing rapidly, yet there are few examples of photo-actuators with practical use cases. One potential application is for the control of intelligent electromagnetic surfaces, or two-dimensional arrays that could shape and control an incident electromagnetic field in ideally any manner. A promising concept to realize such a surface leverages signal refraction via antenna edges, but requires non-metal actuation, large antenna rotations, and high antenna angular accuracy for long periods of time. Here, we present a nonmetal, light-controlled, multi-position inchworm actuator array that can rotate an antenna 88 degrees in incremental steps of less than 3.4 degrees with zero-power shape-persistence. The design is modular and rapidly manufacturable via a layered laser-cutting technique, such that the actuator can be tiled into an array to control the rotation of many antennas. We control the array with a single focused IR light that rasters across the actuators to precisely control all antenna positions. We characterize the response time, accuracy, and repeatability of a single actuator, and demonstrate the array achieving diverse antenna configurations. This work advances the precision and scalability of photothermal actuation not only for use in intelligent electromagnetic surfaces but for any application benefitting from light-controlled actuation.},
  archive   = {C_ICRA},
  author    = {Sophie Paul and Matthew R. Devlin and Elliot W. Hawkes},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610023},
  month     = {5},
  pages     = {684-690},
  title     = {A scalable, light-controlled, individually addressable, non-metal actuator array},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lightweight untethered soft robotic fish. <em>ICRA</em>,
669–675. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Aquatic organisms, due to soft body structure and high agility, have inspired many biomimetic robots. However, considering the issues of insulation and waterproofing, as well as the driving module of soft materials, their control systems are usually larger and heavier. Therefore, small underwater robots often tethered, i.e., it cannot integrate energy and control systems onto the body, which greatly limited in its working range and activity mode. This paper presents a small untethered bionic manta ray. The robotic fish is driven by dielectric elastomer actuators (DEA), which controls the double wing structure on both sides by the central muscle part to simulate the process of the manta ray’s lateral fins fanning to propel itself forward. And the flexible printed circuit board (FPC) constitutes the body of the fish and is also an independent energy control system. The electronic components are evenly distributed on the double-wing structure of the robotic fish to realize the integration of the energy control system. This circuit system can be powered by a small lithium battery and output a periodic voltage to drive the motion of the robotic fish. The masses of our tethered and untethered fish are 1.9g and 5.1g respectively. The swimming speed of these two types of fish can reach 42.5mm/s and 17.0 mm/s. And this design principle can be extended to the research and design of various flexible devices and soft robots.},
  archive   = {C_ICRA},
  author    = {Xiangxing Wang and Xuan Pei and Xinyang Wang and Taogang Hou},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610533},
  month     = {5},
  pages     = {669-675},
  title     = {Lightweight untethered soft robotic fish},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RH20T: A comprehensive robotic dataset for learning diverse
skills in one-shot. <em>ICRA</em>, 653–660. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A key challenge for robotic manipulation in open domains is how to acquire diverse and generalizable skills for robots. Recent progress in one-shot imitation learning and robotic foundation models have shown promise in transferring trained policies to new tasks based on demonstrations. This feature is attractive for enabling robots to acquire new skills and improve their manipulative ability. However, due to limitations in the training dataset, the current focus of the community has mainly been on simple cases, such as push or pick-place tasks, relying solely on visual guidance. In reality, there are many complex skills, some of which may even require both visual and tactile perception to solve. This paper aims to unlock the potential for an agent to generalize to hundreds of real-world skills with multi-modal perception. To achieve this, we have collected a dataset comprising over 110,000 contact-rich robot manipulation sequences across diverse skills, contexts, robots, and camera viewpoints, all collected in the real world. Each sequence in the dataset includes visual, force, audio, and action information. Moreover, we also provide a corresponding human demonstration video and a language description for each robot sequence. We have invested significant efforts in calibrating all the sensors and ensuring a high-quality dataset. The dataset is made publicly available on our website: rh20t.github.io.},
  archive   = {C_ICRA},
  author    = {Hao-Shu Fang and Hongjie Fang and Zhenyu Tang and Jirong Liu and Chenxi Wang and Junbo Wang and Haoyi Zhu and Cewu Lu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611615},
  month     = {5},
  pages     = {653-660},
  title     = {RH20T: A comprehensive robotic dataset for learning diverse skills in one-shot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RoboVQA: Multimodal long-horizon reasoning for robotics.
<em>ICRA</em>, 645–652. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a scalable, bottom-up and intrinsically diverse data collection scheme that can be used for high-level reasoning with long and medium horizons and that has 2.2x higher throughput compared to traditional narrow top-down step-by-step collection. We collect realistic data by performing any user requests within the entirety of 3 office buildings and using multiple embodiments (robot, human, human with grasping tool). With this data, we show that models trained on all embodiments perform better than ones trained on the robot data only, even when evaluated solely on robot episodes. We explore the economics of collection costs and find that for a fixed budget it is beneficial to take advantage of the cheaper human collection along with robot collection. We release a large and highly diverse (29,520 unique instructions) dataset dubbed RoboVQA containing 829,502 (video, text) pairs for robotics-focused visual question answering. We also demonstrate how evaluating real robot experiments with an intervention mechanism enables performing tasks to completion, making it deployable with human oversight even if imperfect while also providing a single performance metric. We demonstrate a single video-conditioned model named RoboVQA-VideoCoCa trained on our dataset that is capable of performing a variety of grounded high-level reasoning tasks in broad realistic settings with a cognitive intervention rate 46% lower than the zeroshot state of the art visual language model (VLM) baseline and is able to guide real robots through long-horizon tasks. The performance gap with zero-shot state-of-the-art models indicates that a lot of grounded data remains to be collected for real-world deployment, emphasizing the critical need for scalable data collection approaches. Finally, we show that video VLMs significantly outperform single-image VLMs with an average error rate reduction of 19% across all VQA tasks. Thanks to video conditioning and dataset diversity, the model can be used as general video value functions (e.g. success and affordance) in situations where actions needs to be recognized rather than states, expanding capabilities and environment understanding for robots. Data and videos are available at robovqa.github.io},
  archive   = {C_ICRA},
  author    = {Pierre Sermanet and Tianli Ding and Jeffrey Zhao and Fei Xia and Debidatta Dwibedi and Keerthana Gopalakrishnan and Christine Chan and Gabriel Dulac-Arnold and Sharath Maddineni and Nikhil J Joshi and Pete Florence and Wei Han and Robert Baruch and Yao Lu and Suvir Mirchandani and Peng Xu and Pannag Sanketi and Karol Hausman and Izhak Shafran and Brian Ichter and Yuan Cao},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610216},
  month     = {5},
  pages     = {645-652},
  title     = {RoboVQA: Multimodal long-horizon reasoning for robotics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TBD pedestrian data collection: Towards rich, portable, and
large-scale natural pedestrian data. <em>ICRA</em>, 637–644. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Social navigation and pedestrian behavior research has shifted towards machine learning-based methods and converged on the topic of modeling inter-pedestrian interactions and pedestrian-robot interactions. For this, large-scale datasets that contain rich information are needed. We describe a portable data collection system, coupled with a semi-autonomous labeling pipeline. As part of the pipeline, we designed a label correction web application that facilitates human verification of automated pedestrian tracking outcomes. Our system enables large-scale data collection in diverse environments and fast trajectory label production. Compared with existing pedestrian data collection methods, our system contains three components: a combination of top-down and ego-centric views, natural human behavior in the presence of a socially appropriate &quot;robot&quot;, and human-verified labels grounded in the metric space. To the best of our knowledge, no prior data collection system has a combination of all three components. We further introduce our ever-expanding dataset from the ongoing data collection effort – the TBD Pedestrian Dataset and show that our collected data is larger in scale, contains richer information when compared to prior datasets with human-verified labels, and supports new research opportunities.},
  archive   = {C_ICRA},
  author    = {Allan Wang and Daisuke Sato and Yasser Corzo and Sonya Simkin and Abhijat Biswas and Aaron Steinfeld},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610335},
  month     = {5},
  pages     = {637-644},
  title     = {TBD pedestrian data collection: Towards rich, portable, and large-scale natural pedestrian data},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards learning-based planning: The nuPlan benchmark for
real-world autonomous driving. <em>ICRA</em>, 629–636. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Machine Learning (ML) has replaced handcrafted methods for perception and prediction in autonomous vehicles. Yet for the equally important planning task, the adoption of ML-based techniques is slow. We present nuPlan, the world’s first real-world autonomous driving dataset and benchmark. The benchmark is designed to test the ability of ML-based planners to handle diverse driving situations and to make safe and efficient decisions. We introduce a new large-scale dataset that consists of 1282 hours of diverse driving scenarios from 4 cities (Las Vegas, Boston, Pittsburgh, and Singapore) and includes high-quality auto-labeled object tracks and traffic light data. We mine and taxonomize common &amp; rare driving scenarios which are used during evaluation to get fine-grained insights into the performance and characteristics of a planner. Beyond the dataset, we provide a simulation and evaluation framework that enables a planner’s actions to be simulated in closed-loop to account for interactions with other traffic participants. We present a detailed analysis of numerous baselines and investigate gaps between ML-based and traditional methods. Find the nuPlan dataset and code at nuplan.org.},
  archive   = {C_ICRA},
  author    = {Napat Karnchanachari and Dimitris Geromichalos and Kok Seang Tan and Nanxiang Li and Christopher Eriksen and Shakiba Yaghoubi and Noushin Mehdipour and Gianmarco Bernasconi and Whye Kit Fong and Yiluan Guo and Holger Caesar},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610077},
  month     = {5},
  pages     = {629-636},
  title     = {Towards learning-based planning: The nuPlan benchmark for real-world autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). InteRACT: Transformer models for human intent prediction
conditioned on robot actions. <em>ICRA</em>, 621–628. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In collaborative human-robot manipulation, a robot must predict human intents and adapt its actions accordingly to smoothly execute tasks. However, the human’s intent in turn depends on actions the robot takes, creating a chicken-or-egg problem. Prior methods ignore such inter-dependency and instead train marginal intent prediction models independent of robot actions. This is because training conditional models is hard given a lack of paired human-robot interaction datasets.Can we instead leverage large-scale human-human interaction data that is more easily accessible? Our key insight is to exploit a correspondence between human and robot actions that enables transfer learning from human-human to human-robot data. We propose a novel architecture, InteRACT, that pre-trains a conditional intent prediction model on large human-human datasets and fine-tunes on a small human-robot dataset. We evaluate on a set of real-world collaborative human-robot manipulation tasks and show that our conditional model improves over various marginal baselines. We also introduce new techniques to tele-operate a 7-DoF robot arm and collect a diverse range of human-robot collaborative manipulation data which we open-source. We release our code and datasets at https://portal-cornell.github.io/InteRACT/.},
  archive   = {C_ICRA},
  author    = {Kushal Kedia and Atiksh Bhardwaj and Prithwish Dan and Sanjiban Choudhury},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610681},
  month     = {5},
  pages     = {621-628},
  title     = {InteRACT: Transformer models for human intent prediction conditioned on robot actions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mobile robot oriented large-scale indoor dataset for dynamic
scene understanding. <em>ICRA</em>, 613–620. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most existing robotic datasets capture static scene data and thus are limited in evaluating robots’ dynamic performance. To address this, we present a mobile robot oriented large-scale indoor dataset, denoted as THUD (Tsinghua University Dynamic) robotic dataset, for training and evaluating their dynamic scene understanding algorithms. Specifically, the THUD dataset construction is first detailed, including organization, acquisition, and annotation methods. It comprises both real-world and synthetic data, collected with a real robot platform and a physical simulation platform, respectively. Our current dataset includes 13 larges-scale dynamic scenarios, 90K image frames, 20M 2D/3D bounding boxes of static and dynamic objects, camera poses, and IMU. The dataset is still continuously expanding. Then, the performance of mainstream indoor scene understanding tasks, e.g. 3D object detection, semantic segmentation, and robot relocalization, is evaluated on our THUD dataset. These experiments reveal serious challenges for some robot scene understanding tasks in dynamic scenes. By sharing this dataset, we aim to foster and iterate new mobile robot algorithms quickly for robot actual working dynamic environment, i.e. complex crowded dynamic scenes.},
  archive   = {C_ICRA},
  author    = {Yi-Fan Tang and Cong Tai and Fang-Xing Chen and Wan-Ting Zhang and Tao Zhang and Xue-Ping Liu and Yong-Jin Liu and Long Zeng},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611489},
  month     = {5},
  pages     = {613-620},
  title     = {Mobile robot oriented large-scale indoor dataset for dynamic scene understanding},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Robotic constrained imitation learning for the peg transfer
task in fundamentals of laparoscopic surgery. <em>ICRA</em>, 606–612.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this study, we present an implementation strategy for a robot that performs peg transfer tasks in Fundamentals of Laparoscopic Surgery (FLS) via imitation learning, aimed at the development of an autonomous robot for laparoscopic surgery. Robotic laparoscopic surgery presents two main challenges: (1) the need to manipulate forceps using ports established on the body surface as fulcrums, and (2) difficulty in perceiving depth information when working with a monocular camera that displays its images on a monitor. Especially, regarding issue (2), most prior research has assumed the availability of depth images or models of a target to be operated on. Therefore, in this study, we achieve more accurate imitation learning with only monocular images by extracting motion constraints from one exemplary motion of skilled operators, collecting data based on these constraints, and conducting imitation learning based on the collected data. We implemented an overall system using two Franka Emika Panda Robot Arms and validated its effectiveness.},
  archive   = {C_ICRA},
  author    = {Kento Kawaharazuka and Kei Okada and Masayuki Inaba},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610059},
  month     = {5},
  pages     = {606-612},
  title     = {Robotic constrained imitation learning for the peg transfer task in fundamentals of laparoscopic surgery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Adaptive whole-body robotic tool-use learning on
low-rigidity plastic-made humanoids using vision and tactile sensors.
<em>ICRA</em>, 583–589. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Various robots have been developed so far; however, we face challenges in modeling the low-rigidity bodies of some robots. In particular, the deflection of the body changes during tool-use due to object grasping, resulting in significant shifts in the tool-tip position and the body’s center of gravity. Moreover, this deflection varies depending on the weight and length of the tool, making these models exceptionally complex. However, there is currently no control or learning method that takes all of these effects into account. In this study, we propose a method for constructing a neural network that describes the mutual relationship among joint angle, visual information, and tactile information from the feet. We aim to train this network using the actual robot data and utilize it for tool-tip control. Additionally, we employ Parametric Bias to capture changes in this mutual relationship caused by variations in the weight and length of tools, enabling us to understand the characteristics of the grasped tool from the current sensor information. We apply this approach to the whole-body tool-use on KXR, a low-rigidity plastic-made humanoid robot, to validate its effectiveness.},
  archive   = {C_ICRA},
  author    = {Kento Kawaharazuka and Kei Okada and Masayuki Inaba},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610913},
  month     = {5},
  pages     = {583-589},
  title     = {Adaptive whole-body robotic tool-use learning on low-rigidity plastic-made humanoids using vision and tactile sensors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GG-LLM: Geometrically grounding large language models for
zero-shot human activity forecasting in human-aware task planning.
<em>ICRA</em>, 568–574. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A robot in a human-centric environment needs to account for the human’s intent and future motion in its task and motion planning to ensure safe and effective operation. This requires symbolic reasoning about probable future actions and the ability to tie these actions to specific locations in the physical environment. While one can train behavioral models capable of predicting human motion from past activities, this approach requires large amounts of data to achieve acceptable long-horizon predictions. More importantly, the resulting models are constrained to specific data formats and modalities. Moreover, connecting predictions from such models to the environment at hand to ensure the applicability of these predictions is an unsolved problem. We present a system that utilizes a Large Language Model (LLM) to infer a human’s next actions from a range of modalities without fine-tuning. A novel aspect of our system that is critical to robotics applications is that it links the predicted actions to specific locations in a semantic map of the environment. Our method leverages the fact that LLMs, trained on a vast corpus of text describing typical human behaviors, encode substantial world knowledge, including probable sequences of human actions and activities. We demonstrate how these localized activity predictions can be incorporated in a human-aware task planner for an assistive robot to reduce the occurrences of undesirable human-robot interactions by 29.2% on average.},
  archive   = {C_ICRA},
  author    = {Moritz A. Graule and Volkan Isler},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611090},
  month     = {5},
  pages     = {568-574},
  title     = {GG-LLM: Geometrically grounding large language models for zero-shot human activity forecasting in human-aware task planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Synthesize efficient safety certificates for learning-based
safe control using magnitude regularization. <em>ICRA</em>, 545–551. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safety certificates based on energy functions can provide demonstrable safety for complex robotic systems. However, all recent studies on learning-based energy function synthesis only consider the feasibility of the control policy, which might cause over-conservativeness and even fail to achieve the control goal. To solve the problem of over-conservative controllers, we proposed the magnitude regularization technique to improve the controller performance of safe controllers by reducing the conservativeness inside the energy function, while keeping the promising provable safety guarantees. Specifically, we quantify the conservativeness by the magnitude of the energy function, and we reduce the conservativeness by adding a magnitude regularization term to the synthesis loss. We propose an algorithm using reinforcement learning (RL) for synthesis to unify the learning process of safe controllers and energy functions. We conducted simulation experiments on Safety Gym and real-robot experiments using small quadrotors. Simulation results show that the proposed algorithm does reduce the conservativeness of the energy function and outperforms baselines in terms of controller performance while maintaining safety. Real-robot experiments have shown that the proposed algorithm indeed reduce conservativeness on the small quadrotors.},
  archive   = {C_ICRA},
  author    = {Haotian Zheng and Haitong Ma and Sifa Zheng and Shengbo Eben Li and Jianqiang Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610959},
  month     = {5},
  pages     = {545-551},
  title     = {Synthesize efficient safety certificates for learning-based safe control using magnitude regularization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LOTUS: Continual imitation learning for robot manipulation
through unsupervised skill discovery. <em>ICRA</em>, 537–544. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce LOTUS, a continual imitation learning algorithm that empowers a physical robot to continuously and efficiently learn to solve new manipulation tasks throughout its lifespan. The core idea behind LOTUS is constructing an ever-growing skill library from a sequence of new tasks with a small number of human demonstrations. LOTUS starts with a continual skill discovery process using an open-vocabulary vision model, which extracts skills as recurring patterns presented in unsegmented demonstrations. Continual skill discovery updates existing skills to avoid catastrophic forgetting of previous tasks and adds new skills to solve novel tasks. LOTUS trains a meta-controller that flexibly composes various skills to tackle vision-based manipulation tasks in the lifelong learning process. Our comprehensive experiments show that LOTUS outperforms state-of-the-art baselines by over 11% in success rate, showing its superior knowledge transfer ability compared to prior methods. More results and videos can be found on the project website: https://ut-austin-rpl.github.io/Lotus/.},
  archive   = {C_ICRA},
  author    = {Weikang Wan and Yifeng Zhu and Rutav Shah and Yuke Zhu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611129},
  month     = {5},
  pages     = {537-544},
  title     = {LOTUS: Continual imitation learning for robot manipulation through unsupervised skill discovery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Probabilistic spiking neural network for robotic tactile
continual learning. <em>ICRA</em>, 530–536. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The sense of touch is essential for robots to perform various daily tasks. Artificial Neural Networks have shown significant promise in advancing robotic tactile learning. However, due to the changing of tactile data distribution as robots encounter new tasks, ANN-based robotic tactile learning suffers from catastrophic forgetting. To solve this problem, we introduce a novel continual learning (CL) framework called the Probabilistic Spiking Neural Network with Variational Continual Learning (PSNN-VCL). In this framework, PSNN introduces uncertainty during spike emission and can apply fast Variational Inference by optimizing the uncertainty through backpropagation, which significantly reduces the required model parameters for VCL. We establish a robotic tactile CL benchmark using publicly available datasets to evaluate our method. Experimental results demonstrated that, compared to other CL methods, PSNN-VCL not only achieves superior performance in terms of widely used CL metrics but also achieves at least a 50% reduction in model parameters on the robotic tactile CL benchmark.},
  archive   = {C_ICRA},
  author    = {Senlin Fang and Yiwen Liu and Chengliang Liu and Jingnan Wang and Yuanzhe Su and Yupo Zhang and Hoiio Kong and Zhengkun Yi and Xinyu Wu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610553},
  month     = {5},
  pages     = {530-536},
  title     = {Probabilistic spiking neural network for robotic tactile continual learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lifelong robot learning with human assisted language
planners. <em>ICRA</em>, 523–529. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large Language Models (LLMs) have been shown to act like planners that can decompose high-level instructions into a sequence of executable instructions. However, current LLM-based planners are only able to operate with a fixed set of skills. We overcome this critical limitation and present a method for using LLM-based planners to query new skills and teach robots these skills in a data and time-efficient manner for rigid object manipulation. Our system can re-use newly acquired skills for future tasks, demonstrating the potential of open world and lifelong learning. We evaluate the proposed framework on multiple tasks in simulation and the real world. Videos are available at: https://sites.google.com/mit.edu/halp-robot-learning},
  archive   = {C_ICRA},
  author    = {Meenal Parakh and Alisha Fong and Anthony Simeonov and Tao Chen and Abhishek Gupta and Pulkit Agrawal},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610225},
  month     = {5},
  pages     = {523-529},
  title     = {Lifelong robot learning with human assisted language planners},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lifelong robot library learning: Bootstrapping composable
and generalizable skills for embodied control with language models.
<em>ICRA</em>, 515–522. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large Language Models (LLMs) have emerged as a new paradigm for embodied reasoning and control, most recently by generating robot policy code that utilizes a custom library of vision and control primitive skills. However, prior arts fix their skills library and steer the LLM with carefully handcrafted prompt engineering, limiting the agent to a stationary range of addressable tasks. In this work, we introduce LRLL, an LLM-based lifelong learning agent that continuously grows the robot skill library to tackle manipulation tasks of ever-growing complexity. LRLL achieves this with four novel contributions: 1) a soft memory module that allows dynamic storage and retrieval of past experiences to serve as context, 2) a self-guided exploration policy that proposes new tasks in simulation, 3) a skill abstractor that distills recent experiences into new library skills, and 4) a lifelong learning algorithm for enabling human users to bootstrap new skills with minimal online interaction. LRLL continuously transfers knowledge from the memory to the library, building composable, general and interpretable policies, while bypassing gradient-based optimization, thus relieving the learner from catastrophic forgetting. Empirical evaluation in a simulated tabletop environment shows that LRLL outperforms end-to-end and vanilla LLM approaches in the lifelong setup while learning skills that are transferable to the real world. Project material will become available at the webpage https://gtziafas.github.io/LRLL_project/.},
  archive   = {C_ICRA},
  author    = {Georgios Tziafas and Hamidreza Kasaei},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611448},
  month     = {5},
  pages     = {515-522},
  title     = {Lifelong robot library learning: Bootstrapping composable and generalizable skills for embodied control with language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adapting to the “open world”: The utility of hybrid
hierarchical reinforcement learning and symbolic planning.
<em>ICRA</em>, 508–514. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Open-world robotic tasks such as autonomous driving pose significant challenges to robot control due to unknown and unpredictable events that disrupt task performance. Neural network-based reinforcement learning (RL) techniques (like DQN, PPO, SAC, etc.) struggle to adapt in large domains and suffer from catastrophic forgetting. Hybrid planning and RL approaches have shown some promise in handling environmental changes but lack efficiency in accommodation speed. To address this limitation, we propose an enhanced hybrid system with a nested hierarchical action abstraction that can utilize previously acquired skills to effectively tackle unexpected novelties. We show that it can adapt faster and generalize better compared to state-of-the-art RL and hybrid approaches, significantly improving robustness when multiple environmental changes occur at the same time.},
  archive   = {C_ICRA},
  author    = {Pierrick Lorang and Helmut Horvath and Tobias Kietreiber and Patrik Zips and Clemens Heitzinger and Matthias Scheutz},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611594},
  month     = {5},
  pages     = {508-514},
  title     = {Adapting to the “Open world”: The utility of hybrid hierarchical reinforcement learning and symbolic planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Experience consistency distillation continual reinforcement
learning for robotic manipulation tasks. <em>ICRA</em>, 501–507. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Continual reinforcement learning, which aims to help robots acquire skills without catastrophic forgetting, obviating the need to re-learn all tasks from scratch. In order to enable lifelong acquisition of skills in robots, replay-based continual reinforcement learning has emerged as a promising research direction. These techniques replay data from previous tasks to mitigate forgetting when learning new skills. However, existing replay-based methods store poor representative experience, and the experience utilization of old tasks is inefficient. To address these issues, we propose an experience consistency distillation method for robot continual reinforcement learning to improve the data efficiency of the experience. Specifically, the experience of old tasks are distilled to obtain Markov Decision Process (MDP) data with high compression ratio and information content. To ensure consistent data distributions before and after distillation, we further utilize a Fréchet Inception Distance (FID) loss as a regularization constraint. In order to improve experience utilization efficiency, the policy is then trained using both the distilled data and current task data, with policy distillation performed based on uncertainty metrics. Our method is validated in the continual reinforcement learning simulation platform and real scene with a UR5e robot arm. Experimental results indicate that our method achieves higher success and lower buffer size requirement compared to other methods.},
  archive   = {C_ICRA},
  author    = {Chao Zhao and Jie Xu and Ru Peng and Xingyu Chen and Kuizhi Mei and Xuguang Lan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611494},
  month     = {5},
  pages     = {501-507},
  title     = {Experience consistency distillation continual reinforcement learning for robotic manipulation tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VERSE: Virtual-gradient aware streaming lifelong learning
with anytime inference. <em>ICRA</em>, 493–500. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Lifelong learning or continual learning is the problem of training an AI agent continuously while also preventing it from forgetting its previously acquired knowledge. Streaming lifelong learning is a challenging setting of lifelong learning with the goal of continuous learning in a dynamic non-stationary environment without forgetting. We introduce a novel approach to lifelong learning, which is streaming (observes each training example only once), requires a single pass over the data, can learn in a class-incremental manner, and can be evaluated on-the-fly (anytime inference). To accomplish these, we propose a novel virtual gradients based approach for continual representation learning which adapts to each new example while also generalizing well on past data to prevent catastrophic forgetting. Our approach also leverages an exponential-moving-average-based semantic memory to further enhance performance. Experiments on diverse datasets with temporally correlated observations demonstrate our method’s efficacy and superior performance over existing methods.},
  archive   = {C_ICRA},
  author    = {Soumya Banerjee and Vinay K. Verma and Avideep Mukherjee and Deepak Gupta and Vinay P. Namboodiri and Piyush Rai},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610702},
  month     = {5},
  pages     = {493-500},
  title     = {VERSE: Virtual-gradient aware streaming lifelong learning with anytime inference},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ZS6D: Zero-shot 6D object pose estimation using vision
transformers. <em>ICRA</em>, 463–469. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As robotic systems increasingly encounter complex and unconstrained real-world scenarios, there is a demand to recognize diverse objects. The state-of-the-art 6D object pose estimation methods rely on object-specific training and therefore do not generalize to unseen objects. Recent novel object pose estimation methods are solving this issue using task-specific fine-tuned CNNs for deep template matching. This adaptation for pose estimation still requires expensive data rendering and training procedures. MegaPose for example is trained on a dataset consisting of two million images showing 20,000 different objects to reach such generalization capabilities. To overcome this shortcoming we introduce ZS6D, for zero-shot novel object 6D pose estimation. Visual descriptors, extracted using pre-trained Vision Transformers (ViT), are used for matching rendered templates against query images of objects and for establishing local correspondences. These local correspondences enable deriving geometric correspondences and are used for estimating the object&#39;s 6D pose with RANSAC- based PnP. This approach showcases that the image descriptors extracted by pre-trained ViTs are well-suited to achieve a notable improvement over two state-of-the-art novel object 6D pose estimation methods, without the need for task-specific fine-tuning. Experiments are performed on LMO, YCBV, and TLESS. In comparison to MegaPose, we improve the Average Recall on all three datasets and compared to OSOP we improve on two datasets. The code is available at https://github.com/PhilippAuss/ZS6D.},
  archive   = {C_ICRA},
  author    = {Philipp Ausserlechner and David Haberger and Stefan Thalhammer and Jean-Baptiste Weibel and Markus Vincze},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611464},
  month     = {5},
  pages     = {463-469},
  title     = {ZS6D: Zero-shot 6D object pose estimation using vision transformers},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Joint response and background learning for UAV visual
tracking. <em>ICRA</em>, 455–462. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Correlation filter (CF)-based approaches have gained widespread attention in the field of unmanned aerial vehicle (UAV) visual tracking due to their light-weight characteristics. However, CFs are prone to generating low-quality response in challenging UAV scenarios, e.g., fast motion and background clutter. In this paper, in order to model the tracker more robustly, we first conduct an effective regularization analysis from the perspectives of response- and background-learning. Specifically, to address response degradation, we propose a module for learning temporal consistency and reversibility of response, supplemented by a novel background-aware module to enhance the ability to learn from negative samples. In addition, we propose a fast coarse-to-fine scale search strategy, which alleviates the challenges in estimating bounding boxes under non-uniform aspect ratios. We have developed two tracker versions, namely RBLT and DeepRBLT, based on the depth of the features. Comprehensive experiments on four UAV benchmarks and one generic benchmark have indicated the superiority of our trackers compared to other state-of-the-art trackers, with enough speed for real-time applications.},
  archive   = {C_ICRA},
  author    = {Biao Wang and Wenling Li and Bin Zhang and Yang Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611308},
  month     = {5},
  pages     = {455-462},
  title     = {Joint response and background learning for UAV visual tracking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LPS-net: Lightweight parameter-shared network for point
cloud-based place recognition. <em>ICRA</em>, 448–454. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With innovation in fields such as autonomous driving and augmented reality, point cloud-based place recognition has gained significant attention. Many methods try to address this problem by extracting and matching global descriptors in a database, but they often must balance the extraction of comprehensive contextual information and large model sizes. To overcome this challenge, we propose a lightweight parameter-shared network (LPS-Net), which includes multiple bidirectional perception units (BPUs) to extract multiscale long-range contextual information and parameter-shared NetVLADs (PS-VLADs) to aggregate descriptors. A BPU includes a parameter-shared convolution module (SharedConv) that significantly compresses the model and enhances its ability to capture informative features. In PS-VLADs, we replace half the parameters used in the original NetVLAD with trainable scalars, which further reduces the model size, and theoretically prove their equivalence. Experimental results demonstrate that LPS-Net achieves state-of-the-art performance at the task of point cloud-based place recognition while maintaining a small model size. Code and supplementary materials can be found at https://github.com/Yavinr/LPS-Net.},
  archive   = {C_ICRA},
  author    = {Chengxin Liu and Guiyou Chen and Ran Song},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610758},
  month     = {5},
  pages     = {448-454},
  title     = {LPS-net: Lightweight parameter-shared network for point cloud-based place recognition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incorporating scene graphs into pre-trained vision-language
models for multimodal open-vocabulary action recognition. <em>ICRA</em>,
440–447. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents Action-SGFA, a novel action feature alignment approach to learn unified joint embeddings across four action modalities incorporating scene graph (SG) comprehension. A new training paradigm for Action-SGFA is also devised to improve pre-trained VL models using datasets with SG annotation. When learning from image-SG pairs, it captures structure-associated action knowledge for visual and textual encoders. SG supervision generates fine-grained captions based on various graph augmentations highlighting different compositional aspects of action scenes. Furthermore, our research reveals that all combinations of paired data are unnecessary to train such unified embeddings, and only image-paired data is sufficient to bind all action modalities together. Our Action-SGFA can leverage existing large VL models, enhancing their zero-shot capabilities of new modalities due to their natural pairings with images. The open-vocabulary zero-shot performance improves with the strength of the pre-trained VL model and the SG comprehension. We establish a new state-of-the-art in several zero-shot action recognition tasks across modalities, significantly surpassing the vanilla skeleton zero-shot method by 27.0% and 19.7% on NTU-60 and NTU-120, respectively. Additionally, in the context of RGB videos, we surpass the state-of-the-art method on Kinetics-400 by 2.1%.},
  archive   = {C_ICRA},
  author    = {Chao Wei and Zhidong Deng},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611454},
  month     = {5},
  pages     = {440-447},
  title     = {Incorporating scene graphs into pre-trained vision-language models for multimodal open-vocabulary action recognition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continuous adaptation in person re-identification for
robotic assistance. <em>ICRA</em>, 425–431. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In scenarios of Human-Robot Interaction (HRI), it is often assumed that the robot should cooperate with the closest individual or that only one person is present. However, in real-life situations, such as shop floor operations, this assumption may not hold. Thus, it becomes necessary for a robot to recognize a specific target in a crowded environment. To address this problem, we propose a person re-identification module that uses continuous visual adaptation techniques. This module ensures that the robot can seamlessly cooperate with the appropriate individual despite its appearance changes or partial or total occlusions. We used both a laboratory environment and an HRI scenario where the robot followed a person to test our framework. During the test, the targets were asked to change their appearance and disappear from the camera’s field of view to test the module’s ability to handle challenging cases of occlusion and outfit variations. We compared our framework with a state-of-the-art Multi-Object Tracking (MOT) method, and the results showed that our module, shortly named CARPE-ID, accurately tracked each selected target throughout the experiments in all cases except for two cases. In contrast, the MOT had an average of 4 tracking errors for each video.},
  archive   = {C_ICRA},
  author    = {Federico Rollo and Andrea Zunino and Nikolaos Tsagarakis and Enrico Mingo Hoffman and Arash Ajoudani},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611226},
  month     = {5},
  pages     = {425-431},
  title     = {Continuous adaptation in person re-identification for robotic assistance},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Masked local-global representation learning for 3D point
cloud domain adaptation. <em>ICRA</em>, 418–424. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Point cloud is a popular and widely used geometric representation, which has attracted significant attention in 3D vision. However, the geometric variability of point cloud representations across different datasets can cause domain discrepancies, which hinder knowledge transfer and model generalization, resulting in degraded performance in target domain. In this paper, we present a novel approach to improve point cloud domain adaptation by employing masked representation learning in a self-supervised manner. Specifically, our method combines masked feature prediction and masked sample consistency to encode both local structure and global semantic information for learning invariant point cloud representation across domains. Moreover, to learn domain-specific representation and transfer knowledge from source to target, we propose prototype-calibrated self-training. By exploiting class-wise prototypes in the shared feature space, the soft pseudo labels can be adaptively denoised, which benefits the decision boundary learning in target domain. We conduct experiments on PointDA-10 and PointSegDA for 3D point cloud shape classification and semantic segmentation, respectively. The results demonstrate the effectiveness of our method and show that we can achieve the new state-of-the-art performance on point cloud domain adaptation.},
  archive   = {C_ICRA},
  author    = {Bowei Xing and Xianghua Ying and Ruibin Wang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611402},
  month     = {5},
  pages     = {418-424},
  title     = {Masked local-global representation learning for 3D point cloud domain adaptation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Diving into the depths of spotting text in multi-domain
noisy scenes. <em>ICRA</em>, 410–417. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When used in a real-world noisy environment, the capacity to generalize to multiple domains is essential for any autonomous scene text spotting system. However, existing state-of-the-art methods employ pretraining and fine-tuning strategies on natural scene datasets, which do not exploit the feature interaction across other complex domains. In this work, we explore and investigate the problem of domain-agnostic scene text spotting, i.e., training a model on multi-domain source data such that it can directly generalize to target domains rather than being specialized for a specific domain or scenario. In this regard, we present the community a text spotting validation benchmark called Under-Water Text (UWT) for noisy underwater scenes to establish an important case study. Moreover, we also design an efficient super-resolution based end-to-end transformer baseline called DA-TextSpotter which achieves comparable or superior performance over existing text spotting architectures for both regular and arbitrary-shaped scene text spotting benchmarks in terms of both accuracy and model efficiency. The dataset, code and pre-trained models have been released in our Github.},
  archive   = {C_ICRA},
  author    = {Alloy Das and Sanket Biswas and Umapada Pal and Josep Lladós},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611120},
  month     = {5},
  pages     = {410-417},
  title     = {Diving into the depths of spotting text in multi-domain noisy scenes},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mobile bot rotation using sound source localization and
distant speech recognition. <em>ICRA</em>, 403–409. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the last few years, mobile robots such as floor cleaners, assistive robots, and home telepresence have become an essential part of our day-to-day activities. In human-robot interaction, speech is the preferred way of communication, especially in indoor environments. This paper proposes a speech module to rotate the mobile robot. It has two components, namely, a distant automatic speech recognizer and a sound source localizer. To build distant speech recognizer, far-field speech data is collected at 1, 3, and 5-meters distances. The model performs well even at a 5-meters distance with a Word Error Rate of 40.38% and a Character Error Rate of 28.85%. The direction of arrival of the speech signal is computed from the 4-mic circular array microphone. The speech module is integrated with the Robot Operating System and physically demonstrated on Turtlebot3 Waffle Pi. It is observed that the speech recognizer and sound source localizer work well in the reverberant indoor environment with a small single-board computer.},
  archive   = {C_ICRA},
  author    = {Swapnil Sontakke and Pradyoth Hegde and Prashant Bannulmath and Deepak K T},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610539},
  month     = {5},
  pages     = {403-409},
  title     = {Mobile bot rotation using sound source localization and distant speech recognition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MagicTac: A novel high-resolution 3D multi-layer grid-based
tactile sensor. <em>ICRA</em>, 388–394. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate robotic control over interactions with the environment is fundamentally grounded in understanding tactile contacts. In this paper, we introduce MagicTac, a novel high-resolution grid-based tactile sensor. This sensor employs a 3D multi-layer grid-based design, inspired by the Magic Cube structure. This structure can help increase the spatial resolution of MagicTac to perceive external interaction contacts. Moreover, the sensor is produced using the multi-material additive manufacturing technique, which simplifies the manufacturing process while ensuring repeatability of production. Compared to traditional vision-based tactile sensors, it offers the advantages of i) high spatial resolution, ii) significant affordability, and iii) fabrication-friendly construction that requires minimal assembly skills. We evaluated the proposed MagicTac in the tactile reconstruction task using the deformation field and optical flow. Results indicated that MagicTac could capture fine textures and is sensitive to dynamic contact information. Through the grid-based multi-material additive manufacturing technique, the affordability and productivity of MagicTac can be enhanced with a minimum manufacturing cost of £4.76 and a minimum manufacturing time of 24.6 minutes.},
  archive   = {C_ICRA},
  author    = {Wen Fan and Haoran Li and Dandan Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610615},
  month     = {5},
  pages     = {388-394},
  title     = {MagicTac: A novel high-resolution 3D multi-layer grid-based tactile sensor},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An image acquisition scheme for visual odometry based on
image bracketing and online attribute control. <em>ICRA</em>, 381–387.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual odometry (VO) system is challenged by complex illumination environments. Image quality and its consistency in the time domain directly determine feature detection and tracking performance, which further affect the robustness and accuracy of the entire system. In this paper, an image acquisition scheme with image bracketing patterns is proposed. Images with different exposure levels are continuously captured to sufficiently explore the scene under varying illumination. An attribute control method is designed to adjust image exposures within the brackets online. Gaussian process regression fits the relationship between image quality metric and exposure via image synthesis technique. The optimal exposures for the next bracket are obtained directly without attempts to ensure a quick response. Experiments show our acquisition system’s effectiveness and performance improvement for VO tasks in complex illumination scenes.},
  archive   = {C_ICRA},
  author    = {Shuyang Zhang and Jinhao He and Bohuan Xue and Jin Wu and Pengyu Yin and Jianhao Jiao and Ming Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611141},
  month     = {5},
  pages     = {381-387},
  title     = {An image acquisition scheme for visual odometry based on image bracketing and online attribute control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Point cloud-based control barrier function regression for
safe and efficient vision-based control. <em>ICRA</em>, 366–372. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Control barrier functions have become an increasingly popular framework for safe real-time control. In this work, we present a computationally low-cost framework for synthesizing barrier functions over point cloud data for safe vision-based control. We take advantage of surface geometry to locally define and synthesize a quadratic CBF over a point cloud. This CBF is used in a CBF-QP for control and verified in simulation on quadrotors and in hardware on quadrotors and the TurtleBot3. This technique enables safe navigation through unstructured and dynamically changing environments and is shown to be significantly more efficient than current methods.},
  archive   = {C_ICRA},
  author    = {Massimiliano De Sa and Prasanth Kotaru and Koushil Sreenath},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610647},
  month     = {5},
  pages     = {366-372},
  title     = {Point cloud-based control barrier function regression for safe and efficient vision-based control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient gesture recognition on spiking convolutional
networks through sensor fusion of event-based and depth data.
<em>ICRA</em>, 345–352. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As intelligent systems become increasingly important in our daily lives, new ways of interaction are needed. Classical user interfaces pose issues for the physically impaired and are partially not practical or convenient. Gesture recognition is an alternative, but often not reactive enough when conventional cameras are used. This work proposes a Spiking Convolutional Neural Network, processing event- and depth data for gesture recognition. The network is simulated using the open-source neuromorphic computing framework LAVA for offline training and evaluation on an embedded system. For the evaluation three open source data sets are used. Since these do not represent the applied bi-modality, a new data set with synchronized event- and depth data was recorded. The results show the viability of temporal encoding on depth information and modality fusion, even on differently encoded data, to be beneficial to network performance and generalization capabilities.},
  archive   = {C_ICRA},
  author    = {Lea Steffen and Thomas Trapp and Arne Roennau and Rüdiger Dillmann},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610824},
  month     = {5},
  pages     = {345-352},
  title     = {Efficient gesture recognition on spiking convolutional networks through sensor fusion of event-based and depth data},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-modal 3D human tracking for robots in complex
environment with siamese point-video transformer. <em>ICRA</em>,
337–344. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tracking a specific person in 3D scene is gaining momentum due to its numerous applications in robotics. Currently, most 3D trackers focus on driving scenarios with neglected jitter and uncomplicated surroundings, which results in their severe degeneration in complex environments, especially on jolting robot platforms (only 20-60% success rate). To improve the accuracy, a Point-Video-based Transformer Tracking model (PVTrack) is presented for robots. It is the first multi-modal 3D human tracking work that incorporates point clouds together with RGB videos to achieve information complementarity. Moreover, PVTrack proposes the Siamese Point-Video Transformer for feature aggregation to overcome dynamic environments, which captures more target-aware information through the hierarchical attention mechanism adaptively. Considering the violent shaking on robots and rugged terrains, a lateral Human-ware Proposal Network is designed together with an Anti-shake Proposal Compensation module. It alleviates the disturbance caused by complex scenes as well as the particularity of the robot platform. Experiments show that our method achieves state-of-the-art performance on both KITTI/Waymo datasets and a quadruped robot for various indoor and outdoor scenes.},
  archive   = {C_ICRA},
  author    = {Shuo Xin and Zhen Zhang and Mengmeng Wang and Xiaojun Hou and Yaowei Guo and Xiao Kang and Liang Liu and Yong Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610979},
  month     = {5},
  pages     = {337-344},
  title     = {Multi-modal 3D human tracking for robots in complex environment with siamese point-video transformer},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stimulate the potential of robots via competition.
<em>ICRA</em>, 322–328. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It is common for us to feel pressure in a competition environment, which arises from the desire to obtain success comparing with other individuals or opponents. Although we might get anxious under the pressure, it could also be a drive for us to stimulate our potentials to the best in order to keep up with others. Inspired by this, we propose a competitive learning framework which is able to help individual robot to acquire knowledge from the competition, fully stimulating its dynamics potential in the race. Specifically, the competition information among competitors is introduced as the additional auxiliary signal to learn advantaged actions. We further build a Multiagent-Race environment, and extensive experiments are conducted, demonstrating that robots trained in competitive environments outperform ones that are trained with SoTA algorithms in single robot environment.},
  archive   = {C_ICRA},
  author    = {Kangyao Huang and Di Guo and Xinyu Zhang and Xiangyang Ji and Huaping Liu},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611581},
  month     = {5},
  pages     = {322-328},
  title     = {Stimulate the potential of robots via competition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-level action tree rollout (MLAT-r): Efficient and
accurate online multiagent policy improvement. <em>ICRA</em>, 315–321.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10610888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Rollout algorithms are renowned for their abilities to correct for the suboptimalities of offline-trained base policies. In the multiagent setting, performing online rollout can require an exponentially large number of optimizations with respect to the number of agents. One-agent-at-a-time algorithms offer computationally efficient approaches to guaranteed policy improvement; however, this improvement is with respect to a state value estimate derived from a potentially poor base policy. Monte Carlo tree search (MCTS) provably converges to the true state value estimates; however, the exponentially large search space often makes its online use limited. Here, we present the Multi-Level Action Tree Rollout (MLAT-R) algorithm. MLAT-R provides 1) provable improvement over a base policy, 2) policy improvement with respect to the true state value, 3) applicability to any number of agents, and 4) an action space that grows linearly with the number of agents rather than exponentially. In this paper, we outline the algorithm, sketch a proof of its improvement over a base policy, and evaluate its performance on a challenging problem for which the base policy cannot reach a terminal state. Despite the challenging experimental setup, our algorithm reached a terminal state in 86% of all experiments, compared to 31% for state-of-the-art one-agent-at-a-time algorithms. In experiments involving MCTS, MLAT-R reached a terminal state in 99% of experiments compared to 92% for MCTS. MLAT-R achieved these results while considering an exponentially smaller action space than MCTS.},
  archive   = {C_ICRA},
  author    = {Andrea Henshall and Sertac Karaman},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610888},
  month     = {5},
  pages     = {315-321},
  title     = {Multi-level action tree rollout (MLAT-r): Efficient and accurate online multiagent policy improvement},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Collision avoidance and navigation for a quadrotor swarm
using end-to-end deep reinforcement learning. <em>ICRA</em>, 300–306.
(<a href="https://doi.org/10.1109/ICRA57147.2024.10611499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {End-to-end deep reinforcement learning (DRL) for quadrotor control promises many benefits – easy deployment, task generalization and real-time execution capability. Prior end-to-end DRL-based methods have showcased the ability to deploy learned controllers onto single quadrotors or quadrotor teams maneuvering in simple, obstacle-free environments. However, the addition of obstacles increases the number of possible interactions exponentially, thereby increasing the difficulty of training RL policies. In this work, we propose an end-to-end DRL approach to control quadrotor swarms in environments with obstacles. We provide our agents a curriculum and a replay buffer of the clipped collision episodes to improve performance in obstacle-rich environments. We implement an attention mechanism to attend to the neighbor robots and obstacle interactions - the first successful demonstration of this mechanism on policies for swarm behavior deployed on severely compute-constrained hardware. Our work is the first work that demonstrates the possibility of learning neighbor-avoiding and obstacle-avoiding control policies trained with end-to-end DRL that transfers zero-shot to real quadrotors. Our approach scales to 32 robots with 80% obstacle density in simulation and 8 robots with 20% obstacle density in physical deployment. Website: https://sites.google.com/view/obst-avoid-swarm-rl},
  archive   = {C_ICRA},
  author    = {Zhehui Huang and Zhaojing Yang and Rahul Krupani and Baskın Şenbaşlar and Sumeet Batra and Gaurav S. Sukhatme},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611499},
  month     = {5},
  pages     = {300-306},
  title     = {Collision avoidance and navigation for a quadrotor swarm using end-to-end deep reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RoCo: Dialectic multi-robot collaboration with large
language models. <em>ICRA</em>, 286–299. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel approach to multi-robot collaboration that harnesses the power of pre-trained large language models (LLMs) for both high-level communication and low-level path planning. Robots are equipped with LLMs to discuss and collectively reason task strategies. They generate sub-task plans and task space waypoint paths, which are used by a multi-arm motion planner to accelerate trajectory planning. We also provide feedback from the environment, such as collision checking, and prompt the LLM agents to improve their plan and waypoints in-context. For evaluation, we introduce RoCoBench, a 6-task benchmark covering a wide range of multi-robot collaboration scenarios, accompanied by a text-only dataset that evaluates LLMs’ agent representation and reasoning capability. We experimentally demonstrate the effectiveness of our approach — it achieves high success rates across all tasks in RoCoBench and adapts to variations in task semantics. Our dialog setup offers high interpretability and flexibility — in real world experiments, we show RoCo easily incorporates human-in-the-loop, where a user can communicate and collaborate with a robot agent to complete tasks together.},
  archive   = {C_ICRA},
  author    = {Zhao Mandi and Shreeya Jain and Shuran Song},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610855},
  month     = {5},
  pages     = {286-299},
  title     = {RoCo: Dialectic multi-robot collaboration with large language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VO-safe reinforcement learning for drone navigation.
<em>ICRA</em>, 279–285. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work is focused on reinforcement learning (RL)-based navigation for drones, whose localisation is based on visual odometry (VO). Such drones should avoid flying into areas with poor visual features, as this can lead to deteriorated localization or complete loss of tracking. To achieve this, we propose a hierarchical control scheme, which uses an RL-trained policy as the high-level controller to generate waypoints for the next control step and a low-level controller to guide the drone to reach subsequent waypoints. For the high-level policy training, unlike other RL-based navigation approaches, we incorporate awareness of VO performance into our policy by introducing pose estimation-related punishment. To aid robots in distinguishing between perception-friendly areas and unfavoured zones, we instead provide semantic scenes, as input for decision-making instead of raw images. This approach also helps minimise the sim-to-real application gap.},
  archive   = {C_ICRA},
  author    = {Feiqiang Lin and Changyun Wei and Raphael Grech and Ze Ji},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611487},
  month     = {5},
  pages     = {279-285},
  title     = {VO-safe reinforcement learning for drone navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Phasic diversity optimization for population-based
reinforcement learning. <em>ICRA</em>, 272–278. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reviewing the previous work of diversity Reinforcement Learning, diversity is often obtained via an augmented loss function, which requires a balance between reward and diversity. Generally, diversity optimization algorithms use Multi-armed Bandits algorithms to select the coefficient in the pre-defined space. However, the dynamic distribution of reward signals for MABs or the conflict between quality and diversity limits the performance of these methods. We introduce the Phasic Diversity Optimization (PDO) algorithm, a Population-Based Training framework that separates reward and diversity training into distinct phases instead of optimizing a multi-objective function. In the auxiliary phase, agents with poor performance diversified via determinants will not replace the better agents in the archive. The decoupling of reward and diversity allows us to use an aggressive diversity optimization in the auxiliary phase without performance degradation. Furthermore, we construct a dogfight scenario for aerial agents to demonstrate the practicality of the PDO algorithm. We introduce two implementations of PDO archive and conduct tests in the newly proposed adversarial dogfight and MuJoCo simulations. The results show that our proposed algorithm achieves better performance than baselines.},
  archive   = {C_ICRA},
  author    = {Jingcheng Jiang and Haiyin Piao and Yu Fu and Yihang Hao and Chuanlu Jiang and Ziqi Wei and Xin Yang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610814},
  month     = {5},
  pages     = {272-278},
  title     = {Phasic diversity optimization for population-based reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An iterative approach for heterogeneous multi-agent route
planning with temporal logic goals and travel duration uncertainty.
<em>ICRA</em>, 257–263. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces an iterative approach to multi-agent route planning under chance constraints. A heterogeneous team of agents with various capabilities is tasked with a Capability Temporal Logic (CaTL) mission, a fragment of Signal Temporal Logic. The agents’ motion is modeled as a finite weighted graph, where the weights represent travel durations. Given the probability distribution over the durations of each edge’s traversal, we want to find paths for all agents such that (a) the specification robustness is maximized, (b) travel time is minimized, and (c) the success probability is maximized. We tackle the problem using an iterative approach. In each stage, it selects edges’ traversal duration and success probabilities and then solves a multi-agent route planning problem. We use an efficient Mixed-Integer Linear Programming (MILP) encoding for the latter. Our method provides a framework for agents to make informed decisions in choosing the most suitable edge attributes (travel durations and success probabilities) that consider agents’ capabilities to perform tasks in the environment. The proposed iterative method leverages graph structure to generate a more efficient search space. The effectiveness of our method is demonstrated through simulated case studies where obtaining the optimal solution would otherwise be computationally expensive. Our approach efficiently explores the solution space, generating better solutions and improving the performance of multi-agent route planning with uncertain travel durations.},
  archive   = {C_ICRA},
  author    = {Kaier Liang and Gustavo A. Cardona and Cristian-Ioan Vasile},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611519},
  month     = {5},
  pages     = {257-263},
  title     = {An iterative approach for heterogeneous multi-agent route planning with temporal logic goals and travel duration uncertainty},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal control synthesis with relaxed global temporal logic
specifications for homogeneous multi-robot teams. <em>ICRA</em>,
250–256. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we address the problem of control synthesis for a homogeneous team of robots given a global temporal logic specification and formal user preferences for relaxation in case of infeasibility. The relaxation preferences are represented as a Weighted Finite-state Edit System and are used to compute a relaxed specification automaton that captures all allowable relaxations of the mission specification and their costs. For synthesis, we introduce a Mixed Integer Linear Programming (MILP) formulation that combines the motion of the team of robots with the relaxed specification automaton. Our approach combines automata-based and MILP-based methods and leverages the strengths of both approaches, while avoiding their shortcomings. Specifically, the relaxed specification automaton explicitly accounts for the progress towards satisfaction, and the MILP-based optimization approach avoids the state-space explosion associated with explicit product-automata construction, thereby efficiently solving the problem. The case studies highlight the efficiency of the proposed approach.},
  archive   = {C_ICRA},
  author    = {Disha Kamale and Cristian-Ioan Vasile},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610142},
  month     = {5},
  pages     = {250-256},
  title     = {Optimal control synthesis with relaxed global temporal logic specifications for homogeneous multi-robot teams},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unraveling the single tangent space fallacy: An analysis and
clarification for applying riemannian geometry in robot learning.
<em>ICRA</em>, 242–249. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the realm of robotics, numerous downstream robotics tasks leverage machine learning methods for processing, modeling, or synthesizing data. Often, this data comprises variables that inherently carry geometric constraints, such as the unit-norm condition of quaternions representing rigid-body orientations or the positive definiteness of stiffness and manipulability ellipsoids. Handling such geometric constraints effectively requires the incorporation of tools from differential geometry into the formulation of machine learning methods. In this context, Riemannian manifolds emerge as a powerful mathematical framework to handle such geometric constraints. Nevertheless, their recent adoption in robot learning has been largely characterized by a mathematically-flawed simplification, hereinafter referred to as the &quot;single tangent space fallacy&quot;. This approach involves merely projecting the data of interest onto a single tangent (Euclidean) space, over which an off-the-shelf learning algorithm is applied. This paper provides a theoretical elucidation of various misconceptions surrounding this approach and offers experimental evidence of its shortcomings. Finally, it presents valuable insights to promote best practices when employing Riemannian geometry within robot learning applications.},
  archive   = {C_ICRA},
  author    = {Noémie Jaquier and Leonel Rozo and Tamim Asfour},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611701},
  month     = {5},
  pages     = {242-249},
  title     = {Unraveling the single tangent space fallacy: An analysis and clarification for applying riemannian geometry in robot learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Safety verification of closed-loop control system with
anytime perception. <em>ICRA</em>, 227–233. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we consider the problem of safety analysis of a closed-loop control system with anytime perception sensor. We formalize the framework and present a general procedure for safety analysis using reachable set computation. We instantiate the procedure for two concrete classes, namely, the classical discrete-time linear system with linear state feedback controller and an extension with variable update rates. We present an exact computational method based on polyhedral manipulations for the first class and an overapproximate method for the second class. Our experimental results demonstrate the feasibility of the approach.},
  archive   = {C_ICRA},
  author    = {Lipsy Gupta and Jahid Chowdhury Choton and Pavithra Prabhakar},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611044},
  month     = {5},
  pages     = {227-233},
  title     = {Safety verification of closed-loop control system with anytime perception},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tactile robot programming: Transferring task constraints
into constraint-based unified force-impedance control. <em>ICRA</em>,
204–210. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Flexible manufacturing lines are required to meet the demand for customized and small batch-size products. Even though state-of-the-art tactile robots may provide the versatility for increased adaptability and flexibility, their potential is yet to be fully exploited. To support robotics deployment in manufacturing, we propose a task-based tactile robot programming paradigm that uses an object-centric tactile skill definition that directly links identified object constraints of the task to the definition of constraint-based unified force-impedance control. In this study, we first explain the basic concept of abstracting the task constraints experienced by the object and transferring them to the robot’s operational space frame. Second, using the object-centric tactile skill definition, we synthesize unified force-impedance control and formalized holonomic constraints to enable flexible task execution. Later, we propose the quantified analysis metrics for the process by analyzing them as a typical example of flexible manipulation disassembly skills, e.g., levering and unscrew-driving regarding their object requirements. Supported by realistic experimental evaluation using a Franka Emika robot, our tactile robot programming approach for the direct translation between task-level constraints and robot control parameter design is shown to be a viable solution for increased robotic deployment in flexible manufacturing lines.},
  archive   = {C_ICRA},
  author    = {Kübra Karacan and Robin Jeanne Kirschner and Hamid Sadeghian and Fan Wu and Sami Haddadin},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610054},
  month     = {5},
  pages     = {204-210},
  title     = {Tactile robot programming: Transferring task constraints into constraint-based unified force-impedance control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flexible omnidirectional driving gear mechanism with
adaptation over arbitrary curvatures. <em>ICRA</em>, 197–203. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10609984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A support structure for flexible displays such as OLED or flexible LEDs was developed using the flexible omnidirectional driving gear mechanism. It is a gear mechanism having two degrees of freedom on one surface. This flexible display mechanism is expected to be placed inside a car dashboard as a human interface and for workspace optimization. In this study, we propose a novel flexible omnidirectional driving gear for supporting flexible displays discussing its design, motion range, repeatability, positional accuracy, and adaptability to any guide surface through magnetic coupling. The experiments showed satisfactory results for positional accuracy and repeatability with adaptability over a wide range of curvatures.},
  archive   = {C_ICRA},
  author    = {Moses Gladson Selvamuthu and Kazuki Abe and Kenjiro Tadakuma and Riichiro Tadakuma},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10609984},
  month     = {5},
  pages     = {197-203},
  title     = {Flexible omnidirectional driving gear mechanism with adaptation over arbitrary curvatures},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and evaluation of a reconfigurable 7-DOF upper limb
rehabilitation exoskeleton with gravity compensation. <em>ICRA</em>,
190–196. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the development of society, aging population and the number of stroke patients is increasing year by year. Rehabilitation exoskeleton can help patients to carry out rehabilitation training and improve their activities of daily living (ADL). First of all, a reconfigurable exoskeleton for upper limb rehabilitation is designed in this paper. The exoskeleton combines gravity compensation with left-right arm switching function through its reconfigurability. Secondly, the motion space and singular configuration of the exoskeleton are analyzed. By changing the working mode of the gravity compensation device, the control experiment of the motor is carried out. The influence of gravity compensation device on motor driving torque and energy consumption is analyzed. Finally, the results of experiment show that, in the best case, the gravity compensation device can reduce the energy consumption by 41.15% and the maximum motor current by 33.56% of the driving element.},
  archive   = {C_ICRA},
  author    = {Linliang Zheng and Qingcong Wu and Yanghui Zhu and Qiang Zhang},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610011},
  month     = {5},
  pages     = {190-196},
  title     = {Design and evaluation of a reconfigurable 7-DOF upper limb rehabilitation exoskeleton with gravity compensation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and experimental characterisation of a novel
quasi-direct drive actuator for highly dynamic robotic applications.
<em>ICRA</em>, 183–189. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents the design and experimental results of a proprioceptive, high-bandwidth quasi-direct drive (QDD) actuator for highly dynamic robotic applications. A comprehensive review of the mechanical design of the PULSE115-60 actuator is presented, with particular focus on the design parameters affecting the dynamic performance of the actuator and a full specification is provided. Fundamental parameters to describe the dynamic behaviour of an actuator are discussed, and an experimental method to determine speed and torque bandwidth of the actuator is presented. A rigorous method to determine backdrive torque is also explained. Finally, experimental results quantifying the dynamic performance of the PULSE115-60 actuator are discussed. The PULSE115-60 actuator has a highly dynamic response, surpassing the torque bandwidth at low torque amplitudes showcased in state-of-the-art literature. The differences between current and torque bandwidth, two concepts often conflated in literature, are elucidated. Experimental procedures detailed in previous work are discussed and a novel standardised procedure is proposed for robust characterisation and fair comparison of different actuation systems. Finally, performance results for PULSE115-60 are presented, demonstrating a torque bandwidth of 66.3 Hz at an amplitude of 6 N•m, ±0.11° of backlash and 0.37 N•m of backdrive torque.},
  archive   = {C_ICRA},
  author    = {C. Adrián Pérez-Díaz and Ignacio Muñoz and Daniel Martin-Hernández and Carlos Candelo-Zuluaga and Ivan Torres and Jordi Marsà and Daniel Sanz-Merodio and Miguel López},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611567},
  month     = {5},
  pages     = {183-189},
  title     = {Design and experimental characterisation of a novel quasi-direct drive actuator for highly dynamic robotic applications},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modular growing mechanism with multi-axis deformation.
<em>ICRA</em>, 176–182. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Plant cells expand and elongate. Their cumulative actuation defines organ morphing. Inspired by this modular transformability, this study proposes a modular concept for growing robots that will be able to grow by adding at their tip Transformable Modules (TMs). We provide a two-module implementation to evaluate the concept viability. We designed and characterized Shape-Retention Bellows (SRBs) that constitute the TM and are used to maintain the shape once the extension force is relaxed. We demonstrate module radial expansion and axial elongation in a straight and bent configuration (up to ~4°). This is the first concept of growing robots to enact the robot&#39;s modularity and transformability for future deployment in distributed growing systems capable of acting in various scenarios.},
  archive   = {C_ICRA},
  author    = {Dongdong Du and Emanuela Del Dottore and Alessio Mondini and Edoardo Sinibaldi and Barbara Mazzolai},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610637},
  month     = {5},
  pages     = {176-182},
  title     = {Modular growing mechanism with multi-axis deformation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A scalable monolithic 3D printable variable stiffness
mechanism. <em>ICRA</em>, 169–175. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Variable Stiffness Mechanisms (VSM) are becoming ubiquitous in mechatronics given the benefit they provide in terms of safety and performance. Despite these assets, VSMs remain fairly complex mechanical devices lacking in compactness, ease of manufacturing and accessibility. In addition, the scarcity of commercially available VSMs requires that such systems are mostly designed in-house. We propose a new type of VSM that improves on the pre-existing Jack Spring concept by making it more compact and robust. The new concept, which we refer to as the Compact Modifier of Active Coils (C-MAC) mechanism, is specifically designed to be manufactured through a monolithic 3D print. This approach enables to modify a minimal set of design features, namely the spring diameter and the coil diameter, to achieve the desired range of stiffness variation. We test the proposed design on six configurations; these show hysteretic energy losses no larger than 35% over the stiffness variation and confirm stiffness to scale according to theory. Stiffness ranging from 0.15 N/mm to 1.02N/mm were measured for an overall device length of 140 mm, including a maximal stroke length of 22 mm. The results confirm excellent scalability and manufacturability of the proposed design, providing a versatile mechanism for fast prototyping and the development of entire 3D printed robotic systems embedding variable stiffness capabilities.},
  archive   = {C_ICRA},
  author    = {Paul Baisamy and A. Stokes and F. Giorgio-Serchi},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610379},
  month     = {5},
  pages     = {169-175},
  title     = {A scalable monolithic 3D printable variable stiffness mechanism},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Research on bionic foldable wing for flapping wing micro air
vehicle. <em>ICRA</em>, 155–160. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a bionic foldable wing that imitates the hind wing of ladybirds. Based on the folding mechanism of the hind wing of ladybirds and the theory of origami, the motion model of the bionic foldable wing is established, yield the motion law of the crease angles and the variation relationship between the panels are obtained. Bionic foldable wings utilise shape memory alloy to drive wings to fold, and embedded torsion springs to release energy to realize the function of wing unfolding. In the experiments of the vehicle equipped with foldable wings, the lift and attitude torque of bionic foldable wings are measured by the F/T sensor. The experimental results indicated that its aerodynamic performance is basically close to that of our optimized non-foldable wings. Moreover, the vehicle with foldable wings has been able to overcome gravity to achieve flight, which provides a novel concept for the research on flapping wing.},
  archive   = {C_ICRA},
  author    = {Shengjie Xiao and Kai Hu and Yuhong Sun and Yun Wang and Bo Qin and Huichao Deng and Xuan Wu and Xilun Ding},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610536},
  month     = {5},
  pages     = {155-160},
  title     = {Research on bionic foldable wing for flapping wing micro air vehicle},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Johnsen-rahbek capstan clutch: A high torque electrostatic
clutch. <em>ICRA</em>, 148–154. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many robotic systems, the holding state consumes power, limits operating time, and increases operating costs. Electrostatic clutches have the potential to improve robotic performance by generating holding torques with low power consumption. A key limitation of electrostatic clutches has been their low specific shear stresses which restrict generated holding torque, limiting many applications. Here we show how combining the Johnsen-Rahbek (JR) effect with the exponential tension scaling capstan effect can produce clutches with the highest specific shear stress in the literature. Our system generated 31.3 N/cm 2 sheer stress and a total holding torque of 7.1 N•m while consuming only 2.5 mW/cm 2 at 500 V. We demonstrate a theoretical model of an electrostatic adhesive capstan clutch and demonstrate how large angle (θ &gt; 2π) designs increase efficiency over planar or small angle (θ &lt; π) clutch designs. We also report the first unfilled polymeric material, polybenzimidazole (PBI), to exhibit the JR-effect.},
  archive   = {C_ICRA},
  author    = {Timothy E. Amish and Jeffrey T. Auletta and Chad C. Kessens and Joshua R. Smith and Jeffrey I. Lipton},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611283},
  month     = {5},
  pages     = {148-154},
  title     = {Johnsen-rahbek capstan clutch: A high torque electrostatic clutch},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generating sparse probabilistic graphs for efficient
planning in uncertain environments. <em>ICRA</em>, 133–139. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Environments with regions of uncertain traversability can be modeled as roadmaps with probabilistic edges for efficient planning under uncertainty. We would like to generate roadmaps that enable planners to efficiently find paths with expected low costs through uncertain environments. The roadmap must be sparse so that the planning problem is tractable, but still contain edges that are likely to contribute to low-cost plans under various realizations of the environmental uncertainty. Determining the optimal set of edges to add to the roadmap without considering an exponential number of traversability scenarios is challenging. We propose the use of a heuristic that bounds the ratio between the expected path cost in our graph and the expected path cost in an optimal graph to determine whether a given edge should be added to the roadmap. We test our approach in several environments, demonstrating that our uncertainty-aware roadmaps effectively trade off between plan quality and planning efficiency for uncertainty-aware agents navigating in the graph.},
  archive   = {C_ICRA},
  author    = {Yasmin Veys and Martina Stadler Kurtz and Nicholas Roy},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610493},
  month     = {5},
  pages     = {133-139},
  title     = {Generating sparse probabilistic graphs for efficient planning in uncertain environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Safe POMDP online planning via shielding. <em>ICRA</em>,
126–132. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Partially observable Markov decision processes (POMDPs) have been widely used in many robotic applications for sequential decision-making under uncertainty. POMDP online planning algorithms such as Partially Observable Monte-Carlo Planning (POMCP) can solve very large POMDPs with the goal of maximizing the expected return. But the resulting policies cannot provide safety guarantees which are imperative for real-world safety-critical tasks (e.g., autonomous driving). In this work, we consider safety requirements represented as almost-sure reach-avoid specifications (i.e., the probability to reach a set of goal states is one and the probability to reach a set of unsafe states is zero). We compute shields that restrict unsafe actions which would violate the almost-sure reach-avoid specifications. We then integrate these shields into the POMCP algorithm for safe POMDP online planning. We propose four distinct shielding methods, differing in how the shields are computed and integrated, including factored variants designed to improve scalability. Experimental results on a set of benchmark domains demonstrate that the proposed shielding methods successfully guarantee safety (unlike the baseline POMCP without shielding) on large POMDPs, with negligible impact on the runtime for online planning.},
  archive   = {C_ICRA},
  author    = {Shili Sheng and David Parker and Lu Feng},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610195},
  month     = {5},
  pages     = {126-132},
  title     = {Safe POMDP online planning via shielding},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributionally robust CVaR-based safety filtering for
motion planning in uncertain environments. <em>ICRA</em>, 103–109. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safety is a core challenge of autonomous robot motion planning, especially in the presence of dynamic and uncertain obstacles. Many recent results use learning and deep learning-based motion planners and prediction modules to predict multiple possible obstacle trajectories and generate obstacle-aware ego robot plans. However, planners that ignore the inherent uncertainties in such predictions incur collision risks and lack formal safety guarantees. In this paper, we present a computationally efficient safety filtering solution to reduce the collision risk of ego robot motion plans using multiple samples of obstacle trajectory predictions. The proposed approach reformulates the collision avoidance problem by computing safe halfspaces based on obstacle sample trajectories using distributionally robust optimization (DRO) techniques. The safe halfspaces are used in a model predictive control (MPC)-like safety filter to apply corrections to the reference ego trajectory thereby promoting safer planning. The efficacy and computational efficiency of our approach are demonstrated through numerical simulations.},
  archive   = {C_ICRA},
  author    = {Sleiman Safaoui and Tyler H. Summers},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611276},
  month     = {5},
  pages     = {103-109},
  title     = {Distributionally robust CVaR-based safety filtering for motion planning in uncertain environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributionally robust chance constrained trajectory
optimization for mobile robots within uncertain safe corridor.
<em>ICRA</em>, 88–94. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safe corridor-based Trajectory Optimization (TO) presents an appealing approach for collision-free path planning of autonomous robots, because its convex formulation can guarantee global optimality. The safe corridor is constructed based on the obstacle map, however, the non-ideal perception induces uncertainty, which is rarely considered in the context of trajectory generation. In this paper, we propose Distributionally Robust Safe Corridor Constraints (DRSCCs) to consider the uncertainty of the safe corridor. Then, we integrate DRSCCs into the trajectory optimization framework using Bernstein basis polynomials. Theoretically, we rigorously prove that the proposed trajectory optimization problem is equivalent to a convex quadratic program, which is computationally efficient to deploy onto real robots. The simulation results show that our method enhances navigation safety by significantly reducing the infeasible motions compared to the baseline. Moreover, the proposed approach is validated through two robotic applications, a micro Unmanned Aerial Vehicle (UAV) and a quadruped robot Unitree A1.},
  archive   = {C_ICRA},
  author    = {Shaohang Xu and Haolin Ruan and Wentao Zhang and Yian Wang and Lijun Zhu and Chin Pang Ho},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611252},
  month     = {5},
  pages     = {88-94},
  title     = {Distributionally robust chance constrained trajectory optimization for mobile robots within uncertain safe corridor},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NoMaD: Goal masked diffusion policies for navigation and
exploration. <em>ICRA</em>, 63–70. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic learning for navigation in unfamiliar environments needs to provide policies for both task-oriented navigation (i.e., reaching a goal that the robot has located), and task-agnostic exploration (i.e., searching for a goal in a novel setting). Typically, these roles are handled by separate models, for example by using subgoal proposals, planning, or separate navigation strategies. In this paper, we describe how we can train a single unified diffusion policy to handle both goal-directed navigation and goal-agnostic exploration, with the latter providing the ability to search novel environments, and the former providing the ability to reach a user-specified goal once it has been located. We show that this unified policy results in better overall performance when navigating to visually indicated goals in novel environments, as compared to approaches that use subgoal proposals from generative models, or prior methods based on latent variable models. We instantiate our method by using a large-scale Transformer-based policy trained on data from multiple ground robots, with a diffusion model decoder to flexibly handle both goal-conditioned and goal-agnostic navigation. Our experiments, conducted on a real-world mobile robot platform, show effective navigation in unseen environments in comparison with five alternative methods, and demonstrate significant improvements in performance and lower collision rates, despite utilizing smaller models than state-of-the-art approaches.},
  archive   = {C_ICRA},
  author    = {Ajay Sridhar and Dhruv Shah and Catherine Glossop and Sergey Levine},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610665},
  month     = {5},
  pages     = {63-70},
  title     = {NoMaD: Goal masked diffusion policies for navigation and exploration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning vision-based bipedal locomotion for challenging
terrain. <em>ICRA</em>, 56–62. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning (RL) for bipedal locomotion has recently demonstrated robust gaits over moderate terrains using only proprioceptive sensing. However, such blind controllers will fail in environments where robots must anticipate and adapt to local terrain, which requires visual perception. In this paper, we propose a fully-learned system that allows bipedal robots to react to local terrain while maintaining commanded travel speed and direction. Our approach first trains a controller in simulation using a heightmap expressed in the robot’s local frame. Next, data is collected in simulation to train a heightmap predictor, whose input is the history of depth images and robot states. We demonstrate that with appropriate domain randomization, this approach allows for successful sim-to-real transfer with no explicit pose estimation and no fine-tuning using real-world data. To the best of our knowledge, this is the first example of sim-to-real learning for vision-based bipedal locomotion over challenging terrains.},
  archive   = {C_ICRA},
  author    = {Helei Duan and Bikram Pandit and Mohitvishnu S. Gadde and Bart Van Marum and Jeremy Dao and Chanho Kim and Alan Fern},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611621},
  month     = {5},
  pages     = {56-62},
  title     = {Learning vision-based bipedal locomotion for challenging terrain},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning continuous control with geometric regularity from
robot intrinsic symmetry. <em>ICRA</em>, 49–55. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Geometric regularity, which leverages data symmetry, has been successfully incorporated into deep learning architectures such as CNNs, RNNs, GNNs, and Transformers. While this concept has been widely applied in robotics to address the curse of dimensionality when learning from high-dimensional data, the inherent reflectional and rotational symmetry of robot structures has not been adequately explored. Drawing inspiration from cooperative multi-agent reinforcement learning, we introduce novel network structures for single-agent control learning that explicitly capture these symmetries. Moreover, we investigate the relationship between the geometric prior and the concept of Parameter Sharing in multi-agent reinforcement learning. Last but not the least, we implement the proposed framework in online and offline learning methods to demonstrate its ease of use. Through experiments conducted on various challenging continuous control tasks on simulators and real robots, we highlight the significant potential of the proposed geometric regularity in enhancing robot learning capabilities.},
  archive   = {C_ICRA},
  author    = {Shengchao Yan and Baohe Zhang and Yuan Zhang and Joschka Boedecker and Wolfram Burgard},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610949},
  month     = {5},
  pages     = {49-55},
  title     = {Learning continuous control with geometric regularity from robot intrinsic symmetry},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VLFM: Vision-language frontier maps for zero-shot semantic
navigation. <em>ICRA</em>, 42–48. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Understanding how humans leverage semantic knowledge to navigate unfamiliar environments and decide where to explore next is pivotal for developing robots capable of human-like search behaviors. We introduce a zero-shot navigation approach, Vision-Language Frontier Maps (VLFM), which is inspired by human reasoning and designed to navigate towards unseen semantic objects in novel environments. VLFM builds occupancy maps from depth observations to identify frontiers, and leverages RGB observations and a pre-trained vision-language model to generate a language-grounded value map. VLFM then uses this map to identify the most promising frontier to explore for finding an instance of a given target object category. We evaluate VLFM in photo-realistic environments from the Gibson, Habitat-Matterport 3D (HM3D), and Matterport 3D (MP3D) datasets within the Habitat simulator. Remarkably, VLFM achieves state-of-the-art results on all three datasets as measured by success weighted by path length (SPL) for the Object Goal Navigation task. Furthermore, we show that VLFM’s zero-shot nature enables it to be readily deployed on real-world robots such as the Boston Dynamics Spot mobile manipulation platform. We deploy VLFM on Spot and demonstrate its capability to efficiently navigate to target objects within an office building in the real world, without any prior knowledge of the environment. The accomplishments of VLFM underscore the promising potential of vision-language models in advancing the field of semantic navigation. Videos of real world deployment can be viewed at naoki.io/vlfm.},
  archive   = {C_ICRA},
  author    = {Naoki Yokoyama and Sehoon Ha and Dhruv Batra and Jiuguang Wang and Bernadette Bucher},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610712},
  month     = {5},
  pages     = {42-48},
  title     = {VLFM: Vision-language frontier maps for zero-shot semantic navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Resilient legged local navigation: Learning to traverse with
compromised perception end-to-end. <em>ICRA</em>, 34–41. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10611254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous robots must navigate reliably in unknown environments even under compromised exteroceptive perception, or perception failures. Such failures often occur when harsh environments lead to degraded sensing, or when the perception algorithm misinterprets the scene due to limited generalization. In this paper, we model perception failures as invisible obstacles and pits, and train a reinforcement learning (RL) based local navigation policy to guide our legged robot. Unlike previous works relying on heuristics and anomaly detection to update navigational information, we train our navigation policy to reconstruct the environment information in the latent space from corrupted perception and react to perception failures end-to-end. To this end, we incorporate both proprioception and exteroception into our policy inputs, thereby enabling the policy to sense collisions on different body parts and pits, prompting corresponding reactions. We validate our approach in simulation and on the real quadruped robot ANYmal running in real-time (&lt;10ms CPU inference). In a quantitative comparison with existing heuristic-based locally reactive planners, our policy increases the success rate over 30% when facing perception failures. Project Page: https://bit.ly/45NBTuh.},
  archive   = {C_ICRA},
  author    = {Chong Zhang and Jin Jin and Jonas Frey and Nikita Rudin and Matías Mattamala and Cesar Cadena and Marco Hutter},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10611254},
  month     = {5},
  pages     = {34-41},
  title     = {Resilient legged local navigation: Learning to traverse with compromised perception end-to-end},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MORALS: Analysis of high-dimensional robot controllers via
topological tools in a latent space. <em>ICRA</em>, 27–33. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Estimating the region of attraction (RoA) for a robot controller is essential for safe application and controller composition. Many existing methods require a closed-form expression that limit applicability to data-driven controllers. Methods that operate only over trajectory rollouts tend to be data-hungry. In prior work, we have demonstrated that topological tools based on Morse Graphs (directed acyclic graphs that combinatorially represent the underlying nonlinear dynamics) offer data-efficient RoA estimation without needing an analytical model. They struggle, however, with high-dimensional systems as they operate over a state-space discretization. This paper presents Morse Graph-aided discovery of Regions of Attraction in a learned Latent Space (MORALS) ** . The approach combines auto-encoding neural networks with Morse Graphs. MORALS shows promising predictive capabilities in estimating attractors and their RoAs for data-driven controllers operating over high-dimensional systems, including a 67-dim humanoid robot and a 96-dim 3-fingered manipulator. It first projects the dynamics of the controlled system into a learned latent space. Then, it constructs a reduced form of Morse Graphs representing the bistability of the underlying dynamics, i.e., detecting when the controller results in a desired versus an undesired behavior. The evaluation on high-dimensional robotic datasets indicates data efficiency in RoA estimation.},
  archive   = {C_ICRA},
  author    = {Ewerton R. Vieira and Aravind Sivaramakrishnan and Sumanth Tangirala and Edgar Granados and Konstantin Mischaikow and Kostas E. Bekris},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610383},
  month     = {5},
  pages     = {27-33},
  title     = {MORALS: Analysis of high-dimensional robot controllers via topological tools in a latent space},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient composite learning robot control under partial
interval excitation. <em>ICRA</em>, 21–26. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Parameter convergence in adaptive control is crucial for improving the stability and robustness of robotic systems. Nevertheless, a stringent condition named persistent excitation (PE) needs to be satisfied to ensure parameter convergence in the conventional adaptive robot control. Composite learning robot control (CLRC) is an innovative methodology that guarantees parameter convergence under a condition of interval excitation (IE) that is strictly weaker than PE. This paper puts forward a time-division multi-channel (TDMC) CLRC strategy such that parameter convergence is achieved even without the IE condition. In the TDMC mechanism, a filtered regressor is integrated with multiple time intervals to generate a generalized prediction error for parameter update, such that excitation information of regressor channels at different instants is exploited more effectively and efficiently to achieve fast and accurate parameter estimation. Global exponential stability with parameter convergence of the closed-loop system is achieved under a partial IE condition that is much weaker than IE. Experiments on a collaborative robot with 7 degrees of freedom have demonstrated the superiority of the proposed approach in both parameter estimation and trajectory tracking compared to start-of-the-art approaches.},
  archive   = {C_ICRA},
  author    = {Tian Shi and Weibing Li and Haoyong Yu and Yongping Pan},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610877},
  month     = {5},
  pages     = {21-26},
  title     = {Efficient composite learning robot control under partial interval excitation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Under pressure: Learning-based analog gauge reading in the
wild. <em>ICRA</em>, 14–20. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose an interpretable framework for reading analog gauges that is deployable on real world robotic systems. Our framework splits the reading task into distinct steps, such that we can detect potential failures at each step. Our system needs no prior knowledge of the type of gauge or the range of the scale and is able to extract the units used. We show that our gauge reading algorithm is able to extract readings with a relative reading error of less than 2%.},
  archive   = {C_ICRA},
  author    = {Maurits Reitsma and Julian Keller and Kenneth Blomqvist and Roland Siegwart},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610793},
  month     = {5},
  pages     = {14-20},
  title     = {Under pressure: Learning-based analog gauge reading in the wild},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A movable microfluidic chip with gap effect for manipulation
of oocytes. <em>ICRA</em>, 8–13. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study proposes a novel movable microfluidic chip in which a microfluidic chip is integrated into a robotic manipulator for manipulating oocytes. The microfluidic device has the ability to release a single oocyte with a gap effect. The robotic manipulator can control the position of the microfluidic chip. The microfluidic chip with a pipette tip is directly fabricated using 3D printing. Xenopus oocyte was used in the experiment. When oocytes move from the back side of the channel to the front side, they generate gaps between each other. The gap distance can reach about 16 times the diameter of the oocyte. In addition, a capacitive sensor was used to detect oocytes in the manipulation processes. The results showed that oocytes were successfully released one by one with no deformation in shape using the movable microfluidic chip. The method has significant advantages in biomedicine engineering and micro-nano-manipulation.},
  archive   = {C_ICRA},
  author    = {Shuzhang Liang and Satoshi Amaya and Hirotaka Sugiura and Hao Mo and Yuguo Dai and Fumihito Arai},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610409},
  month     = {5},
  pages     = {8-13},
  title     = {A movable microfluidic chip with gap effect for manipulation of oocytes},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TinyMPC: Model-predictive control on resource-constrained
microcontrollers. <em>ICRA</em>, 1–7. (<a
href="https://doi.org/10.1109/ICRA57147.2024.10610987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Model-predictive control (MPC) is a powerful tool for controlling highly dynamic robotic systems subject to complex constraints. However, MPC is computationally demanding, and is often impractical to implement on small, resource-constrained robotic platforms. We present TinyMPC, a high-speed MPC solver with a low memory footprint targeting the microcontrollers common on small robots. Our approach is based on the alternating direction method of multipliers (ADMM) and leverages the structure of the MPC problem for efficiency. We demonstrate TinyMPC’s effectiveness by bench-marking against the state-of-the-art solver OSQP, achieving nearly an order of magnitude speed increase, as well as through hardware experiments on a 27 gram quadrotor, demonstrating high-speed trajectory tracking and dynamic obstacle avoidance. TinyMPC is publicly available at https://tinympc.org.},
  archive   = {C_ICRA},
  author    = {Khai Nguyen and Sam Schoedel and Anoushka Alavilli and Brian Plancher and Zachary Manchester},
  booktitle = {2024 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA57147.2024.10610987},
  month     = {5},
  pages     = {1-7},
  title     = {TinyMPC: Model-predictive control on resource-constrained microcontrollers},
  year      = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
