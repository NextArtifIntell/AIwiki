<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>RSS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="rss---134">RSS - 134</h2>
<ul>
<li><details>
<summary>
(2024, July). Stein variational ergodic search. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p001.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Exploration requires that robots reason about numerous ways to cover a space in response to dynamically changing conditions. However, in continuous domains there are potentially infinitely many options for robots to explore which can prove computationally challenging. How then should a robot efficiently optimize and choose exploration strategies to adopt? In this work, we explore this question through the use of variational inference to efficiently solve for distributions of coverage trajectories. Our approach leverages ergodic search methods to optimize coverage trajectories in continuous time and space. In order to reason about distributions of trajectories, we formulate ergodic search as a probabilistic inference problem. We propose to leverage Stein variational methods to approximate a posterior distribution over ergodic trajectories through parallel computation. As a result, it becomes possible to efficiently optimize distributions of feasible coverage trajectories for which robots can adapt exploration. We demonstrate that the proposed Stein variational ergodic search approach facilitates efficient identification of multiple coverage strategies and show online adaptation in a model-predictive control formulation. Simulated and physical experiments demonstrate adaptability and diversity in exploration strategies online.},
  archive   = {C_RSS},
  author    = {Darrick Lee and Cameron Lerch and Fabio Ramos and Ian Abraham},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Stein variational ergodic search},
  url       = {https://www.roboticsproceedings.org/rss20/p001.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Parallel and proximal linear-quadratic methods for
real-time constrained model-predictive control. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p002.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent strides in model predictive control (MPC) underscore a dependence on numerical advancements to efficiently and accurately solve large-scale problems. Given the substantial number of variables characterizing typical whole-body optimal control problems —often numbering in the thousands— exploiting the sparse structure of the numerical problem becomes crucial to meet computational demands, typically in the range of a few milliseconds. A fundamental building block for computing Newton or Sequential Quadratic Programming steps in direct optimal control methods involves addressing the linear-quadratic regulator (LQR) problem. This paper concentrates on equality-constrained problems featuring implicit system dynamics and dual regularization, a characteristic found in advanced interior-point or augmented-Lagrangian solvers. Here, we introduce a parallel algorithm designed for solving an LQR problem with dual regularization. Leveraging a rewriting of the LQR recursion through block elimination, we first enhanced the efficiency of the serial algorithm, then subsequently generalized to handle parametric LQR. This extension enables us to split decision variables and to solve several subproblems concurrently. Our algorithm is implemented (and will be released in open source) in a nonlinear constrained implicit optimal control solver. It showcases improved performance over previous serial formulations and we validate its efficacy by deploying it in the model predictive control of a real quadruped robot.},
  archive   = {C_RSS},
  author    = {Wilson Jallet and Ewen Dantec and Etienne Arlaud and Nicolas Mansard and Justin Carpentier},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Parallel and proximal linear-quadratic methods for real-time constrained model-predictive control},
  url       = {https://www.roboticsproceedings.org/rss20/p002.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Differentiable robust model predictive control.
<em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p003.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deterministic model predictive control (MPC), while powerful, is often insufficient for effectively controlling autonomous systems in the real-world. Factors such as environmental noise and model error can cause deviations from the expected nominal performance. Robust MPC algorithms aim to bridge this gap between deterministic and uncertain control. However, these methods are often excessively difficult to tune for robustness due to the nonlinear and non-intuitive effects that controller parameters have on performance. To address this challenge, we first present a unifying perspective on differentiable optimization for control using the implicit function theorem (IFT), from which existing state-of-the art methods can be derived. Drawing parallels with differential dynamic programming, the IFT enables the derivation of an efficient differentiable optimal control framework. The derived scheme is subsequently paired with a tube-based MPC architecture to facilitate the automatic and real-time tuning of robust controllers in the presence of large uncertainties and disturbances. The proposed algorithm is benchmarked on multiple nonlinear robotic systems, including two systems in the MuJoCo simulator environment and one hardware experiment on the Robotarium testbed, to demonstrate its efficacy.},
  archive   = {C_RSS},
  author    = {Alex Oshin and Hassan Almubarak and Evangelos Theodorou},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Differentiable robust model predictive control},
  url       = {https://www.roboticsproceedings.org/rss20/p003.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Computation-aware learning for stable control with
gaussian process. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p004.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In Gaussian Process (GP) dynamical model learning for robot control, particularly for systems constrained by computational resources like small quadrotors equipped with low-end processors, analyzing stability and designing a stable controller present significant challenges. This paper distinguishes between two types of uncertainty within the posteriors of GP dynamical models: the well-documented mathematical uncertainty stemming from limited data and computational uncertainty arising from constrained computational capabilities, which has been largely overlooked in prior research. Our work demonstrates that computational uncertainty, quantified through a probabilistic approximation of the inverse covariance matrix in GP dynamical models, is essential for stable control under computational constraints. We show that incorporating computational uncertainty can prevent overestimating the region of attraction, a safe subset of the state space with asymptotic stability, thus improving system safety. Building on these insights, we propose an innovative controller design methodology that integrates computational uncertainty within a second-order cone programming framework. Simulations of canonical stable control tasks and experiments of quadrotor tracking exhibit the effectiveness of our method under computational constraints.},
  archive   = {C_RSS},
  author    = {Wenhan Cao and Alexandre Capone and Rishabh Yadav and Sandra Hirche and Wei Pan},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Computation-aware learning for stable control with gaussian process},
  url       = {https://www.roboticsproceedings.org/rss20/p004.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Decentralized multi-robot line-of-sight connectivity
maintenance under uncertainty. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p005.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel decentralized control method to maintain Line-of-Sight connectivity for multi-robot networks in the presence of Guassian-distributed localization uncertainty. In contrast to most existing work that assumes perfect positional information about robots or enforces overly restrictive rigid formation against uncertainty, our method enables robots to preserve Line-of-Sight connectivity with high probability under unbounded Gaussian-like positional noises while remaining minimally intrusive to the original robots’ tasks. This is achieved by a motion coordination framework that jointly optimizes the set of existing Line-of-Sight edges to preserve and control revisions to the nominal task-related controllers, subject to the safety constraints and the corresponding composition of uncertainty-aware Line-of-Sight control constraints. Such compositional control constraints, expressed by our novel notion of probabilistic Line-of-Sight connectivity barrier certificates (PrLOS-CBC) for pairwise robots using control barrier functions, explicitly characterize the deterministic admissible control space for the two robots. The resulting motion ensures Line-of-Sight connectedness for the robot team with high probability. Furthermore, we propose a fully decentralized algorithm that decomposes the motion coordination framework by interleaving the composite constraint specification and solving for the resulting optimization-based controllers. The optimality of our approach is justified by the theoretical proofs. Simulation and real-world experiments results are given to demonstrate the effectiveness of our method.},
  archive   = {C_RSS},
  author    = {Yupeng Yang and Yiwei Lyu and Yanze Zhang and Sha Yi and Wenhao Luo},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Decentralized multi-robot line-of-sight connectivity maintenance under uncertainty},
  url       = {https://www.roboticsproceedings.org/rss20/p005.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Hamilton-jacobi reachability analysis for hybrid
systems with controlled and forced transitions. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p006.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hybrid dynamical systems with nonlinear dynamics are one of the most general modeling tools for representing robotic systems, especially contact-rich systems. However, providing guarantees regarding the safety or performance of nonlinear hybrid systems remains a challenging problem because it requires simultaneous reasoning about continuous state evolution and discrete mode switching. In this work, we address this problem by extending classical Hamilton-Jacobi (HJ) reachability analysis, a formal verification method for continuous-time nonlinear dynamical systems, to hybrid dynamical systems. We characterize the reachable sets for hybrid systems through a generalized value function defined over discrete and continuous states of the hybrid system. We also provide a numerical algorithm to compute this value function and obtain the reachable set. Our framework can compute reachable sets for hybrid systems consisting of multiple discrete modes, each with its own set of nonlinear continuous dynamics, discrete transitions that can be directly commanded or forced by a discrete control input, while still accounting for control bounds and adversarial disturbances in the state evolution. Along with the reachable set, the proposed framework also provides an optimal continuous and discrete controller to ensure system safety. We demonstrate our framework in several simulation case studies, as well as on a real-world testbed to solve the optimal mode planning problem for a quadruped with multiple gaits.},
  archive   = {C_RSS},
  author    = {Javier Borquez and Shuang Peng and Yiyu Chen and Quan Nguyen and Somil Bansal},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Hamilton-jacobi reachability analysis for hybrid systems with controlled and forced transitions},
  url       = {https://www.roboticsproceedings.org/rss20/p006.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). JIGGLE: An active sensing framework for boundary
parameters estimation in deformable surgical environments. <em>RSS</em>.
(<a href="https://www.roboticsproceedings.org/rss20/p007.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Surgical automation can improve the accessibility and consistency of life-saving procedures. Most surgeries require separating layers of tissue to access the surgical site, and suturing to re-attach incisions. These tasks involve deformable manipula- tion to safely identify and alter tissue attachment (boundary) topology. Due to poor visual acuity and frequent occlusions, surgeons tend to carefully manipulate the tissue in ways that enable inference of the tissue’s attachment points without causing unsafe tearing. In a similar fashion, we propose JIGGLE, a framework for estimation and interactive sensing of unknown boundary parameters in deformable surgical environments. This framework has two key components: (1) a probabilistic estimation to identify the current attachment points, achieved by integrating a differentiable soft-body simulator with an extended Kalman filter (EKF), and (2) an optimization-based active control pipeline that generates actions to maximize information gain of the tissue attachments, while simultaneously minimizing safety costs. The robustness of our estimation approach is demonstrated through experiments with real animal tissue, where we infer sutured attachment points using stereo endoscope observations. We also demonstrate the capabilities of our method in handling complex topological changes such as cutting and suturing.},
  archive   = {C_RSS},
  author    = {Nikhil Uday Shinde and Xiao Liang and Fei Liu and Yutong Zhang and Florian Richter and Sylvia Lee Herbert and Michael C. Yip},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {JIGGLE: An active sensing framework for boundary parameters estimation in deformable surgical environments},
  url       = {https://www.roboticsproceedings.org/rss20/p007.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Conformalized teleoperation: Confidently mapping human
inputs to high-dimensional robot actions. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p008.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Assistive robotic arms often have more degrees-of-freedom than a human teleoperator can control with a low-dimensional input, like a joystick. To overcome this challenge, existing approaches use data-driven methods to learn a mapping from low-dimensional human inputs to high-dimensional robot actions. However, determining if such a black-box mapping can confidently infer a user&#39;s intended high-dimensional action from low-dimensional inputs remains an open problem. Our key idea is to adapt the assistive map at training time to additionally estimate high-dimensional action quantiles, and then calibrate these quantiles via rigorous uncertainty quantification methods. Specifically, we leverage adaptive conformal prediction which adjusts the intervals over time, reducing the uncertainty bounds when the mapping is performant and increasing the bounds when the mapping consistently mis-predicts. Furthermore, we propose an uncertainty-interval-based mechanism for detecting high-uncertainty user inputs and robot states. We evaluate the efficacy of our proposed approach in a 2D assistive navigation task and two 7DOF Kinova Jaco tasks involving assistive cup grasping and goal reaching. Our findings demonstrate that conformalized assistive teleoperation manages to detect (but not differentiate between) high uncertainty induced by diverse preferences and induced by low-precision trajectories in the mapping&#39;s training dataset. On the whole, we see this work as a key step towards enabling robots to quantify their own uncertainty and proactively seek intervention when needed.},
  archive   = {C_RSS},
  author    = {Michelle D Zhao and Reid Simmons and Henny Admoni and Andrea Bajcsy},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Conformalized teleoperation: Confidently mapping human inputs to high-dimensional robot actions},
  url       = {https://www.roboticsproceedings.org/rss20/p008.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Optimal non-redundant manipulator surface coverage
with rank-deficient manipulability constraints. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p009.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A generalised solver for the manipulator non-revisiting coverage path planning (NCPP) problem is proposed in this paper. Nonlinear manipulator kinematics and the imposition of task-specific constraints dictate that applying conventional coverage path planning (CPP) solutions based on 2D template matching or cellular decomposition schemes on the target surface invariably results in truncated end-effector motions. Likewise, coverage paths designed directly in joint-space cannot ensure re-occurrences will not arise. More recent SOTA works have proposed finite-step optimal NCPP solutions where singularities are however expressly disregarded. Directly incorporating singular configurations violates the local bijectivity and finite-to-one property in the kinematic mapping, and cannot be properly modelled within existing schemes. This work leverages &quot;valid&#39;&#39; singularities, those that exhibit sufficient manoeuvrability in suitable dimensions to allow continuation of the tracking motion, thus further reducing the number of posture reconfigurations. The scheme assumes a generic representation of surfaces as discrete meshes, symbolising a null probability to locate the points corresponding to valid but singular inverse kinematic configurations, and constructs a practical method to traverse a singularity without explicitly calculating it. Simulated and realistic experiments are carried out where the suitability of the scheme to reduce posture reconfigurations and achieve continuous coverage motions are compared with existing methods. Three scenarios have been examined whereby the planner is able to fulfil motions without discontinuities in all instances.},
  archive   = {C_RSS},
  author    = {Tong Yang and Li Huang and Jaime Valls Miro and Yue Wang and Rong Xiong},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Optimal non-redundant manipulator surface coverage with rank-deficient manipulability constraints},
  url       = {https://www.roboticsproceedings.org/rss20/p009.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). AdaptiGraph: Material-adaptive graph-based neural
dynamics for robotic manipulation. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p010.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Predictive models are a crucial component of many robotic systems. Yet, constructing accurate predictive models for a variety of deformable objects, especially those with unknown physical properties, remains a significant challenge. This paper introduces AdaptiGraph, a learning-based dynamics modeling approach that enables robots to predict, adapt to, and control a wide array of challenging deformable materials with unknown physical properties. AdaptiGraph leverages the highly flexible graph-based neural dynamics (GBND) framework, which represents material bits as particles and employs a graph neural network (GNN) to predict particle motion. Its key innovation is a unified physical property-conditioned GBND model capable of predicting the motions of diverse materials with varying physical properties without retraining. Upon encountering new materials during online deployment, AdaptiGraph utilizes a physical property optimization process for a few-shot adaptation of the model, enhancing its fit to the observed interaction data. The adapted models can precisely simulate the dynamics and predict the motion of various deformable materials, such as ropes, granular media, rigid boxes, and cloth, while adapting to different physical properties, including stiffness, granular size, and center of pressure. On prediction and manipulation tasks involving a diverse set of real-world deformable objects, our method exhibits superior prediction accuracy and task proficiency over non-material-conditioned and non-adaptive models.},
  archive   = {C_RSS},
  author    = {Kaifeng Zhang and Baoyu Li and Kris Hauser and Yunzhu Li},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {AdaptiGraph: Material-adaptive graph-based neural dynamics for robotic manipulation},
  url       = {https://www.roboticsproceedings.org/rss20/p010.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Human-oriented representation learning for robotic
manipulation. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p011.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans inherently possess generalizable visual representations that empower them to efficiently explore and interact with the environments in manipulation tasks. We advocate that such a representation automatically arises from simultaneously learning about multiple simple perceptual skills that are critical for everyday scenarios (e.g., hand detection, state estimate, etc.) and is better suited for learning robot manipulation policies compared to current state-of-the-art visual representations purely based on self-supervised objectives. We formalize this idea through the lens of human-oriented multi-task fine-tuning on top of pre-trained visual encoders, where each task is a perceptual skill tied to human-environment interactions. We introduce Task Fusion Decoder as a plug-and-play embedding translator that utilizes the underlying relationships among these perceptual skills to guide the representation learning towards encoding meaningful structure for what’s important for all perceptual skills, ultimately empowering learning of downstream robotic manipulation tasks. Extensive experiments across a range of robotic tasks and embodiments, in both simulations and real-world environments, show that our Task Fusion Decoder consistently improves the representation of three state-of-the-art visual encoders including R3M, MVP, and EgoVLP, for downstream manipulation policy-learning. More demos, datasets, models, and code can be found at &lt; &gt;.},
  archive   = {C_RSS},
  author    = {Mingxiao Huo and Mingyu Ding and Chenfeng Xu and Thomas Tian and Xinghao Zhu and Yao Mu and Lingfeng Sun and Masayoshi Tomizuka and Wei Zhan},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Human-oriented representation learning for robotic manipulation},
  url       = {https://www.roboticsproceedings.org/rss20/p011.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Dynamic on-palm manipulation via controlled sliding.
<em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p012.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Non-prehensile manipulation enables fast interactions with objects by circumventing the need to grasp and ungrasp as well as handling objects that cannot be grasped through force closure. Current approaches to non-prehensile manipulation focus on static contacts, avoiding the underactuation that comes with sliding. However, the ability to control sliding contact, essentially removing the no-slip constraint, opens up new possibilities in dynamic manipulation. In this paper, we explore a challenging dynamic non-prehensile manipulation task that requires the consideration of the full spectrum of hybrid contact modes. We leverage recent methods in contact-implicit MPC to handle the multi-modal planning aspect of the task. We demonstrate, with careful consideration of integration between the simple model used for MPC and the low-level tracking controller, how contact-implicit MPC can be adapted to dynamic tasks. Surprisingly, despite the known inaccuracies of frictional rigid contact models, our method is able to react to these inaccuracies while still quickly performing the task. Moreover, we do not use common aids such as reference trajectories or motion primitives, highlighting the generality of our approach. To the best of our knowledge, this is the first application of contact-implicit MPC to a dynamic manipulation task in three dimensions.},
  archive   = {C_RSS},
  author    = {William Yang and Michael Posa},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Dynamic on-palm manipulation via controlled sliding},
  url       = {https://www.roboticsproceedings.org/rss20/p012.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Efficient data collection for robotic manipulation via
compositional generalization. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p013.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Data collection has become an increasingly important problem in robotic manipulation, yet there still lacks much understanding of how to effectively collect data to facilitate broad generalization. Recent works on large-scale robotic data collection typically vary many environmental factors of variation (e.g., object types, table textures) during data collection, to cover a diverse range of scenarios. However, they do not explicitly account for the possible compositional abilities of policies trained on the data. If robot policies can compose environmental factors from their data to succeed when encountering unseen factor combinations, we can exploit this to avoid collecting data for situations that composition would address. To investigate this possibility, we conduct thorough empirical studies both in simulation and on a real robot that compare data collection strategies and assess whether visual imitation learning policies can compose environmental factors. We find that policies do exhibit composition, although leveraging prior robotic datasets is critical for this on a real robot. We use these insights to propose better in-domain data collection strategies that exploit composition, which can induce better generalization than naive approaches for the same amount of effort during data collection. We further demonstrate that a real robot policy trained on data from such a strategy achieves a success rate of 77.5% when transferred to entirely new environments that encompass unseen combinations of environmental factors, whereas policies trained using data collected without accounting for environmental variation fail to transfer effectively, with a success rate of only 2.5%. We provide videos at our project website http://iliad.stanford.edu/robot-data-comp/.},
  archive   = {C_RSS},
  author    = {Jensen Gao and Annie Xie and Ted Xiao and Chelsea Finn and Dorsa Sadigh},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Efficient data collection for robotic manipulation via compositional generalization},
  url       = {https://www.roboticsproceedings.org/rss20/p013.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Demonstrating learning from humans on open-source
dexterous robot hands. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p014.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Emulating human-like dexterity with robotic hands has been a long-standing challenge in robotics. In recent years, machine learning has demanded robot hands to be reliable, inexpensive and easy-to-reproduce. For the past few years we have been investigating how to address these demands. We will demonstrate our three robot hands that address this problem ranging from rigid easy-to-simulate hand to soft but strong dexterous robot hands performing three different machine learning tasks. Our first machine learning task will be teleoperation, where we will develop a new mobile arm and hand motion capture system that we will bring to RSS 2024. Second, we will demonstrate how to use human-video and human motion to teach robot hands. Finally, we will show how to continually improve these policies using reinforcement learning in both simulation and the real-world. This demo will be engaging, will serve to demystify dexterous manipulation and inspire researchers to bring robot hands into their own projects. Please see our website at https://leaphand.com/rss2024demo for more interactive information.},
  archive   = {C_RSS},
  author    = {Kenneth Shaw and Ananye Agarwal and Shikhar Bahl and Mohan Kumar Srirama and Alexandre Kirchmeyer and Aditya Kannan and Aravind Sivakumar and Deepak Pathak},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Demonstrating learning from humans on open-source dexterous robot hands},
  url       = {https://www.roboticsproceedings.org/rss20/p014.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Reconciling reality through simulation: A
real-to-sim-to-real approach for robust manipulation. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p015.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Imitation learning methods need significant human supervision to learn policies robust to changes in object poses, physical disturbances, and visual distractors. Reinforcement learning, on the other hand, can explore the environment autonomously to learn robust behaviors but may require impractical amounts of unsafe real-world data collection. To learn performant, robust policies without the burden of unsafe real-world data collection or extensive human supervision, we propose RialTo, a new system for robustifying real-world imitation learning policies via reinforcement learning in digital twin simulation environments constructed on the fly from small amounts of real-world data. To enable this real-to-sim-to-real pipeline, RialTo proposes an easy-to-use interface for quickly scanning and constructing digital twins of real-world environments. We also introduce a novel inverse distillation procedure for bringing real-world demonstrations into simulated environments for efficient fine-tuning, with minimal human intervention and engineering required. We evaluate RialTo across a variety of robotic manipulation problems in the real world, such as robustly stacking dishes on a rack, placing books on a shelf and four other tasks. RialTo increases (over 67%) in policy robustness without requiring extensive human data collection.},
  archive   = {C_RSS},
  author    = {Marcel Torne Villasevil and Anthony Simeonov and Zechu Li and April Chan and Tao Chen and Abhishek Gupta and Pulkit Agrawal},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Reconciling reality through simulation: A real-to-sim-to-real approach for robust manipulation},
  url       = {https://www.roboticsproceedings.org/rss20/p015.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). SAGE: Bridging semantic and actionable parts for
GEneralizable articulated-object manipulation under language
instructions. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p016.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To interact with daily-life articulated objects of diverse structures and functionalities, understanding the object parts plays a central role in both user instruction comprehension and task execution. However, the possible discordance between the semantic meaning and physics functionalities of the parts poses a challenge for designing a general system. To address this problem, we propose SAGE, a novel framework that bridges semantic and actionable parts of articulated objects to achieve generalizable manipulation under natural language instructions. More concretely, given an articulated object, we first observe all the semantic parts on it, conditioned on which an instruction interpreter proposes possible action programs that concretize the natural language instruction. Then, a part-grounding module maps the semantic parts into so-called Generalizable Actionable Parts (GAParts), which inherently carry information about part motion. End-effector trajectories are predicted on the GAParts, which, together with the action program, form an executable policy. Additionally, an interactive feedback module is incorporated to respond to failures, which closes the loop and increases the robustness of the overall framework. Key to the success of our framework is the joint proposal and knowledge fusion between a large vision-language model (VLM) and a small domain-specific model for both context comprehension and part perception, with the former providing general intuitions and the latter serving as expert facts. Both simulation and real-robot experiments show our effectiveness in handling a large variety of articulated objects with diverse language-instructed goals.},
  archive   = {C_RSS},
  author    = {Haoran Geng and Songlin Wei and Congyue Deng and Bokui Shen and He Wang and Leonidas Guibas},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {SAGE: Bridging semantic and actionable parts for GEneralizable articulated-object manipulation under language instructions},
  url       = {https://www.roboticsproceedings.org/rss20/p016.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Demonstrating event-triggered investigation and sample
collection for human scientists using field robots and large foundation
models. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p017.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce a pioneering end-to-end system demonstrated on a team of robots and sensors, designed to augment scientific exploration and discovery for human scientists in remote or inaccessible environments. We demonstrate and analyse our system&#39;s capability in a mock-up test-bed scenario. In this futuristic hypothetical scenario human scientists located in a controlled lunar habitat, are assisted by a team of robots in investigating an unknown seismic phenomena like moon-quakes or meteor impact detected by a sensor network deployed on the lunar surface. They do so by autonomously collecting data, providing contextual semantic information and collecting scientific sample for future analysis upon the direction of humans. This work is among the earliest to present a feasible way to integration large foundational models (LFMs) into field robotic deployment, enabling easy semantic and contextual understanding of the objects in the environment and natural language-based interactions with the robot for the scientist. In addition we bring together state-of-the-art techniques in mapping, object detection, navigation, mobile manipulation, soft grippers, event detection and present details of the integration, insights and lessons learnt from the deployment.},
  archive   = {C_RSS},
  author    = {Tirthankar Bandyopadhyay and Fletcher Talbot and Callum Bennie and Hashini Senaratne and Xun Li and Brendan Tidd and Mingze Xi and Jan Stiefel and Volkan Dedeoglu and Rod Taylor and Tea Molnar and Ziwei Wang and Josh Pinskier and Feng Xu and Lois Liow and Ben Burgess-Limerick and Jesse Haviland and Pavan Sikka and Simon Murrell and Jane Hodgkinson and Jiajun Liu and Fred Pauling and Stanislav Funiak},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Demonstrating event-triggered investigation and sample collection for human scientists using field robots and large foundation models},
  url       = {https://www.roboticsproceedings.org/rss20/p017.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). CraterGrader: Autonomous robotic terrain manipulation
for lunar site preparation and earthmoving. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p018.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Establishing lunar infrastructure is paramount to long-term habitation on the Moon. To meet the demand for future lunar infrastructure development, we present CraterGrader, a novel system for autonomous robotic earthmoving tasks within lunar constraints. In contrast to the current approaches to construction autonomy, CraterGrader uses online perception for dynamic mapping of deformable terrain, devises an energy-efficient material movement plan using an optimization-based transport planner, precisely localizes without GPS, and uses integrated drive and tool control to manipulate regolith with unknown and non-constant geotechnical parameters. We demonstrate CraterGrader&#39;s ability to achieve unprecedented performance in autonomous smoothing and grading within a lunar-like environment, showing that this framework is capable, robust, and a benchmark for future planetary site preparation robotics.},
  archive   = {C_RSS},
  author    = {Ryan Lee and Benjamin Younes and Alexander Pletta and John Harrington and Russell Q. Wong and William Whittaker},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {CraterGrader: Autonomous robotic terrain manipulation for lunar site preparation and earthmoving},
  url       = {https://www.roboticsproceedings.org/rss20/p018.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). POAM: Probabilistic online attentive mapping for
efficient robotic information gathering. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p019.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Gaussian Process (GP) models are widely used for Robotic Information Gathering (RIG) in exploring unknown environments due to their ability to model complex phenomena with non-parametric flexibility and accurately quantify prediction uncertainty. Previous work has developed informative planners and adaptive GP models to enhance the data efficiency of RIG by improving the robot’s sampling strategy to focus on informative regions in non-stationary environments. However, computational efficiency becomes a bottleneck when using GP models in large-scale environments with limited computational resources. We propose a framework – Probabilistic Online Attentive Mapping (POAM) – that leverages the modeling strengths of the non-stationary Attentive Kernel while achieving constant-time computational complexity for online decision-making. POAM guides the optimization process via variational Expectation Maximization, providing constant-time update rules for inducing inputs, variational parameters, and hyperparameters. Extensive experiments in active bathymetric mapping tasks demonstrate that POAM significantly improves computational efficiency, model accuracy, and uncertainty quantification capability compared to existing online sparse GP models.},
  archive   = {C_RSS},
  author    = {Weizhe Chen and Lantao Liu and Roni Khardon},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {POAM: Probabilistic online attentive mapping for efficient robotic information gathering},
  url       = {https://www.roboticsproceedings.org/rss20/p019.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Blending data-driven priors in dynamic games.
<em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p020.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As intelligent robots like autonomous vehicles become increasingly deployed in the presence of people, the extent to which these systems should leverage model-based game-theoretic planners versus data-driven policies for safe, interaction-aware motion planning remains an open question. Existing dynamic game formulations assume all agents are task-driven and behave optimally. However, in reality, humans tend to deviate from the decisions prescribed by these models, and their behavior is better approximated under a noisy-rational paradigm. In this work, we investigate a principled methodology to blend a data-driven reference policy with an optimization-based game-theoretic policy. We formulate KLGame, a type of non-cooperative dynamic game with Kullback-Leibler (KL) regularization with respect to a general, stochastic, and possibly multi-modal reference policy. Our method incorporates, for each decision maker, a tunable parameter that permits modulation between task-driven and data-driven behaviors. We propose an efficient algorithm for computing multimodal approximate feedback Nash equilibrium strategies of KLGame in real time. Through a series of simulated and real-world autonomous driving scenarios, we demonstrate that KLGame policies can more effectively incorporate guidance from the reference policy and account for noisily-rational human behaviors versus non-regularized baselines.},
  archive   = {C_RSS},
  author    = {Justin Lidard and Haimin Hu and Asher Hancock and Zixu Zhang and Albert Gimo Contreras and Vikash Modi and Jonathan DeCastro and Deepak Gopinath and Guy Rosman and Naomi Leonard and Maria Santos and Jaime Fernández Fisac},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Blending data-driven priors in dynamic games},
  url       = {https://www.roboticsproceedings.org/rss20/p020.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Demonstrating HOUND: A low-cost research platform for
high-speed off-road underactuated nonholonomic driving. <em>RSS</em>.
(<a href="https://www.roboticsproceedings.org/rss20/p021.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Off-road autonomy, crucial for applications such as search-and-rescue, agriculture, and planetary exploration, poses unique problems due to challenging terrains, as well as due to the risk involved in testing or deploying such systems. Accessible platforms have the potential to widen the field to a broader set of researchers and students. Existing efforts in making on-road autonomy more accessible have seen success, yet aggressive off-road autonomy remains underserved. We seek to fill this gap by introducing HOUND, a 1/10th-scale, inexpensive, off-road autonomous car platform that can handle challenging outdoor terrains at high speeds. To aid development speed, we integrate HOUND with BeamNG, a state-of-the-art driving simulator to enable both software in the loop as well as hardware in the loop testing. To reduce the extent of ruggedization required, and thus cost, we integrate a rollover prevention system as a safety feature into the platform. Real-world trials over 50 kilometers demonstrate the platform&#39;s longevity and effectiveness over varied terrains and speeds.},
  archive   = {C_RSS},
  author    = {Sidharth Talia and Matthew Schmittle and Alexander Lambert and Alexander Spitzer and Christoforos Mavrogiannis and Siddhartha Srinivasa},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Demonstrating HOUND: A low-cost research platform for high-speed off-road underactuated nonholonomic driving},
  url       = {https://www.roboticsproceedings.org/rss20/p021.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Model predictive control for aggressive driving over
uneven terrain. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p022.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Terrain traversability in unstructured off-road autonomy has traditionally relied on semantic classification, resource-intensive dynamics models, or purely geometry-based methods to predict vehicle-terrain interactions. While inconsequential at low speeds, uneven terrain subjects our full-scale system to safety-critical challenges at operating speeds of 7-10 m/s. This study focuses particularly on uneven terrain such as hills, banks, and ditches. These common high-risk geometries are capable of disabling the vehicle and causing severe passenger injuries if poorly traversed. We introduce a physics-based framework for identifying traversability constraints on terrain dynamics. Using this framework, we derive two fundamental constraints, each with a focus on mitigating rollover and ditch-crossing failures while being fully parallelizable in the sample-based Model Predictive Control (MPC) framework. In addition, we present the design of our planning and control system, which implements our parallelized constraints in MPC and utilizes a low-level controller to meet the demands of our aggressive driving without prior information about the environment and its dynamics. Through real-world experimentation and traversal of hills and ditches, we demonstrate that our approach captures fundamental elements of safe and aggressive autonomy over uneven terrain. Our approach improves upon geometry-based methods by completing comprehensive off-road courses up to 22\% faster while maintaining safe operation.},
  archive   = {C_RSS},
  author    = {Tyler Han and Alex Liu and Anqi Li and Alexander Spitzer and Guanya Shi and Byron Boots},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Model predictive control for aggressive driving over uneven terrain},
  url       = {https://www.roboticsproceedings.org/rss20/p022.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Demonstrating CropFollow++: Robust under-canopy
navigation with keypoints. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p023.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an empirically robust vision-based navigation system for under-canopy agricultural robots using semantic keypoints. Autonomous under-canopy navigation is challenging due to the tight spacing between the crop rows (∼ 0.75 m), degradation in RTK-GPS accuracy due to multipath error, and noise in LiDAR measurements from the excessive clutter. Earlier work called CropFollow addressed these challenges by proposing a learning-based visual navigation system with end-to-end perception. However, this approach has the following limitations: Lack of interpretable representation, and Sensitivity to outlier predictions during occlusion due to lack of a confidence measure. Our system, CropFollow++, introduces modular perception architecture with a learned semantic keypoint representation. This learned representation is more modular, and more interpretable than CropFollow, and provides a confidence measure to detect occlusions. CropFollow++ significantly outperformed CropFollow in terms of the number of collisions needed (13 vs. 33) in field tests spanning ∼ 1.9km each in challenging late-season fields with significant occlusions. We also deployed CropFollow++ in multiple under-canopy cover crop planting robots on a large scale (25 km in total) in various field conditions and we discuss the key lessons learned from this.},
  archive   = {C_RSS},
  author    = {Arun Narenthiran Sivakumar and Mateus Valverde Gasparino and Michael McGuire and Vitor Akihiro Hisano Higuti and M. Ugur Akcal and Girish Chowdhary},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Demonstrating CropFollow++: Robust under-canopy navigation with keypoints},
  url       = {https://www.roboticsproceedings.org/rss20/p023.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). SEEK: Semantic reasoning for object goal navigation in
real world inspection tasks. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p024.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the problem of object-goal navigation in autonomous inspections in real-world environments. Object-goal navigation is crucial to enable effective inspections in various settings, often requiring the robot to identify the target object within a large search space. Current object inspection methods fall short of human efficiency because they typically cannot bootstrap prior and common sense knowledge as humans do. In this paper, we introduce a framework that enables robots to use semantic knowledge from prior spatial configurations of the environment and semantic common sense knowledge. We propose SEEK (Semantic Reasoning for Object Inspection Tasks) that combines semantic prior knowledge with the robot&#39;s observations to search for and navigate toward target objects more efficiently. SEEK maintains two representations: a Dynamic Scene Graph (DSG) and a Relational Semantic Network (RSN). The RSN is a compact and practical model that estimates the probability of finding the target object across spatial elements in the DSG. We propose a novel probabilistic planning framework to search for the object using relational semantic knowledge. Our simulation analyses demonstrate that SEEK outperforms the classical planning and Large Language Models (LLMs)-based methods that are examined in this study in terms of efficiency for object-goal inspection tasks. We validated our approach on a physical legged robot in urban environments, showcasing its practicality and effectiveness in real-world inspection scenarios.},
  archive   = {C_RSS},
  author    = {Muhammad Fadhil Ginting and Sung-Kyun Kim and David Fan and Matteo Palieri and Mykel Kochenderfer and Ali-akbar Agha-mohammadi},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {SEEK: Semantic reasoning for object goal navigation in real world inspection tasks},
  url       = {https://www.roboticsproceedings.org/rss20/p024.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Yell at your robot: Improving on-the-fly from language
corrections. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p025.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hierarchical policies that combine language and low-level control have been shown to perform impressively long-horizon robotic tasks, by leveraging either zero-shot high-level planners like pretrained language and vision-language models (LLMs/VLMs) or models trained on annotated robotic demonstrations. However, for complex and dexterous skills, attaining high success rates on long-horizon tasks still represents a major challenge -- the longer the task is, the more likely it is that some stage will fail. In principle, a robust high-level controller can compensate for low-level failures by dynamically deploying corrections and adjustments, but training such high-level controllers in a way that is aware of the physical capabilities of the low-level skills requires costly demonstrations of entire multi-stage tasks. Can humans help the robot to continuously improve its long-horizon task performance through intuitive and natural feedback? In this paper, we make the following observation: high-level policies that index into sufficiently rich and expressive low-level language-conditioned skills can be readily supervised with human feedback in the form of language corrections. We show that even fine-grained corrections, such as small movements (&quot;move a bit to the left&quot;), can be effectively incorporated into high level policies, and that such corrections can be readily obtained from humans observing the robot and making occasional suggestions. This framework enables robots not only to rapidly adapt to real-time language feedback, but also incorporate this feedback into an iterative training scheme that improves the high-level policy&#39;s ability to correct errors in both low-level execution and high-level decision-making purely from verbal feedback. Our evaluation on real hardware shows that this leads to significant performance improvement in long-horizon, dexterous manipulation tasks without the need for any additional teleoperation.},
  archive   = {C_RSS},
  author    = {Lucy Xiaoyang Shi and Zheyuan Hu and Tony Z. Zhao and Archit Sharma and Karl Pertsch and Jianlan Luo and Sergey Levine and Chelsea Finn},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Yell at your robot: Improving on-the-fly from language corrections},
  url       = {https://www.roboticsproceedings.org/rss20/p025.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Task adaptation in industrial human-robot interaction:
Leveraging riemannian motion policies. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p026.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In real-world industrial environments, modern robots often rely on human operators for crucial decision-making and mission synthesis from individual tasks. Effective and safe collaboration between humans and robots requires systems that can adjust their motion to human intentions, enabling dynamic task planning and adaptation. Addressing the needs of industrial applications, we propose a motion control framework that (i) removes the need for manual control of the robot’s movement; (ii) facilitates the formulation and combination of complex tasks; and (iii) allows the seamless integration of human intent recognition and robot motion planning. For this purpose, we leverage a modular and purely reactive approach for task parametrization and motion generation, embodied by Riemannian Motion Policies. The effectiveness of our method is demonstrated, evaluated and compared to a representative state-of-the-art approach in experimental scenarios, inspired by realistic industrial Human-Robot Interaction settings.},
  archive   = {C_RSS},
  author    = {Mike Allenspach and Michael Pantic and Rik Girod and Lionel Ott and Roland Siegwart},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Task adaptation in industrial human-robot interaction: Leveraging riemannian motion policies},
  url       = {https://www.roboticsproceedings.org/rss20/p026.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Risk-calibrated human-robot interaction via set-valued
intent prediction. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p027.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tasks where robots must anticipate human intent, such as navigating around a cluttered home or sorting everyday items, are challenging because they exhibit a wide range of valid actions that lead to similar outcomes. Moreover, zero-shot cooperation between human-robot partners is an especially challenging problem because it requires the robot to infer and adapt on the fly to a latent human intent, which could vary significantly from human to human. Recently, deep learned motion prediction models have shown promising results in predicting human intent but are prone to being confidently incorrect. In this work, we present Risk-Calibrated Interactive Planning (RCIP), which is a framework for measuring and calibrating risk associated with uncertain action selection in human-robot cooperation, with the fundamental idea that the robot should ask for human clarification when the risk associated with the uncertainty in the human&#39;s intent cannot be controlled. RCIP builds on the theory of set-valued risk calibration to provide a finite-sample statistical guarantee on the cumulative loss incurred by the robot while minimizing the cost of human clarification in complex multi-step settings. Our main insight is to frame the risk control problem as a sequence-level multi-hypothesis testing problem, allowing efficient calibration using a low-dimensional parameter that controls a pre-trained risk-aware policy. Experiments across a variety of simulated and real-world environments demonstrate RCIP&#39;s ability to predict and adapt to a diverse set of dynamic human intents.},
  archive   = {C_RSS},
  author    = {Justin Lidard and Hang Pham and Ariel Bachman and Bryan Boateng and Anirudha Majumdar},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Risk-calibrated human-robot interaction via set-valued intent prediction},
  url       = {https://www.roboticsproceedings.org/rss20/p027.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Constraint-aware intent estimation for dynamic
human-robot object co-manipulation. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p028.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Constraint-aware estimation of human intent is essential for robots to physically collaborate and interact with humans. Further, to achieve fluid collaboration in dynamic tasks intent estimation should be achieved in real-time. In this paper, we present a framework that combines online estimation and control to facilitate robots in interpreting human intentions, and dynamically adjust their actions to assist in dynamic object co-manipulation tasks while considering both robot and human constraints. Central to our approach is the adoption of a Dynamic Systems (DS) model to represent human intent. Such a low-dimensional parameterized model, along with human manipulability and robot kinematic constraints, enables us to predict intent using a particle filter solely based on past motion data and tracking errors. For safe assistive control, we propose a variable impedance controller that adapts the robot&#39;s impedance to offer assistance based on the intent estimation confidence from the DS particle filter. We validate our framework on a challenging real-world human-robot co-manipulation task and present promising results over baselines. Our framework represents a significant step forward in physical human-robot collaboration (pHRC), ensuring that robot cooperative interactions with humans are both feasible and effective. https://tinyurl.com/intent-capability},
  archive   = {C_RSS},
  author    = {Yifei Simon Shao and Tianyu Li and Shafagh Keyvanian and Pratik Chaudhari and Vijay Kumar and Nadia Figueroa},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Constraint-aware intent estimation for dynamic human-robot object co-manipulation},
  url       = {https://www.roboticsproceedings.org/rss20/p028.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Demonstrating HumanTHOR: A simulation platform and
benchmark for human-robot collaboration in a shared workspace.
<em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p029.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human-robot collaboration (HRC) in a shared workspace has become a common pattern in real-world robot applications and has garnered significant research interest. However, most existing studies for human-in-the-loop (HITL) collaboration with robots in a shared workspace evaluate in either simplified game environments or physical platforms, falling short in limited realistic significance or limited scalability. To support future studies, we build an embodied framework named HumanTHOR, which enables humans to act in the simulation environment through VR devices to support HITL collaborations in a shared workspace. To validate our system, we build a benchmark of everyday tasks and conduct a preliminary user study with two baseline algorithms. The results show that the robot can effectively assist humans in collaboration, demonstrating the significance of HRC. The comparison among different levels of baselines affirms that our system can adequately evaluate robot capabilities and serve as a benchmark for different robot algorithms. The experimental results also indicate that there is still much room in the area and our system can provide a preliminary foundation for future HRC research in a shared workspace. More information about the simulation environment, experiment videos, benchmark descriptions, and additional supplementary materials can be found on the website: https://sites.google.com/view/humanthor/.},
  archive   = {C_RSS},
  author    = {Chenxu Wang and Boyuan Du and Jiaxin Xu and Huaping Liu},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Demonstrating HumanTHOR: A simulation platform and benchmark for human-robot collaboration in a shared workspace},
  url       = {https://www.roboticsproceedings.org/rss20/p029.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Developing design guidelines for older adults with
robot learning from demonstration. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p030.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Assistive in-home robots have the potential to enable older adults to age in place by offloading mentally or physically demanding tasks to a robot. However, one challenge for in-home robots is that each individual will have differing needs, preferences, and home environments, which can all change over time. Learning from Demonstration (LfD) is one solution to enable non-expert users to communicate their differing and changing preferences to a robot, but LfD has not been evaluated with a population of older adults. In a human-subjects experiment where participants teach a robot via LfD, we characterize disparities between older and younger adult participants in terms of robot performance, usability, and participant perceptions. We find that older adults are significantly more critical of the robot&#39;s performance and found the LfD process less usable than younger adults. Based on participant performance and feedback, we present design guidelines that will enable roboticists to increase LfD accessibility across demographics.},
  archive   = {C_RSS},
  author    = {Erin Hedlund-Botti and Lakshmi Seelam and Chuxuan Yang and Nathaniel Belles and Zulfiqar Haider Zaidi and Matthew Gombolay},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Developing design guidelines for older adults with robot learning from demonstration},
  url       = {https://www.roboticsproceedings.org/rss20/p030.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). FLAIR: Feeding via long-horizon AcquIsition of
realistic dishes. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p031.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot-assisted feeding holds immense promise for improving the quality of life for individuals with mobility limitations who are unable to feed themselves independently. However, there exists a large gap between the kinds of homogeneous, curated plates existing assistive feeding systems can handle, and truly in-the-wild meals. Feeding realistic plates is immensely challenging due to the sheer range of food items that a robot may encounter, each requiring specialized manipulation strategies which must be sequenced over a long-horizon to feed an entire meal. An assistive feeding system should not only be able to sequence different strategies efficiently in order to feed an entire meal, but also in a way that is mindful of user preferences given the personalized nature of the task. We address this with FLAIR, a system for long-horizon feeding which leverages the commonsense reasoning capabilities of foundation models, along with a library of parameterized skills, to plan and execute user-preferred and efficient bite sequences. In real-world evaluations across 6 highly realistic plates, we find that FLAIR can effectively tap into a library of dexterous skills for efficient plate clearance, while adhering to the diverse preferences of over 42 as evaluated in a user study. We finally demonstrate the real-world efficacy of our approach by deploying our system with an in-mouth bite transfer framework for successfully feeding a care recipient with mobility limitations.},
  archive   = {C_RSS},
  author    = {Rajat Kumar Jenamani and Priya Sundaresan and Maram Sakr and Tapomayukh Bhattacharjee and Dorsa Sadigh},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {FLAIR: Feeding via long-horizon AcquIsition of realistic dishes},
  url       = {https://www.roboticsproceedings.org/rss20/p031.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). The benefits of sound resound: An in-person
replication of the ability of character-like robot sound to improve
perceived social warmth. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p032.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While robot sound is known to impact perceptions of robots, little research to date has addressed the topic of robot sound. In particular, there are few in-person studies surrounding the topic. To expand existing robotic sound research, we conducted an in-person empirical study with $N=30$ participants, as a partial replication of a past online study. We sought to better understand the effects that character-like and functional sounds have on human teammates&#39; perceptions of a robot during a joint in-person task. Participants rated the robot with character-like sound as more socially warm compared against a no-added-sound condition; this result was akin to insights from our previous work that showed benefits of transformative robot sound for warmth and other factors across multiple robot platforms. Additional evidence newly presented in this work also suggests the increased localizability of robots with augmented sonic profiles. The partial replication in the results strengthens our lab&#39;s past findings on robot sound, especially as related to character-like robot sound&#39;s ability to improve perceived robot warmth. This work can help to inform designers and researchers who are interested in enhancing robot interactions via nonverbal robot expression.},
  archive   = {C_RSS},
  author    = {Nnamdi Nwagwu and Adeline Schneider and Ibrahim Syed and Brian J. Zhang and Naomi T. Fitter},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {The benefits of sound resound: An in-person replication of the ability of character-like robot sound to improve perceived social warmth},
  url       = {https://www.roboticsproceedings.org/rss20/p032.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Leveraging large language model for heterogeneous ad
hoc teamwork collaboration. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p033.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Compared with the widely investigated homogeneous multi-robot collaboration, heterogeneous robots with different capabilities can provide a more efficient and flexible collaboration for more complex tasks. In this paper, we consider a more challenging heterogeneous ad hoc teamwork collaboration problem where an ad hoc robot joins an existing heterogeneous team for a shared goal. Specifically, the ad hoc robot collaborates with unknown teammates without prior coordination, and it is expected to generate an appropriate cooperation policy to improve the efficiency of the whole team. To solve this challenging problem, we leverage the remarkable potential of the large language model (LLM) to establish a decentralized heterogeneous ad hoc teamwork collaboration framework that focuses on generating reasonable policy for an ad hoc robot to collaborate with original heterogeneous teammates. A training-free hierarchical dynamic planner is developed using the LLM together with the newly proposed Interactive Reflection of Thoughts (IRoT) method for the ad hoc agent to adapt to different teams. We also build a benchmark testing dataset to evaluate the proposed framework in the heterogeneous ad hoc multi-agent tidying-up task. Extensive comparison and ablation experiments are conducted in the benchmark to demonstrate the effectiveness of the proposed framework. We have also employed the proposed framework in physical robots in a real-world scenario. The experimental videos can be found at https://youtu.be/wHYP5T2WIp0.},
  archive   = {C_RSS},
  author    = {Xinzhu Liu and Peiyan Li and Wenju Yang and Di Guo and Huaping Liu},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Leveraging large language model for heterogeneous ad hoc teamwork collaboration},
  url       = {https://www.roboticsproceedings.org/rss20/p033.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). INTERPRET: Interactive predicate learning from
language feedback for generalizable task planning. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p034.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning abstract state representations and knowledge is crucial for long-horizon robot planning. We present InterPreT, an LLM-powered framework for robots to learn symbolic predicates from language feedback of human non-experts during embodied interaction. The learned predicates provide relational abstractions of the environment state, facilitating the learning of symbolic operators that capture action preconditions and effects. By compiling the learned predicates and operators into a PDDL domain on-the-fly, InterPreT allows effective planning toward arbitrary in-domain goals using a PDDL planner. In both simulated and real-world robot manipulation domains, we demonstrate that InterPreT reliably uncovers the key predicates and operators governing the environment dynamics. Although learned from simple training tasks, these predicates and operators exhibit strong generalization to novel tasks with significantly higher complexity. In the most challenging generalization setting, InterPreT attains success rates of 73% in simulation and 40% in the real world, substantially outperforming baseline methods.},
  archive   = {C_RSS},
  author    = {Muzhi Han and Yifeng Zhu and Song-Chun Zhu and Ying Nian Wu and Yuke Zhu},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {INTERPRET: Interactive predicate learning from language feedback for generalizable task planning},
  url       = {https://www.roboticsproceedings.org/rss20/p034.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Safe planning for articulated robots using
reachability-based obstacle avoidance with spheres. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p035.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generating safe motion plans in real-time is necessary for the wide-scale deployment of robots in unstructured and human-centric environments. These motion plans must be safe to ensure humans are not harmed and nearby objects are not damaged. However, they must also be generated in real-time to ensure the robot can quickly adapt to changes in the environment. Many trajectory optimization methods introduce heuristics that trade-off safety and real-time performance, which can lead to potentially unsafe plans. This paper addresses this challenge by proposing Safe Planning for Articulated Robots Using Reachability-based Obstacle Avoidance With Spheres (SPARROWS). SPARROWS is a receding-horizon trajectory planner that utilizes the combination of a novel reachable set representation and an exact signed distance function to generate provably-safe motion plans. At runtime, SPARROWS uses parameterized trajectories to compute reachable sets composed entirely of spheres that overapproximate the swept volume of the robot&#39;s motion. SPARROWS then performs trajectory optimization to select a safe trajectory that is guaranteed to be collision-free. We demonstrate that SPARROWS&#39; novel reachable set is significantly less conservative than previous approaches. We also demonstrate that SPARROWS outperforms a variety of state-of-the-art methods in solving challenging motion planning tasks in cluttered environments. Code will be released upon acceptance of this manuscript.},
  archive   = {C_RSS},
  author    = {Jonathan Michaux and Adam Li and Qingyi Chen and Che Chen and Ram Vasudevan},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Safe planning for articulated robots using reachability-based obstacle avoidance with spheres},
  url       = {https://www.roboticsproceedings.org/rss20/p035.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Motion planning in foliated manifolds using repetition
roadmap. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p036.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Numerous classes of robotics motion planning problems involve searching in constrained configuration spaces where the constraints change during different stages of the motion, and these kinds of motion planning problems are named multi-modal problems. The most common method to solve these problems is to represent them as a set of manifolds and search for a trajectory across them. Often, instead of using manifolds alone, foliated manifolds, which are a union of disjoint manifolds, are a better way to model the manipulation problem. However, the complexity of planning in foliated manifolds is significant due to the increased number of manifolds, hard task constraints, and complex environments. To tackle these challenges, we propose an efficient planning framework that leverages a dynamic roadmap structure to learn from accumulated experience acquired during previous planning attempts in similar foliated manifolds. When planning in a new foliated manifold, this experience, captured in configuration distributions and an atlas, which are tangential charts approximating the new manifold with constraints, is effectively utilized to guide motion planning. We demonstrate the framework&#39;s performance for manipulation problems with different foliated manifold structures in simulation and real-world scenarios. An open-source will be released soon.},
  archive   = {C_RSS},
  author    = {Jiaming Hu and Shrutheesh Raman Iyer and Jiawei Wang and Henrik I Christensen},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Motion planning in foliated manifolds using repetition roadmap},
  url       = {https://www.roboticsproceedings.org/rss20/p036.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Language-augmented symbolic planner for open-world
task planning. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p037.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Enabling robotic agents to perform complex long-horizon tasks has been a long-standing goal in robotics and artificial intelligence (AI). Despite the potential shown by large language models (LLMs), their planning capabilities remain limited to short-horizon tasks and they are unable to replace the symbolic planning approach. Symbolic planners, on the other hand, may encounter execution errors due to their common assumption of complete domain knowledge which is hard to manually prepare for an open-world setting. In this paper, we introduce a Language-Augmented Symbolic Planner (LASP) that integrates pre-trained LLMs to enable conventional symbolic planners to operate in an open-world environment where only incomplete knowledge of action preconditions, objects, and properties is initially available. In case of execution errors, LASP can utilize the LLM to diagnose the cause of the error based on the observation and interact with the environment to incrementally build up its knowledge base necessary for accomplishing the given tasks. Experiments demonstrate that LASP is proficient in solving planning problems in the open-world setting, performing well even in situations where there are multiple gaps in the knowledge.},
  archive   = {C_RSS},
  author    = {Guanqi Chen and Lei Yang and Ruixing Jia and Zhe Hu and Yizhou Chen and Wei Zhang and Wenping Wang and Jia Pan},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Language-augmented symbolic planner for open-world task planning},
  url       = {https://www.roboticsproceedings.org/rss20/p037.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Collision-affording point trees: SIMD-amenable nearest
neighbors for fast motion planning with pointclouds. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p038.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion planning against sensor data is often a critical bottleneck in real-time robot control. For sampling-based motion planners, which are effective for high-dimensional systems such as manipulators, the most time-intensive component is collision checking. We present a novel spatial data structure, the collision-affording point tree (CAPT): an exact representation of point clouds that accelerates collision-checking queries between robots and point clouds by an order of magnitude, with an average query time of less than 10 nanoseconds on 3D scenes comprising thousands of points. With the CAPT, sampling-based planners can generate valid, high-quality paths in under a millisecond, with total end-to-end computation time faster than 60 FPS, on a single thread of a consumer-grade CPU. We also present a point cloud filtering algorithm, based on space-filling curves, which reduces the number of points in a point cloud while preserving structure. Our approach enables robots to plan at real-time speeds in sensed environments, opening up potential uses of planning for high-dimensional systems in dynamic, changing, and unmodeled environments.},
  archive   = {C_RSS},
  author    = {Clayton Ramsey and Zachary Kingston and Wil Thomason and Lydia E Kavraki},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Collision-affording point trees: SIMD-amenable nearest neighbors for fast motion planning with pointclouds},
  url       = {https://www.roboticsproceedings.org/rss20/p038.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Homotopic path set planning for robot manipulation and
navigation. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p039.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses path set planning that yields important applications in robot manipulation and navigation such as path generation for deformable object keypoints and swarms. A path set refers to the collection of finite agent paths to represent the overall spatial path of a group of keypoints or a swarm, whose collective properties meet spatial and topological constraints. As opposed to planning a single path, simultaneously planning multiple paths with constraints poses nontrivial challenges in complex environments. This paper presents a systematic planning pipeline for homotopic path sets, a widely applicable path set class in robotics. An extended visibility check condition is first proposed to attain a sparse passage distribution amidst dense obstacles. Passage-aware optimal path planning compatible with sampling-based planners is then designed for single path planning with adjustable costs. Large accessible free space for path set accommodation can be achieved by the planned path while having a sufficiently short path length. After specifying the homotopic properties of path sets, path set generation based on deformable path transfer is proposed in an efficient centralized manner. The effectiveness of these methods is validated by extensive simulated and experimental results.},
  archive   = {C_RSS},
  author    = {Jing Huang},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Homotopic path set planning for robot manipulation and navigation},
  url       = {https://www.roboticsproceedings.org/rss20/p039.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Practice makes perfect: Planning to learning skill
parameter policies. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p040.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One promising approach towards effective robot decision making in complex, long-horizon tasks is to sequence together *parameterized skills*. We consider a setting where a robot is initially equipped with (1) a library of parameterized skills, (2) an AI planner for sequencing together the skills given a goal, and (3) a very general prior distribution for selecting skill parameters. Once deployed, the robot should rapidly and autonomously learn to improve its performance by specializing its skill parameter selection policy to the particular objects, goals, and constraints in its environment. In this work, we focus on the active learning problem of choosing which skills to *practice* to maximize expected future task success. We propose that the robot should *estimate* the competence of each skill, *extrapolate* the competence (asking: &quot;how much would the competence improve through practice?&quot;), and *situate* the skill in the task distribution through competence-aware planning. This approach is implemented within a fully autonomous system where the robot repeatedly plans, practices, and learns without any environment resets. Through experiments in simulation, we find that our approach learns effective parameter policies more sample-efficiently than several baselines. Experiments in the real-world demonstrate our approach’s ability to handle noise from perception and control and improve the robot’s ability to solve two long-horizon mobile-manipulation tasks after a few hours of autonomous practice. Project website: http://ees.csail.mit.edu},
  archive   = {C_RSS},
  author    = {Nishanth Kumar and Tom Silver and Willie McClinton and Linfeng Zhao and Stephen Proulx and Tomás Lozano-Pérez and Leslie Pack Kaelbling and Jennifer L. Barry},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Practice makes perfect: Planning to learning skill parameter policies},
  url       = {https://www.roboticsproceedings.org/rss20/p040.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). World models for general surgical grasping.
<em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p041.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Intelligent vision control systems for surgical robots should adapt to unknown and diverse objects while being robust to system disturbances. Previous methods did not meet these requirements due to mainly relying on pose estimation and feature tracking. We propose a world-model-based deep reinforcement learning framework “Grasp Anything for Surgery” (GAS), that learns a pixel-level visuomotor policy for surgical grasping, enhancing both generality and robustness. In particular, a novel method is proposed to estimate the values and uncertainties of depth pixels for a rigid-link object&#39;s inaccurate region based on the empirical prior of the object&#39;s size; both depth and mask images of task objects are encoded to a single compact 3-channel image (size: 64x64x3) by dynamically zooming in the mask regions, minimizing the information loss. The learned controller&#39;s effectiveness is extensively evaluated in simulation and in a real robot. Our learned visuomotor policy handles: i) unseen objects, including 5 types of target grasping objects and a robot gripper, in unstructured real-world surgery environments, and ii) disturbances in perception and control. Note that we are the first work to achieve a unified surgical control system that grasps diverse surgical objects using different robot grippers on real robots in complex surgery scenes (average success rate: 69%). Our system also demonstrates significant robustness across 6 conditions including background variation, target disturbance, camera pose variation, kinematic control error, image noise, and re-grasping after the gripped target object drops from the gripper. Videos and codes can be found on our project page: https://linhongbin.github.io/gas/.},
  archive   = {C_RSS},
  author    = {Hongbin Lin},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {World models for general surgical grasping},
  url       = {https://www.roboticsproceedings.org/rss20/p041.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). SpringGrasp: Synthesizing compliant, dexterous grasps
under shape uncertainty. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p042.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generating stable and robust grasps on arbitrary objects is critical for dexterous robotic hands, marking a significant step towards advanced dexterous manipulation. Previous studies have mostly focused on improving differentiable grasping metrics with the assumption of precisely known object geometry. However, shape uncertainty is ubiquitous due to noisy and partial shape observations, which introduce challenges in grasp planning. We propose SpringGrasp planner, a planner that considers uncertain observations of the object surface for synthesizing compliant dexterous grasps. A compliant dexterous grasp could minimize the effect of unexpected contact with the object, leading to a more stable grasp with shape-uncertain objects. We introduce an analytical and differentiable metric, SpringGrasp metric, that evaluates the dynamic behavior of the entire compliant grasping process. Planning with SpringGrasp planner, our method achieves a grasp success rate of 89% from two viewpoints and 84% from a single viewpoints in experiment with a real robot on 14 common objects. Compared with a force-closure-based planner, our method achieves at least 18% higher grasp success rate.},
  archive   = {C_RSS},
  author    = {Sirui Chen and Jeannette Bohg and Karen Liu},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {SpringGrasp: Synthesizing compliant, dexterous grasps under shape uncertainty},
  url       = {https://www.roboticsproceedings.org/rss20/p042.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). DexCap: Scalable and portable mocap data collection
system for dexterous manipulation. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p043.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Imitation learning from human hand motion data presents a promising avenue for imbuing robots with human-like dexterity in real-world manipulation tasks. Despite this potential, substantial challenges persist, particularly with the portability of existing hand motion capture (mocap) systems and the complexity of translating mocap data into effective robotic policies. To tackle these issues, we introduce DexCap, a portable hand motion capture system, alongside DexIL, a novel imitation algorithm for training dexterous robot skills directly from human hand mocap data. DexCap offers precise, occlusion-resistant tracking of wrist and finger motions based on SLAM and electromagnetic field together with 3D observations of the environment. Utilizing this rich dataset, DexIL employs inverse kinematics and point cloud-based imitation learning to seamlessly replicate human actions with robot hands. Beyond direct learning from human motion, DexCap also offers an optional human-in-the-loop correction mechanism during policy rollouts to refine and further improve task performance. Through extensive evaluation across six challenging dexterous manipulation tasks, our approach not only demonstrates superior performance but also showcases the system&#39;s capability to effectively learn from in-the-wild mocap data, paving the way for future data collection methods in the pursuit of human-level robot dexterity.},
  archive   = {C_RSS},
  author    = {Chen Wang and Haochen Shi and Weizhuo Wang and Ruohan Zhang and Li Fei-Fei and Karen Liu},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {DexCap: Scalable and portable mocap data collection system for dexterous manipulation},
  url       = {https://www.roboticsproceedings.org/rss20/p043.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). GRaCE: Balancing multiple criteria to achieve stable,
collision-free, and functional grasps. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p044.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Abstract—This paper addresses the multi-faceted problem of robot grasping, where multiple criteria may conflict and differ in importance. We introduce a probabilistic framework, Grasp Ranking and Criteria Evaluation (GRaCE), which employs hierarchical rule-based logic and a rank-preserving utility function for grasps based on various criteria such as stability, kinematic constraints, and goal oriented functionalities. GRaCE&#39;s probabilistic nature means the framework handles uncertainty in a principled manner, i.e., the method is able to leverage the probability that a given criteria is satisfied. Additionally, we propose GRaCE-OPT, a hybrid optimization strategy that combines gradient-based and gradient-free methods to effectively navigate the complex, non-convex utility function. Experimental results in both simulated and real-world scenarios show that GRaCE requires fewer samples to achieve comparable or superior performance relative to existing methods. The modular architecture of GRaCE allows for easy customization and adaptation to specific application needs. Code and implementation details can be found online at https://github.com/clear-nus/GRaCE.},
  archive   = {C_RSS},
  author    = {Tasbolat Taunyazov and Kelvin Lin and Harold Soh},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {GRaCE: Balancing multiple criteria to achieve stable, collision-free, and functional grasps},
  url       = {https://www.roboticsproceedings.org/rss20/p044.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Universal manipulation interface: In-the-wild robot
teaching without in-the-wild robots. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p045.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present Universal Manipulation Interface (UMI) -- a data collection and policy learning framework that allows direct skill transfer from in-the-wild human demonstrations to deployable robot policies. UMI employs hand-held grippers coupled with careful interface design to enable portable, low-cost, and information-rich data collection for challenging bimanual and dynamic manipulation demonstrations. To facilitate deployable policy learning, UMI incorporates a carefully designed policy interface with inference-time latency matching and a relative-trajectory action representation. The resulting learned policies are hardware-agnostic and deployable across multiple robot platforms. Equipped with these features, UMI framework unlocks new robot manipulation capabilities, allowing zero-shot generalizable dynamic, bimanual, precise, and long-horizon behaviors, by only changing the training data for each task. We demonstrate UMI’s versatility and efficacy with comprehensive real-world experiments, where policies learned via UMI zero-shot generalize to novel environments and objects when trained on diverse human demonstrations. UMI&#39;s hardware and software system along with our in-the-wild dataset will be open-sourced.},
  archive   = {C_RSS},
  author    = {Cheng Chi and Zhenjia Xu and Chuer Pan and Eric Cousineau and Benjamin Burchfiel and Siyuan Feng and Russ Tedrake and Shuran Song},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Universal manipulation interface: In-the-wild robot teaching without in-the-wild robots},
  url       = {https://www.roboticsproceedings.org/rss20/p045.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Learning any-view 6DoF robotic grasping in cluttered
scenes via neural surface rendering. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p046.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A significant challenge for real-world robotic manipulation is the effective 6DoF grasping of objects in cluttered scenes from any single viewpoint without needing additional scene exploration. This work re-interprets grasping as rendering and introduces NeuGraspNet, a novel method for 6DoF grasp detection that leverages advances in neural volumetric representations and surface rendering. We encode the interaction between a robot&#39;s end-effector and an object&#39;s surface by jointly learning to render the local object surface and learning grasping functions in a shared feature space. Our approach uses global (scene-level) features for grasp generation and local (grasp-level) neural surface features for grasp evaluation. This enables effective, fully implicit 6DoF grasp quality prediction, even in partially observed scenes. NeuGraspNet operates on random viewpoints, common in mobile manipulation scenarios, and outperforms existing implicit and semi-implicit grasping methods. We demonstrate the real-world applicability of the method with a mobile manipulator robot, grasping in open cluttered spaces. Project website at: https://sites.google.com/view/neugraspnet},
  archive   = {C_RSS},
  author    = {Snehal Jauhri and Ishikaa Lunawat and Georgia Chalvatzaki},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Learning any-view 6DoF robotic grasping in cluttered scenes via neural surface rendering},
  url       = {https://www.roboticsproceedings.org/rss20/p046.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Demonstrating adaptive mobile manipulation in retail
environments. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p047.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although autonomous robots have great potential to boost efficiency and throughput across the whole retail chain, they are mostly being deployed in large warehouses and distribution centers. Deploying robots in stores with customers, such as supermarkets, requires substantially more development efforts since they need to safely operate around customers and reliably cope with various uncertainties and disturbances, such as misplaced products. We present our recent efforts in developing a mobile manipulator platform for order picking in realistic supermarket settings. Our robot platform uses state-of-the-art perception and planning algorithms to robustly pick items in the presence of disturbances. In particular, it successfully demonstrates adaptive decision making and rapid replanning. Our robot allows adding new products and teaching new picking maneuvers from demonstrations. We validated our robot in a recreated supermarket in our lab and in a test supermarket of a large Dutch retailer. Our results show how our robot successfully recovers from various disturbances, including misplaced products, errors in picking, and from human interaction. We summarize our lessons learned to bring autonomous robots into real retail environments with customers.},
  archive   = {C_RSS},
  author    = {Max Spahn and Corrado Pezzato and Chadi Salmi and Rick Dekker and Cong Wang and Christian Pek and Jens Kober and Javier Alonso-Mora and Carlos Hernandez Corbato and Martijn Wisse},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Demonstrating adaptive mobile manipulation in retail environments},
  url       = {https://www.roboticsproceedings.org/rss20/p047.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Diffusion meets DAgger: Supercharging eye-in-hand
imitation learning. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p048.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A common failure mode for policies trained with imitation is compounding execution errors at test time. When the learned policy encounters states that are not present in the expert demonstrations, the policy fails, leading to degenerate behavior. The Dataset Aggregation, or DAgger approach to this problem simply collects more data to cover these failure states. However, in practice, this is often prohibitively expensive. In this work, we propose Diffusion Meets DAgger (DMD), a method that reaps the benefits of DAgger but without the cost, for eye-in-hand imitation learning problems. Instead of *collecting* new samples to cover out-of-distribution states, DMD uses recent advances in diffusion models to *synthesize* these samples. This leads to robust performance from few demonstrations. We compare DMD against behavior cloning baseline across four tasks: pushing, stacking, pouring, and hanging a shirt. In pushing, DMD achieves 80% success rate with as few as 8 expert demonstrations, where naive behavior cloning reaches only 20%. In stacking, DMD succeeds on average 92% of the time across 5 cups, versus 40% for BC. When pouring coffee beans, DMD transfers to another cup successfully 80% of the time. Finally, DMD attains 90% success rate for hanging shirt on a clothing rack.},
  archive   = {C_RSS},
  author    = {Xiaoyu Zhang and Matthew Chang and Pranav Kumar and Saurabh Gupta},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Diffusion meets DAgger: Supercharging eye-in-hand imitation learning},
  url       = {https://www.roboticsproceedings.org/rss20/p048.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). RT-h: Action hierarchies using language. <em>RSS</em>.
(<a href="https://www.roboticsproceedings.org/rss20/p049.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Language provides a way to break down complex concepts into digestible pieces. Recent works in robot imitation learning have proposed learning language-conditioned policies that predict actions given visual observations and the high-level task specified in language. These methods leverage the structure of natural language to share data between semantically similar tasks (e.g., &quot;pick coke can&quot; and &quot;pick an apple&quot;) in multi-task datasets. However, as tasks become more semantically diverse (e.g., &quot;pick coke can&quot; and &quot;pour cup&quot;), sharing data between tasks becomes harder and thus learning to map high-level tasks to actions requires substantially more demonstration data. To bridge this divide between tasks and actions, our insight is to teach the robot the language of actions, describing low-level motions with more fine-grained phrases like &quot;move arm forward&quot; or &quot;close gripper&quot;. Predicting these *language motions* as an intermediate step between high-level tasks and actions forces the policy to learn the shared structure of low-level motions across seemingly disparate tasks. Furthermore, a policy that is conditioned on language motions can easily be *corrected* during execution through human-specified language motions. This enables a new paradigm for flexible policies that can learn from human intervention in language. Our method RT-H builds an *action hierarchy* using language motions: it first learns to predict language motions, and conditioned on this along with the high-level task, it then predicts actions, using visual context at all stages. Experimentally we show that RT-H leverages this language-action hierarchy to learn policies that are more robust and flexible by effectively tapping into multi-task datasets. We show that these policies not only allow for responding to language interventions, but can also learn from such interventions and outperform methods that learn from teleoperated interventions. Our website and videos are found at https://rt-hierarchy.github.io.},
  archive   = {C_RSS},
  author    = {Suneel Belkhale and Tianli Ding and Ted Xiao and Pierre Sermanet and Quan Vuong and Jonathan Tompson and Yevgen Chebotar and Debidatta Dwibedi and Dorsa Sadigh},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {RT-H: Action hierarchies using language},
  url       = {https://www.roboticsproceedings.org/rss20/p049.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). RoboCasa: Large-scale simulation of household tasks
for generalist robots. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p050.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advancements in Artificial Intelligence (AI) have largely been propelled by scaling. In Robotics, scaling is hindered by the lack of access to massive robot datasets. We advocate using realistic physical simulation as a means to scale environments, tasks, and datasets for robot learning methods. We present RoboCasa, a large-scale simulation framework for training generalist robots in everyday environments. RoboCasa features realistic and diverse scenes focusing on kitchen environments. We provide thousands of 3D assets across over 150 object categories and dozens of interactable furniture and appliances. We enrich the realism and diversity of our simulation with generative AI tools, such as object assets from text-to-3D models and environment textures from text-to-image models. We design a set of 100 tasks for systematic evaluation, including composite tasks generated by the guidance of large language models. To facilitate learning, we provide high-quality human demonstrations and integrate automated trajectory generation methods to substantially enlarge our datasets with minimal human burden. Our experiments show a clear scaling trend in using synthetically generated robot data for large-scale imitation learning and show great promise in harnessing simulation data in real-world tasks. Videos and open-source code are available at https://robocasa.ai/.},
  archive   = {C_RSS},
  author    = {Soroush Nasiriany and Abhiram Maddukuri and Lance Zhang and Adeet Parikh and Aaron Lo and Abhishek Joshi and Ajay Mandlekar and Yuke Zhu},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {RoboCasa: Large-scale simulation of household tasks for generalist robots},
  url       = {https://www.roboticsproceedings.org/rss20/p050.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Render and diffuse: Aligning image and action spaces
for diffusion-based behaviour cloning. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p051.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the field of Robot Learning, the complex mapping between high-dimensional observations such as RGB images and low-level robotic actions, two inherently very different spaces, constitutes a complex learning problem, especially with limited amounts of data. In this work, we introduce Render and Diffuse (R&amp;D) a method that unifies low-level robot actions and RGB observations within the image space using virtual renders of the 3D model of the robot. Using this joint observation-action representation it computes low-level robot actions using a learnt diffusion process that iteratively updates the virtual renders of the robot. This space unification simplifies the learning problem and introduces inductive biases that are crucial for sample efficiency and spatial generalisation. We thoroughly evaluate several variants of R&amp;D in simulation and showcase their applicability on six everyday tasks in the real world. Our results show that R&amp;D exhibits strong spatial generalisation capabilities and is more sample efficient than more common image-to-action methods.},
  archive   = {C_RSS},
  author    = {Vitalis Vosylius and Younggyo Seo and Jafar Uruç and Stephen James},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Render and diffuse: Aligning image and action spaces for diffusion-based behaviour cloning},
  url       = {https://www.roboticsproceedings.org/rss20/p051.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Vid2Robot: End-to-end video-conditioned policy
learning with cross-attention transformers. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p052.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large-scale multi-task robotic manipulation systems often rely on text to specify the task. In this work, we explore whether a robot can learn by observing humans. To do so, the robot must understand a person&#39;s intent and perform the inferred task despite differences in the embodiments and environments. We introduce Vid2Robot, an end-to-end video-conditioned policy that takes human videos demonstrating manipulation tasks as input and producing robot actions. Our model is trained with a large dataset of prompt video-robot trajectory pairs to learn unified representations of human and robot actions from videos. Vid2Robot uses cross-attention transformer layers between video features and the current robot state to produce the actions and perform the same task as shown in the video. We use auxiliary contrastive losses to align the prompt and robot video representations for better policies. We evaluate Vid2Robot on real-world robots and observe over 20% improvement over BC-Z when using human prompt videos. Further, we also show cross-object motion transfer ability that enables video-conditioned policies to transfer a motion observed on one object in the prompt video to another object in the robot&#39;s own environment. Videos available at https://vid2robot.github.io},
  archive   = {C_RSS},
  author    = {Vidhi Jain and Maria Attarian and Nikhil J Joshi and Ayzaan Wahid and Danny Driess and Quan Vuong and Pannag R Sanketi and Pierre Sermanet and Stefan Welker and Christine Chan and Igor Gilitschenski and Yonatan Bisk and Debidatta Dwibedi},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Vid2Robot: End-to-end video-conditioned policy learning with cross-attention transformers},
  url       = {https://www.roboticsproceedings.org/rss20/p052.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Offline imitation learning through graph search and
retrieval. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p054.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Imitation learning is a powerful machine learning algorithm for a robot to acquire manipulation skills. Nevertheless, many real-world manipulation tasks involve precise and dexterous robot-object interactions, which make it difficult for humans to collect high-quality expert demonstrations. As a result, a robot has to learn skills from suboptimal demonstrations and unstructured interactions, which remains a key challenge. Existing works typically use offline deep reinforcement learning~(RL) to solve this challenge, but in practice these algorithms are unstable and fragile due to the deadly triad issue. To overcome this problem, we propose GSR, a simple yet effective algorithm that learns from suboptimal demonstrations through Graph Search and Retrieval. We first use pretrained representation to organize the interaction experience into a graph and perform a graph search to calculate the values of different behaviors. Then, we apply a retrieval-based procedure to identify the best behavior (actions) on each state and use behavior cloning to learn that behavior. We evaluate our method in both simulation and real-world robotic manipulation tasks with complex visual inputs, covering various precise and dexterous manipulation skills with objects of different physical properties. GSR can achieve a 10% to 30% higher success rate and over 30% higher proficiency compared to baselines.},
  archive   = {C_RSS},
  author    = {Zhao-Heng Yin and Pieter Abbeel},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Offline imitation learning through graph search and retrieval},
  url       = {https://www.roboticsproceedings.org/rss20/p054.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). RVT-2: Learning precise manipulation from few
demonstrations. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p055.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we study how to build a robotic system that can solve multiple 3D manipulation tasks given language instructions. To be useful in industrial and household domains, such a system should be capable of learning new tasks with few demonstrations and solving them precisely. Prior works, like PerAct and RVT, have studied this problem, however, they often struggle with tasks requiring high precision. We study how to make them more effective, precise, and fast. Using a combination of architectural and system-level improvements, we propose RVT-2, a multitask 3D manipulation model that is 6X faster in training and 2X faster in inference than its predecessor RVT. RVT-2 achieves a new state-of-the-art on RLBench, improving the success rate from 65% to 82%. RVT-2 is also effective in the real world, where it can learn tasks requiring high precision, like picking up and inserting plugs, with just 10 demonstrations. Visual results, code, and trained model are provided at: https://robotic-view-transformer-2.github.io/.},
  archive   = {C_RSS},
  author    = {Ankit Goyal and Valts Blukis and Jie Xu and Yijie Guo and Yu-Wei Chao and Dieter Fox},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {RVT-2: Learning precise manipulation from few demonstrations},
  url       = {https://www.roboticsproceedings.org/rss20/p055.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Imitation bootstrapped reinforcement learning.
<em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p056.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite the considerable potential of reinforcement learning (RL), robotics control tasks predominantly rely on imitation learning (IL) due to its better sample efficiency. However, it is costly to collect comprehensive expert demonstrations that enable IL to generalize to all possible scenarios, and any distribution shift would require recollecting data for finetuning. Therefore, RL is appealing if it can build upon IL as an efficient autonomous self-improvement procedure. We propose _imitation bootstrapped reinforcement learning_ (IBRL), a novel framework for sample-efficient RL with demonstrations that first trains an IL policy on the provided demonstrations and then uses it to propose alternative actions for both online exploration and bootstrapping target values. Compared to prior works that oversample the demonstrations or regularize RL with additional imitation loss, IBRL is able to utilize high quality actions from IL policies since the beginning of training, which greatly accelerates exploration and training efficiency. We evaluate IBRL on 6 simulation and 3 real-world tasks spanning various difficulty levels. IBRL significantly outperforms prior methods and the improvement is particularly more prominent in harder tasks.},
  archive   = {C_RSS},
  author    = {Hengyuan Hu and Suvir Mirchandani and Dorsa Sadigh},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Imitation bootstrapped reinforcement learning},
  url       = {https://www.roboticsproceedings.org/rss20/p056.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Rethinking robustness assessment: Adversarial attacks
on learning-based quadrupedal locomotion controllers. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p057.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Legged locomotion has recently achieved remarkable success with the progress of machine learning techniques, especially deep reinforcement learning (RL). Controllers employing neural networks have demonstrated empirical and qualitative robustness against real-world uncertainties, including sensor noise and external perturbations. However, formally investigating the vulnerabilities of these locomotion controllers remains a challenge. This difficulty arises from the requirement to pinpoint vulnerabilities across a long-tailed distribution within a high-dimensional, temporally sequential space. As a first step towards quantitative verification, we propose a computational method that leverages sequential adversarial attacks to identify weaknesses in learned locomotion controllers. Our research demonstrates that, even state-of-the-art robust controllers can fail significantly under well-designed, low-magnitude adversarial sequence. Through experiments in simulation and on the real robot, we validate our approach&#39;s effectiveness, and we illustrate how the results it generates can be used to robustify the original policy and offer valuable insights into the safety of these black-box policies.},
  archive   = {C_RSS},
  author    = {Fan Shi and Chong Zhang and Takahiro Miki and Joonho Lee and Marco Hutter and Stelian Coros},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Rethinking robustness assessment: Adversarial attacks on learning-based quadrupedal locomotion controllers},
  url       = {https://www.roboticsproceedings.org/rss20/p057.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Advancing humanoid locomotion: Mastering challenging
terrains with denoising world model learning. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p058.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humanoid robots, with their human-like skeletal structure, are especially suited for tasks in human-centric environments. However, this structure is accompanied by additional challenges in locomotion controller design, especially in complex real-world environments. As a result, existing humanoid robots are limited to relatively simple terrains, either with model-based control or model-free reinforcement learning. In this work, we introduce Denoising World Model Learning (DWL), an end-to-end reinforcement learning framework for humanoid locomotion control, which demonstrates the world&#39;s first humanoid robot to master real-world challenging terrains such as snowy and inclined land in the wild, up and down stairs, and extremely uneven terrains. All scenarios run the same learned neural network with zero-shot sim-to-real transfer, indicating the superior robustness and generalization capability of the proposed method.},
  archive   = {C_RSS},
  author    = {Xinyang Gu and Yen-Jen Wang and Xiang Zhu and Chengming Shi and Yanjiang Guo and Yichen Liu and Jianyu Chen},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Advancing humanoid locomotion: Mastering challenging terrains with denoising world model learning},
  url       = {https://www.roboticsproceedings.org/rss20/p058.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Agile but safe: Learning collision-free high-speed
legged locomotion. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p059.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Legged robots navigating cluttered environments must be jointly agile for efficient task execution and safe to avoid collisions with obstacles or humans. Existing studies either develop conservative controllers (&lt; 1.0 m/s) to ensure safety, or focus on agility without considering potentially fatal collisions. This paper introduces Agile But Safe (ABS), a learning-based control framework that enables agile and collision-free locomotion for quadrupedal robots. ABS involves an agile policy to execute agile motor skills amidst obstacles and a recovery policy to prevent failures, collaboratively achieving high-speed and collision-free navigation. The policy switch in ABS is governed by a learned control-theoretic reach-avoid value network, which also guides the recovery policy as an objective function, thereby safeguarding the robot in a closed loop. The training process involves the learning of the agile policy, the reach-avoid value network, the recovery policy, and an exteroception representation network, all in simulation. These trained modules can be directly deployed in the real world with onboard sensing and computation, leading to high-speed and collision-free navigation in confined indoor and outdoor spaces with both static and dynamic obstacles.},
  archive   = {C_RSS},
  author    = {Tairan He and Chong Zhang and Wenli Xiao and Guanqi He and Changliu Liu and Guanya Shi},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Agile but safe: Learning collision-free high-speed legged locomotion},
  url       = {https://www.roboticsproceedings.org/rss20/p059.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). RL2AC: Reinforcement learning-based rapid online
adaptive control for legged robot robust locomotion. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p060.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dynamic fast adaptation is one of the basic capabilities that enables the animals to timely and properly adjust its locomotion reacting to the unpredictable changes. Such capability is also essential for the quadruped robot, when working in the unforseen environment. While reinforcement learning (RL) has achieved a significant progress in locomotion control, rapid adaptation to the model uncertainties remains a challenge. In this paper, we seek to ascertain the control mechanism behind the locomotion RL policy, from which we propose a new RL-based Rapid onLine Adaptive Control (RL2AC) algorithm to complementarily combine the RL policy and the adaptive control together. RL2AC is run at a frequency of 1000Hz without the need for simultaneous training with RL. It presents a strong capability against the external disturbances or the sim-to-real gap, resulting in a robust locomotion, which is achieved through proper torque compensation derived from a novel adaptive controller. Various simulation and experiments have demonstrated the effectiveness of the proposed RL2AC against the heavy load, disturbances acted on one leg, lateral torque, sim-to-real gap and various terrains.},
  archive   = {C_RSS},
  author    = {Shangke Lyu and Xin Lang and Han Zhao and Hongyin Zhang and Pengxiang Ding and Donglin Wang},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {RL2AC: Reinforcement learning-based rapid online adaptive control for legged robot robust locomotion},
  url       = {https://www.roboticsproceedings.org/rss20/p060.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). HumanoidBench: Simulated humanoid benchmark for
whole-body locomotion and manipulation. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p061.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humanoid robots hold great promise in assisting humans in diverse environments and tasks, due to their flexibility and adaptability leveraging human-like morphology. However, research in humanoid robots is often bottlenecked by the costly and fragile hardware setups. To accelerate algorithmic research in humanoid robots, we present a high-dimensional, simulated robot learning benchmark, HumanoidBench, featuring a humanoid robot equipped with dexterous hands and a variety of challenging whole-body manipulation and locomotion tasks. Our findings reveal that state-of-the-art reinforcement learning algorithms struggle with most tasks, whereas a hierarchical learning baseline achieves superior performance when supported by robust low-level policies, such as walking or reaching. With HumanoidBench, we provide the robotics community with a platform to identify the challenges arising when solving diverse tasks with humanoid robots, facilitating prompt verification of algorithms and ideas. The open-source code is available at https://humanoid- bench.github.io.},
  archive   = {C_RSS},
  author    = {Carmelo Sferrazza and Dun-Ming Huang and Xingyu Lin and Youngwoon Lee and Pieter Abbeel},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {HumanoidBench: Simulated humanoid benchmark for whole-body locomotion and manipulation},
  url       = {https://www.roboticsproceedings.org/rss20/p061.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). MOKA: Open-world robotic manipulation through
mark-based visual prompting. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p062.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Open-world generalization requires robotic systems to have a profound understanding of the physical world and the user command to solve diverse and complex tasks. While the recent advancement in vision-language models (VLMs) has offered unprecedented opportunities to solve open-world problems, how to leverage their capabilities to control robots remains a grand challenge. In this paper, we introduce Marking Open-world Keypoint Affordances (MOKA), an approach that employs VLMs to solve robotic manipulation tasks specified by free-form language instructions. Central to our approach is a compact point-based representation of affordance, which bridges the VLM’s predictions on observed images and the robot’s actions in the physical world. By prompting the pre-trained VLM, our approach utilizes the VLM’s commonsense knowledge and concept understanding acquired from broad data sources to predict affordances and generate motions. To facilitate the VLM’s reasoning in zero-shot and few-shot manners, we propose a visual prompting technique that annotates marks on images, converting affordance reasoning into a series of visual question-answering problems that are solvable by the VLM. We further explore methods to enhance performance with robot experiences collected by MOKA through in-context learning and policy distillation. We evaluate and analyze MOKA’s performance on various table-top manipulation tasks including tool use, deformable body manipulation, and object rearrangement.},
  archive   = {C_RSS},
  author    = {Kuan Fang and Fangchen Liu and Pieter Abbeel and Sergey Levine},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {MOKA: Open-world robotic manipulation through mark-based visual prompting},
  url       = {https://www.roboticsproceedings.org/rss20/p062.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Collaborative planar pushing of polytopic objects with
multiple robots in complex scenes. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p063.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pushing is a simple yet effective skill for robots to interact with and further change the environment. Related work has been mostly focused on utilizing it as a non-prehensile manipulation primitive for a robotic manipulator. However, it can also be beneficial for low-cost mobile robots that are not equipped with a manipulator. This work tackles the general problem of controlling a team of mobile robots to push collaboratively polytopic objects within complex obstacle-cluttered environments. It incorporates several characteristic challenges for contact-rich tasks such as the hybrid switching among different contact modes and under-actuation due to constrained contact forces. The proposed method is based on hybrid optimization over a sequence of possible modes and the associated pushing forces, where (i) a set of sufficient modes is generated with a multi-directional feasibility estimation, based on quasi-static analyses for general objects and any number of robots; (ii) a hierarchical hybrid search algorithm is designed to iteratively decompose the navigation path via arc segments and select the optimal parameterized mode; and (iii) a nonlinear model predictive controller is proposed to track the desired pushing velocities adaptively online for each robot. The proposed framework is complete under mild assumptions. Its efficiency and effectiveness are validated in high-fidelity simulations and hardware experiments. Robustness to motion and actuation uncertainties is also demonstrated.},
  archive   = {C_RSS},
  author    = {Zili Tang and Yuming Feng and Meng Guo},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Collaborative planar pushing of polytopic objects with multiple robots in complex scenes},
  url       = {https://www.roboticsproceedings.org/rss20/p063.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). AutoMate: Specialist and generalist assembly policies
over diverse geometries. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p064.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic assembly for high-mixture settings requires adaptivity to diverse parts and poses, which is an open challenge. Meanwhile, in other areas of robotics, large models and sim-to-real have led to tremendous progress. Inspired by such work, we present AutoMate, a learning framework and system that consists of 4 parts: 1) a dataset of 100 assemblies compatible with simulation and the real world, along with parallelized simulation environments for policy learning, 2) a novel simulation-based approach for learning specialist (i.e., part-specific) policies and generalist (i.e., unified) assembly policies, 3) demonstrations of specialist policies that individually solve 80 assemblies with ≈80%+ success rates in simulation, as well as a generalist policy that jointly solves 20 assemblies with an 80%+ success rate, and 4) zero-shot sim-to-real transfer that achieves similar (or better) performance than simulation, including on perception-initialized assembly. The key methodological takeaway is that a union of diverse algorithms from manufacturing engineering, character animation, and time-series analysis provides a generic and robust solution for a diverse range of robotic assembly problems.To our knowledge, AutoMate provides the first simulation-based framework for learning specialist and generalist policies over a wide range of assemblies, as well as the first system demonstrating zero-shot sim-to-real transfer over such a range.},
  archive   = {C_RSS},
  author    = {Bingjie Tang and Iretiayo Akinola and Jie Xu and Bowen Wen and Ankur Handa and Karl Van Wyk and Dieter Fox and Gaurav S. Sukhatme and Fabio Ramos and Yashraj Narang},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {AutoMate: Specialist and generalist assembly policies over diverse geometries},
  url       = {https://www.roboticsproceedings.org/rss20/p064.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). An abstract theory of sensor eventification.
<em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p065.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unlike traditional cameras, event cameras measure changes in light intensity and report differences. This paper examines the conditions necessary for other traditional sensors to admit eventified versions that provide adequate information despite outputting only changes. The requirements depend upon the regularity of the signal space, which we show may depend on several factors including structure arising from the interplay of the robot and its environment, the input–output computation needed to achieve its task, as well as the specific mode of access (synchronous, asynchronous, polled, triggered). Also, there are further notions of stability (or non-oscillatory behavior) as desiderata. This paper contributes theory and algorithms (plus a hardness result) that addresses these considerations while developing several elementary robot examples along the way.},
  archive   = {C_RSS},
  author    = {Yulin Zhang and Dylan Shell},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {An abstract theory of sensor eventification},
  url       = {https://www.roboticsproceedings.org/rss20/p065.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Octopi: Object property reasoning with large
tactile-language models. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p066.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Physical reasoning is important for effective robot manipulation. Recent work has investigated both vision and language modalities for physical reasoning; vision can reveal information about objects in the environment and language serves as an abstraction and communication medium for additional context. Although these works have demonstrated success on a variety of physical reasoning tasks, they are limited to physical properties that can be inferred from visual or language inputs. In this work, we investigate combining tactile perception with language, which enables embodied systems to obtain physical properties through interaction and apply commonsense reasoning. We contribute a new dataset PhysiCLeAR, which comprises both physical/property reasoning tasks and annotated tactile videos obtained using a GelSight tactile sensor. We then introduce Octopi, a system that leverages both tactile representation learning and large vision-language models to predict and reason about tactile inputs with minimal language fine-tuning. Our evaluations on PhysiCLeAR show that Octopi is able to effectively use intermediate physical property predictions to improve its performance on various tactile-related tasks. PhysiCLeAR and Octopi are available at https://github.com/clear-nus/octopi.},
  archive   = {C_RSS},
  author    = {Samson Yu and Lin Kelvin and Anxing Xiao and Jiafei Duan and Harold Soh},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Octopi: Object property reasoning with large tactile-language models},
  url       = {https://www.roboticsproceedings.org/rss20/p066.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). 3D diffusion policy: Generalizable visuomotor policy
learning via simple 3D representations. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p067.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Imitation learning provides an efficient way to teach robots dexterous skills; however, learning complex skills robustly and generalizablely usually consumes large amounts of human demonstrations. To tackle this challenging problem, we present 3D Diffusion Policy (DP3), a novel visual imitation learning approach that incorporates the power of 3D visual representations into diffusion policies, a class of conditional action generative models. The core design of DP3 is the utilization of a compact 3D visual representation, extracted from sparse point clouds with an efficient point encoder. In our experiments involving 72 simulation tasks, DP3 successfully handles most tasks with just 10 demonstrations and surpasses baselines with a 24.2% relative improvement. In 4 real robot tasks, DP3 demonstrates precise control with a high success rate of 85%, given only 40 demonstrations of each task, and shows excellent generalization abilities in diverse aspects, including space, viewpoint, appearance, and instance. Interestingly, in real robot experiments, DP3 rarely violates safety requirements, in contrast to baseline methods which frequently do, necessitating human intervention. Our extensive evaluation highlights the critical importance of 3D representations in real-world robot learning. Code and videos are available on https://3d-diffusion-policy.github.io .},
  archive   = {C_RSS},
  author    = {Yanjie Ze and Gu Zhang and Kangning Zhang and Chenyuan Hu and Muhan Wang and Huazhe Xu},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {3D diffusion policy: Generalizable visuomotor policy learning via simple 3D representations},
  url       = {https://www.roboticsproceedings.org/rss20/p067.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). HRP: Human affordances for robotic pre-training.
<em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p068.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In order to *generalize* to various tasks in the wild, robotic agents will need a suitable representation (i.e., vision network) that enables the robot to predict optimal actions given high dimensional vision inputs. However, learning such a representation requires an extreme amount of diverse training data, which is prohibitively expensive to collect on a real robot. How can we overcome this problem? Instead of collecting more robot data, this paper proposes using internet-scale, human videos to extract &quot;affordances,&quot; both at the environment and agent level, and distill them into a pre-trained representation. We present a simple framework for pre-training representations on hand, object, and contact &quot;affordance labels&quot; that highlight relevant objects in images and how to interact with them. These affordances are automatically extracted from human video data (with the help of off-the-shelf computer vision modules) and used to fine-tune existing representations. Our approach can efficiently fine-tune *any* existing representation, and results in models with stronger downstream robotic performance across the board. We experimentally demonstrate (using 3000+ robot trials) that this affordance pre-training scheme boosts performance by a minimum of 15% on 5 real-world tasks, which consider three diverse robot morphologies (including a dexterous hand). Unlike prior works in the space, these representations improve performance across 3 different camera views. Quantitatively, we find that our approach leads to higher levels of generalization in out-of-distribution settings. Videos of our final policies and all code/weights/data can be found on our website: https://www.cs.cmu.edu/~data4robotics/hrp/},
  archive   = {C_RSS},
  author    = {Mohan Kumar Srirama and Sudeep Dasari and Shikhar Bahl and Abhinav Gupta},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {HRP: Human affordances for robotic pre-training},
  url       = {https://www.roboticsproceedings.org/rss20/p068.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). MIRAGE: Cross-embodiment zero-shot policy transfer
with cross-painting. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p069.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to reuse collected data and transfer trained policies between robots could alleviate the burden of additional data collection and training. While existing approaches such as pretraining plus finetuning and co-training show promise, they do not generalize to robots unseen in training. Focusing on common robot arms with similar workspaces and 2-jaw grippers, we investigate the feasibility of zero-shot transfer. Through simulation studies on 8 manipulation tasks, we find that state-based Cartesian control policies can successfully zero-shot transfer to a target robot after accounting for forward dynamics. To address robot visual disparities for vision-based policies, we introduce Mirage, which uses “cross-painting”—masking out the unseen target robot and inpainting the seen source robot—during execution in real time so that it appears to the policy as if the trained source robot were performing the task. Mirage applies to both first-person and third-person camera views and policies that take in both states and images as inputs or only images as inputs. Despite its simplicity, our extensive simulation and physical experiments provide strong evidence that Mirage can successfully zero-shot transfer between different robot arms and grippers with only minimal performance degradation on a variety of manipulation tasks such as picking, stacking, and assembly, significantly outperforming a generalist policy.},
  archive   = {C_RSS},
  author    = {Lawrence Yunliang Chen and Karthik Dharmarajan and Kush Hari and Chenfeng Xu and Quan Vuong and Ken Goldberg},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {MIRAGE: Cross-embodiment zero-shot policy transfer with cross-painting},
  url       = {https://www.roboticsproceedings.org/rss20/p069.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Broadcasting support relations recursively from local
dynamics for object retrieval in clutters. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p070.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In our daily life, cluttered objects are everywhere, from scattered stationery and books cluttering the table to bowls and plates filling the kitchen sink. Retrieving a target object from clutters is an essential while challenging skill for robots, for the difficulty of safely manipulating an object without disturbing others, which requires the robot to plan a manipulation sequence and first move away a few other objects supported by the target object step by step. However, due to the diversity of object configurations (e.g., categories, geometries, locations and poses) and their combinations in clutters, it is difficult for a robot to accurately infer the support relations between objects faraway with various objects in between. In this paper, we study retrieving objects in complicated clutters via a novel method of recursively broadcasting the accurate local dynamics to build a support relation graph of the whole scene, which largely reduces the complexity of the support relation inference and improves the accuracy. Experiments in both simulation and the real world demonstrate the efficiency and effectiveness of our method.},
  archive   = {C_RSS},
  author    = {Yitong Li and Ruihai Wu and Haoran Lu and Chuanruo Ning and Yan Shen and Guanqi Zhan and Hao Dong},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Broadcasting support relations recursively from local dynamics for object retrieval in clutters},
  url       = {https://www.roboticsproceedings.org/rss20/p070.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Consistency policy: Accelerated visuomotor policies
via consistency distillation. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p071.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many robotic systems, such as mobile manipulators or quadrotors, cannot be equipped with high-end GPUs due to space, weight, and power constraints. These constraints prevent these systems from leveraging recent developments in visuomotor policy architectures that require high-end GPUs to achieve fast policy inference. In this paper, we propose Consistency Policy, a faster and similarly powerful alternative to Diffusion Policy for learning visuomotor robot control. By virtue of its fast inference speed, Consistency Policy can enable low latency decision making in resource-constrained robotic setups. A Consistency Policy is distilled from a pretrained Diffusion Policy by enforcing self-consistency along the Diffusion Policy&#39;s learned trajectories. We compare Consistency Policy with Diffusion Policy and other related speed-up methods across 6 simulation tasks as well as three real-world tasks where we demonstrate inference on a laptop GPU. For all these tasks, Consistency Policy speeds up inference by an order of magnitude compared to the fastest alternative method and maintains competitive success rates. We also show that the Conistency Policy training procedure is robust to the pretrained Diffusion Policy&#39;s quality, a useful result that helps practioners avoid extensive testing of the pretrained model. Key design decisions that enabled this performance are the choice of consistency objective, reduced initial sample variance, and the choice of preset chaining steps.},
  archive   = {C_RSS},
  author    = {Aaditya Prasad and Kevin Lin and Jimmy Wu and Linqi Zhou and Jeannette Bohg},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Consistency policy: Accelerated visuomotor policies via consistency distillation},
  url       = {https://www.roboticsproceedings.org/rss20/p071.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). CLOSURE: Fast quantification of pose uncertainty sets.
<em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p072.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate uncertainty quantification of 6D pose estimation from learned noisy measurements (e.g., keypoints and pose hypotheses). Assuming unknown-but-bounded measurement noises, a pose uncertainty set (PURSE) is a subset of SE(3) that contains all possible 6D poses compatible with the measurements. Despite being simple to formulate and its ability to embed uncertainty, the PURSE is difficult to manipulate and interpret due to the many abstract nonconvex polynomial constraints defining it. An appealing simplification of PURSE–motivated by the bounded state estimation error assumption in robust control– is to find its minimum enclosing geodesic ball (MEGB), i.e., a point pose estimation with minimum worst-case error bound. We contribute (i) a geometric interpretation of the nonconvex PURSE, and (ii) a fast algorithm to inner approximate the MEGB. Particularly, we show the PURSE corresponds to the feasible set of a constrained dynamical system or the intersection of multiple geodesic balls, and this perspective allows us to design an algorithm to densely sample the boundary of the PURSE through strategic random walks that are efficiently parallelizable on a GPU. We then use the miniball algorithm by Gärtner (1999) to compute the MEGB of PURSE samples, leading to an inner approximation of the true MEGB. Our algorithm is named CLOSURE (enClosing baLl frOm purSe boUndaRy samplEs) and it enables computing a certificate of approximation tightness by calculating the relative ratio between the size of the inner approximation and the size of the outer approximation GRCC from Tang, Lasserre, and Yang (2023). Running on a single RTX 3090 GPU, CLOSURE achieves the relative ratio of 92.8% on the LM-O object pose estimation dataset, 91.4% on the 3DMatch point cloud registration dataset and 96.6% on the LM object pose estimation dataset with an average runtime below 0.3 seconds. Obtaining comparable worst-case error bound but 398×, 833× and 23.6× faster than the outer approximation GRCC, CLOSURE enables uncertainty quantification of 6D pose estimation to be implemented in real-time robot perception applications.},
  archive   = {C_RSS},
  author    = {Yihuai Gao and Yukai Tang and Han Qi and Heng Yang},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {CLOSURE: Fast quantification of pose uncertainty sets},
  url       = {https://www.roboticsproceedings.org/rss20/p072.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). GOAT: GO to any thing. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p073.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In deployment scenarios such as homes and warehouses, mobile robots are expected to autonomously navigate for extended periods, seamlessly executing tasks articulated in terms that are intuitively understandable by human operators. We present GO To Any Thing (GOAT), a universal navigation system capable of tackling these requirements with three key features: a) Multimodal: it can tackle goals specified via category labels, target images, and language descriptions, b) Lifelong: it benefits from its past experience in the same environment, and c) Platform Agnostic: it can be quickly deployed on robots with different embodiments. GOAT is made possible through a modular system design and a continually augmented instance-aware semantic memory that keeps track of the appearance of objects from different viewpoints in addition to category-level semantics. This enables GOAT to distinguish between different instances of the same category to enable navigation to targets specified by images and language descriptions. In experimental comparisons spanning over 90 hours in 9 different homes consisting of 675 goals selected across 200+ different object instances, we find GOAT achieves an overall success rate of 83%, surpassing previous methods and ablations by 32% (absolute improvement). GOAT improves with experience in the environment, from a 60% success rate at the first goal to a 90% success after exploration. In addition, we demonstrate that GOAT can readily be applied to downstream tasks such as pick and place and social navigation.},
  archive   = {C_RSS},
  author    = {Matthew Chang and Theophile Gervet and Mukul Khanna and Sriram Yenamandra and Dhruv Shah and So Yeon Min and Kavit Shah and Chris Paxton and Saurabh Gupta and Dhruv Batra and Roozbeh Mottaghi and Jitendra Malik and Devendra Singh Chaplot},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {GOAT: GO to any thing},
  url       = {https://www.roboticsproceedings.org/rss20/p073.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Demonstrating arena 3.0: Advancing social navigation
in collaborative and highly dynamic environments. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p074.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Building upon our previous contributions, this paper introduces Arena 3.0, an extension of Arena-Bench, Arena 1.0, and Arena 2.0 focusing on the development, simulation, and benchmarking of social navigation approaches in collaborative environments. We significantly enhance the realism of human behavior simulation by incorporating a diverse array of new social force models and interaction patterns, encompassing both human-human and human-robot dynamics. The platform provides a comprehensive set of new task modes, designed for extensive benchmarking and testing and is capable of generating realistic and human-centric environments dynamically, catering to a broad spectrum of social navigation scenarios. In addition, the platform’s functionalities have been abstracted across three widely used simulators, each tailored for specific training and testing purposes. The platform’s efficacy has been validated through an extensive benchmark and user evaluations of the platform by a global community of researchers and students, which noted the substantial improvement compared to previous versions and expressed interests to utilize the platform for future research and development. Arena 3.0 is openly available at https://github.com/Arena-Rosnav.},
  archive   = {C_RSS},
  author    = {Linh Kästner and Volodymyir Shcherbyna and Huajian Zeng and Tuan Anh Le and Maximilian Ho-Kyung Schreff and Halid Osmaev and Nam Truong Tran and Diego Diaz and Jan Golebiowski and Harold Soh and Jens Lambrecht},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Demonstrating arena 3.0: Advancing social navigation in collaborative and highly dynamic environments},
  url       = {https://www.roboticsproceedings.org/rss20/p074.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). RAG-driver: Generalisable driving explanations with
retrieval-augmented in-context multi-modal large language model
learning. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p075.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We need to trust robots that use often opaque AI methods. They need to explain themselves to us, and we need to trust their explanation. In this regard, explainability plays a critical role in trustworthy autonomous decision-making to foster transparency and acceptance among end users, especially in complex autonomous driving. Recent advancements in Multi-Modal Large Language models (MLLMs) have shown promising potential in enhancing the explainability as a driving agent producing control predictions along with natural language explanations. However, severe data scarcity due to expensive annotation costs and significant domain gaps between different datasets makes the development of a robust and generalisable system an extremely challenging task. Moreover, the prohibitively expensive training requirements of MLLM and the unsolved problem of catastrophic forgetting further limit their generalisability post-deployment.To address these challenges, we present RAG-Driver, a novel retrieval-augmented multi-modal large language model that leverages in-context learning for high-performance, explainable, and generalisable autonomous driving. By grounding in retrieved expert demonstration, we empirically validate that RAG-Driver achieves state-of-the-art performance in producing driving action explanations, justifications, and control signal prediction. More importantly, it exhibits exceptional zero-shot generalisation capabilities to unseen environments without further training endeavours.},
  archive   = {C_RSS},
  author    = {Jianhao Yuan and Shuyang Sun and Daniel Omeiza and Bo Zhao and Paul Newman and Lars Kunze and Matthew Gadd},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {RAG-driver: Generalisable driving explanations with retrieval-augmented in-context multi-modal large language model learning},
  url       = {https://www.roboticsproceedings.org/rss20/p075.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Dynamic adversarial attacks on autonomous driving
systems. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p076.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces an attacking mechanism to challenge the resilience of autonomous driving systems. Specifically, we manipulate the decision-making processes of an autonomous vehicle by dynamically displaying adversarial patches on a screen mounted on another moving vehicle. These patches are optimized to deceive the object detection models into misclassifying targeted objects, e.g., traffic signs. Such manipulation has significant implications for critical multi-vehicle interactions such as intersection crossing, which are vital for safe and efficient autonomous driving systems. Particularly, we make four major contributions. First, we introduce a novel adversarial attack approach where the patch is not co-located with its target, enabling more versatile and stealthy attacks. Moreover, our method utilizes dynamic patches displayed on a screen, allowing for adaptive changes and movements, enhancing the flexibility and performance of the attack. To do so, we design a Screen Image Transformation Network (SIT-Net), which simulates environmental effects on the displayed images, narrowing the gap between simulated and real-world scenarios. Further, we integrate a positional loss term into the adversarial training process to increase the success rate of the dynamic attack. Finally, we shift the focus from merely attacking perceptual systems to influencing the decision-making algorithms of self-driving systems. Our experiments demonstrate the first successful implementation of such dynamic adversarial attacks in real-world autonomous driving scenarios, paving the way for advancements in the field of robust and secure autonomous driving.},
  archive   = {C_RSS},
  author    = {Amirhosein Chahe and Chenan Wang and Abhishek Jeyapratap and Kaidi Xu and Lifeng Zhou},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Dynamic adversarial attacks on autonomous driving systems},
  url       = {https://www.roboticsproceedings.org/rss20/p076.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Hierarchical open-vocabulary 3D scene graphs for
language-grounded robot navigation. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p077.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent open-vocabulary robot mapping methods enrich dense geometric maps with pre-trained visual-language features. While these maps allow for the prediction of point-wise saliency maps when queried for a certain language concept, large-scale environments and abstract queries beyond the object level still pose a considerable hurdle, ultimately limiting language-grounded robotic navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D scene graph mapping approach for language-grounded indoor robot navigation. Leveraging open-vocabulary vision foundation models, we first obtain state-of-the-art open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene graph hierarchy consisting of floor, room, and object concepts, each enriched with open-vocabulary features. Our approach is able to represent multi-story buildings and allows robotic traversal of those using a cross-floor Voronoi graph. HOV-SG is evaluated on three distinct datasets and surpasses previous baselines in open-vocabulary semantic accuracy on the object, room, and floor level while producing a 75% reduction in representation size compared to dense open-vocabulary maps. In order to prove the efficacy and generalization capabilities of HOV-SG, we showcase successful long-horizon language-conditioned robot navigation within real-world multi-story environments. We provide code and trial video data at: https://hovsg.github.io.},
  archive   = {C_RSS},
  author    = {Abdelrhman Werby and Chenguang Huang and Martin Büchner and Abhinav Valada and Wolfram Burgard},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Hierarchical open-vocabulary 3D scene graphs for language-grounded robot navigation},
  url       = {https://www.roboticsproceedings.org/rss20/p077.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). ScrewMimic: Bimanual imitation from human videos with
screw space projection. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p078.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bimanual manipulation is a longstanding challenge in robotics due to the large number of degrees of freedom and the strict spatial and temporal synchronization required to generate meaningful behavior. Humans learn bimanual manipulation skills by watching other humans and by refining their abilities through play. In this work, we aim to enable robots to learn bimanual manipulation behaviors from human video demonstrations and fine-tune them through interaction. Inspired by seminal work in psychology and biomechanics, we propose modeling the interaction between two hands as a serial kinematic linkage — as a screw motion, in particular, that we use to define a new action space for bimanual manipulation: screw actions. We introduce SCREWMIMIC, a framework that leverages this novel action representation to facilitate learning from human demonstration and self-supervised policy fine-tuning. Our experiments demonstrate that SCREWMIMIC is able to learn several complex bimanual behaviors from a single human video demonstration, and that it outperforms baselines that interpret demonstrations and fine-tune directly in the original space of motion of both arms.},
  archive   = {C_RSS},
  author    = {Arpit Bahety and Priyanka Mandikal and Ben Abbatematteo and Roberto Martín-Martín},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {ScrewMimic: Bimanual imitation from human videos with screw space projection},
  url       = {https://www.roboticsproceedings.org/rss20/p078.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). NaVid: Video-based VLM plans the next step for
vision-and-language navigation. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p079.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision-and-language navigation (VLN) stands as a key research problem of Embodied AI, aiming at enabling agents to navigate in unseen environments following linguistic instructions. In this field, generalization is a long-standing challenge, either to out-of-distribution scenes or from Sim to Real. In this paper, we propose NaVid, a video-based large vision language model (VLM), to mitigate such a generalization gap. NaVid makes the first endeavor to showcase the capability of VLMs to achieve state-of-the-art level navigation performance without any maps, odometers, or depth inputs. Following human instruction, NaVid only requires an on-the-fly video stream from a monocular RGB camera equipped on the robot to output the next-step action. Our formulation mimics how humans navigate and naturally gets rid of the problems introduced by odometer noises, and the Sim2Real gaps from map or depth inputs. Moreover, our video-based approach can effectively encode the historical observations of robots as spatio-temporal contexts for decision making and instruction following. We train NaVid with 510k navigation samples collected from continuous environments, including action-planning and instruction-reasoning samples, along with 763k large-scale web data. Extensive experiments show that NaVid achieves state-of-the-art performance in simulation environments and the real world, demonstrating superior cross-dataset and Sim2Real transfer. We thus believe our proposed VLM approach plans the next step for not only the navigation agents but also this research field.},
  archive   = {C_RSS},
  author    = {Jiazhao Zhang and Kunyu Wang and Rongtao Xu and Gengze Zhou and Yicong Hong and Xiaomeng Fang and Qi Wu and Zhizheng Zhang and He Wang},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {NaVid: Video-based VLM plans the next step for vision-and-language navigation},
  url       = {https://www.roboticsproceedings.org/rss20/p079.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). RACER: Epistemic risk-sensitive RL enables fast
driving with fewer crashes. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p080.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning provides an appealing framework for robotic control due to its ability to learn expressive policies purely through real-world interaction. However, this requires addressing real-world constraints, including avoidance of catastrophic failures during training, which might severely impede both learning progress and the performance of the final policy. In many robotics settings, this amounts to avoiding certain &quot;unsafe&quot; states. The high-speed off-road driving task represents a particularly challenging instantiation of this problem: a high-return policy should drive as aggressively and as quickly as possible, which often requires getting close to the edge of the set of &quot;safe&quot; states, and therefore places a particular burden on the method to avoid frequent failures. To both learn highly performant policies and avoid excessive failures, we propose a reinforcement learning framework that combines risk-sensitive control with an adaptive action space curriculum. We propose a reinforcement learning objective that uses a risk-sensitive metric to jointly train a policy and iteratively expand action bounds during training, starting with a low-speed policy and slowly increasing the speed over time. Furthermore, we show that our risk-sensitive objective automatically avoids out-of-distribution states when equipped with an estimator for epistemic uncertainty. We implement our algorithm on a small-scale rally car and show that it is capable of learning high-speed policies for a real-world off-road driving task. We show that our method greatly reduces the number of safety violations during the training process, and actually leads to better final high-speed driving policies at the end of training.},
  archive   = {C_RSS},
  author    = {Kyle Stachowicz and Sergey Levine},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {RACER: Epistemic risk-sensitive RL enables fast driving with fewer crashes},
  url       = {https://www.roboticsproceedings.org/rss20/p080.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Khronos: A unified approach for spatio-temporal
metric-semantic SLAM in dynamic environments. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p081.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Perceiving and understanding highly dynamic and changing environments is a crucial capability for robot autonomy. While large strides have been made towards developing dynamic SLAM approaches that estimate the robot pose accurately, a lesser emphasis has been put on the construction of dense spatio-temporal representations of the robot environment. A detailed understanding of the scene and its evolution through time is crucial for long-term robot autonomy and essential to tasks that require long-term reasoning, such as operating effectively in environments shared with humans and other agents and thus are subject to short and long-term dynamics. To address this challenge, this work defines the Spatio-temporal Metric-semantic SLAM (SMS) problem, and presents a framework to factorize and solve it efficiently. We show that the proposed factorization suggests a natural organization of a spatio-temporal perception system, where a fast process tracks short-term dynamics in an active temporal window, while a slower process reasons over long-term changes in the environment using a factor graph formulation. We provide an efficient implementation of the proposed spatio-temporal perception approach, that we call Khronos, and show that it unifies exiting interpretations of short-term and long-term dynamics and is able to construct a dense spatio-temporal map in real-time. We provide simulated and real results, showing that the spatio-temporal maps built by Khronos are an accurate reflection of a 3D scene over time and that Khronos outperforms baselines across multiple metrics. We further validate our approach on two heterogeneous robots in challenging, large-scale real-world environments.},
  archive   = {C_RSS},
  author    = {Lukas Schmid and Marcus Abate and Yun Chang and Luca Carlone},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Khronos: A unified approach for spatio-temporal metric-semantic SLAM in dynamic environments},
  url       = {https://www.roboticsproceedings.org/rss20/p081.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Demonstrating agile flight from pixels without state
estimation. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p082.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Quadrotors are among the most agile flying robots. Despite recent advances in learning-based control and computer vision, autonomous drones still rely on explicit state estimation. On the other hand, human pilots only rely on a first-person-view video stream from the drone onboard camera to push the platform to its limits and fly robustly in unseen environments. To the best of our knowledge, we present the first vision-based quadrotor system that autonomously navigates through a sequence of gates at high speeds while directly mapping pixels to control commands. Like professional drone-racing pilots, our system does not use explicit state estimation and leverages the same control commands humans use (collective thrust and body rates). We demonstrate agile flight at speeds up to 40km/h with accelerations up to 2g. This is achieved by training vision-based policies with reinforcement learning (RL). The training is facilitated using an asymmetric actor-critic with access to privileged information. To overcome the computational complexity during image-based RL training, we use the inner edges of the gates as a sensor abstraction. This simple yet robust, task-relevant representation can be simulated during training without rendering images. During deployment, a Swin-transformer-based gate detector is used. Our approach enables autonomous agile flight with standard, off-the-shelf hardware. Although our demonstration focuses on drone racing, we believe that our method has an impact beyond drone racing and can serve as a foundation for future research into real-world applications in structured environments.},
  archive   = {C_RSS},
  author    = {Ismail Geles and Leonard Bauersfeld and Angel Romero and Jiaxu Xing and Davide Scaramuzza},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Demonstrating agile flight from pixels without state estimation},
  url       = {https://www.roboticsproceedings.org/rss20/p082.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). You’ve got to feel it to believe it: Multi-modal
bayesian inference for semantic and property prediction. <em>RSS</em>.
(<a href="https://www.roboticsproceedings.org/rss20/p083.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots must be able to understand their surroundings to perform complex tasks in challenging environments and many of these complex tasks require estimates of physical properties such as friction or weight. Estimating such properties using learning is challenging due to the large amounts of labelled data required for training and the difficulty of updating these learned models online at run time. To overcome these challenges, this paper introduces a novel, multi-modal approach for representing semantic predictions and physical property estimates jointly in a probabilistic manner. By using conjugate pairs, the proposed method enables closed-form Bayesian updates given visual and tactile measurements without requiring additional training data. The efficacy of the proposed algorithm is demonstrated through several hardware experiments. In particular, this paper illustrates that by conditioning semantic classifications on physical proper- ties, the proposed method quantitatively outperforms state-of-the-art semantic classification methods that rely on vision alone. To further illustrate its utility, the proposed method is used in several applications including to represent affordance-based properties probabilistically and a challenging terrain traversal task using a legged robot. In the latter task, the proposed method represents the coefficient of friction of the terrain probabilistically, which enables the use of an on-line risk-aware planner that switches the legged robot from a dynamic gait to a static, stable gait when the expected value of the coefficient of friction falls below a given threshold. Videos of these case studies as well as the open-source C++ and ROS interface can be found at https://roahmlab.github.io/multimodal_mapping/.},
  archive   = {C_RSS},
  author    = {Parker Ewen and Hao Chen and Yuzhen Chen and Anran Li and Anup Bagali and Gitesh Gunjal and Ram Vasudevan},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {You’ve got to feel it to believe it: Multi-modal bayesian inference for semantic and property prediction},
  url       = {https://www.roboticsproceedings.org/rss20/p083.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). AnyFeature-VSLAM: Automating the usage of any feature
into visual SLAM. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p084.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Feature-based SLAM heavily relies on the specific type of visual features employed. The most effective feature in some conditions may perform worse or not be suitable for other ones, leading to significant performance variability. Seamlessly switching to the most effective visual feature is a desirable quality for SLAM, but, currently, this involves a cumbersome manual task that demands substantial parameter tuning efforts and expert knowledge. In this paper, we present AnyFeature-VSLAM, an automated visual SLAM pipeline capable of switching to a chosen type of feature effortlessly and without manual intervention. The tuning of parameters associated with visual features is performed automatically to achieve the best performance. We built AnyFeature-VSLAM on top of ORB-SLAM2, one of the most popular and widely used feature-based visual SLAM implementations. Through extensive experiments across various benchmark datasets, we demonstrate that AnyFeature-VSLAM consistently delivers good results irrespective of the chosen visual feature, outperforming baseline implementations. Specifically, our paper includes a quantitative assessment of trajectory estimation involving seven different keypoint and descriptor combinations across thirty sequences spanning four distinct publicly available datasets. Furthermore, we showcase the enhanced flexibility of our system by subjecting it to four additional challenging datasets. Code publicly available at: https://github.com/alejandrofontan/AnyFeature-VSLAM.},
  archive   = {C_RSS},
  author    = {Alejandro Fontan and Javier Civera and Michael Milford},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {AnyFeature-VSLAM: Automating the usage of any feature into visual SLAM},
  url       = {https://www.roboticsproceedings.org/rss20/p084.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). IMESA: Incremental distributed optimization for
collaborative simultaneous localization and mapping. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p085.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a novel incremental distributed back-end algorithm for Collaborative Simultaneous Localization and Mapping (C-SLAM). For real-world deployments, robotic teams require algorithms to compute a consistent state estimate accurately, within online runtime constraints, and with potentially limited communication. Existing centralized, decentralized, and distributed approaches to solving C-SLAM problems struggle to achieve all of these goals. To address this capability gap we present Incremental on Manifold Edge-based Separable ADMM (iMESA) a fully distributed C-SLAM back-end algorithm that can provide a multi-robot team with accurate state estimates in real-time with only sparse pair-wise communication between robots. Extensive evaluation on real and synthetic data demonstrates that iMESA is able to outperform comparable state-of-the-art C-SLAM back-ends.},
  archive   = {C_RSS},
  author    = {Daniel McGann and Michael Kaess},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {IMESA: Incremental distributed optimization for collaborative simultaneous localization and mapping},
  url       = {https://www.roboticsproceedings.org/rss20/p085.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Scalable distance-based multi-agent relative state
estimation via block multiconvex optimization. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p086.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper explores the distance-based relative state estimation problem in large-scale systems, which is hard to solve effectively due to its high-dimensionality and non-convexity. In this paper, we alleviate this inherent hardness to simultaneously achieve scalability and robustness of inference on this problem. Our idea is launched from a universal geometric formulation, called generalized graph realization, for the distance-based relative state estimation problem. Based on this formulation, we introduce two collaborative optimization models, one of which is convex and thus globally solvable, and the other enables fast searching on non-convex landscapes to refine the solution offered by the convex one. Importantly, both models enjoy multiconvex and decomposable structures, allowing efficient and safe solutions using block coordinate descent that enjoys scalability and a distributed nature. The proposed algorithms collaborate to demonstrate superior or comparable solution precision to the current centralized convex relaxation-based methods, which are known for their high optimality. Distinctly, the proposed methods demonstrate scalability and unique computational efficiency beyond the reach of previous convex relaxation-based methods. We also demonstrate that the combination of the two proposed algorithms achieves a more robust pipeline than deploying the local search method alone in a continuous-time scenario.},
  archive   = {C_RSS},
  author    = {Tianyue Wu and Fei Gao},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Scalable distance-based multi-agent relative state estimation via block multiconvex optimization},
  url       = {https://www.roboticsproceedings.org/rss20/p086.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Experience-based multi-agent path finding with narrow
corridors. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p087.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent path finding is a computationally challenging problem that is relevant to many areas in robotics. Experience-based planning methods have been shown to significantly reduce the planning time of this problem, but the type of problem in which experience can be used has so far been limited to warehouse-like environments with ample open space. We present an experience-based multi-agent path finding algorithm that specifically addresses narrow corridors of width 1 (also known as doorways). This expands the domain of experience-based problems to include environments such as most houses, office spaces, retail spaces, and hospitals. We also present novel techniques for conflict resolution strategies that result in up to a $94\%$ decrease in waiting steps per robot and final paths closer to the optimal decoupled path by up to $71\%$ than the strategies used in current experience-based methods. We demonstrate our planner solving problems with hundreds of robots in congested environments in seconds, finding solutions in an allotted time more often than existing state of the art optimal methods.},
  archive   = {C_RSS},
  author    = {Rachel A Moan and Courtney McBeth and Marco Morales and Nancy Amato and Kris Hauser},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Experience-based multi-agent path finding with narrow corridors},
  url       = {https://www.roboticsproceedings.org/rss20/p087.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Event-based visual inertial velometer. <em>RSS</em>.
(<a href="https://www.roboticsproceedings.org/rss20/p088.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Neuromorphic event-based cameras are bio-inspired visual sensors with asynchronous pixels and extremely high temporal resolution. Such favorable properties make them an excellent choice for solving state estimation tasks under aggressive ego motion. However, failures of camera pose tracking are frequently witnessed in state-of-the-art event-based visual odometry systems when the local map cannot be updated in time. One of the biggest roadblocks for this specific field is the absence of efficient and robust methods for data association without imposing any assumption on the environment. This problem seems, however, unlikely to be addressed as in standard vision due to the motion-dependent observability of event data. Therefore, we propose a map-free design for event-based visual-inertial state estimation in this paper. Instead of estimating the position of the event camera, we find that recovering the instantaneous linear velocity is more consistent with the differential working principle of event cameras. The proposed event-based visual-inertial velometer leverages a continuous-time formulation that incrementally fuses the heterogeneous measurements from a stereo event camera and an inertial measurement unit. Experiments on both synthetic and real data demonstrate that the proposed method can recover instantaneous linear velocity in metric scale with low latency.},
  archive   = {C_RSS},
  author    = {Xiuyuan LU and Yi Zhou and Junkai Niu and Sheng Zhong and Shaojie Shen},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Event-based visual inertial velometer},
  url       = {https://www.roboticsproceedings.org/rss20/p088.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Explore until confident: Efficient exploration for
embodied question answering. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p089.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problem of Embodied Question Answering (EQA), which refers to settings where an embodied agent such as a robot needs to actively explore an environment to gather information until it is confident about the answer to a question. In this work, we leverage the strong semantic reasoning capabilities of large vision-language models (VLMs) to efficiently explore and answer such questions. However, there are two main challenges when using VLMs in EQA: they do not have an internal memory for mapping the scene to be able to plan how to explore over time, and their confidence can be miscalibrated and can cause the robot to prematurely stop exploration or over-explore. We propose a method that first builds a semantic map of the scene based on depth information and via visual prompting of a VLM --- leveraging its vast knowledge of relevant regions of the scene for exploration. Next, we use conformal prediction to calibrate the VLM&#39;s question answering confidence, allowing the robot to know when to stop exploration --- leading to a more calibrated and efficient exploration strategy. To test our framework in simulation, we also contribute a new EQA dataset with diverse, realistic human-robot scenarios and scenes built upon the Habitat-Matterport 3D Research Dataset (HM3D). Both simulated and real robot experiments show our proposed approach improves the performance and efficiency over baselines that do no leverage VLM for exploration or do not calibrate its confidence.},
  archive   = {C_RSS},
  author    = {Allen Z. Ren and Jaden Clark and Anushri Dixit and Masha Itkina and Anirudha Majumdar and Dorsa Sadigh},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Explore until confident: Efficient exploration for embodied question answering},
  url       = {https://www.roboticsproceedings.org/rss20/p089.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Octo: An open-source generalist robot policy.
<em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p090.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large policies pretrained on diverse robot datasets have the potential to transform robotic learning: instead of training new policies from scratch, such generalist robot policies may be finetuned with only a little in-domain data, yet generalize broadly. However, to be widely applicable across a range of robotic learning scenarios, environments, and tasks, such policies need to handle diverse sensors and action spaces, accommodate a variety of commonly used robotic platforms, and finetune readily and efficiently to new domains. In this work, we aim to lay the groundwork for developing open-source, widely applicable, generalist policies for robotic manipulation. As a first step, we introduce Octo, a large transformer-based policy trained on 800k trajectories from the Open X-Embodiment dataset, the largest robot manipulation dataset to date. It can be instructed via language commands or goal images and can be effectively finetuned to robot setups with new sensory inputs and action spaces within a few hours on standard consumer GPUs. In experiments across 9 robotic platforms, we demonstrate that Octo serves as a versatile policy initialization that can be effectively finetuned to new observation and action spaces. We also perform detailed ablations of design decisions for the Octo model, from architecture to training data, to guide future research on building generalist robot models.},
  archive   = {C_RSS},
  author    = {Dibya Ghosh and Homer Rich Walke and Karl Pertsch and Kevin Black and Oier Mees and Sudeep Dasari and Joey Hejna and Tobias Kreiman and Charles Xu and Jianlan Luo and You Liang Tan and Lawrence Yunliang Chen and Quan Vuong and Ted Xiao and Pannag R Sanketi and Dorsa Sadigh and Chelsea Finn and Sergey Levine},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Octo: An open-source generalist robot policy},
  url       = {https://www.roboticsproceedings.org/rss20/p090.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Demonstrating OK-robot: What really matters in
integrating open-knowledge models for robotics. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p091.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Remarkable progress has been made in recent years in the fields of vision, language, and robotics. We now have vision models capable of recognizing objects based on language queries, navigation systems that can effectively control mobile systems, and grasping models that can handle a wide range of objects. Despite these advancements, general-purpose applications of robotics still lag behind, even though they rely on these fundamental capabilities of recognition, navigation, and grasping. In this paper, we adopt a systems-first approach to develop a new Open Knowledge-based robotics framework called OK-Robot. By combining Vision-Language Models (VLMs) for object detection, navigation primitives for movement, and grasping primitives for object manipulation, OK-Robot offers a integrated solution for pick-and-drop operations without requiring any training. To evaluate its performance, we run OK-Robot in 10 real-world home environments. The results demonstrate that OK-Robot achieves a 58.5% success rate in open-ended pick-and-drop tasks, representing a new state-of-the-art in Open Vocabulary Mobile Manipulation (OVMM) with nearly 1.8x the performance of prior work. On cleaner, uncluttered environments, OK-Robot&#39;s performance increases to 82%. However, the most important insight gained from OK-Robot is the critical role of nuanced details when combining Open Knowledge systems like VLMs with robotic modules.},
  archive   = {C_RSS},
  author    = {Peiqi Liu and Yaswanth Orru and Jay Vakil and Chris Paxton and Nur Muhammad Mahi Shafiullah and Lerrel Pinto},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Demonstrating OK-robot: What really matters in integrating open-knowledge models for robotics},
  url       = {https://www.roboticsproceedings.org/rss20/p091.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Any-point trajectory modeling for policy learning.
<em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p092.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning from demonstration is a powerful method for teaching robots new skills, and having more demonstration data often improves policy learning. However, the high cost of collecting demonstration data is a significant bottleneck. Videos, as a rich data source, contain knowledge of behaviors, physics, and semantics, but extracting control-specific information from them is challenging due to the lack of action labels. In this work, we introduce a novel framework, \textbf{A}ny-point \textbf{T}rajectory \textbf{M}odeling (ATM), that utilizes video demonstrations by pre-training a trajectory model to predict future trajectories of arbitrary points within a video frame. Once trained, these trajectories provide detailed control guidance, enabling the learning of robust visuomotor policies with minimal action-labeled data. Across the \textbf{130} language-conditioned tasks we evaluated in both simulation and the real world, ATM outperforms strong video pre-training baselines by 80$\%$ on average. Furthermore, we show effective transfer learning of manipulation skills from human videos.},
  archive   = {C_RSS},
  author    = {Chuan Wen and Xingyu Lin and John Ian Reyes So and Kai Chen and Qi Dou and Yang Gao and Pieter Abbeel},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Any-point trajectory modeling for policy learning},
  url       = {https://www.roboticsproceedings.org/rss20/p092.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Pushing the limits of cross-embodiment learning for
manipulation and navigation. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p093.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent years in robotics and imitation learning have shown remarkable progress in training large-scale foundation models by leveraging data across a multitude of embodiments. The success of such policies might lead us to wonder: just how diverse can the robots in the training set be while still facilitating positive transfer? In this work, we study this question in the context of heterogeneous embodiments, examining how even seemingly very different domains such as robotic navigation and manipulation can provide benefits when included in the training data for the same model. We train a single goal-conditioned policy that is capable of controlling a robotic arm, quadcopter, quadruped, and mobile base. We then investigate the extent to which transfer can occur across navigation and manipulation by framing them as a single goal-reaching task. In particular, we find that co-training with navigation data can enhance robustness and performance in goal-conditioned manipulation with a wrist-mounted camera. We then deploy our policy trained only from navigation-only and static manipulation-only data on a mobile manipulator, showing that it can control a similar but novel embodiment in a zero-shot manner. These results provide evidence that large-scale robotic policies can benefit from data collected across a wide variety of embodiments.},
  archive   = {C_RSS},
  author    = {Jonathan Heewon Yang and Catherine Glossop and Arjun Bhorkar and Dhruv Shah and Quan Vuong and Chelsea Finn and Dorsa Sadigh and Sergey Levine},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Pushing the limits of cross-embodiment learning for manipulation and navigation},
  url       = {https://www.roboticsproceedings.org/rss20/p093.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). DrEureka: Language model guided sim-to-real transfer.
<em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p094.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Transferring policies learned in simulation to the real world is a promising strategy for acquiring robot skills at scale. However, sim-to-real approaches typically rely on manual design and tuning of the task reward function as well as the simulation physics parameters, rendering the process slow and human-labor intensive. In this paper, we investigate using Large Language Models (LLMs) to automate and accelerate sim-to-real design. Our LLM-guided sim-to-real approach, DrEureka, requires only the physics simulation for the target task and automatically constructs suitable reward functions and domain randomization distributions to support real-world transfer. We first demonstrate that our approach can discover sim-to-real configurations that are competitive with existing human-designed ones on quadruped locomotion and dexterous manipulation tasks. Then, we showcase that our approach is capable of solving novel robot tasks, such as quadruped balancing and walking atop a yoga ball, without iterative manual design.},
  archive   = {C_RSS},
  author    = {Yecheng Jason Ma and William Liang and Hung-Ju Wang and Yuke Zhu and Linxi Fan and Osbert Bastani and Dinesh Jayaraman},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {DrEureka: Language model guided sim-to-real transfer},
  url       = {https://www.roboticsproceedings.org/rss20/p094.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Set it up!: Functional object arrangement with
compositional generative models. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p095.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies the challenge of developing robots capable of understanding under-specified instructions for creating functional object arrangements, such as &quot;set up a dining table for two&quot;; previous arrangement approaches have focused on much more explicit instructions, such as &quot;put object A on the table.&quot; We introduce a framework, *SetItUp*, for learning to interpret under-specified instructions. SetItUp takes a small number of training examples and a human-crafted program sketch to uncover arrangement rules for specific scene types. By leveraging an intermediate graph-like representation of *abstract spatial relationships* among objects, SetItUp decomposes the arrangement problem into two subproblems: i) learning the arrangement patterns from limited data and ii) grounding these abstract relationships into object poses. SetItUp leverages large language models (LLMs) to propose the abstract spatial relationships among objects in novel scenes as the constraints to be satisfied; then, it composes a library of diffusion models associated with these abstract relationships to find object poses that satisfy the constraints. We validate our framework on a dataset comprising study desks, dining tables, and coffee tables, with the results showing superior performance in generating physically plausible, functional, and aesthetically pleasing object arrangements compared to existing models. \footnote{Project page: https://setitup-rss.github.io/}},
  archive   = {C_RSS},
  author    = {Yiqing Xu and Jiayuan Mao and Yilun Du and Tomás Lozano-Pérez and Leslie Pack Kaelbling and David Hsu},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Set it up!: Functional object arrangement with compositional generative models},
  url       = {https://www.roboticsproceedings.org/rss20/p095.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Keypoint action tokens enable in-context imitation
learning in robotics. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p096.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We show that off-the-shelf text-based Transformers, with no additional training, can perform few-shot in-context visual imitation learning, mapping visual observations to action sequences that emulate the demonstrator&#39;s behaviour. We achieve this by transforming visual observations (inputs) and trajectories of actions (outputs) into sequences of tokens that a text-pretrained Transformer (GPT-4 Turbo) can ingest and generate, via a framework we call Keypoint Action Tokens (KAT). Despite being trained only on language, we show that these Transfermers excel at translating tokenised visual keypoint observations into action trajectories, performing on par or better than state-of-the-art imitation learning (diffusion policies) in the low-data regime on a suite of real-world, everyday tasks. Rather than operating in the language domain as is typical, KAT leverages text-based Transformers to operate in the vision and action domains to learn general patterns in demonstration data for highly efficient imitation learning, indicating promising new avenues for repurposing natural language models for embodied tasks. Videos can be found on our website https://www.robot-learning.uk/keypoint-action-tokens.},
  archive   = {C_RSS},
  author    = {Norman Di Palo and Edward Johns},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Keypoint action tokens enable in-context imitation learning in robotics},
  url       = {https://www.roboticsproceedings.org/rss20/p096.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). ConTac: Continuum-emulated soft skinned arm with
vision-based shape sensing and contact-aware manipulation. <em>RSS</em>.
(<a href="https://www.roboticsproceedings.org/rss20/p097.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic systems employing continuum bodies offer a high degree of dexterity, which provides advantages in terms of accuracy and safety when operating in cluttered environments. However, current methods of describing posture or detecting contact for such continuum structures are focusing on bespoke designs or are limited to a single sensing modality, which could hinder their possibility for scalability and generalization. This study proposes a novel vision-based tactile sensing system, named ConTac, that provides both proprioception and tactile detection for a continuum-emulated arm with soft skin. To realize the mentioned functions, we employ two corresponding deep-learning models trained using simulation data. The models are zero-shot applied to real-world data without fine-tuning. The experimental results show that the system could predict the posture of a skinned robot arm with a mean tip position error of 8.83 mm, while the mean error for touch location was 28.86 mm. We then compared the model performance on two different robot modules, proving the justification of the system. An admittance control strategy is then developed using the shape and contact information, allowing the robot arm to react properly to collisions. The proposed method shows potential in adapting to hyper-redundant or continuum robots, enhancing their perception capabilities and control paradigms.},
  archive   = {C_RSS},
  author    = {Tuan Tai Nguyen and Quan Khanh Luu and Dinh Quang Nguyen and Van Ho},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {ConTac: Continuum-emulated soft skinned arm with vision-based shape sensing and contact-aware manipulation},
  url       = {https://www.roboticsproceedings.org/rss20/p097.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Function based sim-to-real learning for shape control
of deformable free-form surfaces. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p098.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For the shape control of deformable free-form surfaces, simulation plays a crucial role in establishing the mapping between the actuation parameters and the deformed shapes. The differentiation of this forward kinematic mapping is usually employed to solve the inverse kinematic problem for determining the actuation parameters that can realize a target shape. However, the free-form surfaces obtained from simulators are always different from the physically deformed shapes due to the errors introduced by hardware and the simplification adopted in physical simulation. To fill the gap, we propose a novel deformation function based sim-to-real learning method that can map the geometric shape of a simulated model into its corresponding shape of the physical model. Unlike the existing sim-to-real learning methods that rely on completely acquired dense markers, our method accommodates sparsely distributed markers and can resiliently use all captured frames -- even for those in the presence of missing markers. To demonstrate its effectiveness, our sim-to-real method has been integrated into a neural network-based computational pipeline designed to tackle the inverse kinematic problem on a pneumatically actuated deformable mannequin.},
  archive   = {C_RSS},
  author    = {Yingjun Tian and Guoxin Fang and Renbo Su and Weiming Wang and Simeon Gill and Andrew Weightman and Charlie C. L. Wang},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Function based sim-to-real learning for shape control of deformable free-form surfaces},
  url       = {https://www.roboticsproceedings.org/rss20/p098.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Safe &amp; accurate at speed with tendons: A robot arm
for exploring dynamic motion. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p099.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Operating robots precisely and at high speeds has been a long-standing goal of robotics research. Balancing these competing demands is key to enabling the seamless collaboration of robots and humans and increasing task performance. However, traditional motor-driven systems often fall short in this balancing act. Due to their rigid and often heavy design exacerbated by positioning the motors into the joints, faster motions of such robots transfer high forces at impact. To enable precise and safe dynamic motions, we introduce a four degree-of-freedom tendon-driven robot arm. Tendons allow placing the actuation at the base to reduce the robot&#39;s inertia, which we show significantly reduces peak collision forces compared to conventional motor-driven systems. Pairing our robot with pneumatic muscles allows generating high forces and highly accelerated motions, while benefiting from impact resilience through passive compliance. Since tendons are subject to additional friction and hence prone to tear, we validate the reliability of our robotic arm on various experiments, including long-term dynamic motions. We also demonstrate its ease of control by quantifying the nonlinearities of the system and the performance on a challenging dynamic table tennis task learned from scratch using reinforcement learning. We open-source the entire hardware design, which can be largely 3D printed, the control software, and a proprioceptive dataset of 25 days of diverse robot motions.},
  archive   = {C_RSS},
  author    = {Simon Guist and Jan Schneider and Hao Ma and Le Chen and Vincent Berenz and Julian Martus and Heiko Ott and Felix Grüninger and Michael Muehlebach and Jonathan Fiene and Bernhard Schölkopf and Dieter Büchler},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Safe &amp; accurate at speed with tendons: A robot arm for exploring dynamic motion},
  url       = {https://www.roboticsproceedings.org/rss20/p099.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Evolution and learning in differentiable robots.
<em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p100.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The automatic design of robots has existed for 30 years but has been constricted by serial non-differentiable design evaluations, premature convergence to simple bodies or clumsy behaviors, and a lack of sim2real transfer to physical machines. Thus, here we employ massively-parallel differentiable simulations to rapidly and simultaneously optimize individual neural control of behavior across a large population of candidate body plans and return a fitness score for each design based on the performance of its fully optimized behavior. Non-differentiable changes to the mechanical structure of each robot in the population---mutations that rearrange, combine, add, or remove body parts---were applied by a genetic algorithm in an outer loop of search, generating a continuous flow of novel morphologies with highly-coordinated and graceful behaviors honed by gradient descent. This enabled the exploration of several orders-of-magnitude more designs than all previous methods, despite the fact that robots here have the potential to be much more complex, in terms of number of independent motors, than those in prior studies. We found that evolution reliably produces ``increasingly differentiable&#39;&#39; robots: body plans that smooth the loss landscape in which learning operates and thereby provide better training paths toward performant behaviors. Finally, one of the highly differentiable morphologies discovered in simulation was realized as a physical robot and shown to retain its optimized behavior. This provides a cyberphysical platform to investigate the relationship between evolution and learning in biological systems and broadens our understanding of how a robot&#39;s physical structure can influence the ability to train policies for it. Videos and code at https://sites.google.com/view/eldir.},
  archive   = {C_RSS},
  author    = {Luke Strgar and David Matthews and Tyler Hummer and Sam Kriegman},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Evolution and learning in differentiable robots},
  url       = {https://www.roboticsproceedings.org/rss20/p100.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Construction of a multiple-DOF underactuated gripper
with force-sensing via deep learning. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p101.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Under-actuated robotic grippers, regarded as critical components of robotic grasping, have attracted considerable attention. However, existing under-actuated grippers emerge with several primary issues, including low payload, insufficient force sensing, small grasping force, weak grasping stability as well as high cost, hindering widespread applications. Some of these grippers can only implement a single grasping mode, thereby imposing restrictions on dimensional ranges of objects. To well relieve all relevant research gaps, we present a novel under-actuated gripper with two 3-joint fingers, which realizes force feedback control by the deep learning technique- Long Short-Term Memory (LSTM) model, without any force sensor. First, a five-linkage mechanism stacked by double four-linkages is designed as a finger to automatically achieve the transformation between parallel and enveloping grasping modes. This enables the creation of a low-cost under-actuated gripper comprising a single actuator and two 3-phalange fingers. Second, we devise theoretical models of kinematics and power transmission based on the proposed gripper, accurately obtaining fingertip positions and contact forces. Through coupling and decoupling of five-linkage mechanisms, the proposed gripper offers the expected capabilities of grasping payload/force/stability and objects with large dimension ranges. Third, to realize the force control, an LSTM model is proposed to determine the grasping mode for synthesizing force-feedback control policies that exploit contact sensing after outlining the uncertainty of currents using a statistical method. Finally, a series of experiments are implemented to measure quantitative indicators, such as the payload, grasping force, force sensing, grasping stability and the dimension ranges of objects to be grasped. Additionally, the grasping performance of the proposed gripper is verified experimentally to guarantee the high versatility and robustness of the proposed gripper. A very promising strategy combining mechanism design and artificial intelligence (AI) technology will be highly impactful on the construction of robotic grippers. A uploaded video in YouTube: https://youtu.be/TDyCUtxnePQ.},
  archive   = {C_RSS},
  author    = {Jihao Li and Keqi Zhu and Guodong Lu and I-Ming Chen and HUIXU DONG},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Construction of a multiple-DOF underactuated gripper with force-sensing via deep learning},
  url       = {https://www.roboticsproceedings.org/rss20/p101.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). A single motor nano aerial vehicle with novel
peer-to-peer communication and sensing mechanism. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p102.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Communication and position sensing are among the most important capabilities for swarm robots to interact with their peers and perform tasks collaboratively. However, the hardware required to facilitate communication and position sensing is often too complicated, expensive, and bulky to be carried on swarm robots. Here we present Maneuverable Piccolissimo 3 (MP3), a minimalist, single motor drone capable of executing inter-robot communication via infrared light and triangulation-based sensing of relative bearing, distance, and elevation using message arrival time. Thanks to its novel design, MP3 can communicate with peers and localize itself using simple components, keeping its size and mass small and making it inherently safe for human interaction. We present the hardware and software design of MP3 and demonstrate its capability to localize itself, fly stably, and maneuver in the environment using peer-to-peer communication and sensing.},
  archive   = {C_RSS},
  author    = {Jingxian Wang and Andrew G. Curtis and Mark Yim and Michael Rubenstein},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {A single motor nano aerial vehicle with novel peer-to-peer communication and sensing mechanism},
  url       = {https://www.roboticsproceedings.org/rss20/p102.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Design and control of a bipedal robotic character.
<em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p103.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Legged robots have achieved impressive feats in dynamic locomotion in challenging unstructured terrain. However, in entertainment applications, the design and control of these robots face additional challenges in appealing to human audiences. This work aims to unify expressive, artist-directed motions and robust dynamic mobility for legged robots. To this end, we introduce a new bipedal robot, designed with a focus on character-driven mechanical features. We present a reinforcement learning-based control architecture to robustly execute artistic motions conditioned on command signals. During runtime, these command signals are generated by an animation engine which composes and blends between multiple animation sources. Finally, an intuitive operator interface enables real-time show performances with the robot.},
  archive   = {C_RSS},
  author    = {Ruben Grandia and Espen Knoop and Michael A. Hopkins and Georg Wiedebach and Jared Bishop and Steven Pickles and David Müller and Moritz Bächer},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Design and control of a bipedal robotic character},
  url       = {https://www.roboticsproceedings.org/rss20/p103.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). POLICEd RL: Learning closed-loop robot control
policies with provable satisfaction of hard constraints. <em>RSS</em>.
(<a href="https://www.roboticsproceedings.org/rss20/p104.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we seek to learn a robot policy guaranteed to satisfy state constraints. To encourage constraint satisfaction, existing RL algorithms typically rely on Constrained Markov Decision Processes and discourage constraint violations through reward shaping. However, such soft constraints cannot offer safety guarantees. To address this gap, we propose POLICEd RL, a novel RL algorithm explicitly designed to enforce affine hard constraints in closed-loop with a black-box environment. Our key insight is to make the learned policy be affine around the unsafe set and to use this affine region as a repulsive buffer to prevent trajectories from violating the constraint. We prove that such policies exist and guarantee constraint satisfaction. Our proposed framework is applicable to both systems with continuous and discrete state and action spaces and is agnostic to the choice of the RL training algorithm. Our results demonstrate the capacity of POLICEd RL to enforce hard constraints in robotic tasks while significantly outperforming existing methods. Code available at https://iconlab.negarmehr.com/POLICEd-RL/},
  archive   = {C_RSS},
  author    = {Jean-Baptiste Bouvier and Kartik Nagpal and Negar Mehr},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {POLICEd RL: Learning closed-loop robot control policies with provable satisfaction of hard constraints},
  url       = {https://www.roboticsproceedings.org/rss20/p104.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Demonstrating language-grounded motion controller.
<em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p105.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advancements have enabled human-robot collaboration through physical assistance and verbal guidance. However, limitations persist in coordinating robots&#39; physical motions and speech in response to real-time changes in human behavior during collaborative contact tasks. We first derive principles from analyzing physical therapists&#39; movements and speech during patient exercises. These principles are translated into control objectives to: 1) guide users through trajectories, 2) control motion and speech pace to align completion times with varying user cooperation, and 3) dynamically paraphrase speech along the trajectory. We then propose a Language Controller that synchronizes motion and speech, modulating both based on user cooperation. Experiments with 12 users show the Language Controller successfully aligns motion and speech compared to baselines. This provides a framework for fluent human-robot collaboration.},
  archive   = {C_RSS},
  author    = {Ravi Tejwani and Chengyuan Ma and Paco Gomez-Paz and Paolo Bonato and Haruhiko Asada},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Demonstrating language-grounded motion controller},
  url       = {https://www.roboticsproceedings.org/rss20/p105.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). VLMPC: Vision-language model predictive control for
robotic manipulation. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p106.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although Model Predictive Control (MPC) can effectively predict the future states of a system and thus is widely used in robotic manipulation tasks, it does not have the capability of environmental perception, leading to the failure in some complex scenarios. To address this issue, we introduce Vision-Language Model Predictive Control (VLMPC), a robotic manipulation framework which takes advantage of the powerful perception capability of vision language model (VLM) and integrates it with MPC. Specifically, we propose a conditional action sampling module which takes as input a goal image or a language instruction and leverages VLM to sample a set of candidate action sequences. Then, a lightweight action-conditioned video prediction model is designed to generate a set of future frames conditioned on the candidate action sequences. VLMPC produces the optimal action sequence with the assistance of VLM through a hierarchical cost function that formulates both pixel-level and knowledge-level consistence between the current observation and the goal image. We demonstrate that VLMPC outperforms the state-of-the-art methods on public benchmarks. More importantly, our method showcases excellent performance in various real-world tasks of robotic manipulation. We shall release the code and data if the paper is accepted.},
  archive   = {C_RSS},
  author    = {Wentao Zhao and Jiaming Chen and Ziyu Meng and DonghuiMao and Ran Song and Wei Zhang},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {VLMPC: Vision-language model predictive control for robotic manipulation},
  url       = {https://www.roboticsproceedings.org/rss20/p106.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Expressive whole-body control for humanoid robots.
<em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p107.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Can we enable humanoid robots to generate rich, diverse, and expressive motions in the real world? We propose to learn a whole-body control policy on a human-sized robot to mimic human motions as realistic as possible. To train such a policy, we leverage the large-scale human motion capture data from the graphics community in a Reinforcement Learning framework. However, directly performing imitation learning with the motion capture dataset would not work on the real humanoid robot, given the large gap in degrees of freedom and physical capabilities. Our method $\textbf{Ex}$pressive Whole-$\textbf{Body}$ Control ($\textbf{ExBody}$) tackles this problem by encouraging the upper humanoid body to imitate a reference motion, while relaxing the imitation constraint on its two legs and only requiring them to follow a given velocity robustly. With training in simulation and Sim2Real transfer, our policy can control a humanoid robot to walk in different styles, shake hands with humans, and even dance with a human in the real world. We conduct extensive studies and comparisons on diverse motions in both simulation and the real world to show the effectiveness of our approach.},
  archive   = {C_RSS},
  author    = {Xuxin Cheng and Yandong Ji and Junming Chen and Ruihan Yang and Ge Yang and Xiaolong Wang},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Expressive whole-body control for humanoid robots},
  url       = {https://www.roboticsproceedings.org/rss20/p107.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). From compliant to rigid contact simulation: A unified
and efficient approach. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p108.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Whether rigid or compliant, contact interactions are inherent to robot motions, enabling them to move or manipulate things. Contact interactions result from complex physical phenomena, that can be mathematically cast as Nonlinear Complementarity Problems (NCPs) in the context of rigid or compliant point contact interactions. Such a class of complementarity problems is, in general, difficult to solve both from an optimization and numerical perspective. Over the past decades, dedicated and specialized contact solvers, implemented in modern robotics simulators (e.g., Bullet, Drake, MuJoCo, DART, Raisim) have emerged. Yet, most of these solvers tend either to solve a relaxed formulation of the original contact problems (at the price of physical inconsistencies) or to scale poorly with the problem dimension or its numerical conditioning (e.g., a robotic hand manipulating a paper sheet). In this paper, we introduce a unified and efficient approach to solving NCPs in the context of contact simulation. It relies on a sound combination of the Alternating Direction Method of Multipliers (ADMM) and proximal algorithms to account for both compliant and rigid contact interfaces in a unified way. To handle ill-conditioned problems and accelerate the convergence rate, we also propose an efficient update strategy to adapt the ADMM hyperparameters automatically. By leveraging proximal methods, we also propose new algorithmic solutions to efficiently evaluate the inverse dynamics involving rigid and compliant contact interactions, extending the approach developed in MuJoCo. We validate the efficiency and robustness of our contact solver against several alternative contact methods of the literature and benchmark them on various robotics and granular mechanics scenarios. Overall, the proposed approach is shown to be competitive against classic methods for simple contact problems and outperforms existing solutions on more complex scenarios, involving tens of contacts and poor conditioning.},
  archive   = {C_RSS},
  author    = {Justin Carpentier and Quentin Le Lidec and Louis Montaut},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {From compliant to rigid contact simulation: A unified and efficient approach},
  url       = {https://www.roboticsproceedings.org/rss20/p108.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). MPCC++: Model predictive contouring control for
time-optimal flight with safety constraints. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p109.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Quadrotor flight is an extremely challenging problem due to the limited control authority encountered at the limit of handling. Model Predictive Contouring Control (MPCC) has emerged as a promising model-based approach for time optimization problems such as drone racing. However, the standard MPCC formulation used in quadrotor racing introduces the notion of the gates directly in the cost function, creating a multi-objective optimization that continuously trades off between maximizing progress and tracking the path accurately. This paper introduces three key components that enhance the state-of-the-art MPCC approach for drone racing. First and foremost, we provide safety guarantees in the form of a track constraint and terminal set. The track constraint is designed as a spatial constraint which prevents gate collisions while allowing for time optimization only in the cost function. Second, we augment the existing first principles dynamics with a residual term that captures complex aerodynamic effects and thrust forces learned directly from real-world data. Third, we use Trust Region Bayesian Optimization (TuRBO), a state-of-the-art global Bayesian Optimization algorithm, to tune the hyperparameters of the MPCC controller given a sparse reward based on lap time minimization. The proposed approach achieves similar lap times to the best-performing RL policy and outperforms the best model-based controller while satisfying constraints. In both simulation and real world, our approach consistently prevents gate crashes with 100% success rate, while pushing the quadrotor to its physical limits reaching speeds of more than 80km/h.},
  archive   = {C_RSS},
  author    = {Maria Krinner and Angel Romero and Leonard Bauersfeld and Melanie Zeilinger and Andrea Carron and Davide Scaramuzza},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {MPCC++: Model predictive contouring control for time-optimal flight with safety constraints},
  url       = {https://www.roboticsproceedings.org/rss20/p109.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Linear-time differential inverse kinematics: An
augmented lagrangian perspective. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p110.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For decades, inverse kinematics (IK) was an intense and active research area in robotics. Beyond analytical solutions limited to a restricted range of robotic systems and applications, differential inverse kinematics has emerged as a generic class of methods, able to cope with a wider variety of robots and scenarios, with quadratic programming-based approaches as the main paradigm. In this paper, we propose to revisit differential inverse kinematics from the perspective of augmented Lagrangian methods (AL) and the well-known related alternating direction method of multipliers (ADMM). Notably, by leveraging AL techniques and in the spirit of Featherstone algorithms, we introduce a rigid-body dynamics algorithm that solves equality-constrained IK problems with linear complexity in the number of robot joints and number of constraints. Combined with the ADMM strategy developed in the OSQP solver, we provide a new solution for the same class of problems as QP-based differential IK, yet with linear complexity in problem dimensions. We propose an open-source C++ implementation of this approach, which we validate on a large set of problems including manipulation and humanoid locomotion tasks. Our benchmark measures computation times 2--3 $\times$ shorter than the QP-based state of the art.},
  archive   = {C_RSS},
  author    = {Bruce Wingo and Ajay Suresha Sathya and Stéphane Caron and Seth Hutchinson and Justin Carpentier},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Linear-time differential inverse kinematics: An augmented lagrangian perspective},
  url       = {https://www.roboticsproceedings.org/rss20/p110.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). A trajectory tracking algorithm for the LSMS family of
cable-driven cranes. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p111.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Lightweight Surface Manipulation System, or LSMS, is a family of scalable long-reach cable-actuated manipulators. The design of the LSMS has a high payload ratio for efficient operations on planetary surfaces like the Moon or Mars. The LSMS has nonlinear, coupled, and hybrid dynamics. The engineering decisions that led to these challenging dynamics make this structure light and efficient. This paper proposes a novel trajectory tracking algorithm for these cranes that facilitates precise autonomous and teleoperated operations. This algorithm enables these robots to follow complex trajectories that avoid obstacles and pickup regolith in a construction site.},
  archive   = {C_RSS},
  author    = {Javier Puig-Navarro and Dominic R Bisio and John E Pye and Yotam Granov and Joshua N Moser and Jessica S. Friz and Walter J Waltz and Julia E Cline and B. Danette Allen},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {A trajectory tracking algorithm for the LSMS family of cable-driven cranes},
  url       = {https://www.roboticsproceedings.org/rss20/p111.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). AutoGPT+p: Affordance-based task planning using large
language models. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p112.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in task planning leverage Large Language Models (LLMs) to improve generalizability by combining such models with classical planning algorithms to address their inherent limitations in reasoning capabilities. However, these approaches face the challenge of dynamically capturing the initial state of the task planning problem. To alleviate this issue, we propose AutoGPT+P, a system that combines an affordance-based scene representation with a planning system. Affordances are the action possibilities of an agent on the environment and the objects present in it. Thus, deriving the planning domain from an affordance-based scene representation allows symbolic planning with arbitrary objects. AutoGPT+P leverages this representation to derive and execute a plan for a task specified by the user in natural language. In addition to solving planning tasks under a closed-world assumption, AutoGPT+P can also handle planning with incomplete information, such as tasks with missing objects, by exploring the scene, suggesting alternatives, or providing a partial plan. The affordance-based scene representation combines object detection with an Object Affordance Mapping that is automatically generated using ChatGPT. The core planning tool extends existing work by automatically correcting semantic and syntactic errors leading to a success rate of 98% on the SayCan instruction set. Furthermore, we evaluated our approach on our newly created dataset with 150 scenarios covering a wide range of complex tasks with missing objects, achieving a success rate of 79%. The dataset and the code are publicly available at https://git.h2t.iar.kit.edu/sw/autogpt-p.},
  archive   = {C_RSS},
  author    = {Timo Birr and Christoph Pohl and Abdelrahman Younes and Tamim Asfour},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {AutoGPT+P: Affordance-based task planning using large language models},
  url       = {https://www.roboticsproceedings.org/rss20/p112.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Implicit graph search for planning on graphs of convex
sets. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p113.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Smooth, collision-free motion planning is a fundamental challenge in robotics with a wide range of applications such as automated manufacturing, search \&amp; rescue, underwater exploration, etc. Graphs of Convex Sets (GCS) is a recent method for synthesizing smooth trajectories by decomposing the planning space into convex sets, forming a graph to encode the adjacency relationships within the decomposition, and then simultaneously searching this graph and optimizing parts of the trajectory to obtain the final trajectory. To do this, one must solve a Mixed Integer Convex Program (MICP) and to mitigate computational time, GCS proposes a convex relaxation that is empirically very tight. Despite this tight relaxation, motion planning with GCS for real-world robotics problems translates to solving the simultaneous batch optimization problem that may contain millions of constraints and therefore can be slow. This is further exacerbated by the fact that the size of the GCS problem is invariant to the planning query. Motivated by the observation that the trajectory solution lies only on a fraction of the set of convex sets, we present two implicit graph search methods for planning on the graph of convex sets called INSATxGCS (IxG) and IxG*. INterleaved Search And Trajectory optimization (INSAT) is a previously developed algorithm that alternates between searching on a graph and optimizing partial paths to find a smooth trajectory. By using an implicit graph search method INSAT on the graph of convex sets, we achieve faster planning while ensuring stronger guarantees on completeness and optimality. Moveover, introducing a search-based technique to plan on the graph of convex sets enables us to easily leverage well-established techniques such as search parallelization, lazy planning, anytime planning, and replanning as future work. Numerical comparisons against GCS demonstrate the superiority of IxG across several applications, including planning for an 18-degree-of-freedom multi-arm assembly scenario.},
  archive   = {C_RSS},
  author    = {Ramkumar Natarajan and Chaoqi Liu and Howie Choset and Maxim Likhachev},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Implicit graph search for planning on graphs of convex sets},
  url       = {https://www.roboticsproceedings.org/rss20/p113.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Real-time anomaly detection and reactive planning with
large language models. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p114.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Foundation models, e.g., large language models (LLMs), trained on internet-scale data possess zero-shot generalization capabilities that make them a promising technology towards detecting and mitigating out-of-distribution failure modes of robotic systems. Fully realizing this promise, however, poses two challenges: (i) mitigating the considerable computational expense of these models such that they may be applied online, and (ii) incorporating their judgement regarding potential anomalies into a safe control framework. In this work, we present a two-stage reasoning framework: First is a fast binary anomaly classifier that analyzes observations in an LLM embedding space, which may trigger a slower fallback selection stage that utilizes the reasoning capabilities of generative LLMs. These stages correspond to branch points in a model predictive control strategy that maintains the joint feasibility of continuing along various fallback plans to account for the slow reasoner&#39;s latency as soon as an anomaly is detected, thus ensuring safety. We show that our fast anomaly classifier outperforms autoregressive reasoning with state-of-the-art GPT models, even when instantiated with relatively small language models. This enables our runtime monitor to improve the trustworthiness of dynamic robotic systems, such as quadrotors or autonomous vehicles, under resource and time constraints. Videos illustrating our approach in both simulation and real-world experiments are available on our project page: https://sites.google.com/view/aesop-llm.},
  archive   = {C_RSS},
  author    = {Rohan Sinha and Amine Elhafsi and Christopher Agia and Matt Foutter and Edward Schmerling and Marco Pavone},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Real-time anomaly detection and reactive planning with large language models},
  url       = {https://www.roboticsproceedings.org/rss20/p114.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). IHERO: Interactive human-oriented exploration and
supervision under scarce communication. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p115.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Exploration of unknown scenes before human entry is essential for safety and efficiency in numerous scenarios, e.g., subterranean exploration, reconnaissance, search and rescue missions. Fleets of autonomous robots are particularly suitable for this task, via concurrent exploration, multi-sensory perception and autonomous navigation. Communication however among the robots can be severely restricted to only close-range exchange via ad-hoc networks. Although some recent works have addressed the problem of collaborative exploration under restricted communication, the crucial role of the human operator has been mostly neglected. Indeed, the operator may: (i) require timely update regarding the exploration progress and fleet status; (ii) prioritize certain regions; and (iii) dynamically move within the explored area; To facilitate these requests, this work proposes an interactive human-oriented online coordination framework for collaborative exploration and supervision under scarce communication (iHERO). The robots switch smoothly and optimally among fast exploration, intermittent exchange of map and sensory data, and return to the operator for status update. It is ensured that these requests are fulfilled online interactively with a pre-specified latency. Extensive large-scale human-in-the-loop simulations and hardware experiments are performed over numerous challenging scenes, which signify its performance such as explored area and efficiency, and validate its potential applicability to real-world scenarios. The videos are available on https://zl-tian.github.io/iHERO/.},
  archive   = {C_RSS},
  author    = {Zhuoli Tian and Yuyang Zhang and Jinsheng Wei and Meng Guo},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {IHERO: Interactive human-oriented exploration and supervision under scarce communication},
  url       = {https://www.roboticsproceedings.org/rss20/p115.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Who plays first? Optimizing the order of play in
stackelberg games with many robots. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p116.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the multi-agent spatial navigation problem of computing the socially optimal order of play, i.e., the sequence in which the agents commit to their decisions, and its associated equilibrium in an N-player Stackelberg trajectory game. We model this problem as a mixed-integer optimization problem over the space of all possible Stackelberg games associated with the order of play’s permutations. To solve the problem, we introduce Branch and Play (B&amp;P), an efficient and exact algorithm that provably converges to a socially optimal order of play and its Stackelberg equilibrium. As a subroutine for B&amp;P, we employ and extend sequential trajectory planning, i.e., a popular multi-agent control approach, to scalably compute valid local Stackelberg equilibria for any given order of play. We demonstrate the practical utility of B&amp;P to coordinate air traffic control, swarm formation, and delivery vehicle fleets. We find that B&amp;P consistently outperforms various baselines, and computes the socially optimal equilibrium.},
  archive   = {C_RSS},
  author    = {Haimin Hu and Gabriele Dragotto and Zixu Zhang and Kaiqu Liang and Bartolomeo Stellato and Jaime Fernández Fisac},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Who plays first? optimizing the order of play in stackelberg games with many robots},
  url       = {https://www.roboticsproceedings.org/rss20/p116.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Goal-reaching trajectory design near danger with
piecewise affine reach-avoid computation. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p117.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous mobile robots must maintain safety, but should not sacrifice performance, leading to the classical reach-avoid problem: find a trajectory that is guaranteed to reach a goal and avoid obstacles. This paper addresses the near danger case, also known as a narrow gap, where the agent starts near the goal, but must navigate through tight obstacles that block its path. The proposed method builds off the common approach of using a simplified planning model to generate plans, which are then tracked using a high-fidelity tracking model and controller. Existing approaches use reachability analysis to overapproximate the error between these models and ensure safety, but doing so introduces numerical approximation error conservativeness that prevents goal-reaching. The present work instead proposes a Piecewise Affine Reach-avoid Computation (PARC) method to tightly approximate the reachable set of the planning model. PARC significantly reduces conservativeness through a careful choice of the planning model and set representation, along with an effective approach to handling time-varying tracking errors. The utility of this method is demonstrated through extensive numerical experiments in which PARC outperforms state-of-the-art reach avoid methods in near-danger goal reaching. Furthermore, in a simulated demonstration, PARC enables the generation of provably-safe extreme vehicle dynamics drift parking maneuvers. A preliminary hardware demo on a TurtleBot3 also validates the method.},
  archive   = {C_RSS},
  author    = {Long Kiu Chung and Wonsuhk Jung and Chuizheng Kong and Shreyas Kousik},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Goal-reaching trajectory design near danger with piecewise affine reach-avoid computation},
  url       = {https://www.roboticsproceedings.org/rss20/p117.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Partially observable task and motion planning with
uncertainty and risk awareness. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p118.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Integrated task and motion planning (TAMP) has proven to be a valuable approach to generalizable long-horizon robotic manipulation and navigation problems. However, the typical TAMP problem formulation assumes full observability and deterministic action effects. These assumptions limit the ability of the planner to gather information and make decisions that are risk-aware. We propose a strategy for TAMP with Uncertainty and Risk Awareness (TAMPURA) that is capable of efficiently solving long-horizon planning problems with initial-state and action outcome uncertainty, including problems that require information gathering and avoiding undesirable and irreversible outcomes. Our planner reasons under uncertainty at both the abstract task level and continuous controller level. Given a set of closed-loop goal-conditioned controllers operating in the primitive action space and a description of their preconditions and potential capabilities, we learn a high-level abstraction that can be solved efficiently and then refined to continuous actions for execution. We demonstrate our approach on several robotics problems where uncertainty is a crucial factor and show that reasoning under uncertainty in these problems outperforms previously proposed determinized planning, direct search, and reinforcement learning strategies. Lastly, we demonstrate our planner on two real-world robotics problems using recent advancements in probabilistic perception.},
  archive   = {C_RSS},
  author    = {Aidan Curtis and George Matheos and Nishad Gothoskar and Vikash Mansinghka and Joshua B. Tenenbaum and Tomás Lozano-Pérez and Leslie Pack Kaelbling},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Partially observable task and motion planning with uncertainty and risk awareness},
  url       = {https://www.roboticsproceedings.org/rss20/p118.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Logic-skill programming: An optimization-based
approach to sequential skill planning. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p119.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in robot skill learning have unlocked the potential to construct task-agnostic skill libraries, facilitating the seamless sequencing of multiple simple manipulation primitives (aka. skills) to tackle significantly more complex tasks. Nevertheless, determining the optimal sequence for independently learned skills remains an open problem, particularly when the objective is given solely in terms of the final geometric configuration rather than a symbolic goal. To address this challenge, we propose Logic-Skill Programming (LSP), an optimization-based approach that sequences independently learned skills to solve long-horizon tasks. We formulate a first-order extension of a mathematical program to optimize the overall cumulative reward of all skills within a plan, abstracted by the sum of value functions. To solve such programs, we leverage the use of tensor train factorization to construct the value function space, and rely on alternations between symbolic search and skill value optimization to find the appropriate skill skeleton and optimal subgoal sequence. Experimental results indicate that the obtained value functions provide a superior approximation of cumulative rewards compared to state-of-the-art reinforcement learning methods. Furthermore, we validate LSP in three manipulation domains, encompassing both prehensile and non-prehensile primitives. The results demonstrate its capability to identify the optimal solution over the full logic and geometric path. The real-robot experiments showcase the effectiveness of our approach to cope with contact uncertainty and external disturbances in the real world.},
  archive   = {C_RSS},
  author    = {Teng Xue and Amirreza Razmjoo and Suhan Shetty and Sylvain Calinon},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Logic-skill programming: An optimization-based approach to sequential skill planning},
  url       = {https://www.roboticsproceedings.org/rss20/p119.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). DROID: A large-scale in-the-wild robot manipulation
dataset. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p120.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The creation of large, diverse, high-quality robot manipulation datasets is an important stepping stone on the path toward more capable and robust robotic manipulation policies. However, creating such datasets is challenging: collecting robot manipulation data in diverse environments poses logistical and safety challenges and requires substantial investments in hardware and human labour. As a result, even the most general robot manipulation policies today are mostly trained on data collected in a small number of environments with limited scene and task diversity. In this work, we introduce DROID (Distributed Robot Interaction Dataset), a diverse robot manipulation dataset with 65k demonstration trajectories or 350h of interaction data, collected across 564 scenes and 86 tasks by 50 data collectors in North America, Asia, and Europe over the course of 12 months. We demonstrate that training with DROID leads to policies with higher performance, greater robustness, and improved generalization ability. We open source the full dataset, pre-trained model checkpoints, and a detailed guide for reproducing our robot hardware setup.},
  archive   = {C_RSS},
  author    = {Alexander Khazatsky and Karl Pertsch and Suraj Nair and Ashwin Balakrishna and Sudeep Dasari and Siddharth Karamcheti and Soroush Nasiriany and Mohan Kumar Srirama and Lawrence Yunliang Chen and Kirsty Ellis and Peter David Fagan and Joey Hejna and Masha Itkina and Marion Lepert and Yecheng Jason Ma and Patrick Tree Miller and Jimmy Wu and Suneel Belkhale and Shivin Dass and Huy Ha and Arhan Jain and Abraham Lee and Youngwoon Lee and Marius Memmel and Sungjae Park and Ilija Radosavovic and Kaiyuan Wang and Albert Zhan and Kevin Black and Cheng Chi and Kyle Beltran Hatch and Shan Lin and Jingpei Lu and Jean Mercat and Abdul Rehman and Pannag R Sanketi and Archit Sharma and Cody Simpson and Quan Vuong and Homer Rich Walke and Blake Wulfe and Ted Xiao and Jonathan Heewon Yang and Arefeh Yavary and Tony Z. Zhao and Christopher Agia and Rohan Baijal and Mateo Guaman Castro and Daphne Chen and Qiuyu Chen and Trinity Chung and Jaimyn Drake and Ethan Paul Foster and Jensen Gao and David Antonio Herrera and Minho Heo and Kyle Hsu and Jiaheng Hu and Donovon Jackson and Charlotte Le and Yunshuang Li and Roy Lin and Zehan Ma and Abhiram Maddukuri and Suvir Mirchandani and Daniel Morton and Tony Nguyen and Abigail O&#39;Neill and Rosario Scalise and Derick Seale and Victor Son and Stephen Tian and Emi Tran and Andrew E. Wang and Yilin Wu and Annie Xie and Jingyun Yang and Patrick Yin and Yunchu Zhang and Osbert Bastani and Glen Berseth and Jeannette Bohg and Ken Goldberg and Abhinav Gupta and Abhishek Gupta and Dinesh Jayaraman and Joseph J Lim and Jitendra Malik and Roberto Martín-Martín and Subramanian Ramamoorthy and Dorsa Sadigh and Shuran Song and Jiajun Wu and Michael C. Yip and Yuke Zhu and Thomas Kollar and Sergey Levine and Chelsea Finn},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {DROID: A large-scale in-the-wild robot manipulation dataset},
  url       = {https://www.roboticsproceedings.org/rss20/p120.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Multimodal diffusion transformer: Learning versatile
behavior from multimodal goals. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p121.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work introduces the Multimodal Diffusion Transformer (MDT), a novel diffusion policy framework, that excels at learning versatile behavior from multimodal goal specifications with few language annotations. MDT leverages a diffusion based multimodal transformer backbone and two self-supervised auxiliary objectives to master long-horizon manipulation tasks based on multimodal goals. The vast majority of imitation learning methods only learn from individual goal modalities, e.g. either language or goal images. However, existing large-scale imitation learning datasets are only partially labeled with language annotations, which prohibits current methods from learning language conditioned behavior from these datasets. MDT addresses this challenge by introducing a latent goal-conditioned state representation, that is simultaneously trained on multimodal goal instructions. This state representation aligns image and language based goal embeddings and encodes sufficient information to predict future states. The representation is trained via two self-supervised auxiliary objectives that enhance the performance of the presented transformer backbone. MDT shows exceptional performance on 164 tasks provided by the challenging CALVIN and LIBERO benchmarks, including a LIBERO version that contains less than $2\\%$ language annotations. Further, MDT establishes a new record on the CALVIN manipulation challenge, demonstrating an absolute performance improvement of $15\\%$ over prior state-of-the-art methods, that require large-scale pretraining and contain $10\times$ more learnable parameters. MDT demonstrated its ability to solve long-horizon manipulation from sparsely annotated data in both simulated and real-world environments. Demonstrations and Code are available at https://intuitive-robots.github.io/mdt_policy/.},
  archive   = {C_RSS},
  author    = {Moritz Reuss and Ömer Erdinç Yağmurlu and Fabian Wenzel and Rudolf Lioutikov},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Multimodal diffusion transformer: Learning versatile behavior from multimodal goals},
  url       = {https://www.roboticsproceedings.org/rss20/p121.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Don’t start from scratch: Behavioral refinement via
interpolant-based policy diffusion. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p122.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Imitation learning empowers artificial agents to mimic behavior by learning from demonstrations. Recently, diffusion models, which have the ability to model high-dimensional and multimodal distributions, have shown impressive performance on imitation learning tasks. These models learn to shape a policy by diffusing actions (or states) from standard Gaussian noise. However, the target policy to learn is often significantly different from Gaussian and this mismatch can result in poor performance when using a small number of diffusion steps (to improve inference speed) and under limited data. The key idea in this work is that initiating from a more informative source than Gaussian enables diffusion methods to mitigate the above limitations. We contribute both theoretical results, a new method, and empirical findings that show the benefits of using an informative source policy. Our method, which we call BRIDGeR, leverages the stochastic interpolants framework to bridge arbitrary policies, thus enabling a flexible approach towards imitation learning. It generalizes prior work in that standard Gaussians can still be applied, but other source policies can be used if available. In experiments on challenging simulation benchmarks and on real robots, BRIDGeR outperforms state-of-the-art diffusion policies. We provide further analysis on design considerations when applying BRIDGeR. Code for BRIDGeR is available at https://github.com/clear-nus/bridger.},
  archive   = {C_RSS},
  author    = {Kaiqi Chen and Eugene Lim and Lin Kelvin and Yiyang Chen and Harold Soh},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Don&#39;t start from scratch: Behavioral refinement via interpolant-based policy diffusion},
  url       = {https://www.roboticsproceedings.org/rss20/p122.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Learning manipulation by predicting interaction.
<em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p123.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Representation learning approaches for robotic manipulation have boomed in recent years. Due to the scarcity of in-domain robot data, prevailing methodologies tend to leverage large-scale human video datasets to extract generalizable features for visuomotor policy learning. Despite the progress achieved, prior endeavors disregard the interactive dynamics that capture behavior patterns and physical interaction during the manipulation process, resulting in an inadequate understanding of the relationship between objects and the environment. To this end, we propose a general pre-training pipeline that learns Manipulation by Predicting the Interaction (MPI) and enhances the visual representation. Given a pair of keyframes representing the initial and final states, along with language instructions, our algorithm predicts the transition frame and detects the interaction object, respectively. These two learning objectives achieve superior comprehension towards “how-to-interact” and “where-to-interact”. We conduct a comprehensive evaluation of four challenging robotic tasks. The experimental results demonstrate that MPI exhibits remarkable improvement by 10% to 64% compared with previous state-of-the-art in real-world robot platforms as well as simulation environments. Code and checkpoints are publicly shared at https://github.com/OpenDriveLab/MPI.},
  archive   = {C_RSS},
  author    = {Jia Zeng and Qingwen Bu and Bangjun Wang and Wenke Xia and Li Chen and Hao Dong and Haoming Song and Dong Wang and Di Hu and Ping Luo and Heming Cui and Bin Zhao and Xuelong Li and Yu Qiao and Hongyang Li},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Learning manipulation by predicting interaction},
  url       = {https://www.roboticsproceedings.org/rss20/p123.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). URDFormer: A pipeline for constructing articulated
simulation environments from real-world images. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p124.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Constructing accurate and targeted simulation scenes that are both visually and physically realistic is a problem of significant practical interest in domains ranging from robotics to computer vision. This problem has become even more relevant as researchers wielding large data-hungry learning methods seek new sources of training data for physical decision-making systems. However, building simulation models is often still done by hand - a graphic designer and a simulation engineer work with predefined assets to construct rich scenes with realistic dynamic and kinematic properties. While this may scale to small numbers of scenes, to achieve the generalization properties that are required for data-driven robotic control, we require a pipeline that is able to synthesize large numbers of realistic scenes, complete with ``natural&quot; kinematic and dynamic structure. To attack this problem, we develop models for inferring structure and generating simulation scenes from natural images, allowing for scalable scene generation from web-scale datasets. To train these image-to-simulation models, we show how controllable text-to-image generative models can be used in generating paired training data that allows for modeling of the inverse problem, mapping from realistic images back to complete scene models. We show how this paradigm allows us to build large datasets of scenes in simulation with semantic and physical realism. We present an integrated end-to-end pipeline that generates simulation scenes complete with articulated kinematic and dynamic structures from real-world images and use these for training robotic control policies. We then robustly deploy in the real world for tasks like articulated object manipulation. In doing so, our work provides both a data generation pipeline for large-scale generation of simulation environments and an integrated system for training robust robotic control policies in the resulting environments.},
  archive   = {C_RSS},
  author    = {Qiuyu Chen and Aaron Walsman and Marius Memmel and Kaichun Mo and Alex Fang and Dieter Fox and Abhishek Gupta},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {URDFormer: A pipeline for constructing articulated simulation environments from real-world images},
  url       = {https://www.roboticsproceedings.org/rss20/p124.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Learning to learn faster from human feedback with
language model predictive control. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p125.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large language models (LLMs) have been shown to exhibit a wide range of capabilities, such as writing robot code from language commands -- enabling non-experts to direct robot behaviors, modify them based on feedback, or compose them to perform new tasks. However, these capabilities (driven by in-context learning) are limited to short-term interactions, where users&#39; feedback remains relevant for only as long as it fits within the context size of the LLM, and can be forgotten over longer interactions. In this work, we investigate fine-tuning the robot code-writing LLMs, to remember their in-context interactions and improve their teachability i.e., how efficiently they adapt to human inputs (measured by average number of corrections before the user considers the task successful). Our key observation is that when human-robot interactions are viewed as a partially observable Markov decision process (in which human language inputs are observations, and robot code outputs are actions), then training an LLM to complete previous interactions is training a transition dynamics model – that can be combined with classic robotics techniques such as model predictive control (MPC) to discover shorter paths to success. This gives rise to Language Model Predictive Control (LMPC), a framework that fine-tunes PaLM 2 to improve its teachability on 78 tasks across 5 robot embodiments – improving non-expert teaching success rates of unseen tasks by 26.9% while reducing the average number of human corrections from 2.4 to 1.9. Experiments show that LMPC also produces strong meta-learners, improving the success rate of in-context learning new tasks on unseen robot embodiments and APIs by 31.5%. See videos, code, and demos at: https://robot-teaching.github.io/},
  archive   = {C_RSS},
  author    = {Jacky Liang and Fei Xia and Wenhao Yu and Andy Zeng and Maria Attarian and Maria Bauza Villalonga and Matthew Bennice and Alex Bewley and Adil Dostmohamed and Chuyuan Fu and Nimrod Gileadi and Marissa Giustina and Keerthana Gopalakrishnan and Leonard Hasenclever and Jan Humplik and Jasmine Hsu and Nikhil J Joshi and Ben Jyenis and J Chase Kew and Sean Kirmani and Tsang-Wei Edward Lee and Kuang-Huei Lee and Assaf Hurwitz Michaely and Joss Moore and Kenneth Oslund and Dushyant Rao and Allen Z. Ren and Baruch Tabanpour and Quan Vuong and Ayzaan Wahid and Ted Xiao and Ying Xu and Vincent Zhuang and Peng Xu and Erik Frey and Ken Caluwaerts and Tingnan Zhang and Brian Ichter and Jonathan Tompson and Leila Takayama and Vincent Vanhoucke and Izhak Shafran and Maja Mataric and Dorsa Sadigh and Nicolas Heess and Kanishka Rao and Nik Stewart and Jie Tan and Carolina Parada},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Learning to learn faster from human feedback with language model predictive control},
  url       = {https://www.roboticsproceedings.org/rss20/p125.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Natural language can help bridge the Sim2Real gap.
<em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p126.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The main challenge in learning image-conditioned robotic policies is acquiring a visual representation conducive to low-level control. Due to the high dimensionality of the image space, learning a good visual representation requires a considerable amount of visual data. However, when learning in the real world, data is expensive. Sim2Real is a promising paradigm for overcoming data scarcity in the real-world target domain by using a simulator to collect large amounts of cheap data closely related to the target task. However, it is difficult to transfer an image-conditioned policy from sim to real when the domains are very visually dissimilar. To bridge the sim2real visual gap, we propose using natural language descriptions of images as a unifying signal across domains that captures the underlying task-relevant semantics. Our key insight is that if two image observations from different domains are labeled with similar language, the policy should predict similar action distributions for both images. We demonstrate that training the image encoder to predict the language description or the distance between descriptions of a sim or real image serves as a useful, data-efficient pretraining step that helps learn a domain-invariant image representation. We can then use this image encoder as the backbone of an IL policy trained simultaneously on a large amount of simulated and a handful of real demonstrations. Our approach outperforms widely used prior sim2real methods and strong vision-language pretraining baselines like CLIP and R3M by 25 to 40\%. See additional videos and materials at https://robin-lab.cs.utexas.edu/lang4sim2real/.},
  archive   = {C_RSS},
  author    = {Albert Yu and Adeline Foote and Ray Mooney and Roberto Martín-Martín},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Natural language can help bridge the Sim2Real gap},
  url       = {https://www.roboticsproceedings.org/rss20/p126.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). PoCo: Policy composition from and for heterogeneous
robot learning. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p127.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Training general robotic policies from heterogeneous data for different tasks is a significant challenge. Existing robotic datasets vary in different modalities such as color, depth, tactile, and proprioceptive information, and collected in different domains such as simulation, real robots, and human videos. Current methods usually collect and pool all data from one domain to train a single policy to handle such heterogeneity in tasks and domains, which is prohibitively expensive and difficult. In this work, we present a flexible approach, dubbed Policy Composition, to combine information across such diverse modalities and domains for learning scene-level and task-level generalized manipulation skills, by composing different data distributions represented with diffusion models. Our method can use task-level composition for multi-task manipulation and be composed with analytic cost functions to adapt policy behaviors at inference time. We train our method on simulation, human, and real robot data and evaluate in tool-use tasks. The composed policy achieves robust and dexterous performance under varying scenes and tasks and outperforms baselines from a single data source in both simulation and real-world experiments.},
  archive   = {C_RSS},
  author    = {Lirui Wang and Jialiang Zhao and Yilun Du and Edward Adelson and Russ Tedrake},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {PoCo: Policy composition from and for heterogeneous robot learning},
  url       = {https://www.roboticsproceedings.org/rss20/p127.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Tilde: Teleoperation for dexterous in-hand
manipulation learning with a DeltaHand. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p128.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dexterous robotic manipulation remains a challenging domain due to its strict demands for precision and robustness on both hardware and software. While dexterous robotic hands have demonstrated remarkable capabilities in complex tasks, efficiently learning adaptive control policies for hands still presents a significant hurdle given the high dimensionalities of hands and tasks. To bridge this gap, we propose Tilde, an imitation learning-based in-hand manipulation system on a dexterous DeltaHand. It leverages 1) a low-cost, configurable, simple-to-control, soft dexterous robotic hand, DeltaHand, 2) a user-friendly, precise, real-time teleoperation interface, TeleHand, and 3) an efficient and generalizable imitation learning approach with diffusion policies. Our proposed TeleHand has a kinematic twin design to the DeltaHand that enables precise one-to-one joint control of the DeltaHand during teleoperation. This facilitates efficient high-quality data collection of human demonstrations in the real world. To evaluate the effectiveness of our system, we demonstrate the fully autonomous closed-loop deployment of diffusion policies learned from demonstrations across seven dexterous manipulation tasks with an average 90\% success rate.},
  archive   = {C_RSS},
  author    = {Zilin Si and Kevin Lee Zhang and Zeynep Temel and Oliver Kroemer},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Tilde: Teleoperation for dexterous in-hand manipulation learning with a DeltaHand},
  url       = {https://www.roboticsproceedings.org/rss20/p128.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). HACMan++: Spatially-grounded motion primitives for
manipulation. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p129.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although end-to-end robot learning has shown some success for robot manipulation, the learned policies are often not sufficiently robust to variations in object pose or geometry. To improve the policy generalization, we introduce spatially-grounded parameterized motion primitives in our method HACMan++. Specifically, we propose an action representation consisting of three components: &quot;what&quot; primitive type (such as grasp or push) to execute, &quot;where&quot; the primitive will be grounded (e.g. where the gripper will make contact with the world), and &quot;how&quot; the primitive motion is executed, such as parameters specifying the push direction or grasp orientation. These three components define a novel discrete-continuous action space for reinforcement learning. Our framework enables robot agents to learn to chain diverse motion primitives together and select appropriate primitive parameters to complete long-horizon manipulation tasks. By grounding the primitives on a spatial location in the environment, our method is able to effectively generalize across object shape and pose variations. Our approach significantly outperforms existing methods, particularly in complex scenarios demanding both high-level sequential reasoning and object generalization. With zero-shot sim-to-real transfer, our policy succeeds in challenging real-world manipulation tasks, with generalization to unseen objects. Videos can be found on the project website: https://sgmp-rss2024.github.io.},
  archive   = {C_RSS},
  author    = {Bowen Jiang and Yilin Wu and Wenxuan Zhou and Chris Paxton and David Held},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {HACMan++: Spatially-grounded motion primitives for manipulation},
  url       = {https://www.roboticsproceedings.org/rss20/p129.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). RoboPack: Learning tactile-informed dynamics models
for dense packing. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p130.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tactile feedback is critical for understanding the dynamics of both rigid and deformable objects in many manipulation tasks, such as non-prehensile manipulation and dense packing. We introduce an approach that combines visual and tactile sensing for robotic manipulation by learning a neural, tactile-informed dynamics model. Our proposed framework, RoboPack, employs a recurrent graph neural network to estimate object states, including particles and object-level latent physics information, from historical visuo-tactile observations and to perform future state predictions. Our tactile-informed dynamics model, learned from real-world data, can solve downstream robotics tasks with model-predictive control. We demonstrate our approach on a real robot equipped with a compliant Soft-Bubble tactile sensor on non-prehensile manipulation and dense packing tasks, where the robot must infer the physics properties of objects from direct and indirect interactions. Trained on only an average of 30 minutes of real-world interaction data per task, our model can perform online adaptation and make touch-informed predictions. Through extensive evaluations in both long-horizon dynamics prediction and real-world manipulation, our method demonstrates superior effectiveness compared to previous learning-based and physics-based simulation systems.},
  archive   = {C_RSS},
  author    = {Bo Ai and Stephen Tian and Haochen Shi and Yixuan Wang and Cheston Tan and Yunzhu Li and Jiajun Wu},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {RoboPack: Learning tactile-informed dynamics models for dense packing},
  url       = {https://www.roboticsproceedings.org/rss20/p130.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Configuration space distance fields for manipulation
planning. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p131.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The signed distance field (SDF) is a popular implicit shape representation in robotics, providing geometric information about objects and obstacles in a form that can easily be combined with control, optimization and learning techniques. Most often, SDFs are used to represent distances in task space, which corresponds to the familiar notion of distances that we perceive in our 3D world. However, SDFs can mathematically be used in other spaces, including robot configuration spaces. For a robot manipulator, this configuration space typically corresponds to the joint angles for each articulation of the robot. While it is customary in robot planning to express which portions of the configuration space are free from collision with obstacles, it is less common to think of this information as a distance field in the configuration space. In this paper, we demonstrate the potential of considering SDFs in the robot configuration space for optimization, which we call configuration space distance field (or CDField for short). Similarly to the use of SDF in task space, CDField provides an efficient joint angle distance query and direct access to the derivatives (joint angle velocity). Most approaches split the overall computation with one part in task space followed by one part in configuration space (evaluating distances in task space and then computing actions with inverse kinematics). Instead, CDField allows the implicit structure to be leveraged by control, optimization, and learning problems in a unified manner. In particular, we propose an efficient algorithm to compute and fuse CDFields that can be generalized to arbitrary scenes. A corresponding neural CDField representation using multilayer perceptrons (MLPs) is also presented to obtain a compact and continuous representation while improving computation efficiency. We demonstrate the effectiveness of CDField with planar obstacle avoidance examples and with a 7-axis Franka Emika robot in inverse kinematics and manipulation planning tasks.},
  archive   = {C_RSS},
  author    = {Yiming Li and Xuemin Chi and Amirreza Razmjoo and Sylvain Calinon},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Configuration space distance fields for manipulation planning},
  url       = {https://www.roboticsproceedings.org/rss20/p131.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Towards tight convex relaxations for contact-rich
manipulation. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p132.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel method for global motion planning of robotic systems that interact with the environment through contacts. Our method directly handles the hybrid nature of such tasks using tools from convex optimization. We formulate the motion-planning problem as a shortest-path problem in a graph of convex sets, where a path in the graph corresponds to a contact sequence and a convex set models the quasi-static dynamics within a fixed contact mode. For each contact mode, we use semidefinite programming to relax the nonconvex dynamics that results from the simultaneous optimization of the object&#39;s pose, contact locations, and contact forces. The result is a tight convex relaxation of the overall planning problem, that can be efficiently solved and quickly rounded to find a feasible contact-rich trajectory. As an initial application for evaluating our method, we apply it on the task of planar pushing. Exhaustive experiments show that our convex-optimization method generates plans that are consistently within a small percentage of the global optimum, without relying on an initial guess, and that our method succeeds in finding trajectories where a state-of-the-art baseline for contact-rich planning usually fails. We demonstrate the quality of these plans on a real robotic system.},
  archive   = {C_RSS},
  author    = {Bernhard Paus Graesdal and Shao Yuan Chew Chia and Tobia Marcucci and Savva Morozov and Alexandre Amice and Pablo Parrilo and Russ Tedrake},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Towards tight convex relaxations for contact-rich manipulation},
  url       = {https://www.roboticsproceedings.org/rss20/p132.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). THE COLOSSEUM: A benchmark for evaluating
generalization for robotic manipulation. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p133.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To realize effective large-scale, real-world robotic applications, we must evaluate how well our robot policies adapt to changes in environmental conditions. Unfortunately, a majority of studies evaluate robot performance in environments closely resembling or even identical to the training setup. We present THE COLOSSEUM, a novel simulation benchmark, with 20 diverse manipulation tasks, that enables systematical evaluation of models across 14 axes of environmental perturbations. These perturbations include changes in color, texture, and size of objects, table-tops, and backgrounds; we also vary lighting, distractors, physical properties perturbations and camera pose. Using THE COLOSSEUM, we compare 5 state-of-the-art manipulation models to reveal that their success rate degrades between 30-50% across these perturbation factors. When multiple perturbations are applied in unison, the success rate degrades ≥75%. We identify that changing the number of distractor objects, target object color, or lighting conditions are the perturbations that reduce model performance the most. To verify the ecological validity of our results, we show that our results in simulation are correlated ( ̄R2 = 0.614) to similar perturbations in real-world experiments. We open source code for others to use THE COLOSSEUM, and also release code to 3D print the objects used to replicate the real-world perturbations. Ultimately, we hope that THE COLOSSEUM will serve as a benchmark to identify modeling decisions that systematically improve generalization for manipulation.},
  archive   = {C_RSS},
  author    = {Wilbert Pumacay and Ishika Singh and Jiafei Duan and Ranjay Krishna and Jesse Thomason and Dieter Fox},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {THE COLOSSEUM: A benchmark for evaluating generalization for robotic manipulation},
  url       = {https://www.roboticsproceedings.org/rss20/p133.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). One-shot imitation learning with invariance matching
for robotic manipulation. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p134.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning a single universal policy that can perform a diverse set of manipulation tasks is a promising new direction in robotics. However, existing techniques are limited to learning policies that can only perform tasks that are encountered during training, and require a large number of demonstrations to learn new tasks. Humans, on the other hand, often can learn a new task from a single unannotated demonstration. In this work, we propose the Invariance-Matching One-shot Policy Learning (IMOP) algorithm. In contrast to the standard practice of learning the end-effector’s pose directly, IMOP first learns invariant regions of the state space for a given task, and then computes the end-effector’s pose through matching the invariant regions between demonstrations and test scenes. Trained on the 18 RLBench tasks, IMOP achieves a success rate that outperforms the state- of-the-art consistently, by 4.5% on average over the 18 tasks. More importantly, IMOP can learn a novel task from a single unannotated demonstration, and without any fine-tuning, and achieves an average success rate improvement of 11.5% over the state-of-the-art on 22 novel tasks selected across nine categories. IMOP can also generalize to new shapes and learn to manipulate objects that are different from those in the demonstration. Further, IMOP can perform one-shot sim-to-real transfer using a single real-robot demonstration.},
  archive   = {C_RSS},
  author    = {Xinyu Zhang and Abdeslam Boularias},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {One-shot imitation learning with invariance matching for robotic manipulation},
  url       = {https://www.roboticsproceedings.org/rss20/p134.html},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024, July). Tactile-driven non-prehensile object manipulation via
extrinsic contact mode control. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss20/p135.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we consider the problem of non-prehensile manipulation using grasped objects. This problem is a superset of many common manipulation skills including instances of tool-use (e.g., grasped spatula flipping a burger) and assembly (e.g., screwdriver tightening a screw). Here, we present an algorithmic approach for non-prehensile manipulation leveraging a gripper with highly compliant and high-resolution tactile sensors. Our approach solves for robot actions that drive object poses and forces to desired values while obeying the complex dynamics induced by the sensors as well as the constraints imposed by static equilibrium, object kinematics, and frictional contact. Our method is able to produce a variety of manipulation skills and is amenable to gradient-based optimization by exploiting differentiability within contact modes (e.g., specifications of sticking or sliding contacts). We evaluate 4 variants of controllers that attempt to realize these plans and demonstrate a number of complex skills including non-prehensile planar sliding and pivoting on a variety of object geometries. The perception and controls capabilities that drive these skills are the building blocks towards dexterous and reactive autonomy in unstructured environments.},
  archive   = {C_RSS},
  author    = {Miquel Oller and Dmitry Berenson and Nima Fazeli},
  booktitle = {Robotics: Science and Systems XX},
  month     = {7},
  title     = {Tactile-driven non-prehensile object manipulation via extrinsic contact mode control},
  url       = {https://www.roboticsproceedings.org/rss20/p135.html},
  year      = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
