<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>KDD_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="kdd---642">KDD - 642</h2>
<ul>
<li><details>
<summary>
(2024). The 5th international workshop on talent and management
computing (TMC’2024). <em>KDD</em>, 6759–6760. (<a
href="https://doi.org/10.1145/3637528.3671479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In today&#39;s competitive and fast-evolving business environment, it is a critical time for organizations to rethink how to deal with talent and management-related tasks in a quantitative manner. Indeed, thanks to the era of big data, the availability of large-scale talent data provides unparalleled opportunities for business leaders to understand the rules of talent and management, which in turn deliver intelligence for effective decision-making and management for their organizations. In the past few years, talent and management computing have increasingly attracted attention from KDD communities, and a number of research/applied data science efforts have been devoted. To this end, the purpose of this workshop, i.e., the 5th International Workshop on Talent and Management Computing (TMC&#39;2024), is to bring together researchers and practitioners to discuss both the critical problems faced by talent and management-related domains and potential data-driven solutions by leveraging state-of-the-art data mining technologies.},
  archive   = {C_KDD},
  author    = {Zhu, Hengshu and Ge, Yong and Xiong, Hui and Lim, Ee-Peng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671479},
  pages     = {6759–6760},
  title     = {The 5th international workshop on talent and management computing (TMC&#39;2024)},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The fourth international workshop on smart data for
blockchain and distributed ledger (SDBD’24). <em>KDD</em>, 6757–6758.
(<a href="https://doi.org/10.1145/3637528.3671475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the advent of Bitcoin, a cryptographically-enabled peer-to-peer digital payment system, blockchain together with a whole package of distributed ledger technologies, which serve as the underlying foundation of all the crypto-currencies, have been gaining attention from both academia and industry in the last fifteen years. The recent years have witnessed tremendous momentum in the development of blockchain and distributed ledger technologies, largely due to the impressive rise in the market capital of these digital tokens. More and more industries, from banking and insurance, to supply chain and e-commerce, are quickly realizing the great potential in blockchain technology in efficiency boost, process automation and secure data sharing across otherwise isolated data silos. Furthermore, as the recognition of the data value began to sink in, data assets has become an essential part of the development of enterprises and countries. Blockchain technology is regarded as the foundation of digital economy and provides an effective approach for data ownership, pricing and transactions, which are the core issues of data asset management. However, the potential implications of Blockchain technologies go far beyond their application as the technological backbone for cryptocurrencies. Web3.0, using blockchain as underlying technology, allow for various novel application scenarios, which are built upon distributed consensus and thus are hard to block or censor while providing public verifiability of peer-to-peer transactions without a trusted central party. Web3.0 are expected to become the main front for a plethora of highly expressive applications. To more thoroughly explore the potential of blockchain and web3.0 and promote their progress, SDBD&#39;24 will provide a forum for the most recent blockchain and web3.0 research, innovations, and applications, bridging the gap between theory and practice in the design.},
  archive   = {C_KDD},
  author    = {Zhu, Feida and Pei, Jian and Zeller, Michael and Zhang, Bingxue},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671475},
  pages     = {6757–6758},
  title     = {The fourth international workshop on smart data for blockchain and distributed ledger (SDBD&#39;24)},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3rd workshop on uncertainty reasoning and quantification in
decision making (UDM). <em>KDD</em>, 6755–6756. (<a
href="https://doi.org/10.1145/3637528.3671496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Uncertainty reasoning and quantification play a critical role in decision making across various domains, prompting increased attention from both academia and industry. As real-world applications become more complex and data-driven, effectively handling uncertainty becomes paramount for accurate and reliable decision making. This workshop focuses on the critical topics of uncertainty reasoning and quantification in decision making. It provides a platform for experts and researchers from diverse backgrounds to exchange ideas on cutting-edge techniques and challenges in this field. The interdisciplinary nature of uncertainty reasoning and quantification, spanning artificial intelligence, machine learning, statistics, risk analysis, and decision science, will be explored. The workshop aims to address the need for robust and interpretable methods for modeling and quantifying uncertainty, fostering reasoning decision-making in various domains. Participants will have the opportunity to share research findings and practical experiences, promoting collaboration and advancing decision-making practices under uncertainty.},
  archive   = {C_KDD},
  author    = {Zhao, Xujiang and Zhao, Chen and Chen, Feng and Cho, Jin-Hee and Hua, Wei and Chen, Haifeng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671496},
  pages     = {6755–6756},
  title     = {3rd workshop on uncertainty reasoning and quantification in decision making (UDM)},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3rd workshop on end-end customer journey optimization.
<em>KDD</em>, 6753–6754. (<a
href="https://doi.org/10.1145/3637528.3671500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Nowadays, while most machine learning research on customer journey optimization has focused on short-term success metrics such as click-through rates or optimal ad placement, there has been little consideration given to developing a coherent system for end-to-end customer journey optimization. Such a system would encompass all aspects of the customer experience, from presenting the right product value to the right users, to understanding a user&#39;s likelihood of conversion and long-term value to the platform, as well as their propensity for cross-selling and risk of churning. Currently, models and algorithms for customer journey optimization are often developed in isolation, leading to inefficiencies in modeling and data pipelines. Furthermore, the customer is often viewed as a collection of different entities by different organizational departments (such as marketing, sales, and finance), which can lead to additional friction in the customer experience. This workshop seeks to bridge the gap between academic researchers and industrial practitioners who are interested in building holistic solutions for end-to-end customer journey optimization. In addition, with the rising popularity of generative AI and LLM, we want to use this venue to exchange ideas regarding their applications in different stages of customer journey, and how the new technologies could help businesses achieve their KPIs.},
  archive   = {C_KDD},
  author    = {Zhao, Shadow and Bay, Mert and Xu, Anbang and Gupta, Neha},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671500},
  pages     = {6753–6754},
  title     = {3rd workshop on end-end customer journey optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3rd workshop on ethical artificial intelligence: Methods and
applications (EAI). <em>KDD</em>, 6751–6752. (<a
href="https://doi.org/10.1145/3637528.3671482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ethical AI has become increasingly important, and it has been attracting attention from academia and industry, due to its increased popularity in real-world applications with fairness concerns. It also places fundamental importance on ethical considerations in determining legitimate and illegitimate uses of AI. Organizations that apply ethical AI have clearly stated well-defined review processes to ensure adherence to legal guidelines. Therefore, the wave of research at the intersection of ethical AI in data mining and machine learning has also influenced other fields of science, including computer vision, natural language processing, reinforcement learning, and social science. Despite these successes, ethical AI still faces many challenges, such as a lack of interpretable and explainable methods for fairness-aware deep learning models, etc. Consequently, there is an urgent need to bring experts and researchers together at prestigious venues to discuss ethical AI, which has been rarely seen in previous KDD conferences. This workshop will provide a premium platform for both research and industry from different backgrounds to exchange ideas on opportunities, challenges, and cutting-edge techniques in ethical AI.},
  archive   = {C_KDD},
  author    = {Zhao, Chen and Chen, Feng and Wu, Xintao and Li, Jundong and Chen, Haifeng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671482},
  pages     = {6751–6752},
  title     = {3rd workshop on ethical artificial intelligence: Methods and applications (EAI)},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RelKD 2024: The second international workshop on
resource-efficient learning for knowledge discovery. <em>KDD</em>,
6749–6750. (<a href="https://doi.org/10.1145/3637528.3671487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern machine learning techniques, particularly deep learning, have showcased remarkable efficacy across numerous knowledge discovery and data mining applications. However, the advancement of many of these methods is frequently impeded by resource constraint challenges in many scenarios, such as limited labeled data (data-level), small model size requirements in real-world computing platforms (model-level), and efficient mapping of the computations to heterogeneous target hardware (system-level). Addressing all these factors is crucial for effectively and efficiently deploying developed models across a broad spectrum of real-world systems, including large-scale social network analysis, recommendation systems, and real-time anomaly detection. Therefore, there is a critical need to develop efficient learning techniques to address the challenges posed by resource limitations, whether from data, model/algorithm, or system/hardware perspectives. The proposed international workshop on &quot;&amp;lt;u&amp;gt;R&amp;lt;/u&amp;gt;esource-&amp;lt;u&amp;gt;E&amp;lt;/u&amp;gt;fficient &amp;lt;u&amp;gt;L&amp;lt;/u&amp;gt;earning for &amp;lt;u&amp;gt;K&amp;lt;/u&amp;gt;nowledge &amp;lt;u&amp;gt;D&amp;lt;/u&amp;gt;iscovery (RelKD 2024)&quot; will provide a great venue for academic researchers and industrial practitioners to share challenges, solutions, and future opportunities of resource-efficient learning.},
  archive   = {C_KDD},
  author    = {Zhang, Chuxu and Xu, Dongkuan (DK) and Ding, Kaize and Li, Jundong and Javaheripi, Mojan and Mukherjee, Subhabrata and Chawla, Nitesh V. and Liu, Huan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671487},
  pages     = {6749–6750},
  title     = {RelKD 2024: The second international workshop on resource-efficient learning for knowledge discovery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 23rd international workshop on data mining in bioinformatics
(BIOKDD 2024). <em>KDD</em>, 6747–6748. (<a
href="https://doi.org/10.1145/3637528.3671485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The goal of the 22nd International Workshop on Data Mining in Bioinformatics (BIOKDD 2023) is to encourage KDD researchers to solve the numerous problems and challenges in Bioinformatics using Data Mining technologies. Based on the organizers&#39; expertise and communities, BIOKDD 2023 features the theme &quot;Large-Scale Data-Driven Methods for Bioinformatics&quot;. This theme encourages the use of high-performance computing (HPC) to support the training of large machine learning models for problems in Bioinformatics and Computational Biology. The key goal is to accelerate the convergence between Data Mining and Bioinformatics communities to expedite discoveries in basic biology, medicine and healthcare.The goal of the 23rd International Workshop on Data Mining in Bioinformatics (BIOKDD 2024) is to encourage KDD researchers to solve the numerous problems and challenges in Bioinformatics using Data Mining technologies. Based on the organizers&#39; expertise and communities, BIOKDD 2024 features the theme &quot;Advancing Bioinformatics with LLMs and GenAI&quot;. This theme encourages the use of large language models and generative artificial intelligence to solve problems in Bioinformatics and Computational Biology. The key goal is to accelerate the convergence between Data Mining and Bioinformatics communities to expedite discoveries in basic biology, medicine and healthcare.},
  archive   = {C_KDD},
  author    = {Yan, Da and Hamed, Ahmed Abdeen and Chen, Jake Y. and Zaki, Mohammed J.},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671485},
  pages     = {6747–6748},
  title     = {23rd international workshop on data mining in bioinformatics (BIOKDD 2024)},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NL2Code-reasoning and planning with LLMs for code
development. <em>KDD</em>, 6745–6746. (<a
href="https://doi.org/10.1145/3637528.3671505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There is huge value in making software development more productive with AI. An important component of this vision is the capability to translate natural language to a programming language (&quot;NL2Code&quot;) and thus to significantly accelerate the speed at which code is written.This workshop gathers researchers, practitioners, and users from industry and academia that are working on NL2Code, specifically on the problem of using large language models to convert statements posed in a human language to a formal programming language.},
  archive   = {C_KDD},
  author    = {Xing, Ye and Huan, Jun and Tok, Wee Hyong and Shen, Cong and Gehrke, Johannes and Lin, Katherine and Guha, Arjun and Tripp, Omer and Ramanathan, Murali Krishna},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671505},
  pages     = {6745–6746},
  title     = {NL2Code-reasoning and planning with LLMs for code development},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AI for education (AI4EDU): Advancing personalized education
with LLM and adaptive learning. <em>KDD</em>, 6743–6744. (<a
href="https://doi.org/10.1145/3637528.3671498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advanced AI technologies, especially large language models (LLMs) like GPTs, have significantly advanced the field of data mining and led to the development of various LLM-based applications. AI for education (AI4EDU) is a vibrant multi-disciplinary field of data mining, machine learning, and education, with increasing importance and extraordinary potential. In this field, LLM and adaptive learning-based models can be utilized as interfaces in human-in-the-loop education systems, where the model serves as a mediator among the teacher, students, and machine capabilities, including its own. This perspective has several benefits, including the ability to personalize interactions, allow unprecedented flexibility and adaptivity for human-AI collaboration and improve the user experience. However, several challenges still exist, including the need for more robust and efficient algorithms, designing effective user interfaces, and ensuring ethical considerations are addressed. This workshop aims to bring together researchers and practitioners from academia and industry to explore cutting-edge AI technologies for personalized education, especially the potential of LLMs and adaptive learning technologies.},
  archive   = {C_KDD},
  author    = {Wen, Qingsong and Liang, Jing and Sierra, Carles and Luckin, Rose and Tong, Richard and Liu, Zitao and Cui, Peng and Tang, Jiliang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671498},
  pages     = {6743–6744},
  title     = {AI for education (AI4EDU): Advancing personalized education with LLM and adaptive learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The 4th workshop on artificial intelligence-enabled
cybersecurity analytics. <em>KDD</em>, 6741–6742. (<a
href="https://doi.org/10.1145/3637528.3671494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cybersecurity remains a grand societal challenge. Large and constantly changing attack surfaces are non-trivial to protect against malicious actors. Entities like the United States and the European Union have recently emphasized the value of Artificial Intelligence (AI) for advancing cybersecurity. For example, the National Science Foundation has called for AI systems that can enhance cyber threat intelligence, detect new and evolving threats, and analyze massive troves of cybersecurity data. The 4th Workshop on Artificial Intelligence-enabled Cybersecurity Analytics (co-located with ACM KDD) sought to make significant and novel contributions within these relevant topics. Submissions were reviewed by highly qualified AI for cybersecurity researchers and practitioners spanning academia and private industry firms.},
  archive   = {C_KDD},
  author    = {Ullman, Steven and Ampel, Benjamin M. and Samtani, Sagar and Yang, Shanchieh and Chen, Hsinchun},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671494},
  pages     = {6741–6742},
  title     = {The 4th workshop on artificial intelligence-enabled cybersecurity analytics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The 2nd international workshop: From innovation to scale
(I2S) - successfully build, commercialize, and scale AI innovations.
<em>KDD</em>, 6739–6740. (<a
href="https://doi.org/10.1145/3637528.3671502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, there have been exciting and accelerated developments in AI with novel developments in foundation models, deep learning, new AI applications across numerous verticals, and more. In addition, the pace of adoption of these innovations driven by both academic and industry research labs has sped up with both big tech companies and startups looking to deliver value-differentiated products and services. With Generative AI (GenAI) garnering significant attention, the second edition of the I2S workshop focuses on two aspects: First, bringing together AI thought leaders from academia, big tech, and startups to discuss the opportunities, use-case themes, challenges, and risks of GenAI in various business verticals; and Second, bringing together startup founders to share experiences and lessons learned in commercializing GenAI innovations into successful enterprises highlighting challenges through the entire commercial journey - from productization to acquiring customers, building a team, and securing funding.},
  archive   = {C_KDD},
  author    = {Teredesai, Ankur and Zeller, Michael and Shah, Mohak and Bao, Shenghua and Tok, Wee Hyong and Pang, Linsey},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671502},
  pages     = {6739–6740},
  title     = {The 2nd international workshop: From innovation to scale (I2S) - successfully build, commercialize, and scale AI innovations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). First workshop on generative AI for recommender systems and
personalization. <em>KDD</em>, 6737–6738. (<a
href="https://doi.org/10.1145/3637528.3671486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Personalization is key in understanding user behavior and has been a main focus in the fields of knowledge discovery and information retrieval. Building personalized recommender systems is especially important now due to the vast amount of user-generated textual content, which offers deep insights into user preferences. The recent advancements in Large Language Models (LLMs) have significantly impacted research areas, mainly in Natural Language Processing and Knowledge Discovery, giving these models the ability to handle complex tasks and learn context. However, the use of generative models and user-generated text for personalized systems and recommendation is relatively new and has shown some promising results. This workshop is designed to bridge the research gap in these fields and explore personalized applications and recommender systems. We aim to fully leverage generative models to develop AI systems that are not only accurate but also focused on meeting individual user needs. Building upon the momentum of previous successful forums, this workshop seeks to engage a diverse audience from academia and industry, fostering a dialogue that incorporates fresh insights and anticipates over 50 attendees, including key stakeholders in the field.},
  archive   = {C_KDD},
  author    = {Tabari, Narges and Deshmukh, Aniket Anand and Kang, Wang-Cheng and Zamani, Hamed and Gangadharaiah, Rashmi and McAuley, Julian and Karypis, George},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671486},
  pages     = {6737–6738},
  title     = {First workshop on generative AI for recommender systems and personalization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EpiDAMIK 2024: The 7th international workshop on
epidemiology meets data mining and knowledge discovery. <em>KDD</em>,
6735–6736. (<a href="https://doi.org/10.1145/3637528.3671480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While the worst of COVID-19 pandemic has most likely passed us, an occurrence of equally devastating global pandemic or regional epidemic cannot be ruled out in future. H1N1, Zika, SARS, MERS, and Ebola outbreaks over the past few decades have sharply illustrated our enormous vulnerability to emerging infectious diseases. While the data mining research community has demonstrated increased interest in epidemiological applications, much is still left to be desired. For example, there is an urgent need to develop sound theoretical principles and transformative computational approaches that will allow us to address the escalating threat of current and future pandemics. Data mining and knowledge discovery have an important role to play in this regard. Different aspects of infectious disease modeling, analysis, and control have traditionally been studied within the confines of individual disciplines, such as mathematical epidemiology and public health, and data mining and machine learning. Coupled with increasing data generation across multiple domains/sources (e.g., wastewater surveillance, electronic medical records, and social media), there is a clear need for analyzing them to inform public health policies and outcomes timely. Recent advances in disease surveillance and forecasting, and initiatives such as the CDC Flu Challenge, CDC COVID-19 Forecasting Hub etc., have brought these disciplines closer together. On the one hand, public health practitioners seek to use novel datasets, such as Safegraph, Unacast, and Google mobility data, and techniques like Graph Neural Networks. On the other hand, researchers from data mining and machine learning develop novel tools for solving many fundamental problems in the public health policy planning and decision-making process, leveraging novel datasets (e.g., COVID-19 behavioral health surveys, contact tracing trees, and satellite images of urban streets) and combining them with more traditional time series information (e.g., surveillance, hospitalization, and death records). We believe the next stage of advances will result from closer collaborations between these two groups, which is the main objective of epiDAMIK.},
  archive   = {C_KDD},
  author    = {Rodr\&#39;{\i}guez, Alexander and Adhikari, Bijaya and Srivastava, Ajitesh and Pei, Sen and Charpignon, Marie-Laure and Wang, Kai and Chang, Serina and Vullikanti, Anil and Prakash, B. Aditya},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671480},
  pages     = {6735–6736},
  title     = {EpiDAMIK 2024: The 7th international workshop on epidemiology meets data mining and knowledge discovery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The 10th mining and learning from time series workshop: From
classical methods to LLMs. <em>KDD</em>, 6733–6734. (<a
href="https://doi.org/10.1145/3637528.3671489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Time series data has become ubiquitous across various fields such as healthcare, finance, entertainment, and transportation, driven by advancements in sensing technologies that enable continuous monitoring and recording. This growth in data size and complexity presents new challenges for traditional analysis techniques, necessitating the development of advanced, interdisciplinary temporal mining algorithms. The goals of this workshop are to: (1) highlight significant challenges in learning and mining from time series data, such as irregular sampling, spatiotemporal structures, and uncertainty quantification; (2) discuss recent developments in algorithmic, theoretical, statistical, and systems-based approaches for addressing these challenges, including both classical methods and large language models (LLMs); and (3) synergize research efforts by exploring both new and open problems in time series analysis and mining. This workshop will focus on both the theoretical and practical aspects of time series data analysis, providing a platform for researchers and practitioners from academia, government, and industry to discuss potential research directions, critical technical issues, and present solutions for practical applications. Contributions from related fields such as AI, machine learning, data science, and statistics are also included.},
  archive   = {C_KDD},
  author    = {Purushotham, Sanjay and Song, Dongjin and Wen, Qingsong and Huan, Jun and Shen, Cong and Zohren, Stefan and Nevmyvaka, Yuriy},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671489},
  pages     = {6733–6734},
  title     = {The 10th mining and learning from time series workshop: From classical methods to LLMs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Workshop on discovering drift phenomena in evolving data
landscape (DELTA). <em>KDD</em>, 6731–6732. (<a
href="https://doi.org/10.1145/3637528.3671492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automated systems must adapt to evolving environments, yet many struggle with drift phenomena affecting healthcare, finance, and cybersecurity domains. The DELTA workshop addresses this by distinguishing between data and concept drift, aiming to create a practical, human-centric framework for managing drift. The workshop seeks innovative drift detection, prediction, and analysis solutions by uniting researchers and practitioners. DELTA fosters collaboration to advance the understanding and management of drift in dynamic data landscapes by featuring keynotes, paper presentations, interactive sessions, and discussions.},
  archive   = {C_KDD},
  author    = {Piangerelli, Marco and Prenkaj, Bardh and Rotalinti, Ylenia and Joshi, Ananya and Stilo, Giovanni},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671492},
  pages     = {6731–6732},
  title     = {Workshop on discovering drift phenomena in evolving data landscape (DELTA)},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). KDD workshop on evaluation and trustworthiness of generative
AI models. <em>KDD</em>, 6729–6730. (<a
href="https://doi.org/10.1145/3637528.3671481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The KDD workshop on Evaluation and Trustworthiness of Generative AI Models aims to address the critical need for reliable generative AI technologies by exploring comprehensive evaluation strategies. This workshop will delve into various aspects of assessing generative AI models, including Large Language Models (LLMs) and diffusion models, focusing on trustworthiness, safety, bias, fairness, and ethical considerations. With an emphasis on interdisciplinary collaboration, the workshop will feature invited talks, peer-reviewed paper presentations, and panel discussions to advance the state of the art in generative AI evaluation.},
  archive   = {C_KDD},
  author    = {Ling, Yuan and Dong, Shujing and Feng, Yarong and Liu, Zongyi (Joe) and Karypis, George and Reddy, Chandan K.},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671481},
  pages     = {6729–6730},
  title     = {KDD workshop on evaluation and trustworthiness of generative AI models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The 13th international workshop on urban computing.
<em>KDD</em>, 6727–6728. (<a
href="https://doi.org/10.1145/3637528.3671477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Urbanization&#39;s rapid progress has led to many big cities, which have modernized many people&#39;s lives but also engendered big challenges, such as air pollution, increased energy consumption, and traffic congestion. Tackling these challenges was nearly impossible years ago given the complex and dynamic settings of cities. Nowadays, sensing technologies and large-scale computing infrastructures have produced a variety of big data in urban spaces, e.g., human mobility, air quality, traffic patterns, and geographical data. Motivated by the opportunities of building more intelligent cities, we came up with a vision of urban computing, which aims to unlock the power of knowledge from big and heterogeneous data collected in urban spaces and apply this powerful information to solve major issues our cities face today.},
  archive   = {C_KDD},
  author    = {Liang, Yuxuan and Meng, Chuishi and Li, Yanhua and Zheng, Yu and Ye, Jieping and Yang, Qiang and Yu, Philip S. and Wolfson, Ouri},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671477},
  pages     = {6727–6728},
  title     = {The 13th international workshop on urban computing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 2nd workshop on causal inference and machine learning in
practice. <em>KDD</em>, 6726. (<a
href="https://doi.org/10.1145/3637528.3671483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The workshop&#39;s rationale stems from the escalating interest in causal inference and machine learning methodologies within various industrial contexts. This surge in demand underscores the importance for both scholars and practitioners to exchange knowledge and best practices regarding the application of these techniques to tackle real-world challenges. Yet, applying causal machine learning techniques in real-world scenarios presents a range of challenges not addressed in the academic literature. This workshop aims to address the challenges for practical causal machine learning and explore new industry use cases. The workshop will provide a forum for practitioners and researchers to exchange ideas and explore new collaborations. Moreover, this workshop aims to capitalize on the success and achievements of the KDD 2023 Workshop titled &quot;Causal Inference and Machine Learning in Practice&quot;.},
  archive   = {C_KDD},
  author    = {Lee, Jeong-Yoon and Wu, Yifeng and Harinen, Totte and Pan, Jing and Lo, Paul and Zhao, Zhenyu and Chen, Huigang and Zheng, Zeyu and Vanchinathan, Hasta and Wang, Yingfei and Stevenson, Roland},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671483},
  pages     = {6726},
  title     = {2nd workshop on causal inference and machine learning in practice},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The first workshop on AI behavioral science. <em>KDD</em>,
6724–6725. (<a href="https://doi.org/10.1145/3637528.3671503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This workshop initiates a new study field which may be named AI behavioral science. It discusses recent findings, methodologies, applications, and potential societal impacts that are related to analyzing, understanding, and directing the behaviors of AI models, especially those built upon large language models. This half-day workshop includes several keynote and invited talks, a poster session, and a panel discussion.},
  archive   = {C_KDD},
  author    = {Lakkaraju, Himabindu and Mei, Qiaozhu and Tan, Chenhao and Tang, Jie and Xie, Yutong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671503},
  pages     = {6724–6725},
  title     = {The first workshop on AI behavioral science},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The 4th KDD workshop on deep learning for spatiotemporal
data, applications, and systems (DeepSpatial’24). <em>KDD</em>,
6722–6723. (<a href="https://doi.org/10.1145/3637528.3671501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Over the last decades, a rapidly growing volume of spatiotemporal data has been collected from smartphones and GPS, terrestrial, seaborne, airborne, and spaceborne sensors, as well as computational simulations. Meanwhile, advances in deep learning technologies, especially the recent breakthroughs of generative AI and foundation models such as Large Language Models (LLMs) and Large Vision Models (LVMs), have achieved tremendous success in natural language processing and computer vision applications. There is growing anticipation of the same level of accomplishment of AI on spatiotemporal data in tackling grand societal challenges, such as national water resource management, monitoring coastal hazards, energy and food security, as well as mitigation and adaptation to climate change. When deep learning, especially emerging foundation models, intersects spatiotemporal data in scientific domains, it opens up new opportunities and challenges. The workshop aims to bring together academic researchers in both AI and scientific domains, government program managers, leaders from non-profit organizations, as well as industry executives to brainstorm and debate on the emerging opportunities and novel challenges of deep learning (foundation models) for spatiotemporal data inspired by real-world scientific applications.},
  archive   = {C_KDD},
  author    = {Jiang, Zhe and Zhao, Liang and Zhou, Xun and Zhang, Junbo and Shekhar, Shashi and Ye, Jieping},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671501},
  pages     = {6722–6723},
  title     = {The 4th KDD workshop on deep learning for spatiotemporal data, applications, and systems (DeepSpatial&#39;24)},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Artificial intelligence and data science for healthcare:
Bridging data-centric AI and people-centric healthcare. <em>KDD</em>,
6720–6721. (<a href="https://doi.org/10.1145/3637528.3671497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {KDD AIDSH 2024 aims to foster discussions and developments that push the boundaries of Artificial Intelligence (AI) and Data Science (DS) in healthcare, enhance diagnostic accuracy and promote human-centric approaches to healthcare, thus stimulating future interdisciplinary collaborations. This year&#39;s symposium will focus on expanding the application of AI/DS in healthcare/medicine and bridging existing gaps. The workshop invites submissions of full papers as well as work-in-progress on the application of AI/DS in healthcare. The workshop will feature three invited talks from eminent speakers, spanning academia, industry, and clinical researchers. In addition, selected papers will be invited to publish in Health Data Science, a Science Partner Journal. This summary provides a brief description of the half-day workshop to be held on August 26th, 2024. The webpage for the workshop can be found at https://aimel.ai/kdd2024aidsh.},
  archive   = {C_KDD},
  author    = {Hong, Shenda and Yin, Daoxin and Tang, Gongzheng and Fu, Tianfan and Ma, Liantao and Gao, Junyi and Feng, Mengling and Wang, Mai and Yang, Yu and Wang, Fei and Liu, Hongfang and Zhang, Luxia},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671497},
  pages     = {6720–6721},
  title     = {Artificial intelligence and data science for healthcare: Bridging data-centric AI and people-centric healthcare},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FedKDD: International joint workshop on federated learning
for data mining and graph analytics. <em>KDD</em>, 6718–6719. (<a
href="https://doi.org/10.1145/3637528.3671490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep Learning has facilitated various high-stakes applications such as crime detection, urban planning, drug discovery, and healthcare. Its continuous success hinges on learning from massive data in miscellaneous sources, ranging from data with independent distributions to graph-structured data capturing intricate inter-sample relationships. Scaling up the data access requires global collaboration from distributed data owners. Yet, centralizing all data sources to an untrustworthy centralized server will put users&#39; data at risk of privacy leakage or regulation violation. Federated Learning (FL) is a de facto decentralized learning framework that enables knowledge aggregation from distributed users without exposing private data. Though promising advances are witnessed for FL, new challenges are emerging when integrating FL with the rising needs and opportunities in data mining, graph analytics, foundation models, generative AI, and new interdisciplinary applications in science. By hosting this workshop, we aim to attract a broad range of audiences, including researchers and practitioners from academia and industry interested in the emergent challenges in FL. As an effort to advance the fundamental development of FL, this workshop will encourage ideas exchange on the trustworthiness, scalability, and robustness of distributed data mining and graph analytics and their emergent challenges.},
  archive   = {C_KDD},
  author    = {Hong, Junyuan and Yang, Carl and Zhu, Zhuangdi and Xu, Zheng and Baracaldo, Nathalie and Shah, Neil and Avestimehr, Salman and Zhou, Jiayu},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671490},
  pages     = {6718–6719},
  title     = {FedKDD: International joint workshop on federated learning for data mining and graph analytics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TSMO 2024: Two-sided marketplace optimization. <em>KDD</em>,
6716–6717. (<a href="https://doi.org/10.1145/3637528.3671484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, two-sided marketplaces have emerged as viable business models in many real-world applications. In particular, we have moved from the social network paradigm to a network with two distinct types of participants representing the supply and demand of a specific good. Examples of industries include but are not limited to accommodation (Airbnb, Booking.com), video content (YouTube, Instagram, TikTok), ridesharing (Uber, Lyft), online shops (Etsy, Ebay, Facebook Marketplace), music (Spotify, Amazon), app stores (Apple App Store, Google App Store) or job sites (LinkedIn). The traditional research in most of these industries focused on satisfying the demand. OTAs would sell hotel accommodation, TV networks would broadcast their own content, or taxi companies would own their own vehicle fleet. In modern examples like Airbnb, YouTube, Instagram, or Uber, the platforms operate by outsourcing the service they provide to their users, whether they are hosts, content creators or drivers, and have to develop their models considering their needs and goals.},
  archive   = {C_KDD},
  author    = {Grbovic, Mihajlo and Radosavljevic, Vladan and Chen, Minmin and Iliakopoulou-Zanos, Katerina and Noulas, Thanasis and Goyal, Amit and Silvestri, Fabrizio},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671484},
  pages     = {6716–6717},
  title     = {TSMO 2024: Two-sided marketplace optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The third workshop on applied machine learning management.
<em>KDD</em>, 6714–6715. (<a
href="https://doi.org/10.1145/3637528.3671478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Machine learning applications are rapidly adopted by industry leaders in any field. The growth of investment in AI-driven solutions,including the emerging field of General AI (GenAI), has created new challenges in managing Data Science and ML resources, people and projects as a whole. The discipline of managing applied machine learning teams, requires a healthy mix between agile product development tool-set and a long term research oriented mindset. The abilities of investing in deep research while at the same time connecting the outcomes to significant business results create a large knowledge based on management methods and best practices in the field. The Third KDD Workshop on Applied Machine Learning Management brings together applied research managers from various fields to share methodologies and case-studies on management of ML teams, products, and projects, achieving business impact with advanced AI-methods.},
  archive   = {C_KDD},
  author    = {Goldenberg, Dmitri and Meir Lador, Shir and Sokolova, Elena and Cheong, Lin Lee and Sukhwani, Mohak and Potdar, Saloni},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671478},
  pages     = {6714–6715},
  title     = {The third workshop on applied machine learning management},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). KiL 2024: 4th international workshop on knowledge-infused
learning (towards consistent, reliable, explainable, and safe LLMs).
<em>KDD</em>, 6712–6713. (<a
href="https://doi.org/10.1145/3637528.3671495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Knowledge-infused Learning Workshop is a recurring event in ACM&#39;s KDD Conference that gathers the research community on knowledge graphs and knowledge-enabled learning, grounded neurosymbolic AI, explainable and safe AI, and applications in high-stakes decision-making problems. This year, the workshop aligned with Biden&#39;s vision of Responsible AI Development.},
  archive   = {C_KDD},
  author    = {Gaur, Manas and Tsamoura, Efthymia and Raff, Edward and Vedula, Nikhita and Parthasarathy, Srinivasan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671495},
  pages     = {6712–6713},
  title     = {KiL 2024: 4th international workshop on knowledge-infused learning (Towards consistent, reliable, explainable, and safe LLMs)},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fragile earth: Generative and foundational models for
sustainable development. <em>KDD</em>, 6710–6711. (<a
href="https://doi.org/10.1145/3637528.3671493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Fragile Earth Workshop is a recurring event in ACM&#39;s KDD Conference on research in knowledge discovery and data mining that gathers the research community to find and explore how data science can measure and progress climate and social issues, following the United Nations Sustainable Development Goals (SDGs) framework.},
  archive   = {C_KDD},
  author    = {Eftelioglu, Emre and Dilkina, Bistra and Abe, Naoki and Kannan, Ramakrishnan and Chen, Yuzhou and Gel, Yulia R. and Buckingham, Kathleen and Ganguly, Auroop and Hodson, James and Mao, Jiafu},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671493},
  pages     = {6710–6711},
  title     = {Fragile earth: Generative and foundational models for sustainable development},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Workshop on human-interpretable AI. <em>KDD</em>, 6708–6709.
(<a href="https://doi.org/10.1145/3637528.3671499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This workshop aims to spearhead research on Human-Interpretable Artificial Intelligence (HI-AI) by providing: (i) a general overview of the key aspects of HI-AI, in order to equip all researchers with the necessary background and set of definitions; (ii) novel and interesting ideas coming from both invited talks and top paper contributions; (iii) the chance to engage in dialogue with prominent scientists during poster presentations and coffee breaks. The workshop welcomes contributions covering novel interpretable-by-design or post-hoc approaches, as well as theoretical analysis of existing works. Additionally, we accept visionary contributions speculating on the future potential of this field. Finally, we welcome contributions from related fields such as Ethical AI, Knowledge-driven Machine learning, Human-machine Interaction, but also applications in Medicine and Industry, and analyses from Regulatory experts.},
  archive   = {C_KDD},
  author    = {Ciravegna, Gabriele and Espinosa Zarlenga, Mateo and Barbiero, Pietro and Giannini, Francesco and Shams, Zohreh and Garreau, Damien and Jamnik, Mateja and Cerquitelli, Tania},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671499},
  pages     = {6708–6709},
  title     = {Workshop on human-interpretable AI},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AdKDD 2024. <em>KDD</em>, 6706–6707. (<a
href="https://doi.org/10.1145/3637528.3671476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The digital advertising field has always had challenging ML problems, learning from petabytes of data that is highly imbalanced, reactivity times in the milliseconds, and more recently compounded with the complex user&#39;s path to purchase across devices, across platforms, and even online/real-world behavior. The AdKDD workshop continues to be a forum for researchers in advertising, during and after KDD. Our website which hosts slides and abstracts receives approximately 2,000 monthly visits and 1,800 active users during the KDD 2021. In surveys during AdKDD 2019 and 2020, over 60\% agreed that AdKDD is the reason they attended KDD, and over 90\% indicated they would attend next year. The 2024 edition is particularly timely because of the increasing application of Graph-based NN and Generative AI models in advertising. Coupled with privacy-preserving initiatives enforced by GDPR, CCPA the future of computational advertising is at an interesting crossroads. For this edition, we plan to solicit papers that span the spectrum of deep user understanding while remaining privacy-preserving. In addition, we will seek papers that discuss fairness in the context of advertising, to what extent does hyper-personalization work, and whether the ad industry as a whole needs to think through more effective business models such as incrementality. We have hosted several academic and industry luminaries as keynote speakers and have found our invited speaker series hosting expert practitioners to be an audience favorite. We will continue fielding a diverse set of keynote speakers and invited talks for this edition as well. As with past editions, we hope to motivate researchers in this space to think not only about the ML aspects but also to spark conversations about the societal impact of online advertising.},
  archive   = {C_KDD},
  author    = {Bagherjeiran, Abraham and Djuric, Nemanja and Lee, Kuang-Chih and Pang, Linsey and Radosavljevic, Vladan and Rajan, Suju},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671476},
  pages     = {6706–6707},
  title     = {AdKDD 2024},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Workshop on deep learning and large language models for
knowledge graphs (DL4KG). <em>KDD</em>, 6704–6705. (<a
href="https://doi.org/10.1145/3637528.3671491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The use of Knowledge Graphs (KGs) which constitute large networks of real-world entities and their interrelationships, has grown rapidly. A substantial body of research has emerged, exploring the integration of deep learning (DL) and large language models (LLMs) with KGs. This workshop aims to bring together leading researchers in the field to discuss and foster collaborations on the intersection of KG and DL/LLMs.},
  archive   = {C_KDD},
  author    = {Alam, Mehwish and Buscaldi, Davide and Cochez, Michael and Gesese, Genet Asefa and Osborne, Francesco and Reforgiato Recupero, Diego},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671491},
  pages     = {6704–6705},
  title     = {Workshop on deep learning and large language models for knowledge graphs (DL4KG)},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning in finance. <em>KDD</em>, 6703. (<a
href="https://doi.org/10.1145/3637528.3671488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This workshop aims to explore the intersection of Generative AI with the rich tapestry of financial data types, seeking to uncover new methodologies and techniques that can enhance predictive analytics, fraud detection, and customer insights across the sector. By harnessing these advancements in AI, we can pave the way to not only understand customer behavior but also anticipate their needs more effectively, leading to superior customer outcomes and more personalized services. Our objective is to shed light on the challenges and opportunities presented by the diverse data formats in finance. We aim to bridge the gap between the dominance of traditional models for tabular data analysis and the emerging potential of Generative AI to revolutionize the treatment of time series, click streams, and other unstructured data forms.},
  archive   = {C_KDD},
  author    = {Akoglu, Leman and Chawla, Nitesh and Domingo-Ferrer, Josep and Kurshan, Eren and Kumar, Senthil and Naware, Vidyut and Rodriguez-Serrano, Jose A. and Chaturvedi, Isha and Nagrecha, Saurabh and Das, Mahashweta and Faruquie, Tanveer},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671488},
  pages     = {6703},
  title     = {Machine learning in finance},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). KDD 2024 finance day. <em>KDD</em>, 6701–6702. (<a
href="https://doi.org/10.1145/3637528.3673865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Finance Day at KDD 2024 will take place on August 26th in Barcelona, Spain. Following the success of the inaugural event last year, the second edition highlights the significant role of AI in transforming the financial industry. This special day serves as a forum for discussion of innovations at the intersection of AI and finance. An exciting lineup of 12 influential speakers from nine different countries will be featured, representing a mix of government organizations, leading banks, innovative hedge funds, and top academic institutions. These experts will delve into a range of topics, from cutting-edge FinTech innovations to ethical considerations in machine learning, providing a comprehensive overview of the finance and AI. The distinguished speakers include Avanidhar Subrahmanyam from UCLA, Henrike Mueller from the Financial Conduct Authority, Claudia Perlich from Two Sigma, Eyke H\&quot;{u}llermeier from Ludwig-Maximilians-Universit\&quot;{a}t M\&quot;{u}nchen, Senthil Kumar from Capital One, Stefan Zohren from the University of Oxford, Dumitru Roman from SINTEF ICT, Kubilay Atasu from TU Delft, Xiao-Ming Wu from Hong Kong Polytechnic University, Yongjae Lee from UNIST, Jundong Li from the University of Virginia, and Milos Blagojevic from BlackRock.},
  archive   = {C_KDD},
  author    = {Wang, Guiling and Borrajo, Daniel},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3673865},
  pages     = {6701–6702},
  title     = {KDD 2024 finance day},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generative AI day. <em>KDD</em>, 6699–6700. (<a
href="https://doi.org/10.1145/3637528.3673872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Generative AI (AIGC) Day at KDD&#39;24 is a dedicated full-day event for generative AI at KDD. This is an opportunity to bring together researchers, practitioners, and startups to share the insights about the cutting-edge advancements and to discuss the potential societal impacts of LLMs and AIGC. It is exciting that this year, we have invited speakers from both industry (e.g., Amazon, Zhipu AI) and academia (e.g., USC, UCLA). The topics cover various perspectives of generative AI including foundation models, streaming LLMs, LLM training and inference. As demonstrated, data plays a crucial role in developing cutting-edge generative AI models. For example, the Gemini Team has found that &quot;data quality is an important factor for highly-performing models...&#39;&#39;. To date, there is still significant room to define design principles and develop methods for improved data collection, selection, and synthetic data generation for the pre-training and alignment of language, vision, and multi-modal models. Therefore, the Day will invite the speakers and KDD audience to discuss the challenges and opportunities for data mining researchers in the era of generative AI.},
  archive   = {C_KDD},
  author    = {Tang, Jie and Dong, Yuxiao and Vazirgiannis, Michalis},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3673872},
  pages     = {6699–6700},
  title     = {Generative AI day},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). European data science day: KDD-2024 special day.
<em>KDD</em>, 6697–6698. (<a
href="https://doi.org/10.1145/3637528.3673868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The European Data Science Day offers a full day focused exclusively on innovative KDD-relevant research and development projects from national and regional funding programs, as well as corporate, start-up, and nonprofit channels. The idea is to bring together a diverse community of researchers in Data Science, Machine Learning, Language Technologies, and Knowledge Discovery, as well as partnerships in the social and physical sciences/arts, to showcase the state-of-the-art in research and applications.},
  archive   = {C_KDD},
  author    = {Mladenic, Dunja and Roman, Dumitru},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3673868},
  pages     = {6697–6698},
  title     = {European data science day: KDD-2024 special day},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). KDD 2024 special day - AI for environment. <em>KDD</em>,
6695–6696. (<a href="https://doi.org/10.1145/3637528.3673869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Environmental problems such as air pollution monitoring and prevention, flood detection and prevention, land use, forest management, river water quality, wastewater treatment supervision, etc. are more complex than typical real-world problems usually AI faces to. This added complexity rises from several aspects, such as the randomness shown by most of environmental processes involved, the 2D/3D nature of involved problems, the temporal aspects, the spatial aspects, the inexactness of the information, etc.In fact, environmental problems belong to the most difficult problems with a lot of inexactness and uncertainty, and possibly conflicting objectives to be solved according to several classifications such as the one by Funtowicz \&amp;amp; Ravetz (Funtowicz \&amp;amp; Ravetz, 1999), which states that there are 3 kinds of problems. Also, they are non-structured problems in the classification proposed by H. Simon (Simon, 1966).All this complexity means that to effectively solve those problems a lot of knowledge is needed. This knowledge can be theoretical knowledge expressed in mechanistic models, such as the Gravidity Newton&#39;s Theory, or it can be empirical knowledge that can be expressed by means of empirical models, originated by some data and observations (data-driven knowledge) or by the expertise gathered by people when coping with such problems (model-driven knowledge, particularly expert-based knowledge).The KDD 2024 Special Day for AI for environment brings together researchers and practitioners to present their perspective on this very timely topic on how AI can be used for good, and improving the environment where we all live in.},
  archive   = {C_KDD},
  author    = {Gibert, Karina and Tok, Wee Hyong and S\`{a}nchez-Marr\`{e}, Miquel},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3673869},
  pages     = {6695–6696},
  title     = {KDD 2024 special day - AI for environment},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Overview of ACM SIGKDD 2024 AI4Science4AI special day.
<em>KDD</em>, 6693–6694. (<a
href="https://doi.org/10.1145/3637528.3673871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper provides an overview of the ACM SIGKDD 2024 AI4Science4AI special day. It includes information about the organizers, invited speakers, keynote speakers, the event agenda, and insights from related workshops. The AI4Science4AI special day aims to bring together experts in artificial intelligence (AI) and science to discuss the latest developments, challenges, and future directions.},
  archive   = {C_KDD},
  author    = {Ding, Wei and Camps-Valls, Gustau},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3673871},
  pages     = {6693–6694},
  title     = {Overview of ACM SIGKDD 2024 AI4Science4AI special day},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Health day: Building health AI ecosystem: From data
harmonization to knowledge discovery. <em>KDD</em>, 6691–6692. (<a
href="https://doi.org/10.1145/3637528.3673866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ACM KDD 2024 Health Day theme, &quot;Building Health AI Ecosystem: From Data Harmonization to Knowledge Discovery,&quot; highlights the transformative potential of AI-driven ecosystems in healthcare, translational biomedical research, and basic biological research. This extended abstract discusses recent advancements, challenges, and future directions, focusing on integrating AI-ready data sets, interdisciplinary collaborations, and ethical AI practices. It aims to catalyze discussions on the potential of AI ecosystems in revolutionizing healthcare and related fields.},
  archive   = {C_KDD},
  author    = {Chen, Jake and Ping, Peipei},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3673866},
  pages     = {6691–6692},
  title     = {Health day: building health AI ecosystem: from data harmonization to knowledge discovery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Equity, diversity &amp; inclusion (EDI): Special day at ACM
KDD 2024. <em>KDD</em>, 6689–6690. (<a
href="https://doi.org/10.1145/3637528.3673870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Equity, Diversity \&amp;amp; Inclusion event is a special day organized in conjunction with KDD &#39;24, the 30 ^th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, which will take place from Sunday, August 25 to Thursday, August 29, 2024 at the Center de Convencions Internacional de Barcelona in Barcelona, Spain.This special day, scheduled for August 28, 2024, promotes equity, diversity, and inclusion (EDI) in data science, artificial intelligence, and beyond. It will bring together academics, researchers, practitioners, and human resources professionals (i) to present algorithms, techniques, methodologies, and projects in data science that enable responsible data processing and modeling; (ii) to discuss policies, best practices, and guidelines to promote an inclusive work environment and effective collaboration; (iii) to share personal stories to encourage young researchers, including those from groups unrepresented in the research community, to develop strong careers in data science; and (iv) to collaboratively develop and discuss an EDI Manifesto to promote an inclusive workplace environment and guiding principles in the development of research activities.},
  archive   = {C_KDD},
  author    = {Cerquitelli, Tania and Mantrach, Amin},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3673870},
  pages     = {6689–6690},
  title     = {Equity, diversity \&amp;amp; inclusion (EDI): Special day at ACM KDD 2024},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Responsible AI day. <em>KDD</em>, 6688. (<a
href="https://doi.org/10.1145/3637528.3673867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We summarize the goals of the Responsible AI day, giving a glimpse on the program as well as a short biography of the organizers.},
  archive   = {C_KDD},
  author    = {Baeza-Yates, Ricardo and Busl\&#39;{o}n, Nataly},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3673867},
  pages     = {6688},
  title     = {Responsible AI day},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Causal inference with latent variables: Recent advances and
future prospectives. <em>KDD</em>, 6677–6687. (<a
href="https://doi.org/10.1145/3637528.3671450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Causality lays the foundation for the trajectory of our world. Causal inference (CI), which aims to infer intrinsic causal relations among variables of interest, has emerged as a crucial research topic. Nevertheless, the lack of observation of important variables (e.g., confounders, mediators, exogenous variables, etc.) severely compromises the reliability of CI methods. The issue may arise from the inherent difficulty in measuring the variables. Additionally, in observational studies where variables are passively recorded, certain covariates might be inadvertently omitted by the experimenter. Depending on the type of unobserved variables and the specific CI task, various consequences can be incurred if these latent variables are carelessly handled, such as biased estimation of causal effects, incomplete understanding of causal mechanisms, lack of individual-level causal consideration, etc. In this survey, we provide a comprehensive review of recent developments in CI with latent variables. We start by discussing traditional CI techniques when variables of interest are assumed to be fully observed. Afterward, under the taxonomy of circumvention and inference-based methods, we provide an in-depth discussion of various CI strategies to handle latent variables, covering the tasks of causal effect estimation, mediation analysis, counterfactual reasoning, and causal discovery. Furthermore, we generalize the discussion to graph data where interference among units may exist. Finally, we offer fresh aspects for further advancement of CI with latent variables, especially new opportunities in the era of large language models (LLMs).},
  archive   = {C_KDD},
  author    = {Zhu, Yaochen and He, Yinhan and Ma, Jing and Hu, Mengxuan and Li, Sheng and Li, Jundong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671450},
  pages     = {6677–6687},
  title     = {Causal inference with latent variables: Recent advances and future prospectives},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heterogeneous contrastive learning for foundation models and
beyond. <em>KDD</em>, 6666–6676. (<a
href="https://doi.org/10.1145/3637528.3671454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the era of big data and Artificial Intelligence, an emerging paradigm is to utilize contrastive self-supervised learning to model large-scale heterogeneous data. Many existing foundation models benefit from the generalization capability of contrastive self-supervised learning by learning compact and high-quality representations without relying on any label information. Amidst the explosive advancements in foundation models across multiple domains, including natural language processing and computer vision, a thorough survey on heterogeneous contrastive learning for the foundation model is urgently needed. In response, this survey critically evaluates the current landscape of heterogeneous contrastive learning for foundation models, highlighting the open challenges and future trends of contrastive learning. In particular, we first present how the recent advanced contrastive learning-based methods deal with view heterogeneity and how contrastive learning is applied to train and fine-tune the multi-view foundation models. Then, we move to contrastive learning methods for task heterogeneity, including pretraining tasks and downstream tasks, and show how different tasks are combined with contrastive learning loss for different purposes. Finally, we conclude this survey by discussing the open challenges and shedding light on the future directions of contrastive learning.},
  archive   = {C_KDD},
  author    = {Zheng, Lecheng and Jing, Baoyu and Li, Zihao and Tong, Hanghang and He, Jingrui},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671454},
  pages     = {6666–6676},
  title     = {Heterogeneous contrastive learning for foundation models and beyond},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A survey on safe multi-modal learning systems.
<em>KDD</em>, 6655–6665. (<a
href="https://doi.org/10.1145/3637528.3671462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the rapidly evolving landscape of artificial intelligence, multimodal learning systems (MMLS) have gained traction for their ability to process and integrate information from diverse modality inputs. Their expanding use in vital sectors such as healthcare has made safety assurance a critical concern. However, the absence of systematic research into their safety is a significant barrier to progress in this field. To bridge the gap, we present the first taxonomy that systematically categorizes and assesses MMLS safety. This taxonomy is structured around four fundamental pillars that are critical to ensuring the safety of MMLS: robustness, alignment, monitoring, and controllability. Leveraging this taxonomy, we review existing methodologies, benchmarks, and the current state of research, while also pinpointing the principal limitations and gaps in knowledge. Finally, we discuss unique challenges in MMLS safety. In illuminating these challenges, we aim to pave the way for future research, proposing potential directions that could lead to significant advancements in the safety protocols of MMLS.},
  archive   = {C_KDD},
  author    = {Zhao, Tianyi and Zhang, Liangliang and Ma, Yao and Cheng, Lu},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671462},
  pages     = {6655–6665},
  title     = {A survey on safe multi-modal learning systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated mining of structured knowledge from text in the
era of large language models. <em>KDD</em>, 6644–6654. (<a
href="https://doi.org/10.1145/3637528.3671469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Massive amount of unstructured text data are generated daily, ranging from news articles to scientific papers. How to mine structured knowledge from the text data remains a crucial research question. Recently, large language models (LLMs) have shed light on the text mining field with their superior text understanding and instruction-following ability. There are typically two ways of utilizing LLMs: fine-tune the LLMs with human-annotated training data, which is labor intensive and hard to scale; prompt the LLMs in a zero-shot or few-shot way, which cannot take advantage of the useful information in the massive text data. Therefore, it remains a challenge on automated mining of structured knowledge from massive text data in the era of large language models. In this tutorial, we cover the recent advancements in mining structured knowledge using language models with very weak supervision. We will introduce the following topics in this tutorial: (1) introduction to large language models, which serves as the foundation for recent text mining tasks, (2) ontology construction, which automatically enriches an ontology from a massive corpus, (3) weakly-supervised text classification in flat and hierarchical label space, (4) weakly-supervised information extraction, which extracts entity and relation structures.},
  archive   = {C_KDD},
  author    = {Zhang, Yunyi and Zhong, Ming and Ouyang, Siru and Jiao, Yizhu and Zhou, Sizhe and Ding, Linyi and Han, Jiawei},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671469},
  pages     = {6644–6654},
  title     = {Automated mining of structured knowledge from text in the era of large language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Urban foundation models: A survey. <em>KDD</em>, 6633–6643.
(<a href="https://doi.org/10.1145/3637528.3671453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Machine learning techniques are now integral to the advancement of intelligent urban services, playing a crucial role in elevating the efficiency, sustainability, and livability of urban environments. The recent emergence of foundation models such as ChatGPT marks a revolutionary shift in the fields of machine learning and artificial intelligence. Their unparalleled capabilities in contextual understanding, problem solving, and adaptability across a wide range of tasks suggest that integrating these models into urban domains could have a transformative impact on the development of smart cities. Despite growing interest in Urban Foundation Models (UFMs), this burgeoning field faces challenges such as a lack of clear definitions and systematic reviews. To this end, this paper first introduces the concept of UFMs and discusses the unique challenges involved in building them. We then propose a data-centric taxonomy that categorizes and clarifies current UFM-related works, based on urban data modalities and types. Furthermore, we explore the application landscape of UFMs, detailing their potential impact in various urban contexts. Relevant papers and open-source resources have been collated and are continuously updated at: https://github.com/usail-hkust/Awesome-Urban-Foundation-Models.},
  archive   = {C_KDD},
  author    = {Zhang, Weijia and Han, Jindong and Xu, Zhao and Ni, Hang and Liu, Hao and Xiong, Hui},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671453},
  pages     = {6633–6643},
  title     = {Urban foundation models: A survey},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Systems for scalable graph analytics and machine learning:
Trends and methods. <em>KDD</em>, 6627–6632. (<a
href="https://doi.org/10.1145/3637528.3671472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph-theoretic algorithms and graph machine learning models are essential tools for addressing many real-life problems, such as social network analysis and bioinformatics. To support large-scale graph analytics, graph-parallel systems have been actively developed for over one decade, such as Google&#39;s Pregel and Spark&#39;s GraphX, which (i) promote a think-like-a-vertex computing model and target (ii) iterative algorithms and (iii) those problems that output a value for each vertex. However, this model is too restricted for supporting the rich set of heterogeneous operations for graph analytics and machine learning that many real applications demand. In recent years, two new trends emerge in graph-parallel systems research: (1) a novel think-like-a-task computing model that can efficiently support the various computationally expensive problems of subgraph search; and (2) scalable systems for learning graph neural networks. These systems effectively complement the diversity needs of graph-parallel tools that can flexibly work together in a comprehensive graph processing pipeline for real applications, with the capability of capturing structural features. This tutorial will provide an effective categorization of the recent systems in these two directions based on their computing models and adopted techniques, and will review the key design ideas of these systems.},
  archive   = {C_KDD},
  author    = {Yan, Da and Yuan, Lyuheng and Ahmad, Akhlaque and Zheng, Chenguang and Chen, Hongzhi and Cheng, James},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671472},
  pages     = {6627–6632},
  title     = {Systems for scalable graph analytics and machine learning: Trends and methods},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey of large language models for graphs. <em>KDD</em>,
6616–6626. (<a href="https://doi.org/10.1145/3637528.3671460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graphs are an essential data structure utilized to represent relationships in real-world scenarios. Prior research has established that Graph Neural Networks (GNNs) deliver impressive outcomes in graph-centric tasks, such as link prediction and node classification. Despite these advancements, challenges like data sparsity and limited generalization capabilities continue to persist. Recently, Large Language Models (LLMs) have gained attention in natural language processing. They excel in language comprehension and summarization. Integrating LLMs with graph learning techniques has attracted interest as a way to enhance performance in graph learning tasks. In this survey, we conduct an in-depth review of the latest state-of-the-art LLMs applied in graph learning and introduce a novel taxonomy to categorize existing methods based on their framework design. We detail four unique designs: i) GNNs as Prefix, ii) LLMs as Prefix, iii) LLMs-Graphs Integration, and iv) LLMs-Only, highlighting key methodologies within each category. We explore the strengths and limitations of each framework, and emphasize potential avenues for future research, including overcoming current integration challenges between LLMs and graph learning techniques, and venturing into new application areas. This survey aims to serve as a valuable resource for researchers and practitioners eager to leverage large language models in graph learning, and to inspire continued progress in this dynamic field. We consistently maintain the related open-source materials at https://github.com/HKUDS/Awesome-LLM4Graph-Papers.},
  archive   = {C_KDD},
  author    = {Ren, Xubin and Tang, Jiabin and Yin, Dawei and Chawla, Nitesh and Huang, Chao},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671460},
  pages     = {6616–6626},
  title     = {A survey of large language models for graphs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inference optimization of foundation models on AI
accelerators. <em>KDD</em>, 6605–6615. (<a
href="https://doi.org/10.1145/3637528.3671465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Powerful foundation models, including large language models (LLMs), with Transformer architectures have ushered in a new era of Generative AI across various industries. Industry and research community have witnessed a large number of new applications, based on those foundation models. Such applications include question and answer, customer services, image and video generation, and code completions, among others. However, as the number of model parameters reaches to hundreds of billions, their deployment incurs prohibitive inference costs and high latency in real-world scenarios. As a result, the demand for cost-effective and fast inference using AI accelerators is ever more higher. To this end, our tutorial offers a comprehensive discussion on complementary inference optimization techniques using AI accelerators. Beginning with an overview of basic Transformer architectures and deep learning system frameworks, we deep dive into system optimization techniques for fast and memory-efficient attention computations and discuss how they can be implemented efficiently on AI accelerators. Next, we describe architectural elements that are key for fast transformer inference. Finally, we examine various model compression and fast decoding strategies in the same context.},
  archive   = {C_KDD},
  author    = {Park, Youngsuk and Budhathoki, Kailash and Chen, Liangfu and K\&quot;{u}bler, Jonas M. and Huang, Jiaji and Kleindessner, Matth\&quot;{a}us and Huan, Jun and Cevher, Volkan and Wang, Yida and Karypis, George},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671465},
  pages     = {6605–6615},
  title     = {Inference optimization of foundation models on AI accelerators},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explainable artificial intelligence on biosignals for
clinical decision support. <em>KDD</em>, 6597–6604. (<a
href="https://doi.org/10.1145/3637528.3671459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep learning has proven effective in several areas, including computer vision, natural language processing, and disease prediction, which can support clinicians in making decisions along the clinical pathway. However, in order to successfully integrate these algorithms into clinical practice, it is important that their decision-making processes are transparent, explainable, and interpretable. Firstly, this tutorial will introduce targeted eXplainable Artificial Intelligence (XAI) methods to address the urgent need for explainability of deep learning in healthcare applications. In particular, it focuses on algorithms for raw biosignals without prior feature extraction that enable medical diagnoses, specifically electrocardiograms (ECG) -- stemming from the heart -- and electroencephalograms (EEG) representing the electrical activity of the brain. Secondly, participants are provided with a comprehensive workflow that includes both data processing and an introduction to relevant network architectures. Subsequently, various XAI methods are described and it is shown, how the resulting relevance attributions can be visualized on biosignals. Finally, two compelling real-world use cases are presented that demonstrate the effectiveness of XAI in analyzing ECG and EEG signals for disease prediction and sleep classification, respectively. In summary, the tutorial will provide the skills required for gaining insight into the decision process of deep neural networks processing authentic clinical biosignal data.},
  archive   = {C_KDD},
  author    = {Maurer, Miriam Cindy and Metsch, Jacqueline Michelle and Hempel, Philip and Bender, Theresa and Spicher, Nicolai and Hauschild, Anne-Christin},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671459},
  pages     = {6597–6604},
  title     = {Explainable artificial intelligence on biosignals for clinical decision support},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Symbolic regression: A pathway to interpretability towards
automated scientific discovery. <em>KDD</em>, 6588–6596. (<a
href="https://doi.org/10.1145/3637528.3671464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Symbolic regression is a machine learning technique employed for learning mathematical equations directly from data. Mathematical equations capture both functional and causal relationships in the data. In addition, they are simple, compact, generalizable, and interpretable models, making them the best candidates for i) learning inherently transparent models and ii) boosting scientific discovery. Symbolic regression has received a growing interest since the last decade and is tackled using different approaches in supervised and unsupervised deep learning, thanks to the enormous progress achieved in deep learning in the last twenty years. Symbolic regression remains underestimated in conference coverage as a primary form of interpretable AI and a potential candidate for automating scientific discovery. This tutorial overviews symbolic regression: problem definition, approaches, and key limitations, discusses why physical sciences are beneficial to symbolic regression, and explores possible future directions in this research area.},
  archive   = {C_KDD},
  author    = {Makke, Nour and Chawla, Sanjay},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671464},
  pages     = {6588–6596},
  title     = {Symbolic regression: A pathway to interpretability towards automated scientific discovery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A review of graph neural networks in epidemic modeling.
<em>KDD</em>, 6577–6587. (<a
href="https://doi.org/10.1145/3637528.3671455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Since the onset of the COVID-19 pandemic, there has been a growing interest in studying epidemiological models. Traditional mechanistic models mathematically describe the transmission mechanisms of infectious diseases. However, they often fall short when confronted with the growing challenges of today. Consequently, Graph Neural Networks (GNNs) have emerged as a progressively popular tool in epidemic research. In this paper, we endeavor to furnish a comprehensive review of GNNs in epidemic tasks and highlight potential future directions. To accomplish this objective, we introduce hierarchical taxonomies for both epidemic tasks and methodologies, offering a trajectory of development within this domain. For epidemic tasks, we establish a taxonomy akin to those typically employed within the epidemic domain. For methodology, we categorize existing work into Neural Models and Hybrid Models. Following this, we perform an exhaustive and systematic examination of the methodologies, encompassing both the tasks and their technical details. Furthermore, we discuss the limitations of existing methods from diverse perspectives and systematically propose future research directions. This survey aims to bridge literature gaps and promote the progression of this promising field. We hope that it will facilitate synergies between the communities of GNNs and epidemiology, and contribute to their collective progress.},
  archive   = {C_KDD},
  author    = {Liu, Zewen and Wan, Guancheng and Prakash, B. Aditya and Lau, Max S.Y. and Jin, Wei},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671455},
  pages     = {6577–6587},
  title     = {A review of graph neural networks in epidemic modeling},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multimodal pretraining, adaptation, and generation for
recommendation: A survey. <em>KDD</em>, 6566–6576. (<a
href="https://doi.org/10.1145/3637528.3671473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Personalized recommendation serves as a ubiquitous channel for users to discover information tailored to their interests. However, traditional recommendation models primarily rely on unique IDs and categorical features for user-item matching, potentially overlooking the nuanced essence of raw item contents across multiple modalities such as text, image, audio, and video. This underutilization of multimodal data poses a limitation to recommender systems, especially in multimedia services like news, music, and short-video platforms. The recent advancements in large multimodal models offer new opportunities and challenges in developing content-aware recommender systems. This survey seeks to provide a comprehensive exploration of the latest advancements and future trajectories in multimodal pretraining, adaptation, and generation techniques, as well as their applications in enhancing recommender systems. Furthermore, we discuss current open challenges and opportunities for future research in this dynamic domain. We believe that this survey, alongside the curated resources, will provide valuable insights to inspire further advancements in this evolving landscape.},
  archive   = {C_KDD},
  author    = {Liu, Qijiong and Zhu, Jieming and Yang, Yanting and Dai, Quanyu and Du, Zhaocheng and Wu, Xiao-Ming and Zhao, Zhou and Zhang, Rui and Dong, Zhenhua},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671473},
  pages     = {6566–6576},
  title     = {Multimodal pretraining, adaptation, and generation for recommendation: A survey},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Foundation models for time series analysis: A tutorial and
survey. <em>KDD</em>, 6555–6565. (<a
href="https://doi.org/10.1145/3637528.3671451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Time series analysis stands as a focal point within the data mining community, serving as a cornerstone for extracting valuable insights crucial to a myriad of real-world applications. Recent advances in Foundation Models (FMs) have fundamentally reshaped the paradigm of model design for time series analysis, boosting various downstream tasks in practice. These innovative approaches often leverage pre-trained or fine-tuned FMs to harness generalized knowledge tailored for time series analysis. This survey aims to furnish a comprehensive and up-to-date overview of FMs for time series analysis. While prior surveys have predominantly focused on either application or pipeline aspects of FMs in time series analysis, they have often lacked an in-depth understanding of the underlying mechanisms that elucidate why and how FMs benefit time series analysis. To address this gap, our survey adopts a methodology-centric classification, delineating various pivotal elements of time-series FMs, including model architectures, pre-training techniques, adaptation methods, and data modalities. Overall, this survey serves to consolidate the latest advancements in FMs pertinent to time series analysis, accentuating their theoretical underpinnings, recent strides in development, and avenues for future exploration.},
  archive   = {C_KDD},
  author    = {Liang, Yuxuan and Wen, Haomin and Nie, Yuqi and Jiang, Yushan and Jin, Ming and Song, Dongjin and Pan, Shirui and Wen, Qingsong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671451},
  pages     = {6555–6565},
  title     = {Foundation models for time series analysis: A tutorial and survey},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph intelligence with large language models and prompt
learning. <em>KDD</em>, 6545–6554. (<a
href="https://doi.org/10.1145/3637528.3671456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph plays a significant role in representing and analyzing complex relationships in real-world applications such as citation networks, social networks, and biological data. Graph intelligence is rapidly becoming a crucial aspect of understanding and exploiting the intricate interconnections within graph data. Recently, large language models (LLMs) and prompt learning techniques have pushed graph intelligence forward, outperforming traditional Graph Neural Network (GNN) pre-training methods and setting new benchmarks for performance. In this tutorial, we begin by offering a comprehensive review and analysis of existing methods that integrate LLMs with graphs. We introduce existing works based on a novel taxonomy that classifies them into three distinct categories according to the roles of LLMs in graph tasks: as enhancers, predictors, or alignment components. Secondly, we introduce a new learning method that utilizes prompting on graphs, offering substantial potential to enhance graph transfer capabilities across diverse tasks and domains. We discuss existing works on graph prompting within a unified framework and introduce our developed tool for executing a variety of graph prompting tasks. Additionally, we discuss the applications of combining Graphs, LLMs, and prompt learning across various tasks, such as urban computing, recommendation systems, and anomaly detection. This lecture-style tutorial is an extension of our original work published in IJCAI 2024[44] and arXiv[77] with the invitation of KDD24.},
  archive   = {C_KDD},
  author    = {Li, Jia and Sun, Xiangguo and Li, Yuhan and Li, Zhixun and Cheng, Hong and Yu, Jeffrey Xu},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671456},
  pages     = {6545–6554},
  title     = {Graph intelligence with large language models and prompt learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on hypergraph neural networks: An in-depth and
step-by-step guide. <em>KDD</em>, 6534–6544. (<a
href="https://doi.org/10.1145/3637528.3671457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Higher-order interactions (HOIs) are ubiquitous in real-world complex systems and applications. Investigation of deep learning for HOIs, thus, has become a valuable agenda for the data mining and machine learning communities. As networks of HOIs are expressed mathematically as hypergraphs, hypergraph neural networks (HNNs) have emerged as a powerful tool for representation learning on hypergraphs. Given the emerging trend, we present the first survey dedicated to HNNs, with an in-depth and step-by-step guide. Broadly, the present survey overviews HNN architectures, training strategies, and applications. First, we break existing HNNs down into four design components: (i) input features, (ii) input structures, (iii) message-passing schemes, and (iv) training strategies. Second, we examine how HNNs address and learn HOIs with each of their components. Third, we overview the recent applications of HNNs in recommendation, bioinformatics and medical science, time series analysis, and computer vision. Lastly, we conclude with a discussion on limitations and future directions.},
  archive   = {C_KDD},
  author    = {Kim, Sunwoo and Lee, Soo Yong and Gao, Yue and Antelmi, Alessia and Polato, Mirko and Shin, Kijung},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671457},
  pages     = {6534–6544},
  title     = {A survey on hypergraph neural networks: An in-depth and step-by-step guide},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Grounding and evaluation for large language models:
Practical challenges and lessons learned (survey). <em>KDD</em>,
6523–6533. (<a href="https://doi.org/10.1145/3637528.3671467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the ongoing rapid adoption of Artificial Intelligence (AI)-based systems in high-stakes domains, ensuring the trustworthiness, safety, and observability of these systems has become crucial. It is essential to evaluate and monitor AI systems not only for accuracy and quality-related metrics but also for robustness, bias, security, interpretability, and other responsible AI dimensions. We focus on large language models (LLMs) and other generative AI models, which present additional challenges such as hallucinations, harmful and manipulative content, and copyright infringement. In this survey article accompanying our &amp;lt;u&amp;gt;tutorial&amp;lt;/u&amp;gt;, we highlight a wide range of harms associated with generative AI systems, and survey state of the art approaches (along with open challenges) to address these harms.},
  archive   = {C_KDD},
  author    = {Kenthapadi, Krishnaram and Sameki, Mehrnoosh and Taly, Ankur},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671467},
  pages     = {6523–6533},
  title     = {Grounding and evaluation for large language models: Practical challenges and lessons learned (Survey)},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sharing is caring: A practical guide to FAIR(ER) open data
release. <em>KDD</em>, 6513–6522. (<a
href="https://doi.org/10.1145/3637528.3671468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Findable. Accessible. Interoperable. Reusable. Since their introduction in 2016, the FAIR data principles have defined the standards by which scientific researchers share data. However, modern research in data editing and management consistently shows that while the FAIR data principles are widely accepted in theory, they can be much more difficult to understand and implement in practice. In this tutorial, we explore some of the simple, realistic steps scientists can take to FAIRly release open data. We also explore areas where the current FAIR guidelines fall short and offer practical suggestions for making open data FAIR(ER): more Equitable and Realistic. This first involves ways to make datasets themselves more equitably accessible for researchers with disabilities. While equitably accessible data design has some research overlap with paper, presentation, and website design, we suggest several unique distinctions specific to datasets. The &quot;Realistic&#39;&#39; aspect of FAIR(ER) data facilitates a path to translate open data (and research on that data) back to true applications. Driven by national security applications pipelines, we call out important considerations for balancing data editing against data realism.},
  archive   = {C_KDD},
  author    = {Henriksen, Amelia and Mundt, Miranda},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671468},
  pages     = {6513–6522},
  title     = {Sharing is caring: A practical guide to FAIR(ER) open data release},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph machine learning meets multi-table relational data.
<em>KDD</em>, 6502–6512. (<a
href="https://doi.org/10.1145/3637528.3671471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While graph machine learning, and notably graph neural networks (GNNs), have gained immense traction in recent years, application is predicated on access to a known input graph upon which predictive models can be trained. And indeed, within the most widely-studied public evaluation benchmarks such graphs are provided, with performance comparisons conditioned on curated data explicitly adhering to this graph. However, in real-world industrial applications, the situation is often quite different. Instead of a known graph, data are originally collected and stored across multiple tables in a repository, at times with ambiguous or incomplete relational structure. As such, to leverage the latest GNN architectures it is then up to a skilled data scientist to first manually construct a graph using intuition and domain knowledge, a laborious process that may discourage adoption in the first place. To narrow this gap and broaden the applicability of graph ML, we survey existing tools and strategies that can be combined to address the more fundamental problem of predictive tabular modeling over data native to multiple tables, with no explicit relational structure assumed a priori. This involves tracing a comprehensive path through related table join discovery and fuzzy table joining, column alignment, automated relational database (RDB) construction, extracting graphs from RDBs, graph sampling, and finally, graph-centric trainable predictive architectures. Although efforts to build deployable systems that integrate all of these components while minimizing manual effort remain in their infancy, this survey will nonetheless reduce barriers to entry and help steer the graph ML community towards promising research directions and wider real-world impact.},
  archive   = {C_KDD},
  author    = {Gan, Quan and Wang, Minjie and Wipf, David and Faloutsos, Christos},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671471},
  pages     = {6502–6512},
  title     = {Graph machine learning meets multi-table relational data},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on RAG meeting LLMs: Towards retrieval-augmented
large language models. <em>KDD</em>, 6491–6501. (<a
href="https://doi.org/10.1145/3637528.3671470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model&#39;s internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research. Updated information about this survey can be found at: https://advanced-recommender-systems.github.io/RAG-Meets-LLMs/},
  archive   = {C_KDD},
  author    = {Fan, Wenqi and Ding, Yujuan and Ning, Liangbo and Wang, Shijie and Li, Hengyun and Yin, Dawei and Chua, Tat-Seng and Li, Qing},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671470},
  pages     = {6491–6501},
  title     = {A survey on RAG meeting LLMs: Towards retrieval-augmented large language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reasoning and planning with large language models in code
development. <em>KDD</em>, 6480–6490. (<a
href="https://doi.org/10.1145/3637528.3671452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large Language Models (LLMs) are revolutionizing the field of code development by leveraging their deep understanding of code patterns, syntax, and semantics to assist developers in various tasks, from code generation and testing to code understanding and documentation. In this survey, accompanying our proposed lecture-style tutorial for KDD 2024, we explore the multifaceted impact of LLMs on the code development, delving into techniques for generating a high-quality code, creating comprehensive test cases, automatically generating documentation, and engaging in an interactive code reasoning. Throughout the survey, we highlight some crucial components surrounding LLMs, including pre-training, fine-tuning, prompt engineering, iterative refinement, agent planning, and hallucination mitigation. We put forward that such ingredients are essential to harness the full potential of these powerful AI models in revolutionizing software engineering and paving the way for a more efficient, effective, and innovative future in code development.},
  archive   = {C_KDD},
  author    = {Ding, Hao and Fan, Ziwei and Guehring, Ingo and Gupta, Gaurav and Ha, Wooseok and Huan, Jun and Liu, Linbo and Omidvar-Tehrani, Behrooz and Wang, Shiqi and Zhou, Hao},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671452},
  pages     = {6480–6490},
  title     = {Reasoning and planning with large language models in code development},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Recent and upcoming developments in randomized numerical
linear algebra for machine learning. <em>KDD</em>, 6470–6479. (<a
href="https://doi.org/10.1145/3637528.3671461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large matrices arise in many machine learning and data analysis applications, including as representations of datasets, graphs, model weights, and first and second-order derivatives. Randomized Numerical Linear Algebra (RandNLA) is an area which uses randomness to develop improved algorithms for ubiquitous matrix problems. The area has reached a certain level of maturity; but recent hardware trends, efforts to incorporate RandNLA algorithms into core numerical libraries, and advances in machine learning, statistics, and random matrix theory, have lead to new theoretical and practical challenges. This article provides a self-contained overview of RandNLA, in light of these developments.},
  archive   = {C_KDD},
  author    = {Derezi\&#39;{n}ski, Micha\l{} and Mahoney, Michael W.},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671461},
  pages     = {6470–6479},
  title     = {Recent and upcoming developments in randomized numerical linear algebra for machine learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advances in human event modeling: From graph neural networks
to language models. <em>KDD</em>, 6459–6469. (<a
href="https://doi.org/10.1145/3637528.3671466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human events such as hospital visits, protests, and epidemic outbreaks directly affect individuals, communities, and societies. These events are often influenced by factors such as economics, politics, and public policies of our society. The abundance of online data sources such as social networks, official news articles, and personal blogs chronicle societal events, facilitating the development of AI models for social science, public health care, and decision making. Human event modeling generally comprises both the forecasting stage, which estimates future events based on historical data, and interpretation, which seeks to identify influential factors of such events to understand their causative attributes. Recent achievements, fueled by deep learning and the availability of public data, have significantly advanced the field of human event modeling.This survey offers a systematic overview of deep learning technologies for forecasting and interpreting human events, with a primary focus on political events. We first introduce the existing challenges and background in this domain. We then present the problem formulation of event forecasting and interpretation. We investigate recent achievements in graph neural networks, owing to the prevalence of relational data and the efficacy of graph learning models. We also discuss the latest studies that utilize large language models for event reasoning. Lastly, we provide summaries of data resources, open challenges, and future research directions in the study of human event modeling.},
  archive   = {C_KDD},
  author    = {Deng, Songgaojun and de Rijke, Maarten and Ning, Yue},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671466},
  pages     = {6459–6469},
  title     = {Advances in human event modeling: From graph neural networks to language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A review of modern recommender systems using generative
models (gen-RecSys). <em>KDD</em>, 6448–6458. (<a
href="https://doi.org/10.1145/3637528.3671474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traditional recommender systems typically use user-item rating histories as their main data source. However, deep generative models now have the capability to model and sample from complex data distributions, including user-item interactions, text, images, and videos, enabling novel recommendation tasks. This comprehensive, multidisciplinary survey connects key advancements in RS using Generative Models (Gen-RecSys), covering: interaction-driven generative models; the use of large language models (LLM) and textual data for natural language recommendation; and the integration of multimodal models for generating and processing images/videos in RS. Our work highlights necessary paradigms for evaluating the impact and harm of Gen-RecSys and identifies open challenges. This survey accompanies a &quot;tutorial&quot; presented at ACM KDD&#39;24, with supporting materials provided at: https://encr.pw/vDhLq.},
  archive   = {C_KDD},
  author    = {Deldjoo, Yashar and He, Zhankui and McAuley, Julian and Korikov, Anton and Sanner, Scott and Ramisa, Arnau and Vidal, Ren\&#39;{e} and Sathiamoorthy, Maheswaran and Kasirzadeh, Atoosa and Milano, Silvia},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671474},
  pages     = {6448–6458},
  title     = {A review of modern recommender systems using generative models (Gen-RecSys)},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bias and unfairness in information retrieval systems: New
challenges in the LLM era. <em>KDD</em>, 6437–6447. (<a
href="https://doi.org/10.1145/3637528.3671458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the rapid advancements of large language models (LLMs), information retrieval (IR) systems, such as search engines and recommender systems, have undergone a significant paradigm shift. This evolution, while heralding new opportunities, introduces emerging challenges, particularly in terms of biases and unfairness, which may threaten the information ecosystem. In this paper, we present a comprehensive survey of existing works on emerging and pressing bias and unfairness issues in IR systems when the integration of LLMs. We first unify bias and unfairness issues as distribution mismatch problems, providing a groundwork for categorizing various mitigation strategies through distribution alignment. Subsequently, we systematically delve into the specific bias and unfairness issues arising from three critical stages of LLMs integration into IR systems: data collection, model development, and result evaluation. In doing so, we meticulously review and analyze recent literature, focusing on the definitions, characteristics, and corresponding mitigation strategies associated with these issues. Finally, we identify and highlight some open problems and challenges for future work, aiming to inspire researchers and stakeholders in the IR field and beyond to better understand and mitigate bias and unfairness issues of IR in this LLM era. We also consistently maintain a GitHub repository for the relevant papers and resources in this rising direction at https://github.com/KID-22/LLM-IR-Bias-Fairness-Survey.},
  archive   = {C_KDD},
  author    = {Dai, Sunhao and Xu, Chen and Xu, Shicheng and Pang, Liang and Dong, Zhenhua and Xu, Jun},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671458},
  pages     = {6437–6447},
  title     = {Bias and unfairness in information retrieval systems: New challenges in the LLM era},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decoding the AI pen: Techniques and challenges in detecting
AI-generated text. <em>KDD</em>, 6428–6436. (<a
href="https://doi.org/10.1145/3637528.3671463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large Language Models (LLMs) have revolutionized the field of Natural Language Generation (NLG) by demonstrating an impressive ability to generate human-like text. However, their widespread usage introduces challenges that necessitate thoughtful examination, ethical scrutiny, and responsible practices. In this study, we delve into these challenges, explore existing strategies for mitigating them, with a particular emphasis on identifying AI-generated text as the ultimate solution. Additionally, we assess the feasibility of detection from a theoretical perspective and propose novel research directions to address the current limitations in this domain.},
  archive   = {C_KDD},
  author    = {Abdali, Sara and Anarfi, Richard and Barberan, CJ and He, Jia},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671463},
  pages     = {6428–6436},
  title     = {Decoding the AI pen: Techniques and challenges in detecting AI-generated text},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Breaking barriers: A hands-on tutorial on AI-enabled
accessibility to social media content. <em>KDD</em>, 6426–6427. (<a
href="https://doi.org/10.1145/3637528.3671446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reddit&#39;s mission is to bring community, belonging, and empowerment to everyone in the world. This hands-on tutorial explores the immense potential of Artificial Intelligence (AI) to improve accessibility to social media content for individuals with different disabilities, including hearing, visual, and cognitive impairments. We will design and implement a variety of AI-based approaches based on multimodal open-source Large Language Models (LLMs) to bridge the gap between research and real-world applications.},
  archive   = {C_KDD},
  author    = {Villena, Julio and Catal\`{a}, Rosa and Garc\&#39;{\i}a, Janine and Polo, Concepci\&#39;{o}n and Labrador, Yessika and Del Valle, Francisco and Ayyagari, Bhargav},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671446},
  pages     = {6426–6427},
  title     = {Breaking barriers: A hands-on tutorial on AI-enabled accessibility to social media content},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph reasoning with LLMs (GReaL). <em>KDD</em>, 6424–6425.
(<a href="https://doi.org/10.1145/3637528.3671448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graphs are a powerful tool for representing and analyzing complex relationships in real-world applications. Large Language Models (LLMs) have demonstrated impressive capabilities by advancing state-of-the-art on many language-based benchmarks. Their ability to process and understand natural language open exciting possibilities in various domains. Despite the remarkable progress in automated reasoning with natural text, reasoning on graphs with LLMs remains an understudied problem that has recently gained more attention.This tutorial builds upon recent advances in expressing reasoning problems through the lens of tasks on graph data. The first part of the tutorial will provide an in-depth discussion of techniques for representing graphs as inputs to LLMs. The second, hands-on, portion will demonstrate these techniques in a practical setting. As a learning outcome of participating in the tutorial, participants will be able to analyze graphs either on free-tier Colab or their local machines with the help of LLMs.},
  archive   = {C_KDD},
  author    = {Tsitsulin, Anton and Perozzi, Bryan and Fatemi, Bahare and Halcrow, Jonathan J.},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671448},
  pages     = {6424–6425},
  title     = {Graph reasoning with LLMs (GReaL)},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Privacy-preserving federated learning using flower
framework. <em>KDD</em>, 6422–6423. (<a
href="https://doi.org/10.1145/3637528.3671447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {AI projects often face the challenge of limited access to meaningful amounts of training data. In traditional approaches, collecting data in a central location can be problematic, especially in industry settings with sensitive and distributed data. However, there is a solution -&quot;moving the computation to the data&quot; through Federated Learning.Federated Learning, a distributed machine learning approach, offers a promising solution by enabling model training across devices. It is a data minimization approach where direct access to data is not required. Furthermore, federated learning can be combined with techniques like differential privacy, secure aggregation, homomorphic encryption, and others, to further enhance privacy protection. In this hands-on tutorial, we delve into the realm of privacy-preserving machine learning using federated learning, leveraging the Flower framework which is specifically designed to simplify the process of building federated learning systems, as our primary tool. Moreover, we present the foundations of federated learning, explore how different techniques can enhance its privacy aspects, how it is being used in real-world settings today and a series of practical, hands-on code examples that showcase how you can federate any AI project with Flower, an open-source framework for all-this federated.},
  archive   = {C_KDD},
  author    = {Naseri, Mohammad and Fernandez-Marques, Javier and Gao, Yan and Pan, Heng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671447},
  pages     = {6422–6423},
  title     = {Privacy-preserving federated learning using flower framework},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DARE to diversify: DAta driven and diverse LLM REd teaming.
<em>KDD</em>, 6420–6421. (<a
href="https://doi.org/10.1145/3637528.3671444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large language models (LLMs) have been rapidly adopted, as showcased by ChatGPT&#39;s overnight popularity, and are integrated in products used by millions of people every day, such as search engines and productivity suites. Yet the societal impact of LLMs, encompassing both benefits and harms, is not well understood. Inspired by cybersecurity practices, red-teaming is emerging as a technique to uncover model vulnerabilities. Despite increasing attention from industry, academia, and government centered around red-teaming LLMs, such efforts are still limited in the diversity of the red-teaming focus, approaches and participants. Importantly, given that LLMs are becoming ubiquitous, it is imperative that red-teaming efforts are scaled out to include large segments of the research, practitioners and the people whom are directly affected by the deployment of these systems. The goal of this tutorial is two fold. First, we introduce the topic of LLM red-teaming by reviewing the state of the art for red-teaming practices, from participatory events to automatic AI-focused approaches, exposing the gaps in both the techniques and coverage of the targeted harms. Second, we plan to engage the audience in a hands-on and interactive exercise in LLM red-teaming to showcase the ease (or difficulty) of exposing model vulnerabilities, contingent on both the targeted harm and model capabilities. We believe that the KDD community of researchers and practitioners are in a unique position to address the existing gaps in red-teaming approaches, given their longstanding research and practice of extracting knowledge from data.},
  archive   = {C_KDD},
  author    = {Nagireddy, Manish and Guill\&#39;{e}n Pegueroles, Bernat and Baldini, Ioana},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671444},
  pages     = {6420–6421},
  title     = {DARE to diversify: DAta driven and diverse LLM REd teaming},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Practical machine learning for streaming data. <em>KDD</em>,
6418–6419. (<a href="https://doi.org/10.1145/3637528.3671442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Machine Learning for Data Streams has been an important area of research since the late 1990s, and its use in industry has grown significantly over the last few years. However, there is still a gap between the cutting-edge research and the tools that are readily available, which makes it challenging for practitioners, including experienced data scientists, to implement and evaluate these methods in this complex domain. Our tutorial aims to bridge this gap with a dual focus. We will discuss important research topics, such as partially delayed labeled streams, while providing practical demonstrations of their implementation and assessment using CapyMOA, an open-source library that provides efficient algorithm implementations through a high-level Python API. Source code is available in https://github.com/adaptive-machine-learning/CapyMOA while the accompanying tutorials and installation guide are available in https://capymoa.org/.},
  archive   = {C_KDD},
  author    = {Gomes, Heitor Murilo and Bifet, Albert},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671442},
  pages     = {6418–6419},
  title     = {Practical machine learning for streaming data},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Domain-driven LLM development: Insights into RAG and
fine-tuning practices. <em>KDD</em>, 6416–6417. (<a
href="https://doi.org/10.1145/3637528.3671445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To improve Large Language Model (LLM) performance on domain specific applications, ML developers often leverage Retrieval Augmented Generation (RAG) and LLM Fine-Tuning. RAG extends the capabilities of LLMs to specific domains or an organization&#39;s internal knowledge base, without the need to retrain the model. On the other hand, Fine-Tuning approach updates LLM weights with domain-specific data to improve performance on specific tasks. The fine-tuned model is particularly effective to systematically learn new comprehensive knowledge in a specific domain that is not covered by the LLM pre-training. This tutorial walks through the RAG and Fine-Tuning techniques, discusses the insights of their advantages and limitations, and provides best practices of adopting the methodologies for the LLM tasks and use cases. The hands-on labs demonstrate the advanced techniques to optimize the RAG and fine-tuned LLM architecture that handles domain specific LLM tasks. The labs in the tutorial are designed by using a set of open-source python libraries to implement the RAG and fine-tuned LLM architecture.},
  archive   = {C_KDD},
  author    = {dos Santos Junior, Jos\&#39;{e} Cassio and Hu, Rachel and Song, Richard and Bai, Yunfei},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671445},
  pages     = {6416–6417},
  title     = {Domain-driven LLM development: Insights into RAG and fine-tuning practices},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-modal data processing for foundation models: Practical
guidances and use cases. <em>KDD</em>, 6414–6415. (<a
href="https://doi.org/10.1145/3637528.3671441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the foundation models era, efficiently processing multi-modal data is crucial. This tutorial covers key techniques for multi-modal data processing and introduces the open-source Data-Juicer system, designed to tackle the complexities of data variety, quality, and scale. Participants will learn how to use Data-Juicer&#39;s operators and tools for formatting, mapping, filtering, deduplicating, and selecting multi-modal data efficiently and effectively. They will also be familiar with the Data-Juicer Sandbox Lab, where users can easily experiment with diverse data recipes that represent methodical sequences of operators and streamline the creation of scalable data processing pipelines. This experience solidifies the concepts discussed, as well as provides a space for innovation and exploration, highlighting how data recipes can be optimized and deployed in high-performance distributed environments.By the end of this tutorial, attendees will be equipped with the practical knowledge and skills to navigate the multi-modal data processing for foundation models. They will leave with actionable knowledge with an industrial open-source system and an enriched perspective on the importance of high-quality data in AI, poised to implement sustainable and scalable solutions in their projects. The system and related materials are available at https://github.com/modelscope/data-juicer.},
  archive   = {C_KDD},
  author    = {Chen, Daoyuan and Li, Yaliang and Ding, Bolin},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671441},
  pages     = {6414–6415},
  title     = {Multi-modal data processing for foundation models: Practical guidances and use cases},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A tutorial on multi-armed bandit applications for large
language models. <em>KDD</em>, 6412–6413. (<a
href="https://doi.org/10.1145/3637528.3671440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This tutorial offers a comprehensive guide on using multi-armed bandit (MAB) algorithms to improve Large Language Models (LLMs). As Natural Language Processing (NLP) tasks grow, efficient and adaptive language generation systems are increasingly needed. MAB algorithms, which balance exploration and exploitation under uncertainty, are promising for enhancing LLMs.The tutorial covers foundational MAB concepts, including the exploration-exploitation trade-off and strategies like epsilon-greedy, UCB (Upper Confidence Bound), and Thompson Sampling. It then explores integrating MAB with LLMs, focusing on designing architectures that treat text generation options as arms in a bandit problem. Practical aspects like reward design, exploration policies, and scalability are discussed.Real-world case studies demonstrate the benefits of MAB-augmented LLMs in content recommendation, dialogue generation, and personalized content creation, showing how these techniques improve relevance, diversity, and user engagement.},
  archive   = {C_KDD},
  author    = {Bouneffouf, Djallel and F\&#39;{e}raud, Rapha\&quot;{e}l},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671440},
  pages     = {6412–6413},
  title     = {A tutorial on multi-armed bandit applications for large language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hands-on introduction to time series classification and
regression. <em>KDD</em>, 6410–6411. (<a
href="https://doi.org/10.1145/3637528.3671443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Time series classification and regression are rapidly evolving fields that find areas of application in all domains of machine learning and data science. This hands on tutorial will provide an accessible overview of the recent research in these fields, using code examples to introduce the process of implementing and evaluating an estimator. We will show how to easily reproduce published results and how to compare a new algorithm to state-of-the-art. Finally, we will work through real world examples from the field of Electroencephalogram (EEG) classification and regression. EEG machine learning tasks arise in medicine, brain-computer interface research and psychology. We use these problems to how to compare algorithms on problems from a single domain and how to deal with data with different characteristics, such as missing values, unequal length and high dimensionality. The latest advances in the fields of time series classification and regression are all available through the aeon toolkit, an open source, scikit-learn compatible framework for time series machine learning which we use to provide our code examples.},
  archive   = {C_KDD},
  author    = {Bagnall, Anthony and Middlehurst, Matthew and Forestier, Germain and Ismail-Fawaz, Ali and Guillaume, Antoine and Guijo-Rubio, David and Tan, Chang Wei and Dempster, Angus and Webb, Geoffrey I.},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671443},
  pages     = {6410–6411},
  title     = {A hands-on introduction to time series classification and regression},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inductive modeling for realtime cold start recommendations.
<em>KDD</em>, 6400–6409. (<a
href="https://doi.org/10.1145/3637528.3671588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recommendation systems, the timely delivery of new content to their relevant audiences is critical for generating a growing and high quality collection of content for all users. The nature of this problem requires retrieval models to be able to make inferences in real time and with high relevance. There are two specific challenges for cold start contents. First, the information loss problem in a standard Two Tower model, due to the limited feature interactions between the user and item towers, is exacerbated for cold start items due to training data sparsity. Second, the huge volume of user-generated content in industry applications today poses a big bottleneck in the end-to-end latency of recommending new content. To overcome the two challenges, we propose a novel architecture, the Item History Model (IHM). IHM directly injects user-interaction information into the item tower to overcome information loss. In addition, IHM incorporates an inductive structure using attention-based pooling to eliminate the need for recurring training, a key bottleneck for the real-timeness. On both public and industry datasets, we demonstrate that IHM can not only outperform baselines in recommending cold start contents, but also achieves SoTA real-timeness in industry applications.},
  archive   = {C_KDD},
  author    = {Zuo, Chandler and Castaldo, Jonathan and Zhu, Hanqing and Zhang, Haoyu and Liu, Ji and Ou, Yangpeng and Kong, Xiao},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671588},
  pages     = {6400–6409},
  title     = {Inductive modeling for realtime cold start recommendations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bringing multimodality to amazon visual search system.
<em>KDD</em>, 6390–6399. (<a
href="https://doi.org/10.1145/3637528.3671640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image to image matching has been well studied in the computer vision community. Previous studies mainly focus on training a deep metric learning model matching visual patterns between the query image and gallery images. In this study, we show that pure image-to- image matching suffers from false positives caused by matching to local visual patterns. To alleviate this issue, we propose to leverage recent advances in vision-language pretraining research. Specifically, we introduce additional image-text alignment losses into deep metric learning, which serve as constraints to the image-to-image matching loss. With additional alignments between the text (e.g., product title) and image pairs, the model can learn concepts from both modalities explicitly, which avoids matching low-level visual features. We progressively develop two variants, a 3-tower and a 4-tower model, where the latter takes one more short text query input. Through extensive experiments, we show that this change leads to a substantial improvement to the image to image matching problem. We further leveraged this model for multimodal search, which takes both image and reformulation text queries to improve search quality. Both offline and online experiments show strong improvements on the main metrics. Specifically, we see 4.95\% relative improvement on image matching click through rate with the 3-tower model and 1.13\% further improvement from the 4-tower model.},
  archive   = {C_KDD},
  author    = {Zhu, Xinliang and Huang, Sheng-Wei and Ding, Han and Yang, Jinyu and Chen, Kelvin and Zhou, Tao and Neiman, Tal and Xie, Ouye and Tran, Son and Yao, Benjamin and Gray, Douglas and Bindal, Anuj and Dhua, Arnab},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671640},
  pages     = {6390–6399},
  title     = {Bringing multimodality to amazon visual search system},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). STATE: A robust ATE estimator of heavy-tailed metrics for
variance reduction in online controlled experiments. <em>KDD</em>,
6380–6389. (<a href="https://doi.org/10.1145/3637528.3672352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Online controlled experiments play a crucial role in enabling data-driven decisions across a wide range of companies. Variance reduction is an effective technique to improve the sensitivity of experiments, achieving higher statistical power while using fewer samples and shorter experimental periods. However, typical variance reduction methods (e.g., regression-adjusted estimators) are built upon the intuitional assumption of Gaussian distributions and cannot properly characterize the real business metrics with heavy-tailed distributions. Furthermore, outliers diminish the correlation between pre-experiment covariates and outcome metrics, greatly limiting the effectiveness of variance reduction.In this paper, we develop a novel framework that integrates the Student&#39;s t-distribution with machine learning tools to fit heavy-tailed metrics and construct a robust average treatment effect estimator in online controlled experiments, which we call STATE. By adopting a variational EM method to optimize the loglikehood function, we can infer a robust solution that greatly eliminates the negative impact of outliers and achieves significant variance reduction. Moreover, we extend the STATE method from count metrics to ratio metrics by utilizing linear transformation that preserves unbiased estimation, whose variance reduction is more complex but less investigated in existing works. Finally, both simulations on synthetic data and long-term empirical results on Meituan experiment platform demonstrate the effectiveness of our method. Compared with the state-of-the-art estimators (CUPAC/MLRATE), STATE achieves over 50\% variance reduction, indicating it can reach the same statistical power with only half of the observations, or half the experimental duration.},
  archive   = {C_KDD},
  author    = {Zhou, Hao and Sun, Kun and Li, Shaoming and Fan, Yangfeng and Jiang, Guibin and Zheng, Jiaqi and Li, Tao},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672352},
  pages     = {6380–6389},
  title     = {STATE: A robust ATE estimator of heavy-tailed metrics for variance reduction in online controlled experiments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decision focused causal learning for direct counterfactual
marketing optimization. <em>KDD</em>, 6368–6379. (<a
href="https://doi.org/10.1145/3637528.3672353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Marketing optimization plays an important role to enhance user engagement in online Internet platforms. Existing studies usually formulate this problem as a budget allocation problem and solve it by utilizing two fully decoupled stages, i.e., machine learning (ML) and operation research (OR). However, the learning objective in ML does not take account of the downstream optimization task in OR, which causes that the prediction accuracy in ML may be not positively related to the decision quality.Decision Focused Learning (DFL) integrates ML and OR into an end-to-end framework, which takes the objective of the downstream task as the decision loss function and guarantees the consistency of the optimization direction between ML and OR. However, deploying DFL in marketing is non-trivial due to multiple technological challenges. Firstly, the budget allocation problem in marketing is a 0-1 integer stochastic programming problem and the budget is uncertain and fluctuates a lot in real-world settings, which is beyond the general problem background in DFL. Secondly, the counterfactual in marketing causes that the decision loss cannot be directly computed and the optimal solution can never be obtained, both of which disable the common gradient-estimation approaches in DFL. Thirdly, the OR solver is called frequently to compute the decision loss during model training in DFL, which produces huge computational cost and cannot support large-scale training data. In this paper, we propose a decision focused causal learning framework (DFCL) for direct counterfactual marketing optimization, which overcomes the above technological challenges. Both offline experiments and online A/B testing demonstrate the effectiveness of DFCL over the state-of-the-art methods. Currently, DFCL has been deployed in several marketing scenarios in Meituan, one of the largest online food delivery platform in the world.},
  archive   = {C_KDD},
  author    = {Zhou, Hao and Huang, Rongxiao and Li, Shaoming and Jiang, Guibin and Zheng, Jiaqi and Cheng, Bing and Lin, Wei},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672353},
  pages     = {6368–6379},
  title     = {Decision focused causal learning for direct counterfactual marketing optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GraphStorm: All-in-one graph machine learning framework for
industry applications. <em>KDD</em>, 6356–6367. (<a
href="https://doi.org/10.1145/3637528.3671603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph machine learning (GML) is effective in many business applications. However, making GML easy to use and applicable to industry applications with massive datasets remain challenging. We developed GraphStorm, which provides an end-to-end solution for scalable graph construction, graph model training and inference. GraphStorm has the following desirable properties: (a) Easy to use: it can perform graph construction and model training and inference with just a single command; (b) Expert-friendly: GraphStorm contains many advanced GML modeling techniques to handle complex graph data and improve model performance; (c) Scalable: every component in GraphStorm can operate on graphs with billions of nodes and can scale model training and inference to different hardware without changing any code. GraphStorm has been used and deployed for over a &amp;lt;u&amp;gt;dozen&amp;lt;/u&amp;gt; &amp;lt;u&amp;gt;billion-scale&amp;lt;/u&amp;gt; industry applications after its release in May 2023. It is open-sourced in Github: https://github.com/awslabs/graphstorm.},
  archive   = {C_KDD},
  author    = {Zheng, Da and Song, Xiang and Zhu, Qi and Zhang, Jian and Vasiloudis, Theodore and Ma, Runjie and Zhang, Houyu and Wang, Zichen and Adeshina, Soji and Nisa, Israt and Mottini, Alejandro and Cui, Qingjun and Rangwala, Huzefa and Zeng, Belinda and Faloutsos, Christos and Karypis, George},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671603},
  pages     = {6356–6367},
  title     = {GraphStorm: All-in-one graph machine learning framework for industry applications},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lookahead: An inference acceleration framework for large
language model with lossless generation accuracy. <em>KDD</em>,
6344–6355. (<a href="https://doi.org/10.1145/3637528.3671614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As Large Language Models (LLMs) have made significant advancements across various tasks, such as question answering, translation, text summarization, and dialogue systems, the need for accuracy in information becomes crucial, especially for serious financial products serving billions of users like Alipay. However, for a real-world product serving millions of users, the inference speed of LLMs becomes a critical factor compared to a mere experimental model.Hence, this paper presents a generic framework for accelerating the inference process, resulting in a substantial increase in speed and cost reduction for our LLM-based scenarios, with lossless generation accuracy. In the traditional inference process, each token is generated sequentially by the LLM, leading to a time consumption proportional to the number of generated tokens. To enhance this process, our framework, named lookahead, introduces a multi-branch strategy. Instead of generating a single token at a time, we propose a Trie-based retrieval and verification mechanism to be able to accept several tokens at a forward step. Our strategy offers two distinct advantages: (1) it guarantees absolute correctness of the output, avoiding any approximation algorithms, and (2) the worst-case performance of our approach could be comparable with the performance of the conventional process. We conduct extensive experiments to demonstrate the significant improvements achieved by applying our inference acceleration framework. Our framework has been widely deployed in Alipay since April 2023, and obtained remarkable 2.66x to 6.26x speedup. Our code is available at https://github.com/alipay/PainlessInferenceAcceleration.},
  archive   = {C_KDD},
  author    = {Zhao, Yao and Xie, Zhitian and Liang, Chen and Zhuang, Chenyi and Gu, Jinjie},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671614},
  pages     = {6344–6355},
  title     = {Lookahead: An inference acceleration framework for large language model with lossless generation accuracy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DUE: Dynamic uncertainty-aware explanation supervision via
3D imputation. <em>KDD</em>, 6335–6343. (<a
href="https://doi.org/10.1145/3637528.3671641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Explanation supervision aims to enhance deep learning models by integrating additional signals to guide the generation of model explanations, showcasing notable improvements in both the predictability and explainability of the model. However, the application of explanation supervision to higher-dimensional data, such as 3D medical images, remains an under-explored domain. Challenges associated with supervising visual explanations in the presence of an additional dimension include: 1) spatial correlation changed, 2) lack of direct 3D annotations, and 3) uncertainty varies across different parts of the explanation. To address these challenges, we propose a Dynamic Uncertainty-aware Explanation supervision (DUEfootnoteCode available at: https://github.com/AlexQilong/DUE.) framework for 3D explanation supervision that ensures uncertainty-aware explanation guidance when dealing with sparsely annotated 3D data with diffusion-based 3D interpolation. Our proposed framework is validated through comprehensive experiments on diverse real-world medical imaging datasets. The results demonstrate the effectiveness of our framework in enhancing the predictability and explainability of deep learning models in the context of medical imaging diagnosis applications.},
  archive   = {C_KDD},
  author    = {Zhao, Qilong and Zhang, Yifei and Zhu, Mengdan and Gu, Siyi and Gao, Yuyang and Yang, Xiaofeng and Zhao, Liang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671641},
  pages     = {6335–6343},
  title     = {DUE: Dynamic uncertainty-aware explanation supervision via 3D imputation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TACCO: Task-guided co-clustering of clinical concepts and
patient visits for disease subtyping based on EHR data. <em>KDD</em>,
6324–6334. (<a href="https://doi.org/10.1145/3637528.3671594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The growing availability of well-organized Electronic Health Records (EHR) data has enabled the development of various machine learning models towards disease risk prediction. However, existing risk prediction methods overlook the heterogeneity of complex diseases, failing to model the potential disease subtypes regarding their corresponding patient visits and clinical concept subgroups. In this work, we introduce TACCO, a novel framework that jointly discovers clusters of clinical concepts and patient visits based on a hypergraph modeling of EHR data. Specifically, we develop a novel self-supervised co-clustering framework that can be guided by the risk prediction task of specific diseases. Furthermore, we enhance the hypergraph model of EHR data with textual embeddings and enforce the alignment between the clusters of clinical concepts and patient visits through a contrastive objective. Comprehensive experiments conducted on the public MIMIC-III dataset and Emory internal CRADLE dataset over the downstream clinical tasks of phenotype classification and cardiovascular risk prediction demonstrate an average 31.25\% performance improvement compared to traditional ML baselines and a 5.26\% improvement on top of the vanilla hypergraph model without our co-clustering mechanism. In-depth model analysis, clustering results analysis, and clinical case studies further validate the improved utilities and insightful interpretations delivered by TACCO. Code is available at https://github.com/PericlesHat/TACCO.},
  archive   = {C_KDD},
  author    = {Zhang, Ziyang and Cui, Hejie and Xu, Ran and Xie, Yuzhang and Ho, Joyce C. and Yang, Carl},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671594},
  pages     = {6324–6334},
  title     = {TACCO: Task-guided co-clustering of clinical concepts and patient visits for disease subtyping based on EHR data},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Diet-ODIN: A novel framework for opioid misuse detection
with interpretable dietary patterns. <em>KDD</em>, 6312–6323. (<a
href="https://doi.org/10.1145/3637528.3671587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The opioid crisis has been one of the most critical society concerns in the United States. Although the medication assisted treatment (MAT) is recognized as the most effective treatment for opioid misuse and addiction, the various side effects can trigger opioid relapse. In addition to MAT, the dietary nutrition intervention has been demonstrated its importance in opioid misuse prevention and recovery. However, research on the alarming connections between dietary patterns and opioid misuse remain under-explored. In response to this gap, in this paper, we first establish a large-scale multifaceted dietary benchmark dataset related to opioid users at the first attempt and then develop a novel framework - i.e., namely Opioid Misuse Detection with INterpretable Dietary Patterns (Diet-ODIN) - to bridge heterogeneous graph (HG) and large language model (LLM) for the identification of users with opioid misuse and the interpretation of their associated dietary patterns. Specifically, in Diet-ODIN, we first construct an HG to comprehensively incorporate both dietary and health-related information, and then we devise a holistic graph learning framework with noise reduction to fully capitalize both users&#39; individual dietary habits and shared dietary patterns for the detection of users with opioid misuse. To further delve into the intricate correlations between dietary patterns and opioid misuse, we exploit an LLM by utilizing the knowledge obtained from the graph learning model for interpretation. The extensive experimental results based on our established benchmark with quantitative and qualitative measures demonstrate the outstanding performance of Diet-ODIN on exploring the complex interplay between opioid misuse and dietary patterns, by comparison with state-of-the-art baseline methods. Our code, built benchmark and system demo are available at https://github.com/JasonZhangzy1757/Diet-ODIN.},
  archive   = {C_KDD},
  author    = {Zhang, Zheyuan and Wang, Zehong and Hou, Shifu and Hall, Evan and Bachman, Landon and White, Jasmine and Galassi, Vincent and Chawla, Nitesh V. and Zhang, Chuxu and Ye, Yanfang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671587},
  pages     = {6312–6323},
  title     = {Diet-ODIN: A novel framework for opioid misuse detection with interpretable dietary patterns},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GraSS: Combining graph neural networks with expert knowledge
for SAT solver selection. <em>KDD</em>, 6301–6311. (<a
href="https://doi.org/10.1145/3637528.3671627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Boolean satisfiability (SAT) problems are routinely solved by SAT solvers in real-life applications, yet solving time can vary drastically between solvers for the same instance. This has motivated research into machine learning models that can predict, for a given SAT instance, which solver to select among several options. Existing SAT solver selection methods all rely on some hand-picked instance features, which are costly to compute and ignore the structural information in SAT graphs. In this paper we present GraSS, a novel approach for automatic SAT solver selection based on tripartite graph representations of instances and a heterogeneous graph neural network (GNN) model. While GNNs have been previously adopted in other SAT-related tasks, they do not incorporate any domain-specific knowledge and ignore the runtime variation introduced by different clause orders. We enrich the graph representation with domain-specific decisions, such as novel node feature design, positional encodings for clauses in the graph, a GNN architecture tailored to our tripartite graphs and a runtime-sensitive loss function. Through extensive experiments, we demonstrate that this combination of raw representations and domain-specific choices leads to improvements in runtime for a pool of seven state-of-the-art solvers on both an industrial circuit design benchmark, and on instances from the 20-year Anniversary Track of the 2022 SAT Competition.},
  archive   = {C_KDD},
  author    = {Zhang, Zhanguang and Ch\&#39;{e}telat, Didier and Cotnareanu, Joseph and Ghose, Amur and Xiao, Wenyi and Zhen, Hui-Ling and Zhang, Yingxue and Hao, Jianye and Coates, Mark and Yuan, Mingxuan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671627},
  pages     = {6301–6311},
  title     = {GraSS: Combining graph neural networks with expert knowledge for SAT solver selection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unified dual-intent translation for joint modeling of search
and recommendation. <em>KDD</em>, 6291–6300. (<a
href="https://doi.org/10.1145/3637528.3671519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recommendation systems, which assist users in discovering their preferred items among numerous options, have served billions of users across various online platforms. Intuitively, users&#39; interactions with items are highly driven by their unchanging inherent intents (e.g., always preferring high-quality items) and changing demand intents (e.g., wanting a T-shirt in summer but a down jacket in winter). However, both types of intents are implicitly expressed in recommendation scenario, posing challenges in leveraging them for accurate intent-aware recommendations. Fortunately, in search scenario, often found alongside recommendation on the same online platform, users express their demand intents explicitly through their query words. Intuitively, in both scenarios, a user shares the same inherent intent and his/her interactions may be influenced by the same demand intent. It is therefore feasible to utilize the interaction data from both scenarios to reinforce the dual intents for joint intent-aware modeling. But the joint modeling should deal with two problems: (1) accurately modeling users&#39; implicit demand intents in recommendation; (2) modeling the relation between the dual intents and the interactive items. To address these problems, we propose a novel model named Unified Dual-Intents Translation for joint modeling of Search and Recommendation (UDITSR). To accurately simulate users&#39; demand intents in recommendation, we utilize real queries from search data as supervision information to guide its generation. To explicitly model the relation among the triplet &amp;lt;inherent intent, demand intent, interactive item&amp;gt;, we propose a dual-intent translation propagation mechanism to learn the triplet in the same semantic space via embedding translations. Extensive experiments demonstrate that UDITSR outperforms SOTA baselines both in search and recommendation tasks. Moreover, our model has been deployed online on Meituan Waimai platform, leading to an average improvement in GMV (Gross Merchandise Value) of 1.46\% and CTR(Click-Through Rate) of 0.77\% over one month.},
  archive   = {C_KDD},
  author    = {Zhang, Yuting and Wu, Yiqing and Han, Ruidong and Sun, Ying and Zhu, Yongchun and Li, Xiang and Lin, Wei and Zhuang, Fuzhen and An, Zhulin and Xu, Yongjun},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671519},
  pages     = {6291–6300},
  title     = {Unified dual-intent translation for joint modeling of search and recommendation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing smartphone app usage prediction: A click-through
rate ranking approach. <em>KDD</em>, 6281–6290. (<a
href="https://doi.org/10.1145/3637528.3671567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Over the past decade, smartphones have become indispensable personal mobile devices, experiencing a remarkable surge in software apps. These apps empower users to seamlessly connect with various internet services, such as social communication and online shopping. Accurately predicting smartphone app usage can effectively improve user experience and optimize resource utilization. However, existing models often treat app usage prediction as a classification problem, which suffers from issues of app usage imbalance and out-of-distribution (OOD) during deployment. To address these challenges, this paper proposes a novel click-through rate (CTR) ranking-based method for predicting app usage. By transforming the classification problem into a CTR problem, we can eliminate the negative impact of the app usage imbalance issue. To address the OOD issue during deployment, we generate the app click sequence and three types of discriminative features, which enable generalization on unseen apps. The app click sequence and the three types of features serve as inputs for training a CTR estimation model in the cloud, and the trained model is then deployed on the user&#39;s smartphone to predict the CTR for each installed app. The decision-making process involves ranking these CTR values and selecting the app with the highest CTR as the final prediction. Our method has been extensively tested with large-scale app usage data. The results demonstrate that our approach is able to outperform state-of-the-art methods, with improvements over 4.93\% in top-3 accuracy and 6.64\% in top-5 accuracy. It achieves approximately twice the accuracy in predicting apps with low usage frequencies in comparison to baseline methods. Our method has been successfully deployed on the app recommendation system of a leading smartphone manufacturer.},
  archive   = {C_KDD},
  author    = {Zhang, Yuqi and Kang, Meiying and Li, Xiucheng and Qiu, Yu and Li, Zhijun},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671567},
  pages     = {6281–6290},
  title     = {Optimizing smartphone app usage prediction: A click-through rate ranking approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Large language model with curriculum reasoning for visual
concept recognition. <em>KDD</em>, 6269–6280. (<a
href="https://doi.org/10.1145/3637528.3671653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual concept recognition aims to capture the basic attributes of an image and reason about the relationships among them to determine whether the image satisfies a certain concept, and has been widely used in various tasks such as human action recognition and image risk warning. Most existing works adopt deep neural networks for visual concept recognition, which are black-box and incomprehensible to humans, thus making them unacceptable for sensitive domains such as prohibited event detection and risk early warning etc. To address this issue, we propose to combine large language model (LLM) with explainable symbolic reasoning via curriculum reweighting to increase the interpretability and accuracy of visual concept recognition in this paper. However, realizing this goal is challenging given that i) the performance of symbolic representations are limited by the lack of annotated reasoning symbols and rules for most tasks, and ii) the LLMs may suffer from knowlege hallucination and dynamic open environment. To address these issues, in this paper, we propose CurLLM-Reasoner, a curriculum reasoning method based on symbolic reasoning and large language model for visual concept recognition. Specifically, we propose a novel rule enhancement module with a tool library, which fully leverage the reasoning capability of large language models and can generate human-understandable rules without any annotation. We further propose a curriculum data resampling methodology to help the large language model accurately extract from easy to complex rules at different reasoning stages. Extensive experiments on various datasets demonstrate that CurLLM-Reasoner can achieve the state-of-the-art visual concept recognition results with explainable rules while free of human annotations.},
  archive   = {C_KDD},
  author    = {Zhang, Yipeng and Wang, Xin and Chen, Hong and Fan, Jiapei and Wen, Weigao and Xue, Hui and Mei, Hong and Zhu, Wenwu},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671653},
  pages     = {6269–6280},
  title     = {Large language model with curriculum reasoning for visual concept recognition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-behavior collaborative filtering with partial order
graph convolutional networks. <em>KDD</em>, 6257–6268. (<a
href="https://doi.org/10.1145/3637528.3671569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Representing information of multiple behaviors in the single graph collaborative filtering (CF) vector has been a long-standing challenge. This is because different behaviors naturally form separate behavior graphs and learn separate CF embeddings. Existing models merge the separate embeddings by appointing the CF embeddings for some behaviors as the primary embedding and utilizing other auxiliaries to enhance the primary embedding. However, this approach often results in the joint embedding performing well on the main tasks but poorly on the auxiliary ones. To address the problem arising from the separate behavior graphs, we propose the concept of &amp;lt;u&amp;gt;P&amp;lt;/u&amp;gt;artial &amp;lt;u&amp;gt;O&amp;lt;/u&amp;gt;rder Recommendation &amp;lt;u&amp;gt;G&amp;lt;/u&amp;gt;raphs (POG). POG defines the partial order relation of multiple behaviors and models behavior combinations as weighted edges to merge separate behavior graphs into a joint POG. Theoretical proof verifies that POG can be generalized to any given set of multiple behaviors. Based on POG, we propose the tailored &amp;lt;u&amp;gt;P&amp;lt;/u&amp;gt;artial &amp;lt;u&amp;gt;O&amp;lt;/u&amp;gt;rder &amp;lt;u&amp;gt;G&amp;lt;/u&amp;gt;raph &amp;lt;u&amp;gt;C&amp;lt;/u&amp;gt;onvolutional &amp;lt;u&amp;gt;N&amp;lt;/u&amp;gt;etworks (POGCN) that convolute neighbors&#39; information while considering the behavior relations between users and items. POGCN also introduces a partial-order BPR sampling strategy for efficient and effective multiple-behavior CF training. POGCN has been successfully deployed on the homepage of Alibaba for two months, providing recommendation services for over one billion users. Extensive offline experiments conducted on three public benchmark datasets demonstrate that POGCN outperforms state-of-the-art multi-behavior baselines across all types of behaviors. Furthermore, online A/B tests confirm the superiority of POGCN in billion-scale recommender systems.},
  archive   = {C_KDD},
  author    = {Zhang, Yijie and Bei, Yuanchen and Chen, Hao and Shen, Qijie and Yuan, Zheng and Gong, Huan and Wang, Senzhang and Huang, Feiran and Huang, Xiao},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671569},
  pages     = {6257–6268},
  title     = {Multi-behavior collaborative filtering with partial order graph convolutional networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Temporal uplift modeling for online marketing.
<em>KDD</em>, 6247–6256. (<a
href="https://doi.org/10.1145/3637528.3671560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, uplift modeling, also known as individual treatment effect (ITE) estimation, has seen wide applications in online marketing, such as delivering one-time issuance of coupons or discounts to motivate users&#39; purchases. However, complex yet more realistic scenarios involving multiple interventions over time on users are still rarely explored. The challenges include handling the bias from time-varying confounders, determining optimal treatment timing, and selecting among numerous treatments. In this paper, to tackle the aforementioned challenges, we present a temporal point process-based uplift model (TPPUM) that utilizes users&#39; temporal event sequences to estimate treatment effects via counterfactual analysis and temporal point processes. In this model, marketing actions are considered as treatments, user purchases as outcome events, and how treatments alter the future conditional intensity function of generating outcome events as the uplift. Empirical evaluations demonstrate that our method outperforms existing baselines on both real-world and synthetic datasets. In the online experiment conducted in a discounted bundle recommendation scenario involving an average of 3 to 4 interventions per day and hundreds of treatment candidates, we demonstrate how our model outperforms current state-of-the-art methods in selecting the appropriate treatment and timing of treatment, resulting in a 3.6\% increase in application-level revenue.},
  archive   = {C_KDD},
  author    = {Zhang, Xin and Wang, Kai and Wang, Zengmao and Du, Bo and Zhao, Shiwei and Wu, Runze and Shen, Xudong and Lv, Tangjie and Fan, Changjie},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671560},
  pages     = {6247–6256},
  title     = {Temporal uplift modeling for online marketing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dólares or dollars? Unraveling the bilingual prowess of
financial LLMs between spanish and english. <em>KDD</em>, 6236–6246. (<a
href="https://doi.org/10.1145/3637528.3671554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite Spanish&#39;s pivotal role in the global finance industry, a pronounced gap exists in Spanish financial natural language processing (NLP) and application studies compared to English, especially in the era of large language models (LLMs). To bridge this gap, we unveil Tois\&#39;{o}n de Oro, the first bilingual framework that establishes instruction datasets, finetuned LLMs, and evaluation benchmark for financial LLMs in Spanish joint with English. We construct a rigorously curated bilingual instruction dataset including over 144K Spanish and English samples from 15 datasets covering 7 tasks. Harnessing this, we introduce FinMA-ES, an LLM designed for bilingual financial applications. We evaluate our model and existing LLMs using FLARE-ES, the first comprehensive bilingual evaluation benchmark with 21 datasets covering 9 tasks. The FLARE-ES benchmark results reveal a significant multilingual performance gap and bias in existing LLMs. FinMA-ES models surpass SOTA LLMs such as GPT-4 in Spanish financial tasks, due to strategic instruction tuning and leveraging data from diverse linguistic resources, highlighting the positive impact of cross-linguistic transfer. All our datasets, models, and benchmarks have been released.},
  archive   = {C_KDD},
  author    = {Zhang, Xiao and Xiang, Ruoyu and Yuan, Chenhan and Feng, Duanyu and Han, Weiguang and Lopez-Lira, Alejandro and Liu, Xiao-Yang and Qiu, Meikang and Ananiadou, Sophia and Peng, Min and Huang, Jimin and Xie, Qianqian},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671554},
  pages     = {6236–6246},
  title     = {D\&#39;{o}lares or dollars? unraveling the bilingual prowess of financial LLMs between spanish and english},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). A self-boosted framework for calibrated ranking.
<em>KDD</em>, 6226–6235. (<a
href="https://doi.org/10.1145/3637528.3671570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Scale-calibrated ranking systems are ubiquitous in real-world applications nowadays, which pursue accurate ranking quality and calibrated probabilistic predictions simultaneously. For instance, in the advertising ranking system, the predicted click-through rate (CTR) is utilized for ranking and required to be calibrated for the downstream cost-per-click ads bidding. Recently, multi-objective based methods have been wildly adopted as a standard approach for Calibrated Ranking, which incorporates the combination of two loss functions: a pointwise loss that focuses on calibrated absolute values and a ranking loss that emphasizes relative orderings. However, when applied to industrial online applications, existing multi-objective CR approaches still suffer from two crucial limitations First, previous methods need to aggregate the full candidate list within a single mini-batch to compute the ranking loss. Such aggregation strategy violates extensive data shuffling which has long been proven beneficial for preventing overfitting, and thus degrades the training effectiveness. Second, existing multi-objective methods apply the two inherently conflicting loss functions on a single probabilistic prediction, which results in a sub-optimal trade-off between calibration and ranking.To tackle the two limitations, we propose a Self-Boosted framework for Calibrated Ranking (SBCR). In SBCR, the predicted ranking scores by the online deployed model are dumped into context features. With these additional context features, each single item can perceive the overall distribution of scores in the whole ranking list, so that the ranking loss can be constructed without the need for sample aggregation. As the deployed model is a few versions older than the training model, the dumped predictions reveal what was failed to learn and keep boosting the model to correct previously mis-predicted items. Moreover, a calibration module is introduced to decouple the point loss and ranking loss. The two losses are applied before and after the calibration module separately, which elegantly addresses the sub-optimal trade-off problem. We conduct comprehensive experiments on industrial scale datasets and online A/B tests, demonstrating that SBCR can achieve advanced performance on both calibration and ranking. Our method has been deployed on the video search system of Kuaishou, and results in significant performance improvements on CTR and the total amount of time users spend on Kuaishou.},
  archive   = {C_KDD},
  author    = {Zhang, Shunyu and Liu, Hu and Bao, Wentian and Yu, Enyun and Song, Yang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671570},
  pages     = {6226–6235},
  title     = {A self-boosted framework for calibrated ranking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OAG-bench: A human-curated benchmark for academic graph
mining. <em>KDD</em>, 6214–6225. (<a
href="https://doi.org/10.1145/3637528.3672354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the rapid proliferation of scientific literature, versatile academic knowledge services increasingly rely on comprehensive academic graph mining. Despite the availability of public academic graphs, benchmarks, and datasets, these resources often fall short in multi-aspect and fine-grained annotations, are constrained to specific task types and domains, or lack underlying real academic graphs. In this paper, we present OAG-Bench, a comprehensive, multi-aspect, and fine-grained human-curated benchmark based on the Open Academic Graph (OAG). OAG-Bench covers 10 tasks, 20 datasets, 70+ baselines, and 120+ experimental results to date. We propose new data annotation strategies for certain tasks and offer a suite of data pre-processing codes, algorithm implementations, and standardized evaluation protocols to facilitate academic graph mining. Extensive experiments reveal that even advanced algorithms like large language models (LLMs) encounter difficulties in addressing key challenges in certain tasks, such as paper source tracing and scholar profiling. We also introduce the Open Academic Graph Challenge (OAG-Challenge) to encourage community input and sharing. We envisage that OAG-Bench can serve as a common ground for the community to evaluate and compare algorithms in academic graph mining, thereby accelerating algorithm development and advancement in this field. OAG-Bench is accessible at https://www.aminer.cn/data/.},
  archive   = {C_KDD},
  author    = {Zhang, Fanjin and Shi, Shijie and Zhu, Yifan and Chen, Bo and Cen, Yukuo and Yu, Jifan and Chen, Yelin and Wang, Lulu and Zhao, Qingfei and Cheng, Yuqing and Han, Tianyi and An, Yuwei and Zhang, Dan and Tam, Weng Lam and Cao, Kun and Pang, Yunhe and Guan, Xinyu and Yuan, Huihui and Song, Jian and Li, Xiaoyan and Dong, Yuxiao and Tang, Jie},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672354},
  pages     = {6214–6225},
  title     = {OAG-bench: A human-curated benchmark for academic graph mining},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An offline meta black-box optimization framework for
adaptive design of urban traffic light management systems. <em>KDD</em>,
6202–6213. (<a href="https://doi.org/10.1145/3637528.3671606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Complex urban road networks with high vehicle occupancy frequently face severe traffic congestion. Designing an effective strategy for managing multiple traffic lights plays a crucial role in managing congestion. However, most current traffic light management systems rely on human-crafted decisions, which may not adapt well to diverse traffic patterns. In this paper, we delve into two pivotal design components of the traffic light management system that can be dynamically adjusted to various traffic conditions: phase combination and phase time allocation. While numerous studies have sought an efficient strategy for managing traffic lights, most of these approaches consider a fixed traffic pattern and are limited to relatively small road networks. To overcome these limitations, we introduce a novel and practical framework to formulate the optimization of such design components using an offline meta black-box optimization. We then present a simple yet effective method to efficiently find a solution for the aforementioned problem. In our framework, we first collect an offline meta dataset consisting of pairs of design choices and corresponding congestion measures from various traffic patterns. After collecting the dataset, we employ the Attentive Neural Process (ANP) to predict the impact of the proposed design on congestion across various traffic patterns with well-calibrated uncertainty. Finally, Bayesian optimization, with ANP as a surrogate model, is utilized to find an optimal design for unseen traffic patterns through limited online simulations. Our experiment results show that our method outperforms state-of-the-art baselines on complex road networks in terms of the number of waiting vehicles. Surprisingly, the deployment of our method into a real-world traffic system was able to improve traffic throughput by 4.80\% compared to the original strategy.},
  archive   = {C_KDD},
  author    = {Yun, Taeyoung and Lee, Kanghoon and Yun, Sujin and Kim, Ilmyung and Jung, Won-Woo and Kwon, Min-Cheol and Choi, Kyujin and Lee, Yoohyeon and Park, Jinkyoo},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671606},
  pages     = {6202–6213},
  title     = {An offline meta black-box optimization framework for adaptive design of urban traffic light management systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pre-trained KPI anomaly detection model through disentangled
transformer. <em>KDD</em>, 6190–6201. (<a
href="https://doi.org/10.1145/3637528.3671522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In large-scale online service systems, numerous Key Performance Indicators (KPIs), such as service response time and error rate, are gathered in a time-series format. KPI Anomaly Detection (KAD) is a critical data mining problem due to its widespread applications in real-world scenarios. However, KAD faces the challenges of dealing with KPI heterogeneity and noisy data. We propose KAD-Disformer, a KPI Anomaly Detection approach through Disentangled Transformer. KAD-Disformer pre-trains a model on existing accessible KPIs, and the pre-trained model can be effectively &quot;fine-tuned&quot; to unseen KPI using only a handful of samples from the unseen KPI. We propose a series of innovative designs, including disentangled projection for transformer, unsupervised few-shot fine-tuning (uTune), and denoising modules, each of which significantly contributes to the overall performance. Our extensive experiments demonstrate that KAD-Disformer surpasses the state-of-the-art universal anomaly detection model by 13\% in F1-score and achieves comparable performance using only 1/8 of the finetuning samples saving about 25 hours. KAD-Disformer has been successfully deployed in the real-world cloud system serving millions of users, attesting to its feasibility and robustness. Our code is available at https://github.com/NetManAIOps/KAD-Disformer.},
  archive   = {C_KDD},
  author    = {Yu, Zhaoyang and Pei, Changhua and Wang, Xin and Ma, Minghua and Bansal, Chetan and Rajmohan, Saravan and Lin, Qingwei and Zhang, Dongmei and Wen, Xidao and Li, Jianhui and Xie, Gaogang and Pei, Dan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671522},
  pages     = {6190–6201},
  title     = {Pre-trained KPI anomaly detection model through disentangled transformer},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised ranking ensemble model for recommendation.
<em>KDD</em>, 6181–6189. (<a
href="https://doi.org/10.1145/3637528.3671598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When visiting an online platform, a user generates various actions, such as clicks, long views, likes, comments, etc. To capture user preferences in these aspects, we learn these objectives and return multiple rankings of candidate items for each user. We need to aggregate them into one to truncate the candidate set, and ranking ensemble model is proposed for this task. However, there is a critical issue: though we input abundant information, what model learns depends on the supervision. Unfortunately, the existing supervision is poorly designed, leading to serious information loss issue.To address this issue, we designed an unsupervised loss to compel the ranking ensemble model to learn all information of input rankings, including sequential and numerical information. (1) For sequential information, we design a distance measure between two rankings, and train the ensemble ranking to have similar order with all input rankings by minimizing the distance. (2) For numerical information, we design a decoder to reconstruct values of original rankings from the hidden layer of the model, to guarantee that the model captures as much input information as possible. Our unsupervised loss is compatible with all ranking ensemble models. We optimize several widely-used structures to propose unsupervised ranking ensemble models.We devise comprehensive experiments on two real-world datasets to demonstrate the effectiveness of the proposed models. We also apply our model in a short video platform with billions of users, and achieve significant improvement.},
  archive   = {C_KDD},
  author    = {Yu, Wenhui and Liu, Bingqi and Xia, Bin and Xu, Xiaoxiao and Chen, Ying and Li, Yongchang and Hu, Lantao},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671598},
  pages     = {6181–6189},
  title     = {Unsupervised ranking ensemble model for recommendation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unified low-rank compression framework for click-through
rate prediction. <em>KDD</em>, 6169–6180. (<a
href="https://doi.org/10.1145/3637528.3671520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep Click-Through Rate (CTR) prediction models play an important role in modern industrial recommendation scenarios. However, high memory overhead and computational costs limit their deployment in resource-constrained environments. Low-rank approximation is an effective method for computer vision and natural language processing models, but its application in compressing CTR prediction models has been less explored. Due to the limited memory and computing resources, compression of CTR prediction models often confronts three fundamental challenges, i.e., (1). How to reduce the model sizes to adapt to edge devices? (2). How to speed up CTR prediction model inference? (3). How to retain the capabilities of original models after compression? Previous low-rank compression research mostly uses tensor decomposition, which can achieve a high parameter compression ratio, but brings in AUC degradation and additional computing overhead. To address these challenges, we propose a unified low-rank decomposition framework for compressing CTR prediction models. We find that even with the most classic matrix decomposition SVD method, our framework can achieve better performance than the original model. To further improve the effectiveness of our framework, we locally compress the output features instead of compressing the model weights. Our unified low-rank compression framework can be applied to embedding tables and MLP layers in various CTR prediction models. Extensive experiments on two academic datasets and one real industrial benchmark demonstrate that, with 3--5\texttimes{} model size reduction, our compressed models can achieve both faster inference and higher AUC than the uncompressed original models. Our code is at https://github.com/yuhao318/Atomic_Feature_Mimicking.},
  archive   = {C_KDD},
  author    = {Yu, Hao and Fu, Minghao and Ding, Jiandong and Zhou, Yusheng and Wu, Jianxin},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671520},
  pages     = {6169–6180},
  title     = {Unified low-rank compression framework for click-through rate prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SepsisLab: Early sepsis prediction with uncertainty
quantification and active sensing. <em>KDD</em>, 6158–6168. (<a
href="https://doi.org/10.1145/3637528.3671586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sepsis is the leading cause of in-hospital mortality in the USA. Early sepsis onset prediction and diagnosis could significantly improve the survival of sepsis patients. Existing predictive models are usually trained on high-quality data with few missing information, while missing values widely exist in real-world clinical scenarios (especially in the first hours of admissions to the hospital), which causes a significant decrease in accuracy and an increase in uncertainty for the predictive models. The common method to handle missing values is imputation, which replaces the unavailable variables with estimates from the observed data. The uncertainty of imputation results can be propagated to the sepsis prediction outputs, which have not been studied in existing works on either sepsis prediction or uncertainty quantification. In this study, we first define such propagated uncertainty as the variance of prediction output and then introduce uncertainty propagation methods to quantify the propagated uncertainty. Moreover, for the potential high-risk patients with low confidence due to limited observations, we propose a robust active sensing algorithm to increase confidence by actively recommending clinicians to observe the most informative variables. We validate the proposed models in both publicly available data (i.e., MIMIC-III and AmsterdamUMCdb) and proprietary data in The Ohio State University Wexner Medical Center (OSUWMC). The experimental results show that the propagated uncertainty is dominant at the beginning of admissions to hospitals and the proposed algorithm outperforms state-of-the-art active sensing methods. Finally, we implement a SepsisLab system for early sepsis prediction and active sensing based on our pre-trained models. Clinicians and potential sepsis patients can benefit from the system in early prediction and diagnosis of sepsis.},
  archive   = {C_KDD},
  author    = {Yin, Changchang and Chen, Pin-Yu and Yao, Bingsheng and Wang, Dakuo and Caterino, Jeffrey and Zhang, Ping},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671586},
  pages     = {6158–6168},
  title     = {SepsisLab: Early sepsis prediction with uncertainty quantification and active sensing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PAIL: Performance based adversarial imitation learning
engine for carbon neutral optimization. <em>KDD</em>, 6148–6157. (<a
href="https://doi.org/10.1145/3637528.3671611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Achieving carbon neutrality within industrial operations has become increasingly imperative for sustainable development. It is both a significant challenge and a key opportunity for operational optimization in industry 4.0. In recent years, Deep Reinforcement Learning (DRL) based methods offer promising enhancements for sequential optimization processes and can be used for reducing carbon emissions. However, existing DRL methods need a pre-defined reward function to assess the impact of each action on the final sustainable development goals (SDG). In many real applications, such a reward function cannot be given in advance. To address the problem, this study proposes a Performance based Adversarial Imitation Learning (PAIL) engine. It is a novel method to acquire optimal operational policies for carbon neutrality without any pre-defined action rewards. Specifically, PAIL employs a Transformer-based policy generator to encode historical information and predict following actions within a multi-dimensional space. The entire action sequence will be iteratively updated by an environmental simulator. Then PAIL uses a discriminator to minimize the discrepancy between generated sequences and real-world samples of high SDG. In parallel, a Q-learning framework based performance estimator is designed to estimate the impact of each action on SDG. Based on these estimations, PAIL refines generated policies with the rewards from both discriminator and performance estimator. PAIL is evaluated on multiple real-world application cases and datasets. The experiment results demonstrate the effectiveness of PAIL comparing to other state-of-the-art baselines. In addition, PAIL offers meaningful interpretability for the optimization in carbon neutrality.},
  archive   = {C_KDD},
  author    = {Ye, Yuyang and Tang, Lu-An and Wang, Haoyu and Yu, Runlong and Yu, Wenchao and He, Erhu and Chen, Haifeng and Xiong, Hui},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671611},
  pages     = {6148–6157},
  title     = {PAIL: Performance based adversarial imitation learning engine for carbon neutral optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OpenFedLLM: Training large language models on decentralized
private data via federated learning. <em>KDD</em>, 6137–6147. (<a
href="https://doi.org/10.1145/3637528.3671582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trained on massive publicly available data, large language models (LLMs) have demonstrated tremendous success across various fields. While more data contributes to better performance, a disconcerting reality is that high-quality public data will be exhausted in a few years. In this paper, we offer a potential next step for contemporary LLMs: collaborative and privacy-preserving LLM training on the underutilized distributed private data via federated learning (FL), where multiple data owners collaboratively train a shared model without transmitting raw data. To achieve this, we build a concise, integrated, and research-friendly framework/codebase, named OpenFedLLM. It covers federated instruction tuning for enhancing instruction-following capability, federated value alignment for aligning with human values, and 7 representative FL algorithms. Besides, OpenFedLLM supports training on diverse domains, where we cover 8 training datasets; and provides comprehensive evaluations, where we cover 30+ evaluation metrics. Through extensive experiments, we observe that all FL algorithms outperform local training on training LLMs, demonstrating a clear performance improvement across a variety of settings. Notably, in a financial benchmark, Llama2-7B fine-tuned by applying any FL algorithm can outperform GPT-4 by a significant margin, while the model obtained through individual training cannot, demonstrating strong motivation for clients to participate in FL. The code is available at https://github.com/rui-ye/OpenFedLLM. The full version of our paper is available at https://arxiv.org/pdf/2402.06954.},
  archive   = {C_KDD},
  author    = {Ye, Rui and Wang, Wenhao and Chai, Jingyi and Li, Dihan and Li, Zexi and Xu, Yinda and Du, Yaxin and Wang, Yanfeng and Chen, Siheng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671582},
  pages     = {6137–6147},
  title     = {OpenFedLLM: Training large language models on decentralized private data via federated learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing asymmetric web search through question-answer
generation and ranking. <em>KDD</em>, 6127–6136. (<a
href="https://doi.org/10.1145/3637528.3671517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the challenge of the semantic gap between user queries and web content, commonly referred to as asymmetric text matching, within the domain of web search. By leveraging BERT for reading comprehension, current algorithms enable significant advancements in query understanding, but still encounter limitations in effectively resolving the asymmetrical ranking problem due to model comprehension and summarization constraints.To tackle this issue, we propose the QAGR (Question-Answer Generation and Ranking) method, comprising an offline module called QAGeneration and an online module called QARanking. The QAGeneration module utilizes large language models (LLMs) to generate high-quality question-answering pairs for each web page. This process involves two steps: generating question-answer pairs and performing verification to eliminate irrelevant questions, resulting in high-quality questions associated with their respective documents. The QARanking module combines and ranks the generated questions and web page content. To ensure efficient online inference, we design the QARanking model as a homogeneous dual-tower model, incorporating query intent to drive score fusion while balancing keyword matching and asymmetric matching. Additionally, we conduct a preliminary screening of questions for each document, selecting only the top-N relevant questions for further relevance calculation.Empirical results demonstrate the substantial performance improvement of our proposed method in web search. We achieve over 8.7\% relative offline relevance improvement and over 8.5\% online engagement gain compared to the state-of-the-art web search system. Furthermore, we deploy QAGR to online web search engines and share our deployment experience, including production considerations and ablation experiments. This research contributes to advancing the field of asymmetric web search and provides valuable insights for enhancing search engine performance.},
  archive   = {C_KDD},
  author    = {Ye, Dezhi and Liu, Jie and Fan, Jiabin and Tian, Bowen and Zhou, Tianhua and Chen, Xiang and Ma, Jin},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671517},
  pages     = {6127–6136},
  title     = {Enhancing asymmetric web search through question-answer generation and ranking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Deep ensemble shape calibration: Multi-field post-hoc
calibration in online advertising. <em>KDD</em>, 6117–6126. (<a
href="https://doi.org/10.1145/3637528.3671529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the e-commerce advertising scenario, estimating the true probabilities (known as a calibrated estimate) on Click-Through Rate (CTR) and Conversion Rate (CVR) is critical. Previous research has introduced numerous solutions for addressing the calibration problem. These methods typically involve the training of calibrators using a validation set and subsequently applying these calibrators to correct the original estimated values during online inference.However, what sets e-commerce advertising scenarios is the challenge of multi-field calibration. Multi-field calibration requires achieving calibration in each field. In order to achieve multi-field calibration, it is necessary to have a strong data utilization ability. Because the quantity of pCTR specified range for single field-value (such as user ID and item ID) sample is relatively small, which makes the calibrator more difficult to train. However, existing methods have difficulty effectively addressing these issues.To solve these problems, we propose a new method named Deep Ensemble Shape Calibration (DESC). In terms of business understanding and interpretability, we decompose multi-field calibration into value calibration and shape calibration. We introduce innovative basis calibration functions, which enhance both function expression capabilities and data utilization by combining these basis calibration functions. A significant advancement lies in the development of an allocator capable of allocating the most suitable calibrators to different estimation error distributions within diverse fields and values. We achieve significant improvements in both public and industrial datasets. In online experiments, we observe a +2.5\% increase in CVR and +4.0\% in GMV (Gross Merchandise Volume). Our code is now available at: https://github.com/HaoYang0123/DESC.},
  archive   = {C_KDD},
  author    = {Yang, Shuai and Yang, Hao and Zou, Zhuang and Xu, Linhe and Yuan, Shuo and Zeng, Yifan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671529},
  pages     = {6117–6126},
  title     = {Deep ensemble shape calibration: Multi-field post-hoc calibration in online advertising},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FedGTP: Exploiting inter-client spatial dependency in
federated graph-based traffic prediction. <em>KDD</em>, 6105–6116. (<a
href="https://doi.org/10.1145/3637528.3671613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph-based methods have witnessed tremendous success in traffic prediction, largely attributed to their superior ability in capturing and modeling spatial dependencies. However, urban-scale traffic data are usually distributed among various owners, limited in sharing due to privacy restrictions. This fragmentation of data severely hinders interaction across clients, impeding the utilization of inter-client spatial dependencies. Existing studies have yet to address this non-trivial issue, thereby leading to sub-optimal performance. To fill this gap, we propose FedGTP, a new federated graph-based traffic prediction framework that promotes adaptive exploitation of inter-client spatial dependencies to recover close-to-optimal performance complying with privacy regulations like GDPR. We validate FedGTP via large-scale application-driven experiments on real-world datasets. Extensive baseline comparison, ablation study and case study demonstrate that FedGTP indeed surpasses existing methods through fully recovering inter-client spatial dependencies, achieving 21.08\%, 13.48\%, 19.90\% decrease on RMSE, MAE and MAPE, respectively. Our code is available at https://github.com/LarryHawkingYoung/KDD2024_FedGTP},
  archive   = {C_KDD},
  author    = {Yang, Linghua and Chen, Wantong and He, Xiaoxi and Wei, Shuyue and Xu, Yi and Zhou, Zimu and Tong, Yongxin},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671613},
  pages     = {6105–6116},
  title     = {FedGTP: Exploiting inter-client spatial dependency in federated graph-based traffic prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trinity: Syncretizing multi-/long-tail/long-term interests
all in one. <em>KDD</em>, 6095–6104. (<a
href="https://doi.org/10.1145/3637528.3671651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Interest modeling in recommender system has been a constant topic for improving user experience, and typical interest modeling tasks (e.g. multi-interest, long-tail interest and long-term interest) have been investigated in many existing works. However, most of them only consider one interest in isolation, while neglecting their interrelationships. In this paper, we argue that these tasks suffer from a common &quot;interest amnesia&quot; problem, and a solution exists to mitigate it simultaneously. We propose a novel and unified framework in the retrieval stage, &quot;Trinity&quot;, to solve interest amnesia problem and improve multiple interest modeling tasks. We construct a real-time clustering system that enables us to project items into enumerable clusters, and calculate statistical interest histograms over these clusters. Based on these histograms, Trinity recognizes underdelivered themes and remains stable when facing emerging hot topics. Its derived retrievers have been deployed on the recommender system of Douyin, significantly improving user experience and retention. We believe that such practical experience can be well generalized to other scenarios.},
  archive   = {C_KDD},
  author    = {Yan, Jing and Jiang, Liu and Cui, Jianfei and Zhao, Zhichen and Bin, Xingyan and Zhang, Feng and Liu, Zuotao},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671651},
  pages     = {6095–6104},
  title     = {Trinity: Syncretizing multi-/Long-Tail/Long-term interests all in one},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Face4Rag: Factual consistency evaluation for retrieval
augmented generation in chinese. <em>KDD</em>, 6083–6094. (<a
href="https://doi.org/10.1145/3637528.3671656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The prevailing issue of factual inconsistency errors in conventional Retrieval Augmented Generation (RAG) motivates the study of Factual Consistency Evaluation (FCE). Despite the various FCE methods proposed earlier, these methods are evaluated on datasets generated by specific Large Language Models (LLMs). Without a comprehensive benchmark, it remains unexplored how these FCE methods perform on other LLMs with different error distributions or even unseen error types, as these methods may fail to detect the error types generated by other LLMs. To fill this gap, in this paper, we propose the first comprehensive FCE benchmark Face4RAG for RAG independent of the underlying LLM. Our benchmark consists of a synthetic dataset built upon a carefully designed typology for factuality inconsistency error and a real-world dataset constructed from six commonly used LLMs, enabling evaluation of FCE methods on specific error types or real-world error distributions. On the proposed benchmark, we discover the failure of existing FCE methods to detect the logical fallacy, which refers to a mismatch of logic structures between the answer and the retrieved reference. To fix this issue, we further propose a new method called L-Face4RAG with two novel designs of logic-preserving answer decomposition and fact-logic FCE. Extensive experiments show L-Face4RAG substantially outperforms previous methods for factual inconsistency detection on a wide range of tasks, notably beyond the RAG task from which it is originally motivated. Both the benchmark and our proposed method are publicly available. https://huggingface.co/datasets/yq27/Face4RAG},
  archive   = {C_KDD},
  author    = {Xu, Yunqi and Cai, Tianchi and Jiang, Jiyan and Song, Xierui},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671656},
  pages     = {6083–6094},
  title     = {Face4Rag: Factual consistency evaluation for retrieval augmented generation in chinese},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). XRL-bench: A benchmark for evaluating and comparing
explainable reinforcement learning techniques. <em>KDD</em>, 6073–6082.
(<a href="https://doi.org/10.1145/3637528.3671595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement Learning (RL) has demonstrated substantial potential across diverse fields, yet understanding its decision-making process, especially in real-world scenarios where rationality and safety are paramount, is an ongoing challenge. This paper delves in to Explainable RL (XRL), a subfield of Explainable AI (XAI) aimed at unravelling the complexities of RL models. Our focus rests on state-explaining techniques, a crucial subset within XRL methods, as they reveal the underlying factors influencing an agent&#39;s actions at any given time. Despite their significant role, the lack of a unified evaluation framework hinders assessment of their accuracy and effectiveness. To address this, we introduce XRL-Bench, a unified standardized benchmark tailored for the evaluation and comparison of XRL methods, encompassing three main modules: standard RL environments, explainers based on state importance, and standard evaluators. XRL-Bench supports both tabular and image data for state explanation. We also propose TabularSHAP, an innovative and competitive XRL method. We demonstrate the practical utility of TabularSHAP in real-world online gaming services and offer an open-source benchmark platform for the straightforward implementation and evaluation of XRL methods. Our contributions facilitate the continued progression of XRL technology.},
  archive   = {C_KDD},
  author    = {Xiong, Yu and Hu, Zhipeng and Huang, Ye and Wu, Runze and Guan, Kai and Fang, XingChen and Jiang, Ji and Zhou, Tianze and Hu, YuJing and Liu, Haoyu and Lyu, Tangjie and Fan, Changjie},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671595},
  pages     = {6073–6082},
  title     = {XRL-bench: A benchmark for evaluating and comparing explainable reinforcement learning techniques},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Understanding the weakness of large language model agents
within a complex android environment. <em>KDD</em>, 6061–6072. (<a
href="https://doi.org/10.1145/3637528.3671650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large language models (LLMs) have empowered intelligent agents to execute intricate tasks within domain-specific software such as browsers and games. However, when applied to general-purpose software systems like operating systems, LLM agents face three primary challenges. Firstly, the action space is vast and dynamic, posing difficulties for LLM agents to maintain an up-to-date understanding and deliver accurate responses. Secondly, real-world tasks often require inter-application cooperation, demanding farsighted planning from LLM agents. Thirdly, agents need to identify optimal solutions aligning with user constraints, such as security concerns and preferences. These challenges motivate AndroidArena, an environment and benchmark designed to evaluate LLM agents on a modern operating system. To address high-cost of manpower, we design a scalable and semi-automated method to construct the benchmark. In the task evaluation, AndroidArena incorporates accurate and adaptive metrics to address the issue of non-unique solutions. Our findings reveal that even state-of-the-art LLM agents struggle in cross-APP scenarios and adhering to specific constraints. Additionally, we identify a lack of four key capabilities, i.e. understanding, reasoning, exploration, and reflection, as primary reasons for the failure of LLM agents. Furthermore, we provide empirical analysis on the failure of reflection, and improve the success rate by 27\% with our proposed exploration strategy. This work is the first to present valuable insights in understanding fine-grained weakness of LLM agents, and offers a path forward for future research in this area. Environment, benchmark, prompt, and evaluation code for AndroidArena are released at https://github.com/AndroidArenaAgent/AndroidArena.},
  archive   = {C_KDD},
  author    = {Xing, Mingzhe and Zhang, Rongkai and Xue, Hui and Chen, Qi and Yang, Fan and Xiao, Zhen},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671650},
  pages     = {6061–6072},
  title     = {Understanding the weakness of large language model agents within a complex android environment},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Microservice root cause analysis with limited observability
through intervention recognition in the latent space. <em>KDD</em>,
6049–6060. (<a href="https://doi.org/10.1145/3637528.3671530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many failure root cause analysis (RCA) algorithms for microservices have been proposed with the widespread adoption of microservices systems. Existing algorithms generally focus on RCA with ranking single-level (e.g. metric-level or service-level) root cause candidates (RCCs) with comprehensive monitoring metrics. However, many heterogeneous RCCs exist with limited observability in real-world microservices systems. Further, we find that the limited observability may result in inaccurate RCA through real-world failures in eBay. In this paper, for the first time, we propose to &quot;model RCCs as latent variables&quot;. The core idea is to infer the status of RCCs as latent variables with related monitoring metrics instead of directly extracting features from only the observable metrics. Based on this, we propose LatentScope, an unsupervised RCA framework with heterogeneous RCCs under limited observability. A dual-space graph is proposed to model both observable and unobservable variables, with many-to-many relationships between spaces. To achieve fast inference of latent variables and RCA, we propose the LatentRegressor algorithm, which includes Regression-based Latent-space Intervention Recognition (RLIR) to achieve intervention recognition-based RCA in latent space. LatentScope has been deployed in eBay&#39;s production environment and evaluated on both eBay&#39;s real-world failures and a testbed dataset. The evaluation results show that, compared with baseline algorithms, our model significantly improves the Top-1 recall by 9.7\%-57.9\%. The source code of LatentScope and the dataset are available at https://github.com/NetManAIOps/LatentScope.},
  archive   = {C_KDD},
  author    = {Xie, Zhe and Zhang, Shenglin and Geng, Yitong and Zhang, Yao and Ma, Minghua and Nie, Xiaohui and Yao, Zhenhe and Xu, Longlong and Sun, Yongqian and Li, Wentao and Pei, Dan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671530},
  pages     = {6049–6060},
  title     = {Microservice root cause analysis with limited observability through intervention recognition in the latent space},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weather knows what will occur: Urban public nuisance events
prediction and control with meteorological assistance. <em>KDD</em>,
6037–6048. (<a href="https://doi.org/10.1145/3637528.3671639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Urban public nuisance events, like garbage exposure, illegal parking, facilities damage, and etc., impair the quality of life for city residents. Predicting and controlling these nuisances is crucial but complicated due to their ties to subjective and psychological factors. In this study, we reveal a significant correlation between such nuisances and meteorological indicators, influenced by the impact of climate on people&#39;s psychological states. We employ meteorology predictions that are integrated in Hawkes processes to enhance the accuracy of predicting the category and timing of these nuisances. To this end, we propose Spatial-Temporal Two-Tower Transformer (ST-T3), which simultaneously considers spatial data and further improves the prediction accuracy. Evaluated by about three-year data from both downtown and suburban Shanghai, our method outperforms both traditional and advanced prediction systems. We share a portion of the de-identified dataset for open research.},
  archive   = {C_KDD},
  author    = {Xie, Yi and Qiu, Tianyu and Xiong, Yun and Huang, Xiuqi and Gao, Xiaofeng and Chen, Chao and Wang, Qiang and Li, Haihong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671639},
  pages     = {6037–6048},
  title     = {Weather knows what will occur: Urban public nuisance events prediction and control with meteorological assistance},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VecAug: Unveiling camouflaged frauds with cohort
augmentation for enhanced detection. <em>KDD</em>, 6025–6036. (<a
href="https://doi.org/10.1145/3637528.3671527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fraud detection presents a challenging task characterized by ever-evolving fraud patterns and scarce labeled data. Existing methods predominantly rely on graph-based or sequence-based approaches. While graph-based approaches connect users through shared entities to capture structural information, they remain vulnerable to fraudsters who can disrupt or manipulate these connections. In contrast, sequence-based approaches analyze users&#39; behavioral patterns, offering robustness against tampering but overlooking the interactions between similar users. Inspired by cohort analysis in retention and healthcare, this paper introduces VecAug, a novel cohort-augmented learning framework that addresses these challenges by enhancing the representation learning of target users with personalized cohort information. To this end, we first propose a vector burn-in technique for automatic cohort identification, which retrieves a task-specific cohort for each target user. Then, to fully exploit the cohort information, we introduce an attentive cohort aggregation technique for augmenting target user representations. To improve the robustness of such cohort augmentation, we also propose a novel label-aware cohort neighbor separation mechanism to distance negative cohort neighbors and calibrate the aggregated cohort information. By integrating this cohort information with target user representations, VecAug enhances the modeling capacity and generalization capabilities of the model to be augmented. Our framework is flexible and can be seamlessly integrated with existing fraud detection models. We deploy our framework on e-commerce platforms and evaluate it on three fraud detection datasets, and results show that VecAug improves the detection performance of base models by up to 2.48\% in AUC and 22.5\% in R@P0.9, outperforming state-of-the-art methods significantly.},
  archive   = {C_KDD},
  author    = {Xiao, Fei and Cai, Shaofeng and Chen, Gang and Jagadish, H. V. and Ooi, Beng Chin and Zhang, Meihui},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671527},
  pages     = {6025–6036},
  title     = {VecAug: Unveiling camouflaged frauds with cohort augmentation for enhanced detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DuMapNet: An end-to-end vectorization system for city-scale
lane-level map generation. <em>KDD</em>, 6015–6024. (<a
href="https://doi.org/10.1145/3637528.3671579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generating city-scale lane-level maps faces significant challenges due to the intricate urban environments, such as blurred or absent lane markings. Additionally, a standard lane-level map requires a comprehensive organization of lane groupings, encompassing lane direction, style, boundary, and topology, yet has not been thoroughly examined in prior research. These obstacles result in labor-intensive human annotation and high maintenance costs. This paper overcomes these limitations and presents an industrial-grade solution named DuMapNet that outputs standardized, vectorized map elements and their topology in an end-to-end paradigm. To this end, we propose a group-wise lane prediction (GLP) system that outputs vectorized results of lane groups by meticulously tailoring a transformer-based network. Meanwhile, to enhance generalization in challenging scenarios, such as road wear and occlusions, as well as to improve global consistency, a contextual prompts encoder (CPE) module is proposed, which leverages the predicted results of spatial neighborhoods as contextual information. Extensive experiments conducted on large-scale real-world datasets demonstrate the superiority and effectiveness of DuMapNet. Additionally, DuMapNet has already been deployed in production at Baidu Maps since June 2023, supporting lane-level map generation tasks for over 360 cities while bringing a 95\% reduction in costs. This demonstrates that DuMapNet serves as a practical and cost-effective industrial solution for city-scale lane-level map generation.},
  archive   = {C_KDD},
  author    = {Xia, Deguo and Zhang, Weiming and Liu, Xiyan and Zhang, Wei and Gong, Chenting and Huang, Jizhou and Yang, Mengmeng and Yang, Diange},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671579},
  pages     = {6015–6024},
  title     = {DuMapNet: An end-to-end vectorization system for city-scale lane-level map generation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Xinyu: An efficient LLM-based system for commentary
generation. <em>KDD</em>, 6003–6014. (<a
href="https://doi.org/10.1145/3637528.3671537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Commentary provides readers with a deep understanding of events by presenting diverse arguments and evidence. However, creating commentary is a time-consuming task, even for skilled commentators. Large language models (LLMs) have simplified the process of natural language generation, but their direct application in commentary creation still faces challenges due to unique task requirements. These requirements can be categorized into two levels: 1) fundamental requirements, which include creating well-structured and logically consistent narratives, and 2) advanced requirements, which involve generating quality arguments and providing convincing evidence. In this paper, we introduce Xinyu, an efficient LLM-based system designed to assist commentators in generating Chinese commentaries. To meet the fundamental requirements, we deconstruct the generation process into sequential steps, proposing targeted strategies and supervised fine-tuning (SFT) for each step. To address the advanced requirements, we present an argument ranking model for arguments and establish a comprehensive evidence database that includes up-to-date events and classic books, thereby strengthening the substantiation of the evidence with retrieval augmented generation (RAG) technology. To evaluate the generated commentaries more fairly, corresponding to the two-level requirements, we introduce a comprehensive evaluation metric that considers five distinct perspectives in commentary generation. Our experiments confirm the effectiveness of our proposed system. We also observe a significant increase in the efficiency of commentators in real-world scenarios, with the average time spent on creating a commentary dropping from 4 hours to 20 minutes. Importantly, such an increase in efficiency does not compromise the quality of the commentaries.},
  archive   = {C_KDD},
  author    = {Wu, Yiquan and Tang, Bo and Xi, Chenyang and Yu, Yu and Wang, Pengyu and Liu, Yifei and Kuang, Kun and Deng, Haiying and Li, Zhiyu and Xiong, Feiyu and Hu, Jie and Cheng, Peng and Wang, Zhonghao and Wang, Yi and Luo, Yi and Yang, Mingchuan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671537},
  pages     = {6003–6014},
  title     = {Xinyu: An efficient LLM-based system for commentary generation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LaDe: The first comprehensive last-mile express dataset from
industry. <em>KDD</em>, 5991–6002. (<a
href="https://doi.org/10.1145/3637528.3671548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-world last-mile express datasets are crucial for research in logistics, supply chain management, and spatio-temporal data mining. Despite a plethora of algorithms developed to date, no widely accepted, publicly available last-mile express dataset exists to support research in this field. In this paper, we introduce LaDe, the first publicly available last-mile express dataset with millions of packages from the industry. LaDe has three unique characteristics: (1)Large-scale. It involves 10,677k packages of 21k couriers over 6 months of real-world operation. (2)Comprehensive information. It offers original package information, task-event information, as well as couriers&#39; detailed trajecotries and road networks. (3)Diversity. The dataset includes data from various scenarios, including package pick-up and delivery, and from multiple cities, each with its unique spatio-temporal patterns due to their distinct characteristics such as populations. We verify LaDe on three tasks by running several classical baseline models per task. We believe that the large-scale, comprehensive, diverse feature of LaDe can offer unparalleled opportunities to researchers in the supply chain community, data mining community, and beyond. The dataset and code is publicly available at https://huggingface.co/datasets/Cainiao-AI/LaDe.},
  archive   = {C_KDD},
  author    = {Wu, Lixia and Wen, Haomin and Hu, Haoyuan and Mao, Xiaowei and Xia, Yutong and Shan, Ergang and Zheng, Jianbin and Lou, Junhong and Liang, Yuxuan and Yang, Liuqing and Zimmermann, Roger and Lin, Youfang and Wan, Huaiyu},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671548},
  pages     = {5991–6002},
  title     = {LaDe: The first comprehensive last-mile express dataset from industry},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TrajRecovery: An efficient vehicle trajectory recovery
framework based on urban-scale traffic camera records. <em>KDD</em>,
5979–5990. (<a href="https://doi.org/10.1145/3637528.3671558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate vehicle trajectory recovery enables providing indispensable data foundations in intelligent urban transportation. However, existing methods face two challenges: i) the inability to process city-wide vehicle trajectories, and ii) the dependence on a substantial amount of accurate GPS trajectories for model training, leading to poor generalization ability. To address these issues, we propose a novel trajectory recovery system based on vehicle snapshots captured by traffic cameras, named TrajRecovery. TrajRecovery consists of three main components: i) Preprocessor processes traffic cameras and vehicle snapshots to provide necessary data for trajectory recovery; ii) Spatial Transfer Probabilistic Model (STPM) integrates road conditions and driver behavior to compute turning probability at intersections; iii) Trajectory Generator utilizes the output probabilities from STPM to recover a continuous and most likely complete trajectory. We evaluate TrajRecovery on two real datasets from a city in China, demonstrating substantial performance gains compared to state-of-the-art methods. Furthermore, our system is deployed in practical applications at Huawei Company, achieving extraordinary profits in business scenarios.},
  archive   = {C_KDD},
  author    = {Wu, Dongen and Fang, Ziquan and Sun, Qichen and Chen, Lu and Hu, Haiyang and Wang, Fei and Gao, Yunjun},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671558},
  pages     = {5979–5990},
  title     = {TrajRecovery: An efficient vehicle trajectory recovery framework based on urban-scale traffic camera records},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nested fusion: A method for learning high resolution latent
structure of multi-scale measurement data on mars. <em>KDD</em>,
5969–5978. (<a href="https://doi.org/10.1145/3637528.3671596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Mars Perseverance Rover represents a generational change in the scale of measurements that can be taken on Mars, however this increased resolution introduces new challenges for techniques in exploratory data analysis. The multiple different instruments on the rover each measures specific properties of interest to scientists, so analyzing how underlying phenomena affect multiple different instruments together is important to understand the full picture. However each instrument has a unique resolution, making the mapping between overlapping layers of data non-trivial. In this work, we introduce Nested Fusion, a method to combine arbitrarily layered datasets of different resolutions and produce a latent distribution at the highest possible resolution, encoding complex interrelationships between different measurements and scales. Our method is efficient for large datasets, can perform inference even on unseen data, and outperforms existing methods of dimensionality reduction and latent analysis on real-world Mars rover data. We have deployed our method Nested Fusion within a Mars science team at NASA Jet Propulsion Laboratory (JPL) and through multiple rounds of participatory design enabled greatly enhanced exploratory analysis workflows for real scientists. To ensure the reproducibility of our work we have open sourced our code on GitHub at https://github.com/pixlise/NestedFusion.},
  archive   = {C_KDD},
  author    = {Wright, Austin P. and Davidoff, Scott and Chau, Duen Horng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671596},
  pages     = {5969–5978},
  title     = {Nested fusion: A method for learning high resolution latent structure of multi-scale measurement data on mars},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On finding bi-objective pareto-optimal fraud prevention rule
sets for fintech applications. <em>KDD</em>, 5959–5968. (<a
href="https://doi.org/10.1145/3637528.3671521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Rules are widely used in Fintech institutions to make fraud prevention decisions, since rules are highly interpretable thanks to their intuitive if-then structure. In practice, a two-stage framework of fraud prevention decision rule set mining is usually employed in large Fintech institutions; Stage 1 generates a potentially large pool of rules and Stage 2 aims to produce a refined rule subset according to some criteria (typically based on precision and recall). This paper focuses on improving the flexibility and efficacy of this two-stage framework, and is concerned with finding high-quality rule subsets in a bi-objective space (such as precision and recall). To this end, we first introduce a novel algorithm called SpectralRules that directly generates a compact pool of rules in Stage 1 with high diversity. We empirically find such diversity improves the quality of the final rule subset. In addition, we introduce an intermediate stage between Stage 1 and 2 that adopts the concept of Pareto optimality and aims to find a set of non-dominated rule subsets, which constitutes a Pareto front. This intermediate stage greatly simplifies the selection criteria and increases the flexibility of Stage 2. For this intermediate stage, we propose a heuristic-based framework called PORS and we identify that the core of PORS is the problem of solution selection on the front (SSF). We provide a systematic categorization of the SSF problem and a thorough empirical evaluation of various SSF methods on both public and proprietary datasets. On two real application scenarios within Alipay, we demonstrate the advantages of our proposed methodology over existing work.},
  archive   = {C_KDD},
  author    = {Wen, Chengyao and Lou, Yin},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671521},
  pages     = {5959–5968},
  title     = {On finding bi-objective pareto-optimal fraud prevention rule sets for fintech applications},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing pre-ranking performance: Tackling intermediary
challenges in multi-stage cascading recommendation systems.
<em>KDD</em>, 5950–5958. (<a
href="https://doi.org/10.1145/3637528.3671580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large-scale search engines and recommendation systems utilize a three-stage cascading architecture-recall, pre-ranking, and ranking-to deliver relevant results within stringent latency limits. The pre-ranking stage is crucial for filtering a large number of recalled items into a manageable set for the ranking stage, greatly affecting the system&#39;s performance. Pre-ranking faces two intermediary challenges: Sample Selection Bias (SSB) arises when training is based on ranking stage feedback but the evaluation is on a broader recall dataset. Also, compared to the ranking stage, simpler pre-rank models may perform worse and less consistently. Traditional methods to tackle SSB issues include using all recall results and treating unexposed portions as negatives for training, which can be costly and noisy. To boost performance and consistency, some pre-ranking feature interaction enhancers don&#39;t fully fix consistency issues, while methods like knowledge distillation in ranking models ignore exposure bias. Our proposed framework targets these issues with three integral modules: Sample Selection, Domain Adaptation, and Unbiased Distillation. Sample Selection filters recall results to mitigate SSB and compute costs. Domain Adaptation enhances model robustness by assigning pseudo-labels to unexposed samples. Unbiased Distillation uses exposure-independent scores from Domain Adaptation to implement unbiased distillation for the pre-ranking model. The framework focuses on optimizing pre-ranking while maintaining training efficiency. We introduce new metrics for pre-ranking evaluation, while experiments confirm the effectiveness of our framework. Our framework is also deployed in real industrial systems.},
  archive   = {C_KDD},
  author    = {Wei, Jianping and Zhou, Yujie and Wu, Zhengwei and Liu, Ziqi},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671580},
  pages     = {5950–5958},
  title     = {Enhancing pre-ranking performance: Tackling intermediary challenges in multi-stage cascading recommendation systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural optimization with adaptive heuristics for intelligent
marketing system. <em>KDD</em>, 5938–5949. (<a
href="https://doi.org/10.1145/3637528.3671591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Computational marketing has become increasingly important in today&#39;s digital world, facing challenges such as massive heterogeneous data, multi-channel customer journeys, and limited marketing budgets. In this paper, we propose a general framework for marketing AI systems, the Neural Optimization with Adaptive Heuristics (NOAH) framework. NOAH is the first general framework for marketing optimization that considers both to-business (2B) and to-consumer (2C) products, as well as both owned and paid channels. We describe key modules of the NOAH framework, including prediction, optimization, and adaptive heuristics, providing examples for bidding and content optimization. We then detail the successful application of NOAH to LinkedIn&#39;s email marketing system, showcasing significant wins over the legacy ranking system. Additionally, we share details and insights that are broadly useful, particularly on: (i) addressing delayed feedback with lifetime value, (ii) performing large-scale linear programming with randomization, (iii) improving retrieval with audience expansion, (iv) reducing signal dilution in targeting tests, and (v) handling zero-inflated heavy-tail metrics in statistical testing.},
  archive   = {C_KDD},
  author    = {Wei, Changshuai and Zelditch, Benjamin and Chen, Joyce and Ribeiro, Andre Assuncao Silva T and Tay, Jingyi Kenneth and Elizondo, Borja Ocejo and Selvaraj, Sathiya Keerthi and Gupta, Aman and Almeida, Licurgo Benemann De},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671591},
  pages     = {5938–5949},
  title     = {Neural optimization with adaptive heuristics for intelligent marketing system},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Know in AdVance: Linear-complexity forecasting of ad
campaign performance with evolving user interest. <em>KDD</em>,
5926–5937. (<a href="https://doi.org/10.1145/3637528.3671528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-time Bidding (RTB) advertisers wish to know in advance the expected cost and yield of ad campaigns to avoid trial-and-error expenses. However, Campaign Performance Forecasting (CPF), a sequence modeling task involving tens of thousands of ad auctions, poses challenges of evolving user interest, auction representation, and long context, making coarse-grained and static-modeling methods sub-optimal. We propose AdVance, a time-aware framework that integrates local auction-level and global campaign-level modeling. User preference and fatigue are disentangled using a time-positioned sequence of clicked items and a concise vector of all displayed items. Cross-attention, conditioned on the fatigue vector, captures the dynamics of user interest toward each candidate ad. Bidders compete with each other, presenting a complete graph similar to the self-attention mechanism. Hence, we employ a Transformer Encoder to compress each auction into embedding by solving auxiliary tasks. These sequential embeddings are then summarized by a conditional state space model (SSM) to comprehend long-range dependencies while maintaining global linear complexity. Considering the irregular time intervals between auctions, we make SSM&#39;s parameters dependent on the current auction embedding and the time interval. We further condition SSM&#39;s global predictions on the accumulation of local results. Extensive evaluations and ablation studies demonstrate its superiority over state-of-the-art methods. AdVance has been deployed on the Tencent Advertising platform, and A/B tests show a remarkable 4.5\% uplift in Average Revenue per User (ARPU).},
  archive   = {C_KDD},
  author    = {Wang, Xiaoyu and Guo, Yonghui and Sheng, Hui and Lv, Peili and Zhou, Chi and Huang, Wei and Ta, Shiqin and Huang, Dongbo and Yang, Xiujin and Xu, Lan and Zhou, Hao and Ji, Yusheng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671528},
  pages     = {5926–5937},
  title     = {Know in AdVance: Linear-complexity forecasting of ad campaign performance with evolving user interest},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mitigating pooling bias in e-commerce search via false
negative estimation. <em>KDD</em>, 5917–5925. (<a
href="https://doi.org/10.1145/3637528.3671630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Efficient and accurate product relevance assessment is critical for user experiences and business success. Training a proficient relevance assessment model requires high-quality query-product pairs, often obtained through negative sampling strategies. Unfortunately, current methods introduce pooling bias by mistakenly sampling false negatives, diminishing performance and business impact. To address this, we present Bias-mitigating Hard Negative Sampling (BHNS), a novel negative sampling strategy tailored to identify and adjust for false negatives, building upon our original False Negative Estimation algorithm. Our experiments in the Instacart search setting confirm BHNS as effective for practical e-commerce use. Furthermore, comparative analyses on public dataset showcase its domain-agnostic potential for diverse applications.},
  archive   = {C_KDD},
  author    = {Wang, Xiaochen and Xiao, Xiao and Zhang, Ruhan and Zhang, Xuan and Na, Taesik and Tenneti, Tejaswi and Wang, Haixun and Ma, Fenglong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671630},
  pages     = {5917–5925},
  title     = {Mitigating pooling bias in E-commerce search via false negative estimation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Future impact decomposition in request-level
recommendations. <em>KDD</em>, 5905–5916. (<a
href="https://doi.org/10.1145/3637528.3671506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recommender systems, reinforcement learning solutions have shown promising results in optimizing the interaction sequence between users and the system over the long-term performance. For practical reasons, the policy&#39;s actions are typically designed as recommending a list of items to handle users&#39; frequent and continuous browsing requests more efficiently. In this list-wise recommendation scenario, the user state is updated upon every request in the corresponding MDP formulation. However, this request-level formulation is essentially inconsistent with the user&#39;s item-level behavior. In this study, we demonstrate that an item-level optimization approach can better utilize item characteristics and optimize the policy&#39;s performance even under the request-level MDP. We support this claim by comparing the performance of standard request-level methods with the proposed item-level actor-critic framework in both simulation and online experiments. Furthermore, we show that a reward-based future decomposition strategy can better express the item-wise future impact and improve the recommendation accuracy in the long term. To achieve a more thorough understanding of the decomposition strategy, we propose a model-based re-weighting framework with adversarial learning that further boost the performance and investigate its correlation with the reward-based strategy.},
  archive   = {C_KDD},
  author    = {Wang, Xiaobei and Liu, Shuchang and Wang, Xueliang and Cai, Qingpeng and Hu, Lantao and Li, Han and Jiang, Peng and Gai, Kun and Xie, Guangming},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671506},
  pages     = {5905–5916},
  title     = {Future impact decomposition in request-level recommendations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). COMET: NFT price prediction with wallet profiling.
<em>KDD</em>, 5893–5904. (<a
href="https://doi.org/10.1145/3637528.3671621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As the non-fungible token (NFT) market flourishes, price prediction emerges as a pivotal direction for investors gaining valuable insight to maximize returns. However, existing works suffer from a lack of practical definitions and standardized evaluations, limiting their practical application. Moreover, the influence of users&#39; multi-behaviour transactions that are publicly accessible on NFT price is still not explored and exhibits challenges. In this paper, we address these gaps by presenting a practical and hierarchical problem definition. This approach unifies both collection-level and token-level task and evaluation methods, which cater to varied practical requirements of investors. To further understand the impact of user behaviours on the variation of NFT price, we propose a general wallet profiling framework and develop a COmmunity enhanced Multi-bEhavior Transaction graph model, named COMET. COMET profiles wallets with a comprehensive view and considers the impact of diverse relations and interactions within the NFT ecosystem on NFT price variations, thereby improving prediction performance. Extensive experiments conducted in our deployed system demonstrate the superiority of COMET, underscoring its potential in the insight toolkit for NFT investors.},
  archive   = {C_KDD},
  author    = {Wang, Tianfu and Deng, Liwei and Wang, Chao and Lian, Jianxun and Yan, Yue and Yuan, Nicholas Jing and Zhang, Qi and Xiong, Hui},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671621},
  pages     = {5893–5904},
  title     = {COMET: NFT price prediction with wallet profiling},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LiMAML: Personalization of deep recommender models via meta
learning. <em>KDD</em>, 5882–5892. (<a
href="https://doi.org/10.1145/3637528.3671599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the realm of recommender systems, the ubiquitous adoption of deep neural networks has emerged as a dominant paradigm for modeling diverse business objectives. As user bases continue to expand, the necessity of personalization and frequent model updates have assumed paramount significance to ensure the delivery of relevant and refreshed experiences to a diverse array of members. In this work, we introduce an innovative meta-learning solution tailored to the personalization of models for individual members and other entities, coupled with the frequent updates based on the latest user interaction signals. Specifically, we leverage the Model-Agnostic Meta Learning (MAML) algorithm to adapt per-task sub-networks using recent user interaction data. Given the near infeasibility of productionizing original MAML-based models in online recommendation systems, we propose an efficient strategy to operationalize meta-learned sub-networks in production, which involves transforming them into fixed-sized vectors, termed meta embeddings, thereby enabling the seamless deployment of models with hundreds of billions of parameters for online serving. Through extensive experimentation on production data drawn from various applications at LinkedIn, we demonstrate that the proposed solution consistently outperforms the best performing baseline models of those applications, including strong baselines such as using wide-and-deep ID based personalization approach. Our approach has enabled the deployment of a range of highly personalized AI models across diverse LinkedIn applications, leading to substantial improvements in business metrics as well as refreshed experience for our members.},
  archive   = {C_KDD},
  author    = {Wang, Ruofan and Prabhakar, Prakruthi and Srivastava, Gaurav and Wang, Tianqi and Jalali, Zeinab S. and Bharill, Varun and Ouyang, Yunbo and Nigam, Aastha and Venugopalan, Divya and Gupta, Aman and Borisyuk, Fedor and Keerthi, Sathiya and Muralidharan, Ajith},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671599},
  pages     = {5882–5892},
  title     = {LiMAML: Personalization of deep recommender models via meta learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). ADSNet: Cross-domain LTV prediction with an adaptive
siamese network in advertising. <em>KDD</em>, 5872–5881. (<a
href="https://doi.org/10.1145/3637528.3671612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Advertising platforms have evolved in estimating Lifetime Value (LTV) to better align with advertisers&#39; true performance metric which considers cumulative sum of purchases a customer contributes over a period. Accurate LTV estimation is crucial for the precision of the advertising system and the effectiveness of advertisements. However, the sparsity of real-world LTV data presents a significant challenge to LTV predictive model(i.e., pLTV), severely limiting the their capabilities. Therefore, we propose to utilize external data, in addition to the internal data of advertising platform, to expand the size of purchase samples and enhance the LTV prediction model of the advertising platform. To tackle the issue of data distribution shift between internal and external platforms, we introduce an Adaptive Difference Siamese Network (ADSNet), which employs cross-domain transfer learning to prevent negative transfer. Specifically, ADSNet is designed to learn information that is beneficial to the target domain. We introduce a gain evaluation strategy to calculate information gain, aiding the model in learning helpful information for the target domain and providing the ability to reject noisy samples, thus avoiding negative transfer. Additionally, we also design a Domain Adaptation Module as a bridge to connect different domains, reduce the distribution distance between them, and enhance the consistency of representation space distribution. We conduct extensive offline experiments and online A/B tests on a real advertising platform. Our proposed ADSNet method outperforms other methods, improving GINI by 2\%. The ablation study highlights the importance of the gain evaluation strategy in negative gain sample rejection and improving model performance. Additionally, ADSNet significantly improves long-tail prediction. The online A/B tests confirm ADSNet&#39;s efficacy, increasing online LTV by 3.47\% and GMV by 3.89\%.},
  archive   = {C_KDD},
  author    = {Wang, Ruize and Xu, Hui and Cheng, Ying and He, Qi and Zhou, Xing and Feng, Rui and Xu, Wei and Huang, Lei and Jiang, Jie},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671612},
  pages     = {5872–5881},
  title     = {ADSNet: Cross-domain LTV prediction with an adaptive siamese network in advertising},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Know your needs better: Towards structured understanding of
marketer demands with analogical reasoning augmented LLMs. <em>KDD</em>,
5860–5871. (<a href="https://doi.org/10.1145/3637528.3671583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we explore a new way for user targeting, where non-expert marketers could select their target users solely given demands in natural language form. The key to this issue is how to transform natural languages into practical structured logical languages, i.e., the structured understanding of marketer demands. In practical scenarios, the demands of non-expert marketers are often abstract and diverse. Considering the impressive natural language processing ability of large language models (LLMs), we try to leverage LLMs to solve this issue. To stimulate the LLMs&#39; reasoning ability, the chain-of-thought (CoT) prompting method is widely used, but existing methods still have some limitations in our scenario: (1) Previous methods either use simple &quot;Let&#39;s think step by step&quot; spells or provide fixed examples in demonstrations without considering compatibility between prompts and concrete questions, making LLMs ineffective when the marketers&#39; demands are abstract and diverse. (2) Previous methods are often implemented in closed-source models or excessively large models, which is not suitable in industrial practical scenarios. Based on these, we propose ARALLM (i.e., Analogical Reasoning Augmented Large Language Models) consisting of two modules: Analogical Reasoning based Prompting and Reasoning-Augmented Multi-Task Model Distillation. Then, we adopt a retrieval-based method to conduct analogical reasoning with the help of the reasoning library. The experimental results show that this prompting strategy achieves better performance than the ordinary prompting method. Beyond that, we distill knowledge from super LLMs (GPT-3.5) to fine-tune smaller student LLMs in a multi-task training paradigm, enabling the models to be easily deployed in practical environments. Part of our data and code can be found at https://github.com/alipay/Analogic-Reasoning-Augmented-Large-Language-Model.},
  archive   = {C_KDD},
  author    = {Wang, Junjie and Yang, Dan and Hu, Binbin and Shen, Yue and Zhang, Wen and Gu, Jinjie},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671583},
  pages     = {5860–5871},
  title     = {Know your needs better: Towards structured understanding of marketer demands with analogical reasoning augmented LLMs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). BacktrackSTL: Ultra-fast online seasonal-trend
decomposition with backtrack technique. <em>KDD</em>, 5848–5859. (<a
href="https://doi.org/10.1145/3637528.3671510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Seasonal-trend decomposition (STD) is a crucial task in time series data analysis. Due to the challenges of scalability, there is a pressing need for an ultra-fast online algorithm. However, existing algorithms either fail to handle long-period time series (such as OnlineSTL), or need time-consuming iterative processes (such as OneShotSTL). Therefore, we propose BacktrackSTL, the first non-iterative online STD algorithm with period-independent O(1) update complexity. It is also robust to outlier, seasonality shift and trend jump because of the combination of outlier-resilient smoothing, non-local seasonal filtering and backtrack technique. Experimentally, BacktrackSTL decomposes a value within 1.6 μs, which is 15X faster than the state-of-the-art online algorithm OneShotSTL, while maintaining comparable accuracy to the best offline algorithm RobustSTL. We have also deployed BacktrackSTL on the top of Apache Flink to decompose monitoring metrics in Alibaba Cloud for over a year. Besides, we have open-sourced the artifact of this proposal on GitHub.},
  archive   = {C_KDD},
  author    = {Wang, Haoyu and Guo, Hongke and Zhu, Zhaoliang and Zhang, You and Zhou, Yu and Zheng, Xudong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671510},
  pages     = {5848–5859},
  title     = {BacktrackSTL: Ultra-fast online seasonal-trend decomposition with backtrack technique},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TnT-LLM: Text mining at scale with large language models.
<em>KDD</em>, 5836–5847. (<a
href="https://doi.org/10.1145/3637528.3671647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Transforming unstructured text into structured and meaningful forms, organized by useful category labels, is a fundamental step in text mining for downstream analysis and application. However, most existing methods for producing label taxonomies and building text-based label classifiers still rely heavily on domain expertise and manual curation, making the process expensive and time-consuming. This is particularly challenging when the label space is under-specified and large-scale data annotations are unavailable. In this paper, we address these challenges with Large Language Models (LLMs), whose prompt-based interface facilitates the induction and use of large-scale pseudo labels. We propose TnT-LLM, a two-phase framework that employs LLMs to automate the process of end-to-end label generation and assignment with minimal human effort for any given use-case. In the first phase, we introduce a zero-shot, multi-stage reasoning approach which enables LLMs to produce and refine a label taxonomy iteratively. In the second phase, LLMs are used as data labelers that yield training samples so that lightweight supervised classifiers can be reliably built, deployed, and served at scale. We apply TnT-LLM to the analysis of user intent and conversational domain for Bing Copilot (formerly Bing Chat), an open-domain chat-based search engine. Extensive experiments using both human and automatic evaluation metrics demonstrate that TnT-LLM generates more accurate and relevant label taxonomies when compared against state-of-the-art baselines, and achieves a favorable balance between accuracy and efficiency for classification at scale.},
  archive   = {C_KDD},
  author    = {Wan, Mengting and Safavi, Tara and Jauhar, Sujay Kumar and Kim, Yujin and Counts, Scott and Neville, Jennifer and Suri, Siddharth and Shah, Chirag and White, Ryen W. and Yang, Longqi and Andersen, Reid and Buscher, Georg and Joshi, Dhruv and Rangan, Nagu},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671647},
  pages     = {5836–5847},
  title     = {TnT-LLM: Text mining at scale with large language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Chaining text-to-image and large language model: A novel
approach for generating personalized e-commerce banners. <em>KDD</em>,
5825–5835. (<a href="https://doi.org/10.1145/3637528.3671636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Text-to-image models such as stable diffusion have opened a plethora of opportunities for generating art. Recent literature has surveyed the use of text-to-image models for enhancing the work of many creative artists. Many e-commerce platforms employ a manual process to generate the banners, which is time-consuming and has limitations of scalability. In this work, we demonstrate the use of text-to-image models for generating personalized web banners with dynamic content for online shoppers based on their interactions. The novelty in this approach lies in converting users&#39; interaction data to meaningful prompts without human intervention. To this end, we utilize a large language model (LLM) to systematically extract a tuple of attributes from item meta-information. The attributes are then passed to a text-to-image model via prompt engineering to generate images for the banner. Our results show that the proposed approach can create high-quality personalized banners for users.},
  archive   = {C_KDD},
  author    = {Vashishtha, Shanu and Prakash, Abhinav and Morishetti, Lalitesh and Nag, Kaushiki and Arora, Yokila and Kumar, Sushant and Achan, Kannan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671636},
  pages     = {5825–5835},
  title     = {Chaining text-to-image and large language model: A novel approach for generating personalized e-commerce banners},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). R-eval: A unified toolkit for evaluating domain knowledge of
retrieval augmented large language models. <em>KDD</em>, 5813–5824. (<a
href="https://doi.org/10.1145/3637528.3671564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large language models have achieved remarkable success on general NLP tasks, but they may fall short for domain-specific problems. Recently, various Retrieval-Augmented Large Language Models (RALLMs) are proposed to address this shortcoming. However, existing evaluation tools only provide a few baselines and evaluate them on various domains without mining the depth of domain knowledge. In this paper, we address the challenges of evaluating RALLMs by introducing the R-Eval toolkit, a Python toolkit designed to streamline the evaluation of different RAG workflows in conjunction with LLMs. Our toolkit, which supports popular built-in RAG workflows and allows for the incorporation of customized testing data on the specific domain, is designed to be user-friendly, modular, and extensible. We conduct an evaluation of 21 RALLMs across three task levels and two representative domains, revealing significant variations in the effectiveness of RALLMs across different tasks and domains. Our analysis emphasizes the importance of considering both task and domain requirements when choosing a RAG workflow and LLM combination. We are committed to continuously maintaining our platform at https://github.com/THU-KEG/R-Eval to facilitate both the industry and the researchers.},
  archive   = {C_KDD},
  author    = {Tu, Shangqing and Wang, Yuanchun and Yu, Jifan and Xie, Yuyang and Shi, Yaran and Wang, Xiaozhi and Zhang, Jing and Hou, Lei and Li, Juanzi},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671564},
  pages     = {5813–5824},
  title     = {R-eval: A unified toolkit for evaluating domain knowledge of retrieval augmented large language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Choosing a proxy metric from past experiments. <em>KDD</em>,
5803–5812. (<a href="https://doi.org/10.1145/3637528.3671543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many randomized experiments, the treatment effect of the long-term metric (i.e. the primary outcome of interest) is often difficult or infeasible to measure. Such long-term metrics are often slow to react to changes and sufficiently noisy they are challenging to faithfully estimate in short-horizon experiments. A common alternative is to measure several short-term proxy metrics in the hope they closely track the long-term metric -- so they can be used to effectively guide decision-making in the near-term. We introduce a new statistical framework to both define and construct an optimal proxy metric for use in a homogeneous population of randomized experiments. Our procedure first reduces the construction of an optimal proxy metric in a given experiment to a portfolio optimization problem which depends on the true latent treatment effects and noise level of experiment under consideration. We then denoise the observed treatment effects of the long-term metric and a set of proxies in a historical corpus of randomized experiments to extract estimates of the latent treatment effects for use in the optimization problem. One key insight derived from our approach is that the optimal proxy metric for a given experiment is not apriori fixed; rather it should depend on the sample size (or effective noise level) of the randomized experiment for which it is deployed. To instantiate and evaluate our framework, we employ our methodology in a large corpus of randomized experiments from an industrial recommendation system and construct proxy metrics that perform favorably relative to several baselines.},
  archive   = {C_KDD},
  author    = {Tripuraneni, Nilesh and Richardson, Lee and D&#39;Amour, Alexander and Soriano, Jacopo and Yadlowsky, Steve},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671543},
  pages     = {5803–5812},
  title     = {Choosing a proxy metric from past experiments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Business policy experiments using fractional factorial
designs: Consumer retention on DoorDash. <em>KDD</em>, 5793–5802. (<a
href="https://doi.org/10.1145/3637528.3671574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper investigates an approach to both speed up business decision-making and lower the cost of learning through experimentation by factorizing business policies and employing fractional factorial experimental designs for their evaluation. We illustrate how this method integrates with advances in the estimation of heterogeneous treatment effects, elaborating on its advantages and foundational assumptions. We empirically demonstrate the implementation and benefits of our approach and assess its validity in evaluating consumer promotion policies at DoorDash, which is one of the largest delivery platforms in the US. Our approach discovers a policy with 5\% incremental profit at 67\% lower implementation cost.},
  archive   = {C_KDD},
  author    = {Tang, Yixin and Lin, Yicong and Sahni, Navdeep S.},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671574},
  pages     = {5793–5802},
  title     = {Business policy experiments using fractional factorial designs: Consumer retention on DoorDash},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-objective learning to rank by model distillation.
<em>KDD</em>, 5783–5792. (<a
href="https://doi.org/10.1145/3637528.3671597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In online marketplaces, search ranking&#39;s objective is not only to purchase or conversion (primary objective), but to also the purchase outcomes(secondary objectives), e.g. order cancellation(or return), review rating, customer service inquiries, platform long term growth. Multi-objective learning to rank has been widely studied to balance primary and secondary objectives. But traditional approaches in industry face some challenges including expensive parameter tuning leads to sub-optimal solution, suffering from imbalanced data sparsity issue, and being not compatible with ad-hoc objective. In this paper, we propose a distillation-based ranking solution for multi-objective ranking, which optimizes the end-to-end ranking system at Airbnb across multiple ranking models on different objectives along with various considerations to optimize training and serving efficiency to meet industry standards. We found it performs much better than traditional approaches, it doesn&#39;t only significantly increases primary objective by a large margin but also meet secondary objectives constraints and improve model stability. We also demonstrated the proposed system could be further simplified by model self-distillation. Besides this, we did additional simulations to show that this approach could also help us efficiently inject ad-hoc non-differentiable business objective into the ranking system while enabling us to balance our optimization objectives.},
  archive   = {C_KDD},
  author    = {Tang, Jie and Gao, Huiji and He, Liwei and Katariya, Sanjeev},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671597},
  pages     = {5783–5792},
  title     = {Multi-objective learning to rank by model distillation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Beimingwu: A learnware dock system. <em>KDD</em>, 5773–5782.
(<a href="https://doi.org/10.1145/3637528.3671617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The learnware paradigm proposed by Zhou (2016) aims to enable users to leverage numerous existing high-performing models instead of building machine learning models from scratch. This paradigm envisions that: Any developer worldwide can submit their well-trained models spontaneously into a learnware dock system (formerly known as learnware market). The system uniformly generates a specification for each model to form a learnware and accommodates it. As the key component, a specification should represent the capabilities of the model while preserving developer&#39;s original data. Based on the specifications, the learnware dock system can identify and assemble existing learnwares for users to solve new machine learning tasks. Recently, based on reduced kernel mean embedding (RKME) specification, a series of studies have shown the effectiveness of the learnware paradigm theoretically and empirically. However, the realization of a learnware dock system is still missing and remains a big challenge.This paper proposes Beimingwu, the first open-source learnware dock system, providing foundational support for future research. The system provides implementations and extensibility for the entire process of learnware paradigm, including the submitting, usability testing, organization, identification, deployment, and reuse of learnwares. Utilizing Beimingwu, the model development for new user tasks can be significantly streamlined, thanks to integrated architecture and engine design, specifying unified learnware structure and scalable APIs, and the integration of various algorithms for learnware identification and reuse. Notably, this is possible even for users with limited data and minimal expertise in machine learning, without compromising the raw data&#39;s security. The system facilitates the future research implementations in learnware-related algorithms and systems, and lays the ground for hosting a vast array of learnwares and establishing a learnware ecosystem. The system is fully open-source and we expect the research community to benefit from the system. The system and research toolkit have been released on GitLink and GitHub.},
  archive   = {C_KDD},
  author    = {Tan, Zhi-Hao and Liu, Jian-Dong and Bi, Xiao-Dong and Tan, Peng and Zheng, Qin-Cheng and Liu, Hai-Tian and Xie, Yi and Zou, Xiao-Chuan and Yu, Yang and Zhou, Zhi-Hua},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671617},
  pages     = {5773–5782},
  title     = {Beimingwu: A learnware dock system},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing personalized headline generation via offline
goal-conditioned reinforcement learning with large language models.
<em>KDD</em>, 5762–5772. (<a
href="https://doi.org/10.1145/3637528.3671638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, significant advancements have been made in Large Language Models (LLMs) through the implementation of various alignment techniques. These techniques enable LLMs to generate highly tailored content in response to diverse user instructions. Consequently, LLMs have the potential to serve as robust, customizable recommendation systems in the field of content recommendation. However, using LLMs with user individual information and online exploration remains a challenge, which are important perspectives in developing personalized news headline generation algorithms. In this paper, we propose a novel framework to generate personalized news headlines using LLMs with extensive online exploration. The proposed approach involves initially training an offline goal-conditioned policy using supervised learning. Subsequently, online exploration is employed to collect new data for the next training iteration. Results from simulations, experiments, and real-word scenario demonstrate that our framework achieves outstanding performance on established benchmarks and can effectively generate personalized headlines under different reward settings. By treating the LLM as a goal-conditioned agent, the model can perform online exploration by modifying the goals without frequently retraining the model. To the best of our knowledge, this work represents the first investigation into the capability of LLMs to generate customized news headlines with goal-conditioned reinforcement learning via supervised learning within LLMs.},
  archive   = {C_KDD},
  author    = {Tan, Xiaoyu and Cheng, Leijun and Qiu, Xihe and Shi, Shaojie and Cheng, Yuan and Chu, Wei and Xu, Yinghui and Qi, Yuan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671638},
  pages     = {5762–5772},
  title     = {Enhancing personalized headline generation via offline goal-conditioned reinforcement learning with large language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PEMBOT: Pareto-ensembled multi-task boosted trees.
<em>KDD</em>, 5752–5761. (<a
href="https://doi.org/10.1145/3637528.3671619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-task problems frequently arise in machine learning when there are multiple target variables, which share a common synergy while being sufficiently different that optimizing on any of the task does not necessarily imply an optimum for the others. In this work, we develop PEMBOT, a novel Pareto-based multi-task classification framework using a gradient boosted tree architecture. The proposed methodology involves a) generating multiple instances of Pareto optimal trees, b) diverse subset selection using a determinantal point process (DPP) model, and c) ensembling of diverse Pareto optimal trees to yield the final output. We tested our framework on a problem from an e-commerce domain wherein the task is to predict at order placement time the different adverse scenarios in the order shipment journey such as the package getting lost or damaged during shipment. This model enables us to take preemptive measures to prevent these scenarios from happening resulting in significant operational cost savings. Further, to show the generality of our approach, we demonstrate the performance of our algorithm on a publicly available wine quality prediction dataset and compare against state-of-the-art baselines.},
  archive   = {C_KDD},
  author    = {Swamy, Gokul and Saladi, Anoop and Das, Arunita and Niranjan, Shobhit},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671619},
  pages     = {5752–5761},
  title     = {PEMBOT: Pareto-ensembled multi-task boosted trees},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). MGMatch: Fast matchmaking with nonlinear objective and
constraints via multimodal deep graph learning. <em>KDD</em>, 5741–5751.
(<a href="https://doi.org/10.1145/3637528.3671553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As a core problem of online games, matchmaking is to assign players into multiple teams to maximize their gaming experience. With the rapid development of game industry, it is increasingly difficulty to explicitly model players&#39; experiences as linear functions. Instead, it is often modeled in a data-driven way by training a neural network. Meanwhile, complex rules must be satisfied to ensure the robustness of matchmaking, which are often described using logical operators. Therefore, matchmaking in practical scenarios is a challenging combinatorial optimization problem with nonlinear objective, linear constraints and logical constraints, which receives much less attention in previous research. In this paper, we propose a novel deep learning method for high-quality matchmaking in real-time. We first cast the problem as standard mixed-integer programming (MIP) by linearizing ReLU networks and logical constraints. Then, based on supervised learning, we design and train a multi-modal graph learning architecture to predict optimal solutions end-to-end from instance data, and solve a surrogate problem to efficiently obtain feasible solutions. Evaluation results on real industry datasets show that our method can deliver near-optimal solutions within 100ms.},
  archive   = {C_KDD},
  author    = {Sun, Yu and Wang, Kai and Hu, Zhipeng and Wu, Runze and Wu, Yaoxin and Song, Wen and Shen, Xudong and Lv, Tangjie and Fan, Changjie},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671553},
  pages     = {5741–5751},
  title     = {MGMatch: Fast matchmaking with nonlinear objective and constraints via multimodal deep graph learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spending programmed bidding: Privacy-friendly bid
optimization with ROI constraint in online advertising. <em>KDD</em>,
5731–5740. (<a href="https://doi.org/10.1145/3637528.3671540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Privacy policies have disrupted the multi-billion dollar online advertising market by making real-time and precise user data untraceable, which poses significant challenges to the optimization of Return-On-Investment (ROI) constrained products in the online advertising industry. Privacy protection strategies, including event aggregation and reporting delays, hinder access to detailed and instantaneous feedback data, thus incapacitating traditional identity-revealing attribution techniques. In this paper, we introduces a novel Spending Programmed Bidding (SPB) framework to navigate these challenges. SPB is a two-stage framework that separates long horizon delivery spend planning (the macro stage) and short horizon bidding execution (the micro stage). The macro stage models the target ROI to achieve maximum utility and derives the expected spend, whereas the micro stage optimizes the bid price given the expected spend. We further extend our framework to the cross-channel scenario where the agent bids in both privacy-constrained and identity-revealing attribution channels. We find that when privacy-constrained channels are present, SPB is superior to state-of-the-art bidding methods in both offline datasets and online experiments on a large ad platform.},
  archive   = {C_KDD},
  author    = {Su, Yumin and Xiang, Min and Chen, Yifei and Li, Yanbiao and Qin, Tian and Zhang, Hongyi and Li, Yasong and Liu, Xiaobing},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671540},
  pages     = {5731–5740},
  title     = {Spending programmed bidding: Privacy-friendly bid optimization with ROI constraint in online advertising},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-task neural linear bandit for exploration in
recommender systems. <em>KDD</em>, 5723–5730. (<a
href="https://doi.org/10.1145/3637528.3671649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Exposure bias and its induced feedback loop effect are well-known problems in recommender systems. Exploration is believed to be the key to break such feedback loops. While classical contextual bandit algorithms such as Upper-Confidence-Bound and Thompson Sampling have been successful in addressing the exploration-exploitation trade-off in the single-task settings with one clear reward signal, modern recommender systems often leverage multiple rich sources of feedback such as clicks, likes, dislikes, shares, satisfaction survey responses, and employ multi-task learning in practice. It is unclear how one can incorporate exploration in the multi-task setup with different objectives. In this paper, we study an efficient bandit algorithm tailored to multi-task recommender systems, named Multi-task Neural Linear Bandit (mtNLB). In particular, we investigate efficient feature embeddings in the multi-task setups that could be used as contextual features in the Neural Linear Bandit, a contextual bandit algorithm that nicely combines the representation power from DNN and simplicity in uncertainty calculation from linear models. We further study cost-effective approximations of the uncertainty estimate and principled ways to incorporate uncertainty into the multi-task scoring of items. To showcase the efficacy of our proposed method, we conduct live experiments on a large-scale commercial recommendation platform that serves billions of users. We evaluate the quality of the uncertainty estimate and demonstrate its ability to improve exploration across the different dimensions of the reward signals in comparison to baseline approaches.},
  archive   = {C_KDD},
  author    = {Su, Yi and Lu, Haokai and Li, Yuening and Liu, Liang and Bi, Shuchao and Chi, Ed H. and Chen, Minmin},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671649},
  pages     = {5723–5730},
  title     = {Multi-task neural linear bandit for exploration in recommender systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving ego-cluster for network effect measurement.
<em>KDD</em>, 5713–5722. (<a
href="https://doi.org/10.1145/3637528.3671557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The network effect, wherein one user&#39;s activity impacts another user, is common in social network platforms. Many new features in social networks are specifically designed to create a network effect, enhancing user engagement. For instance, content creators tend to produce more when their articles and posts receive positive feedback from followers. This paper discusses a new cluster-level experimentation methodology for measuring creator-side metrics in the context of A/B experiments. It is a crucial part of LinkedIn&#39;s overall strategy to foster a robust creator community and ecosystem. The method is developed based on widely-cited research at LinkedIn but significantly improves the efficiency and flexibility of the clustering algorithm. This improvement results in a stronger capability for measuring creator-side metrics and an increased velocity for creator-related experiments.},
  archive   = {C_KDD},
  author    = {Su, Wentao and Duan, Weitao},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671557},
  pages     = {5713–5722},
  title     = {Improving ego-cluster for network effect measurement},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From variability to stability: Advancing RecSys benchmarking
practices. <em>KDD</em>, 5701–5712. (<a
href="https://doi.org/10.1145/3637528.3671655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the rapidly evolving domain of Recommender Systems (RecSys), new algorithms frequently claim state-of-the-art performance based on evaluations over a limited set of arbitrarily selected datasets. However, this approach may fail to holistically reflect their effectiveness due to the significant impact of dataset characteristics on algorithm performance. Addressing this deficiency, this paper introduces a novel benchmarking methodology to facilitate a fair and robust comparison of RecSys algorithms, thereby advancing evaluation practices. By utilizing a diverse set of 30 open datasets, including two introduced in this work, and evaluating 11 collaborative filtering algorithms across 9 metrics, we critically examine the influence of dataset characteristics on algorithm performance. We further investigate the feasibility of aggregating outcomes from multiple datasets into a unified ranking. Through rigorous experimental analysis, we validate the reliability of our methodology under the variability of datasets, offering a benchmarking strategy that balances quality and computational demands. This methodology enables a fair yet effective means of evaluating RecSys algorithms, providing valuable guidance for future research endeavors.},
  archive   = {C_KDD},
  author    = {Shevchenko, Valeriy and Belousov, Nikita and Vasilev, Alexey and Zholobov, Vladimir and Sosedka, Artyom and Semenova, Natalia and Volodkevich, Anna and Savchenko, Andrey and Zaytsev, Alexey},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671655},
  pages     = {5701–5712},
  title     = {From variability to stability: Advancing RecSys benchmarking practices},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lumos: Empowering multimodal LLMs with scene text
recognition. <em>KDD</em>, 5690–5700. (<a
href="https://doi.org/10.1145/3637528.3671633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce Lumos, the first end-to-end multimodal question-answering system with text understanding capabilities. At the core of Lumos is a Scene Text Recognition (STR) component that extracts text from first person point-of-view images, the output of which is used to augment input to a Multimodal Large Language Model (MM-LLM). While building Lumos, we encountered numerous challenges related to STR quality, overall latency, and model inference. In this paper, we delve into those challenges, and discuss the system architecture, design choices, and modeling techniques employed to overcome these obstacles. We also provide a comprehensive evaluation for each component, showcasing high quality and efficiency.},
  archive   = {C_KDD},
  author    = {Shenoy, Ashish and Lu, Yichao and Jayakumar, Srihari and Chatterjee, Debojeet and Moslehpour, Mohsen and Chuang, Pierce and Harpale, Abhay and Bhardwaj, Vikas and Xu, Di and Zhao, Shicong and Zhao, Longfang and Ramchandani, Ankit and Dong, Xin Luna and Kumar, Anuj},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671633},
  pages     = {5690–5700},
  title     = {Lumos: Empowering multimodal LLMs with scene text recognition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Measuring an LLM’s proficiency at using APIs: A query
generation strategy. <em>KDD</em>, 5680–5689. (<a
href="https://doi.org/10.1145/3637528.3671592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Connecting Large Language Models (LLMs) with the ability to leverage APIs (Web Search, Charting, Calculators, Calendar, Flight Search, Hotel Search, Data Lookup, etc. ) is likely to allow us to solve a variety of new hard problems. Several research efforts have made this observation and suggested recipes for LLMs to emit API calls, and proposed mechanisms by which they can generate additional text conditioned on the output for the API call. However, in practice, the focus has been on relatively simple slot-filling tasks that make an API call rather unlocking novel capabilities by combining different tools, reasoning over the response from a tool, making multiple invocations, or complex planning. In this paper, we pose the following question: what does it mean to say that an LLM is proficient at using a set of APIs? We answer this question in the context of structured APIs by defining seven capabilities for API-use. We provide an approach for generating synthetic tasks that exercise each of these capabilities given only the description of an API. We argue that this provides practitioners with a principled way to construct a dataset to evaluate an LLM&#39;s ability to use a given set of APIs. Through human evaluations, we show that our approach produces high-quality tasks for each of the seven capabilities. We also describe how we used this approach to on-board new API and create principled evaluation sets for multiple LLM-based products.},
  archive   = {C_KDD},
  author    = {Sheng, Ying and Gandhe, Sudeep and Kanagal, Bhargav and Edmonds, Nick and Fisher, Zachary and Tata, Sandeep and Selvan, Aarush},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671592},
  pages     = {5680–5689},
  title     = {Measuring an LLM&#39;s proficiency at using APIs: A query generation strategy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing novelty of top-k recommendations using large
language models and reinforcement learning. <em>KDD</em>, 5669–5679. (<a
href="https://doi.org/10.1145/3637528.3671618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Given an input query, a recommendation model is trained using user feedback data (e.g., click data) to output a ranked list of items. In real-world systems, besides accuracy, an important consideration for a new model is novelty of its top-k recommendations w.r.t. an existing deployed model. However, novelty of top-k items is a difficult goal to optimize a model for, since it involves a non-differentiable sorting operation on the model&#39;s predictions. Moreover, novel items, by definition, do not have any user feedback data. Given the semantic capabilities of large language models, we address these problems using a reinforcement learning (RL) formulation where large language models provide feedback for the novel items. However, given millions of candidate items, the sample complexity of a standard RL algorithm can be prohibitively high. To reduce sample complexity, we reduce the top-k list reward to a set of item-wise rewards and reformulate the state space to consist of &amp;lt;query, item&amp;gt; tuples such that the action space is reduced to a binary decision; and show that this reformulation results in a significantly lower complexity when the number of items is large. We evaluate the proposed algorithm on improving novelty for a query-ad recommendation task on a large-scale search engine. Compared to supervised finetuning on recent &amp;lt;query, ad&amp;gt; pairs, the proposed RL-based algorithm leads to significant novelty gains with minimal loss in recall. We obtain similar results on the ORCAS query-webpage matching dataset and a product recommendation dataset based on Amazon reviews.},
  archive   = {C_KDD},
  author    = {Sharma, Amit and Li, Hua and Li, Xue and Jiao, Jian},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671618},
  pages     = {5669–5679},
  title     = {Optimizing novelty of top-k recommendations using large language models and reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical knowledge guided fault intensity diagnosis of
complex industrial systems. <em>KDD</em>, 5657–5668. (<a
href="https://doi.org/10.1145/3637528.3671610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fault intensity diagnosis (FID) plays a pivotal role in monitoring and maintaining mechanical devices within complex industrial systems. As current FID methods are based on chain of thought without considering dependencies among target classes. To capture and explore dependencies, we propose a &amp;lt;u&amp;gt;h&amp;lt;/u&amp;gt;ierarchical &amp;lt;u&amp;gt;k&amp;lt;/u&amp;gt;nowledge &amp;lt;u&amp;gt;g&amp;lt;/u&amp;gt;uided fault intensity diagnosis framework (HKG) inspired by the tree of thought, which is amenable to any representation learning methods. The HKG uses graph convolutional networks to map the hierarchical topological graph of class representations into a set of interdependent global hierarchical classifiers, where each node is denoted by word embeddings of a class. These global hierarchical classifiers are applied to learned deep features extracted by representation learning, allowing the entire model to be end-to-end learnable. In addition, we develop a re-weighted hierarchical knowledge correlation matrix (Re-HKCM) scheme by embedding inter-class hierarchical knowledge into a data-driven statistical correlation matrix (SCM) which effectively guides the information sharing of nodes in graphical convolutional neural networks and avoids over-smoothing issues. The Re-HKCM is derived from the SCM through a series of mathematical transformations. Extensive experiments are performed on four real-world datasets from different industrial domains (three cavitation datasets from SAMSON AG and one existing publicly) for FID, all showing superior results and outperform recent state-of-the-art FID methods.},
  archive   = {C_KDD},
  author    = {Sha, Yu and Gou, Shuiping and Liu, Bo and Faber, Johannes and Liu, Ningtao and Schramm, Stefan and Stoecker, Horst and Steckenreiter, Thomas and Vnucec, Domagoj and Wetzstein, Nadine and Widl, Andreas and Zhou, Kai},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671610},
  pages     = {5657–5668},
  title     = {Hierarchical knowledge guided fault intensity diagnosis of complex industrial systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tackling concept shift in text classification using
entailment-style modeling. <em>KDD</em>, 5647–5656. (<a
href="https://doi.org/10.1145/3637528.3671541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pre-trained language models (PLMs) have seen tremendous success in text classification (TC) problems in the context of Natural Language Processing (NLP). In many real-world text classification tasks, the class definitions being learned do not remain constant but rather change with time - this is known as concept shift. Most techniques for handling concept shift rely on retraining the old classifiers with the newly labelled data. However, given the amount of training data required to fine-tune large DL models for the new concepts, the associated labelling costs can be prohibitively expensive and time consuming. In this work, we propose a reformulation, converting vanilla classification into an entailment-style problem that requires significantly less data to re-train the text classifier to adapt to new concepts. We demonstrate the effectiveness of our proposed method on both real world \&amp;amp; synthetic datasets achieving absolute F1 gains upto ~6\% and ~30\% respectively in few-shot settings. Further, upon deployment, our solution also helped save 75\% direct labeling costs and 40\% downstream labeling costs overall in a span of 3 months.},
  archive   = {C_KDD},
  author    = {Roychowdhury, Sumegh and Gupta, Karan and Kasa, Siva Rajesh and Srinivasa Murthy, Prasanna},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671541},
  pages     = {5647–5656},
  title     = {Tackling concept shift in text classification using entailment-style modeling},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging exposure networks for detecting fake news
sources. <em>KDD</em>, 5635–5646. (<a
href="https://doi.org/10.1145/3637528.3671539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The scale and dynamic nature of the Web makes real-time detection of misinformation an extremely difficult task. Prior research mostly focused on offline (retrospective) detection of stories or claims using linguistic features of the content, flagging by users, and crowdsourced labels. Here, we develop a novel machine-learning methodology for detecting fake news sources using active learning, and examine the contribution of network, audience, and text features to the model accuracy. Importantly, we evaluate performance in both offline and online settings, mimicking the strategic choices fact-checkers have to make in practice as news sources emerge over time. We find that exposure networks provide information on considerably more sources than sharing networks (+49.6\%), and that the inclusion of exposure features greatly improves classification PR-AUC in both offline (+33\%) and online (+69.2\%) settings. Textual features perform best in offline settings, but their performance deteriorates by 12.0-18.7\% in online settings. Finally, the results show that a few iterations of active learning are sufficient for our model to attain predictive performance to comparable exhaustive labeling while incurring only 24.7\% of the labeling costs. These results stress the importance of exposure networks as a source of valuable information for the investigation of information dissemination in social networks and question the robustness of textual features.},
  archive   = {C_KDD},
  author    = {Reuben, Maor and Friedland, Lisa and Puzis, Rami and Grinberg, Nir},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671539},
  pages     = {5635–5646},
  title     = {Leveraging exposure networks for detecting fake news sources},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Non-autoregressive generative models for reranking
recommendation. <em>KDD</em>, 5625–5634. (<a
href="https://doi.org/10.1145/3637528.3671645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Contemporary recommendation systems are designed to meet users&#39; needs by delivering tailored lists of items that align with their specific demands or interests. In a multi-stage recommendation system, reranking plays a crucial role by modeling the intra-list correlations among items. The key challenge of reranking lies in the exploration of optimal sequences within the combinatorial space of permutations. Recent research proposes a generator-evaluator learning paradigm, where the generator generates multiple feasible sequences and the evaluator picks out the best sequence based on the estimated listwise score. The generator is of vital importance, and generative models are well-suited for the generator function. Current generative models employ an autoregressive strategy for sequence generation. However, deploying autoregressive models in real-time industrial systems is challenging. Firstly, the generator can only generate the target items one by one and hence suffers from slow inference. Secondly, the discrepancy between training and inference brings an error accumulation. Lastly, the left-to-right generation overlooks information from succeeding items, leading to suboptimal performance.To address these issues, we propose a Non-AutoRegressive generative model for reranking Recommendation (NAR4Rec) designed to enhance efficiency and effectiveness. To tackle challenges such as sparse training samples and dynamic candidates, we introduce a matching model. Considering the diverse nature of user feedback, we employ a sequence-level unlikelihood training objective to differentiate feasible sequences from unfeasible ones. Additionally, to overcome the lack of dependency modeling in non-autoregressive models regarding target items, we introduce contrastive decoding to capture correlations among these items. Extensive offline experiments validate the superior performance of NAR4Rec over state-of-the-art reranking methods. Online A/B tests reveal that NAR4Rec significantly enhances the user experience. Furthermore, NAR4Rec has been fully deployed in a popular video app Kuaishou with over 300 million daily active users.},
  archive   = {C_KDD},
  author    = {Ren, Yuxin and Yang, Qiya and Wu, Yichun and Xu, Wei and Wang, Yalong and Zhang, Zhiqiang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671645},
  pages     = {5625–5634},
  title     = {Non-autoregressive generative models for reranking recommendation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Class-incremental learning for time series: Benchmark and
evaluation. <em>KDD</em>, 5613–5624. (<a
href="https://doi.org/10.1145/3637528.3671581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-world environments are inherently non-stationary, frequently introducing new classes over time. This is especially common in time series classification, such as the emergence of new disease classification in healthcare or the addition of new activities in human activity recognition. In such cases, a learning system is required to assimilate novel classes effectively while avoiding catastrophic forgetting of the old ones, which gives rise to the Class-incremental Learning (CIL) problem. However, despite the encouraging progress in the image and language domains, CIL for time series data remains relatively understudied. Existing studies suffer from inconsistent experimental designs, necessitating a comprehensive evaluation and benchmarking of methods across a wide range of datasets. To this end, we first present an overview of the Time Series Class-incremental Learning (TSCIL) problem, highlight its unique challenges, and cover the advanced methodologies. Further, based on standardized settings, we develop a unified experimental framework that supports the rapid development of new algorithms, easy integration of new datasets, and standardization of the evaluation process. Using this framework, we conduct a comprehensive evaluation of various generic and time-series-specific CIL methods in both standard and privacy-sensitive scenarios. Our extensive experiments not only provide a standard baseline to support future research but also shed light on the impact of various design factors such as normalization layers or memory budget thresholds. Codes are available at https://github.com/zqiao11/TSCIL.},
  archive   = {C_KDD},
  author    = {Qiao, Zhongzheng and Pham, Quang and Cao, Zhen and Le, Hoang H. and Suganthan, P. N. and Jiang, Xudong and Ramasamy, Savitha},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671581},
  pages     = {5613–5624},
  title     = {Class-incremental learning for time series: Benchmark and evaluation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Addressing shortcomings in fair graph learning datasets:
Towards a new benchmark. <em>KDD</em>, 5602–5612. (<a
href="https://doi.org/10.1145/3637528.3671616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fair graph learning plays a pivotal role in numerous practical applications. Recently, many fair graph learning methods have been proposed; however, their evaluation often relies on poorly constructed semi-synthetic datasets or substandard real-world datasets. In such cases, even a basic Multilayer Perceptron (MLP) can outperform Graph Neural Networks (GNNs) in both utility and fairness. In this work, we illustrate that many datasets fail to provide meaningful information in the edges, which may challenge the necessity of using graph structures in these problems. To address these issues, we develop and introduce a collection of synthetic, semi-synthetic, and real-world datasets that fulfill a broad spectrum of requirements. These datasets are thoughtfully designed to include relevant graph structures and bias information crucial for the fair evaluation of models. The proposed synthetic and semi-synthetic datasets offer the flexibility to create data with controllable bias parameters, thereby enabling the generation of desired datasets with user-defined bias values with ease. Moreover, we conduct systematic evaluations of these proposed datasets and establish a unified evaluation approach for fair graph learning models. Our extensive experimental results with fair graph learning methods across our datasets demonstrate their effectiveness in benchmarking the performance of these methods. Our datasets and the code for reproducing our experiments are available at https://github.com/XweiQ/Benchmark-GraphFairness.},
  archive   = {C_KDD},
  author    = {Qian, Xiaowei and Guo, Zhimeng and Li, Jialiang and Mao, Haitao and Li, Bingheng and Wang, Suhang and Ma, Yao},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671616},
  pages     = {5602–5612},
  title     = {Addressing shortcomings in fair graph learning datasets: Towards a new benchmark},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatio-temporal consistency enhanced differential network
for interpretable indoor temperature prediction. <em>KDD</em>,
5590–5601. (<a href="https://doi.org/10.1145/3637528.3671608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Indoor temperature prediction is crucial for decision-making in central heating systems. Beyond accuracy, predictions shall be interpretable, i.e. conform to the laws of physics; otherwise, it may lead to system failures or unsafe conditions. However, deep learning models often face criticism regarding interpretability, which limits their application in such settings. To this end, we propose a Spatio-Temporal Consistency enhanced Differential Network (CONST) for interpretable indoor temperature prediction. Our approach mainly consists of a differential predictive module and a spatio-temporal consistency module. Modeling the influential factors, the first module solves the issue of multicollinearity through the differential operation. Considering the heterogeneity of global and local data distributions, the second module characterizes the temporal and spatial consistency to mine the universal pattern by multi-task learning, thereby improving the prediction interpretability. Besides, we propose a set of interpretability metrics to overcome the drawbacks of partial dependence plot metric, which are more practical, zero-centered, flexible, and numerical. We conclude experiments on a real-world dataset with four heating stations. The results demonstrate the advantages of our approach over various baselines, where the interpretability can be improved by more than 8 times on cRPD while maintaining high accuracy. We developed CONST on the SmartHeat system, providing hourly indoor temperature forecasts for 13 heating stations in northern China.},
  archive   = {C_KDD},
  author    = {Qi, Dekang and Yi, Xiuwen and Guo, Chengjie and Huang, Yanyong and Zhang, Junbo and Li, Tianrui and Zheng, Yu},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671608},
  pages     = {5590–5601},
  title     = {Spatio-temporal consistency enhanced differential network for interpretable indoor temperature prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Detecting abnormal operations in concentrated solar power
plants from irregular sequences of thermal images. <em>KDD</em>,
5578–5589. (<a href="https://doi.org/10.1145/3637528.3671623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Concentrated Solar Power (CSP) plants store energy by heating a storage medium with an array of mirrors that focus sunlight onto solar receivers atop a central tower. Operating at high temperatures these receivers face risks such as freezing, deformation, and corrosion, leading to operational failures, downtime, or costly equipment damage. We study the problem of anomaly detection (AD) in sequences of thermal images collected over a year from an operational CSP plant. These images are captured at irregular intervals ranging from one to five minutes throughout the day by infrared cameras mounted on solar receivers. Our goal is to develop a method to extract useful representations from high-dimensional thermal images for AD. It should be able to handle temporal features of the data, which include irregularity, temporal dependency between images and non-stationarity due to a strong daily seasonal pattern. The co-occurrence of low-temperature anomalies that resemble normal images from the start and the end of the operational cycle with high-temperature anomalies poses an additional challenge. We first evaluate state-of-the-art deep image-based AD methods, which have been shown to be effective in deriving meaningful image representations for the detection of anomalies. Then, we introduce a forecasting-based AD method that predicts future thermal images from past sequences and timestamps via a deep sequence model. This method effectively captures specific temporal data features and distinguishes between difficult-to-detect temperature-based anomalies. Our experiments demonstrate the effectiveness of our approach compared to multiple SOTA baselines across multiple evaluation metrics. We have also successfully deployed our solution on five months of unseen data, providing critical insights to our industry partner for the maintenance of the CSP plant. Our code is publicly accessible from https://github.com/sukanyapatra1997/ForecastAD. Additionally, as our dataset is confidential, we release a simulated dataset at https://tinyurl.com/kdd2024Dataset.},
  archive   = {C_KDD},
  author    = {Patra, Sukanya and Sournac, Nicolas and Taieb, Souhaib Ben},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671623},
  pages     = {5578–5589},
  title     = {Detecting abnormal operations in concentrated solar power plants from irregular sequences of thermal images},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ads recommendation in a collapsed and entangled world.
<em>KDD</em>, 5566–5577. (<a
href="https://doi.org/10.1145/3637528.3671607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present Tencent&#39;s ads recommendation system and examine the challenges and practices of learning appropriate recommendation representations. Our study begins by showcasing our approaches to preserving prior knowledge when encoding features of diverse types into embedding representations. We specifically address sequence features, numeric features, and pre-trained embedding features. Subsequently, we delve into two crucial challenges related to feature representation: the dimensional collapse of embeddings and the interest entanglement across different tasks or scenarios. We propose several practical approaches to address these challenges that result in robust and disentangled recommendation representations. We then explore several training techniques to facilitate model optimization, reduce bias, and enhance exploration. Additionally, we introduce three analysis tools that enable us to study feature correlation, dimensional collapse, and interest entanglement. This work builds upon the continuous efforts of Tencent&#39;s ads recommendation team over the past decade. It summarizes general design principles and presents a series of readily applicable solutions and analysis tools. The reported performance is based on our online advertising platform, which handles hundreds of billions of requests daily and serves millions of ads to billions of users.},
  archive   = {C_KDD},
  author    = {Pan, Junwei and Xue, Wei and Wang, Ximei and Yu, Haibin and Liu, Xun and Quan, Shijie and Qiu, Xueming and Liu, Dapeng and Xiao, Lei and Jiang, Jie},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671607},
  pages     = {5566–5577},
  title     = {Ads recommendation in a collapsed and entangled world},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Valuing an engagement surface using a large scale dynamic
causal model. <em>KDD</em>, 5556–5565. (<a
href="https://doi.org/10.1145/3637528.3671604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With recent rapid growth in online shopping, AI-powered Engagement Surfaces (ES) have become ubiquitous across retail services. These engagement surfaces perform an increasing range of functions, including recommending new products for purchase, reminding customers of their orders and providing delivery notifications. Understanding the causal effect of engagement surfaces on value driven for customers and businesses remains an open scientific question. In this paper, we develop a dynamic causal model at scale to disentangle value attributable to an ES, and to assess its effectiveness. We demonstrate the application of this model to inform business decision-making by understanding returns on investment in the ES, and identifying product lines and features where the ES adds the most value.},
  archive   = {C_KDD},
  author    = {Mukerji, Abhimanyu and More, Sushant and Kannan, Ashwin Viswanathan and Ravi, Lakshmi and Chen, Hua and Kohli, Naman and Khawand, Chris and Mandalapu, Dinesh},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671604},
  pages     = {5556–5565},
  title     = {Valuing an engagement surface using a large scale dynamic causal model},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EEG2Rep: Enhancing self-supervised EEG representation
through informative masked inputs. <em>KDD</em>, 5544–5555. (<a
href="https://doi.org/10.1145/3637528.3671600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-supervised approaches for electroencephalography (EEG) representation learning face three specific challenges inherent to EEG data: (1) The low signal-to-noise ratio which challenges the quality of the representation learned, (2) The wide range of amplitudes from very small to relatively large due to factors such as the inter-subject variability, risks the models to be dominated by higher amplitude ranges, and (3) The absence of explicit segmentation in the continuous-valued sequences which can result in less informative representations. To address these challenges, we introduceEEG2Rep, a self-prediction approach for self-supervised representation learning from EEG. Two core novel components of EEG2Rep are as follows: 1) Instead of learning to predict the masked input from raw EEG, EEG2Rep learns to predict masked input in latent representation space, and 2) Instead of conventional masking methods, EEG2Rep uses a new semantic subsequence preserving (SSP) method which provides informative masked inputs to guide EEG2Rep to generate rich semantic representations. In experiments on 6 diverse EEG tasks with subject variability, EEG2Rep significantly outperforms state-of-the-art methods. We show that our semantic subsequence preserving improves the existing masking methods in self-prediction literature and find that preserving 50\% of EEG recordings will result in the most accurate results on all 6 tasks on average. Finally, we show that EEG2Rep is robust to noise addressing a significant challenge that exists in EEG data. Models and code are available at:https://github.com/Navidfoumani/EEG2Rep},
  archive   = {C_KDD},
  author    = {Mohammadi Foumani, Navid and Mackellar, Geoffrey and Ghane, Soheila and Irtza, Saad and Nguyen, Nam and Salehi, Mahsa},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671600},
  pages     = {5544–5555},
  title     = {EEG2Rep: Enhancing self-supervised EEG representation through informative masked inputs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FusionSF: Fuse heterogeneous modalities in a vector
quantized framework for robust solar power forecasting. <em>KDD</em>,
5532–5543. (<a href="https://doi.org/10.1145/3637528.3671509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate solar power forecasting is crucial to integrate photovoltaic plants into the electric grid, schedule and secure the power grid safety. This problem becomes more demanding for those newly installed solar plants which lack sufficient operational data. Current research predominantly relies on historical solar power data or numerical weather prediction in a single-modality format, ignoring the complementary information provided in different modalities. In this paper, we propose a multi-modality fusion framework to integrate historical power data, numerical weather prediction, and satellite images, significantly improving forecast performance. We introduce a vector quantized framework that aligns modalities with varying information densities, striking a balance between integrating sufficient information and averting model overfitting. Our framework demonstrates strong zero-shot forecasting capability, which is especially useful for those newly installed plants. Moreover, we collect and release a multi-modal solar power (MMSP) dataset from real-world plants to further promote the research of multi-modal solar forecasting algorithms. Our extensive experiments show that our model not only operates with robustness but also boosts accuracy in both zero-shot forecasting and scenarios rich with training data, surpassing leading models. We have incorporated it into our eForecaster platform and deployed it for more than 300 solar plants with a total capacity of over 15GW. Our code and dataset are accessible at https://github.com/DAMO-DI-ML/FusionSF.git.},
  archive   = {C_KDD},
  author    = {Ma, Ziqing and Wang, Wenwei and Zhou, Tian and Chen, Chao and Peng, Bingqing and Sun, Liang and Jin, Rong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671509},
  pages     = {5532–5543},
  title     = {FusionSF: Fuse heterogeneous modalities in a vector quantized framework for robust solar power forecasting},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrating system state into spatio temporal graph neural
network for microservice workload prediction. <em>KDD</em>, 5521–5531.
(<a href="https://doi.org/10.1145/3637528.3671508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Microservice architecture has become a driving force in enhancing the modularity and scalability of web applications, as evidenced by the Alipay platform&#39;s operational success. However, a prevalent issue within such infrastructures is the suboptimal utilization of CPU resources due to inflexible resource allocation policies. This inefficiency necessitates the development of dynamic, accurate workload prediction methods to improve resource allocation. In response to this challenge, we present STAMP, a &amp;lt;u&amp;gt;S&amp;lt;/u&amp;gt;patio &amp;lt;u&amp;gt;T&amp;lt;/u&amp;gt;emporal Gr&amp;lt;u&amp;gt;a&amp;lt;/u&amp;gt;ph Network for &amp;lt;u&amp;gt;M&amp;lt;/u&amp;gt;icroservice Workload &amp;lt;u&amp;gt;P&amp;lt;/u&amp;gt;rediction. STAMP is designed to comprehensively address the multifaceted interdependencies between microservices, the temporal variability of workloads, and the critical role of system state in resource utilization. Through a graph-based representation, STAMP effectively maps the intricate network of microservice interactions. It employs time series analysis to capture the dynamic nature of workload changes and integrates system state insights to enhance prediction accuracy. Our empirical analysis, using three distinct real-world datasets, establishes that STAMP exceeds baselines by achieving an average boost of 5.72\% in prediction precision, as measured by RMSE. Upon deployment in Alipay&#39;s microservice environment, STAMP achieves a 33.10\% reduction in resource consumption, significantly outperforming existing online methods. This research solidifies STAMP as a validated framework, offering meaningful contributions to the field of resource management in microservice architecture-based applications.},
  archive   = {C_KDD},
  author    = {Luo, Yang and Gao, Mohan and Yu, Zhemeng and Ge, Haoyuan and Gao, Xiaofeng and Cai, Tengwei and Chen, Guihai},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671508},
  pages     = {5521–5531},
  title     = {Integrating system state into spatio temporal graph neural network for microservice workload prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MISP: A multimodal-based intelligent server failure
prediction model for cloud computing systems. <em>KDD</em>, 5509–5520.
(<a href="https://doi.org/10.1145/3637528.3671568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traditional server failure prediction methods predominantly rely on single-modality data such as system logs or system status curves. This reliance may lead to an incomplete understanding of system health and impending issues, proving inadequate for the complex and dynamic landscape of contemporary cloud computing environments. The potential of multimodal data to provide comprehensive insights is widely acknowledged, yet the lack of a holistic dataset and the challenges inherent in integrating features from both structured and unstructured data have impeded the exploration of multimodal-based server failure prediction. Addressing these challenges, this paper presents an industrial-scale, comprehensive dataset for server failure prediction, comprising nearly 80 types of structured and unstructured data sourced from real-world industrial cloud systems 1. Building on this resource, we introduce MISP, a model that leverages multimodal fusion techniques for server failure prediction. MISP transforms multimodal data into multi-dimensional sequences, extracts and encodes features both within and across the modalities, and ultimately computes the failure probability from the synthesized features. Experiments demonstrate that MISP significantly outperforms existing methods, enhancing prediction accuracy by approximately 25\% over previous state-of-the-art approaches.},
  archive   = {C_KDD},
  author    = {Lu, Xianting and Wang, Yunong and Fu, Yu and Sun, Qi and Ma, Xuhua and Zheng, Xudong and Zhuo, Cheng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671568},
  pages     = {5509–5520},
  title     = {MISP: A multimodal-based intelligent server failure prediction model for cloud computing systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling user retention through generative flow networks.
<em>KDD</em>, 5497–5508. (<a
href="https://doi.org/10.1145/3637528.3671531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recommender systems aim to fulfill the user&#39;s daily demands. While most existing research focuses on maximizing the user&#39;s engagement with the system, it has recently been pointed out that how frequently the users come back for the service also reflects the quality and stability of recommendations. However, optimizing this user retention behavior is non-trivial and poses several challenges including the intractable leave-and-return user activities, the sparse and delayed signal, and the uncertain relations between users&#39; retention and their immediate feedback towards each item in the recommendation list. In this work, we regard the retention signal as an overall estimation of the user&#39;s end-of-session satisfaction and propose to estimate this signal through a probabilistic flow. This flow-based modeling technique can back-propagate the retention reward towards each recommended item in the user session, and we show that the flow combined with traditional learning-to-rank objectives eventually optimizes a non-discounted cumulative reward for both immediate user feedback and user retention. We verify the effectiveness of our method through both offline empirical studies on two public datasets and online A/B tests in an industrial platform.},
  archive   = {C_KDD},
  author    = {Liu, Ziru and Liu, Shuchang and Yang, Bin and Xue, Zhenghai and Cai, Qingpeng and Zhao, Xiangyu and Zhang, Zijian and Hu, Lantao and Li, Han and Jiang, Peng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671531},
  pages     = {5497–5508},
  title     = {Modeling user retention through generative flow networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EmoLLMs: A series of emotional large language models and
annotation tools for comprehensive affective analysis. <em>KDD</em>,
5487–5496. (<a href="https://doi.org/10.1145/3637528.3671552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sentiment analysis and emotion detection are important research topics in natural language processing (NLP) and benefit many downstream tasks. With the widespread application of large language models (LLMs), researchers have started exploring the application of LLMs based on instruction-tuning in the field of sentiment analysis. However, these models only focus on single aspects of affective classification tasks (e.g. sentimental polarity or categorical emotions), and overlook the regression tasks (e.g. sentiment strength or emotion intensity), which leads to poor performance in downstream tasks. The main reason is the lack of comprehensive affective instruction tuning datasets and evaluation benchmarks, which cover various affective classification and regression tasks. Moreover, although emotional information is useful for downstream tasks, existing downstream datasets lack high-quality and comprehensive affective annotations. In this paper, we propose EmoLLMs, the first series of open-sourced instruction-following LLMs for comprehensive affective analysis based on fine-tuning various LLMs with instruction data, the first multi-task affective analysis instruction dataset (AAID) with 234K data samples based on 3 classification tasks and 2 regression tasks to support LLM instruction tuning, and a comprehensive affective evaluation benchmark (AEB) with 8 regression tasks and 6 classification tasks from various sources and domains to test the generalization ability of LLMs. We propose a series of EmoLLMs by fine-tuning LLMs with AAID to solve various affective instruction tasks. We compare our models with a variety of LLMs and sentiment analysis tools on AEB, where our models outperform all other open-sourced LLMs and sentiment analysis tools, and surpass ChatGPT and GPT-4 in most tasks, which shows that the series of EmoLLMs achieve the ChatGPT-level and GPT-4-level generalization capabilities on affective analysis tasks, and demonstrates our models can be used as affective annotation tools. This project is available at https://github.com/lzw108/EmoLLMs/.},
  archive   = {C_KDD},
  author    = {Liu, Zhiwei and Yang, Kailai and Xie, Qianqian and Zhang, Tianlin and Ananiadou, Sophia},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671552},
  pages     = {5487–5496},
  title     = {EmoLLMs: A series of emotional large language models and annotation tools for comprehensive affective analysis},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GRAM: Generative retrieval augmented matching of data
schemas in the context of data security. <em>KDD</em>, 5476–5486. (<a
href="https://doi.org/10.1145/3637528.3671602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Schema matching constitutes a pivotal phase in the data ingestion process for contemporary database systems. Its objective is to discern pairwise similarities between two sets of attributes, each associated with a distinct data table. This challenge emerges at the initial stages of data analytics, such as when incorporating a third-party table into existing databases to inform business insights. Given its significance in the realm of database systems, schema matching has been under investigation since the 2000s. This study revisits this foundational problem within the context of large language models. Adhering to increasingly stringent data security policies, our focus lies on the zero-shot and few-shot scenarios: the model should analyze only a minimal amount of customer data to execute the matching task, contrasting with the conventional approach of scrutinizing the entire data table. We emphasize that the zero-shot or few-shot assumption is imperative to safeguard the identity and privacy of customer data, even at the potential cost of accuracy. The capability to accurately match attributes under such stringent requirements distinguishes our work from previous literature in this domain.},
  archive   = {C_KDD},
  author    = {Liu, Xuanqing and Wang, Runhui and Song, Yang and Kong, Luyang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671602},
  pages     = {5476–5486},
  title     = {GRAM: Generative retrieval augmented matching of data schemas in the context of data security},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards automatic evaluation for LLMs’ clinical
capabilities: Metric, data, and algorithm. <em>KDD</em>, 5466–5475. (<a
href="https://doi.org/10.1145/3637528.3671575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large language models (LLMs) are gaining increasing interests to improve clinical efficiency, owing to their unprecedented performance in modelling natural language. Ensuring the reliable clinical applications, the evaluation of LLMs indeed becomes critical for better mitigating the potential risks, e.g., hallucinations. However, current evaluation methods heavily rely on labor-intensive human participation to achieve human-preferred judgements. To overcome this challenge, we propose an automatic evaluation paradigm tailored to assess the LLMs&#39; capabilities in delivering clinical services, e.g., disease diagnosis and treatment. The evaluation paradigm contains three basic elements: metric, data, and algorithm. Specifically, inspired by professional clinical practice pathways, we formulate a LLM-specific clinical pathway (LCP) to define the clinical capabilities that a doctor agent should possess. Then, Standardized Patients (SPs) from the medical education are introduced as the guideline for collecting medical data for evaluation, which can well ensure the completeness of the evaluation procedure. Leveraging these steps, we develop a multi-agent framework to simulate the interactive environment between SPs and a doctor agent, which is equipped with a Retrieval-Augmented Evaluation (RAE) to determine whether the behaviors of a doctor agent are in accordance with LCP. The above paradigm can be extended to any similar clinical scenarios to automatically evaluate the LLMs&#39; medical capabilities. Applying such paradigm, we construct an evaluation benchmark in the field of urology, including a LCP, a SPs dataset, and an automated RAE. Extensive experiments are conducted to demonstrate the effectiveness of the proposed approach, providing more insights for LLMs&#39; safe and reliable deployments in clinical practice.},
  archive   = {C_KDD},
  author    = {Liu, Lei and Yang, Xiaoyan and Li, Fangzhou and Chi, Chenfei and Shen, Yue and Lyu, Shiwei and Zhang, Ming and Ma, Xiaowei and Lv, Xiangguo and Ma, Liya and Zhang, Zhiqiang and Xue, Wei and Huang, Yiran and Gu, Jinjie},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671575},
  pages     = {5466–5475},
  title     = {Towards automatic evaluation for LLMs&#39; clinical capabilities: Metric, data, and algorithm},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DAG: Deep adaptive and generative k-free community detection
on attributed graphs. <em>KDD</em>, 5454–5465. (<a
href="https://doi.org/10.1145/3637528.3671615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Community detection on attributed graphs with rich semantic and topological information offers great potential for real-world network analysis, especially user matching in online games. Graph Neural Networks (GNNs) have recently enabled Deep Graph Clustering (DGC) methods to learn cluster assignments from semantic and topological information. However, their success depends on the prior knowledge related to the number of communities K, which is unrealistic due to the high costs and privacy issues of acquisition. In this paper, we investigate the community detection problem without prior K, referred to as K-Free Community Detection problem. To address this problem, we propose a novel Deep Adaptive and Generative model~(DAG) for community detection without specifying the prior K. DAG consists of three key components, i.e., a node representation learning module with masked attribute reconstruction, a community affiliation readout module, and a community number search module with group sparsity. These components enable DAG to convert the process of non-differentiable grid search for the community number, i.e., a discrete hyperparameter in existing DGC methods, into a differentiable learning process. In such a way, DAG can simultaneously perform community detection and community number search end-to-end. To alleviate the cost of acquiring community labels in real-world applications, we design a new metric, EDGE, to evaluate community detection methods even when the labels are not feasible. Extensive offline experiments on five public datasets and a real-world online mobile game dataset demonstrate the superiority of our DAG over the existing state-of-the-art (SOTA) methods. DAG has a relative increase of 7.35\% in teams in a Tencent online game compared with the best competitor.},
  archive   = {C_KDD},
  author    = {Liu, Chang and Yang, Yuwen and Ding, Yue and Lu, Hongtao and Lin, Wenqing and Wu, Ziming and Bi, Wendong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671615},
  pages     = {5454–5465},
  title     = {DAG: Deep adaptive and generative K-free community detection on attributed graphs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Beyond binary preference: Leveraging bayesian approaches
for joint optimization of ranking and calibration. <em>KDD</em>,
5442–5453. (<a href="https://doi.org/10.1145/3637528.3671577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Predicting click-through rate (CTR) is a critical task in recommendation systems, where the models are optimized with pointwise loss to infer the probability of items being clicked. In industrial practice, applications also require ranking items based on these probabilities. Existing solutions primarily combine the ranking-based loss, i.e., pairwise and listwise loss, with CTR prediction. However, they can hardly calibrate or generalize well in CTR scenarios where the clicks reflect the binary preference. This is because the binary click feedback leads to a large number of ties, which renders high data sparsity. In this paper, we propose an effective data augmentation strategy, named Beyond Binary Preference (BBP) training framework, to address this problem. Our key idea is to break the ties by leveraging Bayesian approaches, where the beta distribution models click behavior as probability distributions in the training data that naturally break ties. Therefore, we can obtain an auxiliary training label that generates more comparable pairs and improves the ranking performance. Besides, BBP formulates ranking and calibration as a multi-task framework to optimize both objectives simultaneously. Through extensive offline experiments and online tests on various datasets, we demonstrate that BBP significantly outperforms state-of-the-art methods in both ranking and calibration capabilities, showcasing its effectiveness in addressing the limitations of existing methods. Our code is available at https://github.com/AlvinIsonomia/BBP.},
  archive   = {C_KDD},
  author    = {Liu, Chang and Wang, Qiwei and Lin, Wenqing and Ding, Yue and Lu, Hongtao},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671577},
  pages     = {5442–5453},
  title     = {Beyond binary preference: Leveraging bayesian approaches for joint optimization of ranking and calibration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MFTCoder: Boosting code LLMs with multitask fine-tuning.
<em>KDD</em>, 5430–5441. (<a
href="https://doi.org/10.1145/3637528.3671609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Code LLMs have emerged as a specialized research field, with remarkable studies dedicated to enhancing model&#39;s coding capabilities through fine-tuning on pre-trained models. Previous fine-tuning approaches were typically tailored to specific downstream tasks or scenarios, which meant separate fine-tuning for each task, requiring extensive training resources and posing challenges in terms of deployment and maintenance. Furthermore, these approaches failed to leverage the inherent interconnectedness among different code-related tasks. To overcome these limitations, we present a multi-task fine-tuning framework, MFTCoder, that enables simultaneous and parallel fine-tuning on multiple tasks. By incorporating various loss functions, we effectively address common challenges in multi-task learning, such as data imbalance, varying difficulty levels, and inconsistent convergence speeds. Extensive experiments have conclusively demonstrated that our multi-task fine-tuning approach outperforms both individual fine-tuning on single tasks and fine-tuning on a mixed ensemble of tasks. Moreover, MFTCoder offers efficient training capabilities, including efficient data tokenization modes and parameter efficient fine-tuning (PEFT) techniques, resulting in significantly improved speed compared to traditional fine-tuning methods. MFTCoder seamlessly integrates with several mainstream open-source LLMs, such as CodeLLama and Qwen. Our MFTCoder fine-tuned CodeFuse-DeepSeek-33B claimed the top spot on the Big Code Models Leaderboard ranked by WinRate as of January 30, 2024. MFTCoder is open-sourced at https://github.com/codefuse-ai/MFTCOder},
  archive   = {C_KDD},
  author    = {Liu, Bingchang and Chen, Chaoyu and Gong, Zi and Liao, Cong and Wang, Huan and Lei, Zhichao and Liang, Ming and Chen, Dajun and Shen, Min and Zhou, Hailian and Jiang, Wei and Yu, Hang and Li, Jianguo},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671609},
  pages     = {5430–5441},
  title     = {MFTCoder: Boosting code LLMs with multitask fine-tuning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Source localization for cross network information diffusion.
<em>KDD</em>, 5419–5429. (<a
href="https://doi.org/10.1145/3637528.3671624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Source localization aims to locate information diffusion sources only given the diffusion observation, which has attracted extensive attention in the past few years. Existing methods are mostly tailored for single networks and may not be generalized to handle more complex networks like cross-networks. Cross-network is defined as two interconnected networks, where one network&#39;s functionality depends on the other. Source localization on cross-networks entails locating diffusion sources on the source network by only giving the diffused observation in the target network. The task is challenging due to challenges including: 1) diffusion sources distribution modeling; 2) jointly considering both static and dynamic node features; and 3) heterogeneous diffusion patterns learning. In this work, we propose a novel method, namely CNSL, to handle the three primary challenges. Specifically, we propose to learn the distribution of diffusion sources through Bayesian inference and leverage disentangled encoders to learn static and dynamic node features separately. The learning objective is coupled with the cross-network information propagation estimation model to make the inference of diffusion sources considering the overall diffusion process. Additionally, we also provide two novel cross-network datasets collected by ourselves. Extensive experiments are conducted on both datasets to demonstrate the effectiveness of CNSL in handling the source localization on cross-networks.},
  archive   = {C_KDD},
  author    = {Ling, Chen and Chowdhury, Tanmoy and Ji, Jie and Li, Sirui and Z\&quot;{u}fle, Andreas and Zhao, Liang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671624},
  pages     = {5419–5429},
  title     = {Source localization for cross network information diffusion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Understanding the ranking loss for recommendation with
sparse user feedback. <em>KDD</em>, 5409–5418. (<a
href="https://doi.org/10.1145/3637528.3671565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Click-through rate (CTR) prediction is a crucial area of research in online advertising. While binary cross entropy (BCE) has been widely used as the optimization objective for treating CTR prediction as a binary classification problem, recent advancements have shown that combining BCE loss with an auxiliary ranking loss can significantly improve performance. However, the full effectiveness of this combination loss is not yet fully understood. In this paper, we uncover a new challenge associated with the BCE loss in scenarios where positive feedback is sparse: the issue of gradient vanishing for negative samples. We introduce a novel perspective on the effectiveness of the auxiliary ranking loss in CTR prediction: it generates larger gradients on negative samples, thereby mitigating the optimization difficulties when using the BCE loss only and resulting in improved classification ability. To validate our perspective, we conduct theoretical analysis and extensive empirical evaluations on public datasets. Additionally, we successfully integrate the ranking loss into Tencent&#39;s online advertising system, achieving notable lifts of 0.70\% and 1.26\% in Gross Merchandise Value (GMV) for two main scenarios. The code is openly accessible at: https://github.com/SkylerLinn/Understanding-the-Ranking-Loss.},
  archive   = {C_KDD},
  author    = {Lin, Zhutian and Pan, Junwei and Zhang, Shangyu and Wang, Ximei and Xiao, Xi and Huang, Shudong and Xiao, Lei and Jiang, Jie},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671565},
  pages     = {5409–5418},
  title     = {Understanding the ranking loss for recommendation with sparse user feedback},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep bag-of-words model: An efficient and interpretable
relevance architecture for chinese e-commerce. <em>KDD</em>, 5398–5408.
(<a href="https://doi.org/10.1145/3637528.3671559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Text relevance or text matching of query and product is an essential technique for the e-commerce search system to ensure that the displayed products can match the intent of the query. Many studies focus on improving the performance of the relevance model in search system. Recently, pre-trained language models like BERT have achieved promising performance on the text relevance task. While these models perform well on the offline test dataset, there are still obstacles to deploy the pre-trained language model to the online system as their high latency. The two-tower model is extensively employed in industrial scenarios, owing to its ability to harmonize performance with computational efficiency. Regrettably, such models present an opaque &#39;&#39;black box&#39;&#39; nature, which prevents developers from making special optimizations. In this paper, we raise deep Bag-o f-Words (DeepBoW) model, an efficient and interpretable relevance architecture for Chinese e-commerce. Our approach proposes to encode the query and the product into the sparse BoW representation, which is a set of word-weight pairs. The weight means the important or the relevant score between the corresponding word and the raw text. The relevance score is measured by the accumulation of the matched word between the sparse BoW representation of the query and the product. Compared to popular dense distributed representation that usually suffers from the drawback of black-box, the most advantage of the proposed representation model is highly explainable and interventionable, which is a superior advantage to the deployment and operation of online search engines. Moreover, the online efficiency of the proposed model is even better than the most efficient inner product form of dense representation. The proposed model is experimented on three different datasets for learning the sparse BoW representations, including the human-annotation set, the search-log set and the click-through set. Then the models are evaluated by experienced human annotators. Both the auto metrics and the online evaluations show our DeepBoW model achieves competitive performance while the online inference is much more efficient than the other models. Our DeepBoW model has already deployed to the biggest Chinese e-commerce search engine Taobao and served the entire search traffic for over 6 months.},
  archive   = {C_KDD},
  author    = {Lin, Zhe and Tan, Jiwei and Ou, Dan and Chen, Xi and Yao, Shaowei and Zheng, Bo},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671559},
  pages     = {5398–5408},
  title     = {Deep bag-of-words model: An efficient and interpretable relevance architecture for chinese E-commerce},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hyper-local deformable transformers for text spotting on
historical maps. <em>KDD</em>, 5387–5397. (<a
href="https://doi.org/10.1145/3637528.3671589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Text on historical maps contains valuable information providing georeferenced historical, political, and cultural contexts. However, text extraction from historical maps has been challenging due to the lack of (1) effective methods and (2) training data. Previous approaches use ad-hoc steps tailored to only specific map styles. Recent machine learning-based text spotters (e.g., for scene images) have the potential to solve these challenges because of their flexibility in supporting various types of text instances. However, these methods remain challenges in extracting precise image features for predicting every sub-component (boundary points and characters) in a text instance. This is critical because map text can be lengthy and highly rotated with complex backgrounds, posing difficulties in detecting relevant image features from a rough text region. This paper proposes PALETTE, an end-to-end text spotter for scanned historical maps of a wide variety. PALETTE introduces a novel hyper-local sampling module to explicitly learn localized image features around the target boundary points and characters of a text instance for detection and recognition. PALETTE also enables hyper-local positional embeddings to learn spatial interactions between boundary points and characters within and across text instances. In addition, this paper presents a novel approach to automatically generate synthetic map images, SYNTHMAP+, for training text spotters for historical maps. The experiment shows that PALETTE with SYNTHMAP+ outperforms SOTA text spotters on two new benchmark datasets of historical maps, particularly for long and angled text. We have deployed PALETTE with SYNTHMAP+ to process over 60,000 maps in the David Rumsey Historical Map collection and generated over 100 million text labels to support map searching.},
  archive   = {C_KDD},
  author    = {Lin, Yijun and Chiang, Yao-Yi},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671589},
  pages     = {5387–5397},
  title     = {Hyper-local deformable transformers for text spotting on historical maps},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An open and large-scale dataset for multi-modal climate
change-aware crop yield predictions. <em>KDD</em>, 5375–5386. (<a
href="https://doi.org/10.1145/3637528.3671536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Precise crop yield predictions are of national importance for ensuring food security and sustainable agricultural practices. While AI-for-science approaches have exhibited promising achievements in solving many scientific problems such as drug discovery, precipitation nowcasting, etc., the development of deep learning models for predicting crop yields is constantly hindered by the lack of an open and large-scale deep learning-ready dataset with multiple modalities to accommodate sufficient information. To remedy this, we introduce the CropNet dataset, the first terabyte-sized, publicly available, and multi-modal dataset specifically targeting climate change-aware crop yield predictions for the contiguous United States (U.S.) continent at the county level. Our CropNet dataset is composed of three modalities of data, i.e., Sentinel-2 Imagery, WRF-HRRR Computed Dataset, and USDA Crop Dataset, for over 2200 U.S. counties spanning 6 years (2017-2022), expected to facilitate researchers in developing versatile deep learning models for timely and precisely predicting crop yields at the county-level, by accounting for the effects of both short-term growing season weather variations and long-term climate change on crop yields. Besides, we develop the CropNet package, offering three types of APIs, for facilitating researchers in downloading the CropNet data on the fly over the time and region of interest, and flexibly building their deep learning models for accurate crop yield predictions. Extensive experiments have been conducted on our CropNet dataset via employing various types of deep learning solutions, with the results validating the general applicability and the efficacy of the CropNet dataset in climate change-aware crop yield predictions. We have officially released our CropNet dataset on Hugging Face Datasets https://huggingface.co/datasets/CropNet/CropNet and our CropNet package on the Python Package Index (PyPI) https://pypi.org/project/cropnet. Code and tutorials are available at https://github.com/fudong03/CropNet.},
  archive   = {C_KDD},
  author    = {Lin, Fudong and Guillot, Kaleb and Crawford, Summer and Zhang, Yihe and Yuan, Xu and Tzeng, Nian-Feng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671536},
  pages     = {5375–5386},
  title     = {An open and large-scale dataset for multi-modal climate change-aware crop yield predictions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Harvesting efficient on-demand order pooling from skilled
couriers: Enhancing graph representation learning for refining real-time
many-to-one assignments. <em>KDD</em>, 5363–5374. (<a
href="https://doi.org/10.1145/3637528.3671643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The recent past has witnessed a notable surge in on-demand food delivery (OFD) services, offering delivery fulfillment within dozens of minutes after an order is placed. In OFD, pooling multiple orders for simultaneous delivery in real-time order assignment is a pivotal efficiency source, which may in turn extend delivery time. Constructing high-quality order pooling to harmonize platform efficiency with the experiences of consumers and couriers, is crucial to OFD platforms. However, the complexity and real-time nature of order assignment, making extensive calculations impractical, significantly limit the potential for order consolidation. Moreover, offline environment is frequently riddled with unknown factors, posing challenges for the platform&#39;s perceptibility and pooling decisions. Nevertheless, delivery behaviors of skilled couriers (SCs) who know the environment well, can improve system awareness and effectively inform decisions. Hence a SC delivery network (SCDN) is constructed, based on an enhanced attributed heterogeneous network embedding approach tailored for OFD. It aims to extract features from rich temporal and spatial information, and uncover the latent potential for order combinations embedded within SC trajectories. Accordingly, the vast search space of order assignment can be effectively pruned through scalable similarity calculations of low-dimensional vectors, making comprehensive and high-quality pooling outcomes more easily identified in real time. In addition, the acquired embedding outcomes highlight promising subspaces embedded within this space, i.e., scale-effect hotspot areas, which can offer significant potential for elevating courier efficiency. SCDN has now been deployed in Meituan dispatch system. Online tests reveal that with SCDN, the pooling quality and extent have been greatly improved. And our system can boost couriers&#39; efficiency by 45-55\% during noon peak hours, while upholding the timely delivery commitment.},
  archive   = {C_KDD},
  author    = {Liang, Yile and Zhao, Jiuxia and Li, Donghui and Feng, Jie and Zhang, Chen and Ding, Xuetao and Hao, Jinghua and He, Renqing},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671643},
  pages     = {5363–5374},
  title     = {Harvesting efficient on-demand order pooling from skilled couriers: Enhancing graph representation learning for refining real-time many-to-one assignments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UrbanGPT: Spatio-temporal large language models.
<em>KDD</em>, 5351–5362. (<a
href="https://doi.org/10.1145/3637528.3671578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Spatio-temporal prediction aims to forecast and gain insights into the ever-changing dynamics of urban environments across both time and space. Its purpose is to anticipate future patterns, trends, and events in diverse facets of urban life, including transportation, population movement, and crime rates. Although numerous efforts have been dedicated to developing neural network techniques for accurate predictions on spatio-temporal data, it is important to note that many of these methods heavily depend on having sufficient labeled data to generate precise spatio-temporal representations. Unfortunately, the issue of data scarcity is pervasive in practical urban sensing scenarios. In certain cases, it becomes challenging to collect any labeled data from downstream scenarios, intensifying the problem further. Consequently, it becomes necessary to build a spatio-temporal model that can exhibit strong generalization capabilities across diverse spatio-temporal learning scenarios.Taking inspiration from the remarkable achievements of large language models (LLMs), our objective is to create a spatio-temporal LLM that can exhibit exceptional generalization capabilities across a wide range of downstream urban tasks. To achieve this objective, we present the UrbanGPT, which seamlessly integrates a spatio-temporal dependency encoder with the instruction-tuning paradigm. This integration enables LLMs to comprehend the complex inter-dependencies across time and space, facilitating more comprehensive and accurate predictions under data scarcity. To validate the effectiveness of our approach, we conduct extensive experiments on various public datasets, covering different spatio-temporal prediction tasks. The results consistently demonstrate that our UrbanGPT, with its carefully designed architecture, consistently outperforms state-of-the-art baselines. These findings highlight the potential of building large language models for spatio-temporal learning, particularly in zero-shot scenarios where labeled data is scarce. The code and data are available at: https://github.com/HKUDS/UrbanGPT.},
  archive   = {C_KDD},
  author    = {Li, Zhonghang and Xia, Lianghao and Tang, Jiabin and Xu, Yong and Shi, Lei and Xia, Long and Yin, Dawei and Huang, Chao},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671578},
  pages     = {5351–5362},
  title     = {UrbanGPT: Spatio-temporal large language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Text matching indexers in taobao search. <em>KDD</em>,
5339–5350. (<a href="https://doi.org/10.1145/3637528.3671654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Product search is an important service on Taobao, the largest e-commerce platform in China. Through this service, users can easily find products relevant to their specific needs. Coping with billion-size query loads, Taobao product search has traditionally relied on classical term-based retrieval models due to their powerful and interpretable indexes. In essence, efficient retrieval hinges on the proper storage of the inverted index. Recent successes involve reducing the size (pruning) of the inverted index but the construction and deployment of lossless static index pruning in practical product search still pose non-trivial challenges.In this work, we introduce a novel SM art INDexing (SMIND) solution in Taobao product search. SMIND is designed to reduce information loss during the static pruning process by incorporating user search preferences. Specifically, we first construct &quot;user-query-item&#39;&#39; hypergraphs for four different search preferences, namely purchase, click, exposure, and relevance. Then, we develop an efficient TermRank algorithm applied to these hypergraphs, to preserve relevant items based on specific user preferences during the pruning of the inverted indexer. Our approach offers fresh insights into the field of product search, emphasizing that term dependencies in user search preferences go beyond mere text relevance. Moreover, to address the vocabulary mismatch problem inherent in term-based models, we also incorporate an multi-granularity semantic retrieval model to facilitate semantic matching. Empirical results from both offline evaluation and online A/B tests showcase the superiority of SMIND over state-of-the-art methods, especially in commerce metrics with significant improvements of 1.34\% in Pay Order Count and 1.50\% in Gross Merchandise Value. Besides, SMIND effectively mitigates the Matthew effect of user queries and has been in service for hundreds of millions of daily users since November 2022.},
  archive   = {C_KDD},
  author    = {Li, Sen and Lv, Fuyu and Zhang, Ruqing and Ou, Dan and Zhang, Zhixuan and de Rijke, Maarten},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671654},
  pages     = {5339–5350},
  title     = {Text matching indexers in taobao search},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SEFraud: Graph-based self-explainable fraud detection via
interpretative mask learning. <em>KDD</em>, 5329–5338. (<a
href="https://doi.org/10.1145/3637528.3671534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph-based fraud detection has widespread application in modern industry scenarios, such as spam review and malicious account detection. While considerable efforts have been devoted to designing adequate fraud detectors, the interpretability of their results has often been overlooked. Previous works have attempted to generate explanations for specific instances using post-hoc explaining methods such as a GNNExplainer. However, post-hoc explanations can not facilitate the model predictions and the computational cost of these methods cannot meet practical requirements, thus limiting their application in real-world scenarios. To address these issues, we propose SEFraud, a novel graph-based self-explainable fraud detection framework that simultaneously tackles fraud detection and result in interpretability. Concretely, SEFraud first leverages customized heterogeneous graph transformer networks with learnable feature masks and edge masks to learn expressive representations from the informative heterogeneously typed transactions. A new triplet loss is further designed to enhance the performance of mask learning. Empirical results on various datasets demonstrate the effectiveness of SEFraud as it shows considerable advantages in both the fraud detection performance and interpretability of prediction results. Specifically, SEFraud achieves the most significant improvement with 8.6\% on AUC and 8.5\% on Recall over the second best on fraud detection, as well as an average of 10x speed-up regarding the inference time. Last but not least, SEFraud has been deployed and offers explainable fraud detection service for the largest bank in China, Industrial and Commercial Bank of China Limited (ICBC). Results collected from the production environment of ICBC show that SEFraud can provide accurate detection results and comprehensive explanations that align with the expert business understanding, confirming its efficiency and applicability in large-scale online services.},
  archive   = {C_KDD},
  author    = {Li, Kaidi and Yang, Tianmeng and Zhou, Min and Meng, Jiahao and Wang, Shendi and Wu, Yihui and Tan, Boshuai and Song, Hu and Pan, Lujia and Yu, Fan and Sheng, Zhenli and Tong, Yunhai},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671534},
  pages     = {5329–5338},
  title     = {SEFraud: Graph-based self-explainable fraud detection via interpretative mask learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Chromosomal structural abnormality diagnosis by homologous
similarity. <em>KDD</em>, 5317–5328. (<a
href="https://doi.org/10.1145/3637528.3671642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pathogenic chromosome abnormalities are very common among the general population. While numerical chromosome abnormalities can be quickly and precisely detected, structural chromosome abnormalities are far more complex and typically require considerable efforts by human experts for identification. This paper focuses on investigating the modeling of chromosome features and the identification of chromosomes with structural abnormalities. Most existing data-driven methods concentrate on a single chromosome and consider each chromosome independently, overlooking the crucial aspect of homologous chromosomes. In normal cases, homologous chromosomes share identical structures, with the exception that one of them is abnormal. Therefore, we propose an adaptive method to align homologous chromosomes and diagnose structural abnormalities through homologous similarity. Inspired by the process of human expert diagnosis, we incorporate information from multiple pairs of homologous chromosomes simultaneously, aiming to reduce noise disturbance and improve prediction performance. Extensive experiments on real-world datasets validate the effectiveness of our model compared to baselines.},
  archive   = {C_KDD},
  author    = {Li, Juren and Fu, Fanzhe and Wei, Ran and Sun, Yifei and Lai, Zeyu and Song, Ning and Chen, Xin and Yang, Yang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671642},
  pages     = {5317–5328},
  title     = {Chromosomal structural abnormality diagnosis by homologous similarity},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contextual distillation model for diversified
recommendation. <em>KDD</em>, 5307–5316. (<a
href="https://doi.org/10.1145/3637528.3671514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The diversity of recommendation is equally crucial as accuracy in improving user experience. Existing studies, e.g., Determinantal Point Process (DPP) and Maximal Marginal Relevance (MMR), employ a greedy paradigm to iteratively select items that optimize both accuracy and diversity. However, prior methods typically exhibit quadratic complexity, limiting their applications to the re-ranking stage and are not applicable to other recommendation stages with a larger pool of candidate items, such as the pre-ranking and ranking stages. In this paper, we propose Contextual Distillation Model (CDM), an efficient recommendation model that addresses diversification, suitable for the deployment in all stages of industrial recommendation pipelines. Specifically, CDM utilizes the candidate items in the same user request as context to enhance the diversification of the results. We propose a contrastive context encoder that employs attention mechanisms to model both positive and negative contexts. For the training of CDM, we compare each target item with its context embedding and utilize the knowledge distillation framework to learn the win probability of each target item under the MMR algorithm, where the teacher is derived from MMR outputs. During inference, ranking is performed through a linear combination of the recommendation and student model scores, ensuring both diversity and efficiency. We perform offline evaluations on two industrial datasets and conduct online A/B test of CDM on the short-video platform KuaiShou. The considerable enhancements observed in both recommendation quality and diversity, as shown by metrics, provide strong superiority for the effectiveness of CDM.},
  archive   = {C_KDD},
  author    = {Li, Fan and Si, Xu and Tang, Shisong and Wang, Dingmin and Han, Kunyan and Han, Bing and Zhou, Guorui and Song, Yang and Chen, Hechang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671514},
  pages     = {5307–5316},
  title     = {Contextual distillation model for diversified recommendation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AutoWebGLM: A large language model-based web navigating
agent. <em>KDD</em>, 5295–5306. (<a
href="https://doi.org/10.1145/3637528.3671620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large language models (LLMs) have fueled many intelligent web agents, but most existing ones perform far from satisfying in real-world web navigation tasks due to three factors: (1) the complexity of HTML text data (2) versatility of actions on webpages, and (3) task difficulty due to the open-domain nature of the web. In light of these challenges, we develop the open AutoWebGLM based on ChatGLM3-6B. AutoWebGLM can serve as a powerful automated web navigation agent that outperform GPT-4. Inspired by human browsing patterns, we first design an HTML simplification algorithm to represent webpages with vital information preserved succinctly. We then employ a hybrid human-AI method to build web browsing data for curriculum training. Finally, we bootstrap the model by reinforcement learning and rejection sampling to further facilitate webpage comprehension, browser operations, and efficient task decomposition by itself. For comprehensive evaluation, we establish a bilingual benchmark---AutoWebBench---for real-world web navigation tasks. We evaluate AutoWebGLM across diverse web navigation benchmarks, demonstrating its potential to tackle challenging tasks in real environments. Related code, model, and data are released at https://github.com/THUDM/AutoWebGLM.},
  archive   = {C_KDD},
  author    = {Lai, Hanyu and Liu, Xiao and Iong, Iat Long and Yao, Shuntian and Chen, Yuxuan and Shen, Pengbo and Yu, Hao and Zhang, Hanchen and Zhang, Xiaohan and Dong, Yuxiao and Tang, Jie},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671620},
  pages     = {5295–5306},
  title     = {AutoWebGLM: A large language model-based web navigating agent},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Causal machine learning for cost-effective allocation of
development aid. <em>KDD</em>, 5283–5294. (<a
href="https://doi.org/10.1145/3637528.3671551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Sustainable Development Goals (SDGs) of the United Nations provide a blueprint of a better future by &quot;leaving no one behind&quot;, and, to achieve the SDGs by 2030, poor countries require immense volumes of development aid. In this paper, we develop a causal machine learning framework for predicting heterogeneous treatment effects of aid disbursements to inform effective aid allocation. Specifically, our framework comprises three components: (i) a balancing autoencoder that uses representation learning to embed high-dimensional country characteristics while addressing treatment selection bias; (ii) a counterfactual generator to compute counterfactual outcomes for varying aid volumes to address small sample-size settings; and (iii) an inference model that is used to predict heterogeneous treatment-response curves. We demonstrate the effectiveness of our framework using data with official development aid earmarked to end HIV/AIDS in 105 countries, amounting to more than USD 5.2 billion. For this, we first show that our framework successfully computes heterogeneous treatment-response curves using semi-synthetic data. Then, we demonstrate our framework using real-world HIV data. Our framework points to large opportunities for a more effective aid allocation, suggesting that the total number of new HIV infections could be reduced by up to 3.3\% (~50,000 cases) compared to the current allocation practice.},
  archive   = {C_KDD},
  author    = {Kuzmanovic, Milan and Frauen, Dennis and Hatt, Tobias and Feuerriegel, Stefan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671551},
  pages     = {5283–5294},
  title     = {Causal machine learning for cost-effective allocation of development aid},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Know, grow, and protect net worth: Using ML for asset
protection by preventing overdraft fees. <em>KDD</em>, 5272–5282. (<a
href="https://doi.org/10.1145/3637528.3671628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When a customer overdraws their bank account and their balance is negative they are assessed an overdraft fee. Americans pay approximately $15 billion in unnecessary overdraft fees a year, often in $35 increments; users of the Mint personal finance app pay approximately $250 million in fees a year in particular. These overdraft fees are an excessive financial burden and lead to cascading overdraft fees trapping customers in financial hardship. To address this problem, we have created an ML-driven overdraft early warning system (ODEWS) that assesses a customer&#39;s risk of overdrafting within the next week using their banking and transaction data in the Mint app. At-risk customers are sent an alert so they can take steps to avoid the fee, ultimately changing their behavior and financial habits. The system deployed resulted in a $3 million savings in overdraft fees for Mint customers compared to a control group. Moreover, the methodology outlined here is part of a greater effort to provide ML-driven personalized financial advice to help our members know, grow, and protect their net worth, ultimately, achieving their financial goals.},
  archive   = {C_KDD},
  author    = {Kumar, Avishek and Silver, Tyson},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671628},
  pages     = {5272–5282},
  title     = {Know, grow, and protect net worth: Using ML for asset protection by preventing overdraft fees},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FederatedScope-LLM: A comprehensive package for fine-tuning
large language models in federated learning. <em>KDD</em>, 5260–5271.
(<a href="https://doi.org/10.1145/3637528.3671573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large language models (LLMs) have demonstrated great capabilities in various natural language understanding and generation tasks. These pre-trained LLMs can be further improved for specific downstream tasks by fine-tuning. However, the adoption of LLM in real-world applications can be hindered by privacy concerns and the resource-intensive nature of model training and fine-tuning. When multiple entities have similar interested tasks but cannot directly share their local data due to privacy regulations, federated learning (FL) is a mainstream solution to leverage the data of different entities. Besides avoiding direct data sharing, FL can also achieve rigorous data privacy protection, model intelligent property protection, and model customization via composition with different techniques. Despite the aforementioned advantages of FL, fine-tuning LLMs in FL settings still lacks adequate support from the existing frameworks and, therefore, faces challenges in optimizing the consumption of significant communication and computational resources, preparing various data for different tasks, and satisfying diverse information protection demands. In this paper, we discuss these challenges and introduce our package FederatedScope-LLM (FS-LLM) as a main contribution, which consists: (1) We build a complete end-to-end benchmarking pipeline under real-world scenarios, automizing the processes of dataset preprocessing, federated fine-tuning execution or simulation, and performance evaluation; (2) We provide comprehensive and off-the-shelf federated parameter-efficient fine-tuning (PEFT) algorithm implementations and versatile programming interfaces for future extension, enhancing the capabilities of LLMs in FL scenarios with low communication and computation costs, even without accessing the full model; (3) We adopt several accelerating and resource-efficient operators, and provide flexible pluggable sub-routines for interdisciplinary study. We conduct extensive and reproducible experiments to show the effectiveness of FS-LLM and benchmark advanced LLMs with PEFT algorithms in FL. We release FS-LLM at https://github.com/alibaba/FederatedScope/tree/llm.},
  archive   = {C_KDD},
  author    = {Kuang, Weirui and Qian, Bingchen and Li, Zitao and Chen, Daoyuan and Gao, Dawei and Pan, Xuchen and Xie, Yuexiang and Li, Yaliang and Ding, Bolin and Zhou, Jingren},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671573},
  pages     = {5260–5271},
  title     = {FederatedScope-LLM: A comprehensive package for fine-tuning large language models in federated learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Offline reinforcement learning for optimizing production
bidding policies. <em>KDD</em>, 5251–5259. (<a
href="https://doi.org/10.1145/3637528.3671555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The online advertising market, with its thousands of auctions run per second, presents a daunting challenge for advertisers who wish to optimize their spend under a budget constraint. Thus, advertising platforms typically provide automated agents to their customers, which act on their behalf to bid for impression opportunities in real time at scale. Because these proxy agents are owned by the platform but use advertiser funds to operate, there is a strong practical need to balance reliability and explainability of the agent with optimizing power. We propose a generalizable approach to optimizing bidding policies in production environments by learning from real data using offline reinforcement learning. This approach can be used to optimize any differentiable base policy (practically, a heuristic policy based on principles which the advertiser can easily understand), and only requires data generated by the base policy itself. We use a hybrid agent architecture that combines arbitrary base policies with deep neural networks, where only the optimized base policy parameters are eventually deployed, and the neural network part is discarded after training. We demonstrate that such an architecture achieves statistically significant performance gains in both simulated and at-scale production bidding environments compared to the default production bidding policy. Our approach does not incur additional infrastructure, safety, or explainability costs, as it directly optimizes the parameters of existing production routines without necessarily replacing them with black box-style models like neural networks.},
  archive   = {C_KDD},
  author    = {Korenkevych, Dmytro and Cheng, Frank and Balakir, Artsiom and Nikulkov, Alex and Gao, Lingnan and Cen, Zhihao and Xu, Zuobing and Zhu, Zheqing},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671555},
  pages     = {5251–5259},
  title     = {Offline reinforcement learning for optimizing production bidding policies},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). False positives in a/b tests. <em>KDD</em>, 5240–5250. (<a
href="https://doi.org/10.1145/3637528.3671631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A/B tests, or online controlled experiments, are used heavily in the software industry to evaluate implementations of ideas, as the paradigm is the gold standard in science for establishing causality: the changes introduced in the treatment caused the changes to the metrics of interest with high probability. What distinguishes software experiments, or A/B tests, from experiments in many other domains is the scale (e.g., over 100 experiment treatments may launch on a given workday in large companies) and the effect sizes that matter to the business are small (e.g., a 3\% improvement to conversion rate from a single experiment is a cause for celebration). The humbling reality is that most experiments fail to improve key metrics, and success rates of only about 10-20\% are most common. With low success rates, the industry standard alpha threshold of 0.05 implies a high probability of false positives. We begin with motivation about why false positives are expensive in many software domains. We then offer several approaches to estimate the true success rate of experiments, given the observed &quot;win&quot; rate (statistically significant positive improvements), and show examples from Expedia and Optimizely. We offer a modified procedure for experimentation, based in sequential group testing, that selectively extends experiments to reduce false positives, increase power, at a small increase to runtime. We conclude with a discussion of the difference between ideas and experiments in practice, terms that are often incorrectly used interchangeably.},
  archive   = {C_KDD},
  author    = {Kohavi, Ron and Chen, Nanyu},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671631},
  pages     = {5240–5250},
  title     = {False positives in A/B tests},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large scale hierarchical industrial demand time-series
forecasting incorporating sparsity. <em>KDD</em>, 5230–5239. (<a
href="https://doi.org/10.1145/3637528.3671632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hierarchical time-series forecasting (HTSF) is an important problem for many real-world business applications where the goal is to simultaneously forecast multiple time-series that are related to each other via a hierarchical relation. Recent works, however, do not address two important challenges that are typically observed in many demand forecasting applications at large companies. First, many time-series at lower levels of the hierarchy have high sparsity i.e., they have a significant number of zeros. Most HTSF methods do not address this varying sparsity across the hierarchy. Further, they do not scale well to the large size of the real-world hierarchy typically unseen in benchmarks used in literature. We resolve both these challenges by proposing HAILS, a novel probabilistic hierarchical model that enables accurate and calibrated probabilistic forecasts across the hierarchy by adaptively modeling sparse and dense time-series with different distributional assumptions and reconciling them to adhere to hierarchical constraints. We show the scalability and effectiveness of our methods by evaluating them against real-world demand forecasting datasets. We deploy HAILS at a large chemical manufacturing company for a product demand forecasting application with over ten thousand products and observe a significant 8.5\% improvement in forecast accuracy and 23\% better improvement for sparse time-series. The enhanced accuracy and scalability make HAILS a valuable tool for improved business planning and customer experience.},
  archive   = {C_KDD},
  author    = {Kamarthi, Harshavardhan and Sasanur, Aditya B. and Tong, Xinjie and Zhou, Xingyu and Peters, James and Czyzyk, Joe and Prakash, B. Aditya},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671632},
  pages     = {5230–5239},
  title     = {Large scale hierarchical industrial demand time-series forecasting incorporating sparsity},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RJUA-MedDQA: A multimodal benchmark for medical document
question answering and clinical reasoning. <em>KDD</em>, 5218–5229. (<a
href="https://doi.org/10.1145/3637528.3671644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advancements in Large Language Models (LLMs) and Large Multi-modal Models (LMMs) have shown potential in various medical applications, such as Intelligent Medical Diagnosis. Although impressive results have been achieved, we find that existing benchmarks do not reflect the complexity of real medical reports and specialized in-depth reasoning capabilities. In this work, we establish a comprehensive benchmark in the field of medical specialization and introduced RJUA-MedDQA, which contains 2000 real-world Chinese medical report images poses several challenges: comprehensively interpreting imgage content across a wide variety of challenging layouts, possessing the numerical reasoning ability to identify abnormal indicators and demonstrating robust clinical reasoning ability to provide the statement of disease diagnosis, status and advice based on a collection of medical contexts. We carefully design the data generation pipeline and proposed the Efficient Structural Restoration Annotation (ESRA) Method, aimed at restoring textual and tabular content in medical report images. This method substantially enhances annotation efficiency, doubling the productivity of each annotator, and yields a 26.8\% improvement in accuracy. We conduct extensive evaluations, including few-shot assessments of 5 LMMs which are capable of solving Chinese medical QA tasks. To further investigate the limitations and potential of current LMMs, we conduct comparative experiments on a set of strong LLMs by using image-text generated by ESRA method. We report the performance of baselines and offer several observations: (1) The overall performance of existing LMMs is still limited; however LMMs more robust to low-quality and diverse-structured images compared to LLMs. (3) Reasoning across context and image content present significant challenges. We hope this benchmark helps the community make progress on these challenging tasks in multi-modal medical document understanding and facilitate its application in healthcare. Our dataset will be publicly available for noncommercial use at https://github.com/Alipay-Med/medDQA_benchmark.git},
  archive   = {C_KDD},
  author    = {Jin, Congyun and Zhang, Ming and Ma, Weixiao and Li, Yujiao and Wang, Yingbo and Jia, Yabo and Du, Yuliang and Sun, Tao and Wang, Haowen and Fan, Cong and Gu, Jinjie and Chi, Chenfei and Lv, Xiangguo and Li, Fangzhou and Xue, Wei and Huang, Yiran},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671644},
  pages     = {5218–5229},
  title     = {RJUA-MedDQA: A multimodal benchmark for medical document question answering and clinical reasoning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interpretable cascading mixture-of-experts for urban traffic
congestion prediction. <em>KDD</em>, 5206–5217. (<a
href="https://doi.org/10.1145/3637528.3671507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Rapid urbanization has significantly escalated traffic congestion, underscoring the need for advanced congestion prediction services to bolster intelligent transportation systems. As one of the world&#39;s largest ride-hailing platforms, DiDi places great emphasis on the accuracy of congestion prediction to enhance the effectiveness and reliability of their real-time services, such as travel time estimation and route planning. Despite numerous efforts have been made on congestion prediction, most of them fall short in handling heterogeneous and dynamic spatio-temporal dependencies (e.g., periodic and non-periodic congestions), particularly in the presence of noisy and incomplete traffic data. In this paper, we introduce a Congestion Prediction Mixture-of-Experts, CP-MoE, to address the above challenges. We first propose a sparsely-gated Mixture of Adaptive Graph Learners (MAGLs) with congestion-aware inductive biases to improve the model capacity for efficiently capturing complex spatio-temporal dependencies in varying traffic scenarios. Then, we devise two specialized experts to help identify stable trends and periodic patterns within the traffic data, respectively. By cascading these experts with MAGLs, CP-MoE delivers congestion predictions in a more robust and interpretable manner. Furthermore, an ordinal regression strategy is adopted to facilitate effective collaboration among diverse experts. Extensive experiments on real-world datasets demonstrate the superiority of our proposed method compared with state-of-the-art spatio-temporal prediction models. More importantly, CP-MoE has been deployed in DiDi to improve the accuracy and reliability of the travel time estimation system.},
  archive   = {C_KDD},
  author    = {Jiang, Wenzhao and Han, Jindong and Liu, Hao and Tao, Tao and Tan, Naiqiang and Xiong, Hui},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671507},
  pages     = {5206–5217},
  title     = {Interpretable cascading mixture-of-experts for urban traffic congestion prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ERASE: Benchmarking feature selection methods for deep
recommender systems. <em>KDD</em>, 5194–5205. (<a
href="https://doi.org/10.1145/3637528.3671571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep Recommender Systems (DRS) are increasingly dependent on a large number of feature fields for more precise recommendations. Effective feature selection methods are consequently becoming critical for further enhancing the accuracy and optimizing storage efficiencies to align with the deployment demands. This research area, particularly in the context of DRS, is nascent and faces three core challenges. Firstly, variant experimental setups across research papers often yield unfair comparisons, obscuring practical insights. Secondly, the existing literature&#39;s lack of detailed analysis on selection attributes, based on large-scale datasets and a thorough comparison among selection techniques and DRS backbones, restricts the generalizability of findings and impedes deployment on DRS. Lastly, research often focuses on comparing the peak performance achievable by feature selection methods. This approach is typically computationally infeasible for identifying the optimal hyperparameters and overlooks evaluating the robustness and stability of these methods. To bridge these gaps, this paper presents ERASE, a comprehensive bEnchmaRk for feAture SElection for DRS. ERASE comprises a thorough evaluation of eleven feature selection methods, covering both traditional and deep learning approaches, across four public datasets, private industrial datasets, and a real-world commercial platform, achieving significant enhancement. Our code is available online for ease of reproduction.},
  archive   = {C_KDD},
  author    = {Jia, Pengyue and Wang, Yejing and Du, Zhaocheng and Zhao, Xiangyu and Wang, Yichao and Chen, Bo and Wang, Wanyu and Guo, Huifeng and Tang, Ruiming},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671571},
  pages     = {5194–5205},
  title     = {ERASE: Benchmarking feature selection methods for deep recommender systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning metrics that maximise power for accelerated
a/b-tests. <em>KDD</em>, 5183–5193. (<a
href="https://doi.org/10.1145/3637528.3671512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Online controlled experiments are a crucial tool to allow for confident decision-making in technology companies. A North Star metric is defined (such as long-term revenue or user retention), and system variants that statistically significantly improve on this metric in an A/B-test can be considered superior. North Star metrics are typically delayed and insensitive. As a result, the cost of experimentation is high: experiments need to run for a long time, and even then, type-II errors (i.e. false negatives) are prevalent.We propose to tackle this by learning metrics from short-term signals that directly maximise the statistical power they harness with respect to the North Star. We show that existing approaches are prone to overfitting, in that higher average metric sensitivity does not imply improved type-II errors, and propose to instead minimise the p-values a metric would have produced on a log of past experiments. We collect such datasets from two social media applications with over 160 million Monthly Active Users each, totalling over 153 A/B-pairs. Empirical results show that we are able to increase statistical power by up to 78\% when using our learnt metrics stand-alone, and by up to 210\% when used in tandem with the North Star. Alternatively, we can obtain constant statistical power at a sample size that is down to 12\% of what the North Star requires, significantly reducing the cost of experimentation.},
  archive   = {C_KDD},
  author    = {Jeunen, Olivier and Ustimenko, Aleksei},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671512},
  pages     = {5183–5193},
  title     = {Learning metrics that maximise power for accelerated A/B-tests},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decomposed attention segment recurrent neural network for
orbit prediction. <em>KDD</em>, 5172–5182. (<a
href="https://doi.org/10.1145/3637528.3671546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As the focus of space exploration shifts from national agencies to private companies, the interest in space industry has been steadily increasing. With the increasing number of satellites, the risk of collisions between satellites and space debris has escalated, potentially leading to significant property and human losses. Therefore, accurately modeling the orbit is critical for satellite operations. In this work, we propose the Decomposed Attention Segment Recurrent Neural Network (DASR) model, adding two key components, Multi-Head Attention and Tensor Train Decomposition, to SegRNN for orbit prediction. The DASR model applies Multi-Head Attention before segmenting at input data and before the input of the GRU layers. In addition, Tensor Train (TT) Decomposition is applied to the weight matrices of the Multi-Head Attention in both the encoder and decoder. For evaluation, we use three real-world satellite datasets from the Korea Aerospace Research Institute (KARI), which are currently operating: KOMPSAT-3, KOMPSAT-3A, and KOMPSAT-5 satellites. Our proposed model demonstrates superior performance compared to other SOTA baseline models. We demonstrate that our approach has 94.13\% higher predictive performance than the second-best model in the KOMPSAT-3 dataset, 89.79\% higher in the KOMPSAT-3A dataset, and 76.71\% higher in the KOMPSAT-5 dataset.},
  archive   = {C_KDD},
  author    = {Jeong, SeungWon and Woo, Soyeon and Chung, Daewon and Woo, Simon S. and Shin, Youjin},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671546},
  pages     = {5172–5182},
  title     = {Decomposed attention segment recurrent neural network for orbit prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Personalized product assortment with real-time 3D perception
and bayesian payoff estimation. <em>KDD</em>, 5161–5171. (<a
href="https://doi.org/10.1145/3637528.3671518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Product assortment selection is a critical challenge facing physical retailers. Effectively aligning inventory with the preferences of shoppers can increase sales and decrease out-of-stocks. However, in real-world settings the problem is challenging due to the combinatorial explosion of product assortment possibilities. Consumer preferences are typically heterogeneous across space and time, making inventory-preference alignment challenging. Additionally, existing strategies rely on syndicated data, which tends to be aggregated, low resolution, and suffer from high latency. To solve these challenges, we introduce a real-time recommendation system, which we call EdgeRec3D. Our system utilizes recent advances in 3D computer vision for perception and automatic, fine grained sales estimation. These perceptual components run on the edge of the network and facilitate real-time reward signals. Additionally, we develop a Bayesian payoff model to account for noisy estimates from 3D LIDAR data. We rely on spatial clustering to allow the system to adapt to heterogeneous consumer preferences, and a graph-based candidate generation algorithm to address the combinatorial search problem. We test our system in real-world stores across two, 6-8 week A/B tests with beverage products and demonstrate a 35\% and 27\% increase in sales respectively. Finally, we monitor the deployed system for a period of 28 weeks with an observational study and show a 9.4\% increase in sales.},
  archive   = {C_KDD},
  author    = {Jenkins, Porter and Selander, Michael and Jenkins, J. Stockton and Merrill, Andrew and Armstrong, Kyle},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671518},
  pages     = {5161–5171},
  title     = {Personalized product assortment with real-time 3D perception and bayesian payoff estimation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning to bid the interest rate in online unsecured
personal loans. <em>KDD</em>, 5150–5160. (<a
href="https://doi.org/10.1145/3637528.3671584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The unsecured personal loan (UPL) market is a multi-billion dollar market where numerous financial institutions compete. Due to the development of online banking, loan applicants start to compare numerous loan products. They aim for high loan limits and low interest rates. Since loan applicants have a desired loan amount, institutions instead focus on adjusting interest rates. Despite the importance of determining optimal interest strategies, institutions have traditionally relied on heuristic methods by human experts to set interest rates. This is done by adding a target return on assets (ROA) to the applicant&#39;s expected default probability predicted by a credit scoring system (CSS) such as the FICO score.We conceptualize the UPL market dynamics as a repeated auction scenario, where loan applicants (akin to sellers) seek the lowest interest rates, while financial institutions (akin to bidders) aim to maximize profits through higher interest rates. To the best of our knowledge, this is the first time anyone has approached the UPL market through the viewpoint of a repeated auction. While there are several research done in learning to bid in repeated auctions, those works cannot be directly applied to the UPL market due to the lack of any feedback about other bidders&#39; strategies and the need to satisfy the bidder&#39;s target loan volume and profit variance. We present an algorithm named AutoInterest, which is a modification of the dual gradient descent algorithm. In addition, we provide a framework to evaluate interest rate bidding strategies on a benchmark dataset and the credit bureau dataset of actual loan applicants in South Korea. We evaluate AutoInterest on this framework and show higher cumulative profit compared to other common online algorithms and the current fixed strategy used by real institutions.},
  archive   = {C_KDD},
  author    = {Jee, Dong Jun and Jin, Seung Jung and Yoo, Ji Hoon and Ahn, Byunggyu},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671584},
  pages     = {5150–5160},
  title     = {Learning to bid the interest rate in online unsecured personal loans},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Personalised drug identifier for cancer treatment with
transformers using auxiliary information. <em>KDD</em>, 5138–5149. (<a
href="https://doi.org/10.1145/3637528.3671652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cancer remains a global challenge due to its growing clinical and economic burden. Its uniquely personal manifestation, which makes treatment difficult, has fuelled the quest for personalized treatment strategies. Thus, genomic profiling is increasingly becoming part of clinical diagnostic panels. Effective use of such panels requires accurate drug response prediction (DRP) models, which are challenging to build due to limited labelled patient data. Previous methods to address this problem have used various forms of transfer learning. However, they do not explicitly model the variable length sequential structure of the list of mutations in such diagnostic panels. Further, they do not utilize auxiliary information (like patient survival) for model training. We address these limitations through a novel transformer-based method, which surpasses the performance of state-of-the-art DRP models on benchmark data. Code for our method is available at https://github.com/CDAL-SOC/PREDICT-AI.We also present the design of a treatment recommendation system (TRS), which is currently deployed at the National University Hospital, Singapore and is being evaluated in a clinical trial. We discuss why the recommended drugs and their predicted scores alone, obtained from DRP models, are insufficient for treatment planning. Treatment planning for complex cancer cases, in the face of limited clinical validation, requires assessment of many other factors, including several indirect sources of evidence on drug efficacy. We discuss key lessons learnt on model validation and use of indirect supporting evidence to build clinicians&#39; trust and aid their decision making.},
  archive   = {C_KDD},
  author    = {Jayagopal, Aishwarya and Xue, Hansheng and He, Ziyang and Walsh, Robert J. and Hariprasannan, Krishna Kumar and Tan, David Shao Peng and Tan, Tuan Zea and Pitt, Jason J. and Jeyasekharan, Anand D. and Rajan, Vaibhav},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671652},
  pages     = {5138–5149},
  title     = {Personalised drug identifier for cancer treatment with transformers using auxiliary information},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explainable and interpretable forecasts on non-smooth
multivariate time series for responsible gameplay. <em>KDD</em>,
5126–5137. (<a href="https://doi.org/10.1145/3637528.3671657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-variate Time Series (MTS) forecasting has made large strides (with very negligible errors) through recent advancements in neural networks, e.g., Transformers. However, in critical situations like predicting gaming overindulgence that affects one&#39;s mental well-being; an accurate forecast without a contributing evidence (explanation) is irrelevant. Hence, it becomes important that the forecasts are Interpretable - intermediate representation of the forecasted trajectory is comprehensible; as well as Explainable - attentive input features and events are accessible for a personalized and timely intervention of players at risk. While the contributing state of the art research on interpretability primarily focuses on temporally-smooth single-process driven time series data, our online multi-player gameplay data demonstrates intractable temporal randomness due to intrinsic orthogonality between player&#39;s game outcome and their intent to engage further. We introduce a novel deep Actionable Forecasting Network (AFN), which addresses the inter-dependent challenges associated with three exclusive objectives - 1) forecasting accuracy; 2) smooth comprehensible trajectory and 3) explanations via multi-dimensional input features while tackling the challenges introduced by our non-smooth temporal data, together in one single solution. AFN establishes a new benchmark via: (i) achieving 25\% improvement on the MSE of the forecasts on player data in comparison to the SOM-VAE based SOTA networks; (ii) attributing unfavourable progression of a player&#39;s time series to a specific future time step(s), with the premise of eliminating near-future overindulgent player volume by over 18\% with player specific actionable inputs feature(s) and (iii) proactively detecting over 23\% (100\% jump from SOTA) of the to-be overindulgent, players on an average, 4 weeks in advance.},
  archive   = {C_KDD},
  author    = {Jagirdar, Hussain and Talwadker, Rukma and Pareek, Aditya and Agrawal, Pulkit and Mukherjee, Tridib},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671657},
  pages     = {5126–5137},
  title     = {Explainable and interpretable forecasts on non-smooth multivariate time series for responsible gameplay},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-domain LifeLong sequential modeling for online
click-through rate prediction. <em>KDD</em>, 5116–5125. (<a
href="https://doi.org/10.1145/3637528.3671601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Lifelong sequential modeling (LSM) has significantly advanced recommendation systems on social media platforms. Diverging from single-domain LSM, cross-domain LSM involves modeling lifelong behavior sequences from a source domain to a different target domain. In this paper, we propose the Lifelong Cross Network (LCN), a novel approach for cross-domain LSM. LCN features a Cross Representation Production (CRP) module that utilizes contrastive loss to improve the learning of item embeddings, effectively bridging items across domains. This is important for enhancing the retrieval of relevant items in cross-domain lifelong sequences. Furthermore, we propose the Lifelong Attention Pyramid (LAP) module, which contains three cascading attention levels. By adding an intermediate level and integrating the results from all three levels, the LAP module can capture a broad spectrum of user interests and ensure gradient propagation throughout the sequence. The proposed LAP can also achieve remarkable consistency across attention levels, making it possible to further narrow the candidate item pool of the top level. This allows for the use of advanced attention techniques to effectively mitigate the impact of the noise in cross-domain sequences and improve the non-linearity of the representation, all while maintaining computational efficiency. Extensive experiments conducted on both a public dataset and an industrial dataset from the WeChat Channels platform reveal that the LCN outperforms current methods in terms of prediction accuracy and online performance metrics.},
  archive   = {C_KDD},
  author    = {Hou, Ruijie and Yang, Zhaoyang and Ming, Yu and Lu, Hongyu and Zheng, Zhuobin and Chen, Yu and Zeng, Qinsong and Chen, Ming},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671601},
  pages     = {5116–5125},
  title     = {Cross-domain LifeLong sequential modeling for online click-through rate prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed harmonization: Federated clustered batch effect
adjustment and generalization. <em>KDD</em>, 5105–5115. (<a
href="https://doi.org/10.1145/3637528.3671590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Independent and identically distributed (i.i.d.) data is essential to many data analysis and modeling techniques. In the medical domain, collecting data from multiple sites or institutions is a common strategy that guarantees sufficient clinical diversity, determined by the decentralized nature of medical data. However, data from various sites are easily biased by the local environment or facilities, thereby violating the i.i.d. rule. A common strategy is to harmonize the site bias while retaining important biological information. The ComBat is among the most popular harmonization approaches and has recently been extended to handle distributed sites. However, when faced with situations involving newly joined sites in training or evaluating data from unknown/unseen sites, ComBat lacks compatibility and requires retraining with data from all the sites. The retraining leads to significant computational and logistic overhead that is usually prohibitive. In this work, we develop a novel Cluster ComBat harmonization algorithm, which leverages cluster patterns of the data in different sites and greatly advances the usability of ComBat harmonization. We use extensive simulation and real medical imaging data from ADNI to demonstrate the superiority of the proposed approach. Our codes are provided in https://github.com/illidanlab/distributed-cluster-harmonization.},
  archive   = {C_KDD},
  author    = {Hoang, Bao and Pang, Yijiang and Liang, Siqi and Zhan, Liang and Thompson, Paul M. and Zhou, Jiayu},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671590},
  pages     = {5105–5115},
  title     = {Distributed harmonization: Federated clustered batch effect adjustment and generalization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rankability-enhanced revenue uplift modeling framework for
online marketing. <em>KDD</em>, 5093–5104. (<a
href="https://doi.org/10.1145/3637528.3671516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Uplift modeling has been widely employed in online marketing by predicting the response difference between the treatment and control groups, so as to identify the sensitive individuals toward interventions like coupons or discounts. Compared with traditional conversion uplift modeling,revenue uplift modeling exhibits higher potential due to its direct connection with the corporate income. However, previous works can hardly handle the continuous long-tail response distribution in revenue uplift modeling. Moreover, they have neglected to optimize the uplift ranking among different individuals, which is actually the core of uplift modeling. To address such issues, in this paper, we first utilize the zero-inflated lognormal (ZILN) loss to regress the responses and customize the corresponding modeling network, which can be adapted to different existing uplift models. Then, we study the ranking-related uplift modeling error from the theoretical perspective and propose two tighter error bounds as the additional loss terms to the conventional response regression loss. Finally, we directly model the uplift ranking error for the entire population with a listwise uplift ranking loss. The experiment results on offline public and industrial datasets validate the effectiveness of our method for revenue uplift modeling. Furthermore, we conduct large-scale experiments on a prominent online fintech marketing platform, Tencent FiT, which further demonstrates the superiority of our method in real-world applications.},
  archive   = {C_KDD},
  author    = {He, Bowei and Weng, Yunpeng and Tang, Xing and Cui, Ziqiang and Sun, Zexu and Chen, Liang and He, Xiuqiang and Ma, Chen},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671516},
  pages     = {5093–5104},
  title     = {Rankability-enhanced revenue uplift modeling framework for online marketing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Paths2Pair: Meta-path based link prediction in billion-scale
commercial heterogeneous graphs. <em>KDD</em>, 5082–5092. (<a
href="https://doi.org/10.1145/3637528.3671563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Link prediction, determining if a relation exists between two entities, is an essential task in the analysis of heterogeneous graphs with diverse entities and relations. Despite extensive research in link prediction, most existing works focus on predicting the relation type between given pairs of entities. However, it is almost impractical to check every entity pair when trying to find most hidden relations in a billion-scale heterogeneous graph due to the billion squared number of possible pairs. Meanwhile, most methods aggregate information at the node level, potentially leading to the loss of direct connection information between the two nodes. In this paper, we introduce Paths2Pair, a novel framework to address these limitations for link prediction in billion-scale commercial heterogeneous graphs. (i) First, it selects a subset of reliable entity pairs for prediction based on relevant meta-paths. (ii) Then, it utilizes various types of content information from the meta-paths between each selected entity pair to predict whether a target relation exists. We first evaluate our Paths2Pair based on a large-scale dataset, and results show Paths2Pair outperforms state-of-the-art baselines significantly. We then deploy our Paths2Pair on JD Logistics, one of the largest logistics companies in the world, for business expansion. The uncovered relations by Paths2Pair have helped JD Logistics identify 108,709 contacts to attract new company customers, resulting in an 84\% increase in the success rate compared to the state-of-the-practice solution, demonstrating the practical value of our framework. We have released the code of our framework at https://github.com/JQHang/Paths2Pair.},
  archive   = {C_KDD},
  author    = {Hang, Jinquan and Hong, Zhiqing and Feng, Xinyue and Wang, Guang and Yang, Guang and Li, Feng and Song, Xining and Zhang, Desheng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671563},
  pages     = {5082–5092},
  title     = {Paths2Pair: Meta-path based link prediction in billion-scale commercial heterogeneous graphs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FedSecurity: A benchmark for attacks and defenses in
federated learning and federated LLMs. <em>KDD</em>, 5070–5081. (<a
href="https://doi.org/10.1145/3637528.3671545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces FedSecurity, an end-to-end benchmark that serves as a supplementary component of the FedML library for simulating adversarial attacks and corresponding defense mechanisms in Federated Learning (FL). FedSecurity eliminates the need for implementing the fundamental FL procedures, e.g., FL training and data loading, from scratch, thus enables users to focus on developing their own attack and defense strategies. It contains two key components, including FedAttacker that conducts a variety of attacks during FL training, and FedDefender that implements defensive mechanisms to counteract these attacks. FedSecurity has the following features: i) It offers extensive customization options to accommodate a broad range of machine learning models (e.g., Logistic Regression, ResNet, and GAN) and FL optimizers (e.g., FedAVG, FedOPT, and FedNOVA); ii) it enables exploring the effectiveness of attacks and defenses across different datasets and models; and iii) it supports flexible configuration and customization through a configuration file and some APIs. We further demonstrate FedSecurity&#39;s utility and adaptability through federated training of Large Language Models (LLMs) to showcase its potential on a wide range of complex applications.},
  archive   = {C_KDD},
  author    = {Han, Shanshan and Buyukates, Baturalp and Hu, Zijian and Jin, Han and Jin, Weizhao and Sun, Lichao and Wang, Xiaoyang and Wu, Wenxuan and Xie, Chulin and Yao, Yuhang and Zhang, Kai and Zhang, Qifan and Zhang, Yuhui and Joe-Wong, Carlee and Avestimehr, Salman and He, Chaoyang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671545},
  pages     = {5070–5081},
  title     = {FedSecurity: A benchmark for attacks and defenses in federated learning and federated LLMs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning to rank for maps at airbnb. <em>KDD</em>,
5061–5069. (<a href="https://doi.org/10.1145/3637528.3671648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As a two-sided marketplace, Airbnb brings together hosts who own listings for rent with prospective guests from around the globe. Results from a guest&#39;s search for listings are displayed primarily through two interfaces: (1) as a list of rectangular cards that contain on them the listing image, price, rating, and other details, referred to as list-results (2) as oval pins on a map showing the listing price, called map-results. Both these interfaces, since their inception, have used the same ranking algorithm that orders listings by their booking probabilities and selects the top listings for display. But some of the basic assumptions underlying ranking, built for a world where search results are presented as lists, simply break down for maps. This paper describes how we rebuilt ranking for maps by revising the mathematical foundations of how users interact with search results. Our iterative and experiment-driven approach led us through a path full of twists and turns, ending in a unified theory for the two interfaces. Our journey shows how assumptions taken for granted when designing machine learning algorithms may not apply equally across all user interfaces, and how they can be adapted. The net impact was one of the largest improvements in user experience for Airbnb which we discuss as a series of experimental validations.},
  archive   = {C_KDD},
  author    = {Haldar, Malay and Zhang, Hongwei and Bellare, Kedar and Chen, Sherry and Banerjee, Soumyadip and Wang, Xiaotang and Abdool, Mustafa and Gao, Huiji and Tapadia, Pavan and He, Liwei and Katariya, Sanjeev},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671648},
  pages     = {5061–5069},
  title     = {Learning to rank for maps at airbnb},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SentHYMNent: An interpretable and sentiment-driven model for
algorithmic melody harmonization. <em>KDD</em>, 5050–5060. (<a
href="https://doi.org/10.1145/3637528.3671626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Music composition and analysis is an inherently creative task, involving a combination of heart and mind. However, the vast majority of algorithmic music models completely ignore the &quot;heart&quot; component of music, resulting in output that often lacks the rich emotional direction found in human-composed music. Models that try to incorporate musical sentiment rely on a &quot;valence-arousal&quot; model, which insufficiently characterizes emotion in two dimensions. Furthermore, existing methods typically adopt a black-box, music agnostic approach, treating music-theoretical and sentimental understanding as a by-product that can be inferred given sufficient data. In this study, we introduce two major novel elements: a nuanced mixture-based representation for musical sentiment, including a web tool to gather data, as well as a sentiment- and theory-driven harmonization model, SentHYMNent. SentHYMNent employs a novel Hidden Markov Model based on both key and chord transitions, as well as sentiment mixtures, to provide a probabilistic framework for learning key modulations and chordal progressions from a given melodic line and sentiment. Furthermore, our approach leverages compositional principles, resulting in a simpler model that significantly reduces computational burden and enhances interpretability compared to current state-of-the-art algorithmic harmonization methods. Importantly, as shown in our experiments, these improvements do not come at the expense of harmonization quality. We also provide a web app where users can upload their own melodies for SentHYMNent to harmonize.},
  archive   = {C_KDD},
  author    = {Hahn, Stephen and Yin, Jerry and Zhu, Rico and Xu, Weihan and Jiang, Yue and Mak, Simon and Rudin, Cynthia},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671626},
  pages     = {5050–5060},
  title     = {SentHYMNent: An interpretable and sentiment-driven model for algorithmic melody harmonization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generative auto-bidding via conditional diffusion modeling.
<em>KDD</em>, 5038–5049. (<a
href="https://doi.org/10.1145/3637528.3671526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Auto-bidding plays a crucial role in facilitating online advertising by automatically providing bids for advertisers. Reinforcement learning (RL) has gained popularity for auto-bidding. However, most current RL auto-bidding methods are modeled through the Markovian Decision Process (MDP), which assumes the Markovian state transition. This assumption restricts the ability to perform in long horizon scenarios and makes the model unstable when dealing with highly random online advertising environments. To tackle this issue, this paper introduces AI-Generated Bidding (AIGB), a novel paradigm for auto-bidding through generative modeling. In this paradigm, we propose DiffBid, a conditional diffusion modeling approach for bid generation. DiffBid directly models the correlation between the return and the entire trajectory, effectively avoiding error propagation across time steps in long horizons. Additionally, DiffBid offers a versatile approach for generating trajectories that maximize given targets while adhering to specific constraints. Extensive experiments conducted on the real-world dataset and online A/B test on Alibaba advertising platform demonstrate the effectiveness of DiffBid, achieving 2.81\% increase in GMV and 3.36\% increase in ROI.},
  archive   = {C_KDD},
  author    = {Guo, Jiayan and Huo, Yusen and Zhang, Zhilin and Wang, Tianyu and Yu, Chuan and Xu, Jian and Zheng, Bo and Zhang, Yan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671526},
  pages     = {5038–5049},
  title     = {Generative auto-bidding via conditional diffusion modeling},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-task conditional attention network for conversion
prediction in logistics advertising. <em>KDD</em>, 5028–5037. (<a
href="https://doi.org/10.1145/3637528.3671549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Logistics advertising is an emerging task in online-to-offline logistics systems, where logistics companies expand parcel shipping services to new users through advertisements on shopping websites. Compared to existing online e-commerce advertising, logistics advertising has two significant new characteristics: (i) the complex factors in logistics advertising considering both users&#39; offline logistics preference and online purchasing profiles; and (ii) data sparsity and mutual relations among multiple steps due to longer advertising conversion processes. To address these challenges, we design MCAC, a Multi-task Conditional Attention network-based logistics advertising Conversion prediction framework, which consists of (i) an offline shipping preference extraction model to extract the user&#39;s offline logistics preference from historical shipping records, and (ii) a multi-task conditional attention-based conversion rate prediction module to model mutual relations among multiple steps in logistics advertising conversion processes. We evaluate and deploy MCAC on one of the largest e-commerce platforms in China for logistics advertising. Extensive offline experiments show that our method outperforms state-of-the-art baselines in various metrics. Moreover, the conversion rate prediction results of large-scale online A/B testing show that MCAC achieves a 15.22\% improvement compared to existing industrial practices, which demonstrates the effectiveness of the proposed framework.},
  archive   = {C_KDD},
  author    = {Guo, Baoshen and Song, Xining and Wang, Shuai and Gong, Wei and He, Tian and Liu, Xue},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671549},
  pages     = {5028–5037},
  title     = {Multi-task conditional attention network for conversion prediction in logistics advertising},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intelligent agents with LLM-based process automation.
<em>KDD</em>, 5018–5027. (<a
href="https://doi.org/10.1145/3637528.3671646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While intelligent virtual assistants like Siri, Alexa, and Google Assistant have become ubiquitous in modern life, they still face limitations in their ability to follow multi-step instructions and accomplish complex goals articulated in natural language. However, recent breakthroughs in large language models (LLMs) show promise for overcoming existing barriers by enhancing natural language processing and reasoning capabilities. Though promising, applying LLMs to create more advanced virtual assistants still faces challenges like ensuring robust performance and handling variability in real-world user commands. This paper proposes a novel LLM-based virtual assistant that can automatically perform multi-step operations within mobile apps based on high-level user requests. The system represents an advance in assistants by providing an end-to-end solution for parsing instructions, reasoning about goals, and executing actions. LLM-based Process Automation (LLMPA) has modules for decomposing instructions, generating descriptions, detecting interface elements, predicting next actions, and error checking. Experiments demonstrate the system completing complex mobile operation tasks in Alipay based on natural language instructions. This showcases how large language models can enable automated assistants to accomplish real-world tasks. The main contributions are the novel LLMPA architecture optimized for app process automation, the methodology for applying LLMs to mobile apps, and demonstrations of multi-step task completion in a real-world environment. Notably, this work represents the first real-world deployment and extensive evaluation of a large language model-based virtual assistant in a widely used mobile application with an enormous user base numbering in the hundreds of millions.},
  archive   = {C_KDD},
  author    = {Guan, Yanchu and Wang, Dong and Chu, Zhixuan and Wang, Shiyu and Ni, Feiyue and Song, Ruihua and Zhuang, Chenyi},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671646},
  pages     = {5018–5027},
  title     = {Intelligent agents with LLM-based process automation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LASCA: A large-scale stable customer segmentation approach
to credit risk assessment. <em>KDD</em>, 5006–5017. (<a
href="https://doi.org/10.1145/3637528.3671550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Customer segmentation plays a crucial role in credit risk assessment by dividing users into specific risk levels based on their credit scores. Previous methods fail to comprehensively consider the stability in the segmentation process, resulting in frequent changes and inconsistencies in users&#39; risk levels over time. This increases potential risks to a company. To this end, this paper at first introduces and formalizes the concept of stability regret in the segmentation process. However, evaluating stability is challenging due to its black-box nature and the computational burden posed by vast user data sets. To address these challenges, this paper proposes a large-scale stable customer segmentation approach named LASCA. LASCA consists of two phases: high-quality dataset construction (HDC) and reliable data-driven optimization (RDO). Specifically, HDC utilizes an evolutionary algorithm to collect high-quality binning solutions. RDO subsequently builds a reliable surrogate model to search for the most stable binning solution based on the collected dataset. Extensive experiments conducted on real-world large-scale datasets (up to 0.8 billion) show that LASCA surpasses the state-of-the-art binning methods in finding the most stable binning solution. Notably, HDC greatly enhances data quality by 50\%. RDO efficiently discovers more stable binning solutions with a 36\% improvement in stability, accelerating the optimization process by 25 times via data-driven evaluation. Currently, LASCA has been successfully deployed in the large-scale credit risk assessment system of Alipay.},
  archive   = {C_KDD},
  author    = {Gu, Yongfeng and Wu, Yupeng and Lu, Huakang and Lu, Xingyu and Qian, Hong and Zhou, Jun and Zhou, Aimin},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671550},
  pages     = {5006–5017},
  title     = {LASCA: A large-scale stable customer segmentation approach to credit risk assessment},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transportation marketplace rate forecast using signature
transform. <em>KDD</em>, 4997–5005. (<a
href="https://doi.org/10.1145/3637528.3671637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Freight transportation marketplace rates are typically challenging to forecast accurately. In this work, we have developed a novel statistical technique based on signature transforms and have built a predictive and adaptive model to forecast these marketplace rates. Our technique is based on two key elements of the signature transform: one being its universal nonlinearity property, which linearizes the feature space and hence translates the forecasting problem into linear regression, and the other being the signature kernel, which allows for comparing computationally efficiently similarities between time series data. Combined, it allows for efficient feature generation and precise identification of seasonality and regime switching in the forecasting process.An algorithm based on our technique has been deployed by Amazon trucking operations, with far superior forecast accuracy and better interpretability versus commercially available industry models, even during the COVID-19 pandemic and the Ukraine conflict. Furthermore, our technique is able to capture the influence of business cycles and the heterogeneity of the marketplace, improving prediction accuracy by more than fivefold, with an estimated annualized saving of 50 million.},
  archive   = {C_KDD},
  author    = {Gu, Haotian and Guo, Xin and Jacobs, Timothy L. and Kaminsky, Philip and Li, Xinyu},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671637},
  pages     = {4997–5005},
  title     = {Transportation marketplace rate forecast using signature transform},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Controllable multi-behavior recommendation for in-game skins
with large sequential model. <em>KDD</em>, 4986–4996. (<a
href="https://doi.org/10.1145/3637528.3671572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Online games often house virtual shops where players can acquire character skins. Our task is centered on tailoring skin recommendations across diverse scenarios by analyzing historical interactions such as clicks, usage, and purchases. Traditional multi-behavior recommendation models employed for this task are limited. They either only predict skins based on a single type of behavior or merely recommend skins for target behavior type/task. These models lack the ability to control predictions of skins that are associated with different scenarios and behaviors. To overcome these limitations, we utilize the pretraining capabilities of Large Sequential Models (LSMs) coupled with a novel stimulus prompt mechanism and build a controllable multi-behavior recommendation (CMBR) model. In our approach, the pretraining ability is used to encapsulate users&#39; multi-behavioral sequences into the representation of users&#39; general interests. Subsequently, our designed stimulus prompt mechanism stimulates the model to extract scenario-related interests, thus generating potential skin purchases (or clicks and other interactions) for users. To the best of our knowledge, this is the first work to provide controlled multi-behavior recommendations, and also the first to apply the pretraining capabilities of LSMs in game domain. Through offline experiments and online A/B tests, we validate our method significantly outperforms baseline models, exhibiting about a tenfold improvement on various metrics during the offline test.},
  archive   = {C_KDD},
  author    = {Gou, Yanjie and Yao, Yuanzhou and Zhang, Zhao and Wu, Yiqing and Hu, Yi and Zhuang, Fuzhen and Liu, Jiangming and Xu, Yongjun},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671572},
  pages     = {4986–4996},
  title     = {Controllable multi-behavior recommendation for in-game skins with large sequential model},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Residual multi-task learner for applied ranking.
<em>KDD</em>, 4974–4985. (<a
href="https://doi.org/10.1145/3637528.3671523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern e-commerce platforms rely heavily on modeling diverse user feedback to provide personalized services. Consequently, multi-task learning has become an integral part of their ranking systems. However, existing multi-task learning methods encounter two main challenges: some lack explicit modeling of task relationships, resulting in inferior performance, while others have limited applicability due to being computationally intensive, having scalability issues, or relying on strong assumptions. To address these limitations and better fit our real-world scenario, pre-rank in Shopee Search, we introduce in this paper ResFlow, a lightweight multi-task learning framework that enables efficient cross-task information sharing via residual connections between corresponding layers of task networks. Extensive experiments on datasets from various scenarios and modalities demonstrate its superior performance and adaptability over state-of-the-art methods. The online A/B tests in Shopee Search showcase its practical value in large-scale industrial applications, evidenced by a 1.29\% increase in OPU (order-per-user) without additional system latency. ResFlow is now fully deployed in the pre-rank module of Shopee Search. To facilitate efficient online deployment, we propose a novel offline metric Weighted Recall@K, which aligns well with our online metric OPU, addressing the longstanding online-offline metric misalignment issue. Besides, we propose to fuse scores from the multiple tasks additively when ranking items, which outperforms traditional multiplicative fusion.},
  archive   = {C_KDD},
  author    = {Fu, Cong and Wang, Kun and Wu, Jiahua and Chen, Yizhou and Huzhang, Guangda and Ni, Yabo and Zeng, Anxiang and Zhou, Zhiming},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671523},
  pages     = {4974–4985},
  title     = {Residual multi-task learner for applied ranking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CHILI: Chemically-informed large-scale inorganic
nanomaterials dataset for advancing graph machine learning.
<em>KDD</em>, 4962–4973. (<a
href="https://doi.org/10.1145/3637528.3671538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Advances in graph machine learning (ML) have been driven by applications in chemistry, as graphs have remained the most expressive representations of molecules. This has led to progress within both fields, as challenging chemical data has helped improve existing methods and to develop new ones. While early graph ML methods focused primarily on small organic molecules, more recently, the scope of graph ML has expanded to include inorganic materials. Modelling the periodicity and symmetry of inorganic crystalline materials poses unique challenges, which existing graph ML methods are unable to immediately address. Moving to inorganic nanomaterials further increases complexity as the scale of number of nodes within each graph can be broad (10 to 100k). In addition, the bulk of existing graph ML focuses on characterising molecules and materials by predicting target properties with graphs as input. The most exciting applications of graph ML will be in their generative capabilities, in order to explore the vast chemical space from a data-driven perspective. Currently, generative modelling of graphs is not at par with other domains such as images or text, as generating chemically valid molecules and materials of varying properties is not straightforward.In this work, we invite the graph ML community to address these open challenges by presenting two new chemically-informed large-scale inorganic (CHILI) nanomaterials datasets. These datasets contain nanomaterials of different scales and properties represented as graphs of varying sizes. The first dataset is a medium-scale dataset (with overall &amp;gt;6M nodes, &amp;gt;49M edges) of mono-metallic oxide nanomaterials generated from 12 selected crystal types (CHILI-3K). This dataset has a narrower chemical scope focused on an interesting part of chemical space with a lot of active research. The second is a large-scale dataset (with overall &amp;gt;183M nodes, &amp;gt;1.2B edges) of nanomaterials generated from experimentally determined crystal structures (CHILI-100K). The crystal structures used in CHILI-100K are obtained from a curated subset of the Crystallography Open Database (COD) and has a broader chemical scope covering database entries for 68 metals and 11 non-metals. We define 11 property prediction tasks covering node-, edge-, and graph- level tasks that span classification and regression.In addition we also define structure prediction tasks, which are of special interest for nanomaterial research. We benchmark the performance of a wide array of baseline methods starting with simple baselines to multiple off-the-shelf graph neural networks. Based on these benchmarking results, we highlight areas which need future work to achieve useful performance for applications in (nano) materials chemistry. To the best of our knowledge, CHILI-3K and CHILI-100K are the first open-source nanomaterial datasets of this scale - both on the individual graph level and of the dataset as a whole - and the only nanomaterials datasets with high structural and elemental diversity.},
  archive   = {C_KDD},
  author    = {Friis-Jensen, Ulrik and Johansen, Frederik L. and Anker, Andy S. and Dam, Erik B. and Jensen, Kirsten M. \O{}. and Selvan, Raghavendra},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671538},
  pages     = {4962–4973},
  title     = {CHILI: Chemically-informed large-scale inorganic nanomaterials dataset for advancing graph machine learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GRILLBot in practice: Lessons and tradeoffs deploying large
language models for adaptable conversational task assistants.
<em>KDD</em>, 4951–4961. (<a
href="https://doi.org/10.1145/3637528.3671622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We tackle the challenge of building real-world multimodal assistants for complex real-world tasks. We describe the practicalities and challenges of developing and deploying GRILLBot, a leading (first and second prize winning in 2022 and 2023) system deployed in the Alexa Prize TaskBot Challenge. Building on our Open Assistant Toolkit (OAT) framework, we propose a hybrid architecture that leverages Large Language Models (LLMs) and specialised models tuned for specific subtasks requiring very low latency. OAT allows us to define when, how and which LLMs should be used in a structured and deployable manner. For knowledge-grounded question answering and live task adaptations, we show that LLM reasoning abilities over task context and world knowledge outweigh latency concerns. For dialogue state management, we implement a code generation approach and show that specialised smaller models have 84\% effectiveness with 100x lower latency. Overall, we provide insights and discuss tradeoffs for deploying both traditional models and LLMs to users in complex real-world multimodal environments in the Alexa TaskBot challenge. These experiences will continue to evolve as LLMs become more capable and efficient -- fundamentally reshaping OAT and future assistant architectures.},
  archive   = {C_KDD},
  author    = {Fischer, Sophie and Gemmell, Carlos and Tecklenburg, Niklas and Mackie, Iain and Rossetto, Federico and Dalton, Jeffrey},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671622},
  pages     = {4951–4961},
  title     = {GRILLBot in practice: Lessons and tradeoffs deploying large language models for adaptable conversational task assistants},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Achieving a better tradeoff in multi-stage recommender
systems through personalization. <em>KDD</em>, 4939–4950. (<a
href="https://doi.org/10.1145/3637528.3671593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recommender systems in social media websites provide value to their communities by recommending engaging content and meaningful connections. Scaling high-quality recommendations to billions of users in real-time requires sophisticated ranking models operating on a vast number of potential items to recommend, becoming prohibitively expensive computationally. A common technique &quot;funnels&#39;&#39; these items through progressively complex models (&quot;multi-stage&#39;&#39;), each ranking fewer items but at higher computational cost for greater accuracy. This architecture introduces a trade-off between the cost of ranking items and providing users with the best recommendations. A key observation we make in this paper is that, all else equal, ranking more items indeed improves the overall objective but has diminishing returns. Following this observation, we provide a rigorous formulation through the framework of DR-submodularity, and argue that for a certain class of objectives (reward functions), it is possible to improve the trade-off between performance and computational cost in multi-stage ranking systems with strong theoretical guarantees. We show that this class of reward functions that provide this guarantee is large and robust to various noise models. Finally, we describe extensive experimentation of our method on three real-world recommender systems in Facebook, achieving 8.8\% reduction in overall compute resources with no significant impact on recommendation quality, compared to a 0.8\% quality loss in a non-personalized budget allocation.},
  archive   = {C_KDD},
  author    = {Evnine, Ariel and Ioannidis, Stratis and Kalimeris, Dimitris and Kalyanaraman, Shankar and Li, Weiwei and Nir, Israel and Sun, Wei and Weinsberg, Udi},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671593},
  pages     = {4939–4950},
  title     = {Achieving a better tradeoff in multi-stage recommender systems through personalization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing e-commerce spelling correction with fine-tuned
transformer models. <em>KDD</em>, 4928–4938. (<a
href="https://doi.org/10.1145/3637528.3671625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the realm of e-commerce, the process of search stands as the primary point of interaction for users, wielding a profound influence on the platform&#39;s revenue generation. Notably, spelling correction assumes a pivotal role in shaping the user&#39;s search experience by rectifying erroneous query inputs, thus facilitating more accurate retrieval outcomes. Within the scope of this research paper, our aim is to enhance the existing state-of-the-art discriminative model performance with generative modelling strategies while concurrently addressing the engineering concerns associated with real-time online latency, inherent to models of this category. We endeavor to refine LSTM-based classification models for spelling correction through a generative fine-tuning approach hinged upon pre-trained language models. Our comprehensive offline assessments have yielded compelling results, showcasing that transformer-based architectures, such as BART (developed by Facebook) and T5 (a product of Google), have achieved a 4\% enhancement in F1 score compared to baseline models for the English language sites. Furthermore, to mitigate the challenges posed by latency, we have incorporated model pruning techniques like no-teacher distillation. We have undertaken the deployment of our model (English only) as an A/B test candidate for real-time e-commerce traffic, encompassing customers from the US and the UK. The model attest to a 100\% successful request service rate within real-time scenarios, with median, 90th percentile, and 99th percentile (p90/p99) latencies comfortably falling below production service level agreements. Notably, these achievements are further reinforced by positive customer engagement, transactional and search page metrics, including a significant reduction in instances of search results page with low or almost zero recall. Moreover, we have also extended our efforts into fine-tuning a multilingual model, which, notably, exhibits substantial accuracy enhancements, amounting to a minimum of 16\%, across four distinct European languages and English.},
  archive   = {C_KDD},
  author    = {Dutta, Arnab and Polushin, Gleb and Zhang, Xiaoshuang and Stein, Daniel},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671625},
  pages     = {4928–4938},
  title     = {Enhancing E-commerce spelling correction with fine-tuned transformer models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FNSPID: A comprehensive financial news dataset in time
series. <em>KDD</em>, 4918–4927. (<a
href="https://doi.org/10.1145/3637528.3671629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Financial market predictions utilize historical data to anticipate future stock prices and market trends. Traditionally, these predictions have focused on the statistical analysis of quantitative factors, such as stock prices, trading volumes, inflation rates, and changes in industrial production. Recent advancements in large language models motivate the integrated financial analysis of both sentiment data, particularly market news, and numerical factors. Nonetheless, this methodology frequently encounters constraints due to the paucity of extensive datasets that amalgamate both quantitative and qualitative sentiment analyses. To address this challenge, we introduce a large-scale financial dataset, namely, Financial News and Stock Price Integration Dataset (FNSPID). It comprises 29.7 million stock prices and 15.7 million time-aligned financial news records for 4,775 S&amp;amp;P500 companies, covering the period from 1999 to 2023, sourced from 4 stock market news websites. We demonstrate that FNSPID excels existing stock market datasets in scale and diversity while uniquely incorporating sentiment information. Through financial analysis experiments on FNSPID, we propose: (1) the dataset&#39;s size and quality significantly boost market prediction accuracy; (2) adding sentiment scores modestly enhances performance on the transformer-based model; (3) a reproducible procedure that can update the dataset. Completed work, code, documentation, and examples are available at this http URL. FNSPID offers unprecedented opportunities for the financial research community to advance predictive modeling and analysis.},
  archive   = {C_KDD},
  author    = {Dong, Zihan and Fan, Xinyu and Peng, Zhiyuan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671629},
  pages     = {4918–4927},
  title     = {FNSPID: A comprehensive financial news dataset in time series},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Time-aware attention-based transformer (TAAT) for cloud
computing system failure prediction. <em>KDD</em>, 4906–4917. (<a
href="https://doi.org/10.1145/3637528.3671547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Log-based failure prediction helps identify and mitigate system failures ahead of time, increasing the reliability of cloud elastic computing systems. However, most existing log-based failure prediction approaches only focus on semantic information, and do not make full use of the information contained in the timestamps of log messages. This paper proposes time-aware attention-based transformer (TAAT), a failure prediction approach that extracts semantic and temporal information simultaneously from log messages and their timestamps. TAAT first tokenizes raw log messages into specific exceptions, and then performs: 1) exception sequence embedding that reorganizes the exceptions of each node as an ordered sequence and converts them to vectors; 2) time relation estimation that computes time relation matrices from the timestamps; and, 3) time-aware attention that computes semantic correlation matrices from the exception sequences and then combines them with time relation matrices. Experiments on Alibaba Cloud demonstrated that TAAT achieves an approximately 10\% performance improvement compared with the state-of-the-art approaches. TAAT is now used in the daily operation of Alibaba Cloud. Moreover, this paper also releases the real-world cloud computing failure prediction dataset used in our study, which consists of about 2.7 billion syslogs from about 300,000 node controllers during a 4-month period. To our knowledge, this is the largest dataset of its kind, and is expected to be very useful to the community.},
  archive   = {C_KDD},
  author    = {Deng, Lingfei and Wang, Yunong and Wang, Haoran and Ma, Xuhua and Du, Xiaoming and Zheng, Xudong and Wu, Dongrui},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671547},
  pages     = {4906–4917},
  title     = {Time-aware attention-based transformer (TAAT) for cloud computing system failure prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MMBee: Live streaming gift-sending recommendations via
multi-modal fusion and behaviour expansion. <em>KDD</em>, 4896–4905. (<a
href="https://doi.org/10.1145/3637528.3671511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Live streaming services are becoming increasingly popular due to real-time interactions and entertainment. Viewers can chat and send comments or virtual gifts to express their preferences for the streamers. Accurately modeling the gifting interaction not only enhances users&#39; experience but also increases streamers&#39; revenue. Previous studies on live streaming gifting prediction treat this task as a conventional recommendation problem, and model users&#39; preferences using categorical data and observed historical behaviors. However, it is challenging to precisely describe the real-time content changes in live streaming using limited categorical information. Moreover, due to the sparsity of gifting behaviors, capturing the preferences and intentions of users is quite difficult. In this work, we propose MMBee based on real-time &amp;lt;u&amp;gt;M&amp;lt;/u&amp;gt;ulti-&amp;lt;u&amp;gt;M&amp;lt;/u&amp;gt;odal Fusion and &amp;lt;u&amp;gt;Be&amp;lt;/u&amp;gt;haviour &amp;lt;u&amp;gt;E&amp;lt;/u&amp;gt;xpansion to address these issues. Specifically, we first present a Multi-modal Fusion Module with Learnable Query (MFQ) to perceive the dynamic content of streaming segments and process complex multi-modal interactions, including images, text comments and speech. To alleviate the sparsity issue of gifting behaviors, we present a novel Graph-guided Interest Expansion (GIE) approach that learns both user and streamer representations on large-scale gifting graphs with multi-modal attributes. It consists of two main parts: graph node representations pre-training and metapath-based behavior expansion, all of which help model jump out of the specific historical gifting behaviors for exploration and largely enrich the behavior representations. Comprehensive experiment results show that MMBee achieves significant performance improvements on both public datasets and Kuaishou real-world streaming datasets and the effectiveness has been further validated through online A/B experiments. MMBee has been deployed and is serving hundreds of millions of users at Kuaishou.},
  archive   = {C_KDD},
  author    = {Deng, Jiaxin and Wang, Shiyao and Wang, Yuchen and Qi, Jiansong and Zhao, Liqin and Zhou, Guorui and Meng, Gaofeng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671511},
  pages     = {4896–4905},
  title     = {MMBee: Live streaming gift-sending recommendations via multi-modal fusion and behaviour expansion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Metric decomposition in a/b tests. <em>KDD</em>, 4885–4895.
(<a href="https://doi.org/10.1145/3637528.3671556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {More than a decade ago, CUPED (Controlled Experiments Utilizing Pre-Experiment Data) mainstreamed the idea of variance reduction leveraging pre-experiment covariates. Since its introduction, it has been implemented, extended, and modernized by major online experimentation platforms. Despite the wide adoption, it is known by practitioners that the variance reduction rate from CUPED utilizing pre-experimental data varies case by case and has a theoretical limit. In theory, CUPED can be extended to augment a treatment effect estimator utilizing in-experiment data, but practical guidance on how to construct such an augmentation is lacking. In this article, we fill this gap by proposing a new direction for sensitivity improvement via treatment effect augmentation whereby a target metric of interest is decomposed into components with high signal-to-noise disparity. Inference in the context of this decomposition is developed using both frequentist and Bayesian theory. We provide three real world applications demonstrating different flavors of metric decomposition; these applications illustrate the gain in agility metric decomposition yields relative to an un-decomposed analysis.},
  archive   = {C_KDD},
  author    = {Deng, Alex and Hagar, Luke and Stevens, Nathaniel T. and Xifara, Tatiana and Gandhi, Amit},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671556},
  pages     = {4885–4895},
  title     = {Metric decomposition in A/B tests},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NudgeRank: Digital algorithmic nudging for personalized
health. <em>KDD</em>, 4873–4884. (<a
href="https://doi.org/10.1145/3637528.3671562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we describe NudgeRankTM, an innovative digital algorithmic nudging system designed to foster positive health behaviors on a population-wide scale. Utilizing a novel combination of Graph Neural Networks augmented with an extensible Knowledge Graph, this Recommender System is operational in production, delivering personalized and context-aware nudges to over 1.1 million care recipients daily. This enterprise deployment marks one of the largest AI-driven health behavior change initiatives, accommodating diverse health conditions and wearable devices. Rigorous evaluation reveals statistically significant improvements in health outcomes, including a 6.17\% increase in daily steps and 7.61\% more exercise minutes. Moreover, user engagement and program enrollment surged, with a 13.1\% open rate compared to baseline systems&#39; 4\%. Demonstrating scalability and reliability, NudgeRankTM operates efficiently on commodity compute resources while maintaining automation and observability standards essential for production systems.},
  archive   = {C_KDD},
  author    = {Chiam, Jodi and Lim, Aloysius and Teredesai, Ankur},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671562},
  pages     = {4873–4884},
  title     = {NudgeRank: Digital algorithmic nudging for personalized health},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MARLP: Time-series forecasting control for agricultural
managed aquifer recharge. <em>KDD</em>, 4862–4872. (<a
href="https://doi.org/10.1145/3637528.3671533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The rapid decline in groundwater around the world poses a significant challenge to sustainable agriculture. To address this issue, agricultural managed aquifer recharge (Ag-MAR) is proposed to recharge the aquifer by artificially flooding agricultural lands using surface water. Ag-MAR requires a carefully selected flooding schedule to avoid affecting the oxygen absorption of crop roots. However, current Ag-MAR scheduling does not take into account complex environmental factors such as weather and soil oxygen, resulting in crop damage and insufficient recharging amounts. This paper proposes MARLP, the first end-to-end data-driven control system for Ag-MAR. We first formulate Ag-MAR as an optimization problem. To that end, we analyze four-year in-field datasets, which reveal the multi-periodicity feature of the soil oxygen level trends and the opportunity to use external weather forecasts and flooding proposals as exogenous clues for soil oxygen prediction. Then, we design a two-stage forecasting framework. In the first stage, it extracts both the cross-variate dependency and the periodic patterns from historical data to conduct preliminary forecasting. In the second stage, it uses weather-soil and flooding-soil causality to facilitate an accurate prediction of soil oxygen levels. Finally, we conduct model predictive control (MPC) for Ag-MAR flooding. To address the challenge of large action spaces, we devise a heuristic planning module to reduce the number of flooding proposals to enable the search for optimal solutions. Real-world experiments show that MARLP reduces the oxygen deficit ratio by 86.8\% while improving the recharging amount in unit time by 35.8\%, compared with the previous four years.},
  archive   = {C_KDD},
  author    = {Chen, Yuning and Yang, Kang and An, Zhiyu and Holder, Brady and Paloutzian, Luke and Bali, Khaled M. and Du, Wan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671533},
  pages     = {4862–4872},
  title     = {MARLP: Time-series forecasting control for agricultural managed aquifer recharge},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RareBench: Can LLMs serve as rare diseases specialists?
<em>KDD</em>, 4850–4861. (<a
href="https://doi.org/10.1145/3637528.3671576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generalist Large Language Models (LLMs), such as GPT-4, have shown considerable promise in various domains, including medical diagnosis. Rare diseases, affecting approximately 300 million people worldwide, often have unsatisfactory clinical diagnosis rates primarily due to a lack of experienced physicians and the complexity of differentiating among many rare diseases. In this context, recent news such as &quot;ChatGPT correctly diagnosed a 4-year-old&#39;s rare disease after 17 doctors failed&quot; underscore LLMs&#39; potential, yet underexplored, role in clinically diagnosing rare diseases. To bridge this research gap, we introduce RareBench, a pioneering benchmark designed to systematically evaluate the capabilities of LLMs on 4 critical dimensions within the realm of rare diseases. Meanwhile, we have compiled the largest open-source dataset on rare disease patients, establishing a benchmark for future studies in this domain. To facilitate differential diagnosis of rare diseases, we develop a dynamic few-shot prompt methodology, leveraging a comprehensive rare disease knowledge graph synthesized from multiple knowledge bases, significantly enhancing LLMs&#39; diagnostic performance. Moreover, we present an exhaustive comparative study of GPT-4&#39;s diagnostic capabilities against those of specialist physicians. Our experimental findings underscore the promising potential of integrating LLMs into the clinical diagnostic process for rare diseases. This paves the way for exciting possibilities in future advancements in this field.},
  archive   = {C_KDD},
  author    = {Chen, Xuanzhong and Mao, Xiaohao and Guo, Qihan and Wang, Lun and Zhang, Shuyang and Chen, Ting},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671576},
  pages     = {4850–4861},
  title     = {RareBench: Can LLMs serve as rare diseases specialists?},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing multi-field B2B cloud solution matching via
contrastive pre-training. <em>KDD</em>, 4839–4849. (<a
href="https://doi.org/10.1145/3637528.3671513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cloud solutions have gained significant popularity in the technology industry as they offer a combination of services and tools to tackle specific problems. However, despite their widespread use, the task of identifying appropriate company customers for a specific target solution to the sales team of a solution provider remains a complex business problem that existing matching systems have yet to adequately address. In this work, we study the B2B solution matching problem and identify two main challenges of this scenario: (1) the modeling of complex multi-field features and (2) the limited, incomplete, and sparse transaction data. To tackle these challenges, we propose a framework CAMA, which is built with a hierarchical multi-field matching structure as its backbone and supplemented by three data augmentation strategies and a contrastive pre-training objective to compensate for the imperfections in the available data. Through extensive experiments on a real-world dataset, we demonstrate that CAMA outperforms several strong baseline matching models significantly. Furthermore, we have deployed our matching framework on a system of Huawei Cloud. Our observations indicate an improvement of about 30\% compared to the previous online model in terms of Conversion Rate (CVR), which demonstrates its great business value.},
  archive   = {C_KDD},
  author    = {Chen, Haonan and Dou, Zhicheng and Hao, Xuetong and Tao, Yunhao and Song, Shiren and Sheng, Zhenli},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671513},
  pages     = {4839–4849},
  title     = {Enhancing multi-field B2B cloud solution matching via contrastive pre-training},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Diffusion model-based mobile traffic generation with open
data for network planning and optimization. <em>KDD</em>, 4828–4838. (<a
href="https://doi.org/10.1145/3637528.3671544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the rapid development of the Fifth Generation Mobile Communication Technology (5G) networks, network planning and optimization have become increasingly crucial. Generating high-fidelity network traffic data can preemptively estimate the network demands of mobile users, which holds potential for network operators to improve network performance. However, the data required by existing generation methods is predominantly inaccessible to the public, resulting in a lack of reproducibility for the models and high deployment costs in practice. In this article, we propose an Open data-based Diffusion model for mobile traffic generation (OpenDiff), where a multi-positive contrastive learning algorithm is designed to construct conditional information for the diffusion model using entirely publicly available satellite remote sensing images, Point of Interest (POI), and population data. The conditional information contains relevant human activities in geographical areas, which can effectively guide the generation of network traffic data. We further design an attention-based fusion mechanism to capture the implicit correlations between network traffic and human activity features, enhancing the model&#39;s controllable generation capability. We conduct evaluations on three different cities with varying scales, where experimental results verify that our proposed model outperforms existing methods by 14.36\% and 13.05\% in terms of generation fidelity and controllability. To further validate the effectiveness of the model, we leverage the generated traffic data to assist the operators with network planning on a real-world network optimization platform of China Mobile Communications Corporation. The source code is available online:https://github.com/impchai/OpenDiff-diffusion-model-with-open-data.},
  archive   = {C_KDD},
  author    = {Chai, Haoye and Jiang, Tao and Yu, Li},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671544},
  pages     = {4828–4838},
  title     = {Diffusion model-based mobile traffic generation with open data for network planning and optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CompanyKG: A large-scale heterogeneous graph for company
similarity quantification. <em>KDD</em>, 4816–4827. (<a
href="https://doi.org/10.1145/3637528.3671515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents CompanyKG (version 2), a large-scale heterogeneous graph developed for fine-grained company similarity quantification and relationship prediction, crucial for applications in the investment industry such as market mapping, competitor analysis, and mergers and acquisitions. CompanyKG comprises 1.17 million companies represented as graph nodes, enriched with company description embeddings, and 51.06 million weighted edges denoting 15 distinct inter-company relations. To facilitate a thorough evaluation of methods for company similarity quantification and relationship prediction, we have created four annotated evaluation tasks: similarity prediction, competitor retrieval, similarity ranking, and edge prediction. We offer extensive benchmarking results for 11 reproducible predictive methods, categorized into three groups: node-only, edge-only, and node+edge. To our knowledge, CompanyKG is the first large-scale heterogeneous graph dataset derived from a real-world investment platform, specifically tailored for quantifying inter-company similarity and relationships.},
  archive   = {C_KDD},
  author    = {Cao, Lele and von Ehrenheim, Vilhelm and Granroth-Wilding, Mark and Anselmo Stahl, Richard and McCornack, Andrew and Catovic, Armin and Cavalcanti Rocha, Dhiana Deva},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671515},
  pages     = {4816–4827},
  title     = {CompanyKG: A large-scale heterogeneous graph for company similarity quantification},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LiRank: Industrial large scale ranking models at LinkedIn.
<em>KDD</em>, 4804–4815. (<a
href="https://doi.org/10.1145/3637528.3671561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present LiRank, a large-scale ranking framework at LinkedIn that brings to production state-of-the-art modeling architectures and optimization methods. We unveil several modeling improvements, including Residual DCN, which adds attention and residual connections to the famous DCNv2 architecture. We share insights into combining and tuning SOTA architectures to create a unified model, including Dense Gating, Transformers and Residual DCN. We also propose novel techniques for calibration and describe how we productionalized deep learning based explore/exploit methods.To enable effective, production-grade serving of large ranking models, we detail how to train and compress models using quantization and vocabulary compression. We provide details about the deployment setup for large-scale use cases of Feed ranking, Jobs Recommendations, and Ads click-through rate (CTR) prediction.We summarize our learnings from various A/B tests by elucidating the most effective technical approaches. These ideas have contributed to relative metrics improvements across the board at LinkedIn: +0.5\% member sessions in the Feed, +1.76\% qualified job applications for Jobs search and recommendations, and +4.3\% for Ads CTR. We hope this work can provide practical insights and solutions for practitioners interested in leveraging large-scale deep ranking systems.},
  archive   = {C_KDD},
  author    = {Borisyuk, Fedor and Zhou, Mingzhou and Song, Qingquan and Zhu, Siyu and Tiwana, Birjodh and Parameswaran, Ganesh and Dangi, Siddharth and Hertel, Lars and Xiao, Qiang Charles and Hou, Xiaochen and Ouyang, Yunbo and Gupta, Aman and Singh, Sheallika and Liu, Dan and Cheng, Hailing and Le, Lei and Hung, Jonathan and Keerthi, Sathiya and Wang, Ruoyan and Zhang, Fengyu and Kothari, Mohit and Zhu, Chen and Sun, Daqi and Dai, Yun and Luan, Xun and Zhu, Sirou and Wang, Zhiwei and Daftary, Neil and Shen, Qianqi and Jiang, Chengming and Wei, Haichao and Varshney, Maneesh and Ghoting, Amol and Ghosh, Souvik},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671561},
  pages     = {4804–4815},
  title     = {LiRank: Industrial large scale ranking models at LinkedIn},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LiGNN: Graph neural networks at LinkedIn. <em>KDD</em>,
4793–4803. (<a href="https://doi.org/10.1145/3637528.3671566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present LiGNN, a deployed large-scale Graph Neural Networks (GNNs) Framework. We share our insight on developing and deployment of GNNs at large scale at LinkedIn. We present a set of algorithmic improvements to the quality of GNN representation learning including temporal graph architectures with long term losses, effective cold start solutions via graph densification, ID embeddings and multi-hop neighbor sampling. We explain how we built and sped up by 7x our large-scale training on LinkedIn graphs with adaptive sampling of neighbors, grouping and slicing of training data batches, specialized shared-memory queue and local gradient optimization. We summarize our deployment lessons and learnings gathered from A/B test experiments. The techniques presented in this work have contributed to an approximate relative improvements of 1\% of Job application hearing back rate, 2\% Ads CTR lift, 0.5\% of Feed engaged daily active users, 0.2\% session lift and 0.1\% weekly active user lift from people recommendation. We believe that this work can provide practical solutions and insights for engineers who are interested in applying Graph neural networks at large scale.},
  archive   = {C_KDD},
  author    = {Borisyuk, Fedor and He, Shihai and Ouyang, Yunbo and Ramezani, Morteza and Du, Peng and Hou, Xiaochen and Jiang, Chengming and Pasumarthy, Nitin and Bannur, Priya and Tiwana, Birjodh and Liu, Ping and Dangi, Siddharth and Sun, Daqi and Pei, Zhoutao and Shi, Xiao and Zhu, Sirou and Shen, Qianqi and Lee, Kuang-Hsuan and Stein, David and Li, Baolei and Wei, Haichao and Ghoting, Amol and Ghosh, Souvik},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671566},
  pages     = {4793–4803},
  title     = {LiGNN: Graph neural networks at LinkedIn},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large scale generative AI text applied to sports and music.
<em>KDD</em>, 4784–4792. (<a
href="https://doi.org/10.1145/3637528.3671542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the problem of scaling up the production of media content, including commentary and personalized news stories, for large-scale sports and music events worldwide. Our approach relies on generative AI models to transform a large volume of multimodal data (e.g., videos, articles, real-time scoring feeds, statistics, and fact sheets) into coherent and fluent text. Based on this approach, we introduce, for the first time, an AI commentary system, which was deployed to produce automated narrations for highlight packages at the 2023 US Open, Wimbledon, and Masters tournaments. In the same vein, our solution was extended to create personalized content for ESPN Fantasy Football and stories about music artists for the GRAMMY awards. These applications were built using a common software architecture achieved a 15x speed improvement with an average Rouge-L of 82.00 and perplexity of 6.6. Our work was successfully deployed at the aforementioned events, supporting 90 million fans around the world with 8 billion page views, continuously pushing the bounds on what is possible at the intersection of sports, entertainment, and AI.},
  archive   = {C_KDD},
  author    = {Baughman, Aaron and Morales, Eduardo and Agarwal, Rahul and Akay, Gozde and Feris, Rogerio and Johnson, Tony and Hammer, Stephen and Karlinsky, Leonid},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671542},
  pages     = {4784–4792},
  title     = {Large scale generative AI text applied to sports and music},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GradCraft: Elevating multi-task recommendations through
holistic gradient crafting. <em>KDD</em>, 4774–4783. (<a
href="https://doi.org/10.1145/3637528.3671585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recommender systems require the simultaneous optimization of multiple objectives to accurately model user interests, necessitating the application of multi-task learning methods. However, existing multi-task learning methods in recommendations overlook the specific characteristics of recommendation scenarios, falling short in achieving proper gradient balance. To address this challenge, we set the target of multi-task learning as attaining the appropriate magnitude balance and the global direction balance, and propose an innovative methodology named GradCraft in response. GradCraft dynamically adjusts gradient magnitudes to align with the maximum gradient norm, mitigating interference from gradient magnitudes for subsequent manipulation. It then employs projections to eliminate gradient conflicts in directions while considering all conflicting tasks simultaneously, theoretically guaranteeing the global resolution of direction conflicts. GradCraft ensures the concurrent achievement of appropriate magnitude balance and global direction balance, aligning with the inherent characteristics of recommendation scenarios. Both offline and online experiments attest to the efficacy of GradCraft in enhancing multi-task performance in recommendations. The source code for GradCraft can be accessed at https://github.com/baiyimeng/GradCraft.},
  archive   = {C_KDD},
  author    = {Bai, Yimeng and Zhang, Yang and Feng, Fuli and Lu, Jing and Zang, Xiaoxue and Lei, Chenyi and Song, Yang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671585},
  pages     = {4774–4783},
  title     = {GradCraft: Elevating multi-task recommendations through holistic gradient crafting},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DDCDR: A disentangle-based distillation framework for
cross-domain recommendation. <em>KDD</em>, 4764–4773. (<a
href="https://doi.org/10.1145/3637528.3671605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern recommendation platforms frequently encompass multiple domains to cater to the varied preferences of users. Recently, cross-domain learning has gained traction as a significant paradigm within the context of recommendation systems, enabling the leveraging of rich information from a well-endowed source domain to enhance a target domain, often limited by inadequate data resources. A primary concern in cross-domain recommendation is the mitigation of negative transfer-ensuring the selective transference of pertinent knowledge from the source (domain-shared knowledge) while maintaining the integrity of domain-unique insights within the target domain (domain-specific knowledge). In this paper, we propose a novel Disentangle-based Distillation Framework for Cross-Domain Recommendation (DDCDR), designed to operate at the representational level and rooted in the established teacher-student knowledge distillation paradigm. Our methodology begins with the development of a cross-domain teacher model, trained adversarially alongside a domain discriminator. This is followed by the creation of a target domain-specific student model. By employing the trained domain discriminator, we successfully segregate domain-shared from domain-specific representations. The teacher model guides the learning of domain-shared features, while domain-specific features are enhanced via contrastive learning methods. Experiments conducted on both public datasets and an industrial dataset demonstrate DDCDR achieves a new state-of-the-art performance. The implementation within Ant Group&#39;s platform further confirms its online efficacy, manifesting relative improvements of 0.33\% and 0.45\% in Unique Visitor Click-Through Rate (UVCTR) across two distinct recommendation scenarios, compared to baseline performances.},
  archive   = {C_KDD},
  author    = {An, Zhicheng and Gu, Zhexu and Yu, Li and Tu, Ke and Wu, Zhengwei and Hu, Binbin and Zhang, Zhiqiang and Gu, Lihong and Gu, Jinjie},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671605},
  pages     = {4764–4773},
  title     = {DDCDR: A disentangle-based distillation framework for cross-domain recommendation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Television discourse decoded: Comprehensive multimodal
analytics at scale. <em>KDD</em>, 4752–4763. (<a
href="https://doi.org/10.1145/3637528.3671532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we tackle the complex task of analyzing televised debates, with a focus on a prime time news debate show from India. Previous methods, which often relied solely on text, fall short in capturing the multimodal essence of these debates [27]. To address this gap, we introduce a comprehensive automated toolkit that employs advanced computer vision and speech-to-text techniques for large-scale multimedia analysis. Utilizing state-of-the-art computer vision algorithms and speech-to-text methods, we transcribe, diarize, and analyze thousands of YouTube videos of a prime-time television debate show in India. These debates are a central part of Indian media but have been criticized for compromised journalistic integrity and excessive dramatization [18]. Our toolkit provides concrete metrics to assess bias and incivility, capturing a comprehensive multimedia perspective that includes text, audio utterances, and video frames. Our findings reveal significant biases in topic selection and panelist representation, along with alarming levels of incivility. This work offers a scalable, automated approach for future research in multimedia analysis, with profound implications for the quality of public discourse and democratic debate. To catalyze further research in this area, we also release the code, dataset collected and supplemental pdf.1},
  archive   = {C_KDD},
  author    = {Agarwal, Anmol and Priyadarshi, Pratyush and Sinha, Shiven and Gupta, Shrey and Jangra, Hitkul and Kumaraguru, Ponnurangam and Garimella, Kiran},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671532},
  pages     = {4752–4763},
  title     = {Television discourse decoded: Comprehensive multimodal analytics at scale},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic pricing for multi-retailer delivery platforms with
additive deep learning and evolutionary optimization. <em>KDD</em>,
4741–4751. (<a href="https://doi.org/10.1145/3637528.3671634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dynamic Pricing for online retail has been discussed extensively in literature. However, past solutions fell short of addressing the unique challenges of independent multi-retailer platforms for grocery delivery. From limited visibility of retailers&#39; inventories to diverse demand-side dynamics across retail brands and locations, the highly decentralized nature of multi-retailer platforms deviates from the classical framework of modeling price elasticity and cross-elasticity of demand. In this paper, we present a novel scheme to scalable and practical price adjustment in the highly dynamic multi-retailer context. First, we present a deep learning framework to distinctly model complex cross-elasticity relationships via additive neural networks augmented with adversarial data. Second, we present evolutionary optimization agents for adjusting itemized prices in a location-decentralized manner, while adhering to custom business constraints and objectives. The optimization utilizes the genetic algorithm structure, where we introduce a potential mechanism, inspired by bandit algorithms, in order to improve convergence speed by managing exploitation and exploration trade-offs. Our solution is deployed at Shipt and is extendable to other types of multi-retailer platforms, such as restaurant delivery. Finally, we empirically demonstrate performance using public and industry datasets of hundreds and thousands of diverse products across tens of stores, offering an optimization targets coverage scale in the tens of thousands, far larger than experimental setups in past research.},
  archive   = {C_KDD},
  author    = {Abdulaal, Ahmed and Polat, Ali and Narayan, Hari and Zeng, Wenrong and Yi, Yimin},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671634},
  pages     = {4741–4751},
  title     = {Dynamic pricing for multi-retailer delivery platforms with additive deep learning and evolutionary optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Generative AI in e-commerce: What can we expect?
<em>KDD</em>, 4739–4740. (<a
href="https://doi.org/10.1145/3637528.3672503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The impact of generative AI on e-commerce is profound. It has significantly improved the understanding of user intent and serves as a comprehensive product knowledge graph. However, the most substantial disruptions are yet to come, partic- ularly through the rise of autonomous agents. In this talk, I will outline a tentative path toward a future where e-commerce not only offers an unparalleled customer experience but also thrives in a world dominated by generative AI and autonomous agents.},
  archive   = {C_KDD},
  author    = {Wang, Haixun},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672503},
  pages     = {4739–4740},
  title     = {Generative AI in E-commerce: What can we expect?},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable graph learning for your enterprise. <em>KDD</em>,
4737–4738. (<a href="https://doi.org/10.1145/3637528.3672501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Much of the world&#39;s most valued data is stored in relational databases and data warehouses, where the data is organized into many tables connected by primary-foreign key relations. However, building machine learning models using this data is both challenging and time consuming. The core problem is that no machine learning method is capable of learning on multiple tables interconnected by primary-foreign key relations. Current methods can only learn from a single table, so the data must first be manually joined and aggregated into a single training table, the process known as feature engineering. Feature engineering is slow, error prone and leads to suboptimal models. At Kumo.ai we have worked with researchers worldwide to develop an end-to-end deep representation learning approach to directly learn on data laid out across multiple tables [1]. We name our approach Relational Deep Learning (RDL). The core idea is to view relational databases as a temporal, heterogeneous graph, with a node for each row in each table, and edges specified by primary-foreign key links. Message Passing Graph Neural Networks can then automatically learn across the graph to extract representations that leverage all input data, without any manual feature engineering. Our relational deep learning method to encode graph structure into low-dimensional embeddings brings several benefits: (1) automatic learning from the entire data spread across multiple relational tables (2) no manual feature engineering as the system learns optimal embeddings for a target problem; (3) state-of-the-art predictive performance.},
  archive   = {C_KDD},
  author    = {Raghavan, Hema},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672501},
  pages     = {4737–4738},
  title     = {Scalable graph learning for your enterprise},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning for clinical management: From the lab to
the hospital. <em>KDD</em>, 4736. (<a
href="https://doi.org/10.1145/3637528.3672497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Population aging, increasing social demands, and rising costs of treatments are stressing healthcare systems to the point of risking the sustainability of universal and accessible healthcare. A hope in this dismal panorama is that there are large inefficiencies, and so opportunities for getting more from the same resources. To name a few, avoidable hospitalizations, unnecessary medication and tests, and lack of coordination among healthcare agents are estimated to cost several hundred billion euros per year in the EU. Technology can be useful for locating and reducing these inefficiencies, and within technology, the full exploitation of the data that the system collects to record its activity. In this talk, I will review the case for activity data analytics in healthcare, with two main considerations: 1) The need to include information about resources and costs in the models, in addition to clinical knowledge and patient outcomes, and 2) the need to use mostly data that healthcare organizations already collect and is not locked and distributed in silos. Fortunately, data collected for administrative and billing purposes, even though imperfect, partial, and low resolution, can be used to improve efficiency and safety, as well as fairness and equity.I will focus on the work carried out at Amalfi Analytics, a spin-off of my research group at UPC in Barcelona. On the one hand, we have addressed predictive management in hospitals, from influx to the emergency room to availability of surgical areas, beds, and staff. Anticipating activity, needs, and resource availability lets managers improve critical KPIs, e.g. waiting times, but also reduce staff stress, which leads to fewer medical errors and accidents. On the other hand, we have developed a patient cohort analyzer, based mostly on a recent clustering algorithm, that gives experts a fresh view of their patient population and lets them refine protocols and identify high-risk patient groups. This tool has also been used to support territorial planning and resource allocation.These problems have been extensively addressed in the past, but actual penetration of solutions in hospitals is smaller than one could expect. For example, one can find hundreds of papers on predicting influx to emergency rooms or bed demands, but many of them conclude after producing an AUC figure, and even fewer describe a working system that can be exported from the hospital where they were developed to others at an affordable cost. I will describe the approach taken at Amalfi so that hospitals can have such a solution up and running in a few days of work for their IT departments, in what I think is an interesting combination of software engineering and automatic Machine Learning.},
  archive   = {C_KDD},
  author    = {Gavald\`{a}, Ricard},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672497},
  pages     = {4736},
  title     = {Machine learning for clinical management: From the lab to the hospital},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Next-generation intelligent assistants for wearable devices.
<em>KDD</em>, 4735. (<a
href="https://doi.org/10.1145/3637528.3672500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An intelligent assistant shall be an agent that knows you and the world, can receive your requests or predict your needs, and provide you the right services at the right time with your permission. As smart devices such as Amazon Alexa, Google Home, Ray-ban Meta get popular, Intelligent Assistants are gradually playing an important role in people&#39;s lives. The Emergence of wearable devices brings more opportunities and calls for the next generation of Intelligent Assistants. In this talk, we discuss the many challenges and opportunities we face to grow intelligent assistants from voice-only to multi-modal, from context-agnostic to context-aware, from listening to the users&#39; requests to predicting the user&#39;s needs, and from server-side to on-device. We describe our LLM-based solutions toward multi-modality, contextualization, personalization, and retrieval-augmentation, and discuss how they are enabled on devices. We expect these new challenges to open doors to new research areas and start a new chapter for providing personal assistance services},
  archive   = {C_KDD},
  author    = {Dong, Xin Luna},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672500},
  pages     = {4735},
  title     = {Next-generation intelligent assistants for wearable devices},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lessons learned while running ML models in harsh
environments. <em>KDD</em>, 4734. (<a
href="https://doi.org/10.1145/3637528.3672499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Once a very large payment processor client told us: &#39;if we are down for 5 minutes, we open the evening news - so don&#39;t screw up&#39;. Processing billions of dollars per day, many financial institutions, need to continuously fight organized crime in the form of transaction fraud, stolen cards, anti-money laundering, account opening fraud, impersonations scams, phishing, and many other exotic and ever changing attacks from organized crime groups worldwide. In fact, it is estimated that in 2023 the global losses in fraud scams and bank fraud reached 485.6 billion. However, in addition to having very good detection rates and very low false positive rates, financial institutions also need to maintain very high availability rates, very low latencies, very high throughputs, automatic fault tolerance, auto scale up and down, and more. In this talk we cover some lessons related to running ML models in harsh, mission critical environments. We describe data issues, scale issues, ethical issues, system issues, security issues, compliance issues, business and regulation issues, and some architectural tradeoffs and architectural evolutions.},
  archive   = {C_KDD},
  author    = {Bizarro, Pedro},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672499},
  pages     = {4734},
  title     = {Lessons learned while running ML models in harsh environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Repeat-aware neighbor sampling for dynamic graph learning.
<em>KDD</em>, 4722–4733. (<a
href="https://doi.org/10.1145/3637528.3672001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dynamic graph learning equips the edges with time attributes and allows multiple links between two nodes, which is a crucial technology for understanding evolving data scenarios like traffic prediction and recommendation systems. Existing works obtain the evolving patterns mainly depending on the most recent neighbor sequences. However, we argue that whether two nodes will have interaction with each other in the future is highly correlated with the same interaction that happened in the past. Only considering the recent neighbors overlooks the phenomenon of repeat behavior and fails to accurately capture the temporal evolution of interactions. To fill this gap, this paper presents RepeatMixer, which considers evolving patterns of first and high-order repeat behavior in the neighbor sampling strategy and temporal information learning. Firstly, we define the first-order repeat-aware nodes of the source node as the destination nodes that have interacted historically and extend this concept to high orders as nodes in the destination node&#39;s high-order neighbors. Then, we extract neighbors of the source node that interacted before the appearance of repeat-aware nodes with a slide window strategy as its neighbor sequence. Next, we leverage both the first and high-order neighbor sequences of source and destination nodes to learn temporal patterns of interactions via an MLP-based encoder. Furthermore, considering the varying temporal patterns on different orders, we introduce a time-aware aggregation mechanism that adaptively aggregates the temporal representations from different orders based on the significance of their interaction time sequences. Experimental results demonstrate the superiority of RepeatMixer over state-of-the-art models in link prediction tasks, underscoring the effectiveness of the proposed repeat-aware neighbor sampling strategy.},
  archive   = {C_KDD},
  author    = {Zou, Tao and Mao, Yuhao and Ye, Junchen and Du, Bowen},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672001},
  pages     = {4722–4733},
  title     = {Repeat-aware neighbor sampling for dynamic graph learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MacroHFT: Memory augmented context-aware reinforcement
learning on high frequency trading. <em>KDD</em>, 4712–4721. (<a
href="https://doi.org/10.1145/3637528.3672064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {High-frequency trading (HFT) that executes algorithmic trading in short time scales, has recently occupied the majority of cryptocurrency market. Besides traditional quantitative trading methods, reinforcement learning (RL) has become another appealing approach for HFT due to its terrific ability of handling high-dimensional financial data and solving sophisticated sequential decision-making problems, e.g., hierarchical reinforcement learning (HRL) has shown its promising performance on second-level HFT by training a router to select only one sub-agent from the agent pool to execute the current transaction. However, existing RL methods for HFT still have some defects: 1) standard RL-based trading agents suffer from the overfitting issue, preventing them from making effective policy adjustments based on financial context; 2) due to the rapid changes in market conditions, investment decisions made by an individual agent are usually one-sided and highly biased, which might lead to significant loss in extreme markets. To tackle these problems, we propose a novel Memory Augmented Context-aware Reinforcement learning method On HFT, a.k.a. MacroHFT, which consists of two training phases: 1) we first train multiple types of sub-agents with the market data decomposed according to various financial indicators, specifically market trend and volatility, where each agent owns a conditional adapter to adjust its trading policy according to market conditions; 2) then we train a hyper-agent to mix the decisions from these sub-agents and output a consistently profitable meta-policy to handle rapid market fluctuations, equipped with a memory mechanism to enhance the capability of decision-making. Extensive experiments on various cryptocurrency markets demonstrate that MacroHFT can achieve state-of-the-art performance on minute-level trading tasks. Code has been released in https://github.com/ZONG0004/MacroHFT.},
  archive   = {C_KDD},
  author    = {Zong, Chuqiao and Wang, Chaojie and Qin, Molei and Feng, Lei and Wang, Xinrun and An, Bo},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672064},
  pages     = {4712–4721},
  title     = {MacroHFT: Memory augmented context-aware reinforcement learning on high frequency trading},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Topology-monitorable contrastive learning on dynamic graphs.
<em>KDD</em>, 4700–4711. (<a
href="https://doi.org/10.1145/3637528.3671777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph contrastive learning is a representative self-supervised graph learning that has demonstrated excellent performance in learning node representations. Despite the extensive studies on graph contrastive learning models, most existing models are tailored to static graphs, hindering their application to real-world graphs which are often dynamically evolving. Directly applying these models to dynamic graphs brings in severe efficiency issues in repetitively updating the learned embeddings. To address this challenge, we propose IDOL, a novel contrastive learning framework for dynamic graph representation learning. IDOL conducts the graph propagation process based on a specially designed Personalized PageRank algorithm which can capture the topological changes incrementally. This effectively eliminates heavy recomputation while maintaining high learning quality. Our another main design is a topology-monitorable sampling strategy which lays the foundation of graph contrastive learning. We further show that the design in IDOL achieves a desired performance guarantee. Our experimental results on multiple dynamic graphs show that IDOL outperforms the strongest baselines on node classification tasks in various performance metrics.},
  archive   = {C_KDD},
  author    = {Zhu, Zulun and Wang, Kai and Liu, Haoyu and Li, Jintang and Luo, Siqiang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671777},
  pages     = {4700–4711},
  title     = {Topology-monitorable contrastive learning on dynamic graphs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). One fits all: Learning fair graph neural networks for
various sensitive attributes. <em>KDD</em>, 4688–4699. (<a
href="https://doi.org/10.1145/3637528.3672029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent studies have highlighted fairness issues in Graph Neural Networks (GNNs), where they produce discriminatory predictions against specific protected groups categorized by sensitive attributes such as race and age. While various efforts to enhance GNN fairness have made significant progress, these approaches are often tailored to specific sensitive attributes. Consequently, they necessitate retraining the model from scratch to accommodate changes in the sensitive attribute requirement, resulting in high computational costs. To gain deeper insights into this issue, we approach the graph fairness problem from a causal modeling perspective, where we identify the confounding effect induced by the sensitive attribute as the underlying reason. Motivated by this observation, we formulate the fairness problem in graphs from an invariant learning perspective, which aims to learn invariant representations across environments. Accordingly, we propose a graph fairness framework based on invariant learning, namely FairINV, which enables the training of fair GNNs to accommodate various sensitive attributes within a single training session. Specifically, FairINV incorporates sensitive attribute partition and trains fair GNNs by eliminating spurious correlations between the label and various sensitive attributes. Experimental results on several real-world datasets demonstrate that FairINV significantly outperforms state-of-the-art fairness approaches, underscoring its effectiveness. Our code is available via: https://github.com/ZzoomD/FairINV/.},
  archive   = {C_KDD},
  author    = {Zhu, Yuchang and Li, Jintang and Bian, Yatao and Zheng, Zibin and Chen, Liang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672029},
  pages     = {4688–4699},
  title     = {One fits all: Learning fair graph neural networks for various sensitive attributes},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ControlTraj: Controllable trajectory generation with
topology-constrained diffusion model. <em>KDD</em>, 4676–4687. (<a
href="https://doi.org/10.1145/3637528.3671866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generating trajectory data is among promising solutions to addressing privacy concerns, collection costs, and proprietary restrictions usually associated with human mobility analyses. However, existing trajectory generation methods are still in their infancy due to the inherent diversity and unpredictability of human activities, grappling with issues such as fidelity, flexibility, and generalizability. To overcome these obstacles, we propose ControlTraj, a Controllable Trajectory generation framework with the topology-constrained diffusion model. Distinct from prior approaches, ControlTraj utilizes a diffusion model to generate high-fidelity trajectories while integrating the structural constraints of road network topology to guide the geographical outcomes. Specifically, we develop a novel road segment autoencoder to extract fine-grained road segment embedding. The encoded features, along with trip attributes, are subsequently merged into the proposed geographic denoising UNet architecture, named GeoUNet, to synthesize geographic trajectories from white noise. Through experimentation across three real-world data settings, ControlTraj demonstrates its ability to produce human-directed, high-fidelity trajectory generation with adaptability to unexplored geographical contexts.},
  archive   = {C_KDD},
  author    = {Zhu, Yuanshao and Yu, James Jianqiao and Zhao, Xiangyu and Liu, Qidong and Ye, Yongchao and Chen, Wei and Zhang, Zijian and Wei, Xuetao and Liang, Yuxuan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671866},
  pages     = {4676–4687},
  title     = {ControlTraj: Controllable trajectory generation with topology-constrained diffusion model},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed thresholded counting with limited interaction.
<em>KDD</em>, 4664–4675. (<a
href="https://doi.org/10.1145/3637528.3671868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Problems in the area of distributed computing have been extensively studied. In this paper, we focus on the Distributed Thresholded Counting problem in the coordinator model. In this problem, we have k sites holding their input and communicating with a central coordinator. The coordinator&#39;s task is to determine whether the sum of inputs is larger than a threshold. While the communication complexity of this basic problem has been studied for decades, it is still not well understood. Our work considers the worst-case communication cost for an algorithm that uses limited interaction - i.e. a bounded number of rounds of communication. Algorithms in previous research usually need O(\l{}og\l{}og N) or O(k) rounds. In comparison, in the deterministic case, our algorithm achieves optimal communication complexity in only α(k) rounds, where α(k) denotes the inverse Ackermann function and is nearly constant. We also give a randomized algorithm that balances communication, rounds, and error probability.},
  archive   = {C_KDD},
  author    = {Zhu, Xiaoyi and Tian, Yuxiang and Huang, Zengfeng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671868},
  pages     = {4664–4675},
  title     = {Distributed thresholded counting with limited interaction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Propagation structure-aware graph transformer for robust and
interpretable fake news detection. <em>KDD</em>, 4652–4663. (<a
href="https://doi.org/10.1145/3637528.3672024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The rise of social media has intensified fake news risks, prompting a growing focus on leveraging graph learning methods such as graph neural networks (GNNs) to understand post-spread patterns of news. However, existing methods often produce less robust and interpretable results as they assume that all information within the propagation graph is relevant to the news item, without adequately eliminating noise from engaged users. Furthermore, they inadequately capture intricate patterns inherent in long-sequence dependencies of news propagation due to their use of shallow GNNs aimed at avoiding the over-smoothing issue, consequently diminishing their overall accuracy. In this paper, we address these issues by proposing the Propagation Structure-aware Graph Transformer (PSGT). Specifically, to filter out noise from users within propagation graphs, PSGT first designs a noise-reduction self-attention mechanism based on the information bottleneck principle, aiming to minimize or completely remove the noise attention links among task-irrelevant users. Moreover, to capture multi-scale propagation structures while considering long-sequence features, we present a novel relational propagation graph as a position encoding for the graph Transformer, enabling the model to capture both propagation depth and distance relationships of users. Extensive experiments demonstrate the effectiveness, interpretability, and robustness of our PSGT.},
  archive   = {C_KDD},
  author    = {Zhu, Junyou and Gao, Chao and Yin, Ze and Li, Xianghua and Kurths, Juergen},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672024},
  pages     = {4652–4663},
  title     = {Propagation structure-aware graph transformer for robust and interpretable fake news detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic hotel pricing at online travel platforms: A
popularity and competitiveness aware demand learning approach.
<em>KDD</em>, 4641–4651. (<a
href="https://doi.org/10.1145/3637528.3671921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dynamic pricing, which suggests the optimal prices based on the dynamic demands, has received considerable attention in academia and industry. On online hotel booking platforms, room demand fluctuates due to various factors, notably hotel popularity and competition. In this paper, we propose a dynamic pricing approach with popularity and competitiveness-aware demand learning. Specifically, we introduce a novel demand function that incorporates popularity and competitiveness coefficients to comprehensively model the price elasticity of demand. We develop a dynamic demand prediction network that focuses on learning these coefficients in the proposed demand function, enhancing the interpretability and accuracy of price suggestion. The model is trained in a multi-task framework that effectively leverages the correlations of demands among groups of similar hotels to alleviate data sparseness in room-level occupancy prediction. Comprehensive experiments conducted on real-world datasets validate the superiority of our method over state-of-the-art baselines in both demand prediction and dynamic pricing. Our model has been successfully deployed on a popular online travel platform, serving tens of millions of users and hoteliers.},
  archive   = {C_KDD},
  author    = {Zhu, Fanwei and Xiao, Wendong and Yu, Yao and Liu, Zemin and Chen, Zulong and Cai, Weibin},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671921},
  pages     = {4641–4651},
  title     = {Dynamic hotel pricing at online travel platforms: A popularity and competitiveness aware demand learning approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural collapse anchored prompt tuning for generalizable
vision-language models. <em>KDD</em>, 4631–4640. (<a
href="https://doi.org/10.1145/3637528.3671690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large-scale vision-language (V-L) models have demonstrated remarkable generalization capabilities for downstream tasks through prompt tuning. However, the mechanisms behind the learned text representations are unknown, limiting further generalization gains, and the limitations are more severe when faced with the prevalent class imbalances seen in web-sourced datasets. Recent advances in the neural collapse (NC) phenomenon of vision-only models suggest that the optimal representation structure is the simplex ETF, which paves the way to study representations in V-L models. In this paper, we make the first attempt to use NC for examining the representations in V-L models via prompt tuning. It is found that NC optimality of text-to-image representations shows a positive correlation with downstream generalizability, which is more severe under class imbalance settings. To improve the representations, we propose Neural-collapse-anchored Prompt Tuning (NPT), a novel method that learns prompts with text and image representations that satisfy the same simplex Equiangular Tight Frame (ETF). NPT incorporates two regularization terms: language-modality collapse and multi-modality isomorphism; and it is compatible with other prompt tuning methods. Extensive experiments show that NPT can consistently help to improve existing prompt tuning techniques across 11 datasets for both balanced and imbalanced settings.},
  archive   = {C_KDD},
  author    = {Zhu, Didi and Li, Zexi and Zhang, Min and Yuan, Junkun and Liu, Jiashuo and Kuang, Kun and Wu, Chao},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671690},
  pages     = {4631–4640},
  title     = {Neural collapse anchored prompt tuning for generalizable vision-language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CURLS: Causal rule learning for subgroups with significant
treatment effect. <em>KDD</em>, 4619–4630. (<a
href="https://doi.org/10.1145/3637528.3671951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In causal inference, estimating heterogeneous treatment effects (HTE) is critical for identifying how different subgroups respond to interventions, with broad applications in fields such as precision medicine and personalized advertising. Although HTE estimation methods aim to improve accuracy, how to provide explicit subgroup descriptions remains unclear, hindering data interpretation and strategic intervention management. In this paper, we propose CURLS, a novel rule learning method leveraging HTE, which can effectively describe subgroups with significant treatment effects. Specifically, we frame causal rule learning as a discrete optimization problem, finely balancing treatment effect with variance and considering the rule interpretability. We design an iterative procedure based on the minorize-maximization algorithm and solve a submodular lower bound as an approximation for the original. Quantitative experiments and qualitative case studies verify that compared with state-of-the-art methods, CURLS can find subgroups where the estimated and true effects are 16.1\% and 13.8\% higher and the variance is 12.0\% smaller, while maintaining similar or better estimation accuracy and rule interpretability. Code is available at https://osf.io/zwp2k/.},
  archive   = {C_KDD},
  author    = {Zhou, Jiehui and Yang, Linxiao and Liu, Xingyu and Gu, Xinyue and Sun, Liang and Chen, Wei},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671951},
  pages     = {4619–4630},
  title     = {CURLS: Causal rule learning for subgroups with significant treatment effect},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Synthesizing multimodal electronic health records via
predictive diffusion models. <em>KDD</em>, 4607–4618. (<a
href="https://doi.org/10.1145/3637528.3671836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Synthesizing electronic health records (EHR) data has become a preferred strategy to address data scarcity, improve data quality, and model fairness in healthcare. However, existing approaches for EHR data generation predominantly rely on state-of-the-art generative techniques like generative adversarial networks, variational autoencoders, and language models. These methods typically replicate input visits, resulting in inadequate modeling of temporal dependencies between visits and overlooking the generation of time information, a crucial element in EHR data. Moreover, their ability to learn visit representations is limited due to simple linear mapping functions, thus compromising generation quality. To address these limitations, we propose a novel EHR data generation model called EHRPD. It is a diffusion-based model designed to predict the next visit based on the current one while also incorporating time interval estimation. To enhance generation quality and diversity, we introduce a novel time-aware visit embedding module and a pioneering predictive denoising diffusion probabilistic model (P-DDPM). Additionally, we devise a predictive U-Net (PU-Net) to optimize P-DDPM. We conduct experiments on two public datasets and evaluate EHRPD from fidelity, privacy, and utility perspectives. The experimental results demonstrate the efficacy and utility of the proposed EHRPD in addressing the aforementioned limitations and advancing EHR data generation.},
  archive   = {C_KDD},
  author    = {Zhong, Yuan and Wang, Xiaochen and Wang, Jiaqi and Zhang, Xiaokun and Wang, Yaqing and Huai, Mengdi and Xiao, Cao and Ma, Fenglong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671836},
  pages     = {4607–4618},
  title     = {Synthesizing multimodal electronic health records via predictive diffusion models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient and effective implicit dynamic graph neural
network. <em>KDD</em>, 4595–4606. (<a
href="https://doi.org/10.1145/3637528.3672026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Implicit graph neural networks have gained popularity in recent years as they capture long-range dependencies while improving predictive performance in static graphs. Despite the tussle between performance degradation due to the oversmoothing of learned embeddings and long-range dependency being more pronounced in dynamic graphs, as features are aggregated both across neighborhood and time, no prior work has proposed an implicit graph neural model in a dynamic setting.In this paper, we present Implicit Dynamic Graph Neural Network (IDGNN) a novel implicit neural network for dynamic graphs which is the first of its kind. A key characteristic of IDGNN is that it demonstrably is well-posed, i.e., it is theoretically guaranteed to have a fixed-point representation. We then demonstrate that the standard iterative algorithm often used to train implicit models is computationally expensive in our dynamic setting as it involves computing gradients, which themselves have to be estimated in an iterative manner. To overcome this, we pose an equivalent bilevel optimization problem and propose an efficient single-loop training algorithm that avoids iterative computation by maintaining moving averages of key components of the gradients. We conduct extensive experiments on real-world datasets on both classification and regression tasks to demonstrate the superiority of our approach over state-of-the-art baselines. We also demonstrate that our bi-level optimization framework maintains the performance of the expensive iterative algorithm while obtaining up to 1600x speed-up.},
  archive   = {C_KDD},
  author    = {Zhong, Yongjian and Vu, Hieu and Yang, Tianbao and Adhikari, Bijaya},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672026},
  pages     = {4595–4606},
  title     = {Efficient and effective implicit dynamic graph neural network},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BitLINK: Temporal linkage of address clusters in bitcoin
blockchain. <em>KDD</em>, 4583–4594. (<a
href="https://doi.org/10.1145/3637528.3672037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the Bitcoin blockchain, an entity (e.g., a gambling service) may control multiple distinct address clusters. Links (i.e., trust relationships) between these disjoint address clusters can be established when one cluster is abandoned, and a new one is formed shortly thereafter. To link the clusters across time, we have developed a deep neural network model that exploits these synchronous actions derived from unlabeled data in a self-supervised manner. This model assesses whether two clusters exhibit synchronous temporal signatures indicative of a shared entity ownership.We validated our model on 26 real-world entities identified by WalletExplorer [36]. In addition to the existing knowledge, our analysis revealed more transaction history by linking address clusters for three major services: HelixMixer, Primedice, and Bitcoin Fog, as well as 60 other services. This enables us to address questions related to the revenue and expenditures of these services and create informative aggregate statistics. Readers can find code and data on our support website: http://www.bitlinkwallet.com.},
  archive   = {C_KDD},
  author    = {Zhong, Sheng and Mueen, Abdullah},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672037},
  pages     = {4583–4594},
  title     = {BitLINK: Temporal linkage of address clusters in bitcoin blockchain},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bridging and compressing feature and semantic spaces for
robust graph neural networks: An information theory perspective.
<em>KDD</em>, 4571–4582. (<a
href="https://doi.org/10.1145/3637528.3671870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The emerging Graph Convolutional Networks (GCNs) have attracted widespread attention in graph learning, due to their good ability of aggregating the information between higher-order neighbors. However, real-world graph data contains high noise and redundancy, making it hard for GCNs to accurately depict the complete relationships between nodes, which seriously degrades the quality of graph representations. Moreover, existing studies commonly ignore the distribution difference between feature and semantic spaces in graphs, causing inferior model generalization. To address these challenges, we propose DIB-RGCN, a novel robust GCN framework, to explore the optimal graph representation with the guidance of the well-designed dual information bottleneck principle. First, we analyze the reasons for distribution differences and theoretically prove that minimal sufficient representations in specific spaces cannot promise optimal performance for downstream tasks. Next, we design new dual channels to regularize feature and semantic spaces, eliminating the sharing of task-irrelevant information between spaces. Different from existing denoising algorithms that adopt a random dropping manner, we innovatively replace potential noisy features and edges with local neighboring representations. This design lowers edge-specific coefficient assignment, alleviating the interference of original representations while retaining graph structures. Further, we maximize the sharing of task-relevant information between feature and semantic spaces to alleviate the difference between them. Using real-world datasets, extensive experiments demonstrate the robustness of the proposed DIB-RGCN, which outperforms state-of-the-art methods on classification tasks.},
  archive   = {C_KDD},
  author    = {Zhong, Luying and Lin, Renjie and Li, Jiayin and Wang, Shiping and Chen, Zheyi},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671870},
  pages     = {4571–4582},
  title     = {Bridging and compressing feature and semantic spaces for robust graph neural networks: An information theory perspective},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LogParser-LLM: Advancing efficient log parsing with large
language models. <em>KDD</em>, 4559–4570. (<a
href="https://doi.org/10.1145/3637528.3671810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Logs are ubiquitous digital footprints, playing an indispensable role in system diagnostics, security analysis, and performance optimization. The extraction of actionable insights from logs is critically dependent on the log parsing process, which converts raw logs into structured formats for downstream analysis. Yet, the complexities of contemporary systems and the dynamic nature of logs pose significant challenges to existing automatic parsing techniques. The emergence of Large Language Models (LLM) offers new horizons. With their expansive knowledge and contextual prowess, LLMs have been transformative across diverse applications. Building on this, we introduce LogParser-LLM, a novel log parser integrated with LLM capabilities. This union seamlessly blends semantic insights with statistical nuances, obviating the need for hyper-parameter tuning and labeled training data, while ensuring rapid adaptability through online parsing. Further deepening our exploration, we address the intricate challenge of parsing granularity, proposing a new metric and integrating human interactions to allow users to calibrate granularity to their specific needs. Our method&#39;s efficacy is empirically demonstrated through evaluations on the Loghub-2k and the large-scale LogPub benchmark. In evaluations on the LogPub benchmark, involving an average of 3.6 million logs per dataset across 14 datasets, our LogParser-LLM requires only 272.5 LLM invocations on average, achieving a 90.6\% F1 score for grouping accuracy and an 81.1\% for parsing accuracy. These results demonstrate the method&#39;s high efficiency and accuracy, outperforming current state-of-the-art log parsers, including pattern-based, neural network-based, and existing LLM-enhanced approaches.},
  archive   = {C_KDD},
  author    = {Zhong, Aoxiao and Mo, Dengyao and Liu, Guiyang and Liu, Jinbu and Lu, Qingda and Zhou, Qi and Wu, Jiesheng and Li, Quanzheng and Wen, Qingsong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671810},
  pages     = {4559–4570},
  title     = {LogParser-LLM: Advancing efficient log parsing with large language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Relaxing continuous constraints of equivariant graph neural
networks for broad physical dynamics learning. <em>KDD</em>, 4548–4558.
(<a href="https://doi.org/10.1145/3637528.3671957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Incorporating Euclidean symmetries (e.g. rotation equivariance) as inductive biases into graph neural networks has improved their generalization ability and data efficiency in unbounded physical dynamics modeling. However, in various scientific and engineering applications, the symmetries of dynamics are frequently discrete due to the boundary conditions. Thus, existing GNNs either over-look necessary symmetry, resulting in suboptimal representation ability, or impose excessive equivariance, which fails to generalize to unobserved symmetric dynamics. In this work, we propose a general Discrete Equivariant Graph Neural Network (DEGNN) that guarantees equivariance to a given discrete point group. Specifically, we show that such discrete equivariant message passing could be constructed by transforming geometric features into permutation-invariant embeddings. Through relaxing continuous equivariant constraints, DEGNN can employ more geometric feature combinations to approximate unobserved physical object interaction functions. Two implementation approaches of DEGNN are proposed based on ranking or pooling permutation-invariant functions. We apply DEGNN to various physical dynamics, ranging from particle, molecular, crowd to vehicle dynamics. In twenty scenarios, DEGNN significantly outperforms existing state-of-the-art approaches. Moreover, we show that DEGNN is data efficient, learning with less data, and can generalize across scenarios such as unobserved orientation.},
  archive   = {C_KDD},
  author    = {Zheng, Zinan and Liu, Yang and Li, Jia and Yao, Jianhua and Rong, Yu},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671957},
  pages     = {4548–4558},
  title     = {Relaxing continuous constraints of equivariant graph neural networks for broad physical dynamics learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SiGeo: Sub-one-shot NAS via geometry of loss landscape.
<em>KDD</em>, 4536–4547. (<a
href="https://doi.org/10.1145/3637528.3671712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Neural Architecture Search (NAS) has become a widely used tool for automating neural network design. While one-shot NAS methods have successfully reduced computational requirements, they often require extensive training. On the other hand, zero-shot NAS utilizes training-free proxies to evaluate a candidate architecture&#39;s test performance but has two limitations: (1) inability to use the information gained as a network improves with training and (2) unreliable performance, particularly in complex domains like RecSys, due to the multi-modal data inputs and complex architecture configurations. To synthesize the benefits of both methods, we introduce a &quot;sub-one-shot&quot; paradigm that serves as a bridge between zero-shot and one-shot NAS. In sub-one-shot NAS, the supernet is trained using only a small subset of the training data, a phase we refer to as &quot;warm-up.&quot; Within this framework, we present SiGeo, a proxy founded on a novel theoretical framework that connects the supernet warm-up with the efficacy of the proxy. Extensive experiments have consistently shown that SiGeo, when properly warmed up, surpasses state-of-the-art NAS proxies in many established NAS benchmarks in the computer vision domain. Furthermore, when tested on recommendation system benchmarks, SiGeo demonstrates its ability to match the performance of state-of-the-art weight-sharing one-shot NAS methods while significantly reducing computational costs by approximately 60\%.},
  archive   = {C_KDD},
  author    = {Zheng, Hua and Liu, Kuang-Hung and Fedorov, Igor and Zhang, Xin and Chen, Wen-Yen and Wen, Wei},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671712},
  pages     = {4536–4547},
  title     = {SiGeo: Sub-one-shot NAS via geometry of loss landscape},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spuriousness-aware meta-learning for learning robust
classifiers. <em>KDD</em>, 4524–4535. (<a
href="https://doi.org/10.1145/3637528.3672006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Spurious correlations are brittle associations between certain attributes of inputs and target variables, such as the correlation between an image background and an object class. Deep image classifiers often leverage them for predictions, leading to poor generalization on the data where the correlations do not hold. Mitigating the impact of spurious correlations is crucial towards robust model generalization, but it often requires annotations of the spurious correlations in data -- a strong assumption in practice. In this paper, we propose a novel learning framework based on meta-learning, termed SPUME -- SPUriousness-aware MEta-learning, to train an image classifier to be robust to spurious correlations. We design the framework to iteratively detect and mitigate the spurious correlations that the classifier excessively relies on for predictions. To achieve this, we first propose to utilize a pre-trained vision-language model to extract text-format attributes from images. These attributes enable us to curate data with various class-attribute correlations, and we formulate a novel metric to measure the degree of these correlations&#39; spuriousness. Then, to mitigate the reliance on spurious correlations, we propose a meta-learning strategy in which the support (training) sets and query (test) sets in tasks are curated with different spurious correlations that have high degrees of spuriousness. By meta-training the classifier on these spuriousness-aware meta-learning tasks, our classifier can learn to be invariant to the spurious correlations. We demonstrate that our method is robust to spurious correlations without knowing them a priori and achieves the best on five benchmark datasets with different robustness measures. Our code is available at https://github.com/gtzheng/SPUME.},
  archive   = {C_KDD},
  author    = {Zheng, Guangtao and Ye, Wenqian and Zhang, Aidong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672006},
  pages     = {4524–4535},
  title     = {Spuriousness-aware meta-learning for learning robust classifiers},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Urban-focused multi-task offline reinforcement learning
with contrastive data sharing. <em>KDD</em>, 4512–4523. (<a
href="https://doi.org/10.1145/3637528.3671823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Enhancing diverse human decision-making processes in an urban environment is a critical issue across various applications, including ride-sharing vehicle dispatching, public transportation management, and autonomous driving. Offline reinforcement learning (RL) is a promising approach to learn and optimize human urban strategies (or policies) from pre-collected human-generated spatial-temporal urban data. However, standard offline RL faces two significant challenges: (1) data scarcity and data heterogeneity, and (2) distributional shift. In this paper, we introduce MODA - a Multi-Task Offline Reinforcement Learning with Contrastive Data Sharing approach. MODA addresses the challenges of data scarcity and heterogeneity in a multi-task urban setting through Contrastive Data Sharing among tasks. This technique involves extracting latent representations of human behaviors by contrasting positive and negative data pairs. It then shares data presenting similar representations with the target task, facilitating data augmentation for each task. Moreover, MODA develops a novel model-based multi-task offline RL algorithm. This algorithm constructs a robust Markov Decision Process (MDP) by integrating a dynamics model with a Generative Adversarial Network (GAN). Once the robust MDP is established, any online RL or planning algorithm can be applied. Extensive experiments conducted in a real-world multi-task urban setting validate the effectiveness of MODA. The results demonstrate that MODA exhibits significant improvements compared to state-of-the-art baselines, showcasing its capability in advancing urban decision-making processes. We also made our code available to the research community.},
  archive   = {C_KDD},
  author    = {Zhao, Xinbo and Zhang, Yingxue and Zhang, Xin and Yang, Yu and Xie, Yiqun and Li, Yanhua and Luo, Jun},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671823},
  pages     = {4512–4523},
  title     = {Urban-focused multi-task offline reinforcement learning with contrastive data sharing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GeoMix: Towards geometry-aware data augmentation.
<em>KDD</em>, 4500–4511. (<a
href="https://doi.org/10.1145/3637528.3671700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mixup has shown considerable success in mitigating the challenges posed by limited labeled data in image classification. By synthesizing samples through the interpolation of features and labels, Mixup effectively addresses the issue of data scarcity. However, it has rarely been explored in graph learning tasks due to the irregularity and connectivity of graph data. Specifically, in node classification tasks, Mixup presents a challenge in creating connections for synthetic data. In this paper, we propose Geometric Mixup (GeoMix), a simple and interpretable Mixup approach leveraging in-place graph editing. It effectively utilizes geometry information to interpolate features and labels with those from the nearby neighborhood, generating synthetic nodes and establishing connections for them. We conduct theoretical analysis to elucidate the rationale behind employing geometry information for node Mixup, emphasizing the significance of locality enhancement-a critical aspect of our method&#39;s design. Extensive experiments demonstrate that our lightweight Geometric Mixup achieves state-of-the-art results on a wide variety of standard datasets with limited labeled data. Furthermore, it significantly improves the generalization capability of underlying GNNs across various challenging out-of-distribution generalization tasks. Our code is available at https://github.com/WtaoZhao/geomix.},
  archive   = {C_KDD},
  author    = {Zhao, Wentao and Wu, Qitian and Yang, Chenxiao and Yan, Junchi},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671700},
  pages     = {4500–4511},
  title     = {GeoMix: Towards geometry-aware data augmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conformalized link prediction on graph neural networks.
<em>KDD</em>, 4490–4499. (<a
href="https://doi.org/10.1145/3637528.3672061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph Neural Networks (GNNs) excel in diverse tasks, yet their applications in high-stakes domains are often hampered by unreliable predictions. Although numerous uncertainty quantification methods have been proposed to address this limitation, they often lackrigorous uncertainty estimates. This work makes the first attempt to introduce a distribution-free and model-agnostic uncertainty quantification approach to construct a predictive interval with a statistical guarantee for GNN-based link prediction. We term it asconformalized link prediction. Our approach builds upon conformal prediction (CP), a framework that promises to construct statistically robust prediction sets or intervals. There are two primary challenges: first, given dependent data like graphs, it is unclear whether the critical assumption in CP --- exchangeability --- still holds when applied to link prediction. Second, even if the exchangeability assumption is valid for conformalized link prediction, we need to ensure high efficiency, i.e., the resulting prediction set or the interval length is small enough to provide useful information. To tackle these challenges, we first theoretically and empirically establish a permutation invariance condition for the application of CP in link prediction tasks, along with an exact test-time coverage. Leveraging the important structural information in graphs, we then identify a novel and crucial connection between a graph&#39;s adherence to the power law distribution and the efficiency of CP. This insight leads to the development of a simple yet effective sampling-based method to align the graph structure with a power law distribution prior to the standard CP procedure. Extensive experiments demonstrate that for conformalized link prediction, our approach achieves the desired marginal coverage while significantly improving the efficiency of CP compared to baseline methods.},
  archive   = {C_KDD},
  author    = {Zhao, Tianyi and Kang, Jian and Cheng, Lu},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672061},
  pages     = {4490–4499},
  title     = {Conformalized link prediction on graph neural networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-source unsupervised domain adaptation on graphs with
transferability modeling. <em>KDD</em>, 4479–4489. (<a
href="https://doi.org/10.1145/3637528.3671829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we tackle a new problem ofmulti-source unsupervised domain adaptation (MSUDA) for graphs, where models trained on annotated source domains need to be transferred to the unsupervised target graph for node classification. Due to the discrepancy in distribution across domains, the key challenge is how to select good source instances and how to adapt the model. Diverse graph structures further complicate this problem, rendering previous MSUDA approaches less effective. In this work, we present the framework Selective Multi-source Adaptation for Graph (SelMAG ), with a graph-modeling-based domain selector, a sub-graph node selector, and a bi-level alignment objective for the adaptation. Concretely, to facilitate the identification of informative source data, the similarity across graphs is disentangled and measured with the transferability of a graph-modeling task set, and we use it as evidence for source domain selection. A node selector is further incorporated to capture the variation in transferability of nodes within the same source domain. To learn invariant features for adaptation, we align the target domain to selected source data both at the embedding space by minimizing the optimal transport distance and at the classification level by distilling the label function. Modules are explicitly learned to select informative source data and conduct the alignment in virtual training splits with a meta-learning strategy. Experimental results on five graph datasets show the effectiveness of the proposed method.},
  archive   = {C_KDD},
  author    = {Zhao, Tianxiang and Luo, Dongsheng and Zhang, Xiang and Wang, Suhang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671829},
  pages     = {4479–4489},
  title     = {Multi-source unsupervised domain adaptation on graphs with transferability modeling},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pre-training and prompting for few-shot node classification
on text-attributed graphs. <em>KDD</em>, 4467–4478. (<a
href="https://doi.org/10.1145/3637528.3671952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The text-attributed graph (TAG) is one kind of important real-world graph-structured data with each node associated with raw texts. For TAGs, traditional few-shot node classification methods directly conduct training on the pre-processed node features and do not consider the raw texts. The performance is highly dependent on the choice of the feature pre-processing method. In this paper, we propose P2TAG, a framework designed for few-shot node classification on TAGs with graph pre-training and prompting. P2TAG first pre-trains the language model (LM) and graph neural network (GNN) on TAGs with self-supervised loss. To fully utilize the ability of language models, we adapt the masked language modeling objective for our framework. The pre-trained model is then used for the few-shot node classification with a mixed prompt method, which simultaneously considers both text and graph information. We conduct experiments on six real-world TAGs, including paper citation networks and product co-purchasing networks. Experimental results demonstrate that our proposed framework outperforms existing graph few-shot learning methods on these datasets with +18.98\% ~ +32.14\% improvements.},
  archive   = {C_KDD},
  author    = {Zhao, Huanjing and Yang, Beining and Cen, Yukuo and Ren, Junyu and Zhang, Chenhui and Dong, Yuxiao and Kharlamov, Evgeny and Zhao, Shu and Tang, Jie},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671952},
  pages     = {4467–4478},
  title     = {Pre-training and prompting for few-shot node classification on text-attributed graphs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Counteracting duration bias in video recommendation via
counterfactual watch time. <em>KDD</em>, 4455–4466. (<a
href="https://doi.org/10.1145/3637528.3671817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In video recommendation, an ongoing effort is to satisfy users&#39; personalized information needs by leveraging their logged watch time. However, watch time prediction suffers from duration bias, hindering its ability to reflect users&#39; interests accurately. Existing label-correction approaches attempt to uncover user interests through grouping and normalizing observed watch time according to video duration. Although effective to some extent, we found that these approaches regard completely played records (i.e., a user watches the entire video) as equally high interest, which deviates from what we observed on real datasets: users have varied explicit feedback proportion when completely playing videos. In this paper, we introduce the counterfactual watch time (CWT), the potential watch time a user would spend on the video if its duration is sufficiently long. Analysis shows that the duration bias is caused by the truncation of CWT due to the video duration limitation, which usually occurs on those completely played records. Besides, a Counterfactual Watch Model (CWM) is proposed, revealing that CWT equals the time users get the maximum benefit from video recommender systems. Moreover, a cost-based transform function is defined to transform the CWT into the estimation of user interest, and the model can be learned by optimizing a counterfactual likelihood function defined over observed user watch times. Extensive experiments on three real video recommendation datasets and online A/B testing demonstrated that CWM effectively enhanced video recommendation accuracy and counteracted the duration bias.},
  archive   = {C_KDD},
  author    = {Zhao, Haiyuan and Cai, Guohao and Zhu, Jieming and Dong, Zhenhua and Xu, Jun and Wen, Ji-Rong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671817},
  pages     = {4455–4466},
  title     = {Counteracting duration bias in video recommendation via counterfactual watch time},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). All in one and one for all: A simple yet effective method
towards cross-domain graph pretraining. <em>KDD</em>, 4443–4454. (<a
href="https://doi.org/10.1145/3637528.3671913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large Language Models (LLMs) have revolutionized the fields of computer vision (CV) and natural language processing (NLP). One of the most notable advancements of LLMs is that a single model is trained on vast and diverse datasets spanning multiple domains -- a paradigm we term &#39;All in One&#39;. This methodology empowers LLMs with super generalization capabilities, facilitating an encompassing comprehension of varied data distributions. Leveraging these capabilities, a single LLM demonstrates remarkable versatility across a variety of domains -- a paradigm we term &#39;One for All&#39;. However, applying this idea to the graph field remains a formidable challenge, with cross-domain pretraining often resulting in negative transfer. This issue is particularly important in few-shot learning scenarios, where the paucity of training data necessitates the incorporation of external knowledge sources. In response to this challenge, we propose a novel approach called Graph COordinators for PrEtraining (GCOPE), that harnesses the underlying commonalities across diverse graph datasets to enhance few-shot learning. Our novel methodology involves a unification framework that amalgamates disparate graph datasets during the pretraining phase to distill and transfer meaningful knowledge to target tasks. Extensive experiments across multiple graph datasets demonstrate the superior efficacy of our approach. By successfully leveraging the synergistic potential of multiple graph datasets for pretraining, our work stands as a pioneering contribution to the realm of graph foundational model. Code available at https://github.com/cshhzhao/GCOPE.},
  archive   = {C_KDD},
  author    = {Zhao, Haihong and Chen, Aochuan and Sun, Xiangguo and Cheng, Hong and Li, Jia},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671913},
  pages     = {4443–4454},
  title     = {All in one and one for all: A simple yet effective method towards cross-domain graph pretraining},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VertiMRF: Differentially private vertical federated data
synthesis. <em>KDD</em>, 4431–4442. (<a
href="https://doi.org/10.1145/3637528.3671771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Data synthesis is a promising solution to share data for various downstream analytic tasks without exposing raw data. However, without a theoretical privacy guarantee, a synthetic dataset would still leak some sensitive information in raw data. As a countermeasure, differential privacy is widely adopted to safeguard data synthesis by strictly limiting the released information. This technique is advantageous yet presents significant challenges in the vertical federated setting, where data attributes are distributed among different data parties. The main challenge lies in maintaining privacy while efficiently and precisely reconstructing the correlation between attributes. In this paper, we propose a novel algorithm called VertiMRF, designed explicitly for generating synthetic data in the vertical setting and providing differential privacy protection for all information shared from data parties. We introduce techniques based on the Flajolet-Martin (FM) sketch for encoding local data satisfying differential privacy and estimating cross-party marginals. We provide theoretical privacy and utility proof for encoding in this multi-attribute data. Collecting the locally generated private Markov Random Field (MRF) and the sketches, a central server can reconstruct a global MRF, maintaining the most useful information. Two critical techniques introduced in our VertiMRF are dimension reduction and consistency enforcement, preventing the noise of FM sketch from overwhelming the information of attributes with large domain sizes when building the global MRF. These two techniques allow flexible and inconsistent binning strategies of local private MRF and the data sketching module, which can preserve information to the greatest extent. We conduct extensive experiments on four real-world datasets to evaluate the effectiveness of VertiMRF. End-to-end comparisons demonstrate the superiority of VertiMRF.},
  archive   = {C_KDD},
  author    = {Zhao, Fangyuan and Li, Zitao and Ren, Xuebin and Ding, Bolin and Yang, Shusen and Li, Yaliang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671771},
  pages     = {4431–4442},
  title     = {VertiMRF: Differentially private vertical federated data synthesis},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Algorithmic fairness generalization under covariate and
dependence shifts simultaneously. <em>KDD</em>, 4419–4430. (<a
href="https://doi.org/10.1145/3637528.3671909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The endeavor to preserve the generalization of a fair and invariant classifier across domains, especially in the presence of distribution shifts, becomes a significant and intricate challenge in machine learning. In response to this challenge, numerous effective algorithms have been developed with a focus on addressing the problem of fairness-aware domain generalization. These algorithms are designed to navigate various types of distribution shifts, with a particular emphasis on covariate and dependence shifts. In this context, covariate shift pertains to changes in the marginal distribution of input features, while dependence shift involves alterations in the joint distribution of the label variable and sensitive attributes. In this paper, we introduce a simple but effective approach that aims to learn a fair and invariant classifier by simultaneously addressing both covariate and dependence shifts across domains. We assert the existence of an underlying transformation model can transform data from one domain to another, while preserving the semantics related to non-sensitive attributes and classes. By augmenting various synthetic data domains through the model, we learn a fair and invariant classifier in source domains. This classifier can then be generalized to unknown target domains, maintaining both model prediction and fairness concerns. Extensive empirical studies on four benchmark datasets demonstrate that our approach surpasses state-of-the-art methods.},
  archive   = {C_KDD},
  author    = {Zhao, Chen and Jiang, Kai and Wu, Xintao and Wang, Haoliang and Khan, Latifur and Grant, Christan and Chen, Feng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671909},
  pages     = {4419–4430},
  title     = {Algorithmic fairness generalization under covariate and dependence shifts simultaneously},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning flexible time-windowed granger causality
integrating heterogeneous interventional time series data. <em>KDD</em>,
4408–4418. (<a href="https://doi.org/10.1145/3637528.3672023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Granger causality, commonly used for inferring causal structures from time series data, has been adopted in widespread applications across various fields due to its intuitive explainability and high compatibility with emerging deep neural network prediction models. To alleviate challenges in better deciphering causal structures unambiguously from time series, the use of interventional data has become a practical approach. However, existing methods have yet to be explored in the context of imperfect interventions with unknown targets, which are more common and often more beneficial in a wide range of real-world applications. Additionally, the identifiability issues of Granger causality with unknown interventional targets in complex network models remain unsolved. Our work presents a theoretically-grounded method that infers Granger causal structure and identifies unknown targets by leveraging heterogeneous interventional time series data. We further illustrate that learning Granger causal structure and recovering interventional targets can mutually promote each other. Comparative experiments demonstrate that our method outperforms several robust baseline methods in learning Granger causal structure from interventional time series data.},
  archive   = {C_KDD},
  author    = {Zhang, Ziyi and Ren, Shaogang and Qian, Xiaoning and Duffield, Nick},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672023},
  pages     = {4408–4418},
  title     = {Learning flexible time-windowed granger causality integrating heterogeneous interventional time series data},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Long-term vessel trajectory imputation with physics-guided
diffusion probabilistic model. <em>KDD</em>, 4398–4407. (<a
href="https://doi.org/10.1145/3637528.3672086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Maritime traffic management increasingly relies on vessel position information provided by terrestrial and satellite networks of the Automatic Identification System (AIS). Unfortunately, the problem of missing AIS data can lead to long-term gaps in vessel trajectory, raising corresponding security concerns regarding collision risks and illicit activities. Existing imputation approaches are often constrained by vehicle-based low-sampling trajectories, hindering their ability to address unique characteristics of maritime transportation systems and long-term missing scenarios. To tackle these challenges, we propose a novel generative framework for long-term vessel trajectory imputation. Our framework considers irregular tracks of vessels, which differ from those of cars due to the absence of a structured road network, and ensures the continuity of multi-point imputed trajectories. Specifically, we first utilize a pre-trained trajectory embedding block to capture patterns of vessel movements. Subsequently, we introduce a diffusion-based model for generating missing trajectories, where observed trajectory modeling with transformer encoding architecture and embeddings of both historical vessel trajectory and external factors serve as conditional information. In particular, we design a physics-guided discriminator in the training stage, which imposes kinematic constraints between locations and angles to improve the continuity of the imputed trajectories. Comprehensive experiments and analysis on a real-world AIS dataset confirm the effectiveness of our proposed approach.},
  archive   = {C_KDD},
  author    = {Zhang, Zhiwen and Fan, Zipei and Lv, Zewu and Song, Xuan and Shibasaki, Ryosuke},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672086},
  pages     = {4398–4407},
  title     = {Long-term vessel trajectory imputation with physics-guided diffusion probabilistic model},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rethinking graph backdoor attacks: A distribution-preserving
perspective. <em>KDD</em>, 4386–4397. (<a
href="https://doi.org/10.1145/3637528.3671910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph Neural Networks (GNNs) have shown remarkable performance in various tasks. However, recent works reveal that GNNs are vulnerable to backdoor attacks. Generally, backdoor attack poisons the graph by attaching backdoor triggers and the target class label to a set of nodes in the training graph. A GNN trained on the poisoned graph will then be misled to predict test nodes attached with trigger to the target class. Despite their effectiveness, our empirical analysis shows that triggers generated by existing methods tend to be out-of-distribution (OOD), which significantly differ from the clean data. Hence, these injected triggers can be easily detected and pruned with widely used outlier detection methods in real-world applications. Therefore, in this paper, we study a novel problem of unnoticeable graph backdoor attacks with in-distribution (ID) triggers. To generate ID triggers, we introduce an OOD detector in conjunction with an adversarial learning strategy to generate the attributes of the triggers within distribution. To ensure a high attack success rate with ID triggers, we introduce novel modules designed to enhance trigger memorization by the victim model trained on poisoned graph. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed method in generating in distribution triggers that can bypass various defense strategies while maintaining a high attack success rate.},
  archive   = {C_KDD},
  author    = {Zhang, Zhiwei and Lin, Minhua and Dai, Enyan and Wang, Suhang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671910},
  pages     = {4386–4397},
  title     = {Rethinking graph backdoor attacks: A distribution-preserving perspective},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). Representation learning of geometric trees. <em>KDD</em>,
4374–4385. (<a href="https://doi.org/10.1145/3637528.3671688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Geometric trees are characterized by their tree-structured layout and spatially constrained nodes and edges, which significantly impacts their topological attributes. This inherent hierarchical structure plays a crucial role in domains such as neuron morphology and river geomorphology, but traditional graph representation methods often overlook these specific characteristics of tree structures. To address this, we introduce a new representation learning framework tailored for geometric trees. It first features a unique message passing neural network, which is both provably geometrical structure-recoverable and rotation-translation invariant. To address the data label scarcity issue, our approach also includes two innovative training targets that reflect the hierarchical ordering and geometric structure of these geometric trees. This enables fully self-supervised learning without explicit labels. We validate our method&#39;s effectiveness on eight real-world datasets, demonstrating its capability to represent geometric trees.},
  archive   = {C_KDD},
  author    = {Zhang, Zheng and Zhang, Allen and Nelson, Ruth and Ascoli, Giorgio and Zhao, Liang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671688},
  pages     = {4374–4385},
  title     = {Representation learning of geometric trees},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint auction in the online advertising market.
<em>KDD</em>, 4362–4373. (<a
href="https://doi.org/10.1145/3637528.3671746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Online advertising is a primary source of income for e-commerce platforms. In the current advertising pattern, the oriented targets are the online store owners who are willing to pay extra fees to enhance the position of their stores. On the other hand, brand suppliers are also desirable to advertise their products in stores to boost brand sales. However, the currently used advertising mode cannot satisfy the demand of both stores and brand suppliers simultaneously. To address this, we innovatively propose a joint advertising model termed &#39;&#39;Joint Auction&#39;&#39;, allowing brand suppliers and stores to collaboratively bid for advertising slots, catering to both their needs. However, conventional advertising auction mechanisms are not suitable for this novel scenario. In this paper, we propose JRegNet, a neural network architecture for the optimal joint auction design, to generate mechanisms that can achieve the optimal revenue and guarantee (near-)dominant strategy incentive compatibility and individual rationality. Finally, multiple experiments are conducted on synthetic and real data to demonstrate that our proposed joint auction significantly improves platform&#39;s revenue compared to the known baselines.},
  archive   = {C_KDD},
  author    = {Zhang, Zhen and Li, Weian and Lei, Yahui and Wang, Bingzhe and Zhang, Zhicheng and Qi, Qi and Liu, Qiang and Wang, Xingxing},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671746},
  pages     = {4362–4373},
  title     = {Joint auction in the online advertising market},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LLM4DyG: Can large language models solve spatial-temporal
problems on dynamic graphs? <em>KDD</em>, 4350–4361. (<a
href="https://doi.org/10.1145/3637528.3671709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In an era marked by the increasing adoption of Large Language Models (LLMs) for various tasks, there is a growing focus on exploring LLMs&#39; capabilities in handling web data, particularly graph data. Dynamic graphs, which capture temporal network evolution patterns, are ubiquitous in real-world web data. Evaluating LLMs&#39; competence in understanding spatial-temporal information on dynamic graphs is essential for their adoption in web applications, which remains unexplored in the literature. In this paper, we bridge the gap via proposing to evaluate LLMs&#39; spatial-temporal understanding abilities on dynamic graphs, to the best of our knowledge, for the first time. Specifically, we propose the LLM4DyG benchmark, which includes nine specially designed tasks considering the capability evaluation of LLMs from both temporal and spatial dimensions. Then, we conduct extensive experiments to analyze the impacts of different data generators, data statistics, prompting techniques, and LLMs on the model performance. Finally, we propose Disentangled Spatial-Temporal Thoughts (DST2) for LLMs on dynamic graphs to enhance LLMs&#39; spatial-temporal understanding abilities. Our main observations are: 1) LLMs have preliminary spatial-temporal understanding abilities on dynamic graphs, 2) Dynamic graph tasks show increasing difficulties for LLMs as the graph size and density increase, while not sensitive to the time span and data generation mechanism, 3) the proposed DST2 prompting method can help to improve LLMs&#39; spatial-temporal understanding abilities on dynamic graphs for most tasks. The data and codes are publicly available at Github.},
  archive   = {C_KDD},
  author    = {Zhang, Zeyang and Wang, Xin and Zhang, Ziwei and Li, Haoyang and Qin, Yijian and Zhu, Wenwu},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671709},
  pages     = {4350–4361},
  title     = {LLM4DyG: Can large language models solve spatial-temporal problems on dynamic graphs?},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Geometric view of soft decorrelation in self-supervised
learning. <em>KDD</em>, 4338–4349. (<a
href="https://doi.org/10.1145/3637528.3671914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Contrastive learning, a form of Self-Supervised Learning (SSL), typically consists of an alignment term and a regularization term. The alignment term minimizes the distance between the embeddings of a positive pair, while the regularization term prevents trivial solutions and expresses prior beliefs about the embeddings. As a widely used regularization technique, soft decorrelation has been employed by several non-contrastive SSL methods to avoid trivial solutions. While the decorrelation term is designed to address the issue of dimensional collapse, we find that it fails to achieve this goal theoretically and experimentally. Based on such a finding, we extend the soft decorrelation regularization to minimize the distance between the covariance matrix and an identity matrix. We provide a new perspective on the geometric distance between positive definite matrices to investigate why the soft decorrelation cannot efficiently solve the dimensional collapse. Furthermore, we construct a family of loss functions utilizing the Bregman Matrix Divergence (BMD), with the soft decorrelation representing a specific instance within this family. We prove that a loss function (LogDet) in this family can solve the issue of dimensional collapse. Our novel loss functions based on BMD exhibit superior performance compared to the soft decorrelation and other baseline techniques, as demonstrated by experimental results on graph and image datasets.},
  archive   = {C_KDD},
  author    = {Zhang, Yifei and Zhu, Hao and Song, Zixing and Chen, Yankai and Fu, Xinyu and Meng, Ziqiao and Koniusz, Piotr and King, Irwin},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671914},
  pages     = {4338–4349},
  title     = {Geometric view of soft decorrelation in self-supervised learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Topology-aware embedding memory for continual learning on
expanding networks. <em>KDD</em>, 4326–4337. (<a
href="https://doi.org/10.1145/3637528.3671732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Memory replay based techniques have shown great success for continual learning with incrementally accumulated Euclidean data. Directly applying them to continually expanding networks, however, leads to the potential memory explosion problem due to the need to buffer representative nodes and their associated topological neighborhood structures. To this end, we systematically analyze the key challenges in the memory explosion problem, and present a general framework,i.e., Parameter Decoupled Graph Neural Networks (PDGNNs) with Topology-aware Embedding Memory (TEM), to tackle this issue. The proposed framework not only reduces the memory space complexity from O (ndL) to O (n)1: memory budget, d: average node degree, L: the radius of the GNN receptive field, but also fully utilizes the topological information for memory replay. Specifically, PDGNNs decouple trainable parameters from the computation ego-subnetwork viaTopology-aware Embeddings (TEs), which compress ego-subnetworks into compact vectors (i.e., TEs) to reduce the memory consumption. Based on this framework, we discover a unique pseudo-training effect in continual learning on expanding networks and this effect motivates us to develop a novel coverage maximization sampling strategy that can enhance the performance with a tight memory budget. Thorough empirical studies demonstrate that, by tackling the memory explosion problem and incorporating topological information into memory replay, PDGNNs with TEM significantly outperform state-of-the-art techniques, especially in the challenging class-incremental setting.},
  archive   = {C_KDD},
  author    = {Zhang, Xikun and Song, Dongjin and Chen, Yixin and Tao, Dacheng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671732},
  pages     = {4326–4337},
  title     = {Topology-aware embedding memory for continual learning on expanding networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multimodal foundation agent for financial trading:
Tool-augmented, diversified, and generalist. <em>KDD</em>, 4314–4325.
(<a href="https://doi.org/10.1145/3637528.3671801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Financial trading is a crucial component of the markets, informed by a multimodal information landscape encompassing news, prices, and Kline charts, and encompasses diverse tasks such as quantitative trading and high-frequency trading with various assets. While advanced AI techniques like deep learning and reinforcement learning are extensively utilized in finance, their application in financial trading tasks often faces challenges due to inadequate handling of multimodal data and limited generalizability across various tasks. To address these challenges, we present FinAgent, a multimodal foundational agent with tool augmentation for financial trading. FinAgent&#39;s market intelligence module processes a diverse range of data-numerical, textual, and visual-to accurately analyze the financial market. Its unique dual-level reflection module not only enables rapid adaptation to market dynamics but also incorporates a diversified memory retrieval system, enhancing the agent&#39;s ability to learn from historical data and improve decision-making processes. The agent&#39;s emphasis on reasoning for actions fosters trust in its financial decisions. Moreover, FinAgent integrates established trading strategies and expert insights, ensuring that its trading approaches are both data-driven and rooted in sound financial principles. With comprehensive experiments on 6 financial datasets, including stocks and Crypto, FinAgent significantly outperforms 12 state-of-the-art baselines in terms of 6 financial metrics with over 36\% average improvement on profit. Specifically, a 92.27\% return (a 84.39\% relative improvement) is achieved on one dataset. Notably, FinAgent is the first advanced multimodal foundation agent designed for financial trading tasks.},
  archive   = {C_KDD},
  author    = {Zhang, Wentao and Zhao, Lingxuan and Xia, Haochong and Sun, Shuo and Sun, Jiaze and Qin, Molei and Li, Xinyi and Zhao, Yuqing and Zhao, Yilei and Cai, Xinyu and Zheng, Longtao and Wang, Xinrun and An, Bo},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671801},
  pages     = {4314–4325},
  title     = {A multimodal foundation agent for financial trading: Tool-augmented, diversified, and generalist},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Irregular traffic time series forecasting based on
asynchronous spatio-temporal graph convolutional networks. <em>KDD</em>,
4302–4313. (<a href="https://doi.org/10.1145/3637528.3671665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate traffic forecasting is crucial for the development of Intelligent Transportation Systems (ITS), playing a pivotal role in modern urban traffic management. Traditional forecasting methods, however, struggle with the irregular traffic time series resulting from adaptive traffic signal controls, presenting challenges in asynchronous spatial dependency, irregular temporal dependency, and predicting variable-length sequences. To this end, we propose an Asynchronous Spatio-tEmporal graph convolutional nEtwoRk (ASeer) tailored for irregular traffic time series forecasting. Specifically, we first propose an Asynchronous Graph Diffusion Network to capture the spatial dependency between asynchronously measured traffic states regulated by adaptive traffic signals. After that, to capture the temporal dependency within irregular traffic state sequences, a personalized time encoding is devised to embed the continuous time signals. Then, we propose a Transformable Time-aware Convolution Network, which adapts meta-filters for time-aware convolution on the sequences with inconsistent temporal flow. Additionally, a Semi-Autoregressive Prediction Network, comprising a state evolution unit and a semiautoregressive predictor, is designed to predict variable-length traffic sequences effectively and efficiently. Extensive experiments on a newly established benchmark demonstrate the superiority of ASeer compared with twelve competitive baselines across six metrics.},
  archive   = {C_KDD},
  author    = {Zhang, Weijia and Zhang, Le and Han, Jindong and Liu, Hao and Fu, Yanjie and Zhou, Jingbo and Mei, Yu and Xiong, Hui},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671665},
  pages     = {4302–4313},
  title     = {Irregular traffic time series forecasting based on asynchronous spatio-temporal graph convolutional networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards adaptive neighborhood for advancing temporal
interaction graph modeling. <em>KDD</em>, 4290–4301. (<a
href="https://doi.org/10.1145/3637528.3671877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Temporal Graph Networks (TGNs) have demonstrated their remarkable performance in modeling temporal interaction graphs. These works can generate temporal node representations by encoding the surrounding neighborhoods for the target node. However, an inherent limitation of existing TGNs is their reliance onfixed, hand-crafted rules for neighborhood encoding, overlooking the necessity for an adaptive and learnable neighborhood that can accommodate both personalization and temporal evolution across different timestamps. In this paper, we aim to enhance existing TGNs by introducing anadaptive neighborhood encoding mechanism. We present SEAN (Selective Encoding for Adaptive Neighborhood), a flexible plug-and-play model that can be seamlessly integrated with existing TGNs, effectively boosting their performance. To achieve this, we decompose the adaptive neighborhood encoding process into two phases: (i) representative neighbor selection, and (ii) temporal-aware neighborhood information aggregation. Specifically, we propose the Representative Neighbor Selector component, which automatically pinpoints the most important neighbors for the target node. It offers a tailored understanding of each node&#39;s unique surrounding context, facilitating personalization. Subsequently, we propose a Temporal-aware Aggregator, which synthesizes neighborhood aggregation by selectively determining the utilization of aggregation routes and decaying the outdated information, allowing our model to adaptively leverage both the contextually significant and current information during aggregation. We conduct extensive experiments by integrating SEAN into three representative TGNs, evaluating their performance on four public datasets and one financial benchmark dataset introduced in this paper. The results demonstrate that SEAN consistently leads to performance improvements across all models, achieving SOTA performance and exceptional robustness.},
  archive   = {C_KDD},
  author    = {Zhang, Siwei and Chen, Xi and Xiong, Yun and Wu, Xixi and Zhang, Yao and Fu, Yongrui and Zhao, Yinglong and Zhang, Jiawei},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671877},
  pages     = {4290–4301},
  title     = {Towards adaptive neighborhood for advancing temporal interaction graph modeling},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Knowledge distillation with perturbed loss: From a vanilla
teacher to a proxy teacher. <em>KDD</em>, 4278–4289. (<a
href="https://doi.org/10.1145/3637528.3671851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Knowledge distillation is a popular technique to transfer knowledge from a large teacher model to a small student model. Typically, the student learns to imitate the teacher by minimizing the KL divergence of its output distribution with the teacher&#39;s output distribution. In this work, we argue that such a learning objective is sub-optimal because there exists a discrepancy between the teacher&#39;s output distribution and the ground truth label distribution. Therefore, forcing the student to blindly imitate the unreliable teacher output distribution leads to inferior performance. To this end, we propose a novel knowledge distillation objective PTLoss by first representing the vanilla KL-based distillation loss function via a Maclaurin series and then perturbing the leading-order terms in this series. This perturbed loss implicitly transforms the original teacher into a proxy teacher with a distribution closer to the ground truth distribution. We establish the theoretical connection between this &quot;distribution closeness&#39;&#39; and the student model generalizability, which enables us to select the PTLoss&#39;s perturbation coefficients in a principled way. Extensive experiments on six public benchmark datasets demonstrate the effectiveness of PTLoss with teachers of different scales.},
  archive   = {C_KDD},
  author    = {Zhang, Rongzhi and Shen, Jiaming and Liu, Tianqi and Liu, Jialu and Bendersky, Michael and Najork, Marc and Zhang, Chao},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671851},
  pages     = {4278–4289},
  title     = {Knowledge distillation with perturbed loss: From a vanilla teacher to a proxy teacher},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Logical reasoning with relation network for inductive
knowledge graph completion. <em>KDD</em>, 4268–4277. (<a
href="https://doi.org/10.1145/3637528.3671911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inductive knowledge graph completion (KGC) aims to infer the missing relation for a set of newly-coming entities that never appeared in the training set. Such a setting is more in line with reality, as real-world KGs are constantly evolving and introducing new knowledge. Recent studies have shown promising results using message passing over subgraphs to embed newly-coming entities for inductive KGC. However, the inductive capability of these methods is usually limited by two key issues. (i) KGC always suffers from data sparsity, and the situation is even exacerbated in inductive KGC where new entities often have few or no connections to the original KG. (ii) Cold-start problem. It is over coarse-grained for accurate KG reasoning to generate representations for new entities by gathering the local information from few neighbors. To this end, we propose a novel i&amp;lt;u&amp;gt;N&amp;lt;/u&amp;gt;f&amp;lt;u&amp;gt;O&amp;lt;/u&amp;gt;max &amp;lt;u&amp;gt;R&amp;lt;/u&amp;gt;el&amp;lt;u&amp;gt;A&amp;lt;/u&amp;gt;tion &amp;lt;u&amp;gt;N&amp;lt;/u&amp;gt;etwork, namely NORAN, for inductive KG completion. It aims to mine latent relation patterns for inductive KG completion. Specifically, by centering on relations, NORAN provides a hyper view towards KG modeling, where the correlations between relations can be naturally captured as entity-independent logical evidence to conduct inductive KGC. Extensive experiment results on five benchmarks show that our framework substantially outperforms the state-of-the-art KGC methods.},
  archive   = {C_KDD},
  author    = {Zhang, Qinggang and Duan, Keyu and Dong, Junnan and Zheng, Pai and Huang, Xiao},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671911},
  pages     = {4268–4277},
  title     = {Logical reasoning with relation network for inductive knowledge graph completion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multivariate log-based anomaly detection for distributed
database. <em>KDD</em>, 4256–4267. (<a
href="https://doi.org/10.1145/3637528.3671725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Distributed databases are fundamental infrastructures of today&#39;s large-scale software systems such as cloud systems. Detecting anomalies in distributed databases is essential for maintaining software availability. Existing approaches, predominantly developed using Loghub-a comprehensive collection of log datasets from various systems-lack datasets specifically tailored to distributed databases, which exhibit unique anomalies. Additionally, there&#39;s a notable absence of datasets encompassing multi-anomaly, multi-node logs. Consequently, models built upon these datasets, primarily designed for standalone systems, are inadequate for distributed databases, and the prevalent method of deeming an entire cluster anomalous based on irregularities in a single node leads to a high false-positive rate. This paper addresses the unique anomalies and multivariate nature of logs in distributed databases. We expose the first open-sourced, comprehensive dataset with multivariate logs from distributed databases. Utilizing this dataset, we conduct an extensive study to identify multiple database anomalies and to assess the effectiveness of state-of-the-art anomaly detection using multivariate log data. Our findings reveal that relying solely on logs from a single node is insufficient for accurate anomaly detection on distributed database. Leveraging these insights, we propose MultiLog, an innovative multivariate log-based anomaly detection approach tailored for distributed databases. Our experiments, based on this novel dataset, demonstrate MultiLog&#39;s superiority, outperforming existing state-of-the-art methods by approximately 12\%.},
  archive   = {C_KDD},
  author    = {Zhang, Lingzhe and Jia, Tong and Jia, Mengxi and Li, Ying and Yang, Yong and Wu, Zhonghai},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671725},
  pages     = {4256–4267},
  title     = {Multivariate log-based anomaly detection for distributed database},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Asynchronous vertical federated learning for kernelized AUC
maximization. <em>KDD</em>, 4244–4255. (<a
href="https://doi.org/10.1145/3637528.3671930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vertical Federated Learning (VFL) has garnered significant attention due to its applicability in multi-party collaborative learning and the increasing demand for privacy-preserving measures. Most existing VFL algorithms primarily focus on accuracy as the training model metric. However, the data we access is often imbalanced in the real world, making it difficult for models based on accuracy to correctly classify minority samples. The Area Under the Curve (AUC) serves as an effective metric to evaluate the performance of a model on imbalanced data. Therefore, optimizing AUC can enhance the model&#39;s ability to handle imbalanced data. Besides, computational resources within VFL systems are also imbalanced, which makes synchronous VFL algorithms are difficult to apply in the real world. To address the double imbalance issue, we propose Asynchronous Vertical Federated Kernelized AUC Maximization (AVFKAM). Specifically, AVFKAM asynchronously updates a kernel model based on triply stochastic gradients with respect to (w.r.t.) the pairwise loss and random feature approximation. To facilitate theoretical analysis, we transfer the asynchrony of model coefficients to the functional gradient through a dual relationship between coefficients and objective function. Furthermore, we demonstrate that AVFKAM converges to the optimal solution at a rate of O(1/t), where t represents the global iteration number, and discuss the security of the model. If t is denoted as the global iteration number, we provide that it converges to the optimal solution with the rate of O(1/t). Finally, experimental results on various benchmark datasets demonstrate that AVFKAM maintains high AUC performance and efficiency.},
  archive   = {C_KDD},
  author    = {Zhang, Ke and Wang, Ganyu and Li, Han and Wang, Yulong and Chen, Hong and Gu, Bin},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671930},
  pages     = {4244–4255},
  title     = {Asynchronous vertical federated learning for kernelized AUC maximization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Optimized cost per click in online advertising: A
theoretical analysis. <em>KDD</em>, 4232–4243. (<a
href="https://doi.org/10.1145/3637528.3671767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, Optimized Cost Per Click (OCPC) and Optimized Cost Per Mille (OCPM) have emerged as the most widely adopted pricing models in the online advertising industry. However, the existing literature has yet to identify the specific conditions under which these models outperform traditional pricing models like Cost Per Click (CPC) and Cost Per Action (CPA). To fill the gap, this paper builds an economic model that compares OCPC with CPC and CPA theoretically, which incorporates out-site scenarios and outside options as two key factors. Our analysis reveals that OCPC can effectively replace CPA by tackling the problem of advertisers strategically manipulating conversion reporting in out-site scenarios where conversions occur outside the advertising platform. Furthermore, OCPC exhibits the potential to surpass CPC in platform payoffs by providing higher advertiser payoffs and consequently attracting more advertisers. However, if advertisers have less competitive outside options and consistently stay in the focal platform, the platform may achieve higher payoffs using CPC. Our findings deliver valuable insights for online advertising platforms in selecting optimal pricing models, and provide recommendations for further enhancing their payoffs. To the best of our knowledge, this is the first study to analyze OCPC from an economic perspective. Moreover, our analysis can be applied to the OCPM model as well.},
  archive   = {C_KDD},
  author    = {Zhang, Kaichen and Yuan, Zixuan and Xiong, Hui},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671767},
  pages     = {4232–4243},
  title     = {Optimized cost per click in online advertising: A theoretical analysis},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heuristic learning with graph neural networks: A unified
framework for link prediction. <em>KDD</em>, 4223–4231. (<a
href="https://doi.org/10.1145/3637528.3671946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Link prediction is a fundamental task in graph learning, inherently shaped by the topology of the graph. While traditional heuristics are grounded in graph topology, they encounter challenges in generalizing across diverse graphs. Recent research efforts have aimed to leverage the potential of heuristics, yet a unified formulation accommodating both local and global heuristics remains undiscovered. Drawing insights from the fact that both local and global heuristics can be represented by adjacency matrix multiplications, we propose a unified matrix formulation to accommodate and generalize various heuristics. We further propose the Heuristic Learning Graph Neural Network (HL-GNN) to efficiently implement the formulation. HL-GNN adopts intra-layer propagation and inter-layer connections, allowing it to reach a depth of around 20 layers with lower time complexity than GCN. Extensive experiments on the Planetoid, Amazon, and OGB datasets underscore the effectiveness and efficiency of HL-GNN. It outperforms existing methods by a large margin in prediction performance. Additionally, HL-GNN is several orders of magnitude faster than heuristic-inspired methods while requiring only a few trainable parameters. The case study further demonstrates that the generalized heuristics and learned weights are highly interpretable.},
  archive   = {C_KDD},
  author    = {Zhang, Juzheng and Wei, Lanning and Xu, Zhen and Yao, Quanming},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671946},
  pages     = {4223–4231},
  title     = {Heuristic learning with graph neural networks: A unified framework for link prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Diverse intra- and inter-domain activity style fusion for
cross-person generalization in activity recognition. <em>KDD</em>,
4213–4222. (<a href="https://doi.org/10.1145/3637528.3671828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing domain generalization (DG) methods for cross-person generalization tasks often face challenges in capturing intra- and inter-domain style diversity, resulting in domain gaps with the target domain. In this study, we explore a novel perspective to tackle this problem, a process conceptualized as domain padding. This proposal aims to enrich the domain diversity by synthesizing intra- and inter-domain style data while maintaining robustness to class labels. We instantiate this concept using a conditional diffusion model and introduce a style-fused sampling strategy to enhance data generation diversity. In contrast to traditional condition-guided sampling, our style-fused sampling strategy allows for the flexible use of one or more random styles to guide data synthesis. This feature presents a notable advancement: it allows for the maximum utilization of possible permutations and combinations among existing styles to generate a broad spectrum of new style instances. Empirical evaluations on a broad range of datasets demonstrate that our generated data achieves remarkable diversity within the domain space. Both intra- and inter-domain generated data have proven to be significant and valuable, contributing to varying degrees of performance enhancements. Notably, our approach outperforms state-of-the-art DG methods in all human activity recognition tasks.},
  archive   = {C_KDD},
  author    = {Zhang, Junru and Feng, Lang and Liu, Zhidan and Wu, Yuhan and He, Yang and Dong, Yabo and Xu, Duanqing},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671828},
  pages     = {4213–4222},
  title     = {Diverse intra- and inter-domain activity style fusion for cross-person generalization in activity recognition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Natural language explainable recommendation with robustness
enhancement. <em>KDD</em>, 4203–4212. (<a
href="https://doi.org/10.1145/3637528.3671781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Natural language explainable recommendation has become a promising direction to facilitate more efficient and informed user decisions. Previous models mostly focus on how to enhance the explanation accuracy. However, the robustness problem has been largely ignored, which requires the explanations generated for similar user-item pairs should not be too much different. Different from traditional classification problems, improving the robustness of natural languages has two unique characteristics: (1) Different token importances, that is, different tokens play various roles in representing the complete sentence, and the robustness requirements for predicting them should also be different. (2) Continuous token semantics, that is, the similarity of the output should be judged based on semantics, and the sequences without any token-level overlap may also be highly similar. Based on these characteristics, we formulate and solve a novel problem in the recommendation domain, that is, robust natural language explainable recommendation. To the best of our knowledge, it is the first time in this field. Specifically, we base our modeling on adversarial robust optimization and design four types of heuristic methods to modify the adversarial outputs with weighted token probabilities and synonym replacements. Furthermore, to consider the mutual influence between the above characteristics, we regard language generation as a decision-making problem and design a dual-policy reinforcement learning framework to improve the robustness of the generated languages. We conduct extensive experiments to demonstrate the effectiveness of our framework.},
  archive   = {C_KDD},
  author    = {Zhang, Jingsen and Tang, Jiakai and Chen, Xu and Yu, Wenhui and Hu, Lantao and Jiang, Peng and Li, Han},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671781},
  pages     = {4203–4212},
  title     = {Natural language explainable recommendation with robustness enhancement},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Enabling collaborative test-time adaptation in dynamic
environment via federated learning. <em>KDD</em>, 4191–4202. (<a
href="https://doi.org/10.1145/3637528.3671908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep learning models often suffer performance degradation when test data diverges from training data. Test-Time Adaptation (TTA) aims to adapt a trained model to the test data distribution using unlabeled test data streams. In many real-world applications, it is quite common for the trained model to be deployed across multiple devices simultaneously. Although each device can execute TTA independently, it fails to leverage information from the test data of other devices. To address this problem, we introduce Federated Learning (FL) to TTA to facilitate on-the-fly collaboration among devices during test time. The workflow involves clients (i.e., the devices) executing TTA locally, uploading their updated models to a central server for aggregation, and downloading the aggregated model for inference. However, implementing FL in TTA presents many challenges, especially in establishing inter-client collaboration in dynamic environment, where the test data distribution on different clients changes over time in different manners. To tackle these challenges, we propose a server-side Temporal-Spatial Aggregation (TSA) method. TSA utilizes a temporal-spatial attention module to capture intra-client temporal correlations and inter-client spatial correlations. To further improve robustness against temporal-spatial heterogeneity, we propose a heterogeneity-aware augmentation method and optimize the module using a self-supervised approach. More importantly, TSA can be implemented as a plug-in to TTA methods in distributed environments. Experiments on multiple datasets demonstrate that TSA outperforms existing methods and exhibits robustness across various levels of heterogeneity. The code is available at https://github.com/ZhangJiayuan-BUAA/FedTSA.},
  archive   = {C_KDD},
  author    = {Zhang, Jiayuan and Liu, Xuefeng and Zhang, Yukang and Zhu, Guogang and Niu, Jianwei and Tang, Shaojie},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671908},
  pages     = {4191–4202},
  title     = {Enabling collaborative test-time adaptation in dynamic environment via federated learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Subspace selection based prompt tuning with nonconvex
nonsmooth black-box optimization. <em>KDD</em>, 4179–4190. (<a
href="https://doi.org/10.1145/3637528.3671986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce a novel framework for black-box prompt tuning with a subspace learning and selection strategy, leveraging derivative-free optimization algorithms. This approach is crucial for scenarios where user interaction with language models is restricted to API usage, without direct access to their internal structures or gradients, a situation typical in Language-Model-as-a-Service (LMaaS). Our framework focuses on exploring the low-dimensional subspace of continuous prompts. Previous work on black-box prompt tuning necessitates a substantial number of API calls due to the random choice of the subspace. To tackle this problem, we propose to use a simple zeroth-order optimization algorithm to tackle nonconvex optimization challenges with nonsmooth nonconvex regularizers: the Zeroth-Order Mini-Batch Stochastic Proximal Gradient method (ZO-MB-SPG). A key innovation is the incorporation of nonsmooth nonconvex regularizers, including the indicator function of the l0 constraint, which enhances our ability to select optimal subspaces for prompt optimization. The experimental results show that our proposed black-box prompt tuning method on a few labeled samples can attain similar performance to the methods applicable to LMaaS with much fewer API calls.},
  archive   = {C_KDD},
  author    = {Zhang, Haozhen and Zhang, Hualin and Gu, Bin and Chang, Yi},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671986},
  pages     = {4179–4190},
  title     = {Subspace selection based prompt tuning with nonconvex nonsmooth black-box optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Item-difficulty-aware learning path recommendation: From a
real walking perspective. <em>KDD</em>, 4167–4178. (<a
href="https://doi.org/10.1145/3637528.3671947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning path recommendation aims to provide learners with a reasonable order of items to achieve their learning goals. Intuitively, the learning process on the learning path can be metaphorically likened to walking. Despite extensive efforts in this area, most previous methods mainly focus on the relationship among items but overlook the difficulty of items, which may raise two issues from a real walking perspective: (1) The path may be rough: When learners tread the path without considering item difficulty, it&#39;s akin to walking a dark, uneven road, making learning harder and dampening interest. (2) The path may be inefficient: Allowing learners only a few attempts on very challenging items before switching, or persisting with a difficult item despite numerous attempts without mastery, can result in inefficiencies in the learning journey. To conquer the above limitations, we propose a novel method named Difficulty-constrained Learning Path Recommendation (DLPR), which is aware of item difficulty. Specifically, we first explicitly categorize items into learning items and practice items, then construct a hierarchical graph to model and leverage item difficulty adequately. Then we design a Difficulty-driven Hierarchical Reinforcement Learning (DHRL) framework to facilitate learning paths with efficiency and smoothness. Finally, extensive experiments on three different simulators demonstrate our framework achieves state-of-the-art performance.},
  archive   = {C_KDD},
  author    = {Zhang, Haotian and Shen, Shuanghong and Xu, Bihan and Huang, Zhenya and Wu, Jinze and Sha, Jing and Wang, Shijin},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671947},
  pages     = {4167–4178},
  title     = {Item-difficulty-aware learning path recommendation: From a real walking perspective},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Brant-x: A unified physiological signal alignment
framework. <em>KDD</em>, 4155–4166. (<a
href="https://doi.org/10.1145/3637528.3671953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Physiological signals serve as indispensable clues for understanding various physiological states of human bodies. Most existing works have focused on a single type of physiological signals for a range of application scenarios. However, as the body is a holistic biological system, the inherent interconnection among various physiological data should not be neglected. In particular, given the brain&#39;s role as the control center for vital activities, electroencephalogram (EEG) exhibits significant correlations with other physiological signals. Therefore, the correlation between EEG and other physiological signals holds potential to improve performance in various scenarios. Nevertheless, achieving this goal is still constrained by several challenges: the scarcity of simultaneously collected physiological data, the differences in correlations between various signals, and the correlation differences between various tasks. To address these issues, we propose a unified physiological signal alignment framework, Brant-X, to model the correlation between EEG and other signals. Our approach (1) employs the EEG foundation model to data-efficiently transfer the rich knowledge in EEG to other physiological signals, and (2) introduces the two-level alignment to fully align the semantics of EEG and other signals from different semantic scales. In the experiments, Brant-X achieves state-of-the-art performance compared with task-agnostic and task-specific baselines on various downstream tasks in diverse scenarios, including sleep stage classification, emotion recognition, freezing of gaits detection, and eye movement communication. Moreover, the analysis on the arrhythmia detection task and the visualization in case study further illustrate the effectiveness of Brant-X in the knowledge transfer from EEG to other physiological signals. The model&#39;s homepage is at https://github.com/zjunet/Brant-X/.},
  archive   = {C_KDD},
  author    = {Zhang, Daoze and Yuan, Zhizhang and Chen, Junru and Chen, Kerui and Yang, Yang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671953},
  pages     = {4155–4166},
  title     = {Brant-X: A unified physiological signal alignment framework},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Path-specific causal reasoning for fairness-aware cognitive
diagnosis. <em>KDD</em>, 4143–4154. (<a
href="https://doi.org/10.1145/3637528.3672049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cognitive Diagnosis (CD), which leverages students and exercise data to predict students&#39; proficiency levels on different knowledge concepts, is one of fundamental components in Intelligent Education. Due to the scarcity of student-exercise interaction data, most existing methods focus on making the best use of available data, such as exercise content and student information (e.g., educational context). Despite the great progress, the abuse of student sensitive information has not been paid enough attention. Due to the important position of CD in Intelligent Education, employing sensitive information when making diagnosis predictions will cause serious social issues. Moreover, data-driven neural networks are easily misled by the shortcut between input data and output prediction, exacerbating this problem. Therefore, it is crucial to eliminate the negative impact of sensitive information in CD models. In response, we argue that sensitive attributes of students can also provide useful information, and only the shortcuts directly related to the sensitive information should be eliminated from the diagnosis process. Thus, we employ causal reasoning and design a novel Path-Specific Causal Reasoning Framework (PSCRF) to achieve this goal. Specifically, we first leverage an encoder to extract features and generate embeddings for general information and sensitive information of students. Then, we design a novel attribute-oriented predictor to decouple the sensitive attributes, in which fairness-related sensitive features will be eliminated and other useful information will be retained. Finally, we designed a multi-factor constraint to ensure the performance of fairness and diagnosis performance simultaneously. Extensive experiments over real-world datasets (e.g., PISA dataset) demonstrate the effectiveness of our proposed PSCRF.},
  archive   = {C_KDD},
  author    = {Zhang, Dacao and Zhang, Kun and Wu, Le and Tian, Mi and Hong, Richang and Wang, Meng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672049},
  pages     = {4143–4154},
  title     = {Path-specific causal reasoning for fairness-aware cognitive diagnosis},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GPFedRec: Graph-guided personalization for federated
recommendation. <em>KDD</em>, 4131–4142. (<a
href="https://doi.org/10.1145/3637528.3671702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The federated recommendation system is an emerging AI service architecture that provides recommendation services in a privacy-preserving manner. Using user-relation graphs to enhance federated recommendations is a promising topic. However, it is still an open challenge to construct the user-relation graph while preserving data locality-based privacy protection in federated settings. Inspired by a simple motivation, similar users share a similar vision (embeddings) to the same item set, this paper proposes a novel Graph-guided Personalization for Federated Recommendation (GPFedRec). The proposed method constructs a user-relation graph from user-specific personalized item embeddings at the server without accessing the users&#39; interaction records. The personalized item embedding is locally fine-tuned on each device, and then a user-relation graph will be constructed by measuring the similarity among client-specific item embeddings. Without accessing users&#39; historical interactions, we embody the data locality-based privacy protection of vanilla federated learning. Furthermore, a graph-guided aggregation mechanism is designed to leverage the user-relation graph and federated optimization framework simultaneously. Extensive experiments on five benchmark datasets demonstrate GPFedRec&#39;s superior performance. The in-depth study validates that GPFedRec can generally improve existing federated recommendation methods as a plugin while keeping user privacy safe. Code is available https://github.com/Zhangcx19/GPFedRec},
  archive   = {C_KDD},
  author    = {Zhang, Chunxu and Long, Guodong and Zhou, Tianyi and Zhang, Zijian and Yan, Peng and Yang, Bo},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671702},
  pages     = {4131–4142},
  title     = {GPFedRec: Graph-guided personalization for federated recommendation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conditional logical message passing transformer for complex
query answering. <em>KDD</em>, 4119–4130. (<a
href="https://doi.org/10.1145/3637528.3671869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Complex Query Answering (CQA) over Knowledge Graphs (KGs) is a challenging task. Given that KGs are usually incomplete, neural models are proposed to solve CQA by performing multi-hop logical reasoning. However, most of them cannot perform well on both one-hop and multi-hop queries simultaneously. Recent work proposes a logical message passing mechanism based on the pre-trained neural link predictors. While effective on both one-hop and multi-hop queries, it ignores the difference between the constant and variable nodes in a query graph. In addition, during the node embedding update stage, this mechanism cannot dynamically measure the importance of different messages, and whether it can capture the implicit logical dependencies related to a node and received messages remains unclear. In this paper, we propose Conditional Logical Message Passing Transformer (CLMPT), which considers the difference between constants and variables in the case of using pre-trained neural link predictors and performs message passing conditionally on the node type. We empirically verified that this approach can reduce computational costs without affecting performance. Furthermore, CLMPT uses the transformer to aggregate received messages and update the corresponding node embedding. Through the self-attention mechanism, CLMPT can assign adaptive weights to elements in an input set consisting of received messages and the corresponding node and explicitly model logical dependencies between various elements. Experimental results show that CLMPT is a new state-of-the-art neural CQA model. https://github.com/qianlima-lab/CLMPT.},
  archive   = {C_KDD},
  author    = {Zhang, Chongzhi and Peng, Zhiping and Zheng, Junhao and Ma, Qianli},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671869},
  pages     = {4119–4130},
  title     = {Conditional logical message passing transformer for complex query answering},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effective generation of feasible solutions for integer
programming via guided diffusion. <em>KDD</em>, 4107–4118. (<a
href="https://doi.org/10.1145/3637528.3671783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Feasible solutions are crucial for Integer Programming (IP) since they can substantially speed up the solving process. In many applications, similar IP instances often exhibit similar structures and shared solution distributions, which can be potentially modeled by deep learning methods. Unfortunately, existing deep-learning-based algorithms, such as Neural Diving [21] and Predict-and-search framework [8], are limited to generating only partial feasible solutions, and they must rely on solvers like SCIP and Gurobi to complete the solutions for a given IP problem. In this paper, we propose a novel framework that generates complete feasible solutions end-to-end. Our framework leverages contrastive learning to characterize the relationship between IP instances and solutions, and learns latent embeddings for both IP instances and their solutions. Further, the framework employs diffusion models to learn the distribution of solution embeddings conditioned on IP representations, with a dedicated guided sampling strategy that accounts for both constraints and objectives. We empirically evaluate our framework on four typical datasets of IP problems, and show that it effectively generates complete feasible solutions with a high probability (&amp;gt; 89.7 \%) without the reliance of Solvers and the quality of solutions is comparable to the best heuristic solutions from Gurobi. Furthermore, by integrating our method&#39;s sampled partial solutions with the CompleteSol heuristic from SCIP [19], the resulting feasible solutions outperform those from state-of-the-art methods across all datasets, exhibiting a 3.7 to 33.7\% improvement in the gap to optimal values, and maintaining a feasible ratio of over 99.7\% for all datasets.},
  archive   = {C_KDD},
  author    = {Zeng, Hao and Wang, Jiaqi and Das, Avirup and He, Junying and Han, Kunpeng and Hu, Haoyuan and Sun, Mingfei},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671783},
  pages     = {4107–4118},
  title     = {Effective generation of feasible solutions for integer programming via guided diffusion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UniST: A prompt-empowered universal model for urban
spatio-temporal prediction. <em>KDD</em>, 4095–4106. (<a
href="https://doi.org/10.1145/3637528.3671662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Urban spatio-temporal prediction is crucial for informed decision-making, such as traffic management, resource optimization, and emergence response. Despite remarkable breakthroughs in pretrained natural language models that enable one model to handle diverse tasks, a universal solution for spatio-temporal prediction remains challenging. Existing prediction approaches are typically tailored for specific spatio-temporal scenarios, requiring task-specific model designs and extensive domain-specific training data. In this study, we introduce UniST, a universal model designed for general urban spatio-temporal prediction across a wide range of scenarios. Inspired by large language models, UniST achieves success through: (i) utilizing diverse spatio-temporal data, (ii) effective pre-training to capture complex spatio-temporal relationships, (iii) spatio-temporal knowledge-guided prompts to enhance generalization capabilities. These designs together unlock the potential of building a universal model for various scenarios. Extensive experiments on more than 20 spatio-temporal scenarios demonstrate UniST&#39;s efficacy in advancing state-of-the-art performance, especially in few-shot and zero-shot prediction. The datasets and code implementation are released on https://github.com/tsinghua-fib-lab/UniST.},
  archive   = {C_KDD},
  author    = {Yuan, Yuan and Ding, Jingtao and Feng, Jie and Jin, Depeng and Li, Yong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671662},
  pages     = {4095–4106},
  title     = {UniST: A prompt-empowered universal model for urban spatio-temporal prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph cross supervised learning via generalized knowledge.
<em>KDD</em>, 4083–4094. (<a
href="https://doi.org/10.1145/3637528.3671830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The success of GNNs highly relies on the accurate labeling of data. Existing methods of ensuring accurate labels, such as weakly-supervised learning, mainly focus on the existing nodes in the graphs. However, in reality, new nodes always continuously emerge on dynamic graphs, with different categories and even label noises. To this end, we formulate a new problem, Graph Cross-Supervised Learning, or Graph Weak-Shot Learning, that describes the challenges of modeling new nodes with novel classes and potential label noises. To solve this problem, we propose Lipshitz-regularized Mixture-of-Experts similarity network (LIME), a novel framework to encode new nodes and handle label noises. Specifically, we first design a node similarity network to capture the knowledge from the original classes, aiming to obtain insights for the emerging novel classes. Then, to enhance the similarity network&#39;s generalization to new nodes that could have a distribution shift, we employ the Mixture-of-Experts technique to increase the generalization of knowledge learned by the similarity network. To further avoid losing generalization ability during training, we introduce the Lipschitz bound to stabilize model output and alleviate the distribution shift issue. Empirical experiments validate LIME&#39;s effectiveness: we observe a substantial enhancement of up to 11.34\% in node classification accuracy compared to the backbone model when subjected to the challenges of label noise on novel classes across five benchmark datasets. The code can be accessed through https://github.com/xiangchi-yuan/Graph-Cross-Supervised-Learning.},
  archive   = {C_KDD},
  author    = {Yuan, Xiangchi and Tian, Yijun and Zhang, Chunhui and Ye, Yanfang and Chawla, Nitesh V. and Zhang, Chuxu},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671830},
  pages     = {4083–4094},
  title     = {Graph cross supervised learning via generalized knowledge},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DipDNN: Preserving inverse consistency and approximation
efficiency for invertible learning. <em>KDD</em>, 4071–4082. (<a
href="https://doi.org/10.1145/3637528.3672036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Consistent bi-directional inferences are the key for many machine learning applications. Without consistency, inverse learning-based inferences can cause fuzzy images, erroneous control signals, and cascading failure in SCADA systems. Since standard deep neural networks (DNNs) are not inherently invertible to offer consistency, some past methods reconstruct DNN architecture analytically for one-to-one correspondence but compromise key features such as universal approximation. Other work maintains the capability of universal approximation in DNNs via iterative numerical approximation. However, these methods limit their applications significantly due to Lipschitz conditions and issues of numerical convergence. The dilemma of the analytical and numerical methods is the incompatibility between nonlinear layer compositions and bijective function construction for inverse modeling. Based on the observation, we propose decomposed-invertible-pathway DNNs (DipDNN). It relaxes the redundant reconstruction of nested DNN in the former methods and eases the Lipschitz constraint. As a result, we strictly guarantee the consistency of global inverse modeling without harming DNN&#39;s capability for universal approximation. As numerical stability and generalizability are keys for controlling critical infrastructures, we integrate contractive property with a parallel structure for inductive biases, leading to stable performance. Numerical results show that DipDNN performs significantly better than past methods, thanks to its enforcement of inverse consistency, numerical stability, and physical regularization.},
  archive   = {C_KDD},
  author    = {Yuan, Jingyi and Weng, Yang and Blasch, Erik},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672036},
  pages     = {4071–4082},
  title     = {DipDNN: Preserving inverse consistency and approximation efficiency for invertible learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unveiling privacy vulnerabilities: Investigating the role of
structure in graph data. <em>KDD</em>, 4059–4070. (<a
href="https://doi.org/10.1145/3637528.3672013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The public sharing of user information opens the door for adversaries to infer private data, leading to privacy breaches and facilitating malicious activities. While numerous studies have concentrated on privacy leakage via public user attributes, the threats associated with the exposure of user relationships, particularly through network structure, are often neglected. This study aims to fill this critical gap by advancing the understanding and protection against privacy risks emanating from network structure, moving beyond direct connections with neighbors to include the broader implications of indirect network structural patterns. To achieve this, we first investigate the problem of Graph Privacy Leakage via Structure (GPS), and introduce a novel measure, the Generalized Homophily Ratio, to quantify the various mechanisms contributing to privacy breach risks in GPS. Based on this insight, we develop a novel graph private attribute inference attack, which acts as a pivotal tool for evaluating the potential for privacy leakage through network structures under worst-case scenarios. To protect users&#39; private data from such vulnerabilities, we propose a graph data publishing method incorporating a learnable graph sampling technique, effectively transforming the original graph into a privacy-preserving version. Extensive experiments demonstrate that our attack model poses a significant threat to user privacy, and our graph data publishing method successfully achieves the optimal privacy-utility trade-off compared to baselines.},
  archive   = {C_KDD},
  author    = {Yuan, Hanyang and Xu, Jiarong and Wang, Cong and Yang, Ziqi and Wang, Chunping and Yin, Keting and Yang, Yang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672013},
  pages     = {4059–4070},
  title     = {Unveiling privacy vulnerabilities: Investigating the role of structure in graph data},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RIGL: A unified reciprocal approach for tracing the
independent and group learning processes. <em>KDD</em>, 4047–4058. (<a
href="https://doi.org/10.1145/3637528.3671711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the realm of education, both independent learning and group learning are esteemed as the most classic paradigms. The former allows learners to self-direct their studies, while the latter is typically characterized by teacher-directed scenarios. Recent studies in the field of intelligent education have leveraged deep temporal models to trace the learning process, capturing the dynamics of students&#39; knowledge states, and have achieved remarkable performance. However, existing approaches have primarily focused on modeling the independent learning process, with the group learning paradigm receiving less attention. Moreover, the reciprocal effect between the two learning processes, especially their combined potential to foster holistic student development, remains inadequately explored. To this end, in this paper, we propose RIGL, a unified Reciprocal model to trace knowledge states at both the individual and group levels, drawing from the Independent and Group Learning processes. Specifically, we first introduce a time frame-aware reciprocal embedding module to concurrently model both student and group response interactions across various time frames. Subsequently, we employ reciprocal enhanced learning modeling to fully exploit the comprehensive and complementary information between the two behaviors. Furthermore, we design a relation-guided temporal attentive network, comprised of dynamic graph modeling coupled with a temporal self-attention mechanism. It is used to delve into the dynamic influence of individual and group interactions throughout the learning processes, which is crafted to explore the dynamic intricacies of both individual and group interactions during the learning sequences. Conclusively, we introduce a bias-aware contrastive learning module to bolster the stability of the model&#39;s training. Extensive experiments on four real-world educational datasets clearly demonstrate the effectiveness of the proposed RIGL model. Our codes are available at https://github.com/LabyrinthineLeo/RIGL.},
  archive   = {C_KDD},
  author    = {Yu, Xiaoshan and Qin, Chuan and Shen, Dazhong and Yang, Shangshang and Ma, Haiping and Zhu, Hengshu and Zhang, Xingyi},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671711},
  pages     = {4047–4058},
  title     = {RIGL: A unified reciprocal approach for tracing the independent and group learning processes},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BoKA: Bayesian optimization based knowledge amalgamation for
multi-unknown-domain text classification. <em>KDD</em>, 4035–4046. (<a
href="https://doi.org/10.1145/3637528.3671963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With breakthroughs in pretrained language models, a large number of finetuned models specialized in distinct domains have surfaced online. Yet, when faced with a fresh dataset covering multiple (sub)domains, their performance might degrade. Reusing these available finetuned models to train a new model is a more feasible solution than the finetuning method that demands extensive manual labeling. Knowledge Amalgamation (KA) is such a model reusing technique, which derives a new model (termed student model) by amalgamating those trained models (termed teacher models) tailored for distinct domains, bypassing the need for manual labeling. However, when the domains of text samples are unknown, selecting a number of appropriate teacher models (simply called a combination) for reuse becomes complicated. To learn an accurate student model, the classical KA method resorts to manual selections, a process both tedious and inefficient. Our study pioneers the automation of this combination selection process for KA in the fundamental text classification task, an area previously unexplored.In this paper, we introduce BoKA : an automatic knowledge amalgamation framework for identifying a combination that can learn a superior student model without human labor. Through the lens of Bayesian optimization, BoKA iteratively samples a subset of possible combinations for amalgamation instead of manual selections. Furthermore, we introduce a novel KA method tailored for text classification, which guides the student model using both soft and pseudo-hard labels from the teacher models when their predictions are closely aligned; in cases of significant disagreement, it uses randomly generated labels. Experiments on two public multi-domain datasets show that BoKA achieves remarkable efficiency by sampling only up to 5.5\% of all potential combinations. Moreover, BoKA is capable of matching or even surpassing leading zero-shot large language models, despite having dozens of times fewer parameters.},
  archive   = {C_KDD},
  author    = {Yu, Linzhu and Li, Huan and Chen, Ke and Shou, Lidan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671963},
  pages     = {4035–4046},
  title     = {BoKA: Bayesian optimization based knowledge amalgamation for multi-unknown-domain text classification},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Personalized federated continual learning via
multi-granularity prompt. <em>KDD</em>, 4023–4034. (<a
href="https://doi.org/10.1145/3637528.3671948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Personalized Federated Continual Learning (PFCL) is a new practical scenario that poses greater challenges in sharing and personalizing knowledge. PFCL not only relies on knowledge fusion for server aggregation at the global spatial-temporal perspective but also needs model improvement for each client according to the local requirements. Existing methods, whether in Personalized Federated Learning (PFL) or Federated Continual Learning (FCL), have overlooked the multi-granularity representation of knowledge, which can be utilized to overcome Spatial-Temporal Catastrophic Forgetting (STCF) and adopt generalized knowledge to itself by coarse-to-fine human cognitive mechanisms. Moreover, it allows more effectively to personalized shared knowledge, thus serving its own purpose. To this end, we propose a novel concept called multi-granularity prompt, i.e., coarse-grained global prompt acquired through the common model learning process, and fine-grained local prompt used to personalize the generalized representation. The former focuses on efficiently transferring shared global knowledge without spatial forgetting, and the latter emphasizes specific learning of personalized local knowledge to overcome temporal forgetting. In addition, we design a selective prompt fusion mechanism for aggregating knowledge of global prompts distilled from different clients. By the exclusive fusion of coarse-grained knowledge, we achieve the transmission and refinement of common knowledge among clients, further enhancing the performance of personalization. Extensive experiments demonstrate the effectiveness of the proposed method in addressing STCF as well as improving personalized performance.},
  archive   = {C_KDD},
  author    = {Yu, Hao and Yang, Xin and Gao, Xin and Kang, Yan and Wang, Hao and Zhang, Junbo and Li, Tianrui},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671948},
  pages     = {4023–4034},
  title     = {Personalized federated continual learning via multi-granularity prompt},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PolygonGNN: Representation learning for polygonal geometries
with heterogeneous visibility graph. <em>KDD</em>, 4012–4022. (<a
href="https://doi.org/10.1145/3637528.3671738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Polygon representation learning is essential for diverse applications, encompassing tasks such as shape coding, building pattern classification, and geographic question answering. While recent years have seen considerable advancements in this field, much of the focus has been on single polygons, overlooking the intricate inner- and inter-polygonal relationships inherent in multipolygons. To address this gap, our study introduces a comprehensive framework specifically designed for learning representations of polygonal geometries, particularly multipolygons. Central to our approach is the incorporation of a heterogeneous visibility graph, which seamlessly integrates both inner- and inter-polygonal relationships. To enhance computational efficiency and minimize graph redundancy, we implement a heterogeneous spanning tree sampling method. Additionally, we devise a rotation-translation invariant geometric representation, ensuring broader applicability across diverse scenarios. Finally, we introduce Multipolygon-GNN, a novel model tailored to leverage the spatial and semantic heterogeneity inherent in the visibility graph. Experiments on five real-world and synthetic datasets demonstrate its ability to capture informative representations for polygonal geometries.},
  archive   = {C_KDD},
  author    = {Yu, Dazhou and Hu, Yuntong and Li, Yun and Zhao, Liang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671738},
  pages     = {4012–4022},
  title     = {PolygonGNN: Representation learning for polygonal geometries with heterogeneous visibility graph},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-consistent deep geometric learning for heterogeneous
multi-source spatial point data prediction. <em>KDD</em>, 4001–4011. (<a
href="https://doi.org/10.1145/3637528.3671737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-source spatial point data prediction is crucial in fields like environmental monitoring and natural resource management, where integrating data from various sensors is the key to achieving a holistic environmental understanding. Existing models in this area often fall short due to their domain-specific nature and lack a strategy for integrating information from various sources in the absence of ground truth labels. Key challenges include evaluating the quality of different data sources and modeling spatial relationships among them effectively. Addressing these issues, we introduce an innovative multi-source spatial point data prediction framework that adeptly aligns information from varied sources without relying on ground truth labels. A unique aspect of our method is the &#39;fidelity score,&#39; a quantitative measure for evaluating the reliability of each data source. Furthermore, we develop a geo-location-aware graph neural network tailored to accurately depict spatial relationships between data points. Our framework has been rigorously tested on two real-world datasets and one synthetic dataset. The results consistently demonstrate its superior performance over existing state-of-the-art methods.},
  archive   = {C_KDD},
  author    = {Yu, Dazhou and Gong, Xiaoyun and Li, Yun and Qiu, Meikang and Zhao, Liang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671737},
  pages     = {4001–4011},
  title     = {Self-consistent deep geometric learning for heterogeneous multi-source spatial point data prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GinAR: An end-to-end multivariate time series forecasting
model suitable for variable missing. <em>KDD</em>, 3989–4000. (<a
href="https://doi.org/10.1145/3637528.3672055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multivariate time series forecasting (MTSF) is crucial for decision-making to precisely forecast the future values/trends, based on the complex relationships identified from historical observations of multiple sequences. Recently, Spatial-Temporal Graph Neural Networks (STGNNs) have gradually become the theme of MTSF model as their powerful capability in mining spatial-temporal dependencies, but almost of them heavily rely on the assumption of historical data integrity. In reality, due to factors such as data collector failures and time-consuming repairment, it is extremely challenging to collect the whole historical observations without missing any variable. In this case, STGNNs can only utilize a subset of normal variables and easily suffer from the incorrect spatial-temporal dependency modeling issue, resulting in the degradation of their forecasting performance. To address the problem, in this paper, we propose a novel Graph Interpolation Attention Recursive Network (named GinAR) to precisely model the spatial-temporal dependencies over the limited collected data for forecasting. In GinAR, it consists of two key components, that is, interpolation attention and adaptive graph convolution to take place of the fully connected layer of simple recursive units, and thus are capable of recovering all missing variables and reconstructing the correct spatial-temporal dependencies for recursively modeling of multivariate time series data, respectively. Extensive experiments conducted on five real-world datasets demonstrate that GinAR outperforms 11 SOTA baselines, and even when 90\% of variables are missing, it can still accurately predict the future values of all variables.},
  archive   = {C_KDD},
  author    = {Yu, Chengqing and Wang, Fei and Shao, Zezhi and Qian, Tangwen and Zhang, Zhao and Wei, Wei and Xu, Yongjun},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672055},
  pages     = {3989–4000},
  title     = {GinAR: An end-to-end multivariate time series forecasting model suitable for variable missing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Top-down bayesian posterior sampling for sum-product
networks. <em>KDD</em>, 3977–3988. (<a
href="https://doi.org/10.1145/3637528.3671876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sum-product networks (SPNs) are probabilistic models characterized by exact and fast evaluation of fundamental probabilistic operations. Its superior computational tractability has led to applications in many fields, such as machine learning with time constraints or accuracy requirements and real-time systems. The structural constraints of SPNs supporting fast inference, however, lead to increased learning-time complexity and can be an obstacle to building highly expressive SPNs. This study aimed to develop a Bayesian learning approach that can be efficiently implemented on large-scale SPNs. We derived a new full conditional probability of Gibbs sampling by marginalizing multiple random variables to expeditiously obtain the posterior distribution. The complexity analysis revealed that our sampling algorithm works efficiently even for the largest possible SPN. Furthermore, we proposed a hyperparameter tuning method that balances the diversity of the prior distribution and optimization efficiency in large-scale SPNs. Our method has improved learning-time complexity and demonstrated computational speed tens to more than one hundred times faster and superior predictive performance in numerical experiments on more than 20 datasets.},
  archive   = {C_KDD},
  author    = {Yokoi, Soma and Sato, Issei},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671876},
  pages     = {3977–3988},
  title     = {Top-down bayesian posterior sampling for sum-product networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised generative feature transformation via graph
contrastive pre-training and multi-objective fine-tuning. <em>KDD</em>,
3966–3976. (<a href="https://doi.org/10.1145/3637528.3672015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Feature transformation is to derive a new feature set from original features to augment the AI power of data. In many science domains such as material performance screening, while feature transformation can model material formula interactions and compositions and discover performance drivers, supervised labels are collected from expensive and lengthy experiments. This issue motivates an Unsupervised Feature Transformation Learning (UFTL) problem. Prior literature, such as manual transformation, supervised feedback guided search, and PCA, either relies on domain knowledge or expensive supervised feedback, or suffers from large search space, or overlooks non-linear feature-feature interactions. UFTL imposes a major challenge on existing methods: how to design a new unsupervised paradigm that captures complex feature interactions and avoids large search space? To fill this gap, we connect graph, contrastive, and generative learning to develop a measurement-pretrain-finetune paradigm for UFTL. For unsupervised feature set utility measurement, we propose a feature value consistency preservation perspective and develop a mean discounted cumulative gain like unsupervised metric to evaluate feature set utility. For unsupervised feature set representation pretraining, we regard a feature set as a feature-feature interaction graph, and develop an unsupervised graph contrastive learning encoder to embed feature sets into vectors. For generative transformation finetuning, we regard a feature set as a feature cross sequence and feature transformation as sequential generation. We develop a deep generative feature transformation model that coordinates the pretrained feature set encoder and the gradient information extracted from a feature set utility evaluator to optimize a transformed feature generator. Finally, we conduct extensive experiments to demonstrate the effectiveness, efficiency, traceability, and explicitness of our framework.},
  archive   = {C_KDD},
  author    = {Ying, Wangyang and Wang, Dongjie and Hu, Xuanming and Zhou, Yuanchun and Aggarwal, Charu C. and Fu, Yanjie},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672015},
  pages     = {3966–3976},
  title     = {Unsupervised generative feature transformation via graph contrastive pre-training and multi-objective fine-tuning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dataset regeneration for sequential recommendation.
<em>KDD</em>, 3954–3965. (<a
href="https://doi.org/10.1145/3637528.3671841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The sequential recommender (SR) system is a crucial component of modern recommender systems, as it aims to capture the evolving preferences of users. Significant efforts have been made to enhance the capabilities of SR systems. These methods typically follow the model-centric paradigm, which involves developing effective models based on fixed datasets. However, this approach often overlooks potential quality issues and flaws inherent in the data. Driven by the potential of data-centric AI, we propose a novel data-centric paradigm for developing an ideal training dataset using a model-agnostic dataset regeneration framework called DR4SR. This framework enables the regeneration of a dataset with exceptional cross-architecture generalizability. Additionally, we introduce the DR4SR+ framework, which incorporates a model-aware dataset personalizer to tailor the regenerated dataset specifically for a target model. To demonstrate the effectiveness of the data-centric paradigm, we integrate our framework with various model-centric methods and observe significant performance improvements across four widely adopted datasets. Furthermore, we conduct in-depth analyses to explore the potential of the data-centric paradigm and provide valuable insights. The code can be found at https://github.com/USTC-StarTeam/DR4SR.},
  archive   = {C_KDD},
  author    = {Yin, Mingjia and Wang, Hao and Guo, Wei and Liu, Yong and Zhang, Suojuan and Zhao, Sirui and Lian, Defu and Chen, Enhong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671841},
  pages     = {3954–3965},
  title     = {Dataset regeneration for sequential recommendation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Using self-supervised learning can improve model fairness.
<em>KDD</em>, 3942–3953. (<a
href="https://doi.org/10.1145/3637528.3671991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-supervised learning (SSL) has become the de facto training paradigm of large models, where pre-training is followed by supervised fine-tuning using domain-specific data and labels. Despite demonstrating comparable performance with supervised methods, comprehensive efforts to assess SSL&#39;s impact on machine learning fairness (i.e., performing equally on different demographic breakdowns) are lacking. Hypothesizing that SSL models would learn more generic, hence less biased representations, this study explores the impact of pre-training and fine-tuning strategies on fairness. We introduce a fairness assessment framework for SSL, comprising five stages: defining dataset requirements, pre-training, fine-tuning with gradual unfreezing, assessing representation similarity conditioned on demographics, and establishing domain-specific evaluation processes. We evaluate our method&#39;s generalizability on three real-world human-centric datasets (i.e., MIMIC, MESA, and GLOBEM) by systematically comparing hundreds of SSL and fine-tuned models on various dimensions spanning from the intermediate representations to appropriate evaluation metrics. Our findings demonstrate that SSL can significantly improve model fairness, while maintaining performance on par with supervised methods-exhibiting up to a 30\% increase in fairness with minimal loss in performance through self-supervision. We posit that such differences can be attributed to representation dissimilarities found between the best- and the worst-performing demographics across models-up to x13 greater for protected attributes with larger performance discrepancies between segments. Code: https://github.com/Nokia-Bell-Labs/SSLfairness},
  archive   = {C_KDD},
  author    = {Yfantidou, Sofia and Spathis, Dimitris and Constantinides, Marios and Vakali, Athena and Quercia, Daniele and Kawsar, Fahim},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671991},
  pages     = {3942–3953},
  title     = {Using self-supervised learning can improve model fairness},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Embedding two-view knowledge graphs with class inheritance
and structural similarity. <em>KDD</em>, 3931–3941. (<a
href="https://doi.org/10.1145/3637528.3671941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Numerous large-scale knowledge graphs (KGs) fundamentally represent two-view KGs: an ontology-view KG with abstract classes in ontology and an instance-view KG with specific collections of entities instantiated from ontology classes. Two-view KG embedding aims to jointly learn continuous vector representations of entities and relations in the aforementioned two-view KGs. In essence, an ontology schema exhibits a tree-like structure guided by class hierarchies, which leads classes to form inheritance hierarchies. However, existing two-view KG embedding models neglect those hierarchies, which provides the necessity to reflect class inheritance. On the other hand, KG is constructed based on a pre-defined ontology schema that includes heterogeneous relations between classes. Furthermore, these relations are defined within the scope of those among classes since instances inherit all the properties of their corresponding classes, which reveals structural similarity between two multi-relational networks. Despite the consideration to bridge the gap among two-view KG representations, existing methods ignore the existence of structural similarity between two-view KGs. To address these issues, we propose a novel two-view KG embedding model, CISS, considering Class Inheritance and Structural Similarity between two-view KGs. To deal with class inheritance, we utilize class sets, each of which is composed of sibling classes, to learn fine-grained class representations. In addition, we configure virtual instance-view KG from clustered instances and compare subgraph representations of two-view KGs to enhance structural similarity between them. Experimental results show our superior performance compared to existing models.},
  archive   = {C_KDD},
  author    = {Yeom, Kyuhwan and Yang, Hyeongjun and Park, Gayeon and Jeon, Myeongheon and Ko, Yunjeong and Oh, Byungkook and Lee, Kyong-Ho},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671941},
  pages     = {3931–3941},
  title     = {Embedding two-view knowledge graphs with class inheritance and structural similarity},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RPMixer: Shaking up time series forecasting with random
projections for large spatial-temporal data. <em>KDD</em>, 3919–3930.
(<a href="https://doi.org/10.1145/3637528.3671881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Spatial-temporal forecasting systems play a crucial role in addressing numerous real-world challenges. In this paper, we investigate the potential of addressing spatial-temporal forecasting problems using general time series forecasting models, i.e., models that do not leverage the spatial relationships among the nodes. We propose a all-Multi-Layer Perceptron (all-MLP) time series forecasting architecture called RPMixer. The all-MLP architecture was chosen due to its recent success in time series forecasting benchmarks. Furthermore, our method capitalizes on the ensemble-like behavior of deep neural networks, where each individual block within the network behaves like a base learner in an ensemble model, particularly when identity mapping residual connections are incorporated. By integrating random projection layers into our model, we increase the diversity among the blocks&#39; outputs, thereby improving the overall performance of the network. Extensive experiments conducted on the largest spatial-temporal forecasting benchmark datasets demonstrate that the proposed method outperforms 14 alternative methods.},
  archive   = {C_KDD},
  author    = {Yeh, Chin-Chia Michael and Fan, Yujie and Dai, Xin and Saini, Uday Singh and Lai, Vivian and Aboagye, Prince Osei and Wang, Junpeng and Chen, Huiyuan and Zheng, Yan and Zhuang, Zhongfang and Wang, Liang and Zhang, Wei},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671881},
  pages     = {3919–3930},
  title     = {RPMixer: Shaking up time series forecasting with random projections for large spatial-temporal data},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient and effective anchored densest subgraph search: A
convex-programming based approach. <em>KDD</em>, 3907–3918. (<a
href="https://doi.org/10.1145/3637528.3671727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The quest to identify local dense communities closely connected to predetermined seed nodes is vital across numerous applications. Given the seed nodes R, the R-subgraph density of a subgraph S is defined as traditional graph density of S with penalties on the nodes in S / R. The state-of-the-art (SOTA) anchored densest subgraph model, which is based on R-subgraph density, is designed to address the community search problem. However, it often struggles to efficiently uncover truly dense communities. To eliminate this issue, we propose a novel NR-subgraph density metric, a nuanced measure that identifies communities intimately linked to seed nodes and also exhibiting overall high graph density. We redefine the anchored densest subgraph search problem through the lens of NR-subgraph density and cast it as a Linear Programming (LP) problem. This allows us to transition into a dual problem, tapping into the efficiency and effectiveness of convex programming-based iterative algorithm. To solve this redefined problem, we propose two algorithms: FDP, an iterative method that swiftly attains near-optimal solutions, and FDPE, an exact approach that ensures full convergence. We perform extensive experiments on 12 real-world networks. The results show that our proposed algorithms not only outperform the SOTA methods by 3.6~14.1 times in terms of running time, but also produce subgraphs with superior internal quality.},
  archive   = {C_KDD},
  author    = {Ye, Xiaowei and Li, Rong-Hua and Liang, Lei and Liu, Zhizhen and Lin, Longlong and Wang, Guoren},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671727},
  pages     = {3907–3918},
  title     = {Efficient and effective anchored densest subgraph search: A convex-programming based approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Approximate matrix multiplication over sliding windows.
<em>KDD</em>, 3896–3906. (<a
href="https://doi.org/10.1145/3637528.3671819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large-scale streaming matrix multiplication is very common in various applications, sparking significant interest in develop efficient algorithms for approximate matrix multiplication (AMM) over streams. In addition, many practical scenarios require to process time-sensitive data and aim to compute matrix multiplication for most recent columns of the data matrices rather than the entire matrices, which motivated us to study efficient AMM algorithms over sliding windows. In this paper, we present two novel deterministic algorithms for this problem and provide corresponding error guarantees. We further reduce the space and time costs of our methods for sparse matrices by performing an approximate singular value decomposition which can utilize the sparsity of matrices. Extensive experimental results on both synthetic and real-world datasets validate our theoretical analysis and highlight the efficiency of our methods.},
  archive   = {C_KDD},
  author    = {Yao, Ziqi and Li, Lianzhi and Chen, Mingsong and Wei, Xian and Chen, Cheng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671819},
  pages     = {3896–3906},
  title     = {Approximate matrix multiplication over sliding windows},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AdaRD: An adaptive response denoising framework for robust
learner modeling. <em>KDD</em>, 3886–3895. (<a
href="https://doi.org/10.1145/3637528.3671684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learner modeling is a crucial task in online learning environments, where Cognitive Diagnosis Models (CDMs) are employed to assess learners&#39; knowledge mastery levels based on recorded response logs. However, the prevalence of noise in recorded response data poses significant challenges, including various behaviors such as guess and slip, casual answers, and system-induced errors. The existence of noise degrades the accuracy of diagnosis results and learner performance predictions. In this work, we propose a general framework, Adaptive Response Denoising (AdaRD), designed to salvage CDMs from the influence of noisy learner-exercise responses. AdaRD extends existing CDMs, incorporating primary training for denoised CDMs and auxiliary training for additional denoising support. The primary training employs binary Generalized Cross Entropy (GCE) loss to slow down the large update of learner knowledge states caused by noisy responses. Simultaneously, we utilize the variance of diagnosed knowledge mastery levels between primary and auxiliary diagnosis modules as a criterion to downweight high-variance responses that are likely to be noisy. In this manner, the proposed framework can prune noisy response learning during training, thereby enhancing the accuracy and robustness of CDMs. Extensive experiments on both real-world and synthetic datasets validate AdaRD&#39;s effectiveness in mitigating the impact of noisy learner-exercise responses.},
  archive   = {C_KDD},
  author    = {Yao, Fangzhou and Liu, Qi and Yue, Linan and Gao, Weibo and Li, Jiatong and Li, Xin and He, Yuanjing},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671684},
  pages     = {3886–3895},
  title     = {AdaRD: An adaptive response denoising framework for robust learner modeling},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). User welfare optimization in recommender systems with
competing content creators. <em>KDD</em>, 3874–3885. (<a
href="https://doi.org/10.1145/3637528.3672021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Driven by the new economic opportunities created by the creator economy, an increasing number of content creators rely on and compete for revenue generated from online content recommendation platforms. This burgeoning competition reshapes the dynamics of content distribution and profoundly impacts long-term user welfare on the platform. However, the absence of a comprehensive picture of global user preference distribution often traps the competition, especially the creators, in states that yield sub-optimal user welfare. To encourage creators to best serve a broad user population with relevant content, it becomes the platform&#39;s responsibility to leverage its information advantage regarding user preference distribution to accurately signal creators.In this study, we perform system-side user welfare optimization under a competitive game setting among content creators. We propose an algorithmic solution for the platform, which dynamically computes a sequence of weights for each user based on their satisfaction of the recommended content. These weights are then utilized to design mechanisms that adjust the recommendation policy or the post-recommendation rewards, thereby influencing creators&#39; content production strategies. To validate the effectiveness of our proposed method, we report our findings from a series of experiments, including: 1. a proof-of-concept negative example illustrating how creators&#39; strategies converge towards sub-optimal states without platform intervention; 2. offline experiments employing our proposed intervention mechanisms on diverse datasets; and 3. results from a three-week online experiment conducted on Instagram Reels short-video recommendation platform.},
  archive   = {C_KDD},
  author    = {Yao, Fan and Liao, Yiming and Wu, Mingzhe and Li, Chuanhao and Zhu, Yan and Yang, James and Liu, Jingzhou and Wang, Qifan and Xu, Haifeng and Wang, Hongning},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672021},
  pages     = {3874–3885},
  title     = {User welfare optimization in recommender systems with competing content creators},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rethinking order dispatching in online ride-hailing
platforms. <em>KDD</em>, 3863–3873. (<a
href="https://doi.org/10.1145/3637528.3672028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Achieving optimal order dispatching has been a long-standing challenge for online ride-hailing platforms. Early methods would make shortsighted matchings as they only consider order prices alone as the edge weights in the driver-order bipartite graph, thus harming the platform&#39;s revenue. To address this problem, recent works evaluate the value of the order&#39;s destination region to be the long-term income a driver could obtain in average in such region and incorporate it into the order&#39;s edge weight to influence the matching results. However, they often result in insufficient driver supplies in many regions, as the values evaluated in different regions vary greatly, mainly because the impact of one region&#39;s value on the future number of drivers and revenue in other regions is overlooked. This paper models such impact within a cooperative Markov game, which involves each value&#39;s impact over the platform&#39;s revenue with the goal to find the optimal region values for revenue maximization. To solve this game, our work proposes a novelgoal-reaching collaboration (GRC) algorithm that realizes credit assignment from a novel goal-reaching perspective, addressing the difficulty for accurate credit assignment with large-scale agents of previous methods and resolving the conflict between credit assignment and offline reinforcement learning. Specifically, during training, GRC predicts the city&#39;s future state through an environment model and utilizes a scoring model to rate the predicted states to judge their levels of profitability, where high-scoring states are regarded as the goal states. Then, the policies in the game are updated to promote the city to stay in the goal states for as long as possible. To evaluate GRC, we deploy a baseline policy online in several cities for three weeks to collect real-world dataset. Training and testing results on the collected dataset indicate that our GRC consistently outperforms the baselines in different cities and peak periods.},
  archive   = {C_KDD},
  author    = {Yang, Zhaoxing and Jin, Haiming and Fan, Guiyun and Lu, Min and Liu, Yiran and Yue, Xinlang and Pan, Hao and Xu, Zhe and Wu, Guobin and Li, Qun and Wang, Xiaotong and Guo, Jiecheng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672028},
  pages     = {3863–3873},
  title     = {Rethinking order dispatching in online ride-hailing platforms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph bottlenecked social recommendation. <em>KDD</em>,
3853–3862. (<a href="https://doi.org/10.1145/3637528.3671807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the emergence of social networks, social recommendation has become an essential technique for personalized services. Recently, graph-based social recommendations have shown promising results by capturing the high-order social influence. Most empirical studies of graph-based social recommendations directly take the observed social networks into formulation, and produce user preferences based on social homogeneity. Despite the effectiveness, we argue that social networks in the real-world are inevitably noisy~(existing redundant social relations), which may obstruct precise user preference characterization. Nevertheless, identifying and removing redundant social relations is challenging due to a lack of labels. In this paper, we focus on learning the denoised social structure to facilitate recommendation tasks from an information bottleneck perspective. Specifically, we propose a novel Graph Bottlenecked Social Recommendation (GBSR) framework to tackle the social noise issue. GBSR is a model-agnostic social denoising framework, that aims to maximize the mutual information between the denoised social graph and recommendation labels, meanwhile minimizing it between the denoised social graph and the original one. This enables GBSR to learn the minimal yet sufficient social structure, effectively reducing redundant social relations and enhancing social recommendations. Technically, GBSR consists of two elaborate components, preference-guided social graph refinement, and HSIC-based bottleneck learning. Extensive experimental results demonstrate the superiority of the proposed GBSR, including high performances and good generality combined with various backbones. Our code is available at: https://github.com/yimutianyang/KDD24-GBSR.},
  archive   = {C_KDD},
  author    = {Yang, Yonghui and Wu, Le and Wang, Zihan and He, Zhuangzhuang and Hong, Richang and Wang, Meng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671807},
  pages     = {3853–3862},
  title     = {Graph bottlenecked social recommendation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SEBot: Structural entropy guided multi-view contrastive
learning for social bot detection. <em>KDD</em>, 3841–3852. (<a
href="https://doi.org/10.1145/3637528.3671871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advancements in social bot detection have been driven by the adoption of Graph Neural Networks. The social graph, constructed from social network interactions, contains benign and bot accounts that influence each other. However, previous graph-based detection methods that follow the transductive message-passing paradigm may not fully utilize hidden graph information and are vulnerable to adversarial bot behavior. The indiscriminate message passing between nodes from different categories and communities results in excessively homogeneous node representations, ultimately reducing the effectiveness of social bot detectors. In this paper, we propose SEBot, a novel multi-view graph-based contrastive learning-enabled social bot detector. In particular, we use structural entropy as an uncertainty metric to optimize the entire graph&#39;s structure and subgraph-level granularity, revealing the implicitly existing hierarchical community structure. And we design an encoder to enable message passing beyond the homophily assumption, enhancing robustness to adversarial behaviors of social bots. Finally, we employ multi-view contrastive learning to maximize mutual information between different views and enhance the detection performance through multi-task learning. Experimental results demonstrate that our approach significantly improves the performance of social bot detection compared with SOTA methods.},
  archive   = {C_KDD},
  author    = {Yang, Yingguang and Wu, Qi and He, Buyun and Peng, Hao and Yang, Renyu and Hao, Zhifeng and Liao, Yong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671871},
  pages     = {3841–3852},
  title     = {SEBot: Structural entropy guided multi-view contrastive learning for social bot detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Your neighbor matters: Towards fair decisions under
networked interference. <em>KDD</em>, 3829–3840. (<a
href="https://doi.org/10.1145/3637528.3671960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the era of big data, decision-making in social networks may introduce bias due to interconnected individuals. For instance, in peer-to-peer loan platforms on the Web, considering an individual&#39;s attributes along with those of their interconnected neighbors, including sensitive attributes, is vital for loan approval or rejection downstream. Unfortunately, conventional fairness approaches often assume independent individuals, overlooking the impact of one person&#39;s sensitive attribute on others&#39; decisions. To fill this gap, we introduce &quot;Interference-aware Fairness&quot; (IAF) by defining two forms of discrimination as Self-Fairness (SF) and Peer-Fairness (PF), leveraging advances in interference analysis within causal inference. Specifically, SF and PF causally capture and distinguish discrimination stemming from an individual&#39;s sensitive attributes (with fixed neighbors&#39; sensitive attributes) and from neighbors&#39; sensitive attributes (with fixed self&#39;s sensitive attributes), separately. Hence, a network-informed decision model is fair only when SF and PF are satisfied simultaneously, as interventions in individuals&#39; sensitive attributes or those of their peers both yield equivalent outcomes. To achieve IAF, we develop a deep doubly robust framework to estimate and regularize SF and PF metrics for decision models. Extensive experiments on synthetic and real-world datasets validate our proposed concepts and methods.},
  archive   = {C_KDD},
  author    = {Yang, Wenjing and Wang, Haotian and Li, Haoxuan and Zou, Hao and Jin, Ruochun and Kuang, Kun and Cui, Peng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671960},
  pages     = {3829–3840},
  title     = {Your neighbor matters: Towards fair decisions under networked interference},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ReCDA: Concept drift adaptation with representation
enhancement for network intrusion detection. <em>KDD</em>, 3818–3828.
(<a href="https://doi.org/10.1145/3637528.3672007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The deployment of learning-based models to detect malicious activities in network traffic flows is significantly challenged by concept drift. With evolving attack technology and dynamic attack behaviors, the underlying data distribution of recently arrived traffic flows deviates from historical empirical distributions over time. Existing approaches depend on a significant amount of labeled drifting samples to facilitate the deep model to handle concept drift, which faces labor-intensive manual labeling and the risk of label noise. In this paper, we propose ReCDA, a Concept Drift Adaptation method with Representation enhancement, which consists of a self-supervised representation enhancement stage and a weakly-supervised classifier tuning stage. Specifically, in the initial stage, ReCDA introduces drift-aware perturbation and representation alignment to facilitate the model in acquiring robust representations from drift-aware and drift-invariant perspectives. Moreover, in the subsequent stage, a meticulously crafted instructive sampling strategy and a robust representation constraint encourage the model to learn discriminative knowledge about benign and malicious activities during fine-tuning, thereby enhancing performance further. We conduct comprehensive evaluations on several benchmark datasets under varying degrees of concept drift. The experiment results demonstrate the superior adaptability and robustness of the proposed method.},
  archive   = {C_KDD},
  author    = {Yang, Shuo and Zheng, Xinran and Li, Jinze and Xu, Jinfeng and Wang, Xingjun and Ngai, Edith C. H.},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672007},
  pages     = {3818–3828},
  title     = {ReCDA: Concept drift adaptation with representation enhancement for network intrusion detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conversational dueling bandits in generalized linear models.
<em>KDD</em>, 3806–3817. (<a
href="https://doi.org/10.1145/3637528.3671892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Conversational recommendation systems elicit user preferences by interacting with users to obtain their feedback on recommended commodities. Such systems utilize a multi-armed bandit framework to learn user preferences in an online manner and have received great success in recent years. However, existing conversational bandit methods have several limitations. First, they only enable users to provide explicit binary feedback on the recommended items or categories, leading to ambiguity in interpretation. In practice, users are usually faced with more than one choice. Relative feedback, known for its informativeness, has gained increasing popularity in recommendation system design. Moreover, current contextual bandit methods mainly work under linear reward assumptions, ignoring practical non-linear reward structures in generalized linear models. Therefore, in this paper, we introduce relative feedback-based conversations into conversational recommendation systems through the integration of dueling bandits in generalized linear models (GLM) and propose a novel conversational dueling bandit algorithm called ConDuel. Theoretical analyses of regret upper bounds and empirical validations on synthetic and real-world data underscore ConDuel&#39;s efficacy. We also demonstrate the potential to extend our algorithm to multinomial logit bandits with theoretical and experimental guarantees, which further proves the applicability of the proposed framework.},
  archive   = {C_KDD},
  author    = {Yang, Shuhua and Yuan, Hui and Zhang, Xiaoying and Wang, Mengdi and Zhang, Hong and Wang, Huazheng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671892},
  pages     = {3806–3817},
  title     = {Conversational dueling bandits in generalized linear models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Practical single domain generalization via training-time and
test-time learning. <em>KDD</em>, 3794–3805. (<a
href="https://doi.org/10.1145/3637528.3671806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Single domain generalization aims to learn a model that generalizes well to unseen target domains by using a related source domain. However, most existing methods only focus on improving the generalization performance of the model during training, making it difficult to achieve satisfactory performance when deployed in the target domain with large domain shifts. In this paper, we propose a Practical Single Domain Generalization (PSDG) method, which first leverages the knowledge in a source domain to establish a model with good generalization ability in the training phase, and subsequently updates the model to adapt to target domain data using knowledge in the unlabeled target domain during the testing phase. Specifically, during training, PSDG leverages a newly proposed style (e.g., background features) generator named StyIN to generate novel domain data. Moreover, PSDG introduces style-diversity regularization to constantly synthesize distinct styles to expand the coverage of training data, and introduces object-consistency regularization to capture consistency between the currently generated data and the original data, making the model filter style knowledge during training. During testing, PSDG uses a sample-aware and sharpness-aware minimization method to seek for a flat entropy minimum surface for further model optimization by using the knowledge in the unlabeled target domain. Using three real-world datasets the experiments have demonstrated the effectiveness of PSDG, in comparison with several state-of-the-art methods.},
  archive   = {C_KDD},
  author    = {Yang, Shuai and Zhang, Zhen and Gu, Lichuan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671806},
  pages     = {3794–3805},
  title     = {Practical single domain generalization via training-time and test-time learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effective clustering on large attributed bipartite graphs.
<em>KDD</em>, 3782–3793. (<a
href="https://doi.org/10.1145/3637528.3671764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Attributed bipartite graphs (ABGs) are an expressive data model for describing the interactions between two sets of heterogeneous nodes that are associated with rich attributes, such as customer-product purchase networks and author-paper authorship graphs. Partitioning the target node set in such graphs into k disjoint clusters (referred to as k-ABGC) finds widespread use in various domains, including social network analysis, recommendation systems, information retrieval, and bioinformatics. However, the majority of existing solutions towards k-ABGC either overlook attribute information or fail to capture bipartite graph structures accurately, engendering severely compromised result quality. The severity of these issues is accentuated in real ABGs, which often encompass millions of nodes and a sheer volume of attribute data, rendering effective k-ABGC over such graphs highly challenging.In this paper, we propose TPO, an effective and efficient approach to k-ABGC that achieves superb clustering performance on multiple real datasets. TPO obtains high clustering quality through two major contributions: (i) a novel formulation and transformation of the k-ABGC problem based on multi-scale attribute affinity specialized for capturing attribute affinities between nodes with the consideration of their multi-hop connections in ABGs, and (ii) a highly efficient solver that includes a suite of carefully-crafted optimizations for sidestepping explicit affinity matrix construction and facilitating faster convergence. Extensive experiments, comparing TPO against 19 baselines over 5 real ABGs, showcase the superior clustering quality of TPO measured against ground-truth labels. Moreover, compared to the state of the arts, TPO is often more than 40x faster over both small and large ABGs.},
  archive   = {C_KDD},
  author    = {Yang, Renchi and Wu, Yidu and Lin, Xiaoyang and Wang, Qichen and Chan, Tsz Nam and Shi, Jieming},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671764},
  pages     = {3782–3793},
  title     = {Effective clustering on large attributed bipartite graphs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hypformer: Exploring efficient transformer fully in
hyperbolic space. <em>KDD</em>, 3770–3781. (<a
href="https://doi.org/10.1145/3637528.3672039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hyperbolic geometry have shown significant potential in modeling complex structured data, particularly those with underlying tree-like and hierarchical structures. Despite the impressive performance of various hyperbolic neural networks across numerous domains, research on adapting the Transformer to hyperbolic space remains limited. Previous attempts have mainly focused on modifying self-attention modules in the Transformer. However, these efforts have fallen short of developing a complete hyperbolic Transformer. This stems primarily from: (i) the absence of well-defined modules in hyperbolic space, including linear transformation layers, LayerNorm layers, activation functions, dropout operations, etc. (ii) the quadratic time complexity of the existing hyperbolic self-attention module w.r.t the number of input tokens, which hinders its scalability. To address these challenges, we propose, Hypformer, a novel hyperbolic Transformer based on the Lorentz model of hyperbolic geometry. In Hypformer, we introduce two foundational blocks that define the essential modules of the Transformer in hyperbolic space. Furthermore, we develop a linear self-attention mechanism in hyperbolic space, enabling hyperbolic Transformer to process billion-scale graph data and long-sequence inputs for the first time. Our experimental results confirm the effectiveness and efficiency of method across various datasets, demonstrating its potential as an effective and scalable solution for large-scale data representation and large models.},
  archive   = {C_KDD},
  author    = {Yang, Menglin and Verma, Harshit and Zhang, Delvin Ce and Liu, Jiahong and King, Irwin and Ying, Rex},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672039},
  pages     = {3770–3781},
  title     = {Hypformer: Exploring efficient transformer fully in hyperbolic space},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Efficient decision rule list learning via unified sequence
submodular optimization. <em>KDD</em>, 3758–3769. (<a
href="https://doi.org/10.1145/3637528.3671827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Interpretable models are crucial in many high-stakes decision-making applications. In this paper, we focus on learning a decision rule list for binary and multi-class classification. Different from rule set learning problems, learning an optimal rule list involves not only learning a set of rules, but also their orders. In addition, many existing algorithms rely on rule pre-mining to handle large-scale high-dimensional data, which leads to suboptimal rule list model and degrades its generalization accuracy and interpretablity. In this paper, we learn a rule list from the sequence submodular perspective. We consider the rule list as a sequence and define the cover set for each rule. Then we formulate a sequence function which combines both model complexity and classification accuracy. Based on its appealing sequence submodular property, we propose a general distorted greedy insert algorithm under Minorization-Maximization (MM) framework, which gradually inserts rules with highest inserting gain to the rule list. The rule generation process is treated as a subproblem, allowing our method to learn the rule list through a unified framework which avoids rule pre-mining. We further provide a theoretical lower bound of our greedy insert algorithm in rule list learning. Experimental results show that our algorithm achieves better accuracy and interpretability than the state-of-the-art rule learning methods, and in particular it scales well on large-scale datasets, especially on high-dimensional data.},
  archive   = {C_KDD},
  author    = {Yang, Linxiao and Yang, Jingbang and Sun, Liang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671827},
  pages     = {3758–3769},
  title     = {Efficient decision rule list learning via unified sequence submodular optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Balanced confidence calibration for graph neural networks.
<em>KDD</em>, 3747–3757. (<a
href="https://doi.org/10.1145/3637528.3671741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper delves into the confidence calibration in prediction when using Graph Neural Networks (GNNs), which has emerged as a notable challenge in the field. Despite their remarkable capabilities in processing graph-structured data, GNNs are prone to exhibit lower confidence in their predictions than what the actual accuracy warrants. Recent advances attempt to address this by minimizing prediction entropy to enhance confidence levels. However, this method inadvertently risks leading to over-confidence in model predictions. Our investigation in this work reveals that most existing GNN calibration methods predominantly focus on the highest logit, thereby neglecting the entire spectrum of prediction probabilities. To alleviate this limitation, we introduce a novel framework called Balanced Calibrated Graph Neural Network (BCGNN), specifically designed to establish a balanced calibration between over-confidence and under-confidence in GNNs&#39; prediction. To theoretically support our proposed method, we further demonstrate the mechanism of the BCGNN framework in effective confidence calibration and significant trustworthiness improvement in prediction. We conduct extensive experiments to examine the developed framework. The empirical results show our method&#39;s superior performance in predictive confidence and trustworthiness, affirming its practical applicability and effectiveness in real-world scenarios.},
  archive   = {C_KDD},
  author    = {Yang, Hao and Wang, Min and Wang, Qi and Lao, Mingrui and Zhou, Yun},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671741},
  pages     = {3747–3757},
  title     = {Balanced confidence calibration for graph neural networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards test time adaptation via calibrated entropy
minimization. <em>KDD</em>, 3736–3746. (<a
href="https://doi.org/10.1145/3637528.3671672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robust models must demonstrate strong generalizability, even amid environmental changes. However, the complex variability and noise in real-world data often lead to a pronounced performance gap between the training and testing phases. Researchers have recently introduced test-time-domain adaptation (TTA) to address this challenge. TTA methods primarily adapt source-pretrained models to a target domain using only unlabeled test data. This study found that existing TTA methods consider only the largest logit as a pseudo-label and aim to minimize the entropy of test time predictions. This maximizes the predictive confidence of the model. However, this corresponds to the model being overconfident in the local test scenarios. In response, we introduce a novel confidence-calibration loss function called Calibrated Entropy Test-Time Adaptation (CETA), which considers the model&#39;s largest logit and the next-highest-ranked one, aiming to strike a balance between overconfidence and underconfidence. This was achieved by incorporating a sample-wise regularization term. We also provide a theoretical foundation for the proposed loss function. Experimentally, our method outperformed existing strategies on benchmark corruption datasets across multiple models, underscoring the efficacy of our approach.},
  archive   = {C_KDD},
  author    = {Yang, Hao and Wang, Min and Jiang, Jinshen and Zhou, Yun},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671672},
  pages     = {3736–3746},
  title     = {Towards test time adaptation via calibrated entropy minimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Noisy label removal for partial multi-label learning.
<em>KDD</em>, 3724–3735. (<a
href="https://doi.org/10.1145/3637528.3671677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the problem of partial multi-label learning (PML), a challenging weakly supervised learning framework, where each sample is associated with a candidate label set comprising both ground-true labels and noisy labels. We theoretically reveal that an increased number of noisy labels in the candidate label set leads to an enlarged generalization error bound, consequently degrading the classification performance. Accordingly, the key to solving PML lies in accurately removing the noisy labels within the candidate label set. To achieve this objective, we leverage prior knowledge about the noisy labels in PML, which suggests that they only exist within the candidate label set and possess binary values. Specifically, we propose a constrained regression model to learn a PML classifier and select the noisy labels. The constraints of the model strictly enforce the location and value of the noisy labels. Simultaneously, the supervision information provided by the candidate label set is unreliable due to the presence of noisy labels. In contrast, the non-candidate labels of a sample precisely indicate the classes to which the sample does not belong. To aid in the selection of noisy labels, we construct a competitive classifier based on the non-candidate labels. The PML classifier and the competitive classifier form a competitive relationship, encouraging mutual learning. We formulate the proposed model as a discrete optimization problem to effectively remove the noisy labels, and we solve it using an alternative algorithm. Extensive experiments conducted on 6 real-world partial multi-label data sets and 7 synthetic data sets, employing various evaluation metrics, demonstrate that our method significantly outperforms state-of-the-art PML methods. The code implementation is publicly available at https://github.com/Yangfc-ML/NLR.},
  archive   = {C_KDD},
  author    = {Yang, Fuchao and Jia, Yuheng and Liu, Hui and Dong, Yongqiang and Hou, Junhui},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671677},
  pages     = {3724–3735},
  title     = {Noisy label removal for partial multi-label learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Revisiting reciprocal recommender systems: Metrics,
formulation, and method. <em>KDD</em>, 3714–3723. (<a
href="https://doi.org/10.1145/3637528.3671734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reciprocal recommender systems~(RRS), conducting bilateral recommendations between two involved parties, have gained increasing attention for enhancing matching efficiency. However, the majority of existing methods in the literature still reuse conventional ranking metrics to separately assess the performance on each side of the recommendation process. These methods overlook the fact that the ranking outcomes of both sides collectively influence the effectiveness of the RRS, neglecting the necessity of a more holistic evaluation and a capable systemic solution.In this paper, we systemically revisit the task of reciprocal recommendation, by introducing the new metrics, formulation, and method. Firstly, we propose five new evaluation metrics that comprehensively and accurately assess the performance of RRS from three distinct perspectives: overall coverage, bilateral stability, and balanced ranking. These metrics provide a more holistic understanding of the system&#39;s effectiveness and enable a comprehensive evaluation. Furthermore, we formulate the RRS from a causal perspective, formulating recommendations as bilateral interventions, which can better model the decoupled effects of potential influencing factors. By utilizing the potential outcome framework, we further develop a model-agnostic causal reciprocal recommendation method that considers the causal effects of recommendations. Additionally, we introduce a reranking strategy to maximize matching outcomes, as measured by the proposed metrics. Extensive experiments on two real-world datasets from recruitment and dating scenarios demonstrate the effectiveness of our proposed metrics and approach. The code and dataset are available at: https://github.com/RUCAIBox/CRRS.},
  archive   = {C_KDD},
  author    = {Yang, Chen and Dai, Sunhao and Hou, Yupeng and Zhao, Wayne Xin and Xu, Jun and Song, Yang and Zhu, Hengshu},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671734},
  pages     = {3714–3723},
  title     = {Revisiting reciprocal recommender systems: Metrics, formulation, and method},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient subgraph GNN with provable substructure
counting power. <em>KDD</em>, 3702–3713. (<a
href="https://doi.org/10.1145/3637528.3671731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate the enhancement of graph neural networks&#39; (GNNs) representation power through their ability in substructure counting. Recent advances have seen the adoption of subgraph GNNs, which partition an input graph into numerous subgraphs, subsequently applying GNNs to each to augment the graph&#39;s overall representation. Despite their ability to identify various substructures, subgraph GNNs are hindered by significant computational and memory costs. In this paper, we tackle a critical question: Is it possible for GNNs to count substructures both efficiently and provably? Our approach begins with a theoretical demonstration that the distance to rooted nodes in subgraphs is key to boosting the counting power of subgraph GNNs. To avoid the need for repetitively applying GNN across all subgraphs, we introduce precomputed structural embeddings that encapsulate this crucial distance information. Experiments validate that our proposed model retains the counting power of subgraph GNNs while achieving significantly faster performance.},
  archive   = {C_KDD},
  author    = {Yan, Zuoyu and Zhou, Junru and Gao, Liangcai and Tang, Zhi and Zhang, Muhan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671731},
  pages     = {3702–3713},
  title     = {An efficient subgraph GNN with provable substructure counting power},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient mixture of experts based on large language models
for low-resource data preprocessing. <em>KDD</em>, 3690–3701. (<a
href="https://doi.org/10.1145/3637528.3671873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Data preprocessing (DP) that transforms erroneous and raw data to a clean version is a cornerstone of the data mining pipeline. Due to the diverse requirements of downstream tasks, data scientists and domain experts have to handcraft domain-specific rules or train ML models with annotated examples, which is costly/time-consuming. In this paper, we present MELD (&amp;lt;u&amp;gt;M&amp;lt;/u&amp;gt;ixture of &amp;lt;u&amp;gt;E&amp;lt;/u&amp;gt;xperts on &amp;lt;u&amp;gt;L&amp;lt;/u&amp;gt;arge Language Models for &amp;lt;u&amp;gt;D&amp;lt;/u&amp;gt;ata Preprocessing), a universal solver for low-resource DP. MELD adopts a Mixture-of-Experts (MoE) architecture that enables the amalgamation and enhancement of domain-specific experts trained on limited annotated examples. To fine-tune MELD, we develop a suite of expert-tuning and MoE-tuning techniques, including a retrieval augmented generation (RAG) system, meta-path search for data augmentation, expert refinement and router network training based on information bottleneck. To further verify the effectiveness of MELD, we theoretically prove that MoE in MELD is superior than a single expert and the router network is able to dispatch data to the right experts. Finally, we conducted extensive experiments on 19 datasets over 10 DP tasks to show that MELD outperforms the state-of-the-art methods in both effectiveness and efficiency. More importantly, MELD is able to be fine-tuned in a low-resource environment, e.g. a local, single and low-priced 3090 GPU.},
  archive   = {C_KDD},
  author    = {Yan, Mengyi and Wang, Yaoshu and Pang, Kehan and Xie, Min and Li, Jianxin},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671873},
  pages     = {3690–3701},
  title     = {Efficient mixture of experts based on large language models for low-resource data preprocessing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Team up GBDTs and DNNs: Advancing efficient and effective
tabular prediction with tree-hybrid MLPs. <em>KDD</em>, 3679–3689. (<a
href="https://doi.org/10.1145/3637528.3671964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tabular datasets play a crucial role in various applications. Thus, developing efficient, effective, and widely compatible prediction algorithms for tabular data is important. Currently, two prominent model types, Gradient Boosted Decision Trees (GBDTs) and Deep Neural Networks (DNNs), have demonstrated performance advantages on distinct tabular prediction tasks. However, selecting an effective model for a specific tabular dataset is challenging, often demanding time-consuming hyperparameter tuning. To address this model selection dilemma, this paper proposes a new framework that amalgamates the advantages of both GBDTs and DNNs, resulting in a DNN algorithm that is as efficient as GBDTs and is competitively effective regardless of dataset preferences for GBDTs or DNNs. Our idea is rooted in an observation that deep learning (DL) offers a larger parameter space that can represent a well-performing GBDT model, yet the current back-propagation optimizer struggles to efficiently discover such optimal functionality. On the other hand, during GBDT development, hard tree pruning, entropy-driven feature gate, and model ensemble have proved to be more adaptable to tabular data. By combining these key components, we present a Tree-hybrid simple MLP (T-MLP). In our framework, a tensorized, rapidly trained GBDT feature gate, a DNN architecture pruning approach, as well as a vanilla back-propagation optimizer collaboratively train a randomly initialized MLP model. Comprehensive experiments show that T-MLP is competitive with extensively tuned DNNs and GBDTs in their dominating tabular benchmarks (88 datasets) respectively, all achieved with compact model storage and significantly reduced training duration. The codes and full experiment results are available at https://github.com/jyansir/tmlp.},
  archive   = {C_KDD},
  author    = {Yan, Jiahuan and Chen, Jintai and Wang, Qianxing and Chen, Danny Z. and Wu, Jian},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671964},
  pages     = {3679–3689},
  title     = {Team up GBDTs and DNNs: Advancing efficient and effective tabular prediction with tree-hybrid MLPs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FedRoLA: Robust federated learning against model poisoning
via layer-based aggregation. <em>KDD</em>, 3667–3678. (<a
href="https://doi.org/10.1145/3637528.3671906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Federated Learning (FL) is increasingly vulnerable to model poisoning attacks, where malicious clients degrade the global model&#39;s accuracy with manipulated updates. Unfortunately, most existing defenses struggle to handle the scenarios when multiple adversaries exist, and often rely on historical or validation data, rendering them ill-suited for the dynamic and diverse nature of real-world FL environments. Exacerbating these limitations is the fact that most existing defenses also fail to account for the distinctive contributions of Deep Neural Network (DNN) layers in detecting malicious activity, leading to the unnecessary rejection of benign updates. To bridge these gaps, we introduce FedRoLa, a cutting-edge similarity-based defense method optimized for FL. Specifically, FedRoLa leverages global model parameters and client updates independently, moving away from reliance on historical or validation data. It features a unique layer-based aggregation with dynamic layer selection, enhancing threat detection, and includes a dynamic probability method for balanced security and model performance. Through comprehensive evaluations using different DNN models and real-world datasets, FedRoLa demonstrates substantial improvements over the status quo approaches in global model accuracy, achieving up to 4\% enhancement in terms of accuracy, reducing false positives to 6.4\%, and securing an 92.8\% true positive rate.},
  archive   = {C_KDD},
  author    = {Yan, Gang and Wang, Hao and Yuan, Xu and Li, Jian},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671906},
  pages     = {3667–3678},
  title     = {FedRoLA: Robust federated learning against model poisoning via layer-based aggregation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Extreme meta-classification for large-scale zero-shot
retrieval. <em>KDD</em>, 3657–3666. (<a
href="https://doi.org/10.1145/3637528.3672046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We develop accurate and efficient solutions for large-scale retrieval tasks where novel (zero-shot) items can arrive continuously at a rapid pace. Conventional Siamese-style approaches embed both queries and items through a small encoder and retrieve the items lying closest to the query. While this approach allows efficient addition and retrieval of novel items, the small encoder lacks sufficient capacity for the necessary world knowledge in complex retrieval tasks. The extreme classification approaches have addressed this by learning a separate classifier for each item observed in the training set which significantly increases the representation capacity of the model. Such classifiers outperform Siamese approaches on observed items, but cannot be trained for novel items due to data and latency constraints. To bridge these gaps, this paper develops: (1) A new algorithmic framework, EMMETT, which efficiently synthesizes classifiers on-the-fly for novel items, by relying on the readily available classifiers for observed items; (2) A new algorithm, IRENE, which is a simple and effective instance of EMMETT that is specifically suited for large-scale deployments, and (3) A new theoretical framework for analyzing the generalization performance in large-scale zero-shot retrieval which guides our algorithm and training related design decisions. Comprehensive experiments are conducted on a wide range of retrieval tasks which demonstrate that IRENE improves the zero-shot retrieval accuracy by up to 15\% points in Recall@10 when added on top of leading encoders. Additionally, on an online A/B test in a large-scale ad retrieval task in a major search engine, IRENE improved the ad click-through rate by 4.2\%. Lastly, we validate our design choices through extensive ablative experiments. The source code for IRENE is available at https://aka.ms/irene.},
  archive   = {C_KDD},
  author    = {Yadav, Sachin and Saini, Deepak and Buvanesh, Anirudh and Paliwal, Bhawna and Dahiya, Kunal and Asokan, Siddarth and Prabhu, Yashoteja and Jiao, Jian and Varma, Manik},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672046},
  pages     = {3657–3666},
  title     = {Extreme meta-classification for large-scale zero-shot retrieval},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving multi-modal recommender systems by denoising and
aligning multi-modal content and user feedback. <em>KDD</em>, 3645–3656.
(<a href="https://doi.org/10.1145/3637528.3671703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-modal recommender systems (MRSs) are pivotal in diverse online web platforms and have garnered considerable attention in recent years. However, previous studies overlook the challenges of (1)noisy multi-modal content, (2) noisy user feedback, and (3) aligning multi-modal content and user feedback. To tackle these challenges, we propose Denoising and Aligning Multi-modal Recommender System (DA-MRS). To mitigate noise in multi-modal content, DA-MRS first constructs item-item graphs determined by consistent content similarity across modalities. To denoise user feedback, DA-MRS associates the probability of observed feedback with multi-modal content and devises a denoised BPR loss. Furthermore, DA-MRS implements Alignment guided by User preference to enhance task-specific item representation and Alignment guided by graded Item relations to provide finer-grained alignment. Extensive experiments verify that DA-MRS is a plug-and-play framework and achieves significant and consistent improvements across various datasets, backbone models, and noisy scenarios.},
  archive   = {C_KDD},
  author    = {Xv, Guipeng and Li, Xinyu and Xie, Ruobing and Lin, Chen and Liu, Chong and Xia, Feng and Kang, Zhanhui and Lin, Leyu},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671703},
  pages     = {3645–3656},
  title     = {Improving multi-modal recommender systems by denoising and aligning multi-modal content and user feedback},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ProtoMix: Augmenting health status representation learning
via prototype-based mixup. <em>KDD</em>, 3633–3644. (<a
href="https://doi.org/10.1145/3637528.3671937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the widespread adoption of electronic health records (EHR) data, deep learning techniques have been broadly utilized for various health prediction tasks. Nevertheless, the labeled data scarcity issue restricts the prediction power of these deep models. To enhance the generalization capability of deep learning models when faced with such situations, a common trend is to train generative adversarial networks (GANs) or diffusion models for data augmentation. However, due to limitations in sample size and potential label imbalance issues, these methods are prone to mode collapse problems. This results in the generation of new samples that fail to preserve the subtype structure within EHR data, thereby limiting their practicality in health prediction tasks that generally require detailed patient phenotyping. Aiming at the above problems, we propose a &amp;lt;u&amp;gt;Proto&amp;lt;/u&amp;gt;type-based &amp;lt;u&amp;gt;Mix&amp;lt;/u&amp;gt;up method, dubbed ProtoMix, which combines prior knowledge of intrinsic data features from subtype centroids (i.e., prototypes) to guide the synthesis of new samples. Specifically, ProtoMix employs a prototype-guided mixup training task to shift the decision boundary away from the subtypes. Then, ProtoMix optimizes the sampling weights in different areas of the data manifold via a prototype-guided mixup sampling strategy. Throughout the training process, ProtoMix dynamically expands the training distribution using an adaptive mixing coefficient computation method. Experimental evaluations on three real-world datasets demonstrate the efficacy of ProtoMix.},
  archive   = {C_KDD},
  author    = {Xu, Yongxin and Jiang, Xinke and Chu, Xu and Xiao, Yuzhen and Zhang, Chaohe and Ding, Hongxin and Zhao, Junfeng and Wang, Yasha and Xie, Bing},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671937},
  pages     = {3633–3644},
  title     = {ProtoMix: Augmenting health status representation learning via prototype-based mixup},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PeFAD: A parameter-efficient federated framework for time
series anomaly detection. <em>KDD</em>, 3621–3632. (<a
href="https://doi.org/10.1145/3637528.3671753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the proliferation of mobile sensing techniques, huge amounts of time series data are generated and accumulated in various domains, fueling plenty of real-world applications. In this setting, time series anomaly detection is practically important. It endeavors to identify deviant samples from the normal sample distribution in time series. Existing approaches generally assume that all the time series is available at a central location. However, we are witnessing the decentralized collection of time series due to the deployment of various edge devices. To bridge the gap between the decentralized time series data and the centralized anomaly detection algorithms, we propose a &amp;lt;u&amp;gt;P&amp;lt;/u&amp;gt;arameter-&amp;lt;u&amp;gt;e&amp;lt;/u&amp;gt;fficient &amp;lt;u&amp;gt;F&amp;lt;/u&amp;gt;ederated &amp;lt;u&amp;gt;A&amp;lt;/u&amp;gt;nomaly &amp;lt;u&amp;gt;D&amp;lt;/u&amp;gt;etection framework named PeFAD with the increasing privacy concerns. PeFAD for the first time employs the pre-trained language model (PLM) as the body of the client&#39;s local model, which can benefit from its cross-modality knowledge transfer capability. To reduce the communication overhead and local model adaptation cost, we propose a parameter-efficient federated training module such that clients only need to fine-tune small-scale parameters and transmit them to the server for update. PeFAD utilizes a novel anomaly-driven mask selection strategy to mitigate the impact of neglected anomalies during training. A knowledge distillation operation on a synthetic privacy-preserving dataset that is shared by all the clients is also proposed to address the data heterogeneity issue across clients. We conduct extensive evaluations on four real datasets, where PeFAD outperforms existing state-of-the-art baselines by up to 28.74\%.},
  archive   = {C_KDD},
  author    = {Xu, Ronghui and Miao, Hao and Wang, Senzhang and Yu, Philip S. and Wang, Jianxin},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671753},
  pages     = {3621–3632},
  title     = {PeFAD: A parameter-efficient federated framework for time series anomaly detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FlexCare: Leveraging cross-task synergy for flexible
multimodal healthcare prediction. <em>KDD</em>, 3610–3620. (<a
href="https://doi.org/10.1145/3637528.3671974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multimodal electronic health record (EHR) data can offer a holistic assessment of a patient&#39;s health status, supporting various predictive healthcare tasks. Recently, several studies have embraced the multitask learning approach in the healthcare domain, exploiting the inherent correlations among clinical tasks to predict multiple outcomes simultaneously. However, existing methods necessitate samples to possess complete labels for all tasks, which places heavy demands on the data and restricts the flexibility of the model. Meanwhile, within a multitask framework with multimodal inputs, how to comprehensively consider the information disparity among modalities and among tasks still remains a challenging problem. To tackle these issues, a unified healthcare prediction model, also named by FlexCare, is proposed to flexibly accommodate incomplete multimodal inputs, promoting the adaption to multiple healthcare tasks. The proposed model breaks the conventional paradigm of parallel multitask prediction by decomposing it into a series of asynchronous single-task prediction. Specifically, a task-agnostic multimodal information extraction module is presented to capture decorrelated representations of diverse intra- and inter-modality patterns. Taking full account of the information disparities between different modalities and different tasks, we present a task-guided hierarchical multimodal fusion module that integrates the refined modality-level representations into an individual patient-level representation. Experimental results on multiple tasks from MIMIC-IV/MIMIC-CXR/MIMIC-NOTE datasets demonstrate the effectiveness of the proposed method. Additionally, further analysis underscores the feasibility and potential of employing such a multitask strategy in the healthcare domain. The source code is available at https://github.com/mhxu1998/FlexCare **REMOVE 2nd URL**://github.com/mhxu1998/FlexCare.},
  archive   = {C_KDD},
  author    = {Xu, Muhao and Zhu, Zhenfeng and Li, Youru and Zheng, Shuai and Zhao, Yawei and He, Kunlun and Zhao, Yao},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671974},
  pages     = {3610–3620},
  title     = {FlexCare: Leveraging cross-task synergy for flexible multimodal healthcare prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Temporal prototype-aware learning for active voltage control
on power distribution networks. <em>KDD</em>, 3598–3609. (<a
href="https://doi.org/10.1145/3637528.3671790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Active Voltage Control (AVC) on the Power Distribution Networks (PDNs) aims to stabilize the voltage levels to ensure efficient and reliable operation of power systems. With the increasing integration of distributed energy resources, recent efforts have explored employing multi-agent reinforcement learning (MARL) techniques to realize effective AVC. Existing methods mainly focus on the acquisition of short-term AVC strategies, i.e., only learning AVC within the short-term training trajectories of a singular diurnal cycle. However, due to the dynamic nature of load demands and renewable energy, the operation states of real-world PDNs may exhibit significant distribution shifts across varying timescales (e.g., daily and seasonal changes). This can render those short-term strategies suboptimal or even obsolete when performing continuous AVC over extended periods. In this paper, we propose a novel temporal prototype-aware learning method, abbreviated as TPA, to learn time-adaptive AVC under short-term training trajectories. At the heart of TPA are two complementary components, namely multi-scale dynamic encoder and temporal prototype-aware policy, that can be readily incorporated into various MARL methods. The former component integrates a stacked transformer network to learn underlying temporal dependencies at different timescales of the PDNs, while the latter implements a learnable prototype matching mechanism to construct a dedicated AVC policy that can dynamically adapt to the evolving operation states. Experimental results on the AVC benchmark with different PDN sizes demonstrate that the proposed TPA surpasses the state-of-the-art counterparts not only in terms of control performance but also by offering model transferability. Our code is available at https://github.com/Canyizl/TPA-for-AVC.},
  archive   = {C_KDD},
  author    = {Xu, Feiyang and Liu, Shunyu and Qing, Yunpeng and Zhou, Yihe and Wang, Yuwen and Song, Mingli},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671790},
  pages     = {3598–3609},
  title     = {Temporal prototype-aware learning for active voltage control on power distribution networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). FRNet: Frequency-based rotation network for long-term time
series forecasting. <em>KDD</em>, 3586–3597. (<a
href="https://doi.org/10.1145/3637528.3671713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Long-term time series forecasting (LTSF) aims to predict future values for a long time based on historical data. The period term is an essential component of the time series, which is complex yet important for LTSF. Although existing studies have achieved promising results, they still have limitations in modeling dynamic complicated periods. Most studies only focus on static periods with fixed time steps, while very few studies attempt to capture dynamic periods in the time domain. In this paper, we dissect the original time series in time and frequency domains and empirically find that changes in periods are more easily captured and quantified in the frequency domain. Based on this observation, we propose to explore dynamic period features using rotation in the frequency domain. To this end, we develop the frequency-based rotation network (FRNet), a novel LTSF method to effectively capture the features of the dynamic complicated periods. FRNet decomposes the original time series into period and trend components. Based on the complex-valued linear networks, it leverages a period frequency rotation module to predict the period component and a patch frequency rotation module to predict the trend component, respectively. Extensive experiments on seven real-world datasets consistently demonstrate the superiority of FRNet over various state-of-the-art methods. The source code is available at https://github.com/SiriZhang45/FRNet.},
  archive   = {C_KDD},
  author    = {Zhang, Xinyu and Feng, Shanshan and Ma, Jianghong and Lin, Huiwei and Li, Xutao and Ye, Yunming and Li, Fan and Ong, Yew Soon},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671713},
  pages     = {3586–3597},
  title     = {FRNet: Frequency-based rotation network for long-term time series forecasting},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Preventing strategic behaviors in collaborative inference
for vertical federated learning. <em>KDD</em>, 3574–3585. (<a
href="https://doi.org/10.1145/3637528.3671663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vertical federated learning (VFL) is an emerging collaborative machine learning paradigm to facilitate the utilization of private features distributed across multiple parties. During the inference process of VFL, the involved parties need to upload their local embeddings to be aggregated for the final prediction. Despite its remarkable performances, the inference process of the current VFL system is vulnerable to the strategic behavior of involved parties, as they could easily change the uploaded local embeddings to exert direct influences on the prediction result. In a representative case study of federated recommendation, we find the allocation of display opportunities to be severely disrupted due to the parties&#39; preferences in display content. In order to elicit the true local embeddings for VFL system, we propose a distribution-based penalty mechanism to detect and penalize the strategic behaviors in collaborative inference. As the key motivation of our design, we theoretically prove the power of constraining the distribution of uploaded embeddings in preventing the dishonest parties from achieving higher utility. Our mechanism leverages statistical two-sample tests to distinguish whether the distribution of uploaded embeddings is reasonable, and penalize the dishonest party through deactivating her uploaded embeddings. The resulted mechanism could be shown to admit truth-telling to converge to a Bayesian Nash equilibrium asymptotically under mild conditions. The experimental results further demonstrate the effectiveness of the proposed mechanism to reduce the dishonest utility increase of strategic behaviors and promote the truthful uploading of local embeddings in inferences.},
  archive   = {C_KDD},
  author    = {Xing, Yidan and Zheng, Zhenzhe and Wu, Fan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671663},
  pages     = {3574–3585},
  title     = {Preventing strategic behaviors in collaborative inference for vertical federated learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards lightweight graph neural network search with
curriculum graph sparsification. <em>KDD</em>, 3563–3573. (<a
href="https://doi.org/10.1145/3637528.3671706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph Neural Architecture Search (GNAS) has achieved superior performance on various graph-structured tasks. However, existing GNAS studies overlook the applications of GNAS in resource-constrained scenarios. This paper proposes to design a joint graph data and architecture mechanism, which identifies important sub-architectures via the valuable graph data. To search for optimal lightweight Graph Neural Networks (GNNs), we propose a Lightweight &amp;lt;u&amp;gt;G&amp;lt;/u&amp;gt;raph &amp;lt;u&amp;gt;N&amp;lt;/u&amp;gt;eural &amp;lt;u&amp;gt;A&amp;lt;/u&amp;gt;rchitecture &amp;lt;u&amp;gt;S&amp;lt;/u&amp;gt;earch with Graph &amp;lt;u&amp;gt;S&amp;lt;/u&amp;gt;pars&amp;lt;u&amp;gt;I&amp;lt;/u&amp;gt;fication and Network &amp;lt;u&amp;gt;P&amp;lt;/u&amp;gt;runing (GASSIP) method. In particular, GASSIP comprises an operation-pruned architecture search module to enable efficient lightweight GNN search. Meanwhile, we design a novel curriculum graph data sparsification module with an architecture-aware edge-removing difficulty measurement to help select optimal sub-architectures. With the aid of two differentiable masks, we iteratively optimize these two modules to efficiently search for the optimal lightweight architecture. Extensive experiments on five benchmarks demonstrate the effectiveness of GASSIP. Particularly, our method achieves on-par or even higher node classification performance with half or fewer model parameters of searched GNNs and a sparser graph.},
  archive   = {C_KDD},
  author    = {Xie, Beini and Chang, Heng and Zhang, Ziwei and Zhang, Zeyang and Wu, Simin and Wang, Xin and Meng, Yuan and Zhu, Wenwu},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671706},
  pages     = {3563–3573},
  title     = {Towards lightweight graph neural network search with curriculum graph sparsification},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Make your home safe: Time-aware unsupervised user behavior
anomaly detection in smart homes via loss-guided mask. <em>KDD</em>,
3551–3562. (<a href="https://doi.org/10.1145/3637528.3671708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Smart homes, powered by the Internet of Things, offer great convenience but also pose security concerns due to abnormal behaviors, such as improper operations of users and potential attacks from malicious attackers. Several behavior modeling methods have been proposed to identify abnormal behaviors and mitigate potential risks. However, their performance often falls short because they do not effectively learn less frequent behaviors, consider temporal context, or account for the impact of noise in human behaviors. In this paper, we propose SmartGuard, an autoencoder-based unsupervised user behavior anomaly detection framework. First, we design a Loss-guided Dynamic Mask Strategy (LDMS) to encourage the model to learn less frequent behaviors, which are often overlooked during learning. Second, we propose a Three-level Time-aware Position Embedding (TTPE) to incorporate temporal information into positional embedding to detect temporal context anomaly. Third, we propose a Noise-aware Weighted Reconstruction Loss (NWRL) that assigns different weights for routine behaviors and noise behaviors to mitigate the interference of noise behaviors during inference. Comprehensive experiments demonstrate that SmartGuard consistently outperforms state-of-the-art baselines and also offers highly interpretable results.},
  archive   = {C_KDD},
  author    = {Xiao, Jingyu and Xu, Zhiyao and Zou, Qingsong and Li, Qing and Zhao, Dan and Fang, Dong and Li, Ruoyu and Tang, Wenxin and Li, Kang and Zuo, Xudong and Hu, Penghui and Jiang, Yong and Weng, Zixuan and Lyu, Michael R.},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671708},
  pages     = {3551–3562},
  title     = {Make your home safe: Time-aware unsupervised user behavior anomaly detection in smart homes via loss-guided mask},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How to avoid jumping to conclusions: Measuring the
robustness of outstanding facts in knowledge graphs. <em>KDD</em>,
3539–3550. (<a href="https://doi.org/10.1145/3637528.3671763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An outstanding fact (OF) is a striking claim by which some entities stand out from their peers on some attribute. OFs serve data journalism, fact checking, and recommendation. However, one could jump to conclusions by selecting truthful OFs while intentionally or inadvertently ignoring lateral contexts and data that render them less striking. This jumping conclusion bias from unstable OFs may disorient the public, including voters and consumers, raising concerns about fairness and transparency in political and business competition. It is thus ethically imperative for several stakeholders to measure the robustness of OFs with respect to lateral contexts and data. Unfortunately, a capacity for such inspection of OFs mined from knowledge graphs (KGs) is missing. In this paper, we propose a methodology that inspects the robustness of OFs in KGs by perturbation analysis. We define (1) entity perturbation, which detects outlying contexts by perturbing context entities in the OF; and (2) data perturbation, which considers plausible data that render an OF less striking. We compute the expected strikingness scores of OFs over perturbation relevance distributions and assess an OF as robust if its measured strikingness does not deviate significantly from the expected. We devise a suite of exact and sampling algorithms for perturbation analysis on large KGs. Extensive experiments reveal that our methodology accurately and efficiently detects frail OFs generated by existing mining approaches on KGs. We also show the effectiveness of our approaches through case and user studies.},
  archive   = {C_KDD},
  author    = {Xiao, Hanhua and Li, Yuchen and Wang, Yanhao and Karras, Panagiotis and Mouratidis, Kyriakos and Avlona, Natalia Rozalia},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671763},
  pages     = {3539–3550},
  title     = {How to avoid jumping to conclusions: Measuring the robustness of outstanding facts in knowledge graphs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ReFound: Crafting a foundation model for urban region
understanding upon language and visual foundations. <em>KDD</em>,
3527–3538. (<a href="https://doi.org/10.1145/3637528.3671992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Understanding urban regional characteristics is pivotal in driving critical insights for urban planning and management. We have witnessed the successful application of pre-trained Foundation Models (FMs) in generating universal representations for various downstream tasks. However, applying this principle to the geospatial domain remains challenging, primarily due to the difficulty of gathering extensive data for developing a dedicated urban foundation model. Though there have been some attempts to empower the existing FMs with urban data, most of them focus on single-modality FMs without considering the multi-modality nature of urban region understanding tasks. To address this gap, we introduce ReFound - a novel framework for &amp;lt;u&amp;gt;Re&amp;lt;/u&amp;gt;-training a &amp;lt;u&amp;gt;Found&amp;lt;/u&amp;gt;ation model for urban region understanding, harnessing the strengths of both language and visual FMs. In this framework, we first invent a Mixture-of-Geospatial-Expert (MoGE) Transformer, to effectively integrate the embedding of multi-source geospatial data. Building on this, ReFound is enhanced by jointly distilling knowledge from language, visual, and visual-language FMs respectively, thus augmenting its generalization capabilities. Meanwhile, we design a masked geospatial data modeling approach alongside a cross-modal spatial alignment mechanism, to enhance the spatial knowledge of ReFound derived from geospatial data. Extensive experiments conducted on six real-world datasets over three urban region understanding tasks demonstrate the superior performance of our framework.},
  archive   = {C_KDD},
  author    = {Xiao, Congxi and Zhou, Jingbo and Xiao, Yixiong and Huang, Jizhou and Xiong, Hui},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671992},
  pages     = {3527–3538},
  title     = {ReFound: Crafting a foundation model for urban region understanding upon language and visual foundations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Motif-consistent counterfactuals with adversarial refinement
for graph-level anomaly detection. <em>KDD</em>, 3518–3526. (<a
href="https://doi.org/10.1145/3637528.3672050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph-level anomaly detection is significant in diverse domains. To improve detection performance, counterfactual graphs have been exploited to benefit the generalization capacity by learning causal relations. Most existing studies directly introduce perturbations (e.g., flipping edges) to generate counterfactual graphs, which are prone to alter the semantics of generated examples and make them off the data manifold, resulting in sub-optimal performance. To address these issues, we propose a novel approach, Motif-consistent Counterfactuals with Adversarial Refinement (MotifCAR), for graph-level anomaly detection. The model combines the motif of one graph, the core subgraph containing the identification (category) information, and the contextual subgraph (non-motif) of another graph to produce a raw counterfactual graph. However, the produced raw graph might be distorted and cannot satisfy the important counterfactual properties: Realism, Validity, Proximity and Sparsity. Towards that, we present a Generative Adversarial Network (GAN)-based graph optimizer to refine the raw counterfactual graphs. It adopts the discriminator to guide the generator to generate graphs close to realistic data, i.e., meet the property Realism. Further, we design the motif consistency to force the motif of the generated graphs to be consistent with the realistic graphs, meeting the property Validity. Also, we devise the contextual loss and connection loss to control the contextual subgraph and the newly added links to meet the properties Proximity and Sparsity. As a result, the model can generate high-quality counterfactual graphs. Experiments demonstrate the superiority of MotifCAR.},
  archive   = {C_KDD},
  author    = {Xiao, Chunjing and Pang, Shikang and Tai, Wenxin and Huang, Yanlong and Trajcevski, Goce and Zhou, Fan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672050},
  pages     = {3518–3526},
  title     = {Motif-consistent counterfactuals with adversarial refinement for graph-level anomaly detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Performative debias with fair-exposure optimization driven
by strategic agents in recommender systems. <em>KDD</em>, 3507–3517. (<a
href="https://doi.org/10.1145/3637528.3671786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Data bias, e.g., popularity impairs the dynamics of two-sided markets within recommender systems. This overshadows the less visible but potentially intriguing long-tail items that could capture user interest. Despite the abundance of research surrounding this issue, it still poses challenges and remains a hot topic in academic circles. Along this line, in this paper, we developed a re-ranking approach in dynamic settings with fair-exposure optimization driven by strategic agents. Designed for the producer side, the execution of agents assumes content creators can modify item features based on strategic incentives to maximize their exposure. This iterative process entails an end-to-end optimization, employing differentiable ranking operators that simultaneously target accuracy and fairness. Joint objectives ensure the performance of recommendations while enhancing the visibility of tail items. We also leveraged the performativity nature of predictions to illustrate how strategic learning influences content creators to shift towards fairness efficiently, thereby incentivizing features of tail items. Through comprehensive experiments on both public and industrial datasets, we have substantiated the effectiveness and dominance of the proposed method especially on unveiling the potential of tail items.},
  archive   = {C_KDD},
  author    = {Xiang, Zhichen and Zhao, Hongke and Zhao, Chuang and He, Ming and Fan, Jianping},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671786},
  pages     = {3507–3517},
  title     = {Performative debias with fair-exposure optimization driven by strategic agents in recommender systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predicting cascading failures with a hyperparametric
diffusion model. <em>KDD</em>, 3495–3506. (<a
href="https://doi.org/10.1145/3637528.3672048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we study cascading failures in power grids through the lens of information diffusion models. Similar to the spread of rumors or influence in an online social network, it has been observed that failures (outages) in a power grid can spread contagiously, driven by viral spread mechanisms. We employ a stochastic diffusion model that is Markovian (memoryless) and local (the activation of one node, i.e., transmission line, can only be caused by its neighbors). Our model integrates viral diffusion principles with physics-based concepts, by correlating the diffusion weights (contagion probabilities between transmission lines) with the hyperparametric Information Cascades (IC) model. We show that this diffusion model can be learned from traces of cascading failures, enabling accurate modeling and prediction of failure propagation. This approach facilitates actionable information through well-understood and efficient graph analysis methods and graph diffusion simulations. Furthermore, by leveraging the hyperparametric model, we can predict diffusion and mitigate the risks of cascading failures even in unseen grid configurations, whereas existing methods falter due to a lack of training data. Extensive experiments based on a benchmark power grid and simulations therein show that our approach effectively captures the failure diffusion phenomena and guides decisions to strengthen the grid, reducing the risk of large-scale cascading failures. Additionally, we characterize our model&#39;s sample complexity, improving upon the existing bound.},
  archive   = {C_KDD},
  author    = {Xiang, Bin and Cautis, Bogdan and Xiao, Xiaokui and Mula, Olga and Niyato, Dusit and Lakshmanan, Laks V.S.},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672048},
  pages     = {3495–3506},
  title     = {Predicting cascading failures with a hyperparametric diffusion model},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FLea: Addressing data scarcity and label skew in federated
learning via privacy-preserving feature augmentation. <em>KDD</em>,
3484–3494. (<a href="https://doi.org/10.1145/3637528.3671899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Federated Learning (FL) enables model development by leveraging data distributed across numerous edge devices without transferring local data to a central server. However, existing FL methods still face challenges when dealing with scarce and label-skewed data across devices, resulting in local model overfitting and drift, consequently hindering the performance of the global model. In response to these challenges, we propose a pioneering framework called FLea, incorporating the following key components: i) A global feature buffer that stores activation-target pairs shared from multiple clients to support local training. This design mitigates local model drift caused by the absence of certain classes; ii) A feature augmentation approach based on local and global activation mix-ups for local training. This strategy enlarges the training samples, thereby reducing the risk of local overfitting; iii) An obfuscation method to minimize the correlation between intermediate activations and the source data, enhancing the privacy of shared features. To verify the superiority of FLea, we conduct extensive experiments using a wide range of data modalities, simulating different levels of local data scarcity and label skew. The results demonstrate that FLea consistently outperforms state-of-the-art FL counterparts (among 13 of the experimented 18 settings, the improvement is over 5\%) while concurrently mitigating the privacy vulnerabilities associated with shared features. Code is available at https://github.com/XTxiatong/FLea.git.},
  archive   = {C_KDD},
  author    = {Xia, Tong and Ghosh, Abhirup and Qiu, Xinchi and Mascolo, Cecilia},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671899},
  pages     = {3484–3494},
  title     = {FLea: Addressing data scarcity and label skew in federated learning via privacy-preserving feature augmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast computation of kemeny’s constant for directed graphs.
<em>KDD</em>, 3472–3483. (<a
href="https://doi.org/10.1145/3637528.3671859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Kemeny&#39;s constant for random walks on a graph is defined as the mean hitting time from one node to another selected randomly according to the stationary distribution. It has found numerous applications and attracted considerable research interest. However, exact computation of Kemeny&#39;s constant requires matrix inversion, which scales poorly for large networks with millions of nodes. Existing approximation algorithms either leverage properties exclusive to undirected graphs or involve inefficient simulation, leaving room for further optimization. To address these limitations for directed graphs, we propose two novel approximation algorithms for estimating Kemeny&#39;s constant on directed graphs with theoretical error guarantees. Extensive numerical experiments on real-world networks validate the superiority of our algorithms over baseline methods in terms of efficiency and accuracy.},
  archive   = {C_KDD},
  author    = {Xia, Haisong and Zhang, Zhongzhi},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671859},
  pages     = {3472–3483},
  title     = {Fast computation of kemeny&#39;s constant for directed graphs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A deep prediction framework for multi-source information via
heterogeneous GNN. <em>KDD</em>, 3460–3471. (<a
href="https://doi.org/10.1145/3637528.3671966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Predicting information diffusion is a fundamental task in online social networks (OSNs). Recent studies mainly focus on the popularity prediction of specific content but ignore the correlation between multiple pieces of information. The topic is often used to correlate such information and can correspond to multi-source information. The popularity of a topic relies not only on information diffusion time but also on users&#39; followership. Current solutions concentrate on hard time partition, lacking versatility. Meanwhile, the hop-based sampling adopted in state-of-the-art (SOTA) methods encounters redundant user followership. Moreover, many SOTA methods are not designed with good modularity and lack evaluation for each functional module and enlightening discussion. This paper presents a novel extensible framework, coined as HIF, for effective popularity prediction in OSNs with four original contributions. First, HIF adopts a soft partition of users and time intervals to better learn users&#39; behavioral preferences over time. Second, HIF utilizes weighted sampling to optimize the construction of heterogeneous graphs and reduce redundancy. Furthermore, HIF supports multi-task collaborative optimization to improve its learning capability. Finally, as an extensible framework, HIF provides generic module slots to combine different submodules (e.g., RNNs, Transformer encoders). Experiments show that HIF significantly improves performance and interpretability compared to SOTAs.},
  archive   = {C_KDD},
  author    = {Wu, Zhen and Zhou, Jingya and Zhang, Jinghui and Liu, Ling and Huang, Chizhou},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671966},
  pages     = {3460–3471},
  title     = {A deep prediction framework for multi-source information via heterogeneous GNN},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cost-efficient fraud risk optimization with submodularity in
insurance claim. <em>KDD</em>, 3448–3459. (<a
href="https://doi.org/10.1145/3637528.3672012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The fraudulent insurance claim is critical for the insurance industry. Insurance companies or agency platforms aim to confidently estimate the fraud risk of claims by gathering data from various sources. Although more data sources can improve the estimation accuracy, they inevitably lead to increased costs. Therefore, a great challenge of fraud risk verification lies in well balancing these two aspects. To this end, this paper proposes a framework named cost-efficient fraud risk optimization with submodularity (CEROS) to optimize the process of fraud risk verification. CEROS efficiently allocates investigation resources across multiple information sources, balancing the trade-off between accuracy and cost. CEROS consists of two parts that we propose: a submodular set-wise classification model called SSCM to estimate the submodular objective function, and a primal-dual algorithm with segmentation point called PDA-SP to solve the objective function. Specifically, SSCM models the fraud probability associated with multiple information sources and ensures the properties of submodularity of fraud risk without making independence assumption. The submodularity in SSCM enables PDA-SP to significantly speed up dual optimization. Theoretically, we disclose that when PDA-SP optimizes this dual optimization problem, the process is monotonicity. Finally, the trade-off coefficients output by PDA-SP that balance accuracy and cost in fraud risk verification are applied to online insurance claim decision-making. We conduct experiments on offline trials and online A/B tests in two business areas at Alipay: healthcare insurance recommendation and claim verification. The extensive results indicate that, compared with other methods, CEROS achieves acceleration of 66.9\% in convergence speed and meanwhile 18.8\% in cost reduction. Currently, CEROS has been successfully deployed in Alipay.},
  archive   = {C_KDD},
  author    = {Wu, Yupeng and Zhu, Zhibo and Ma, Chaoyi and Qian, Hong and Lu, Xingyu and Zhang, Yangwenhui and Qin, Xiaobo and Fei, Binjie and Zhou, Jun and Zhou, Aimin},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672012},
  pages     = {3448–3459},
  title     = {Cost-efficient fraud risk optimization with submodularity in insurance claim},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DFGNN: Dual-frequency graph neural network for sign-aware
feedback. <em>KDD</em>, 3437–3447. (<a
href="https://doi.org/10.1145/3637528.3671701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The graph-based recommendation has achieved great success in recent years. However, most existing graph-based recommendations focus on capturing user preference based on positive edges/feedback, while ignoring negative edges/feedback (e.g., dislike, low rating) that widely exist in real-world recommender systems. How to utilize negative feedback in graph-based recommendations still remains underexplored. In this study, we first conducted a comprehensive experimental analysis and found that (1) existing graph neural networks are not well-suited for modeling negative feedback, which acts as a high-frequency signal in a user-item graph. (2) The graph-based recommendation suffers from the representation degeneration problem. Based on the two observations, we propose a novel model that models positive and negative feedback from a frequency filter perspective called Dual-frequency Graph Neural Network for Sign-aware Recommendation (DFGNN). Specifically, in DFGNN, the designed dual-frequency graph filter (DGF) captures both low-frequency and high-frequency signals that contain positive and negative feedback. Furthermore, the proposed signed graph regularization is applied to maintain the user/item embedding uniform in the embedding space to alleviate the representation degeneration problem. Additionally, we conduct extensive experiments on real-world datasets and demonstrate the effectiveness of the proposed model. Codes of our model will be released upon acceptance.},
  archive   = {C_KDD},
  author    = {Wu, Yiqing and Xie, Ruobing and Zhang, Zhao and Zhang, Xu and Zhuang, Fuzhen and Lin, Leyu and Kang, Zhanhui and Xu, Yongjun},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671701},
  pages     = {3437–3447},
  title     = {DFGNN: Dual-frequency graph neural network for sign-aware feedback},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unifying graph convolution and contrastive learning in
collaborative filtering. <em>KDD</em>, 3425–3436. (<a
href="https://doi.org/10.1145/3637528.3671840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph-based models and contrastive learning have emerged as prominent methods in Collaborative Filtering (CF). While many existing models in CF incorporate these methods in their design, there seems to be a limited depth of analysis regarding the foundational principles behind them. This paper bridges graph convolution, a pivotal element of graph-based models, with contrastive learning through a theoretical framework. By examining the learning dynamics and equilibrium of the contrastive loss, we offer a fresh lens to understand contrastive learning via graph theory, emphasizing its capability to capture high-order connectivity. Building on this analysis, we further show that the graph convolutional layers often used in graph-based models are not essential for high-order connectivity modeling and might contribute to the risk of oversmoothing. Stemming from our findings, we introduce Simple Contrastive Collaborative Filtering (SCCF), a simple and effective algorithm based on a naive embedding model and a modified contrastive loss. The efficacy of the algorithm is demonstrated through extensive experiments across four public datasets. The experiment code is available at https://github.com/wu1hong/SCCF.},
  archive   = {C_KDD},
  author    = {Wu, Yihong and Zhang, Le and Mo, Fengran and Zhu, Tianyu and Ma, Weizhi and Nie, Jian-Yun},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671840},
  pages     = {3425–3436},
  title     = {Unifying graph convolution and contrastive learning in collaborative filtering},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ProCom: A few-shot targeted community detection algorithm.
<em>KDD</em>, 3414–3424. (<a
href="https://doi.org/10.1145/3637528.3671749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Targeted community detection aims to distinguish a particular type of community in the network. This is an important task with a lot of real-world applications, e.g., identifying fraud groups in transaction networks. Traditional community detection methods fail to capture the specific features of the targeted community and detect all types of communities indiscriminately. Semi-supervised community detection algorithms, emerged as a feasible alternative, are inherently constrained by their limited adaptability and substantial reliance on a large amount of labeled data, which demands extensive domain knowledge and manual effort.In this paper, we address the aforementioned weaknesses in targeted community detection by focusing on few-shot scenarios. We propose ProCom, a novel framework that extends the &quot;pre-train, prompt&#39;&#39; paradigm, offering a low-resource, high-efficiency, and transferable solution. Within the framework, we devise a dual-level context-aware pre-training method that fosters a deep understanding of latent communities in the network, establishing a rich knowledge foundation for downstream tasks. In the prompt learning stage, we reformulate the targeted community detection task into pre-training objectives, allowing the extraction of specific knowledge relevant to the targeted community to facilitate effective and efficient inference. By leveraging both the general community knowledge acquired during pre-training and the specific insights gained from the prompt communities, ProCom exhibits remarkable adaptability across different datasets. We conduct extensive experiments on five benchmarks to evaluate the ProCom framework, demonstrating its SOTA performance under few-shot scenarios, strong efficiency, and transferability across diverse datasets.},
  archive   = {C_KDD},
  author    = {Wu, Xixi and Xiong, Kaiyu and Xiong, Yun and He, Xiaoxin and Zhang, Yao and Jiao, Yizhu and Zhang, Jiawei},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671749},
  pages     = {3414–3424},
  title     = {ProCom: A few-shot targeted community detection algorithm},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Counterfactual generative models for time-varying
treatments. <em>KDD</em>, 3402–3413. (<a
href="https://doi.org/10.1145/3637528.3671950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Estimating the counterfactual outcome of treatment is essential for decision-making in public health and clinical science, among others. Often, treatments are administered in a sequential, time-varying manner, leading to an exponentially increased number of possible counterfactual outcomes. Furthermore, in modern applications, the outcomes are high-dimensional and conventional average treatment effect estimation fails to capture disparities in individuals. To tackle these challenges, we propose a novel conditional generative framework capable of producing counterfactual samples under time-varying treatment, without the need for explicit density estimation. Our method carefully addresses the distribution mismatch between the observed and counterfactual distributions via a loss function based on inverse probability re-weighting, and supports integration with state-of-the-art conditional generative models such as the guided diffusion and conditional variational autoencoder. We present a thorough evaluation of our method using both synthetic and real-world data. Our results demonstrate that our method is capable of generating high-quality counterfactual samples and outperforms the state-of-the-art baselines.},
  archive   = {C_KDD},
  author    = {Wu, Shenghao and Zhou, Wenbin and Chen, Minshuo and Zhu, Shixiang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671950},
  pages     = {3402–3413},
  title     = {Counterfactual generative models for time-varying treatments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CoRAL: Collaborative retrieval-augmented large language
models improve long-tail recommendation. <em>KDD</em>, 3391–3401. (<a
href="https://doi.org/10.1145/3637528.3671901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The long-tail recommendation is a challenging task for traditional recommender systems, due to data sparsity and data imbalance issues. The recent development of large language models (LLMs) has shown their abilities in complex reasoning, which can help to deduce users&#39; preferences based on very few previous interactions. However, since most LLM-based systems rely on items&#39; semantic meaning as the sole evidence for reasoning, the collaborative information of user-item interactions is neglected, which can cause the LLM&#39;s reasoning to be misaligned with task-specific collaborative information of the dataset. To further align LLMs&#39; reasoning to task-specific user-item interaction knowledge, we introduce collaborative retrieval-augmented LLMs, CoRAL, which directly incorporate collaborative evidence into the prompts. Based on the retrieved user-item interactions, the LLM can analyze shared and distinct preferences among users, and summarize the patterns indicating which types of users would be attracted by certain items. The retrieved collaborative evidence prompts the LLM to align its reasoning with the user-item interaction patterns in the dataset. However, since the capacity of the input prompt is limited, finding the minimally-sufficient collaborative information for recommendation tasks can be challenging. We propose to find the optimal interaction set through a sequential decision-making process and develop a retrieval policy learned through a reinforcement learning (RL) framework, CoRAL. Our experimental results show that CoRAL can significantly improve LLMs&#39; reasoning abilities on specific recommendation tasks. Our analysis also reveals that CoRAL can more efficiently explore collaborative information through reinforcement learning.},
  archive   = {C_KDD},
  author    = {Wu, Junda and Chang, Cheng-Chun and Yu, Tong and He, Zhankui and Wang, Jianing and Hou, Yupeng and McAuley, Julian},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671901},
  pages     = {3391–3401},
  title     = {CoRAL: Collaborative retrieval-augmented large language models improve long-tail recommendation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributional network of networks for modeling data
heterogeneity. <em>KDD</em>, 3379–3390. (<a
href="https://doi.org/10.1145/3637528.3671994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Heterogeneous data widely exists in various high-impact applications. Domain adaptation and out-of-distribution generalization paradigms have been formulated to handle the data heterogeneity across domains. However, most existing domain adaptation and out-of-distribution generalization algorithms do not explicitly explain how the label information can be adaptively propagated from the source domains to the target domain. Furthermore, little effort has been devoted to theoretically understanding the convergence of existing algorithms based on neural networks.To address these problems, in this paper, we propose a generic distributional network of networks (TENON) framework, where each node of the main network represents an individual domain associated with a domain-specific network. In this case, the edges within the main network indicate the domain similarity, and the edges within each network indicate the sample similarity. The crucial idea of TENON is to characterize the within-domain label smoothness and cross-domain parameter smoothness in a unified framework. The convergence and optimality of TENON are theoretically analyzed. Furthermore, we show that based on the TENON framework, domain adaptation and out-of-distribution generalization can be naturally formulated as transductive and inductive distribution learning problems, respectively. This motivates us to develop two instantiated algorithms (TENON-DA and TENON-OOD) of the proposed TENON framework for domain adaptation and out-of-distribution generalization. The effectiveness and efficiency of TENON-DA and TENON-OOD are verified both theoretically and empirically.},
  archive   = {C_KDD},
  author    = {Wu, Jun and He, Jingrui and Tong, Hanghang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671994},
  pages     = {3379–3390},
  title     = {Distributional network of networks for modeling data heterogeneity},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fake news in sheep’s clothing: Robust fake news detection
against LLM-empowered style attacks. <em>KDD</em>, 3367–3378. (<a
href="https://doi.org/10.1145/3637528.3671977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It is commonly perceived that fake news and real news exhibit distinct writing styles, such as the use of sensationalist versus objective language. However, we emphasize that style-related features can also be exploited for style-based attacks. Notably, the advent of powerful Large Language Models (LLMs) has empowered malicious actors to mimic the style of trustworthy news sources, doing so swiftly, cost-effectively, and at scale. Our analysis reveals that LLM-camouflaged fake news content significantly undermines the effectiveness of state-of-the-art text-based detectors (up to 38\% decrease in F1 Score), implying a severe vulnerability to stylistic variations. To address this, we introduce SheepDog, a style-robust fake news detector that prioritizes content over style in determining news veracity. SheepDog achieves this resilience through (1) LLM-empowered news reframings that inject style diversity into the training process by customizing articles to match different styles; (2) a style-agnostic training scheme that ensures consistent veracity predictions across style-diverse reframings; and (3) content-focused veracity attributions that distill content-centric guidelines from LLMs for debunking fake news, offering supplementary cues and potential intepretability that assist veracity prediction. Extensive experiments on three real-world benchmarks demonstrate SheepDog&#39;s style robustness and adaptability to various backbones.},
  archive   = {C_KDD},
  author    = {Wu, Jiaying and Guo, Jiafeng and Hooi, Bryan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671977},
  pages     = {3367–3378},
  title     = {Fake news in sheep&#39;s clothing: Robust fake news detection against LLM-empowered style attacks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural manifold operators for learning the evolution of
physical dynamics. <em>KDD</em>, 3356–3366. (<a
href="https://doi.org/10.1145/3637528.3671779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modeling the evolution of physical dynamics is a foundational problem in science and engineering, and it is regarded as the modeling of an operator mapping between infinite-dimensional functional spaces. Operator learning methods, learning the underlying infinite-dimensional operator in a high-dimensional latent space, have shown significant potential in modeling physical dynamics. However, there remains insufficient research on how to approximate an infinite-dimensional operator using a finite-dimensional parameter space. Inappropriate dimensionality representation of the underlying operator leads to convergence difficulties, decreasing generalization capability, and violating the physical consistency. To address the problem, we present Neural Manifold Operator (NMO) to learn the invariant subspace with the intrinsic dimension to parameterize infinite-dimensional underlying operators. NMO achieves state-of-the-art performance in statistical and physical metrics and gains 23.35\% average improvement on three real-world scenarios and four equation-governed scenarios across a wide range of multi-disciplinary fields. Our paradigm has demonstrated universal effectiveness across various model structure implementations, including Multi-Layer Perceptron, Convolutional Neural Networks, and Transformers. Experimentally, we prove that the intrinsic dimension calculated by our paradigm is the optimal dimensional representation of the underlying operators. We release our code at https://github.com/AI4EarthLab/Neural-Manifold-Operators.},
  archive   = {C_KDD},
  author    = {Wu, Hao and Weng, Kangyu and Zhou, Shuyi and Huang, Xiaomeng and Xiong, Wei},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671779},
  pages     = {3356–3366},
  title     = {Neural manifold operators for learning the evolution of physical dynamics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FedBiOT: LLM local fine-tuning in federated learning without
full model. <em>KDD</em>, 3345–3355. (<a
href="https://doi.org/10.1145/3637528.3671897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large language models (LLMs) show amazing performance on many domain-specific tasks after fine-tuning with some appropriate data. However, many domain-specific data are privately distributed across multiple owners. Thus, this dilemma raises the interest in how to perform LLM fine-tuning in federated learning (FL). However, confronted with limited computation and communication capacities, FL clients struggle to fine-tune an LLM effectively. To this end, we introduce FedBiOT, a resource-efficient LLM fine-tuning approach to FL. Specifically, our method involves the server generating a compressed LLM and aligning its performance with the full model. Subsequently, the clients fine-tune a lightweight yet important part of the compressed model, referred to as an adapter. Notice that as the server has no access to the private data owned by the clients, the data used for alignment by the server has a different distribution from the one used for fine-tuning by clients. We formulate the problem into a bi-level optimization problem to minimize the negative effect of data discrepancy and derive the updating rules for the server and clients. We conduct extensive experiments on LLaMA-2, empirically showing that the adapter has exceptional performance when reintegrated into the global LLM. The results also indicate that the proposed FedBiOT significantly reduces resource consumption compared to existing benchmarks, all while achieving comparable performance levels.},
  archive   = {C_KDD},
  author    = {Wu, Feijie and Li, Zitao and Li, Yaliang and Ding, Bolin and Gao, Jing},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671897},
  pages     = {3345–3355},
  title     = {FedBiOT: LLM local fine-tuning in federated learning without full model},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dense subgraph discovery meets strong triadic closure.
<em>KDD</em>, 3334–3344. (<a
href="https://doi.org/10.1145/3637528.3671697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Finding dense subgraphs is a core problem with numerous graph mining applications such as community detection in social networks and anomaly detection. However, in many real-world networks connections are not equal. One way to label edges as either strong or weak is to use strong triadic closure~(STC). Here, if one node connects strongly with two other nodes, then those two nodes should be connected at least with a weak edge. STC-labelings are not unique and finding the maximum number of strong edges is NP-hard. In this paper, we apply STC to dense subgraph discovery. More formally, our score for a given subgraph is the ratio between the sum of the number of strong edges and weak edges, weighted by a user parameter λ, and the number of nodes of the subgraph. Our goal is to find a subgraph and an STC-labeling maximizing the score. We show that for λ = 1, our problem is equivalent to finding the densest subgraph, while for λ = 0, our problem is equivalent to finding the largest clique, making our problem NP-hard. We propose an exact algorithm based on integer linear programming and four practical polynomial-time heuristics. We present an extensive experimental study that shows that our algorithms can find the ground truth in synthetic datasets and run efficiently in real-world datasets.},
  archive   = {C_KDD},
  author    = {Wickrama Arachchi, Chamalee and Kumpulainen, Iiro and Tatti, Nikolaj},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671697},
  pages     = {3334–3344},
  title     = {Dense subgraph discovery meets strong triadic closure},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From supervised to generative: A novel paradigm for tabular
deep learning with large language models. <em>KDD</em>, 3323–3333. (<a
href="https://doi.org/10.1145/3637528.3671975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tabular data is foundational to predictive modeling in various crucial industries, including healthcare, finance, retail, sustainability, etc. Despite the progress made in specialized models, there is an increasing demand for universal models that can transfer knowledge, generalize from limited data, and follow human instructions. These are challenges that current tabular deep learning approaches have not fully tackled. Here we introduce Generative Tabular Learning (GTL), a novel framework that integrates the advanced functionalities of large language models (LLMs)-such as prompt-based zero-shot generalization and in-context learning-into tabular deep learning. GTL capitalizes on the pre-training of LLMs on diverse tabular data, enhancing their understanding of domain-specific knowledge, numerical sequences, and statistical dependencies critical for accurate predictions. Our empirical study spans 384 public datasets, rigorously analyzing GTL&#39;s convergence and scaling behaviors and assessing the impact of varied data templates. The GTL-enhanced LLaMA-2 model demonstrates superior zero-shot and in-context learning capabilities across numerous classification and regression tasks. Notably, it achieves this without fine-tuning, outperforming traditional methods and rivaling state-of-the-art models like GPT-4 in certain cases. Through GTL, we not only foster a deeper integration of LLMs&#39; sophisticated abilities into tabular data comprehension and application but also offer a new training resource and a test bed for LLMs to enhance their ability to comprehend tabular data. To facilitate reproducible research, we release our code, data, and model checkpoints at https://github.com/microsoft/Industrial-Foundation-Models.},
  archive   = {C_KDD},
  author    = {Wen, Xumeng and Zhang, Han and Zheng, Shun and Xu, Wei and Bian, Jiang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671975},
  pages     = {3323–3333},
  title     = {From supervised to generative: A novel paradigm for tabular deep learning with large language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unveiling vulnerabilities of contrastive recommender systems
to poisoning attacks. <em>KDD</em>, 3311–3322. (<a
href="https://doi.org/10.1145/3637528.3671795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Contrastive learning (CL) has recently gained prominence in the domain of recommender systems due to its great ability to enhance recommendation accuracy and improve model robustness. Despite its advantages, this paper identifies a vulnerability of CL-based recommender systems that they are more susceptible to poisoning attacks aiming to promote individual items. Our analysis indicates that this vulnerability is attributed to the uniform spread of representations caused by the InfoNCE loss. Furthermore, theoretical and empirical evidence shows that optimizing this loss favors smooth spectral values of representations. This finding suggests that attackers could facilitate this optimization process of CL by encouraging a more uniform distribution of spectral values, thereby enhancing the degree of representation dispersion. With these insights, we attempt to reveal a potential poisoning attack against CL-based recommender systems, which encompasses a dual-objective framework: one that induces a smoother spectral value distribution to amplify the InfoNCE loss&#39;s inherent dispersion effect, named dispersion promotion; and the other that directly elevates the visibility of target items, named rank promotion. We validate the threats of our attack model through extensive experimentation on four datasets. By shedding light on these vulnerabilities, our goal is to advance the development of more robust CL-based recommender systems. The code is available at https://github.com/CoderWZW/ARLib.},
  archive   = {C_KDD},
  author    = {Wang, Zongwei and Yu, Junliang and Gao, Min and Yin, Hongzhi and Cui, Bin and Sadiq, Shazia},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671795},
  pages     = {3311–3322},
  title     = {Unveiling vulnerabilities of contrastive recommender systems to poisoning attacks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). FedSAC: Dynamic submodel allocation for collaborative
fairness in federated learning. <em>KDD</em>, 3299–3310. (<a
href="https://doi.org/10.1145/3637528.3671748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collaborative fairness stands as an essential element in federated learning to encourage client participation by equitably distributing rewards based on individual contributions. Existing methods primarily focus on adjusting gradient allocations among clients to achieve collaborative fairness. However, they frequently overlook crucial factors such as maintaining consistency across local models and catering to the diverse requirements of high-contributing clients. This oversight inevitably decreases both fairness and model accuracy in practice. To address these issues, we propose FedSAC, a novel Federated learning framework with dynamic Submodel Allocation for Collaborative fairness, backed by a theoretical convergence guarantee. First, we present the concept of &quot;bounded collaborative fairness (BCF)&quot;, which ensures fairness by tailoring rewards to individual clients based on their contributions. Second, to implement the BCF, we design a submodel allocation module with a theoretical guarantee of fairness. This module incentivizes high-contributing clients with high-performance submodels containing a diverse range of crucial neurons, thereby preserving consistency across local models. Third, we further develop a dynamic aggregation module to adaptively aggregate submodels, ensuring the equitable treatment of low-frequency neurons and consequently enhancing overall model accuracy. Extensive experiments conducted on three public benchmarks demonstrate that FedSAC outperforms all baseline methods in both fairness and model accuracy. We see this work as a significant step towards incentivizing broader client participation in federated learning. The source code is available at https://github.com/wangzihuixmu/FedSAC.},
  archive   = {C_KDD},
  author    = {Wang, Zihui and Wang, Zheng and Lyu, Lingjuan and Peng, Zhaopeng and Yang, Zhicheng and Wen, Chenglu and Yu, Rongshan and Wang, Cheng and Fan, Xiaoliang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671748},
  pages     = {3299–3310},
  title     = {FedSAC: Dynamic submodel allocation for collaborative fairness in federated learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised learning for graph dataset condensation.
<em>KDD</em>, 3289–3298. (<a
href="https://doi.org/10.1145/3637528.3671682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph dataset condensation (GDC) reduces a dataset with many graphs into a smaller dataset with fewer graphs while maintaining model training accuracy. GDC saves the storage cost and hence accelerates training. Although several GDC methods have been proposed, they are all supervised and require massive labels for the graphs, while graph labels can be scarce in many practical scenarios. To fill this gap, we propose a self-supervised graph dataset condensation method called SGDC, which does not require label information. Our initial design starts with the classical bilevel optimization paradigm for dataset condensation and incorporates contrastive learning techniques. But such a solution yields poor accuracy due to the biased gradient estimation caused by data augmentation. To solve this problem, we introduce representation matching, which conducts training by aligning the representations produced by the condensed graphs with the target representations generated by a pre-trained SSL model. This design eliminates the need for data augmentation and avoids biased gradient. We further propose a graph attention kernel, which not only improves accuracy but also reduces running time when combined with self-supervised kernel ridge regression (KRR). To simplify SGDC and make it more robust, we adopt a adjacency matrix reusing approach, which reuses the topology of the original graphs for the condensed graphs instead of repeatedly learning topology during training. Our evaluations on seven graph datasets find that SGDC improves model accuracy by up to 9.7\% compared with 5 state-of-the-art baselines, even if they use label information. Moreover, SGDC is significantly more efficient than the baselines.},
  archive   = {C_KDD},
  author    = {Wang, Yuxiang and Yan, Xiao and Jin, Shiyu and Huang, Hao and Xu, Quanqing and Zhang, Qingchen and Du, Bo and Jiang, Jiawei},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671682},
  pages     = {3289–3298},
  title     = {Self-supervised learning for graph dataset condensation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Unveiling global interactive patterns across graphs:
Towards interpretable graph neural networks. <em>KDD</em>, 3277–3288.
(<a href="https://doi.org/10.1145/3637528.3671838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph Neural Networks (GNNs) have emerged as a prominent framework for graph mining, leading to significant advances across various domains. Stemmed from the node-wise representations of GNNs, existing explanation studies have embraced the subgraph-specific viewpoint that attributes the decision results to the salient features and local structures of nodes. However, graph-level tasks necessitate long-range dependencies and global interactions for advanced GNNs, deviating significantly from subgraph-specific explanations. To bridge this gap, this paper proposes a novel intrinsically interpretable scheme for graph classification, termed as Global Interactive Pattern (GIP) learning, which introduces learnable global interactive patterns to explicitly interpret decisions. GIP first tackles the complexity of interpretation by clustering numerous nodes using a constrained graph clustering module. Then, it matches the coarsened global interactive instance with a batch of self-interpretable graph prototypes, thereby facilitating a transparent graph-level reasoning process. Extensive experiments conducted on both synthetic and real-world benchmarks demonstrate that the proposed GIP yields significantly superior interpretability and competitive performance to the state-of-the-art counterparts. Our code will be made publicly available¹.},
  archive   = {C_KDD},
  author    = {Wang, Yuwen and Liu, Shunyu and Zheng, Tongya and Chen, Kaixuan and Song, Mingli},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671838},
  pages     = {3277–3288},
  title     = {Unveiling global interactive patterns across graphs: Towards interpretable graph neural networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AsyncET: Asynchronous representation learning for knowledge
graph entity typing. <em>KDD</em>, 3267–3276. (<a
href="https://doi.org/10.1145/3637528.3671832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Knowledge graph entity typing (KGET) aims to predict the missing entity types in knowledge graphs (KG). The relationship between entities and their corresponding types is often expressed using a single relation, hasType. However, hasType has a limited capability for modeling diverse entity-type relationships in the embedding space. In this paper, we first introduce multiple auxiliary relations to model the complex entity-type relationship. We propose an efficient and robust algorithm to group similar entity types together and assign a unique auxiliary relation to each group. Then, with the auxiliary relations, we propose an Asynchronous representation learning framework for KGET, named AsyncET, where entity and type embeddings are updated alternatively. Consequently, the quality of entity embeddings is gradually improved during training by infusing type information. In addition, entity types with different granularities and semantics can be properly modeled in the embedding space. Experimental results show that AsyncET can substantially improve the performance of embedding-based methods on the KGET task and has a significant advantage over state-of-the-art neural network-based methods in terms of model sizes and inference time.},
  archive   = {C_KDD},
  author    = {Wang, Yun-Cheng and Ge, Xiou and Wang, Bin and Kuo, C.-C. Jay},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671832},
  pages     = {3267–3276},
  title     = {AsyncET: Asynchronous representation learning for knowledge graph entity typing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DPSW-sketch: A differentially private sketch framework for
frequency estimation over sliding windows. <em>KDD</em>, 3255–3266. (<a
href="https://doi.org/10.1145/3637528.3671694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The sliding window model of computation captures scenarios in which data are continually arriving in the form of a stream, and only the most recent w items are used for analysis. In this setting, an algorithm needs to accurately track some desired statistics over the sliding window using a small space. When data streams contain sensitive information about individuals, the algorithm is also urgently needed to provide a provable guarantee of privacy. In this paper, we focus on the two fundamental problems of privately (1) estimating the frequency of an arbitrary item and (2) identifying the most frequent items (i.e., heavy hitters), in the sliding window model. We propose DPSW-Sketch, a sliding window framework based on the count-min sketch that not only satisfies differential privacy over the stream but also approximates the results for frequency and heavy-hitter queries within bounded errors in sublinear time and space w.r.t. w. Extensive experiments on five real-world and synthetic datasets show that DPSW-Sketch provides significantly better utility-privacy trade-offs than state-of-the-art methods.},
  archive   = {C_KDD},
  author    = {Wang, Yiping and Wang, Yanhao and Chen, Cen},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671694},
  pages     = {3255–3266},
  title     = {DPSW-sketch: A differentially private sketch framework for frequency estimation over sliding windows},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EAGER: Two-stream generative recommender with
behavior-semantic collaboration. <em>KDD</em>, 3245–3254. (<a
href="https://doi.org/10.1145/3637528.3671775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generative retrieval has recently emerged as a promising approach to sequential recommendation, framing candidate item retrieval as an autoregressive sequence generation problem. However, existing generative methods typically focus solely on either behavioral or semantic aspects of item information, neglecting their complementary nature and thus resulting in limited effectiveness. To address this limitation, we introduce EAGER, a novel generative recommendation framework that seamlessly integrates both behavioral and semantic information. Specifically, we identify three key challenges in combining these two types of information: a unified generative architecture capable of handling two feature types, ensuring sufficient and independent learning for each type, and fostering subtle interactions that enhance collaborative information utilization. To achieve these goals, we propose (1) a two-stream generation architecture leveraging a shared encoder and two separate decoders to decode behavior tokens and semantic tokens with a confidence-based ranking strategy; (2) a global contrastive task with summary tokens to achieve discriminative decoding for each type of information; and (3) a semantic-guided transfer task designed to implicitly promote cross-interactions through reconstruction and estimation objectives. We validate the effectiveness of EAGER on four public benchmarks, demonstrating its superior performance compared to existing methods. Our source code will be publicly available on PapersWithCode.com.},
  archive   = {C_KDD},
  author    = {Wang, Ye and Xun, Jiahao and Hong, Minjie and Zhu, Jieming and Jin, Tao and Lin, Wang and Li, Haoyuan and Li, Linjun and Xia, Yan and Zhao, Zhou and Dong, Zhenhua},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671775},
  pages     = {3245–3254},
  title     = {EAGER: Two-stream generative recommender with behavior-semantic collaboration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Warming up cold-start CTR prediction by learning
item-specific feature interactions. <em>KDD</em>, 3233–3244. (<a
href="https://doi.org/10.1145/3637528.3671784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recommendation systems, new items are continuously introduced, initially lacking interaction records but gradually accumulating them over time. Accurately predicting the click-through rate (CTR) for these items is crucial for enhancing both revenue and user experience. While existing methods focus on enhancing item ID embeddings for new items within general CTR models, they tend to adopt a global feature interaction approach, often overshadowing new items with sparse data by those with abundant interactions. Addressing this, our work introduces EmerG, a novel approach that warms up cold-start CTR prediction by learning item-specific feature interaction patterns. EmerG utilizes hypernetworks to generate an item-specific feature graph based on item characteristics, which is then processed by a Graph Neural Network (GNN). This GNN is specially tailored to provably capture feature interactions at any order through a customized message passing mechanism. We further design a meta learning strategy that optimizes parameters of hypernetworks and GNN across various item CTR prediction tasks, while only adjusting a minimal set of item-specific parameters within each task. This strategy effectively reduces the risk of overfitting when dealing with limited data. Extensive experiments on benchmark datasets validate that EmerG consistently performs the best given no, a few and sufficient instances of new items.},
  archive   = {C_KDD},
  author    = {Wang, Yaqing and Piao, Hongming and Dong, Daxiang and Yao, Quanming and Zhou, Jingbo},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671784},
  pages     = {3233–3244},
  title     = {Warming up cold-start CTR prediction by learning item-specific feature interactions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing long-tailed link prediction in graph neural
networks through structure representation enhancement. <em>KDD</em>,
3222–3232. (<a href="https://doi.org/10.1145/3637528.3671864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Link prediction, as a fundamental task for graph neural networks (GNNs), has boasted significant progress in varied domains. Its success is typically influenced by the expressive power of node representation, but recent developments reveal the inferior performance of low-degree nodes owing to their sparse neighbor connections, known as the degree-based long-tailed problem. Will the degree-based long-tailed distribution similarly constrain the efficacy of GNNs on link prediction? Unexpectedly, our study reveals that only a mild correlation exists between node degree and predictive accuracy, and more importantly, the number of common neighbors between node pairs exhibits a strong correlation with accuracy. Considering node pairs with less common neighbors, i.e., tail node pairs, make up a substantial fraction of the dataset but achieve worse performance, we propose that link prediction also faces the long-tailed problem. Therefore, link prediction of GNNs is greatly hindered by the tail node pairs. After knowing the weakness of link prediction, a natural question is how can we eliminate the negative effects of the skewed long-tailed distribution on common neighbors so as to improve the performance of link prediction? Towards this end, we introduce our long-tailed framework (LTLP), which is designed to enhance the performance of tail node pairs on link prediction by increasing common neighbors. Two key modules in LTLP respectively supplement high-quality edges for tail node pairs and enforce representational alignment between head and tail node pairs within the same category, thereby improving the performance of tail node pairs. Empirical results across five datasets confirm that our approach not only achieves SOTA performance but also greatly reduces the performance bias between the head and tail. These findings underscore the efficacy and superiority of our framework in addressing the long-tailed problem in link prediction.},
  archive   = {C_KDD},
  author    = {Wang, Yakun and Wang, Daixin and Liu, Hongrui and Hu, Binbin and Yan, Yingcui and Zhang, Qiyang and Zhang, Zhiqiang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671864},
  pages     = {3222–3232},
  title     = {Optimizing long-tailed link prediction in graph neural networks through structure representation enhancement},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DiffCrime: A multimodal conditional diffusion model for
crime risk map inference. <em>KDD</em>, 3212–3221. (<a
href="https://doi.org/10.1145/3637528.3671843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Crime risk map plays a crucial role in urban planning and public security management. Traditionally, it is obtained solely from historical crime incidents or inferred from limited environmental factors, which are not sufficient to accurately model the occurrences of crimes over the geographical space well. Motivated by the impressive and realistic conditional generating power of diffusion models, in this paper, we propose a multimodal conditional diffusion method, namely, DiffCrime, to infer the crime risk map based on datasets in various domains, i.e., historical crime incidents, satellite imagery, and map imagery. It is equipped with a history-gated multimodal denoising network, i.e., HamNet, dedicated to the crime risk map inference. HamNet emphasizes the importance of historical crime data via a Gated-based History Fusion (GHF) module and adaptively controls multimodal conditions to be fused across different diffusion time steps via a Time step-Aware Modality Fusion (TAMF) module. Extensive experiments on two real-world datasets demonstrate the effectiveness of DiffCrime, which outperforms baselines by at least 43\% and 31\% in terms of RMSE, respectively.},
  archive   = {C_KDD},
  author    = {Wang, Shuliang and Pan, Xinyu and Ruan, Sijie and Han, Haoyu and Wang, Ziyu and Yuan, Hanning and Zhu, Jiabao and Li, Qi},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671843},
  pages     = {3212–3221},
  title     = {DiffCrime: A multimodal conditional diffusion model for crime risk map inference},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A hierarchical and disentangling interest learning
framework for unbiased and true news recommendation. <em>KDD</em>,
3200–3211. (<a href="https://doi.org/10.1145/3637528.3671944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the era of information explosion, news recommender systems are crucial for users to effectively and efficiently discover their interested news. However, most of the existing news recommender systems face two major issues, hampering recommendation quality. Firstly, they often oversimplify users&#39; reading interests, neglecting their hierarchical nature, spanning from high-level event (e.g., US Election) related interests to low-level news article-specifc interests. Secondly, existing work often assumes a simplistic context, disregarding the prevalence of fake news and political bias under the real-world context. This oversight leads to recommendations of biased or fake news, posing risks to individuals and society. To this end, this paper addresses these gaps by introducing a novel framework, the Hierarchical and Disentangling Interest learning framework (HDInt). HDInt incorporates a hierarchical interest learning module and a disentangling interest learning module. The former captures users&#39; high- and low-level interests, enhancing next-news recommendation accuracy. The latter effectively separates polarity and veracity information from news contents and model them more specifcally, promoting fairness- and truth-aware reading interest learning for unbiased and true news recommendations. Extensive experiments on two real-world datasets demonstrate HDInt&#39;s superiority over state-of-the-art news recommender systems in delivering accurate, unbiased, and true news recommendations.},
  archive   = {C_KDD},
  author    = {Wang, Shoujin and Wang, Wentao and Zhang, Xiuzhen and Wang, Yan and Liu, Huan and Chen, Fang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671944},
  pages     = {3200–3211},
  title     = {A hierarchical and disentangling interest learning framework for unbiased and true news recommendation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advancing molecule invariant representation via privileged
substructure identification. <em>KDD</em>, 3188–3199. (<a
href="https://doi.org/10.1145/3637528.3671886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph neural networks (GNNs) have revolutionized molecule representation learning by modeling molecules as graphs, with atoms represented as nodes and chemical bonds as edges. Despite their progress, they struggle with out-of-distribution scenarios, such as changes in size or scaffold of molecules with identical properties. Some studies attempt to mitigate this issue through graph invariant learning, which penalizes prediction variance across environments to learn invariant representations. But in the realm of molecules, core functional groups forming privileged substructures dominate molecular properties and remain invariant across distribution shifts. This highlights the need for integrating this prior knowledge and ensuring the environment split compatible with molecule invariant learning. To bridge this gap, we propose a novel framework named MILI. Specifically, we first formalize molecule invariant learning based on privileged substructure identification and introduce substructure invariance constraint. Building on this foundation, we theoretically establish two criteria for environment splits conducive to molecule invariant learning. Inspired by these criteria, we develop a dual-head graph neural network. A shared identifier identifies privileged substructures, while environment and task heads generate predictions based on variant and privileged substructures. Through the interaction of two heads, the environments are split and optimized to meet our criteria. The unified MILI guarantees that molecule invariant learning and environment split achieve mutual enhancement from theoretical analysis and network design. Extensive experiments across eight benchmarks validate the effectiveness of MILI compared to state-of-the-art baselines.},
  archive   = {C_KDD},
  author    = {Wang, Ruijia and Dai, Haoran and Yang, Cheng and Song, Le and Shi, Chuan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671886},
  pages     = {3188–3199},
  title     = {Advancing molecule invariant representation via privileged substructure identification},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CutAddPaste: Time series anomaly detection by exploiting
abnormal knowledge. <em>KDD</em>, 3176–3187. (<a
href="https://doi.org/10.1145/3637528.3671739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Detecting time-series anomalies is extremely intricate due to the rarity of anomalies and imbalanced sample categories, which often result in costly and challenging anomaly labeling. Most of the existing approaches largely depend on assumptions of normality, overlooking labeled abnormal samples. While anomaly assumptions based methods can incorporate prior knowledge of anomalies for data augmentation in training classifiers, the adopted random or coarse-grained augmentation approaches solely focus on pointwise anomalies and lack cutting-edge domain knowledge, making them less likely to achieve better performance. This paper introduces CutAddPaste, a novel anomaly assumption-based approach for detecting time-series anomalies. It primarily employs a data augmentation strategy to generate pseudo anomalies, by exploiting prior knowledge of anomalies as much as possible. At the core of CutAddPaste is cutting patches from random positions in temporal subsequence samples, adding linear trend terms, and pasting them into other samples, so that it can well approximate a variety of anomalies, including point and pattern anomalies. Experiments on standard benchmark datasets demonstrate that our method outperforms the state-of-the-art approaches.},
  archive   = {C_KDD},
  author    = {Wang, Rui and Mou, Xudong and Yang, Renyu and Gao, Kai and Liu, Pin and Liu, Chongwei and Wo, Tianyu and Liu, Xudong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671739},
  pages     = {3176–3187},
  title     = {CutAddPaste: Time series anomaly detection by exploiting abnormal knowledge},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The heterophilic snowflake hypothesis: Training and
empowering GNNs for heterophilic graphs. <em>KDD</em>, 3164–3175. (<a
href="https://doi.org/10.1145/3637528.3671791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph Neural Networks (GNNs) have become pivotal tools for a range of graph-based learning tasks. Notably, most current GNN architectures operate under the assumption of homophily, whether explicitly or implicitly. While this underlying assumption is frequently adopted, it is not universally applicable, which can result in potential shortcomings in learning effectiveness. In this paper, or the first time, we transfer the prevailing concept of &quot;one node one receptive field&quot; to the heterophilic graph. By constructing a proxy label predictor, we enable each node to possess a latent prediction distribution, which assists connected nodes in determining whether they should aggregate their associated neighbors. Ultimately, every node can have its own unique aggregation hop and pattern, much like each snowflake is unique and possesses its own characteristics. Based on observations, we innovatively introduce the Heterophily Snowflake Hypothesis and provide an effective solution to guide and facilitate research on heterophilic graphs and beyond. We conduct comprehensive experiments including (1) main results on 10 graphs with varying heterophily ratios across 10 backbones; (2) scalability on various deep GNN backbones (SGC, JKNet, etc.) across various large number of layers (2,4,6,8,16,32 layers); (3) comparison with conventional snowflake hypothesis; (4) efficiency comparison with existing graph pruning algorithms. Our observations show that our framework acts as a versatile operator for diverse tasks. It can be integrated into various GNN frameworks, boosting performance in-depth and offering an explainable approach to choosing the optimal network depth. The source code is available at https://github.com/bingreeky/HeteroSnoH.},
  archive   = {C_KDD},
  author    = {Wang, Kun and Zhang, Guibin and Zhang, Xinnan and Fang, Junfeng and Wu, Xun and Li, Guohao and Pan, Shirui and Huang, Wei and Liang, Yuxuan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671791},
  pages     = {3164–3175},
  title     = {The heterophilic snowflake hypothesis: Training and empowering GNNs for heterophilic graphs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The snowflake hypothesis: Training and powering GNN with one
node one receptive field. <em>KDD</em>, 3152–3163. (<a
href="https://doi.org/10.1145/3637528.3671766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite Graph Neural Networks (GNNs) demonstrating considerable promise in graph representation learning tasks, GNNs predominantly face significant issues with overfitting and over-smoothing as they go deeper as models of computer vision (CV) realm. The success of artificial intelligence in computer vision and natural language processing largely stems from its ability to train deep models effectively. We have thus conducted a systematic study on deep GNN models. Our findings indicate that the current success of deep GNNs primarily stems from (I) the adoption of innovations from CNNs, such as residual/skip connections, or (II) the tailor-made aggregation algorithms like DropEdge. However, these algorithms often lack intrinsic interpretability and indiscriminately treat all nodes within a given layer in a similar manner, thereby failing to capture the nuanced differences among various nodes. In this paper, we introduce the Snowflake Hypothesis -- a novel paradigm underpinning the concept of &quot;one node, one receptive field&#39;&#39;. The hypothesis draws inspiration from the unique and individualistic patterns of each snowflake, proposing a corresponding uniqueness in the receptive fields of nodes in the GNNs. We employ the simplest gradient and node-level cosine distance as guiding principles to regulate the aggregation depth for each node, and conduct comprehensive experiments including: (1) different training scheme; (2) various shallow and deep GNN backbones; (3) various numbers of layers (8, 16, 32, 64) on multiple benchmarks; (4) compare with different aggregation strategies. The observational results demonstrate that our framework can serve as a universal operator for a range of tasks, and it displays tremendous potential on deep GNNs. Code is available at: https://github.com/CunWang520/Snowhypothe.},
  archive   = {C_KDD},
  author    = {Wang, Kun and Li, Guohao and Wang, Shilong and Zhang, Guibin and Wang, Kai and You, Yang and Fang, Junfeng and Peng, Xiaojiang and Liang, Yuxuan and Wang, Yang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671766},
  pages     = {3152–3163},
  title     = {The snowflake hypothesis: Training and powering GNN with one node one receptive field},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). POND: Multi-source time series domain adaptation with
information-aware prompt tuning. <em>KDD</em>, 3140–3151. (<a
href="https://doi.org/10.1145/3637528.3671721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Time series domain adaptation stands as a pivotal and intricate challenge with diverse applications, including but not limited to human activity recognition, sleep stage classification, and machine fault diagnosis. Despite the numerous domain adaptation techniques proposed to tackle this complex problem, they primarily focus on domain adaptation from a single source domain. Yet, it is more crucial to investigate domain adaptation from multiple domains due to the potential for greater improvements. To address this, three important challenges need to be overcome: 1). The lack of exploration to utilize domain-specific information for domain adaptation, 2). The difficulty to learn domain-specific information that changes over time, and 3). The difficulty to evaluate learned domain-specific information. In order to tackle these challenges simultaneously, in this paper, we introduce PrOmpt-based domaiN Discrimination (POND), the first framework to utilize prompts for time series domain adaptation. Specifically, to address Challenge 1, we extend the idea of prompt tuning to time series analysis and learn prompts to capture common and domain-specific information from all source domains. To handle Challenge 2, we introduce a conditional module for each source domain to generate prompts from time series input data. For Challenge 3, we propose two criteria to select good prompts, which are used to choose the most suitable source domain for domain adaptation. The efficacy and robustness of our proposed POND model are extensively validated through experiments across 50 scenarios encompassing four datasets. Experimental results demonstrate that our proposed POND model outperforms all state-of-the-art comparison methods by up to 66\% on the F1-score.},
  archive   = {C_KDD},
  author    = {Wang, Junxiang and Bai, Guangji and Cheng, Wei and Chen, Zhengzhang and Zhao, Liang and Chen, Haifeng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671721},
  pages     = {3140–3151},
  title     = {POND: Multi-source time series domain adaptation with information-aware prompt tuning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DyPS: Dynamic parameter sharing in multi-agent reinforcement
learning for spatio-temporal resource allocation. <em>KDD</em>,
3128–3139. (<a href="https://doi.org/10.1145/3637528.3672052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In large-scale metropolis, it is critical to efficiently allocate various resources such as electricity, medical care, and transportation to meet the living demands of citizens, according to the spatio-temporal distributions of resources and demands. Previous researchers have done plentiful work on such problems by leveraging Multi-Agent Reinforcement Learning (MARL) methods, where multiple agents cooperatively regulate and allocate the resources to meet the demands. However, facing the great number of agents in large cities, existing MARL methods lack efficient parameter sharing strategies among agents to reduce computational complexity. There remain two primary challenges in efficient parameter sharing: (1) during the RL training process, the behavior of agents changes significantly, limiting the performance of group parameter sharing based on fixed role division decided before training; (2) the behavior of agents forms complicated action trajectories, where their role characteristics are implicit, adding difficulty to dynamically adjusting agent role divisions during the training process. In this paper, we propose Dynamic Parameter Sharing (DyPS) to solve the above challenges. We design self-supervised learning tasks to extract the implicit behavioral characteristics from the action trajectories of agents. Based on the obtained behavioral characteristics, we propose a hierarchical MARL framework capable of dynamically revising the agent role divisions during the training process and thus shares parameters among agents with the same role, reducing computational complexity. In addition, our framework can be combined with various typical MARL algorithms, including IPPO, MAPPO, etc. We conduct 7 experiments in 4 representative resource allocation scenarios, where extensive results demonstrate our method&#39;s superior performance, outperforming the state-of-the-art baseline methods by up to 31\%. Our source codes are available at https://github.com/tsinghua-fib-lab/DyPS.},
  archive   = {C_KDD},
  author    = {Wang, Jingwei and Hao, Qianyue and Huang, Wenzhen and Fan, Xiaochen and Tang, Zhentao and Wang, Bin and Hao, Jianye and Li, Yong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672052},
  pages     = {3128–3139},
  title     = {DyPS: Dynamic parameter sharing in multi-agent reinforcement learning for spatio-temporal resource allocation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel prompt tuning for graph transformers: Tailoring
prompts to graph topologies. <em>KDD</em>, 3116–3127. (<a
href="https://doi.org/10.1145/3637528.3671804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep graph prompt tuning (DeepGPT), which only tunes a set of continuous prompts for graph transformers, significantly decreases the storage usage during training. However, DeepGPT is limited by its uniform prompts to input graphs with various structures. This is because different graph structures dictate various feature interactions between nodes, while the uniform prompts are not dynamic to tailor the feature transformation for the graph topology. In this paper, we propose a &amp;lt;u&amp;gt;T&amp;lt;/u&amp;gt;opo-specific &amp;lt;u&amp;gt;G&amp;lt;/u&amp;gt;raph &amp;lt;u&amp;gt;P&amp;lt;/u&amp;gt;rompt &amp;lt;u&amp;gt;T&amp;lt;/u&amp;gt;uning (TGPT ), which provides topo-specific prompts tailored to the topological structures of input graphs. Specifically, TGPT learns trainable embeddings for graphlets and frequencies, where graphlets are fundamental sub-graphs that describe the structure around specific nodes. Based on the statistic data about graphlets of input graph, topo-specific prompts are generated by graphlet embeddings and frequency embeddings. The topo-specific prompts include node-level topo-specific prompts for specified nodes, a graph-level topo-specific prompt for the entire graph, and a task-specific prompt to learn task-related information. They are all inserted into specific graph nodes to perform feature transformation, providing specified feature transformation for input graphs with different topological structures. Extensive experiments show that our method outperforms existing lightweight fine-tuning methods and DeepGPT in molecular graph classification and regression with comparable parameters.},
  archive   = {C_KDD},
  author    = {Wang, Jingchao and Deng, Zhengnan and Lin, Tongxu and Li, Wenyuan and Ling, Shaobin},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671804},
  pages     = {3116–3127},
  title     = {A novel prompt tuning for graph transformers: Tailoring prompts to graph topologies},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust predictions with ambiguous time delays: A bootstrap
strategy. <em>KDD</em>, 3104–3115. (<a
href="https://doi.org/10.1145/3637528.3671920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In contemporary data-driven environments, the generation and processing of multivariate time series data is an omnipresent challenge, often complicated by time delays between different time series. These delays, originating from a multitude of sources like varying data transmission dynamics, sensor interferences, and environmental changes, introduce significant complexities. Traditional Time Delay Estimation methods, which typically assume a fixed constant time delay, may not fully capture these variabilities, compromising the precision of predictive models in diverse settings.To address this issue, we introduce the Time Series Model Bootstrap (TSMB), a versatile framework designed to handle potentially varying or even nondeterministic time delays in time series modeling. Contrary to traditional approaches that hinge on the assumption of a single, consistent time delay, TSMB adopts a non-parametric stance, acknowledging and incorporating time delay uncertainties. TSMB significantly bolsters the performance of models that are trained and make predictions using this framework, making it highly suitable for a wide range of dynamic and interconnected data environments.Our comprehensive evaluations, conducted on real-world datasets with different types of time delays, confirm the adaptability and effectiveness of TSMB in multiple contexts. These include, but are not limited to, power and occupancy forecasting in intelligent infrastructures, air quality monitoring, and intricate processes like mineral processing. Further diagnostic analyses strengthen the case for the TSMB estimator&#39;s robustness, underlining its significance in scenarios where ambiguity in time delays can have a significant impact on the predictive task.},
  archive   = {C_KDD},
  author    = {Wang, Jiajie and Lin, Zhiyuan Jerry and Chen, Wen},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671920},
  pages     = {3104–3115},
  title     = {Robust predictions with ambiguous time delays: A bootstrap strategy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning from emergence: A study on proactively inhibiting
the monosemantic neurons of artificial neural networks. <em>KDD</em>,
3092–3103. (<a href="https://doi.org/10.1145/3637528.3671776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, emergence has received widespread attention from the research community along with the success of large-scale models. Different from the literature, we hypothesize a key factor that promotes the performance during the increase of scale: the reduction of monosemantic neurons that can only form one-to-one correlations with specific features. Monosemantic neurons tend to be sparser and have negative impacts on the performance in large models. Inspired by this insight, we propose an intuitive idea to identify monosemantic neurons and inhibit them. However, achieving this goal is a non-trivial task as there is no unified quantitative evaluation metric and simply banning monosemantic neurons does not promote polysemanticity in neural networks. Therefore, we first propose a new metric to measure the monosemanticity of neurons with the guarantee of efficiency for online computation, then introduce a theoretically supported method to suppress monosemantic neurons and proactively promote the ratios of polysemantic neurons in training neural networks. We validate our conjecture that monosemanticity brings about performance change at different model scales on a variety of neural networks and benchmark datasets in different areas, including language, image, and physics simulation tasks. Further experiments validate our analysis and theory regarding the inhibition of monosemanticity.},
  archive   = {C_KDD},
  author    = {Wang, Jiachuan and Di, Shimin and Chen, Lei and Ng, Charles Wang Wai},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671776},
  pages     = {3092–3103},
  title     = {Learning from emergence: A study on proactively inhibiting the monosemantic neurons of artificial neural networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Effective edge-wise representation learning in
edge-attributed bipartite graphs. <em>KDD</em>, 3081–3091. (<a
href="https://doi.org/10.1145/3637528.3671805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph representation learning (GRL) is to encode graph elements into informative vector representations, which can be used in downstream tasks for analyzing graph-structured data and has seen extensive applications in various domains. However, the majority of extant studies on GRL are geared towards generating node representations, which cannot be readily employed to perform edge-based analytics tasks in edge-attributed bipartite graphs (EABGs) that pervade the real world, e.g., spam review detection in customer-product reviews and identifying fraudulent transactions in user-merchant networks. Compared to node-wise GRL, learning edge representations (ERL) on such graphs is challenging due to the need to incorporate the structure and attribute semantics from the perspective of edges while considering the separate influence of two heterogeneous node sets U and V in bipartite graphs. To our knowledge, despite its importance, limited research has been devoted to this frontier, and existing workarounds all suffer from sub-par results.Motivated by this, this paper designs EAGLE, an effective ERL method for EABGs. Building on an in-depth and rigorous theoretical analysis, we propose the factorized feature propagation (FFP) scheme for edge representations with adequate incorporation of long-range dependencies of edges/features without incurring tremendous computation overheads. We further ameliorate FFP as a dual-view FFP by taking into account the influences from nodes in U and V severally in ERL. Extensive experiments on 5 real datasets showcase the effectiveness of the proposed EAGLE models in semi-supervised edge classification tasks. In particular, EAGLE can attain a considerable gain of at most 38.11\% in AP and 1.86\% in AUC when compared to the best baselines.},
  archive   = {C_KDD},
  author    = {Wang, Hewen and Yang, Renchi and Xiao, Xiaokui},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671805},
  pages     = {3081–3091},
  title     = {Effective edge-wise representation learning in edge-attributed bipartite graphs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FedNLR: Federated learning with neuron-wise learning rates.
<em>KDD</em>, 3069–3080. (<a
href="https://doi.org/10.1145/3637528.3672042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Federated Learning (FL) suffers from severe performance degradation due to the data heterogeneity among clients. Some existing work suggests that the fundamental reason is that data heterogeneity can cause local model drift, and therefore proposes to calibrate the direction of local updates to solve this problem. Though effective, existing methods generally take the model as a whole, which lacks a deep understanding of how the neurons within deep classification models evolve during local training to form model drift. In this paper, we bridge this gap by performing an intuitive and theoretical analysis of the activation changes of each neuron during local training. Our analysis shows that the high activation of some neurons on the samples of a certain class will be reduced during local training when these samples are not included in the client, which we call neuron drift, thus leading to the performance reduction of this class. Motivated by this, we propose a novel and simple algorithm called FedNLR, which utilizes &amp;lt;u&amp;gt;N&amp;lt;/u&amp;gt;euron-wise &amp;lt;u&amp;gt;L&amp;lt;/u&amp;gt;earning &amp;lt;u&amp;gt;R&amp;lt;/u&amp;gt;ates during the FL local training process. The principle behind this is to enhance the learning of neurons bound to local classes on local data knowledge while reducing the decay of non-local classes knowledge stored in neurons. Experimental results demonstrate that FedNLR achieves state-of-the-art performance on federated learning with popular deep neural networks.},
  archive   = {C_KDD},
  author    = {Wang, Haozhao and Zheng, Peirong and Han, Xingshuo and Xu, Wenchao and Li, Ruixuan and Zhang, Tianwei},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672042},
  pages     = {3069–3080},
  title     = {FedNLR: Federated learning with neuron-wise learning rates},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Unsupervised heterogeneous graph rewriting attack via node
clustering. <em>KDD</em>, 3057–3068. (<a
href="https://doi.org/10.1145/3637528.3671716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-supervised learning (SSL) has become one of the most popular learning paradigms and has achieved remarkable success in the graph field. Recently, a series of pre-training studies on heterogeneous graphs (HGs) using SSL have been proposed considering the heterogeneity of real-world graph data. However, verification of the robustness of heterogeneous graph pre-training is still a research gap. Most existing researches focus on supervised attacks on graphs, which are limited to a specific scenario and will not work when labels are not available. In this paper, we propose a novel unsupervised heterogeneous graph rewriting attack via node clustering (HGAC) that can effectively attack HG pre-training models without using labels. Specifically, a heterogeneous edge rewriting strategy is designed to ensure the rationality and concealment of the attacks. Then, a tailored heterogeneous graph contrastive learning (HGCL) is used as a surrogate model. Moreover, we leverage node clustering results of the clean HGs as the pseudo-labels to guide the optimization of structural attacks. Extensive experiments exhibit powerful attack performances of our HGAC on various downstream tasks (i.e., node classification, node clustering, metapath prediction, and visualization) under poisoning attack and evasion attack.},
  archive   = {C_KDD},
  author    = {Wang, Haosen and Xu, Can and Shi, Chenglong and Zheng, Pengfei and Zhang, Shiming and Cheng, Minhao and Chen, Hongyang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671716},
  pages     = {3057–3068},
  title     = {Unsupervised heterogeneous graph rewriting attack via node clustering},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mastering long-tail complexity on graphs: Characterization,
learning, and generalization. <em>KDD</em>, 3045–3056. (<a
href="https://doi.org/10.1145/3637528.3671880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the context of long-tail classification on graphs, the vast majority of existing work primarily revolves around the development of model debiasing strategies, intending to mitigate class imbalances and enhance the overall performance. Despite the notable success, there is very limited literature that provides a theoretical tool for characterizing the behaviors of long-tail classes in graphs and gaining insight into generalization performance in real-world scenarios. To bridge this gap, we propose a generalization bound for long-tail classification on graphs by formulating the problem in the fashion of multi-task learning, i.e., each task corresponds to the prediction of one particular class. Our theoretical results show that the generalization performance of long-tail classification is dominated by the overall loss range and the task complexity. Building upon the theoretical findings, we propose a novel generic framework HierTail for long-tail classification on graphs. In particular, we start with a hierarchical task grouping module that allows us to assign related tasks into hypertasks and thus control the complexity of the task space; then, we further design a balanced contrastive learning module to adaptively balance the gradients of both head and tail classes to control the loss range across all tasks in a unified fashion. Extensive experiments demonstrate the effectiveness of HierTail in characterizing long-tail classes on real graphs, which achieves up to 12.9\% improvement over the leading baseline method in balanced accuracy.},
  archive   = {C_KDD},
  author    = {Wang, Haohui and Jing, Baoyu and Ding, Kaize and Zhu, Yada and Cheng, Wei and Zhang, Si and Fan, Yonghui and Zhang, Liqing and Zhou, Dawei},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671880},
  pages     = {3045–3056},
  title     = {Mastering long-tail complexity on graphs: Characterization, learning, and generalization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Revisiting local PageRank estimation on undirected graphs:
Simple and optimal. <em>KDD</em>, 3036–3044. (<a
href="https://doi.org/10.1145/3637528.3671820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a simple and optimal algorithm, BackMC, for local PageRank estimation in undirected graphs: given an arbitrary target node t in an undirected graph G comprising n nodes and m edges, BackMC accurately estimates the PageRank score of node t while assuring a small relative error and a high success probability. The worst-case computational complexity of BackMC is upper bounded by O(1/dmin ⋅ min(dt, m1/2)), where dmin denotes the minimum degree of G, and dt denotes the degree of t, respectively. Compared to the previously best upper bound of O(log n ⋅ min(dt, m1/2)) (VLDB &#39;23), which is derived from a significantly more complex algorithm and analysis, our BackMC improves the computational complexity for this problem by a factor of Θ(log n/dmin) with a much simpler algorithm. Furthermore, we establish a matching lower bound of Ω(1/dmin ⋅ min(dt, m1/2)) for any algorithm that attempts to solve the problem of local PageRank estimation, demonstrating the theoretical optimality of our BackMC. We conduct extensive experiments on various large-scale real-world and synthetic graphs, where BackMC consistently shows superior performance.},
  archive   = {C_KDD},
  author    = {Wang, Hanzhi},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671820},
  pages     = {3036–3044},
  title     = {Revisiting local PageRank estimation on undirected graphs: Simple and optimal},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Routing evidence for unseen actions in video moment
retrieval. <em>KDD</em>, 3024–3035. (<a
href="https://doi.org/10.1145/3637528.3671693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Video moment retrieval (VMR) is a cutting-edge vision-language task locating a segment in a video according to the query. Though the methods have achieved significant performance, they assume that training and testing samples share the same action types, hindering real-world application. In this paper, we specifically consider a new problem: video moment retrieval by queries with unseen actions. We propose a plug-and-play structure, Routing Evidence (RE), with multiple evidence-learning heads and dynamically route one to locate a sentence with an unseen action. Each evidence-learning head estimates the uncertainty while regressing timestamps. We formulate the evidence distribution by a Normal-Inverse Gamma function and design a router to select the most appropriate distribution for a sample. Empirically, we study the efficacy of RE on three updated databases where training and testing samples contain different action types. We find that RE outperforms other state-of-the-art methods with a more robust predictor. Code and data will be available at https://github.com/dieuroi/Routing-Evidence.},
  archive   = {C_KDD},
  author    = {Wang, Guolong and Wu, Xun and Qin, Zheng and Shi, Liangliang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671693},
  pages     = {3024–3035},
  title     = {Routing evidence for unseen actions in video moment retrieval},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CE-RCFR: Robust counterfactual regression for
consensus-enabled treatment effect estimation. <em>KDD</em>, 3013–3023.
(<a href="https://doi.org/10.1145/3637528.3672054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Estimating individual treatment effects (ITE) from observational data is challenging due to the absence of counterfactuals and the treatment selection bias. Prevalent ITE estimation methods tackle these challenges by aligning the treated and controlled distributions in the representational space. However, two critical issues have long been overlooked: (1)Mini-batch sampling sensitivity (MSS) issue, where representation distribution alignment at a mini-batch level is vulnerable to poor sampling cases, such as data imbalance and outliers; (2)Inconsistent representation learning (IRL) issue, where representation learning within a unified backbone network suffers from inconsistent gradient update directions due to the distribution skew between different treatment groups. To resolve these issues, we propose CE-RCFR, a &amp;lt;u&amp;gt;R&amp;lt;/u&amp;gt;obust &amp;lt;u&amp;gt;C&amp;lt;/u&amp;gt;ounter&amp;lt;u&amp;gt;F&amp;lt;/u&amp;gt;actual &amp;lt;u&amp;gt;R&amp;lt;/u&amp;gt;egression framework for &amp;lt;u&amp;gt;C&amp;lt;/u&amp;gt;onsensus-&amp;lt;u&amp;gt;E&amp;lt;/u&amp;gt;nabled causal effect estimation, including a relaxed distribution discrepancy regularizer (RDDR) module and a consensus-enabled aggregator (CEA) module. Specifically, for the robust representation alignment perspective, RDDR addresses the MSS issue by minimizing unbalanced optimal transport divergence between different treatment groups with a relaxed marginal constraint. For the accurate representation optimization perspective, CEA addresses the IRL issue by resolving the consistent gradient update directions on shared parameters within the backbone network. Extensive experiments demonstrate that CE-RCFR significantly outperforms the state-of-the-art methods in treatment effect estimations.},
  archive   = {C_KDD},
  author    = {Wang, Fan and Chen, Chaochao and Liu, Weiming and Fan, Tianhao and Liao, Xinting and Tan, Yanchao and Qi, Lianyong and Zheng, Xiaolin},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672054},
  pages     = {3013–3023},
  title     = {CE-RCFR: Robust counterfactual regression for consensus-enabled treatment effect estimation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforced compressive neural architecture search for
versatile adversarial robustness. <em>KDD</em>, 3001–3012. (<a
href="https://doi.org/10.1145/3637528.3672009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Prior research on neural architecture search (NAS) for adversarial robustness has revealed that a lightweight and adversarially robust sub-network could exist in a non-robust large teacher network. Such a sub-network is generally discovered based on heuristic rules to perform neural architecture search. However, heuristic rules are inadequate to handle diverse adversarial attacks and different &quot;teacher&quot; network capacity. To address this key challenge, we propose Reinforced Compressive Neural Architecture Search (RC-NAS), aiming to achieve Versatile Adversarial Robustness. Specifically, we define novel task settings that compose datasets, adversarial attacks, and teacher network configuration. Given diverse tasks, we develop an innovative dual-level training paradigm that consists of a meta-training and a fine-tuning phase to effectively expose the RL agent to diverse attack scenarios (in meta-training), and make it adapt quickly to locate an optimal sub-network (in fine-tuning) for previously unseen scenarios. Experiments show that our framework could achieve adaptive compression towards different initial teacher networks, datasets, and adversarial attacks, resulting in more lightweight and adversarially robust architectures. We also provide a theoretical analysis to explain why the reinforcement learning (RL)-guided adversarial architectural search helps adversarial robustness over standard adversarial training methods.},
  archive   = {C_KDD},
  author    = {Wang, Dingrong and Sapkota, Hitesh and Tao, Zhiqiang and Yu, Qi},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672009},
  pages     = {3001–3012},
  title     = {Reinforced compressive neural architecture search for versatile adversarial robustness},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Global human-guided counterfactual explanations for
molecular properties via reinforcement learning. <em>KDD</em>,
2991–3000. (<a href="https://doi.org/10.1145/3637528.3672045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Counterfactual explanations of Graph Neural Networks (GNNs) offer a powerful way to understand data that can naturally be represented by a graph structure. Furthermore, in many domains, it is highly desirable to derive data-driven global explanations or rules that can better explain the high-level properties of the models and data in question. However, evaluating global counterfactual explanations is hard in real-world datasets due to a lack of human-annotated ground truth, which limits their use in areas like molecular sciences. Additionally, the increasing scale of these datasets provides a challenge for random search-based methods. In this paper, we develop a novel global explanation model RLHEX for molecular property prediction. It aligns the counterfactual explanations with human-defined principles, making the explanations more interpretable and easy for experts to evaluate. RLHEX includes a VAE-based graph generator to generate global explanations and an adapter to adjust the latent representation space to human-defined principles. Optimized by Proximal Policy Optimization (PPO), the global explanations produced by RLHEX cover 4.12\% more input graphs and reduce the distance between the counterfactual explanation set and the input set by 0.47\% on average across three molecular datasets. RLHEX provides a flexible framework to incorporate different human-designed principles into the counterfactual explanation generation process, aligning these explanations with domain expertise. The code and data are released at https://github.com/dqwang122/RLHEX.},
  archive   = {C_KDD},
  author    = {Wang, Danqing and Antoniades, Antonis and Luong, Kha-Dinh and Zhang, Edwin and Kosan, Mert and Li, Jiachen and Singh, Ambuj and Wang, William Yang and Li, Lei},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672045},
  pages     = {2991–3000},
  title     = {Global human-guided counterfactual explanations for molecular properties via reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-scale detection of anomalous spatio-temporal
trajectories in evolving trajectory datasets. <em>KDD</em>, 2980–2990.
(<a href="https://doi.org/10.1145/3637528.3671874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A trajectory is a sequence of timestamped point locations that captures the movement of an object such as a vehicle. Such trajectories encode complex spatial and temporal patterns and provide rich information about object mobility and the underlying infrastructures, typically road networks, within which the movements occur. A trajectory dataset is evolving when new trajectories are included continuously. The ability to detect anomalous trajectories in online fashion in this setting is fundamental and challenging functionality that has many applications, e.g., location-based services. State-of-the-art solutions determine anomalies based on the shapes or routes of trajectories, ignoring potential anomalies caused by different sampling rates or time offsets. We propose a multi-scale model, termed MST-OATD, for anomalous streaming trajectory detection that considers both the spatial and temporal aspects of trajectories. The model&#39;s multi-scale capabilities aim to enable extraction of trajectory features at multiple scales. In addition, to improve model evolvability and to contend with changes in trajectory patterns, the model is equipped with a learned ranking model that updates the training set as new trajectories are included. Experiments on real datasets offer evidence that the model can outperform state-of-the-art solutions and is capable of real-time anomaly detection. Further, the learned ranking model achieves promising results when updating the training set with newly arrived trajectories.},
  archive   = {C_KDD},
  author    = {Wang, Chenhao and Chen, Lisi and Shang, Shuo and Jensen, Christian S. and Kalnis, Panos},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671874},
  pages     = {2980–2990},
  title     = {Multi-scale detection of anomalous spatio-temporal trajectories in evolving trajectory datasets},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pre-training with transferable attention for addressing
market shifts in cross-market sequential recommendation. <em>KDD</em>,
2970–2979. (<a href="https://doi.org/10.1145/3637528.3671698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cross-market recommendation (CMR) involves selling the same set of items across multiple nations or regions within a transfer learning framework. However, CMR&#39;s distinctive characteristics, including limited data sharing due to privacy policies, absence of user overlap, and a shared item set between markets present challenges for traditional recommendation methods. Moreover, CMR experiences market shifts, leading to differences in item popularity and user preferences among different markets. This study focuses on cross-market sequential recommendation (CMSR) and proposes the Cross-market Attention Transferring with Sequential Recommendation (CAT-SR) framework to address these challenges and market shifts. CAT-SR incorporates a pre-training strategy emphasizing item-item correlation, selective self-attention transferring for effective transfer learning, and query and key adapters for market-specific user preferences. Experimental results on real-world cross-market datasets demonstrate the superiority of CAT-SR, and ablation studies validate the benefits of its components across different geographical continents. CAT-SR offers a robust and adaptable solution for cross-market sequential recommendation. The code is available at https://github.com/ChenMetanoia/CATSR-KDD/.},
  archive   = {C_KDD},
  author    = {Wang, Chen and Fan, Ziwei and Yang, Liangwei and Yang, Mingdai and Liu, Xiaolong and Liu, Zhiwei and Yu, Philip},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671698},
  pages     = {2970–2979},
  title     = {Pre-training with transferable attention for addressing market shifts in cross-market sequential recommendation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Provable adaptivity of adam under non-uniform smoothness.
<em>KDD</em>, 2960–2969. (<a
href="https://doi.org/10.1145/3637528.3671718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Adam is widely adopted in practical applications due to its fast convergence. However, its theoretical analysis is still far from satisfactory. Existing convergence analyses for Adam rely on the bounded smoothness assumption, referred to as the L-smooth condition. Unfortunately, this assumption does not hold for many deep learning tasks. Moreover, we believe that this assumption obscures the true benefit of Adam, as the algorithm can adapt its update magnitude according to local smoothness. This important feature of Adam becomes irrelevant when assuming globally bounded smoothness. This paper studies the convergence of randomly reshuffled Adam (RR Adam) with diminishing learning rate, which is the major version of Adam adopted in deep learning tasks. We present the first convergence analysis of RR Adam without the bounded smoothness assumption. We demonstrate that RR Adam can maintain its convergence properties when smoothness is linearly bounded by the gradient norm, referred to as the (L0, L1)-smooth condition. We further compare Adam to SGD when both methods use diminishing learning rate. We refine the existing lower bound of SGD and show that SGD can be slower than Adam. To our knowledge, this is the first time that Adam and SGD are rigorously compared in the same setting and the advantage of Adam is revealed.},
  archive   = {C_KDD},
  author    = {Wang, Bohan and Zhang, Yushun and Zhang, Huishuai and Meng, Qi and Sun, Ruoyu and Ma, Zhi-Ming and Liu, Tie-Yan and Luo, Zhi-Quan and Chen, Wei},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671718},
  pages     = {2960–2969},
  title     = {Provable adaptivity of adam under non-uniform smoothness},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). STONE: A spatio-temporal OOD learning framework kills both
spatial and temporal shifts. <em>KDD</em>, 2948–2959. (<a
href="https://doi.org/10.1145/3637528.3671680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traffic prediction is a crucial task in the Intelligent Transportation System (ITS), receiving significant attention from both industry and academia. Numerous spatio-temporal graph convolutional networks have emerged for traffic prediction and achieved remarkable success. However, these models have limitations in terms of generalization and scalability when dealing with Out-of-Distribution (OOD) graph data with both structural and temporal shifts. To tackle the challenges of spatio-temporal shift, we propose a framework called STONE by learning invariable node dependencies, which achieve stable performance in variable environments. STONE initially employs gated-transformers to extract spatial and temporal semantic graphs. These two kinds of graphs represent spatial and temporal dependencies, respectively. Then we design three techniques to address spatio-temporal shifts. Firstly, we introduce a Fr\&#39;{e}chet embedding method that is insensitive to structural shifts, and this embedding space can integrate loose position dependencies of nodes within the graph. Secondly, we propose a graph intervention mechanism to generate multiple variant environments by perturbing two kinds of semantic graphs without any data augmentations, and STONE can explore invariant node representation from environments. Finally, we further introduce an explore-to-extrapolate risk objective to enhance the variety of generated environments. We conduct experiments on multiple traffic datasets, and the results demonstrate that our proposed model exhibits competitive performance in terms of generalization and scalability.},
  archive   = {C_KDD},
  author    = {Wang, Binwu and Ma, Jiaming and Wang, Pengkun and Wang, Xu and Zhang, Yudong and Zhou, Zhengyang and Wang, Yang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671680},
  pages     = {2948–2959},
  title     = {STONE: A spatio-temporal OOD learning framework kills both spatial and temporal shifts},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flexible graph neural diffusion with latent class
representation learning. <em>KDD</em>, 2936–2947. (<a
href="https://doi.org/10.1145/3637528.3671860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In existing graph data, the connection relationships often exhibit uniform weights, leading to the model aggregating neighboring nodes with equal weights across various connection types. However, this uniform aggregation of diverse information diminishes the discriminability of node representations, contributing significantly to the over-smoothing issue in models. In this paper, we propose the Flexible Graph Neural Diffusion (FGND) model, incorporating latent class representation to address the misalignment between graph topology and node features. In particular, we combine latent class representation learning with the inherent graph topology to reconstruct the diffusion matrix during the graph diffusion process. We introduce the sim metric to quantify the degree of mismatch between graph topology and node features. By flexibly adjusting the dependency level on node features through the hyperparameter, we accommodate diverse adjacency relationships. The effective filtering of noise in the topology also allows the model to capture higher order information, significantly alleviating the over-smoothing problem. Meanwhile, we model the graphical diffusion process as a set of differential equations and employ advanced partial differential equation tools to obtain more accurate solutions. Empirical evaluations on five benchmarks reveal that our FGND model outperforms existing popular GNN methods in terms of both overall performance and stability under data perturbations. Meanwhile, our model exhibits superior performance in comparison to models tailored for heterogeneous graphs and those designed to address oversmoothing issues.},
  archive   = {C_KDD},
  author    = {Wan, Liangtian and Han, Huijin and Sun, Lu and Zhang, Zixun and Ning, Zhaolong and Yan, Xiaoran and Xia, Feng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671860},
  pages     = {2936–2947},
  title     = {Flexible graph neural diffusion with latent class representation learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online drift detection with maximum concept discrepancy.
<em>KDD</em>, 2924–2935. (<a
href="https://doi.org/10.1145/3637528.3672016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Continuous learning from an immense volume of data streams becomes exceptionally critical in the internet era. However, data streams often do not conform to the same distribution over time, leading to a phenomenon called concept drift. Since a fixed static model is unreliable for inferring concept-drifted data streams, establishing an adaptive mechanism for detecting concept drift is crucial. Current methods for concept drift detection primarily assume that the labels or error rates of downstream models are given and/or underlying statistical properties exist in data streams. These approaches, however, struggle to address high-dimensional data streams with intricate irregular distribution shifts, which are more prevalent in real-world scenarios. In this paper, we propose MCD-DD, a novel concept drift detection method based on maximum concept discrepancy, inspired by the maximum mean discrepancy. Our method can adaptively identify varying forms of concept drift by contrastive learning of concept embeddings without relying on labels or statistical properties. With thorough experiments under synthetic and real-world scenarios, we demonstrate that the proposed method outperforms existing baselines in identifying concept drifts and enables qualitative analysis with high explainability.},
  archive   = {C_KDD},
  author    = {Wan, Ke and Liang, Yi and Yoon, Susik},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672016},
  pages     = {2924–2935},
  title     = {Online drift detection with maximum concept discrepancy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rotative factorization machines. <em>KDD</em>, 2912–2923.
(<a href="https://doi.org/10.1145/3637528.3671740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Feature interaction learning (FIL) focuses on capturing the complex relationships among multiple features for building predictive models, which is widely used in real-world tasks. Despite the research progress, existing FIL methods suffer from two major limitations. Firstly, they mainly model the feature interactions within a bounded order (e.g., small integer order) due to the exponential growth of the interaction terms. Secondly, the interaction order of each feature is often independently learned, which lacks the flexibility to capture the feature dependencies in varying contexts. To address these issues, we present Rotative Factorization Machines (RFM), based on the key idea that represents each feature as a polar angle in the complex plane. As such, the feature interactions are converted into a series of complex rotations, where the orders are cast into the rotation coefficients, thereby allowing for the learning of arbitrarily large order. Further, we propose a novel self-attentive rotation function that models the rotation coefficients through a rotation-based attention mechanism, which can adaptively learn the interaction orders under different interaction contexts. Moreover, it incorporates a modulus amplification network to learn the modulus of the complex features, which further enhances the expressive capacity. Our proposed approach provides a general FIL framework, and many existing models can be instantiated in this framework, e.g., factorization machines. In theory, it possesses more strong capacities to model complex feature relationships, and can learn arbitrary features from varied contexts. Extensive experiments conducted on five widely used datasets have demonstrated the effectiveness of our approach.},
  archive   = {C_KDD},
  author    = {Tian, Zhen and Shi, Yuhong and Wu, Xiangkun and Zhao, Wayne Xin and Wen, Ji-Rong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671740},
  pages     = {2912–2923},
  title     = {Rotative factorization machines},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Latent diffusion-based data augmentation for continuous-time
dynamic graph model. <em>KDD</em>, 2900–2911. (<a
href="https://doi.org/10.1145/3637528.3671863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Continuous-Time Dynamic Graph (CTDG) precisely models evolving real-world relationships, drawing heightened interest in dynamic graph learning across academia and industry. However, existing CTDG models encounter challenges stemming from noise and limited historical data. Graph Data Augmentation (GDA) emerges as a critical solution, yet current approaches primarily focus on static graphs and struggle to effectively address the dynamics inherent in CTDGs. Moreover, these methods often demand substantial domain expertise for parameter tuning and lack theoretical guarantees for augmentation efficacy. To address these issues, we propose Conda, a novel latent diffusion-based GDA method tailored for CTDGs. Conda features a sandwich-like architecture, incorporating a Variational Auto-Encoder (VAE) and a conditional diffusion model, aimed at generating enhanced historical neighbor embeddings for target nodes. Unlike conventional diffusion models trained on entire graphs via pre-training, Conda requires historical neighbor sequence embeddings of target nodes for training, thus facilitating more targeted augmentation. We integrate Conda into the CTDG model and adopt an alternating training strategy to optimize performance. Extensive experimentation across six widely used real-world datasets showcases the consistent performance improvement of our approach, particularly in scenarios with limited historical data.},
  archive   = {C_KDD},
  author    = {Tian, Yuxing and Jiang, Aiwen and Huang, Qi and Guo, Jian and Qi, Yiyan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671863},
  pages     = {2900–2911},
  title     = {Latent diffusion-based data augmentation for continuous-time dynamic graph model},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). URRL-IMVC: Unified and robust representation learning for
incomplete multi-view clustering. <em>KDD</em>, 2888–2899. (<a
href="https://doi.org/10.1145/3637528.3671887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Incomplete multi-view clustering (IMVC) aims to cluster multi-view data that are only partially available. This poses two main challenges: effectively leveraging multi-view information and mitigating the impact of missing views. Prevailing solutions employ cross-view contrastive learning and missing view recovery techniques. However, they either neglect valuable complementary information by focusing only on consensus between views or provide unreliable recovered views due to the absence of supervision. To address these limitations, we propose a novel Unified and Robust Representation Learning for Incomplete Multi-View Clustering (URRL-IMVC). URRL-IMVC directly learns a unified embedding that is robust to view missing conditions by integrating information from multiple views and neighboring samples. Firstly, to overcome the limitations of cross-view contrastive learning, URRL-IMVC incorporates an attention-based auto-encoder framework to fuse multi-view information and generate unified embeddings. Secondly, URRL-IMVC directly enhances the robustness of the unified embedding against view-missing conditions through KNN imputation and data augmentation techniques, eliminating the need for explicit missing view recovery. Finally, incremental improvements are introduced to further enhance the overall performance, such as the Clustering Module and the customization of the Encoder. We extensively evaluate the proposed URRL-IMVC framework on various benchmark datasets, demonstrating its state-of-the-art performance. Furthermore, comprehensive ablation studies are performed to validate the effectiveness of our design.},
  archive   = {C_KDD},
  author    = {Teng, Ge and Mao, Ting and Shen, Chen and Tian, Xiang and Liu, Xuesong and Chen, Yaowu and Ye, Jieping},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671887},
  pages     = {2888–2899},
  title     = {URRL-IMVC: Unified and robust representation learning for incomplete multi-view clustering},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Causal estimation of exposure shifts with neural networks
and an application to inform air quality standards in the US.
<em>KDD</em>, 2876–2887. (<a
href="https://doi.org/10.1145/3637528.3671761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A fundamental task in causal inference is estimating the effect of a distribution shift in the treatment variable. We refer to this problem as shift-response function (SRF) estimation. Existing neural network methods for causal inference lack theoretical guarantees and practical implementations for SRF estimation. In this paper, we introduce Targeted Regularization for Exposure Shifts with Neural Networks (TRESNET), a method to estimate SRFs with robustness and efficiency guarantees. Our contributions are twofold. First, we propose a targeted regularization loss for neural networks with theoretical properties that ensure double robustness and asymptotic efficiency specific to SRF estimation. Second, we extend targeted regularization to support loss functions from the exponential family to accommodate non-continuous outcome distributions (e.g., discrete counts). We conduct benchmark experiments demonstrating TRESNET&#39;s broad applicability and competitiveness. We then apply our method to a key policy question in public health to estimate the causal effect of revising the US National Ambient Air Quality Standards (NAAQS) for PM 2.5 from 12 μg/m3 to 9 μg/m3. This change has been recently proposed by the US Environmental Protection Agency (EPA). Our goal is to estimate the reduction in deaths that would result from this anticipated revision using data consisting of 68 million individuals across the U.S.},
  archive   = {C_KDD},
  author    = {Tec, Mauricio and Josey, Kevin and Mudele, Oladimeji and Dominici, Francesca},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671761},
  pages     = {2876–2887},
  title     = {Causal estimation of exposure shifts with neural networks and an application to inform air quality standards in the US},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EcoVal: An efficient data valuation framework for machine
learning. <em>KDD</em>, 2866–2875. (<a
href="https://doi.org/10.1145/3637528.3672068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Quantifying the value of data within a machine learning workflow can play a pivotal role in making more strategic decisions in machine learning initiatives. The existing Shapley value based frameworks for data valuation in machine learning are computationally expensive as they require considerable amount of repeated training of the model to obtain the Shapley value. In this paper, we introduce an efficient data valuation framework EcoVal, to estimate the value of data for machine learning models in a fast and practical manner. Instead of directly working with individual data sample, we determine the value of a cluster of similar data points. This value is further propagated amongst all the member cluster points. We show that the overall value of the data can be determined by estimating the intrinsic and extrinsic value of each data. This is enabled by formulating the performance of a model as aproduction function, a concept which is popularly used to estimate the amount of output based on factors like labor and capital in a traditional free economic market. We provide a formal proof of our valuation technique and elucidate the principles and mechanisms that enable its accelerated performance. We demonstrate the real-world applicability of our method by showcasing its effectiveness for both in-distribution and out-of-sample data. This work addresses one of the core challenges of efficient data valuation at scale in machine learning models. The code is available at https://github.com/respai-lab/ecoval.},
  archive   = {C_KDD},
  author    = {Tarun, Ayush and Chundawat, Vikram and Mandal, Murari and Tan, Hong Ming and Chen, Bowei and Kankanhalli, Mohan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672068},
  pages     = {2866–2875},
  title     = {EcoVal: An efficient data valuation framework for machine learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards robust recommendation via decision boundary-aware
graph contrastive learning. <em>KDD</em>, 2854–2865. (<a
href="https://doi.org/10.1145/3637528.3671661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, graph contrastive learning (GCL) has received increasing attention in recommender systems due to its effectiveness in reducing bias caused by data sparsity. However, most existing GCL models rely on heuristic approaches and usually assume entity independence when constructing contrastive views. We argue that these methods struggle to strike a balance between semantic invariance and view hardness across the dynamic training process, both of which are critical factors in graph contrastive learning.To address the above issues, we propose a novel GCL-based recommendation framework RGCL, which effectively maintains the semantic invariance of contrastive pairs and dynamically adapts as the model capability evolves through the training process. Specifically, RGCL first introduces decision boundary-aware adversarial perturbations to constrain the exploration space of contrastive augmented views, avoiding the decrease of task-specific information. Furthermore, to incorporate global user-user and item-item collaboration relationships for guiding on the generation of hard contrastive views, we propose an adversarial-contrastive learning objective to construct a relation-aware view-generator. Besides, considering that unsupervised GCL could potentially narrower margins between data points and the decision boundary, resulting in decreased model robustness, we introduce the adversarial examples based on maximum perturbations to achieve margin maximization. We also provide theoretical analyses on the effectiveness of our designs. Through extensive experiments on five public datasets, we demonstrate the superiority of RGCL compared against twelve baseline models.},
  archive   = {C_KDD},
  author    = {Tang, Jiakai and Dai, Sunhao and Sun, Zexu and Chen, Xu and Xu, Jun and Yu, Wenhui and Hu, Lantao and Jiang, Peng and Li, Han},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671661},
  pages     = {2854–2865},
  title     = {Towards robust recommendation via decision boundary-aware graph contrastive learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HiGPT: Heterogeneous graph language model. <em>KDD</em>,
2842–2853. (<a href="https://doi.org/10.1145/3637528.3671987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Heterogeneous graph learning aims to capture complex relationships and diverse relational semantics among entities in a heterogeneous graph to obtain meaningful representations for nodes and edges. Recent advancements in heterogeneous graph neural networks (HGNNs) have achieved state-of-the-art performance by considering relation heterogeneity and using specialized message functions and aggregation rules. However, existing frameworks for heterogeneous graph learning have limitations in generalizing across diverse heterogeneous graph datasets. Most of these frameworks follow the &quot;pre-train&quot; and &quot;fine-tune&quot; paradigm on the same dataset, which restricts their capacity to adapt to new and unseen data. This raises the question: &quot;Can we generalize heterogeneous graph models to be well-adapted to diverse downstream learning tasks with distribution shifts in both node token sets and relation type heterogeneity?&quot; To tackle those challenges, we propose HiGPT, a general large graph model with &amp;lt;u&amp;gt;H&amp;lt;/u&amp;gt;eterogeneous graph &amp;lt;u&amp;gt;i&amp;lt;/u&amp;gt;nstruction-tuning paradigm. Our framework enables learning from arbitrary heterogeneous graphs without the need for any fine-tuning process from downstream datasets. To handle distribution shifts in heterogeneity, we introduce an in-context heterogeneous graph tokenizer that captures semantic relationships in different heterogeneous graphs, facilitating model adaptation. We incorporate a large corpus of heterogeneity-aware graph instructions into our HiGPT, enabling the model to effectively comprehend complex relation heterogeneity and distinguish between various types of graph tokens. Furthermore, we introduce the Mixture-of-Thought (MoT) instruction augmentation paradigm to mitigate data scarcity by generating diverse and informative instructions. Through comprehensive evaluations conducted in various settings, our proposed framework demonstrates exceptional performance in terms of generalization performance, surpassing current leading benchmarks. We make our model implementation openly available, along with comprehensive details at: https://github.com/HKUDS/HiGPT.},
  archive   = {C_KDD},
  author    = {Tang, Jiabin and Yang, Yuhao and Wei, Wei and Shi, Lei and Xia, Long and Yin, Dawei and Huang, Chao},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671987},
  pages     = {2842–2853},
  title     = {HiGPT: Heterogeneous graph language model},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning attributed graphlets: Predictive graph mining by
graphlets with trainable attribute. <em>KDD</em>, 2830–2841. (<a
href="https://doi.org/10.1145/3637528.3671970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The graph classification problem has been widely studied; however, achieving an interpretable model with high predictive performance remains a challenging issue. This paper proposes an interpretable classification algorithm for attributed graph data, called LAGRA (Learning Attributed GRAphlets). LAGRA learns importance weights for small attributed subgraphs, called attributed graphlets (AGs), while simultaneously optimizing their attribute vectors. This enables us to obtain a combination of subgraph structures and their attribute vectors that strongly contribute to discriminating different classes. A significant characteristics of LAGRA is that all the subgraph structures in the training dataset can be considered as a candidate structures of AGs. This approach can explore all the potentially important subgraphs exhaustively, but obviously, a na\&quot;{\i}ve implementation can require a large amount of computations. To mitigate this issue, we propose an efficient pruning strategy by combining the proximal gradient descent and a graph mining tree search. Our pruning strategy can ensure that the quality of the solution is maintained compared to the result without pruning. We empirically demonstrate that LAGRA has superior or comparable prediction performance to the standard existing algorithms including graph neural networks, while using only a small number of AGs in an interpretable manner.},
  archive   = {C_KDD},
  author    = {Tajima, Shinji and Sugihara, Ren and Kitahara, Ryota and Karasuyama, Masayuki},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671970},
  pages     = {2830–2841},
  title     = {Learning attributed graphlets: Predictive graph mining by graphlets with trainable attribute},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical linear symbolized tree-structured neural
processes. <em>KDD</em>, 2818–2829. (<a
href="https://doi.org/10.1145/3637528.3671861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traditional Neural Processes (NPs) and their variants aim to learn relationships between context sample points but do not consider multi-level information, resulting in a limited ability to learn complex distributions.This paper draws inspiration from features such as the hierarchical nature and interpretability of tree-like structures. This paper proposes a Hierarchical Linear Symbolized Tree-structured Neural Processes (HLNPs) architecture. This framework utilizes variables to build a top-down hierarchical linear symbolized tree-structured network architecture, enhancing positional representation information in a hierarchical manner along the deterministic path. In the latent distribution, the hierarchical linear symbolized tree-structured network approximates functions discretely through a layered approach. By decomposing the latent complex distribution into several simpler sub-problems using sum and product symbols, the upper bound of optimization is thereby increased. The tree structure discretizes variables to capture model uncertainty in the form of entropy. This approach also imparts a causal effect to the HLNPs model. Finally, we demonstrate the effectiveness of the HLNPs models for 1D data, Bayesian optimization, and 2D data.},
  archive   = {C_KDD},
  author    = {Tai, Jin yang and Guo, Yi ke},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671861},
  pages     = {2818–2829},
  title     = {Hierarchical linear symbolized tree-structured neural processes},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised denoising through independent cascade graph
augmentation for robust social recommendation. <em>KDD</em>, 2806–2817.
(<a href="https://doi.org/10.1145/3637528.3671958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Social Recommendation (SR) typically exploits neighborhood influence in the social network to enhance user preference modeling. However, users&#39; intricate social behaviors may introduce noisy social connections for user modeling and harm the models&#39; robustness. Existing solutions to alleviate social noise either filter out the noisy connections or generate new potential social connections. Due to the absence of labels, the former approaches may retain uncertain connections for user preference modeling while the latter methods may introduce additional social noise. Through data analysis, we discover that (1) social noise likely comes from the connected users with low preference similarity; and (2) Opinion Leaders (OLs) play a pivotal role in influence dissemination, surpassing high-similarity neighbors, regardless of their preference similarity with trusting peers. Guided by these observations, we propose a novel Self-Supervised Denoising approach through Independent Cascade Graph Augmentation, for more robust SR. Specifically, we employ the independent cascade diffusion model to generate an augmented graph view, which traverses the social graph and activates the edges in sequence to simulate the cascading influence spread. To steer the augmentation towards a denoised social graph, we (1) introduce a hierarchical contrastive loss to prioritize the activation of OLs first, followed by high-similarity neighbors, while weakening the low-similarity neighbors; and (2) integrate an information bottleneck based contrastive loss, aiming to minimize mutual information between original and augmented graphs yet preserve sufficient information for improved SR. Experiments conducted on two public datasets demonstrate that our model outperforms the state-of-the-art while also exhibiting higher robustness to different extents of social noise.},
  archive   = {C_KDD},
  author    = {Sun, Youchen and Sun, Zhu and Du, Yingpeng and Zhang, Jie and Ong, Yew Soon},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671958},
  pages     = {2806–2817},
  title     = {Self-supervised denoising through independent cascade graph augmentation for robust social recommendation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DIVE: Subgraph disagreement for graph out-of-distribution
generalization. <em>KDD</em>, 2794–2805. (<a
href="https://doi.org/10.1145/3637528.3671878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the challenge of out-of-distribution (OOD) generalization in graph machine learning, a field rapidly advancing yet grappling with the discrepancy between source and target data distributions. Traditional graph learning algorithms, based on the assumption of uniform distribution between training and test data, falter in real-world scenarios where this assumption fails, resulting in suboptimal performance. A principal factor contributing to this suboptimal performance is the inherent simplicity bias of neural networks trained through Stochastic Gradient Descent (SGD), which prefer simpler features over more complex yet equally or more predictive ones. This bias leads to a reliance on spurious correlations, adversely affecting OOD performance in various tasks such as image recognition, natural language understanding, and graph classification. Current methodologies, including subgraph-mixup and information bottleneck approaches, have achieved partial success but struggle to overcome simplicity bias, often reinforcing spurious correlations. To tackle this, our study introduces a new learning paradigm for graph OOD issue. We propose DIVE, training a collection of models to focus on all label-predictive subgraphs by encouraging the models to foster divergence on the subgraph mask, which circumvents the limitation of a model solely focusing on the subgraph corresponding to simple structural patterns. Specifically, we employs a regularizer to punish overlap in extracted subgraphs across models, thereby encouraging different models to concentrate on distinct structural patterns. Model selection for robust OOD performance is achieved through validation accuracy. Tested across four datasets from GOOD benchmark and one dataset from DrugOOD benchmark, our approach demonstrates significant improvement over existing methods, effectively addressing the simplicity bias and enhancing generalization in graph machine learning.},
  archive   = {C_KDD},
  author    = {Sun, Xin and Wang, Liang and Liu, Qiang and Wu, Shu and Wang, Zilei and Wang, Liang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671878},
  pages     = {2794–2805},
  title     = {DIVE: Subgraph disagreement for graph out-of-distribution generalization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Going where, by whom, and at what time: Next location
prediction considering user preference and temporal regularity.
<em>KDD</em>, 2784–2793. (<a
href="https://doi.org/10.1145/3637528.3671916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Next location prediction is a crucial task in human mobility modeling, and is pivotal for many downstream applications like location-based recommendation and transportation planning. Although there has been a large body of research tackling this problem, the usefulness of user preference and temporal regularity remains underrepresented. Specifically, previous studies usually neglect the explicit user preference information entailed from human trajectories and fall short in utilizing the arrival time of next location, as a key determinant on next location. To address these limitations, we propose a Multi-Context aware Location Prediction model (MCLP) to predict next locations for individuals, where it explicitly models user preference and the next arrival time as context. First, we utilize a topic model to extract user preferences for different types of locations from historical human trajectories. Second, we develop an arrival time estimator to construct a robust arrival time embedding based on the multi-head attention mechanism. The two components provide pivotal contextual information for the subsequent prediction. Finally, we utilize the Transformer architecture to mine sequential patterns and integrate multiple contextual information to predict the next locations. Experimental results on two real-world mobility datasets show that our proposed MCLP outperforms baseline methods.},
  archive   = {C_KDD},
  author    = {Sun, Tianao and Fu, Ke and Huang, Weiming and Zhao, Kai and Gong, Yongshun and Chen, Meng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671916},
  pages     = {2784–2793},
  title     = {Going where, by whom, and at what time: Next location prediction considering user preference and temporal regularity},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Dual-assessment driven pruning: Iterative optimizing
layer-wise sparsity for large language model. <em>KDD</em>, 2775–2783.
(<a href="https://doi.org/10.1145/3637528.3671780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large Language Models (LLMs) have demonstrated efficacy in various domains, but deploying these models is economically challenging due to extensive parameter counts. Numerous efforts have been dedicated to reducing the parameter count of these models without compromising performance, employing a technique known as model pruning. Conventional pruning methods assess the significance of weights within individual layers and typically apply uniform sparsity levels across all layers, potentially neglecting the varying significance of each layer. To address this oversight, we first propose a dual-assessment driven pruning strategy that employs both intra-layer metric and global performance metric to comprehensively evaluate the impact of pruning. Then our method leverages an iterative optimization algorithm to find the optimal layer-wise sparsity distribution, thereby minimally impacting model performance. Extensive benchmark evaluations on state-of-the-art LLM architectures such as LLaMAv2 and OPT across a variety of NLP tasks demonstrate the effectiveness of our approach. When applied to the LLaMaV2-7B model with an overall pruning sparsity of 80\%, our method achieves a 50\% reduction in perplexity compared to the benchmark. The results indicate that our method significantly outperforms existing state-of-the-art methods in preserving performance after pruning.},
  archive   = {C_KDD},
  author    = {Sun, Qinghui and Wang, Weilun and Zhu, Yanni and He, Shenghuan and Yi, Hao and Cai, Zehua and Liu, Hong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671780},
  pages     = {2775–2783},
  title     = {Dual-assessment driven pruning: Iterative optimizing layer-wise sparsity for large language model},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CrossLight: Offline-to-online reinforcement learning for
cross-city traffic signal control. <em>KDD</em>, 2765–2774. (<a
href="https://doi.org/10.1145/3637528.3671927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The recent advancements in Traffic Signal Control (TSC) have highlighted the potential of Reinforcement Learning (RL) as a promising solution to alleviate traffic congestion. Current research in this area primarily concentrates on either online or offline learning strategies, aiming to create optimized policies for specific cities. Nevertheless, the transferability of these policies to new cities is impeded by constraints such as the limited availability of high-quality data and the expensive and risky exploration process. To this end, in this paper, we present an innovative cross-city Traffic Signal Control (TSC) paradigm called CrossLight. Our approach involves meta training using offline data from source cities and adaptively fine-tuning in the target city. This novel methodology aims to address the challenges of transferring TSC policies across different cities effectively. In our proposed approach, we start by acquiring meta-decision pattern knowledge through trajectory dynamics reconstruction via pre-training in source cities. To address disparities in road network topologies between cities, we dynamically construct city topological structures based on the extracted meta-knowledge during the offline meta-training phase. These structures are then used to distill pattern-structure aware representations of decision trajectories from the source cities. To identify effective initial parameters for the learnable components, we employ the Model-Agnostic Meta-Learning (MAML) framework, a popular meta-learning approach. During adaptive fine-tuning in the target city, we introduce a replay buffer that is iteratively updated using online interactions with a rank and filter mechanism. This mechanism, along with a carefully designed exploration strategy, ensures a balance between exploitation and exploration, thereby fostering both the diversity and quality of the trajectories for fine-tuning. Finally, extensive experiments across four cities validate that CrossLight achieves comparable performance in new cities with minimal fine-tuning iterations, surpassing both existing online and offline methods. This success underscores that our CrossLight framework emerges as a groundbreaking and potent paradigm, offering a feasible and effective solution to the intelligent transportation community.},
  archive   = {C_KDD},
  author    = {Sun, Qian and Zha, Rui and Zhang, Le and Zhou, Jingbo and Mei, Yu and Li, Zhiling and Xiong, Hui},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671927},
  pages     = {2765–2774},
  title     = {CrossLight: Offline-to-online reinforcement learning for cross-city traffic signal control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast computation for the forest matrix of an evolving graph.
<em>KDD</em>, 2755–2764. (<a
href="https://doi.org/10.1145/3637528.3671822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The forest matrix plays a crucial role in network science, opinion dynamics, and machine learning, offering deep insights into the structure of and dynamics on networks. In this paper, we study the problem of querying entries of the forest matrix in evolving graphs, which more accurately represent the dynamic nature of real-world networks compared to static graphs. To address the unique challenges posed by evolving graphs, we first introduce two approximation algorithms, SFQ and SFQPlus, for static graphs. SFQ employs a probabilistic interpretation of the forest matrix, while SFQPlus incorporates a novel variance reduction technique and is theoretically proven to offer enhanced accuracy. Based on these two algorithms, we further devise two dynamic algorithms centered around efficiently maintaining a list of spanning converging forests. This approach ensures O(1) runtime complexity for updates, including edge additions and deletions, as well as for querying matrix elements, and provides an unbiased estimation of forest matrix entries. Finally, through extensive experiments on various real-world networks, we demonstrate the efficiency and effectiveness of our algorithms. Particularly, our algorithms are scalable to massive graphs with more than forty million nodes.},
  archive   = {C_KDD},
  author    = {Sun, Haoxin and Zhou, Xiaotian and Zhang, Zhongzhi},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671822},
  pages     = {2755–2764},
  title     = {Fast computation for the forest matrix of an evolving graph},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mitigating negative transfer in cross-domain recommendation
via knowledge transferability enhancement. <em>KDD</em>, 2745–2754. (<a
href="https://doi.org/10.1145/3637528.3671799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cross-Domain Recommendation (CDR) is a promising technique to alleviate data sparsity by transferring knowledge across domains. However, the negative transfer issue in the presence of numerous domains has received limited attention. Most existing methods transfer all information from source domains to the target domain without distinction. This introduces harmful noise and irrelevant features, resulting in suboptimal performance. Although some methods decompose user features into domain-specific and domain-shared components, they fail to consider other causes of negative transfer. Worse still, we argue that simple feature decomposition is insufficient for multi-domain scenarios. To bridge this gap, we propose TrineCDR, the TRIple-level kNowledge transferability Enhanced model for multi-target CDR. Unlike previous methods, TrineCDR captures single domain and targeted cross-domain embeddings to serve multi-domain recommendation. For the latter, we identify three fundamental causes of negative transfer, ranging from micro to macro perspectives, and correspondingly enhance knowledge transferability at three different levels: the feature level, the interaction level, and the domain level. Through these efforts, TrineCDR effectively filters out noise and irrelevant information from source domains, leading to more comprehensive and accurate representations in the target domain. We extensively evaluate the proposed model on real-world datasets, sampled from Amazon and Douban, under both dual-target and multi-target scenarios. The experimental results demonstrate the superiority of TrineCDR over state-of-the-art cross-domain recommendation methods.},
  archive   = {C_KDD},
  author    = {Song, Zijian and Zhang, Wenhan and Deng, Lifang and Zhang, Jiandong and Wu, Zhihua and Bian, Kaigui and Cui, Bin},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671799},
  pages     = {2745–2754},
  title     = {Mitigating negative transfer in cross-domain recommendation via knowledge transferability enhancement},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Marrying dialogue systems with data visualization:
Interactive data visualization generation from natural language
conversations. <em>KDD</em>, 2733–2744. (<a
href="https://doi.org/10.1145/3637528.3671935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Data visualization (DV) has become the prevailing tool in the market due to its effectiveness into illustrating insights in vast amounts of data. To lower the barrier of using DVs, automatic DV tasks, such as natural language question (NLQ) to visualization translation (formally called text-to-vis), have been investigated in the research community. However, text-to-vis assumes the NLQ to be well-organized and expressed in a single sentence. However, in real-world settings, complex DV is needed through consecutive exchanges between the DV system and the users. In this paper, we propose a new task named CoVis, short for &amp;lt;u&amp;gt;Co&amp;lt;/u&amp;gt;nversational text-to-&amp;lt;u&amp;gt;Vis&amp;lt;/u&amp;gt;ualization, aiming at constructing DVs through a series of interactions between users and the system. Since it is the task which has not been studied in the literature, we first build a benchmark dataset named Dial-NVBench, including dialogue sessions with a sequence of queries from a user and responses from the system. The ultimate goal of each dialogue session is to create a suitable DV. However, this process can contain diverse dialogue queries, such as seeking information about the dataset, manipulating parts of the data, and visualizing the data. Then, we propose a multi-modal neural network named MMCoVisNet to answer these DV-related queries. In particular, MMCoVisNet first fully understands the dialogue context and determines the corresponding responses. Then, it uses adaptive decoders to provide the appropriate replies: (i) a straightforward text decoder is used to produce general responses, (ii) an SQL-form decoder is applied to synthesize data querying responses, and (iii) a DV-form decoder tries to construct the appropriate DVs. We comparatively evaluate MMCoVisNet with other baselines over our proposed benchmark dataset. Experimental results validate that MMCoVisNet performs better than existing baselines and achieves a state-of-the-art performance.},
  archive   = {C_KDD},
  author    = {Song, Yuanfeng and Zhao, Xuefang and Wong, Raymond Chi-Wing},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671935},
  pages     = {2733–2744},
  title     = {Marrying dialogue systems with data visualization: Interactive data visualization generation from natural language conversations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On early detection of hallucinations in factual question
answering. <em>KDD</em>, 2721–2732. (<a
href="https://doi.org/10.1145/3637528.3671796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While large language models (LLMs) have taken great strides towards helping humans with a plethora of tasks, hallucinations remain a major impediment towards gaining user trust. The fluency and coherence of model generations even when hallucinating makes detection a difficult task. In this work, we explore if the artifacts associated with the model generations can provide hints that the generation will contain hallucinations. Specifically, we probe LLMs at 1) the inputs via Integrated Gradients based token attribution, 2) the outputs via the Softmax probabilities, and 3) the internal state via self-attention and fully-connected layer activations for signs of hallucinations on open-ended question answering tasks. Our results show that the distributions of these artifacts tend to differ between hallucinated and non-hallucinated generations. Building on this insight, we train binary classifiers that use these artifacts as input features to classify model generations into hallucinations and non-hallucinations. These hallucination classifiers achieve up to 0.80 AUROC. We also show that tokens preceding a hallucination can already predict the subsequent hallucination even before it occurs.},
  archive   = {C_KDD},
  author    = {Snyder, Ben and Moisescu, Marius and Zafar, Muhammad Bilal},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671796},
  pages     = {2721–2732},
  title     = {On early detection of hallucinations in factual question answering},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MAML-en-LLM: Model agnostic meta-training of LLMs for
improved in-context learning. <em>KDD</em>, 2711–2720. (<a
href="https://doi.org/10.1145/3637528.3671905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Adapting large language models (LLMs) to unseen tasks with incontext training samples without fine-tuning remains an important research problem. To learn a robust LLM that adapts well to unseen tasks, multiple meta-training approaches have been proposed such as MetaICL and MetaICT, which involve meta-training pre-trained LLMs on a wide variety of diverse tasks. These meta-training approaches essentially perform in-context multi-task fine-tuning and evaluate on a disjointed test set of tasks. Even though they achieve impressive performance, their goal is never to compute a truly general set of parameters. In this paper, we propose MAML-en-LLM, a novel method for meta-training LLMs, which can learn truly generalizable parameters that not only performs well on disjointed tasks but also adapts to unseen tasks. We see an average increase of 2\% on unseen domains in the performance while a massive 4\% improvement on adaptation performance. Furthermore, we demonstrate that MAML-en-LLM outperforms baselines in settings with limited amount of training data on both seen and unseen domains by an average of 2\%. Finally, we discuss the effects of type of tasks, optimizers and task complexity, an avenue barely explored in metatraining literature. Exhaustive experiments across 7 task settings along with two data settings demonstrate that models trained with MAML-en-LLM outperform SOTA meta-training approaches.},
  archive   = {C_KDD},
  author    = {Sinha, Sanchit and Yue, Yuguang and Soto, Victor and Kulkarni, Mayank and Lu, Jianhua and Zhang, Aidong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671905},
  pages     = {2711–2720},
  title     = {MAML-en-LLM: Model agnostic meta-training of LLMs for improved in-context learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CoLiDR: Concept learning using aggregated disentangled
representations. <em>KDD</em>, 2699–2710. (<a
href="https://doi.org/10.1145/3637528.3671938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Interpretability of Deep Neural Networks using concept-based models offers a promising way to explain model behavior through human understandable concepts. A parallel line of research focuses on disentangling the data distribution into its underlying generative factors, in turn explaining the data generation process. While both directions have received extensive attention, little work has been done on explaining concepts in terms of generative factors to unify mathematically disentangled representations and human-understandable concepts as an explanation for downstream tasks. In this paper, we propose a novel method CoLiDR - which utilizes a disentangled representation learning setup for learning mutually independent generative factors and subsequently learns to aggregate the said representations into human-understandable concepts using a novel aggregation/decomposition module. Experiments are conducted on datasets with both known and unknown latent generative factors. Our method successfully aggregates disentangled generative factors into concepts while maintaining parity with state-of-the-art concept-based approaches. Quantitative and visual analysis of the learned aggregation procedure demonstrates the advantages of our work compared to commonly used concept-based models over four challenging datasets. Lastly, our work is generalizable to an arbitrary number of concepts and generative factors - making it flexible enough to be suitable for various types of data.},
  archive   = {C_KDD},
  author    = {Sinha, Sanchit and Xiong, Guangzhi and Zhang, Aidong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671938},
  pages     = {2699–2710},
  title     = {CoLiDR: Concept learning using aggregated disentangled representations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LPFormer: An adaptive graph transformer for link prediction.
<em>KDD</em>, 2686–2698. (<a
href="https://doi.org/10.1145/3637528.3672025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Link prediction is a common task on graph-structured data that has seen applications in a variety of domains. Classically, hand-crafted heuristics were used for this task. Heuristic measures are chosen such that they correlate well with the underlying factors related to link formation. In recent years, a new class of methods has emerged that combines the advantages of message-passing neural networks (MPNN) and heuristics methods. These methods perform predictions by using the output of an MPNN in conjunction with a &quot;pairwise encoding&quot; that captures the relationship between nodes in the candidate link. They have been shown to achieve strong performance on numerous datasets. However, current pairwise encodings often contain a strong inductive bias, using the same underlying factors to classify all links. This limits the ability of existing methods to learn how to properly classify a variety of different links that may form from different factors. To address this limitation, we propose a new method, LPFormer, which attempts to adaptively learn the pairwise encodings for each link. LPFormer models the link factors via an attention module that learns the pairwise encoding that exists between nodes by modeling multiple factors integral to link prediction. Extensive experiments demonstrate that LPFormer can achieve SOTA performance on numerous datasets while maintaining efficiency. The code is available at The code is available at https://github.com/HarryShomer/LPFormer.},
  archive   = {C_KDD},
  author    = {Shomer, Harry and Ma, Yao and Mao, Haitao and Li, Juanhui and Wu, Bo and Tang, Jiliang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672025},
  pages     = {2686–2698},
  title     = {LPFormer: An adaptive graph transformer for link prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Orthogonality matters: Invariant time series representation
for out-of-distribution classification. <em>KDD</em>, 2674–2685. (<a
href="https://doi.org/10.1145/3637528.3671768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Previous works for time series classification tend to assume that both the training and testing sets originate from the same distribution. This oversimplification deviates from the complexity of reality and makes it challenging to generalize methods to out-of-distribution (OOD) time series data. Currently, there are limited works focusing on time series OOD generalization, and they typically disentangle time series into domain-agnostic and domain-specific features and design tasks to intensify the distinction between the two. However, previous models purportedly yielding domain-agnostic features continue to harbor domain-specific information, thereby diminishing their adaptability to OOD data. To address this gap, we introduce a novel model called Invariant Time Series Representation (ITSR). ITSR achieves a learnable orthogonal decomposition of time series using two sets of orthogonal axes. In detail, ITSR projects time series onto these two sets of axes separately and obtains mutually orthogonal invariant features and relevant features. ITSR theoretically ensures low similarity between these two features and further incorporates various tasks to optimize them. Furthermore, we explore the benefits of preserving orthogonality between invariant and relevant features for OOD time series classification in theory. The results on four real-world datasets underscore the superiority of ITSR over state-of-the-art methods and demonstrate the critical role of maintaining orthogonality between invariant and relevant features. Our code is available at https://github.com/CGCL-codes/ITSR.},
  archive   = {C_KDD},
  author    = {Shi, Ruize and Huang, Hong and Yin, Kehan and Zhou, Wei and Jin, Hai},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671768},
  pages     = {2674–2685},
  title     = {Orthogonality matters: Invariant time series representation for out-of-distribution classification},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient and long-tailed generalization for pre-trained
vision-language model. <em>KDD</em>, 2663–2673. (<a
href="https://doi.org/10.1145/3637528.3671945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pre-trained vision-language models like CLIP have shown powerful zero-shot inference ability via image-text matching and prove to be strong few-shot learners in various downstream tasks. However, in real-world scenarios, adapting CLIP to downstream tasks may encounter the following challenges: 1) data may exhibit long-tailed data distributions and might not have abundant samples for all the classes; 2) There might be emerging tasks with new classes that contain no samples at all. To overcome them, we propose a novel framework to achieve efficient and long-tailed generalization, which can be termed as Candle. During the training process, we propose compensating logit-adjusted loss to encourage large margins of prototypes and alleviate imbalance both within the base classes and between the base and new classes. For efficient adaptation, we treat the CLIP model as a black box and leverage the extracted features to obtain visual and textual prototypes for prediction. To make full use of multi-modal information, we also propose cross-modal attention to enrich the features from both modalities. For effective generalization, we introduce virtual prototypes for new classes to make up for their lack of training images. Candle achieves state-of-the-art performance over extensive experiments on 11 diverse datasets while substantially reducing the training time, demonstrating the superiority of our approach. The source code is available at https://github.com/shijxcs/Candle.},
  archive   = {C_KDD},
  author    = {Shi, Jiang-Xin and Zhang, Chi and Wei, Tong and Li, Yu-Feng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671945},
  pages     = {2663–2673},
  title     = {Efficient and long-tailed generalization for pre-trained vision-language model},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MSPipe: Efficient temporal GNN training via staleness-aware
pipeline. <em>KDD</em>, 2651–2662. (<a
href="https://doi.org/10.1145/3637528.3671844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Memory-based Temporal Graph Neural Networks (MTGNNs) are a class of temporal graph neural networks that utilize a node memory module to capture and retain long-term temporal dependencies, leading to superior performance compared to memory-less counterparts. However, the iterative reading and updating process of the memory module in MTGNNs to obtain up-to-date information needs to follow the temporal dependencies. This introduces significant overhead and limits training throughput. Existing optimizations for static GNNs are not directly applicable to MTGNNs due to differences in training paradigm, model architecture, and the absence of a memory module. Moreover, these optimizations do not effectively address the challenges posed by temporal dependencies, making them ineffective for MTGNN training. In this paper, we propose MSPipe, a general and efficient framework for memory-based TGNNs that maximizes training throughput while maintaining model accuracy. Our design specifically addresses the unique challenges associated with fetching and updating node memory states in MTGNNs by integrating staleness into the memory module. However, simply introducing a predefined staleness bound in the memory module to break temporal dependencies may lead to suboptimal performance and lack of generalizability across different models and datasets. To overcome this, we introduce an online pipeline scheduling algorithm in MSPipe that strategically breaks temporal dependencies with minimal staleness and delays memory fetching to obtain fresher memory states. This is achieved without stalling the MTGNN training stage or causing resource contention. Additionally, we design a staleness mitigation mechanism to enhance training convergence and model accuracy. Furthermore, we provide convergence analysis and demonstrate that MSPipe maintains the same convergence rate as vanilla sampling-based GNN training. Experimental results show that MSPipe achieves up to 2.45\texttimes{} speed-up without sacrificing accuracy, making it a promising solution for efficient MTGNN training. The implementation of our paper can be found at the following link: https://github.com/PeterSH6/MSPipe.},
  archive   = {C_KDD},
  author    = {Sheng, Guangming and Su, Junwei and Huang, Chao and Wu, Chuan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671844},
  pages     = {2651–2662},
  title     = {MSPipe: Efficient temporal GNN training via staleness-aware pipeline},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing OOD detection in molecular graphs: A novel
approach with diffusion models. <em>KDD</em>, 2640–2650. (<a
href="https://doi.org/10.1145/3637528.3671785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite the recent progress of molecular representation learning, its effectiveness is assumed on the close-world assumptions that training and testing graphs are from identical distribution. The open-world test dataset is often mixed with out-of-distribution (OOD) samples, where the deployed models will struggle to make accurate predictions. The misleading estimations of molecules&#39; properties in drug screening or design can result in the tremendous waste of wet-lab resources and delay the discovery of novel therapies. Traditional detection methods need to trade off OOD detection and in-distribution (ID) classification performance since they share the same representation learning model. In this work, we propose to detect OOD molecules by adopting an auxiliary diffusion model-based framework, which compares similarities between input molecules and reconstructed graphs. Due to the generative bias towards reconstructing ID training samples, the similarity scores of OOD molecules will be much lower to facilitate detection. Although it is conceptually simple, extending this vanilla framework to practical detection applications is still limited by two significant challenges. First, the popular similarity metrics based on Euclidian distance fail to consider the complex graph structure. Second, the generative model involving iterative denoising steps is notoriously time-consuming especially when it runs on the enormous pool of drugs. To address these challenges, our research pioneers an approach of Prototypical Graph Reconstruction for Molecular OOd Detection, dubbed as PGR-MOOD. Specifically, PGR-MOOD hinges on three innovations: i) An effective metric to comprehensively quantify the matching degree of input and reconstructed molecules according to their discrete edges and continuous node features; ii) A creative graph generator to construct a list of prototypical graphs that are in line with ID distribution but away from OOD one; iii) An efficient and scalable OOD detector to compare the similarity between test samples and pre-constructed prototypical graphs and omit the generative process on every new molecule. Extensive experiments on ten benchmark datasets and six baselines are conducted to demonstrate our superiority: PGR-MOOD achieves more than 8\% of average improvement in terms of detection AUC and AUPR accompanied by the reduced cost of testing time and memory consumption. The anonymous code is in: https://github.com/se7esx/PGR-MOOD.},
  archive   = {C_KDD},
  author    = {Shen, Xu and Wang, Yili and Zhou, Kaixiong and Pan, Shirui and Wang, Xin},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671785},
  pages     = {2640–2650},
  title     = {Optimizing OOD detection in molecular graphs: A novel approach with diffusion models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Capturing homogeneous influence among students: Hypergraph
cognitive diagnosis for intelligent education systems. <em>KDD</em>,
2628–2639. (<a href="https://doi.org/10.1145/3637528.3672002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cognitive diagnosis is a vital upstream task in intelligent education systems. It models the student-exercise interaction, aiming to infer the students&#39; proficiency levels on each knowledge concept. This paper observes that most existing methods can hardly effectively capture the homogeneous influence due to its inherent complexity. That is to say, although students exhibit similar performance on given exercises, their proficiency levels inferred by these methods vary significantly, resulting in shortcomings in interpretability and efficacy. Given the complexity of homogeneous influence, a hypergraph could be a choice due to its flexibility and capability of modeling high-order similarity which aligns with the nature of homogeneous influence. However, before incorporating hypergraph, one at first needs to address the challenges of distorted homogeneous influence, sparsity of response logs, and over-smoothing. To this end, this paper proposes a hypergraph cognitive diagnosis model (HyperCDM) to address these challenges and effectively capture the homogeneous influence. Specifically, to avoid distortion, HyperCDM employs a divide-and-conquer strategy to learn student, exercise and knowledge representations in their own hypergraphs respectively, and interconnects them via a feature-based interaction function. To construct hypergraphs based on sparse response logs, the auto-encoder is utilized to preprocess response logs and K-means is applied to cluster students. To mitigate over-smoothing, momentum hypergraph convolution networks are designed to partially keep previous representations during the message propagation. Extensive experiments on both offline and online real-world datasets show that HyperCDM achieves state-of-the-art performance in terms of interpretability and capturing homogeneous influence effectively, and is competitive in generalization. The ablation study verifies the efficacy of each component, and the case study explicitly showcases the homogeneous influence captured by HyperCDM.},
  archive   = {C_KDD},
  author    = {Shen, Junhao and Qian, Hong and Liu, Shuo and Zhang, Wei and Jiang, Bo and Zhou, Aimin},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672002},
  pages     = {2628–2639},
  title     = {Capturing homogeneous influence among students: Hypergraph cognitive diagnosis for intelligent education systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). STEMO: Early spatio-temporal forecasting with
multi-objective reinforcement learning. <em>KDD</em>, 2618–2627. (<a
href="https://doi.org/10.1145/3637528.3671922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accuracy and timeliness are indeed often conflicting goals in prediction tasks. Premature predictions may yield a higher rate of false alarms, whereas delaying predictions to gather more information can render them too late to be useful. In applications such as wildfires, crimes, and traffic jams, timely forecasting are vital for safeguarding human life and property. Consequently, finding a balance between accuracy and timeliness is crucial. In this paper, we propose an early spatio-temporal forecasting model based on Multi-Objective reinforcement learning that can either implement an optimal policy given a preference or infer the preference based on a small number of samples. The model addresses two primary challenges: 1) enhancing the accuracy of early forecasting and 2) providing the optimal policy for determining the most suitable prediction time for each area. Our method demonstrates superior performance on three large-scale real-world datasets, surpassing existing methods in early spatio-temporal forecasting tasks.},
  archive   = {C_KDD},
  author    = {Shao, Wei and Kang, Yufan and Peng, Ziyan and Xiao, Xiao and Wang, Lei and Yang, Yuhui and Salim, Flora D.},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671922},
  pages     = {2618–2627},
  title     = {STEMO: Early spatio-temporal forecasting with multi-objective reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Offline imitation learning with model-based reverse
augmentation. <em>KDD</em>, 2608–2617. (<a
href="https://doi.org/10.1145/3637528.3672059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In offline Imitation Learning (IL), one of the main challenges is the covariate shift between the expert observations and the actual distribution encountered by the agent, because it is difficult to determine what action an agent should take when outside the state distribution of the expert demonstrations. Recently, the model-free solutions introduced supplementary data and identified the latent expert-similar samples to augment the reliable samples during learning. Model-based solutions build forward dynamic models with conservatism quantification and then generate additional trajectories in the neighborhood of expert demonstrations. However, without reward supervision, these methods are often over-conservative in the out-of-expert-support regions, because only in states close to expert-observed states can there be a preferred action enabling policy optimization. To encourage more exploration on expert-unobserved states, we propose a novel model-based framework, called offline Imitation Learning with Self-paced Reverse Augmentation (SRA). Specifically, we build a reverse dynamic model from the offline demonstrations, which can efficiently generate trajectories leading to the expert-observed states in a self-paced style. Then, we use the subsequent reinforcement learning method to learn from the augmented trajectories and transit from expert-unobserved states to expert-observed states. This framework not only explores the expert-unobserved states but also guides maximizing long-term returns on these states, ultimately enabling generalization beyond the expert data. Empirical results show that our proposal could effectively mitigate the covariate shift and achieve the state-of-the-art performance on the offline imitation learning benchmarks. Project website: https://www.lamda.nju.edu.cn/shaojj/KDD24_SRA/.},
  archive   = {C_KDD},
  author    = {Shao, Jie-Jing and Shi, Hao-Sen and Guo, Lan-Zhe and Li, Yu-Feng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672059},
  pages     = {2608–2617},
  title     = {Offline imitation learning with model-based reverse augmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Certified robustness on visual graph matching via searching
optimal smoothing range. <em>KDD</em>, 2596–2607. (<a
href="https://doi.org/10.1145/3637528.3671852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep visual graph matching (GM) is a challenging combinatorial task that involves finding a permutation matrix that indicates the correspondence between keypoints from a pair of images. Like many learning systems, empirical studies have shown that visual GM is susceptible to adversarial attacks, with reliability issues in downstream applications. To the best of our knowledge, certifying robustness for deep visual GM remains an open challenge with two main difficulties: how to handle the paired inputs together with the heavily non-linear permutation output space (especially at large scale), and how to balance the trade-off between certified robustness and matching performance. Inspired by the randomized smoothing (RS) technique, we propose the Certified Robustness based on the Optimal Smoothing Range Search (CR-OSRS) technique to fulfill the robustness guarantee for deep visual GM. First, unlike conventional RS methods that use isotropic Gaussian distributions for smoothing, we build the smoothed model with paired joint Gaussian distributions, which capture the structural information among keypoints, and mitigate the performance degradation caused by smoothing. For the vast space of the permutation output, we devise a similarity-based partitioning method that can lower the computational complexity and certification difficulty. We then derive a stringent robustness guarantee that links the certified space of inputs to their corresponding fixed outputs. Second, we design a global optimization method to search for optimal joint Gaussian distributions and facilitate a larger certified space and better performance. Third, we apply data augmentation and a similarity-based regularizer in training to enhance smoothed model performance. Lastly, for the high-dimensional and multivariable nature of the certified space, we propose two methods (sampling and marginal radii) to evaluate it. Experimental results on public benchmarks show that our method achieves state-of-the-art certified robustness.},
  archive   = {C_KDD},
  author    = {Shao, Huaqing and Wang, Lanjun and Wang, Yongwei and Ren, Qibing and Yan, Junchi},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671852},
  pages     = {2596–2607},
  title     = {Certified robustness on visual graph matching via searching optimal smoothing range},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NeuroCut: A neural approach for robust graph partitioning.
<em>KDD</em>, 2584–2595. (<a
href="https://doi.org/10.1145/3637528.3671815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph partitioning aims to divide a graph into k disjoint subsets while optimizing a specific partitioning objective. The majority of formulations related to graph partitioning exhibit NP-hardness due to their combinatorial nature. Conventional methods, like approximation algorithms or heuristics, are designed for distinct partitioning objectives and fail to achieve generalization across other important partitioning objectives. Recently machine learning-based methods have been developed that learn directly from data. Further, these methods have a distinct advantage of utilizing node features that carry additional information. However, these methods assume differentiability of target partitioning objective functions and cannot generalize for an unknown number of partitions, i.e., they assume the number of partitions is provided in advance. In this study, we develop NeuroCut with two key innovations over previous methodologies. First, by leveraging a reinforcement learning-based framework over node representations derived from a graph neural network and positional features, NeuroCut can accommodate any optimization objective, even those with non-differentiable functions. Second, we decouple the parameter space and the partition count making NeuroCut inductive to any unseen number of partition, which is provided at query time. Through empirical evaluation, we demonstrate that NeuroCut excels in identifying high-quality partitions, showcases strong generalization across a wide spectrum of partitioning objectives, and exhibits strong generalization to unseen partition count.},
  archive   = {C_KDD},
  author    = {Shah, Rishi and Jain, Krishnanshu and Manchanda, Sahil and Medya, Sourav and Ranu, Sayan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671815},
  pages     = {2584–2595},
  title     = {NeuroCut: A neural approach for robust graph partitioning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-explainable temporal graph networks based on graph
information bottleneck. <em>KDD</em>, 2572–2583. (<a
href="https://doi.org/10.1145/3637528.3671962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Temporal Graph Neural Networks (TGNN) have the ability to capture both the graph topology and dynamic dependencies of interactions within a graph over time. There has been a growing need to explain the predictions of TGNN models due to the difficulty in identifying how past events influence their predictions. Since the explanation model for a static graph cannot be readily applied to temporal graphs due to its inability to capture temporal dependencies, recent studies proposed explanation models for temporal graphs. However, existing explanation models for temporal graphs rely on post-hoc explanations, requiring separate models for prediction and explanation, which is limited in two aspects: efficiency and accuracy of explanation. In this work, we propose a novel built-in explanation framework for temporal graphs, called Self-Explainable Temporal Graph Networks based on Graph Information Bottleneck (TGIB). TGIB provides explanations for event occurrences by introducing stochasticity in each temporal event based on the Information Bottleneck theory. Experimental results demonstrate the superiority of TGIB in terms of both the link prediction performance and explainability compared to state-of-the-art methods. This is the first work that simultaneously performs prediction and explanation for temporal graphs in an end-to-end manner. The source code of TGIB is available at https://github.com/sang-woo-seo/TGIB.},
  archive   = {C_KDD},
  author    = {Seo, Sangwoo and Kim, Sungwon and Jung, Jihyeong and Lee, Yoonho and Park, Chanyoung},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671962},
  pages     = {2572–2583},
  title     = {Self-explainable temporal graph networks based on graph information bottleneck},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised learning of time series representation via
diffusion process and imputation-interpolation-forecasting mask.
<em>KDD</em>, 2560–2571. (<a
href="https://doi.org/10.1145/3637528.3671673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Time Series Representation Learning (TSRL) focuses on generating informative representations for various Time Series (TS) modeling tasks. Traditional Self-Supervised Learning (SSL) methods in TSRL fall into four main categories: reconstructive, adversarial, contrastive, and predictive, each with a common challenge of sensitivity to noise and intricate data nuances. Recently, diffusion-based methods have shown advanced generative capabilities. However, they primarily target specific application scenarios like imputation and forecasting, leaving a gap in leveraging diffusion models for generic TSRL. Our work, Time Series Diffusion Embedding (TSDE), bridges this gap as the first diffusion-based SSL TSRL approach. TSDE segments TS data into observed and masked parts using an Imputation-Interpolation-Forecasting (IIF) mask. It applies a trainable embedding function, featuring dual-orthogonal Transformer encoders with a crossover mechanism, to the observed part. We train a reverse diffusion process conditioned on the embeddings, designed to predict noise added to the masked part. Extensive experiments demonstrate TSDE&#39;s superiority in imputation, interpolation, forecasting, anomaly detection, classification, and clustering. We also conduct an ablation study, present embedding visualizations, and compare inference speed, further substantiating TSDE&#39;s efficiency and validity in learning representations of TS data.},
  archive   = {C_KDD},
  author    = {Senane, Zineb and Cao, Lele and Buchner, Valentin Leonhard and Tashiro, Yusuke and You, Lei and Herman, Pawel Andrzej and Nordahl, Mats and Tu, Ruibo and von Ehrenheim, Vilhelm},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671673},
  pages     = {2560–2571},
  title     = {Self-supervised learning of time series representation via diffusion process and imputation-interpolation-forecasting mask},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DPHGNN: A dual perspective hypergraph neural networks.
<em>KDD</em>, 2548–2559. (<a
href="https://doi.org/10.1145/3637528.3672047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Message passing on hypergraphs has been a standard framework for learning higher-order correlations between hypernodes. Recently-proposed hypergraph neural networks (HGNNs) can be categorized into spatial and spectral methods based on their design choices. In this work, we analyze the impact of change in hypergraph topology on the suboptimal performance of HGNNs and propose DPHGNN, a novel dual-perspective HGNN that introduces equivariant operator learning to capture lower-order semantics by inducing topology-aware spatial and spectral inductive biases. DPHGNN employs a unified framework to dynamically fuse lower-order explicit feature representations from the underlying graph into the super-imposed hypergraph structure. We benchmark DPHGNN over eight benchmark hypergraph datasets for the semi-supervised hypernode classification task and obtain superior performance compared to seven state-of-the-art baselines. We also provide a theoretical framework and a synthetic hypergraph isomorphism test to express the power of spatial HGNNs and quantify the expressivity of DPHGNN beyond the Generalized Weisfeiler Leman (1-GWL) test. Finally, DPHGNN was deployed by our partner e-commerce company, Meesho for the Return-to-Origin (RTO) prediction task, which shows ~7\% higher macro F1-Score than the best baseline.},
  archive   = {C_KDD},
  author    = {Saxena, Siddhant and Ghatak, Shounak and Kolla, Raghu and Mukherjee, Debashis and Chakraborty, Tanmoy},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672047},
  pages     = {2548–2559},
  title     = {DPHGNN: A dual perspective hypergraph neural networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable temporal motif densest subnetwork discovery.
<em>KDD</em>, 2536–2547. (<a
href="https://doi.org/10.1145/3637528.3671889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Finding dense subnetworks, with density based on edges or more complex structures, such as subgraphs or k-cliques, is a fundamental algorithmic problem with many applications. While the problem has been studied extensively in static networks, much remains to be explored for temporal networks.In this work we introduce the novel problem of identifying the temporal motif densest subnetwork, i.e., the densest subnetwork with respect to temporal motifs, which are high-order patterns characterizing temporal networks. Identifying temporal motifs is an extremely challenging task, and thus, efficient methods are required. To address this challenge, we design two novel randomized approximation algorithms with rigorous probabilistic guarantees that provide high-quality solutions. We perform extensive experiments showing that our methods outperform baselines. Furthermore, our algorithms scale on networks with up to billions of temporal edges, while baselines cannot handle such large networks. We use our techniques to analyze a financial network and show that our formulation reveals important network structures, such as bursty temporal events and communities of users with similar interests.},
  archive   = {C_KDD},
  author    = {Sarpe, Ilie and Vandin, Fabio and Gionis, Aristides},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671889},
  pages     = {2536–2547},
  title     = {Scalable temporal motif densest subnetwork discovery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LARP: Language audio relational pre-training for cold-start
playlist continuation. <em>KDD</em>, 2524–2535. (<a
href="https://doi.org/10.1145/3637528.3671772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As online music consumption increasingly shifts towards playlist-based listening, the task of playlist continuation, in which an algorithm suggests songs to extend a playlist in a personalized and musically cohesive manner, has become vital to the success of music streaming services. Currently, many existing playlist continuation approaches rely on collaborative filtering methods to perform their recommendations. However, such methods will struggle to recommend songs that lack interaction data, an issue known as the cold-start problem. Current approaches to this challenge design complex mechanisms for extracting relational signals from sparse collaborative signals and integrating them into content representations. However, these approaches leave content representation learning out of scope and utilize frozen, pre-trained content models that may not be aligned with the distribution or format of a specific musical setting. Furthermore, even the musical state-of-the-art content modules are either (1) incompatible with the cold-start setting or (2) unable to effectively integrate cross-modal and relational signals. In this paper, we introduce LARP, a multi-modal cold-start playlist continuation model, to effectively overcome these limitations. LARP is a three-stage contrastive learning framework that integrates both multi-modal and relational signals into its learned representations. Our framework uses increasing stages of task-specific abstraction: within-track (language-audio) contrastive loss, track-track contrastive loss, and track-playlist contrastive loss. Experimental results on two publicly available datasets demonstrate the efficacy of LARP over uni-modal and multi-modal models for playlist continuation in a cold-start setting. Finally, this work pioneers the perspective of addressing cold-start recommendation via relational representation learning. Code and dataset are released at: https://github.com/Rsalganik1123/LARP/},
  archive   = {C_KDD},
  author    = {Salganik, Rebecca and Liu, Xiaohao and Ma, Yunshan and Kang, Jian and Chua, Tat-Seng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671772},
  pages     = {2524–2535},
  title     = {LARP: Language audio relational pre-training for cold-start playlist continuation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel feature space augmentation method to improve
classification performance and evaluation reliability. <em>KDD</em>,
2512–2523. (<a href="https://doi.org/10.1145/3637528.3671736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Classification tasks in many real-world domains are exacerbated by class imbalance, relatively small sample sizes compared to high dimensionality, and measurement uncertainty. The problem of class imbalance has been extensively studied, and data augmentation methods based on interpolation of minority class instances have been proposed as a viable solution to mitigate imbalance. It remains to be seen whether augmentation can be applied to improve the overall performance while maintaining stability, especially with a limited number of samples. In this paper, we present a novel feature-space augmentation technique that can be applied to high-dimensional data for classification tasks and address these issues. Our method utilizes uniform random sampling and introduces synthetic instances by taking advantage of the local distributions of individual features in the observed instances. The core augmentation algorithm is class-invariant, which opens up an unexplored avenue of simultaneously improving and stabilizing performance by augmenting unlabeled instances. The proposed method is evaluated using a comprehensive performance analysis involving multiple classifiers and metrics. Comparative analysis with existing feature space augmentation methods strongly suggests that the proposed algorithm can result in improved classification performance while also increasing the overall reliability of the performance evaluation.},
  archive   = {C_KDD},
  author    = {Saimon, Sakhawat Hossain and Najnin, Tanzira and Ruan, Jianhua},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671736},
  pages     = {2512–2523},
  title     = {A novel feature space augmentation method to improve classification performance and evaluation reliability},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CoSLight: Co-optimizing collaborator selection and
decision-making to enhance traffic signal control. <em>KDD</em>,
2500–2511. (<a href="https://doi.org/10.1145/3637528.3671998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Effective multi-intersection collaboration is pivotal for reinforcement-learning-based traffic signal control to alleviate congestion. Existing work mainly chooses neighboring intersections as collaborators. However, quite a lot of congestion, even some wide-range congestion, is caused by non-neighbors failing to collaborate. To address these issues, we propose to separate the collaborator selection as a second policy to be learned, concurrently being updated with the original signal-controlling policy. Specifically, the selection policy in real-time adaptively selects the best teammates according to phase- and intersection-level features. Empirical results on both synthetic and real-world datasets provide robust validation for the superiority of our approach, offering significant improvements over existing state-of-the-art methods. Code is available at https://github.com/bonaldli/CoSLight.},
  archive   = {C_KDD},
  author    = {Ruan, Jingqing and Li, Ziyue and Wei, Hua and Jiang, Haoyuan and Lu, Jiaming and Xiong, Xuantang and Mao, Hangyu and Zhao, Rui},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671998},
  pages     = {2500–2511},
  title     = {CoSLight: Co-optimizing collaborator selection and decision-making to enhance traffic signal control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RHiOTS: A framework for evaluating hierarchical time series
forecasting algorithms. <em>KDD</em>, 2491–2499. (<a
href="https://doi.org/10.1145/3637528.3672062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce the Robustness of Hierarchically Organized Time Series (RHiOTS) framework, designed to assess the robustness of hierarchical time series forecasting models and algorithms on real-world datasets. Hierarchical time series, where lower-level forecasts must sum to upper-level ones, are prevalent in various contexts, such as retail sales across countries. Current empirical evaluations of forecasting methods are often limited to a small set of benchmark datasets, offering a narrow view of algorithm behavior. RHiOTS addresses this gap by systematically altering existing datasets and modifying the characteristics of individual series and their interrelations. It uses a set of parameterizable transformations to simulate those changes in the data distribution. Additionally, RHiOTS incorporates an innovative visualization component, turning complex, multidimensional robustness evaluation results into intuitive, easily interpretable visuals. This approach allows an in-depth analysis of algorithm and model behavior under diverse conditions. We illustrate the use of RHiOTS by analyzing the predictive performance of several algorithms. Our findings show that traditional statistical methods are more robust than state-of-the-art deep learning algorithms, except when the transformation effect is highly disruptive. Furthermore, we found no significant differences in the robustness of the algorithms when applying specific reconciliation methods, such as MinT. RHiOTS provides researchers with a comprehensive tool for understanding the nuanced behavior of forecasting algorithms, offering a more reliable basis for selecting the most appropriate method for a given problem.},
  archive   = {C_KDD},
  author    = {Roque, Luis and Soares, Carlos and Torgo, Lu\&#39;{\i}s},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672062},
  pages     = {2491–2499},
  title     = {RHiOTS: A framework for evaluating hierarchical time series forecasting algorithms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fast exact algorithm to enumerate maximal pseudo-cliques
in large sparse graphs. <em>KDD</em>, 2479–2490. (<a
href="https://doi.org/10.1145/3637528.3672066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pseudo-cliques (subgraphs with almost all possible edges) have many applications. But they do not satisfy the convertible antimonotone constraint (as we prove here). So, it is hard to reduce the search space of pseudo-cliques and list them efficiently. To our knowledge, only two exact algorithms, namely, ODES and PCE, were proposed for this purpose, but both have high execution times. Here, we present an exact algorithm named Fast Pseudo-Clique Enumerator (FPCE). It employs some pruning techniques we derived to reduce the search space. Our experiment on 15 real and 16 synthetic graphs shows that (i) on real graphs, FPCE is, on average, 38.6 and 6.5 times faster than ODES and PCE, respectively, whereas (ii) on synthetic graphs, FPCE is, on average, 39.7 and 3.1 times faster than ODES and PCE, respectively. We apply FPCE and a popular heuristic method on a PPI network to identify pseudo-cliques. FPCE outputs match with more known protein complexes, are more accurate, and are biologically more significant - suggesting that the exact computation of pseudo-cliques may give better insights. For its speed, FPCE is a suitable choice in such cases.},
  archive   = {C_KDD},
  author    = {Rahman, Ahsanur and Roy, Kalyan and Maliha, Ramiza and Chowdhury, Townim Faisal},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672066},
  pages     = {2479–2490},
  title     = {A fast exact algorithm to enumerate maximal pseudo-cliques in large sparse graphs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pre-train and refine: Towards higher efficiency in
k-agnostic community detection without quality degradation.
<em>KDD</em>, 2467–2478. (<a
href="https://doi.org/10.1145/3637528.3671686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Community detection (CD) is a classic graph inference task that partitions nodes of a graph into densely connected groups. While many CD methods have been proposed with either impressive quality or efficiency, balancing the two aspects remains a challenge. This study explores the potential of deep graph learning to achieve a better trade-off between the quality and efficiency of K-agnostic CD, where the number of communities K is unknown. We propose PRoCD (&amp;lt;u&amp;gt;P&amp;lt;/u&amp;gt;re-training \&amp;amp; &amp;lt;u&amp;gt;R&amp;lt;/u&amp;gt;efinement f&amp;lt;u&amp;gt;O&amp;lt;/u&amp;gt;r &amp;lt;u&amp;gt;C&amp;lt;/u&amp;gt;ommunity &amp;lt;u&amp;gt;D&amp;lt;/u&amp;gt;etection), a simple yet effective method that reformulates K-agnostic CD as the binary node pair classification. PRoCD follows a pre-training \&amp;amp; refinement paradigm inspired by recent advances in pre-training techniques. We first conduct the offline pre-training of PRoCD on small synthetic graphs covering various topology properties. Based on the inductive inference across graphs, we then generalize the pre-trained model (with frozen parameters) to large real graphs and use the derived CD results as the initialization of an existing efficient CD method (e.g., InfoMap) to further refine the quality of CD results. In addition to benefiting from the transfer ability regarding quality, the online generalization and refinement can also help achieve high inference efficiency, since there is no time-consuming model optimization. Experiments on public datasets with various scales demonstrate that PRoCD can ensure higher efficiency in K-agnostic CD without significant quality degradation.},
  archive   = {C_KDD},
  author    = {Qin, Meng and Zhang, Chaorui and Gao, Yu and Zhang, Weixi and Yeung, Dit-Yan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671686},
  pages     = {2467–2478},
  title     = {Pre-train and refine: Towards higher efficiency in K-agnostic community detection without quality degradation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ORCDF: An oversmoothing-resistant cognitive diagnosis
framework for student learning in online education systems.
<em>KDD</em>, 2455–2466. (<a
href="https://doi.org/10.1145/3637528.3671988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cognitive diagnosis models (CDMs) are designed to learn students&#39; mastery levels using their response logs. CDMs play a fundamental role in online education systems since they significantly influence downstream applications such as teachers&#39; guidance and computerized adaptive testing. Despite the success achieved by existing CDMs, we find that they suffer from a thorny issue that the learned students&#39; mastery levels are too similar. This issue, which we refer to as oversmoothing, could diminish the CDMs&#39; effectiveness in downstream tasks. CDMs comprise two core parts: learning students&#39; mastery levels and assessing mastery levels by fitting the response logs. This paper contends that the oversmoothing issue arises from that existing CDMs seldom utilize response signals on exercises in the learning part but only use them as labels in the assessing part. To this end, this paper proposes an oversmoothing-resistant cognitive diagnosis framework (ORCDF) to enhance existing CDMs by utilizing response signals in the learning part. Specifically, ORCDF introduces a novel response graph to inherently incorporate response signals as types of edges. Then, ORCDF designs a tailored response-aware graph convolution network (RGC) that effectively captures the crucial response signals within the response graph. Via ORCDF, existing CDMs are enhanced by replacing the input embeddings with the outcome of RGC, allowing for the consideration of response signals on exercises in the learning part. Extensive experiments on real-world datasets show that ORCDF not only helps existing CDMs alleviate the oversmoothing issue but also significantly enhances the models&#39; prediction and interpretability performance. Moreover, the effectiveness of ORCDF is validated in the downstream task of computerized adaptive testing.},
  archive   = {C_KDD},
  author    = {Qian, Hong and Liu, Shuo and Li, Mingjia and Li, Bingdong and Liu, Zhi and Zhou, Aimin},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671988},
  pages     = {2455–2466},
  title     = {ORCDF: An oversmoothing-resistant cognitive diagnosis framework for student learning in online education systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reimagining graph classification from a prototype view with
optimal transport: Algorithm and theorem. <em>KDD</em>, 2444–2454. (<a
href="https://doi.org/10.1145/3637528.3671696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, Graph Neural Networks (GNNs) have achieved inspiring performances in graph classification tasks. However, the message passing mechanism in GNNs implicitly utilizes the topological information of the graph, which may lead to a potential loss of structural information. Furthermore, the graph classification decision process based on GNNs resembles a black box and lacks sufficient transparency. The non-linear classifier following the GNNs also defaults to the assumption that each class is represented by a single vector, thereby limiting the diversity of intra-class representations.To address these issues, we propose a novel prototype-based graph classification framework that introduces the Fused Gromov-Wasserstein (FGW) distance in Optimal Transport (OT) as the similarity measure. In this way, the model explicitly exploits the structural information on the graph through OT while leading to a more transparent and straightforward classification process. The introduction of prototypes also inherently addresses the issue of limited within-class representations. Besides, to alleviate the widely acknowledged computational complexity issue of FGW distance calculation, we devise a simple yet effective NN-based FGW distance approximator, which can enable full GPU training acceleration with a marginal performance loss. In theory, we analyze the generalization performance of the proposed method and derive an O (1 over N) generalization bound, where the proof techniques can be extended to a broader range of prototype-based classification frameworks. Experimental results show that the proposed framework achieves competitive and superior performance on several widely used graph classification benchmark datasets. The code is avaliable at https://github.com/ChnQ/PGOT.},
  archive   = {C_KDD},
  author    = {Qian, Chen and Tang, Huayi and Liang, Hong and Liu, Yong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671696},
  pages     = {2444–2454},
  title     = {Reimagining graph classification from a prototype view with optimal transport: Algorithm and theorem},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). QSketch: An efficient sketch for weighted cardinality
estimation in streams. <em>KDD</em>, 2432–2443. (<a
href="https://doi.org/10.1145/3637528.3671695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Estimating cardinality, i.e., the number of distinct elements, of a data stream is a fundamental problem in areas like databases, computer networks, and information retrieval. This study delves into a broader scenario where each element carries a positive weight. Unlike traditional cardinality estimation, limited research exists on weighted cardinality, with current methods requiring substantial memory and computational resources, challenging for devices with limited capabilities and real-time applications like anomaly detection. To address these issues, we propose QSketch, a memory-efficient sketch method for estimating weighted cardinality in streams. QSketch uses a quantization technique to condense continuous variables into a compact set of integer variables, with each variable requiring only 8 bits, making it 8 times smaller than previous methods. Furthermore, we leverage dynamic properties during QSketch generation to significantly enhance estimation accuracy and achieve a lower time complexity of O(1) for updating estimations upon encountering a new element. Experimental results on synthetic and real-world datasets show that QSketch is approximately 30\% more accurate and two orders of magnitude faster than the state-of-the-art, using only 1/8 of the memory.},
  archive   = {C_KDD},
  author    = {Qi, Yiyan and Li, Rundong and Wang, Pinghui and Sun, Yufang and Xing, Rui},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671695},
  pages     = {2432–2443},
  title     = {QSketch: An efficient sketch for weighted cardinality estimation in streams},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unifying evolution, explanation, and discernment: A
generative approach for dynamic graph counterfactuals. <em>KDD</em>,
2420–2431. (<a href="https://doi.org/10.1145/3637528.3671831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present GRACIE (Graph Recalibration and Adaptive Counterfactual Inspection and Explanation), a novel approach for generative classification and counterfactual explanations of dynamically changing graph data. We study graph classification problems through the lens of generative classifiers. We propose a dynamic, self-supervised latent variable model that updates by identifying plausible counterfactuals for input graphs and recalibrating decision boundaries through contrastive optimization. Unlike prior work, we do not rely on linear separability between the learned graph representations to find plausible counterfactuals. Moreover, GRACIE eliminates the need for stochastic sampling in latent spaces and graph-matching heuristics. Our work distills the implicit link between generative classification and loss functions in the latent space, a key insight to understanding recent successes with this architecture. We further observe the inherent trade-off between validity and pulling explainee instances towards the central region of the latent space, empirically demonstrating our theoretical findings. In extensive experiments on synthetic and real-world graph data, we attain considerable improvements, reaching ~99\% validity when sampling sets of counterfactuals even in the challenging setting of dynamic data landscapes.},
  archive   = {C_KDD},
  author    = {Prenkaj, Bardh and Villaiz\&#39;{a}n-Vallelado, Mario and Leemann, Tobias and Kasneci, Gjergji},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671831},
  pages     = {2420–2431},
  title     = {Unifying evolution, explanation, and discernment: A generative approach for dynamic graph counterfactuals},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CASH via optimal diversity for ensemble learning.
<em>KDD</em>, 2411–2419. (<a
href="https://doi.org/10.1145/3637528.3671894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Combined Algorithm Selection and Hyperparameter Optimization (CASH) problem is pivotal in Automatic Machine Learning (AutoML). Most leading approaches combine Bayesian optimization with post-hoc ensemble building to create advanced AutoML systems. Bayesian optimization (BO) typically focuses on identifying a singular algorithm and its hyperparameters that outperform all other configurations. Recent developments have highlighted an oversight in prior CASH methods: the lack of consideration for diversity among the base learners of the ensemble. This oversight was overcome by explicitly injecting the search for diversity into the traditional CASH problem. However, despite recent developments, BO&#39;s limitation lies in its inability to directly optimize ensemble generalization error, offering no theoretical assurance that increased diversity correlates with enhanced ensemble performance. Our research addresses this gap by establishing a theoretical foundation that integrates diversity into the core of BO for direct ensemble learning. We explore a theoretically sound framework that describes the relationship between pair-wise diversity and ensemble performance, which allows our Bayesian optimization framework Optimal Diversity Bayesian Optimization (OptDivBO) to directly and efficiently minimize ensemble generalization error. OptDivBO guarantees an optimal balance between pairwise diversity and individual model performance, setting a new precedent in ensemble learning within CASH. Empirical results on 20 public datasets show that OptDivBO achieves the best average test ranks of 1.57 and 1.4 in classification and regression tasks.},
  archive   = {C_KDD},
  author    = {Poduval, Pranav and Patnala, Sanjay Kumar and Oberoi, Gaurav and Srivasatava, Nitish and Asthana, Siddhartha},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671894},
  pages     = {2411–2419},
  title     = {CASH via optimal diversity for ensemble learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fredformer: Frequency debiased transformer for time series
forecasting. <em>KDD</em>, 2400–2410. (<a
href="https://doi.org/10.1145/3637528.3671928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Transformer model has shown leading performance in time series forecasting. Nevertheless, in some complex scenarios, it tends to learn low-frequency features in the data and overlook high-frequency features, showing a frequency bias. This bias prevents the model from accurately capturing important high-frequency data features. In this paper, we undertake empirical analyses to understand this bias and discover that frequency bias results from the model disproportionately focusing on frequency features with higher energy. Based on our analysis, we formulate this bias and propose Fredformer, a Transformer-based framework designed to mitigate frequency bias by learning features equally across different frequency bands. This approach prevents the model from overlooking lower amplitude features important for accurate forecasting. Extensive experiments show the effectiveness of our proposed approach, which can outperform other baselines in different real-world time-series datasets. Furthermore, we introduce a lightweight variant of the Fredformer with an attention matrix approximation, which achieves comparable performance but with much fewer parameters and lower computation costs. The code is available at: https://github.com/chenzRG/Fredformer},
  archive   = {C_KDD},
  author    = {Piao, Xihao and Chen, Zheng and Murayama, Taichi and Matsubara, Yasuko and Sakurai, Yasushi},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671928},
  pages     = {2400–2410},
  title     = {Fredformer: Frequency debiased transformer for time series forecasting},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How powerful is graph filtering for recommendation.
<em>KDD</em>, 2388–2399. (<a
href="https://doi.org/10.1145/3637528.3671789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It has been shown that the effectiveness of graph convolutional network (GCN) for recommendation is attributed to the spectral graph filtering. Most GCN-based methods consist of a graph filter or followed by a low-rank mapping optimized based on supervised training. However, we show two limitations suppressing the power of graph filtering: (1) Lack of generality. Due to the varied noise distribution, graph filters fail to denoise sparse data where noise is scattered across all frequencies, while supervised training results in worse performance on dense data where noise is concentrated in middle frequencies that can be removed by graph filters without training. (2) Lack of expressive power. We theoretically show that linear GCN (LGCN) that is effective on collaborative filtering (CF) cannot generate arbitrary embeddings, implying the possibility that optimal data representation might be unreachable.To tackle the first limitation, we show close relation between noise distribution and the sharpness of spectrum where a sharper spectral distribution is more desirable causing data noise to be separable from important features without training. Based on this observation, we propose a generalized graph normalization (G2N) with hyperparameters adjusting the sharpness of spectral distribution in order to redistribute data noise to assure that it can be removed by graph filtering without training. As for the second limitation, we propose an individualized graph filter (IGF) adapting to the different confidence levels of the user preference that interactions can reflect, which is proved to be able to generate arbitrary embeddings. By simplifying LGCN, we further propose a simplified graph filtering for CF (SGFCF) which only requires the top-K singular values for recommendation. Finally, experimental results on four datasets with different density settings demonstrate the effectiveness and efficiency of our proposed methods.},
  archive   = {C_KDD},
  author    = {Peng, Shaowen and Liu, Xin and Sugiyama, Kazunari and Mine, Tsunenori},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671789},
  pages     = {2388–2399},
  title     = {How powerful is graph filtering for recommendation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TSC: A simple two-sided constraint against over-smoothing.
<em>KDD</em>, 2376–2387. (<a
href="https://doi.org/10.1145/3637528.3671954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph Convolutional Neural Network (GCN), a widely adopted method for analyzing relational data, enhances node discriminability through the aggregation of neighboring information. Usually, stacking multiple layers can improve the performance of GCN by leveraging information from high-order neighbors. However, the increase of the network depth will induce the over-smoothing problem, which can be attributed to the quality and quantity of neighbors changing: (a) neighbor quality, node&#39;s neighbors become overlapping in high order, leading to aggregated information becoming indistinguishable, (b) neighbor quantity, the exponentially growing aggregated neighbors submerges the node&#39;s initial feature by recursively aggregating operations. Current solutions mainly focus on one of the above causes and seldom consider both at once. Aiming at tackling both causes of over-smoothing in one shot, we introduce a simple Two-Sided Constraint (TSC) for GCNs, comprising two straightforward yet potent techniques: random masking and contrastive constraint. The random masking acts on the representation matrix&#39;s columns to regulate the degree of information aggregation from neighbors, thus preventing the convergence of node representations. Meanwhile, the contrastive constraint, applied to the representation matrix&#39;s rows, enhances the discriminability of the nodes. Designed as a plug-in module, TSC can be easily coupled with GCN or SGC architectures. Experimental analyses on diverse real-world graph datasets verify that our approach markedly reduces the convergence of node&#39;s representation and the performance degradation in deeper GCN.},
  archive   = {C_KDD},
  author    = {Peng, Furong and Liu, Kang and Lu, Xuan and Qian, Yuhua and Yan, Hongren and Ma, Chao},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671954},
  pages     = {2376–2387},
  title     = {TSC: A simple two-sided constraint against over-smoothing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CoMAL: Contrastive active learning for multi-label text
classification. <em>KDD</em>, 2364–2375. (<a
href="https://doi.org/10.1145/3637528.3671754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-label text classification (MLTC) allows a given text to be associated with multiple labels, which well suits many real-world data mining scenarios. However, the annotation effort of MLTC is inevitably expensive and time-consuming. Although multi-label active learning provides a cost-effective solution, it still faces two major challenges: (i) constructing decent feature space to distinguish the confusing semantics of different labels; (ii) defining proper sampling criteria to measure a sample&#39;s joint effect over the entire label space. To bridge these gaps, we propose a Contrastive Multi-label Active Learning framework (CoMAL) that gives an effective data acquisition strategy. Specifically, a contrastive decoupling mechanism is introduced to fully release the semantic information of multiple labels into the latent space. Then, we devise a hybrid criterion that balances two data value measures: (i) similarity-enhanced label cardinality inconsistency reflects the uncertainty of data predictions. (ii) positive feature diversity evaluates the positive-propensity semantic diversity to handle the label sparsity. Extensive experiments demonstrate that our CoMAL outperforms the current state-of-the-art multi-label active learning approaches. Code for CoMAL is available at https://github.com/chengzju/CoMAL.},
  archive   = {C_KDD},
  author    = {Peng, Cheng and Wang, Haobo and Chen, Ke and Shou, Lidan and Yao, Chang and Wu, Runze and Chen, Gang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671754},
  pages     = {2364–2375},
  title     = {CoMAL: Contrastive active learning for multi-label text classification},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable rule lists learning with sampling. <em>KDD</em>,
2352–2363. (<a href="https://doi.org/10.1145/3637528.3671989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning interpretable models has become a major focus of machine learning research, given the increasing prominence of machine learning in socially important decision-making. Among interpretable models, rule lists are among the best-known and easily interpretable ones. However, finding optimal rule lists is computationally challenging, and current approaches are impractical for large datasets.We present a novel and scalable approach to learn nearly optimal rule lists from large datasets. Our algorithm uses sampling to efficiently obtain an approximation of the optimal rule list with rigorous guarantees on the quality of the approximation. In particular, our algorithm guarantees to find a rule list with accuracy very close to the optimal rule list when a rule list with high accuracy exists. Our algorithm builds on the VC-dimension of rule lists, for which we prove novel upper and lower bounds. Our experimental evaluation on large datasets shows that our algorithm identifies nearly optimal rule lists with a speed-up up to two orders of magnitude over state-of-the-art exact approaches. Moreover, our algorithm is as fast as, and sometimes faster than, recent heuristic approaches, while reporting higher quality rule lists. In addition, the rules reported by our algorithm are more similar to the rules in the optimal rule list than the rules from heuristic approaches.},
  archive   = {C_KDD},
  author    = {Pellegrina, Leonardo and Vandin, Fabio},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671989},
  pages     = {2352–2363},
  title     = {Scalable rule lists learning with sampling},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BTTackler: A diagnosis-based framework for efficient deep
learning hyperparameter optimization. <em>KDD</em>, 2340–2351. (<a
href="https://doi.org/10.1145/3637528.3671933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hyperparameter optimization (HPO) is known to be costly in deep learning, especially when leveraging automated approaches. Most of the existing automated HPO methods are accuracy-based, i.e., accuracy metrics are used to guide the trials of different hyperparameter configurations amongst a specific search space. However, many trials may encounter severe training problems, such as vanishing gradients and insufficient convergence, which can hardly be reflected by accuracy metrics in the early stages of the training and often result in poor performance. This leads to an inefficient optimization trajectory because the bad trials occupy considerable computation resources and reduce the probability of finding excellent hyperparameter configurations within a time limitation. In this paper, we propose Bad Trial Tackler (BTTackler), a novel HPO framework that introduces training diagnosis to identify training problems automatically and hence tackles bad trials. BTTackler diagnoses each trial by calculating a set of carefully designed quantified indicators and triggers early termination if any training problems are detected. Evaluations are performed on representative HPO tasks consisting of three classical deep neural networks (DNN) and four widely used HPO methods. To better quantify the effectiveness of an automated HPO method, we propose two new measurements based on accuracy and time consumption. Results show the advantage of BTTackler on two-fold: (1) it reduces 40.33\% of time consumption to achieve the same accuracy comparable to baseline methods on average and (2) it conducts 44.5\% more top-10 trials than baseline methods on average within a given time budget. We also released an open-source Python library that allows users to easily apply BTTackler to automated HPO processes with minimal code changesfootnotehttps://github.com/thuml/BTTackler.},
  archive   = {C_KDD},
  author    = {Pei, Zhongyi and Cen, Zhiyao and Huang, Yipeng and Wang, Chen and Liu, Lin and Yu, Philip and Long, Mingsheng and Wang, Jianmin},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671933},
  pages     = {2340–2351},
  title     = {BTTackler: A diagnosis-based framework for efficient deep learning hyperparameter optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast multidimensional partial fourier transform with
automatic hyperparameter selection. <em>KDD</em>, 2328–2339. (<a
href="https://doi.org/10.1145/3637528.3671667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Given a multidimensional array, how can we optimize the computation process for a part of Fourier coefficients? Discrete Fourier transform plays an overarching role in various data mining tasks. Recent interest has focused on efficiently calculating a small part of Fourier coefficients, exploiting the energy compaction property of real-world data. Current methods for partial Fourier transform frequently encounter efficiency issues, yet the adoption of pre-computation techniques within the PFT algorithm has shown promising performance. However, PFT still faces limitations in handling multidimensional data efficiently and requires manual hyperparameter tuning, leading to additional costs.In this paper, we propose Auto-MPFT (Automatic Multidimensional Partial Fourier Transform), which efficiently computes a subset of Fourier coefficients in multidimensional data without the need for manual hyperparameter search. Auto-MPFT leverages multivariate polynomial approximation for trigonometric functions, generalizing its domain to multidimensional Euclidean space. Moreover, we present a convex optimization-based algorithm for automatically selecting the optimal hyperparameter of Auto-MPFT. We provide a rigorous proof for the explicit reformulation of the original optimization problem of Auto-MPFT, demonstrating the process that converts it into a well-established unconstrained convex optimization problem. Extensive experiments show that Auto-MPFT surpasses existing partial Fourier transform methods and optimized FFT libraries, achieving up to 7.6x increase in speed without sacrificing accuracy. In addition, our optimization algorithm accurately finds the optimal hyperparameter for Auto-MPFT, significantly reducing the cost associated with hyperparameter search.},
  archive   = {C_KDD},
  author    = {Park, Yong-chan and Kim, Jongjin and Kang, U},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671667},
  pages     = {2328–2339},
  title     = {Fast multidimensional partial fourier transform with automatic hyperparameter selection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ontology enrichment for effective fine-grained entity
typing. <em>KDD</em>, 2318–2327. (<a
href="https://doi.org/10.1145/3637528.3671857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fine-grained entity typing (FET) is the task of identifying specific entity types at a fine-grained level for entity mentions based on their contextual information. Conventional methods for FET require extensive human annotation, which is time-consuming and costly given the massive scale of data. Recent studies have been developing weakly supervised or zero-shot approaches. We study the setting of zero-shot FET where only an ontology is provided. However, most existing ontology structures lack rich supporting information and even contain ambiguous relations, making them ineffective in guiding FET. Recently developed language models, though promising in various few-shot and zero-shot NLP tasks, may face challenges in zero-shot FET due to their lack of interaction with task-specific ontology. In this study, we propose \o{}urs, where we (1) enrich each node in the ontology structure with two categories of extra information:instance information for training sample augmentation andtopic information to relate types with contexts, and (2) develop a coarse-to-fine typing algorithm that exploits the enriched information by training an entailment model with contrasting topics and instance-based augmented training samples. Our experiments show that \o{}urs achieves high-quality fine-grained entity typing without human annotation, outperforming existing zero-shot methods by a large margin and rivaling supervised methods. \o{}urs also enjoys strong transferability to unseen and finer-grained types. We will open source this work upon acceptance.},
  archive   = {C_KDD},
  author    = {Ouyang, Siru and Huang, Jiaxin and Pillai, Pranav and Zhang, Yunyi and Zhang, Yu and Han, Jiawei},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671857},
  pages     = {2318–2327},
  title     = {Ontology enrichment for effective fine-grained entity typing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reliable confidence intervals for information retrieval
evaluation using generative a.i. <em>KDD</em>, 2307–2317. (<a
href="https://doi.org/10.1145/3637528.3671883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The traditional evaluation of information retrieval (IR) systems is generally very costly as it requires manual relevance annotation from human experts. Recent advancements in generative artificial intelligence -specifically large language models (LLMs)- can generate relevance annotations at an enormous scale with relatively small computational costs. Potentially, this could alleviate the costs traditionally associated with IR evaluation and make it applicable to numerous low-resource applications. However, generated relevance annotations are not immune to (systematic) errors, and as a result, directly using them for evaluation produces unreliable results.In this work, we propose two methods based on prediction-powered inference and conformal risk control that utilize computer-generated relevance annotations to place reliable confidence intervals (CIs) around IR evaluation metrics. Our proposed methods require a small number of reliable annotations from which the methods can statistically analyze the errors in the generated annotations. Using this information, we can place CIs around evaluation metrics with strong theoretical guarantees. Unlike existing approaches, our conformal risk control method is specifically designed for ranking metrics and can vary its CIs per query and document. Our experimental results show that our CIs accurately capture both the variance and bias in evaluation based on LLM annotations, better than the typical empirical bootstrapping estimates. We hope our contributions bring reliable evaluation to the many IR applications where this was traditionally infeasible.},
  archive   = {C_KDD},
  author    = {Oosterhuis, Harrie and Jagerman, Rolf and Qin, Zhen and Wang, Xuanhui and Bendersky, Michael},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671883},
  pages     = {2307–2317},
  title     = {Reliable confidence intervals for information retrieval evaluation using generative A.I.},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mining of switching sparse networks for missing value
imputation in multivariate time series. <em>KDD</em>, 2296–2306. (<a
href="https://doi.org/10.1145/3637528.3671760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multivariate time series data suffer from the problem of missing values, which hinders the application of many analytical methods. To achieve the accurate imputation of these missing values, exploiting inter-correlation by employing the relationships between sequences (i.e., a network) is as important as the use of temporal dependency, since a sequence normally correlates with other sequences. Moreover, exploiting an adequate network depending on time is also necessary since the network varies over time. However, in real-world scenarios, we normally know neither the network structure nor when the network changes beforehand. Here, we propose a missing value imputation method for multivariate time series, namely MissNet, that is designed to exploit temporal dependency with a state-space model and inter-correlation by switching sparse networks. The network encodes conditional independence between features, which helps us understand the important relationships for imputation visually. Our algorithm, which scales linearly with reference to the length of the data, alternatively infers networks and fills in missing values using the networks while discovering the switching of the networks. Extensive experiments demonstrate that MissNet outperforms the state-of-the-art algorithms for multivariate time series imputation and provides interpretable results.},
  archive   = {C_KDD},
  author    = {Obata, Kohei and Kawabata, Koki and Matsubara, Yasuko and Sakurai, Yasushi},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671760},
  pages     = {2296–2306},
  title     = {Mining of switching sparse networks for missing value imputation in multivariate time series},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CheatAgent: Attacking LLM-empowered recommender systems via
LLM agent. <em>KDD</em>, 2284–2295. (<a
href="https://doi.org/10.1145/3637528.3671837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, Large Language Model (LLM)-empowered recommender systems (RecSys) have brought significant advances in personalized user experience and have attracted considerable attention. Despite the impressive progress, the research question regarding the safety vulnerability of LLM-empowered RecSys still remains largely under-investigated. Given the security and privacy concerns, it is more practical to focus on attacking the black-box RecSys, where attackers can only observe the system&#39;s inputs and outputs. However, traditional attack approaches employing reinforcement learning (RL) agents are not effective for attacking LLM-empowered RecSys due to the limited capabilities in processing complex textual inputs, planning, and reasoning. On the other hand, LLMs provide unprecedented opportunities to serve as attack agents to attack RecSys because of their impressive capability in simulating human-like decision-making processes. Therefore, in this paper, we propose a novel attack framework called CheatAgent by harnessing the human-like capabilities of LLMs, where an LLM-based agent is developed to attack LLM-Empowered RecSys. Specifically, our method first identifies the insertion position for maximum impact with minimal input modification. After that, the LLM agent is designed to generate adversarial perturbations to insert at target positions. To further improve the quality of generated perturbations, we utilize the prompt tuning technique to improve attacking strategies via feedback from the victim RecSys iteratively. Extensive experiments across three real-world datasets demonstrate the effectiveness of our proposed attacking method.},
  archive   = {C_KDD},
  author    = {Ning, Liang-bo and Wang, Shijie and Fan, Wenqi and Li, Qing and Xu, Xin and Chen, Hao and Huang, Feiran},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671837},
  pages     = {2284–2295},
  title     = {CheatAgent: Attacking LLM-empowered recommender systems via LLM agent},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving the consistency in cross-lingual cross-modal
retrieval with 1-to-k contrastive learning. <em>KDD</em>, 2272–2283. (<a
href="https://doi.org/10.1145/3637528.3671787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cross-lingual Cross-modal Retrieval (CCR) is an essential task in web search, which aims to break the barriers between modality and language simultaneously and achieves image-text retrieval in the multi-lingual scenario with a single model. In recent years, excellent progress has been made based on cross-lingual cross-modal pre-training; particularly, the methods based on contrastive learning on large-scale data have significantly improved retrieval tasks. However, these methods directly follow the existing pre-training methods in the cross-lingual or cross-modal domain, leading to two problems of inconsistency in CCR: The methods with cross-lingual style suffer from the intra-modal error propagation, resulting in inconsistent recall performance across languages in the whole dataset. The methods with cross-modal style suffer from the inter-modal optimization direction bias, resulting in inconsistent rank across languages within each instance, which cannot be reflected by Recall@K. To solve these problems, we propose a simple but effective 1-to-K contrastive learning method, which treats each language equally and eliminates error propagation and optimization bias. In addition, we propose a new evaluation metric, Mean Rank Variance (MRV), to reflect the rank inconsistency across languages within each instance. Extensive experiments on four CCR datasets show that our method improves both recall rates and MRV with smaller-scale pre-trained data, achieving the new state-of-art.},
  archive   = {C_KDD},
  author    = {Nie, Zhijie and Zhang, Richong and Feng, Zhangchi and Huang, Hailang and Liu, Xudong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671787},
  pages     = {2272–2283},
  title     = {Improving the consistency in cross-lingual cross-modal retrieval with 1-to-K contrastive learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ImputeFormer: Low rankness-induced transformers for
generalizable spatiotemporal imputation. <em>KDD</em>, 2260–2271. (<a
href="https://doi.org/10.1145/3637528.3671751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Missing data is a pervasive issue in both scientific and engineering tasks, especially for the modeling of spatiotemporal data. Existing imputation solutions mainly include low-rank models and deep learning models. The former assumes general structural priors but has limited model capacity. The latter possesses salient expressivity, but lacks prior knowledge of the underlying spatiotemporal structures. Leveraging the strengths of both two paradigms, we demonstrate a low rankness-induced Transformer to achieve a balance between strong inductive bias and high expressivity. The exploitation of the inherent structures of spatiotemporal data enables our model to learn balanced signal-noise representations, making it generalizable for a variety of imputation tasks. We demonstrate its superiority in terms of accuracy, efficiency, and versatility in heterogeneous datasets, including traffic flow, solar energy, smart meters, and air quality. Promising empirical results provide strong conviction that incorporating time series primitives, such as low-rankness, can substantially facilitate the development of a generalizable model to approach a wide range of spatiotemporal imputation problems.},
  archive   = {C_KDD},
  author    = {Nie, Tong and Qin, Guoyang and Ma, Wei and Mei, Yuewen and Sun, Jian},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671751},
  pages     = {2260–2271},
  title     = {ImputeFormer: Low rankness-induced transformers for generalizable spatiotemporal imputation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Money never sleeps: Maximizing liquidity mining yields in
decentralized finance. <em>KDD</em>, 2248–2259. (<a
href="https://doi.org/10.1145/3637528.3671942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The popularity of decentralized finance has drawn attention to liquidity mining (LM). In LM, a user deposits her cryptocurrencies into liquidity pools to provide liquidity for exchanges and earn yields. Different liquidity pools offer varying yields and require different pairs of cryptocurrencies. A user can exchange a cryptocurrency for another with some exchange costs. Thus, an LM solution consists of exchange transactions and deposit transactions, guaranteeing (1) each exchange transaction must exchange one cryptocurrency for another at a specific rate (i.e., the exchange constraint); (2) the amounts of cryptocurrencies deposited in a liquidity pool must exceed the required threshold (i.e., the minimum constraint); (3) each deposit transaction must deposit a specific pair of cryptocurrencies at a certain rate in a liquidity pool (i.e., the deposit constraint); and (4) the cryptocurrencies used in the solution do not exceed the cryptocurrencies that the user has (i.e., the budget constraint). Selecting the most profitable LM solution is challenging due to the vast number of candidate solutions. To address this challenge, we define the yield maximization liquidity mining (YMLM) problem. Given a set of liquidity pools, a set of the user&#39;s cryptocurrencies, a set of exchange rates, and an evaluation function, YMLM aims to find an LM solution with maximal yields, satisfying the minimum, exchange, deposit, and budget constraints. We prove that YMLM is NP-hard and cannot be solved by algorithms with constant approximation ratios. To tackle YMLM, we propose two algorithms, namely YMLM_GD and YMLM_SK, with parameterized approximation ratios. Extensive experiments on both real and synthetic datasets show that our approaches outperform the baselines in yields.},
  archive   = {C_KDD},
  author    = {Ni, Wangze and Yiwei, Zhao and Sun, Weijie and Chen, Lei and Cheng, Peng and Zhang, Chen Jason and Lin, Xuemin},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671942},
  pages     = {2248–2259},
  title     = {Money never sleeps: Maximizing liquidity mining yields in decentralized finance},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantifying and estimating the predictability upper bound of
univariate numeric time series. <em>KDD</em>, 2236–2247. (<a
href="https://doi.org/10.1145/3637528.3671995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The intrinsic predictability of a given time series indicates how well an (ideal) algorithm could potentially predict it when trained on the time series data. Being able to compute the intrinsic predictability helps the developers of prediction algorithms immensely in deciding whether there is further optimization potential, as it tells them how close they are to what is (theoretically) achievable. We call the intrinsic predictability the predictability upper bound ¶imax and propose a novel method for quantifying and estimating it for univariate numeric time series. So far, this has only been done for symbolic time series, even though most real-world time series are numeric by nature. We base our technique on the close relationship between entropy and predictability, utilizing the entropy rate of a time series to compute ¶imax . Since existing entropy rate estimators, such as those based on the Lempel-Ziv compression algorithm, only work for symbolic data, we develop new estimators using tolerance thresholds for matching numeric values. We demonstrate that ¶imax is an effective upper bound that characterizes the intrinsic predictability of a time series. We give formal proofs and we validate our arguments experimentally by comparing ¶imax with the prediction accuracy of different state-of-the-art models on various real-world datasets from different domains.},
  archive   = {C_KDD},
  author    = {Mohammed, Jamal and B\&quot;{o}hlen, Michael H. and Helmer, Sven},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671995},
  pages     = {2236–2247},
  title     = {Quantifying and estimating the predictability upper bound of univariate numeric time series},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning causal networks from episodic data. <em>KDD</em>,
2224–2235. (<a href="https://doi.org/10.1145/3637528.3671999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In numerous real-world domains, spanning from environmental monitoring to long-term medical studies, observations do not arrive in a single batch but rather over time in episodes. This challenges the traditional assumption in causal discovery of a single, observational dataset, not only because each episode may be a biased sample of the population but also because multiple episodes could differ in the causal interactions underlying the observed variables. We address these issues using notions of context switches and episodic selection bias, and introduce a framework for causal modeling of episodic data. We show under which conditions we can apply information-theoretic scoring criteria for causal discovery while preserving consistency. To in practice discover the causal model progressively over time, we propose the CONTINENT algorithm which, taking inspiration from continual learning, discovers the causal model in an online fashion without having to re-learn the model upon arrival of each new episode. Our experiments over a variety of settings including selection bias, unknown interventions, and network changes showcase that CONTINENT works well in practice and outperforms the baselines by a clear margin.},
  archive   = {C_KDD},
  author    = {Mian, Osman and Mameche, Sarah and Vreeken, Jilles},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671999},
  pages     = {2224–2235},
  title     = {Learning causal networks from episodic data},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scaling training data with lossy image compression.
<em>KDD</em>, 2212–2223. (<a
href="https://doi.org/10.1145/3637528.3671904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Empirically-determined scaling laws have been broadly successful in predicting the evolution of large machine learning models with training data and number of parameters. As a consequence, they have been useful for optimizing the allocation of limited resources, most notably compute time.In certain applications, storage space is an important constraint, and data format needs to be chosen carefully as a consequence. Computer vision is a prominent example: images are inherently analog, but are always stored in a digital format using a finite number of bits. Given a dataset of digital images, the number of bits L to store each of them can be further reduced using lossy data compression. This, however, can degrade the quality of the model trained on such images, since each example has lower resolution.In order to capture this trade-off and optimize storage of training data, we propose a &#39;storage scaling law&#39; that describes the joint evolution of test error with sample size and number of bits per image. We prove that this law holds within a stylized model for image compression, and verify it empirically on two computer vision tasks, extracting the relevant parameters. We then show that this law can be used to optimize the lossy compression level. At given storage, models trained on optimally compressed images present a significantly smaller test error with respect to models trained on the original data. Finally, we investigate the potential benefits of randomizing the compression level.},
  archive   = {C_KDD},
  author    = {Mentzer, Katherine L and Montanari, Andrea},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671904},
  pages     = {2212–2223},
  title     = {Scaling training data with lossy image compression},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interpretable transformer hawkes processes: Unveiling
complex interactions in social networks. <em>KDD</em>, 2200–2211. (<a
href="https://doi.org/10.1145/3637528.3671720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Social networks represent complex ecosystems where the interactions between users or groups play a pivotal role in information dissemination, opinion formation, and social interactions. Effectively harnessing event sequence data within social networks to unearth interactions among users or groups has persistently posed a challenging frontier within the realm of point processes. Current deep point process models face inherent limitations within the context of social networks, constraining both their interpretability and expressive power. These models encounter challenges in capturing interactions among users or groups and often rely on parameterized extrapolation methods when modeling intensity over non-event intervals, limiting their capacity to capture intricate intensity patterns, particularly beyond observed events. To address these challenges, this study proposes modifications to Transformer Hawkes processes (THP), leading to the development of interpretable Transformer Hawkes processes (ITHP). ITHP inherits the strengths of THP while aligning with statistical nonlinear Hawkes processes, thereby enhancing its interpretability and providing valuable insights into interactions between users or groups. Additionally, ITHP enhances the flexibility of the intensity function over non-event intervals, making it better suited to capture complex event propagation patterns in social networks. Experimental results, both on synthetic and real data, demonstrate the effectiveness of ITHP in overcoming the identified limitations. Moreover, they highlight ITHP&#39;s applicability in the context of exploring the complex impact of users or groups within social networks. Our code is available at https://github.com/waystogetthere/Interpretable-Transformer- Hawkes-Process.git.},
  archive   = {C_KDD},
  author    = {Meng, Zizhuo and Wan, Ke and Huang, Yadong and Li, Zhidong and Wang, Yang and Zhou, Feng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671720},
  pages     = {2200–2211},
  title     = {Interpretable transformer hawkes processes: Unveiling complex interactions in social networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fair column subset selection. <em>KDD</em>, 2189–2199. (<a
href="https://doi.org/10.1145/3637528.3672005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The problem of column subset selection asks for a subset of columns from an input matrix such that the matrix can be reconstructed as accurately as possible within the span of the selected columns. A natural extension is to consider a setting where the matrix rows are partitioned into two groups, and the goal is to choose a subset of columns that minimizes the maximum reconstruction error of both groups, relative to their respective best rank-k approximation. Extending the known results of column subset selection to this fair setting is not straightforward: in certain scenarios it is unavoidable to choose columns separately for each group, resulting in double the expected column count. We propose a deterministic leverage-score sampling strategy for the fair setting and show that sampling a column subset of minimum size becomes NP-hard in the presence of two groups. Despite these negative results, we give an approximation algorithm that guarantees a solution within 1.5 times the optimal solution size. We also present practical heuristic algorithms based on rank-revealing QR factorization. Finally, we validate our methods through an extensive set of experiments using real-world data.},
  archive   = {C_KDD},
  author    = {Matakos, Antonis and Ordozgoiti, Bruno and Thejaswi, Suhas},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672005},
  pages     = {2189–2199},
  title     = {Fair column subset selection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A uniformly bounded correlation function for spatial point
patterns. <em>KDD</em>, 2177–2188. (<a
href="https://doi.org/10.1145/3637528.3671891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A point pattern is a dataset of coordinates, typically in 2D or 3D space. Point patterns are ubiquitous in diverse applications including Geographic Information Systems, Astronomy, Ecology, Biology and Medicine. Among the statistics used to quantify point patterns, most are based on Ripley&#39;s K-function, which measures the deviation of the observed pattern from a completely random arrangement of points. This approach is useful for constructing null hypothesis tests, but Ripley&#39;s K and its variants are less suitable as quantitative effect sizes because their ranges and expected values generally depend on the scale or the size of the region in which the pattern is observed. To address this, we propose a new function that behaves like a correlation coefficient for point patterns: it is tightly bounded by -1 and 1, with a value of -1 corresponding to a maximally dispersed arrangement of points, 0 indicating complete spatial randomness, and 1 representing maximal clustering. These properties are independent of scale and observation window size assuming appropriate edge correction. Evaluating our function on simulated data, we show that it has comparable statistical calibration and power to K-based baselines. We hope that the ease of interpretation of our bounded function will facilitate the analysis of spatial data across multiple fields.},
  archive   = {C_KDD},
  author    = {Martynova, Evgenia and Textor, Johannes},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671891},
  pages     = {2177–2188},
  title     = {A uniformly bounded correlation function for spatial point patterns},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FLAIM: AIM-based synthetic data generation in the federated
setting. <em>KDD</em>, 2165–2176. (<a
href="https://doi.org/10.1145/3637528.3671990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Preserving individual privacy while enabling collaborative data sharing is crucial for organizations. Synthetic data generation is one solution, producing artificial data that mirrors the statistical properties of private data. While numerous techniques have been devised under differential privacy, they predominantly assume data is centralized. However, data is often distributed across multiple clients in a federated manner. In this work, we initiate the study of federated synthetic tabular data generation. Building upon a SOTA central method known as AIM, we present DistAIM and FLAIM. We first show that it is straightforward to distribute AIM, extending a recent approach based on secure multi-party computation which necessitates additional overhead, making it less suited to federated scenarios. We then demonstrate that naively federating AIM can lead to substantial degradation in utility under the presence of heterogeneity. To mitigate both issues, we propose an augmented FLAIM approach that maintains a private proxy of heterogeneity. We simulate our methods across a range of benchmark datasets under different degrees of heterogeneity and show we can improve utility while reducing overhead.},
  archive   = {C_KDD},
  author    = {Maddock, Samuel and Cormode, Graham and Maple, Carsten},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671990},
  pages     = {2165–2176},
  title     = {FLAIM: AIM-based synthetic data generation in the federated setting},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph anomaly detection with few labels: A data-centric
approach. <em>KDD</em>, 2153–2164. (<a
href="https://doi.org/10.1145/3637528.3671929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Anomalous node detection in a static graph faces significant challenges due to the rarity of anomalies and the substantial cost of labeling their deviant structure and attribute patterns. These challenges give rise to data-centric problems, including extremely imbalanced data distributions and intricate graph learning, which significantly impede machine learning and deep learning methods from discerning the patterns of graph anomalies with few labels. While these issues remain crucial, much of the current research focuses on addressing the induced technical challenges, treating the shortage of labeled data as a given. Distinct from previous efforts, this work focuses on tackling the data-centric problems by generating auxiliary training nodes that conform to the original graph topology and attribute distribution. We categorize this approach as data-centric, aiming to enhance existing anomaly detectors by training them on our synthetic data. However, the methods for generating nodes and the effectiveness of utilizing synthetic data for graph anomaly detection remain unexplored in the realm. To answer these questions, we thoroughly investigate the denoising diffusion model. Drawing from our observations on the diffusion process, we illuminate the shifts in graph energy distribution and establish two principles for designing denoising neural networks tailored to graph anomaly generation. From the insights, we propose a diffusion-based graph generation method to synthesize training nodes, which can be promptly integrated to work with existing anomaly detectors. The empirical results on eight widely-used datasets demonstrate our generated data can effectively enhance the nine state-of-the-art graph detectors&#39; performance.},
  archive   = {C_KDD},
  author    = {Ma, Xiaoxiao and Li, Ruikun and Liu, Fanzhen and Ding, Kaize and Yang, Jian and Wu, Jia},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671929},
  pages     = {2153–2164},
  title     = {Graph anomaly detection with few labels: A data-centric approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable differentiable causal discovery in the presence of
latent confounders with skeleton posterior. <em>KDD</em>, 2141–2152. (<a
href="https://doi.org/10.1145/3637528.3672031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Differentiable causal discovery has made significant advancements in the learning of directed acyclic graphs. However, its application to real-world datasets remains restricted due to the ubiquity of latent confounders and the requirement to learn maximal ancestral graphs (MAGs). To date, existing differentiable MAG learning algorithms have been limited to small datasets and failed to scale to larger ones (e.g., with more than 50 variables).The key insight in this paper is that the causal skeleton, which is the undirected version of the causal graph, has potential for improving accuracy and reducing the search space of the optimization procedure, thereby enhancing the performance of differentiable causal discovery. Therefore, we seek to address a two-fold challenge to harness the potential of the causal skeleton for differentiable causal discovery in the presence of latent confounders: (1) scalable and accurate estimation of skeleton and (2) universal integration of skeleton estimation with differentiable causal discovery.To this end, we propose SPOT (Skeleton Posterior-guided OpTimization), a two-phase framework that harnesses skeleton posterior for differentiable causal discovery in the presence of latent confounders. On the contrary to a &quot;point-estimation&quot;, SPOT seeks to estimate the posterior distribution of skeletons given the dataset. It first formulates the posterior inference as an instance of amortized inference problem and concretizes it with a supervised causal learning (SCL)-enabled solution to estimate the skeleton posterior. To incorporate the skeleton posterior with differentiable causal discovery, SPOT then features a skeleton posterior-guided stochastic optimization procedure to guide the optimization of MAGs.Extensive experiments on various datasets show that SPOT substantially outperforms SOTA methods for MAG learning. SPOT also demonstrates its effectiveness in the accuracy of skeleton posterior estimation in comparison with non-parametric bootstrap-based, or more recently, variational inference-based methods. Finally, we observe that the adoption of skeleton posterior exhibits strong promise in various causal discovery tasks.},
  archive   = {C_KDD},
  author    = {Ma, Pingchuan and Ding, Rui and Fu, Qiang and Zhang, Jiaru and Wang, Shuai and Han, Shi and Zhang, Dongmei},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672031},
  pages     = {2141–2152},
  title     = {Scalable differentiable causal discovery in the presence of latent confounders with skeleton posterior},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Handling varied objectives by online decision making.
<em>KDD</em>, 2130–2140. (<a
href="https://doi.org/10.1145/3637528.3671812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Conventional machine learning typically assume a fixed learning objective throughout the learning process. However, for real-world tasks in open and dynamic environments, objectives can change frequently. For example, in autonomous driving, a car has several default modes, but a user&#39;s concern for speed and fuel consumption varies depending on road conditions and personal needs. We formulate this problem as learning with varied objectives (LVO), where the goal is to optimize a dynamic weighted combination of multiple sub-objectives by sequentially selecting actions that incur different losses on these sub-objectives. We propose the VaRons algorithm, which estimates the action-wise performance on each sub-objective and adaptively selects decisions according to the dynamic requirements on different sub-objectives. Further, we extend our approach to cases involving contextual representations and propose the ConVaRons algorithm, assuming parameterized linear structure that links contextual features to the main objective. Both the VaRons and ConVaRons are provably minimax optimal with respect to the time horizon T, with ConVaRons showing better dependency with the number of sub-objectives K. Experiments on dynamic classifier and real-world cluster service allocation tasks validate the effectiveness of our methods and support our theoretical findings.},
  archive   = {C_KDD},
  author    = {Ma, Lanjihong and Zhang, Zhen-Yu and Ding, Yao-Xiang and Zhou, Zhi-Hua},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671812},
  pages     = {2130–2140},
  title     = {Handling varied objectives by online decision making},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PolyFormer: Scalable node-wise filters via polynomial graph
transformer. <em>KDD</em>, 2118–2129. (<a
href="https://doi.org/10.1145/3637528.3671849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Spectral Graph Neural Networks have demonstrated superior performance in graph representation learning. However, many current methods focus on employing shared polynomial coefficients for all nodes, i.e., learning node-unified filters, which limits the filters&#39; flexibility for node-level tasks. The recent DSF attempts to overcome this limitation by learning node-wise coefficients based on positional encoding. However, the initialization and updating process of the positional encoding are burdensome, hindering scalability on large-scale graphs. In this work, we propose a scalable node-wise filter, PolyAttn. Leveraging the attention mechanism, PolyAttn can directly learn node-wise filters in an efficient manner, offering powerful representation capabilities. Building on PolyAttn, we introduce the whole model, named PolyFormer. In the lens of Graph Transformer models, PolyFormer, which calculates attention scores within nodes, shows great scalability. Moreover, the model captures spectral information, enhancing expressiveness while maintaining efficiency. With these advantages, PolyFormer offers a desirable balance between scalability and expressiveness for node-level tasks. Extensive experiments demonstrate that our proposed methods excel at learning arbitrary node-wise filters, showing superior performance on both homophilic and heterophilic graphs, and handling graphs containing up to 100 million nodes. The code is available at https://github.com/air029/PolyFormer.},
  archive   = {C_KDD},
  author    = {Ma, Jiahong and He, Mingguo and Wei, Zhewei},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671849},
  pages     = {2118–2129},
  title     = {PolyFormer: Scalable node-wise filters via polynomial graph transformer},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Low rank multi-dictionary selection at scale. <em>KDD</em>,
2106–2117. (<a href="https://doi.org/10.1145/3637528.3671723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The sparse dictionary coding framework represents signals as a linear combination of a few predefined dictionary atoms. It has been employed for images, time series, graph signals and recently for 2-way (or 2D) spatio-temporal data employing jointly temporal and spatial dictionaries. Large and over-complete dictionaries enable high-quality models, but also pose scalability challenges which are exacerbated in multi-dictionary settings. Hence, an important problem that we address in this paper is: How to scale multi-dictionary coding for large dictionaries and datasets?We propose a multi-dictionary atom selection technique for low-rank sparse coding named LRMDS. To enable scalability to large dictionaries and datasets, it progressively selects groups of row-column atom pairs based on their alignment with the data and performs convex relaxation coding via the corresponding sub-dictionaries. We demonstrate both theoretically and experimentally that when the data has a low-rank encoding with a sparse subset of the atoms, LRMDS is able to select them with strong guarantees under mild assumptions. Furthermore, we demonstrate the scalability and quality of LRMDS in both synthetic and real-world datasets and for a range of coding dictionaries. It achieves 3 times to 10 times speed-up compared to baselines, while obtaining up to two orders of magnitude improvement in representation quality on some of the real world datasets given a fixed target number of atoms.},
  archive   = {C_KDD},
  author    = {Ma, Boya and McNeil, Maxwell and Magner, Abram and Bogdanov, Petko},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671723},
  pages     = {2106–2117},
  title     = {Low rank multi-dictionary selection at scale},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-context backdoor attacks against graph prompt
learning. <em>KDD</em>, 2094–2105. (<a
href="https://doi.org/10.1145/3637528.3671956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph Prompt Learning (GPL) bridges significant disparities between pretraining and downstream applications to alleviate the knowledge transfer bottleneck in real-world graph learning. While GPL offers superior effectiveness in graph knowledge transfer and computational efficiency, the security risks posed by backdoor poisoning effects embedded in pretrained models remain largely unexplored. Our study provides a comprehensive analysis of GPL&#39;s vulnerability to backdoor attacks. We introduce CrossBA, the first cross-context backdoor attack against GPL, which manipulates only the pretraining phase without requiring knowledge of downstream applications. Our investigation reveals both theoretically and empirically that tuning trigger graphs, combined with prompt transformations, can seamlessly transfer the backdoor threat from pretrained encoders to downstream applications.Through extensive experiments involving 3 representative GPL methods across 5 distinct cross-context scenarios and 5 benchmark datasets of node and graph classification tasks, we demonstrate that CrossBA consistently achieves high attack success rates while preserving the functionality of downstream applications over clean input. We also explore potential countermeasures against CrossBA and conclude that current defenses are insufficient to mitigate CrossBA. Our study highlights the persistent backdoor threats to GPL systems, raising trustworthiness concerns in the practices of GPL techniques.},
  archive   = {C_KDD},
  author    = {Lyu, Xiaoting and Han, Yufei and Wang, Wei and Qian, Hangwei and Tsang, Ivor and Zhang, Xiangliang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671956},
  pages     = {2094–2105},
  title     = {Cross-context backdoor attacks against graph prompt learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning multi-view molecular representations with
structured and unstructured knowledge. <em>KDD</em>, 2082–2093. (<a
href="https://doi.org/10.1145/3637528.3672043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Capturing molecular knowledge with representation learning approaches holds significant potential in vast scientific fields such as chemistry and life science. An effective and generalizable molecular representation is expected to capture the consensus and complementary molecular expertise from diverse views and perspectives. However, existing works fall short in learning multi-view molecular representations, due to challenges in explicitly incorporating view information and handling molecular knowledge from heterogeneous sources. To address these issues, we present MV-Mol, a molecular representation learning model that harvests multi-view molecular expertise from chemical structures, unstructured knowledge from biomedical texts, and structured knowledge from knowledge graphs. We utilize text prompts to model view information and design a fusion architecture to extract view-based molecular representations. We develop a two-stage pre-training procedure, exploiting heterogeneous data of varying quality and quantity. Through extensive experiments, we show that MV-Mol provides improved representations that substantially benefit molecular property prediction. Additionally, MV-Mol exhibits state-of-the-art performance in multi-modal comprehension of molecular structures and texts. Code and data are available at https://github.com/PharMolix/OpenBioMed.},
  archive   = {C_KDD},
  author    = {Luo, Yizhen and Yang, Kai and Hong, Massimo and Liu, Xing Yi and Nie, Zikun and Zhou, Hao and Nie, Zaiqing},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672043},
  pages     = {2082–2093},
  title     = {Learning multi-view molecular representations with structured and unstructured knowledge},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FUGNN: Harmonizing fairness and utility in graph neural
networks. <em>KDD</em>, 2072–2081. (<a
href="https://doi.org/10.1145/3637528.3671834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fairness-aware Graph Neural Networks (GNNs) often face a challenging trade-off, where prioritizing fairness may require compromising utility. In this work, we re-examine fairness through the lens of spectral graph theory, aiming to reconcile fairness and utility within the framework of spectral graph learning. We explore the correlation between sensitive features and spectrum in GNNs, using theoretical analysis to delineate the similarity between original sensitive features and those after convolution under different spectra. Our analysis reveals a reduction in the impact of similarity when the eigenvectors associated with the largest magnitude eigenvalue exhibit directional similarity. Based on these theoretical insights, we propose FUGNN, a novel spectral graph learning approach that harmonizes the conflict between fairness and utility. FUGNN ensures algorithmic fairness and utility by truncating the spectrum and optimizing eigenvector distribution during the encoding process. The fairness-aware eigenvector selection reduces the impact of convolution on sensitive features while concurrently minimizing the sacrifice of utility. FUGNN further optimizes the distribution of eigenvectors through a transformer architecture. By incorporating the optimized spectrum into the graph convolution network, FUGNN effectively learns node representations. Experiments on six real-world datasets demonstrate the superiority of FUGNN over baseline methods. The codes are available at https://github.com/yushuowiki/FUGNN.},
  archive   = {C_KDD},
  author    = {Luo, Renqiang and Huang, Huafei and Yu, Shuo and Han, Zhuoyang and He, Estrid and Zhang, Xiuzhen and Xia, Feng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671834},
  pages     = {2072–2081},
  title     = {FUGNN: Harmonizing fairness and utility in graph neural networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AdaGMLP: AdaBoosting GNN-to-MLP knowledge distillation.
<em>KDD</em>, 2060–2071. (<a
href="https://doi.org/10.1145/3637528.3671699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph Neural Networks (GNNs) have revolutionized graph-based machine learning, but their heavy computational demands pose challenges for latency-sensitive edge devices in practical industrial applications. In response, a new wave of methods, collectively known as GNN-to-MLP Knowledge Distillation, has emerged. They aim to transfer GNN-learned knowledge to a more efficient MLP student, which offers faster, resource-efficient inference while maintaining competitive performance compared to GNNs. However, these methods face significant challenges in situations with insufficient training data and incomplete test data, limiting their applicability in real-world applications. To address these challenges, we propose AdaGMLP, an AdaBoosting GNN-to-MLP Knowledge Distillation framework. It leverages an ensemble of diverse MLP students trained on different subsets of labeled nodes, addressing the issue of insufficient training data. Additionally, it incorporates a Node Alignment technique for robust predictions on test data with missing or incomplete features. Our experiments on seven benchmark datasets with different settings demonstrate that AdaGMLP outperforms existing G2M methods, making it suitable for a wide range of latency-sensitive real-world applications. We have submitted our code to the GitHub repository (https://github.com/WeigangLu/AdaGMLP-KDD24).},
  archive   = {C_KDD},
  author    = {Lu, Weigang and Guan, Ziyu and Zhao, Wei and Yang, Yaming},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671699},
  pages     = {2060–2071},
  title     = {AdaGMLP: AdaBoosting GNN-to-MLP knowledge distillation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural collapse inspired debiased representation learning
for min-max fairness. <em>KDD</em>, 2048–2059. (<a
href="https://doi.org/10.1145/3637528.3671902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although machine learning algorithms demonstrate impressive performance, their trustworthiness remains a critical issue, particularly concerning fairness when implemented in real-world applications. Many notions of group fairness aim to minimize disparities in performance across protected groups. However, it can inadvertently reduce performance in certain groups, leading to sub-optimal outcomes. In contrast, Min-max group fairness notion prioritizes the improvement for the worst-performing group, thereby advocating a utility-promoting approach to fairness. However, it has been proven that existing efforts to achieve Min-max fairness exhibit limited effectiveness. In response to this challenge, we leverage the recently proposed &quot;Neural Collapse&#39;&#39; framework to re-examine Empirical Risk Minimization (ERM) training, specifically investigating the root causes of poor performance in minority groups. The layer-peeled model is employed to decompose a network into two parts: an encoder to learn latent representation, and a subsequent classifier, with a systematic characterization of their training behaviors being conducted. Our analysis reveals that while classifiers achieve maximum separation, the separability of representations is insufficient, particularly for minority groups. This indicates the sub-optimal performance in minority groups stems from less separable representations, rather than classifiers. To tackle this issue, we introduce a novel strategy that incorporates a frozen classifier to directly enhance representation. Furthermore, we introduce two easily implemented loss functions to guide the learning process. The experimental assessments carried out on real-world benchmark datasets spanning the domains of Computer Vision, Natural Language Processing, and Tabular data demonstrate that our approach outperforms existing state-of-the-art methods in promoting the Min-max fairness notion.},
  archive   = {C_KDD},
  author    = {Lu, Shenyu and Chai, Junyi and Wang, Xiaoqian},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671902},
  pages     = {2048–2059},
  title     = {Neural collapse inspired debiased representation learning for min-max fairness},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-dimensional distributed sparse classification with
scalable communication-efficient global updates. <em>KDD</em>,
2037–2047. (<a href="https://doi.org/10.1145/3637528.3672038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As the size of datasets used in statistical learning continues to grow, distributed training of models has attracted increasing attention. These methods partition the data and exploit parallelism to reduce memory and runtime, but suffer increasingly from communication costs as the data size or the number of iterations grows. Recent work on linear models has shown that a surrogate likelihood can be optimized locally to iteratively improve on an initial solution in a communication-efficient manner. However, existing versions of these methods experience multiple shortcomings as the data size becomes massive, including diverging updates and efficiently handling sparsity. In this work we develop solutions to these problems which enable us to learn a communication-efficient distributed logistic regression model even beyond millions of features. In our experiments we demonstrate a large improvement in accuracy over distributed algorithms with only a few distributed update steps needed, and similar or faster runtimes. Our code is available at https://github.com/FutureComputing4AI/ProxCSL.},
  archive   = {C_KDD},
  author    = {Lu, Fred and Curtin, Ryan R. and Raff, Edward and Ferraro, Francis and Holt, James},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672038},
  pages     = {2037–2047},
  title     = {High-dimensional distributed sparse classification with scalable communication-efficient global updates},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Diffusion-based cloud-edge-device collaborative learning for
next POI recommendations. <em>KDD</em>, 2026–2036. (<a
href="https://doi.org/10.1145/3637528.3671743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The rapid expansion of Location-Based Social Networks (LBSNs) has highlighted the importance of effective next Point-of-Interest (POI) recommendations, which leverage historical check-in data to predict users&#39; next POIs to visit. Traditional centralized deep neural networks (DNNs) offer impressive POI recommendation performance but face challenges due to privacy concerns and limited timeliness. In response, on-device POI recommendations have been introduced, utilizing federated learning (FL) and decentralized approaches to ensure privacy and recommendation timeliness. However, these methods often suffer from computational strain on devices and struggle to adapt to new users and regions. This paper introduces a novel collaborative learning framework, Diffusion-Based Cloud-Edge-Device Collaborative Learning for Next POI Recommendations (DCPR), leveraging the diffusion model known for its success across various domains. DCPR operates with a cloud-edge-device architecture to offer region-specific and highly personalized POI recommendations while reducing on-device computational burdens. DCPR minimizes on-device computational demands through a unique blend of global and local learning processes. Our evaluation with two real-world datasets demonstrates DCPR&#39;s superior performance in recommendation accuracy, efficiency, and adaptability to new users and regions, marking a significant step forward in on-device POI recommendation technology.},
  archive   = {C_KDD},
  author    = {Long, Jing and Ye, Guanhua and Chen, Tong and Wang, Yang and Wang, Meng and Yin, Hongzhi},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671743},
  pages     = {2026–2036},
  title     = {Diffusion-based cloud-edge-device collaborative learning for next POI recommendations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AIM: Attributing, interpreting, mitigating data unfairness.
<em>KDD</em>, 2014–2025. (<a
href="https://doi.org/10.1145/3637528.3671797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Data collected in the real world often encapsulates historical discrimination against disadvantaged groups and individuals. Existing fair machine learning (FairML) research has predominantly focused on mitigating discriminative bias in the model prediction, with far less effort dedicated towards exploring how to trace biases present in the data, despite its importance for the transparency and interpretability of FairML. To fill this gap, we investigate a novel research problem: discovering samples that reflect biases/prejudices from the training data. Grounding on the existing fairness notions, we lay out a sample bias criterion and propose practical algorithms for measuring and countering sample bias. The derived bias score provides intuitive sample-level attribution and explanation of historical bias in data. On this basis, we further design two FairML strategies via sample-bias-informed minimal data editing. They can mitigate both group and individual unfairness at the cost of minimal or zero predictive utility loss. Extensive experiments and analyses on multiple real-world datasets demonstrate the effectiveness of our methods in explaining and mitigating unfairness. Code is available at https://github.com/ZhiningLiu1998/AIM.},
  archive   = {C_KDD},
  author    = {Liu, Zhining and Qiu, Ruizhong and Zeng, Zhichen and Zhu, Yada and Hamann, Hendrik and Tong, Hanghang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671797},
  pages     = {2014–2025},
  title     = {AIM: Attributing, interpreting, mitigating data unfairness},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generative pretrained hierarchical transformer for time
series forecasting. <em>KDD</em>, 2003–2013. (<a
href="https://doi.org/10.1145/3637528.3671855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent efforts have been dedicated to enhancing time series forecasting accuracy by introducing advanced network architectures and self-supervised pretraining strategies. Nevertheless, existing approaches still exhibit two critical drawbacks. Firstly, these methods often rely on a single dataset for training, limiting the model&#39;s generalizability due to the restricted scale of the training data. Secondly, the one-step generation schema is widely followed, which necessitates a customized forecasting head and overlooks the temporal dependencies in the output series, and also leads to increased training costs under different horizon length settings. To address these issues, we propose a novel generative pretrained hierarchical transformer architecture for forecasting, named GPHT. There are two aspects of key designs in GPHT. On the one hand, we advocate for constructing a mixed dataset under the channel-independent assumption for pretraining our model, comprising various datasets from diverse data scenarios. This approach significantly expands the scale of training data, allowing our model to uncover commonalities in time series data and facilitating improved transfer to specific datasets. On the other hand, GPHT employs an auto-regressive forecasting approach, effectively modeling temporal dependencies in the output series. Importantly, no customized forecasting head is required, enablinga single model to forecast at arbitrary horizon settings. We conduct sufficient experiments on eight datasets with mainstream self-supervised pretraining models and supervised models. The results demonstrated that GPHT surpasses the baseline models across various fine-tuning and zero/few-shot learning settings in the traditional long-term forecasting task, providing support for verifying the feasibility of pretraining time series large models. We make our codes publicly availablefootnotehttps://github.com/icantnamemyself/GPHT.},
  archive   = {C_KDD},
  author    = {Liu, Zhiding and Yang, Jiqian and Cheng, Mingyue and Luo, Yucong and Li, Zhi},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671855},
  pages     = {2003–2013},
  title     = {Generative pretrained hierarchical transformer for time series forecasting},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph data condensation via self-expressive graph structure
reconstruction. <em>KDD</em>, 1992–2002. (<a
href="https://doi.org/10.1145/3637528.3671710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the increasing demands of training graph neural networks (GNNs) on large-scale graphs, graph data condensation has emerged as a critical technique to relieve the storage and time costs during the training phase. It aims to condense the original large-scale graph to a much smaller synthetic graph while preserving the essential information necessary for efficiently training a downstream GNN. However, existing methods concentrate either on optimizing node features exclusively or endeavor to independently learn node features and the graph structure generator. They could not explicitly leverage the information of the original graph structure and failed to construct an interpretable graph structure for the synthetic dataset. To address these issues, we introduce a novel framework named Graph Data Condensation via Self-expressive Graph Structure Reconstruction (GCSR). Our method stands out by (1) explicitly incorporating the original graph structure into the condensing process and (2) capturing the nuanced interdependencies between the condensed nodes by reconstructing an interpretable self-expressive graph structure. Extensive experiments and comprehensive analysis validate the efficacy of the proposed method across diverse GNN models and datasets. Our code is available at https://github.com/zclzcl0223/GCSR.},
  archive   = {C_KDD},
  author    = {Liu, Zhanyu and Zeng, Chaolv and Zheng, Guanjie},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671710},
  pages     = {1992–2002},
  title     = {Graph data condensation via self-expressive graph structure reconstruction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dataset condensation for time series classification via dual
domain matching. <em>KDD</em>, 1980–1991. (<a
href="https://doi.org/10.1145/3637528.3671675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Time series data has been demonstrated to be crucial in various research fields. The management of large quantities of time series data presents challenges in terms of deep learning tasks, particularly for training a deep neural network. Recently, a technique named Dataset Condensation has emerged as a solution to this problem. This technique generates a smaller synthetic dataset that has comparable performance to the full real dataset in downstream tasks such as classification. However, previous methods are primarily designed for image and graph datasets, and directly adapting them to the time series dataset leads to suboptimal performance due to their inability to effectively leverage the rich information inherent in time series data, particularly in the frequency domain. In this paper, we propose a novel framework named Dataset Condensation for Time Series Classification via Dual Domain Matching (CondTSC) which focuses on the time series classification dataset condensation task. Different from previous methods, our proposed framework aims to generate a condensed dataset that matches the surrogate objectives in both the time and frequency domains. Specifically, CondTSC incorporates multi-view data augmentation, dual domain training, and dual surrogate objectives to enhance the dataset condensation process in the time and frequency domains. Through extensive experiments, we demonstrate the effectiveness of our proposed framework, which outperforms other baselines and learns a condensed synthetic dataset that exhibits desirable characteristics such as conforming to the distribution of the original data.},
  archive   = {C_KDD},
  author    = {Liu, Zhanyu and Hao, Ke and Zheng, Guanjie and Yu, Yanwei},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671675},
  pages     = {1980–1991},
  title     = {Dataset condensation for time series classification via dual domain matching},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Revisiting modularity maximization for graph clustering: A
contrastive learning perspective. <em>KDD</em>, 1968–1979. (<a
href="https://doi.org/10.1145/3637528.3671967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph clustering, a fundamental and challenging task in graph mining, aims to classify nodes in a graph into several disjoint clusters. In recent years, graph contrastive learning (GCL) has emerged as a dominant line of research in graph clustering and advances the new state-of-the-art. However, GCL-based methods heavily rely on graph augmentations and contrastive schemes, which may potentially introduce challenges such as semantic drift and scalability issues. Another promising line of research involves the adoption of modularity maximization, a popular and effective measure for community detection, as the guiding principle for clustering tasks. Despite the recent progress, the underlying mechanism of modularity maximization is still not well understood. In this work, we dig into the hidden success of modularity maximization for graph clustering. Our analysis reveals the strong connections between modularity maximization and graph contrastive learning, where positive and negative examples are naturally defined by modularity. In light of our results, we propose a community-aware graph clustering framework, coined \o{}urs, which leverages modularity maximization as a contrastive pretext task to effectively uncover the underlying information of communities in graphs, while avoiding the problem of semantic drift. Extensive experiments on multiple graph datasets verify the effectiveness of \o{}urs in terms of scalability and clustering performance compared to state-of-the-art graph clustering methods. Notably, \o{}urs easily scales a sufficiently large graph with 100M nodes while outperforming strong baselines.},
  archive   = {C_KDD},
  author    = {Liu, Yunfei and Li, Jintang and Chen, Yuehe and Wu, Ruofan and Wang, Ericbk and Zhou, Jing and Tian, Sheng and Shen, Shuheng and Fu, Xing and Meng, Changhua and Wang, Weiqiang and Chen, Liang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671967},
  pages     = {1968–1979},
  title     = {Revisiting modularity maximization for graph clustering: A contrastive learning perspective},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Probabilistic attention for sequential recommendation.
<em>KDD</em>, 1956–1967. (<a
href="https://doi.org/10.1145/3637528.3671733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sequential Recommendation (SR) navigates users&#39; dynamic preferences through modeling their historical interactions. The incorporation of the popular Transformer framework, which captures long relationships through pairwise dot products, has notably benefited SR. However, prevailing research in this domain faces three significant challenges: (i) Existing studies directly adopt the primary component of Transformer (i.e., the self-attention mechanism), without a clear explanation or tailored definition for its specific role in SR; (ii) The predominant focus on pairwise computations overlooks the global context or relative prevalence of item pairs within the overall sequence; (iii) Transformer primarily pursues relevance-dominated relationships, neglecting another essential objective in recommendation, i.e., diversity. In response, this work introduces a fresh perspective to elucidate the attention mechanism in SR. Here, attention is defined as dependency interactions among items, quantitatively determined under a global probabilistic model by observing the probabilities of corresponding item subsets. This viewpoint offers a precise and context-specific definition of attention, leading to the design of a distinctive attention mechanism tailored for SR. Specifically, we transmute the well-formulated global, repulsive interactions in Determinantal Point Processes (DPPs) to effectively model dependency interactions. Guided by the repulsive interactions, a theoretically and practically feasible DPP kernel is designed, enabling our attention mechanism to directly consider category/topic distribution for enhancing diversity. Consequently, the &amp;lt;u&amp;gt;P&amp;lt;/u&amp;gt;robabilistic &amp;lt;u&amp;gt;Att&amp;lt;/u&amp;gt;ention mechanism (PAtt) for sequential recommendation is developed. Experimental results demonstrate the excellent scalability and adaptability of our attention mechanism, which significantly improves recommendation performance in terms of both relevance and diversity.},
  archive   = {C_KDD},
  author    = {Liu, Yuli and Walder, Christian and Xie, Lexing and Liu, Yiqun},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671733},
  pages     = {1956–1967},
  title     = {Probabilistic attention for sequential recommendation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). BadSampler: Harnessing the power of catastrophic forgetting
to poison byzantine-robust federated learning. <em>KDD</em>, 1944–1955.
(<a href="https://doi.org/10.1145/3637528.3671879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Federated Learning (FL) is susceptible to poisoning attacks, wherein compromised clients manipulate the global model by modifying local datasets or sending manipulated model updates. Experienced defenders can readily detect and mitigate the poisoning effects of malicious behaviors using Byzantine-robust aggregation rules. However, the exploration of poisoning attacks in scenarios where such behaviors are absent remains largely unexplored for Byzantine-robust FL. This paper addresses the challenging problem of poisoning Byzantine-robust FL by introducing catastrophic forgetting. To fill this gap, we first formally define generalization error and establish its connection to catastrophic forgetting, paving the way for the development of a clean-label data poisoning attack named BadSampler. This attack leverages only clean-label data (i.e., without poisoned data) to poison Byzantine-robust FL and requires the adversary to selectively sample training data with high loss to feed model training and maximize the model&#39;s generalization error. We formulate the attack as an optimization problem and present two elegant adversarial sampling strategies, Top-k sampling, and meta-sampling, to approximately solve it. Additionally, our formal error upper bound and time complexity analysis demonstrate that our design can preserve attack utility with high efficiency. Extensive evaluations on two real-world datasets illustrate the effectiveness and performance of our proposed attacks.},
  archive   = {C_KDD},
  author    = {Liu, Yi and Wang, Cong and Yuan, Xingliang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671879},
  pages     = {1944–1955},
  title     = {BadSampler: Harnessing the power of catastrophic forgetting to poison byzantine-robust federated learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ACER: Accelerating complex event recognition via two-phase
filtering under range bitmap-based indexes. <em>KDD</em>, 1933–1943. (<a
href="https://doi.org/10.1145/3637528.3671814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Complex event recognition (CER) refers to identifying specific patterns composed of several primitive events in event stores. Since full-scanning event stores to identify primitive events holding query constraint conditions will incur costly I/O overhead, a mainstream and practical approach is using index techniques to obtain these events. However, prior index-based approaches suffer from significant I/O and sorting overhead when dealing with high predicate selectivity or long query window (common in real-world applications), which leads to high query latency. To address this issue, we propose ACER, a Range Bitmap-based index, to accelerate CER. Firstly, ACER achieves a low index space overhead by grouping the events with the same type into a cluster and compressing the cluster data, alleviating the I/O overhead of reading indexes. Secondly, ACER builds Range Bitmaps in batch (block) for queried attributes and ensures that the events of each cluster in the index block are chronologically ordered. Then, ACER can always obtain ordered query results for a specific event type through merge operations, avoiding sorting overhead. Most importantly, ACER avoids unnecessary disk access in indexes and events via two-phase filtering based on the window condition, thus alleviating the I/O overhead further. Our experiments on six real-world and synthetic datasets demonstrate that ACER reduces the query latency by up to one order of magnitude compared with SOTA techniques.},
  archive   = {C_KDD},
  author    = {Liu, Shizhe and Dai, Haipeng and Song, Shaoxu and Li, Meng and Dai, Jingsong and Gu, Rong and Chen, Guihai},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671814},
  pages     = {1933–1943},
  title     = {ACER: Accelerating complex event recognition via two-phase filtering under range bitmap-based indexes},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An unsupervised learning framework combined with heuristics
for the maximum minimal cut problem. <em>KDD</em>, 1921–1932. (<a
href="https://doi.org/10.1145/3637528.3671704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Maximum Minimal Cut Problem (MMCP), a NP-hard combinatorial optimization (CO) problem, has not received much attention due to the demanding and challenging bi-connectivity constraint. Moreover, as a CO problem, it is also a daunting task for machine learning, especially without labeled instances. To deal with these problems, this work proposes an unsupervised learning framework combined with heuristics for MMCP that can provide valid and high-quality solutions. As far as we know, this is the first work that explores machine learning and heuristics to solve MMCP. The unsupervised solver is inspired by a relaxation-plus-rounding approach, the relaxed solution is parameterized by graph neural networks, and the cost and penalty of MMCP are explicitly written out, which can train the model end-to-end. A crucial observation is that each solution corresponds to at least one spanning tree. Based on this finding, a heuristic solver that implements tree transformations by adding vertices is utilized to repair and improve the solution quality of the unsupervised solver. Alternatively, the graph is simplified while guaranteeing solution consistency, which reduces the running time. We conduct extensive experiments to evaluate our framework and give a specific application. The results demonstrate the superiority of our method against two techniques designed.},
  archive   = {C_KDD},
  author    = {Liu, Huaiyuan and Liu, Xianzhang and Yang, Donghua and Wang, Hongzhi and Long, Yingchi and Ji, Mengtong and Miao, Dongjing and Liang, Zhiyu},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671704},
  pages     = {1921–1932},
  title     = {An unsupervised learning framework combined with heuristics for the maximum minimal cut problem},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Asymmetric beta loss for evidence-based safe semi-supervised
multi-label learning. <em>KDD</em>, 1909–1920. (<a
href="https://doi.org/10.1145/3637528.3671756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The goal of semi-supervised multi-label learning (SSMLL) is to improve model performance by leveraging the information of unlabeled data. Recent studies usually adopt the pseudo-labeling strategy to tackle unlabeled data based on the assumption that labeled and unlabeled data share the same distribution. However, in realistic scenarios, unlabeled examples are often collected through cost-effective methods, inevitably introducing out-of-distribution (OOD) data, leading to a significant decline in model performance. In this paper, we propose a safe semi-supervised multi-label learning framework based on the theory of evidential deep learning (EDL), with the goal of achieving robust and effective unlabeled data exploitation. On one hand, we propose the asymmetric beta loss to not only compensate for the lack of robustness in common MLL losses, but also to solve the inherent positive-negative imbalance problem faced by the EDL losses in MLL. On the other hand, to construct a robust SSMLL framework, we adopt a dual-head structure to generate class probabilities and instance uncertainties. The former are used to generate pseudo-labels, while the latter are utilized to filter OOD examples. To avoid the need for threshold estimation, we develop a dual-measurement weighted loss function to safely perform unlabeled training. Extensive experiments on multiple benchmark datasets verify the effectiveness of the proposed method in both OOD detection and SSMLL tasks.},
  archive   = {C_KDD},
  author    = {Liu, Hao-Zhe and Xie, Ming-Kun and Zong, Chen-Chen and Huang, Sheng-Jun},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671756},
  pages     = {1909–1920},
  title     = {Asymmetric beta loss for evidence-based safe semi-supervised multi-label learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-task learning for routing problem with cross-problem
zero-shot generalization. <em>KDD</em>, 1898–1908. (<a
href="https://doi.org/10.1145/3637528.3672040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vehicle routing problems (VRP) are very important in many real-world applications and has been studied for several decades. Recently, neural combinatorial optimization (NCO) has attracted growing research effort. NCO is to train a neural network model to solve an optimization problem in question. However, existing NCO methods often build a different model for each routing problem, which significantly hinders their application in some areas where there are many different VRP variants to solve. In this work, we make a first attempt to tackle the crucial challenge of cross-problem generalization in NCO. We formulate VRPs as different combinations of a set of shared underlying attributes and solve them simultaneously via a single model through attribute composition. In this way, our proposed model can successfully solve VRPs with unseen attribute combinations in a zero-shot generalization manner. In our experiments, the neural model is trained on five VRP variants and its performance is tested on eleven VRP variants. The experimental results show that the model demonstrates superior performance on these eleven VRP variants, reducing the average gap to around 5\% from over 20\% and achieving a notable performance boost on both benchmark datasets and real-world logistics scenarios.},
  archive   = {C_KDD},
  author    = {Liu, Fei and Lin, Xi and Wang, Zhenkun and Zhang, Qingfu and Xialiang, Tong and Yuan, Mingxuan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672040},
  pages     = {1898–1908},
  title     = {Multi-task learning for routing problem with cross-problem zero-shot generalization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast query of biharmonic distance in networks. <em>KDD</em>,
1887–1897. (<a href="https://doi.org/10.1145/3637528.3671856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Thebiharmonic distance (BD) is a fundamental metric that measures the distance of two nodes in a graph. It has found applications in network coherence, machine learning, and computational graphics, among others. In spite of BD&#39;s importance, efficient algorithms for the exact computation or approximation of this metric on large graphs remain notably absent. In this work, we provide several algorithms to estimate BD, building on a novel formulation of this metric. These algorithms enjoy locality property (that is, they only read a small portion of the input graph) and at the same time possess provable performance guarantees. In particular, our main algorithms approximate the BD between any node pair with an arbitrarily small additive error ε in time O(1/ε2 poly(log n/ε)). Furthermore, we perform an extensive empirical study on several benchmark networks, validating the performance and accuracy of our algorithms.},
  archive   = {C_KDD},
  author    = {Liu, Changan and Zehmakan, Ahad N. and Zhang, Zhongzhi},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671856},
  pages     = {1887–1897},
  title     = {Fast query of biharmonic distance in networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TDNetGen: Empowering complex network resilience prediction
with generative augmentation of topology and dynamics. <em>KDD</em>,
1875–1886. (<a href="https://doi.org/10.1145/3637528.3671934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Predicting the resilience of complex networks, which represents the ability to retain fundamental functionality amidst external perturbations or internal failures, plays a critical role in understanding and improving real-world complex systems. Traditional theoretical approaches grounded in nonlinear dynamical systems rely on prior knowledge of network dynamics. On the other hand, data-driven approaches frequently encounter the challenge of insufficient labeled data, a predicament commonly observed in real-world scenarios. In this paper, we introduce a novel resilience prediction framework for complex networks, designed to tackle this issue through generative data augmentation of network topology and dynamics. The core idea is the strategic utilization of the inherent joint distribution present in unlabeled network data, facilitating the learning process of the resilience predictor by illuminating the relationship between network topology and dynamics. Experiment results on three network datasets demonstrate that our proposed framework TDNetGen can achieve high prediction accuracy up to 85\%-95\%. Furthermore, the framework still demonstrates a pronounced augmentation capability in extreme low-data regimes, thereby underscoring its utility and robustness in enhancing the prediction of network resilience. We have open-sourced our code in the following link, https://github.com/tsinghua-fib-lab/TDNetGen.},
  archive   = {C_KDD},
  author    = {Liu, Chang and Ding, Jingtao and Song, Yiwen and Li, Yong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671934},
  pages     = {1875–1886},
  title     = {TDNetGen: Empowering complex network resilience prediction with generative augmentation of topology and dynamics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FAST: An optimization framework for fast additive
segmentation in transparent ML. <em>KDD</em>, 1863–1874. (<a
href="https://doi.org/10.1145/3637528.3671996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present FAST, an optimization framework for fast additive segmentation. FAST segments piecewise constant shape functions for each feature in a dataset to produce transparent additive models. The framework leverages a novel optimization procedure to fit these models ~2 orders of magnitude faster than existing state-of-the-art methods, such as explainable boosting machines[20]. We also develop new feature selection algorithms in the FAST framework to fit parsimonious models that perform well. Through experiments and case studies, we show that FAST improves the computational efficiency and interpretability of additive models.},
  archive   = {C_KDD},
  author    = {Liu, Brian and Mazumder, Rahul},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671996},
  pages     = {1863–1874},
  title     = {FAST: An optimization framework for fast additive segmentation in transparent ML},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CASA: Clustered federated learning with asynchronous
clients. <em>KDD</em>, 1851–1862. (<a
href="https://doi.org/10.1145/3637528.3671979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Clustered Federated Learning (CFL) is an emerging paradigm to extract insights from data on IoT devices. Through iterative client clustering and model aggregation, CFL adeptly manages data heterogeneity, ensures privacy, and delivers personalized models to heterogeneous devices. Traditional CFL approaches, which operate synchronously, suffer from prolonged latency for waiting slow devices during clustering and aggregation. This paper advocates a shift to asynchronous CFL, allowing the server to process client updates as they arrive. This shift enhances training efficiency yet introduces complexities to the iterative training cycle. To this end, we present CASA, a novel CFL scheme for Clustering-Aggregation Synergy under Asynchrony. Built upon a holistic theoretical understanding of asynchrony&#39;s impact on CFL, CASA adopts a bi-level asynchronous aggregation method and a buffer-aided dynamic clustering strategy to harmonize between clustering and aggregation. Extensive evaluations on standard benchmarks show that CASA outperforms representative baselines in model accuracy and achieves 2.28-6.49\texttimes{} higher convergence speed.},
  archive   = {C_KDD},
  author    = {Liu, Boyi and Ma, Yiming and Zhou, Zimu and Shi, Yexuan and Li, Shuyuan and Tong, Yongxin},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671979},
  pages     = {1851–1862},
  title     = {CASA: Clustered federated learning with asynchronous clients},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CONFIDE: Contextual finite difference modelling of PDEs.
<em>KDD</em>, 1839–1850. (<a
href="https://doi.org/10.1145/3637528.3671676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a method for inferring an explicit PDE from a data sample generated by previously unseen dynamics, based on a learned context. The training phase integrates knowledge of the form of the equation with a differential scheme, while the inference phase yields a PDE that fits the data sample and enables both signal prediction and data explanation. We include results of extensive experimentation, comparing our method to SOTA approaches, together with ablation studies that examine different flavors of our solution.},
  archive   = {C_KDD},
  author    = {Linial, Ori and Avner, Orly and Di Castro, Dotan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671676},
  pages     = {1839–1850},
  title     = {CONFIDE: Contextual finite difference modelling of PDEs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the convergence of zeroth-order federated tuning for
large language models. <em>KDD</em>, 1827–1838. (<a
href="https://doi.org/10.1145/3637528.3671865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The confluence of Federated Learning (FL) and Large Language Models (LLMs) is ushering in a new era in privacy-preserving natural language processing. However, the intensive memory requirements for fine-tuning LLMs pose significant challenges, especially when deploying on clients with limited computational resources. To circumvent this, we explore the novel integration of Memory-efficient Zeroth-Order Optimization within a federated setting, a synergy we term as FedMeZO. Our study is the first to examine the theoretical underpinnings of FedMeZO in the context of LLMs, tackling key questions regarding the influence of large parameter spaces on optimization behavior, the establishment of convergence properties, and the identification of critical parameters for convergence to inform personalized federated strategies. Our extensive empirical evidence supports the theory, showing that FedMeZO not only converges faster than traditional first-order methods such as FedAvg but also significantly reduces GPU memory usage during training to levels comparable to those during inference. Moreover, the proposed personalized FL strategy that is built upon the theoretical insights to customize the client-wise learning rate can effectively accelerate loss reduction. We hope our work can help to bridge theoretical and practical aspects of federated fine-tuning for LLMs, thereby stimulating further advancements and research in this area.},
  archive   = {C_KDD},
  author    = {Ling, Zhenqing and Chen, Daoyuan and Yao, Liuyi and Li, Yaliang and Shen, Ying},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671865},
  pages     = {1827–1838},
  title     = {On the convergence of zeroth-order federated tuning for large language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bridging items and language: A transition paradigm for large
language model-based recommendation. <em>KDD</em>, 1816–1826. (<a
href="https://doi.org/10.1145/3637528.3671884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Harnessing Large Language Models (LLMs) for recommendation is rapidly emerging, which relies on two fundamental steps to bridge the recommendation item space and the language space: 1) item indexing utilizes identifiers to represent items in the language space, and 2) generation grounding associates LLMs&#39; generated token sequences to in-corpus items. However, previous methods exhibit inherent limitations in the two steps. Existing ID-based identifiers (e.g., numeric IDs) and description-based identifiers (e.g., titles) either lose semantics or lack adequate distinctiveness. Moreover, prior generation grounding methods might generate invalid identifiers, thus misaligning with in-corpus items. To address these issues, we propose a novel Transition paradigm for LLM-based Recommender (named TransRec) to bridge items and language. Specifically, TransRec presents multi-facet identifiers, which simultaneously incorporate ID, title, and attribute for item indexing to pursue both distinctiveness and semantics. Additionally, we introduce a specialized data structure for TransRec to ensure generating valid identifiers only and utilize substring indexing to encourage LLMs to generate from any position of identifiers. Lastly, TransRec presents an aggregated grounding module to leverage generated multi-facet identifiers to rank in-corpus items efficiently. We instantiate TransRec on two backbone models, BART-large and LLaMA-7B.},
  archive   = {C_KDD},
  author    = {Lin, Xinyu and Wang, Wenjie and Li, Yongqi and Feng, Fuli and Ng, See-Kiong and Chua, Tat-Seng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671884},
  pages     = {1816–1826},
  title     = {Bridging items and language: A transition paradigm for large language model-based recommendation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust auto-bidding strategies for online advertising.
<em>KDD</em>, 1804–1815. (<a
href="https://doi.org/10.1145/3637528.3671729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In online advertising, existing auto-bidding strategies for bid shading mainly adopt the approach of first predicting the winning price distribution and then calculating the optimal bid. However, the winning price information available to the Demand Side Platforms (DSPs) is extremely limited, and the associated uncertainties make it challenging for DSPs to accurately estimate winning price distribution. To address this challenge, we conducted a comprehensive analysis of the process by which DSPs obtain winning price information, and abstracted two types of uncertainties from it: known uncertainty and unknown uncertainty. Based on these uncertainties, we proposed two levels of robust bidding strategies: Robust Bidding for Censorship (RBC) and Robust Bidding for Distribution Shift (RBDS), which offer guarantees for the surplus in the worst-case scenarios under uncertain conditions. Experimental results on public datasets demonstrate that our robust bidding strategies consistently enable DSPs to achieve superior surpluses, both on test sets and under worst-case conditions.},
  archive   = {C_KDD},
  author    = {Lin, Qilong and Zheng, Zhenzhe and Wu, Fan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671729},
  pages     = {1804–1815},
  title     = {Robust auto-bidding strategies for online advertising},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PSMC: Provable and scalable algorithms for motif conductance
based graph clustering. <em>KDD</em>, 1793–1803. (<a
href="https://doi.org/10.1145/3637528.3671666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Higher-order graph clustering aims to partition the graph using frequently occurring subgraphs (i.e., motifs), instead of the lower-order edges, as the atomic clustering unit, which has been recognized as the state-of-the-art solution in ground truth community detection and knowledge discovery. Motif conductance is one of the most promising higher-order graph clustering models due to its strong interpretability. However, existing motif conductance based graph clustering algorithms are mainly limited by a seminal two-stage reweighting computing framework, needing to enumerate all motif instances to obtain an edge-weighted graph for partitioning. However, such a framework has two-fold vital defects: (1) It can only provide a quadratic bound for the motif with three vertices, and whether there is provable clustering quality for other motifs is still an open question. (2) The enumeration procedure of motif instances incurs prohibitively high costs against large motifs or large dense graphs due to combinatorial explosions. Besides, expensive spectral clustering or local graph diffusion on the edge-weighted graph also makes existing methods unable to handle massive graphs with millions of nodes. To overcome these dilemmas, we propose a &amp;lt;u&amp;gt;P&amp;lt;/u&amp;gt;rovable and &amp;lt;u&amp;gt;S&amp;lt;/u&amp;gt;calable &amp;lt;u&amp;gt;M&amp;lt;/u&amp;gt;otif &amp;lt;u&amp;gt;C&amp;lt;/u&amp;gt;onductance algorithm PSMC, which has a fixed and motif-independent approximation ratio for any motif. Specifically, PSMC first defines a new vertex metric Motif Resident based on the given motif, which can be computed locally. Then, it iteratively deletes the vertex with the smallest motif resident value very efficiently using novel dynamic update technologies. Finally, it outputs the locally optimal result during the above iterative process. To further boost efficiency, we propose several effective bounds to estimate the motif resident value of each vertex, which can greatly reduce computational costs. Empirical results on real-life and synthetic demonstrate that our proposed algorithms achieve 3.2-32 times speedup and improve the quality by at least 12 times than the state-of-the art baselines.},
  archive   = {C_KDD},
  author    = {Lin, Longlong and Jia, Tao and Wang, Zeli and Zhao, Jin and Li, Rong-Hua},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671666},
  pages     = {1793–1803},
  title     = {PSMC: Provable and scalable algorithms for motif conductance based graph clustering},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MulSTE: A multi-view spatio-temporal learning framework with
heterogeneous event fusion for demand-supply prediction. <em>KDD</em>,
1781–1792. (<a href="https://doi.org/10.1145/3637528.3672030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, integrated warehouse and distribution logistics systems are widely used in E-commerce industries to adjust to constantly changing customer demands. It makes the prediction of purchase demand and delivery supply capacity a crucial problem to streamline operations and improve efficiency. The interaction between such demand and supply not only relies on their economic relationships but also on consumer psychology caused by daily events, such as epidemics, promotions, and festivals. Although existing studies have made great efforts in the joint prediction of demand and supply considering modeling the demand-supply interactions, they seldom refer to the impacts of diverse events. In this work, we propose MulSTE, a Multi-view Spatio-Temporal learning framework with heterogeneous Event fusion. Firstly, an Event Fusion Representation (EFR) module is designed to fuse the textual, numerical, and categorical heterogeneous information for emergent and periodic events. Secondly, a Multi-graph Adaptive Convolution Recurrent Network (MGACRN) is developed as the spatio-temporal encoder (ST-Encoder) to capture the evolutional features of demand, supply, and events. Thirdly, the Event Gated Demand-Supply Interaction Attention (EGIA) module is designed to model the demand-supply interactions during events. The evaluations are conducted on two real-world datasets collected from JD Logistics and public websites. The experimental results show that our method outperforms state-of-the-art baselines in various metrics.},
  archive   = {C_KDD},
  author    = {Lin, Li and Lu, Zhiqiang and Wang, Shuai and Liu, Yunhuai and Hong, Zhiqing and Wang, Haotian and Wang, Shuai},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672030},
  pages     = {1781–1792},
  title     = {MulSTE: A multi-view spatio-temporal learning framework with heterogeneous event fusion for demand-supply prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). When box meets graph neural network in tag-aware
recommendation. <em>KDD</em>, 1770–1780. (<a
href="https://doi.org/10.1145/3637528.3671973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Last year has witnessed the re-flourishment of tag-aware recommender systems supported by the LLM-enriched tags. Unfortunately, though large efforts have been made, current solutions may fail to describe the diversity and uncertainty inherent in user preferences with only tag-driven profiles. Recently, with the development of geometry-based techniques, e.g., box embeddings, the diversity of user preferences now could be fully modeled as the range within a box in high dimension space. However, defect still exists as these approaches are incapable of capturing high-order neighbor signals, i.e., semantic-rich multi-hop relations within the user-tag-item tripartite graph, which severely limits the effectiveness of user modeling. To deal with this challenge, in this paper, we propose a novel framework, called BoxGNN, to perform message aggregation via combinations of logical operations, thereby incorporating high-order signals. Specifically, we first embed users, items, and tags as hyper-boxes rather than simple points in the representation space, and define two logical operations, i.e., union and intersection, to facilitate the subsequent process. Next, we perform the message aggregation mechanism via the combination of logical operations, to obtain the corresponding high-order box representations. Finally, we adopt a volume-based learning objective with Gumbel smoothing techniques to refine the representation of boxes. Extensive experiments on two publicly available datasets and one LLM-enhanced e-commerce dataset have validated the superiority of BoxGNN compared with various state-of-the-art baselines. The code is released online: https://github.com/critical88/BoxGNN.},
  archive   = {C_KDD},
  author    = {Lin, Fake and Zhao, Ziwei and Zhu, Xi and Zhang, Da and Shen, Shitian and Li, Xueying and Xu, Tong and Zhang, Suojuan and Chen, Enhong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671973},
  pages     = {1770–1780},
  title     = {When box meets graph neural network in tag-aware recommendation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Image similarity using an ensemble of context-sensitive
models. <em>KDD</em>, 1758–1769. (<a
href="https://doi.org/10.1145/3637528.3672004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image similarity has been extensively studied in computer vision. In recent years, machine-learned models have shown their ability to encode more semantics than traditional multivariate metrics. However, in labelling semantic similarity, assigning a numerical score to a pair of images is impractical, making the improvement and comparisons on the task difficult. In this work, we present a more intuitive approach to build and compare image similarity models based on labelled data in the form of A:R vs B:R, i.e., determining if an image A is closer to a reference image R than another image B. We address the challenges of sparse sampling in the image space (R, A, B) and biases in the models trained with context-based data by using an ensemble model. Our testing results show that the ensemble model constructed performs ~5\% better than the best individual context-sensitive models. They also performed better than the models that were directly fine-tuned using mixed imagery data as well as existing deep embeddings, e.g., CLIP [30] and DINO [3]. This work demonstrates that context-based labelling and model training can be effective when an appropriate ensemble approach is used to alleviate the limitation due to sparse sampling.},
  archive   = {C_KDD},
  author    = {Liao, Zukang and Chen, Min},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672004},
  pages     = {1758–1769},
  title     = {Image similarity using an ensemble of context-sensitive models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Customizing graph neural network for CAD assembly
recommendation. <em>KDD</em>, 1746–1757. (<a
href="https://doi.org/10.1145/3637528.3671788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {CAD assembly modeling, which refers to using CAD software to design new products from a catalog of existing machine components, is important in the industrial field. The graph neural network (GNN) based recommender system for CAD assembly modeling can help designers make decisions and speed up the design process by recommending the next required component based on the existing components in CAD software. These components can be represented as a graph naturally. However, present recommender systems for CAD assembly modeling adopt fixed GNN architectures, which may be sub-optimal for different manufacturers with different data distribution. Therefore, to customize a well-suited recommender system for different manufacturers, we propose a novel neural architecture search (NAS) framework, dubbed CusGNN, which can design data-specific GNN automatically. Specifically, we design a search space from three dimensions (i.e., aggregation, fusion, and readout functions), which contains a wide variety of GNN architectures. Then, we develop an effective differentiable search algorithm to search high-performing GNN from the search space. Experimental results show that the customized GNNs achieve 1.5-5.1\% higher top-10 accuracy compared to previous manual designed methods, demonstrating the superiority of the proposed approach. Code and data are available at https://github.com/BUPT-GAMMA/CusGNN.},
  archive   = {C_KDD},
  author    = {Liang, Fengqi and Zhao, Huan and Quan, Yuhan and Fang, Wei and Shi, Chuan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671788},
  pages     = {1746–1757},
  title     = {Customizing graph neural network for CAD assembly recommendation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rethinking fair graph neural networks from re-balancing.
<em>KDD</em>, 1736–1745. (<a
href="https://doi.org/10.1145/3637528.3671826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Driven by the powerful representation ability of Graph Neural Networks (GNNs), plentiful GNN models have been widely deployed in many real-world applications. Nevertheless, due to distribution disparities between different demographic groups, fairness in high-stake decision-making systems is receiving increasing attention. Although lots of recent works devoted to improving the fairness of GNNs and achieved considerable success, they all require significant architectural changes or additional loss functions requiring more hyper-parameter tuning. Surprisingly, we find that simple re-balancing methods can easily match or surpass existing fair GNN methods. We claim that the imbalance across different demographic groups is a significant source of unfairness, resulting in imbalanced contributions from each group to the parameters updating. However, these simple re-balancing methods have their own shortcomings during training. In this paper, we propose FairGB, &amp;lt;u&amp;gt;Fair&amp;lt;/u&amp;gt; &amp;lt;u&amp;gt;G&amp;lt;/u&amp;gt;raph Neural Network via re-&amp;lt;u&amp;gt;B&amp;lt;/u&amp;gt;alancing, which mitigates the unfairness of GNNs by group balancing. Technically, FairGB consists of two modules: counterfactual node mixup and contribution alignment loss. Firstly, we select counterfactual pairs across inter-domain and inter-class, and interpolate the ego-networks to generate new samples. Guided by analysis, we can reveal the debiasing mechanism of our model by the causal view and prove that our strategy can make sensitive attributes statistically independent from target labels. Secondly, we reweigh the contribution of each group according to gradients. By combining these two modules, they can mutually promote each other. Experimental results on benchmark datasets show that our method can achieve state-of-the-art results concerning both utility and fairness metrics. Code is available at https://github.com/ZhixunLEE/FairGB.},
  archive   = {C_KDD},
  author    = {Li, Zhixun and Dong, Yushun and Liu, Qiang and Yu, Jeffrey Xu},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671826},
  pages     = {1736–1745},
  title     = {Rethinking fair graph neural networks from re-balancing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). ZeroG: Investigating cross-dataset zero-shot
transferability in graphs. <em>KDD</em>, 1725–1735. (<a
href="https://doi.org/10.1145/3637528.3671982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the development of foundation models such as large language models, zero-shot transfer learning has become increasingly significant. This is highlighted by the generative capabilities of NLP models like GPT-4, and the retrieval-based approaches of CV models like CLIP, both of which effectively bridge the gap between seen and unseen data. In the realm of graph learning, the continuous emergence of new graphs and the challenges of human labeling also amplify the necessity for zero-shot transfer learning, driving the exploration of approaches that can generalize across diverse graph data without necessitating dataset-specific and label-specific fine-tuning. In this study, we extend such paradigms to &amp;lt;u&amp;gt;Zero&amp;lt;/u&amp;gt;-shot transferability in &amp;lt;u&amp;gt;G&amp;lt;/u&amp;gt;raphs by introducing ZeroG, a new framework tailored to enable cross-dataset generalization. Addressing the inherent challenges such as feature misalignment, mismatched label spaces, and negative transfer, we leverage a language model to encode both node attributes and class semantics, ensuring consistent feature dimensions across datasets. We also propose a prompt-based subgraph sampling module that enriches the semantic information and structure information of extracted subgraphs using prompting nodes and neighborhood aggregation, respectively. We further adopt a lightweight fine-tuning strategy that reduces the risk of overfitting and maintains the zero-shot learning efficacy of the language model. The results underscore the effectiveness of our model in achieving significant cross-dataset zero-shot transferability, opening pathways for the development of graph foundation models.},
  archive   = {C_KDD},
  author    = {Li, Yuhan and Wang, Peisong and Li, Zhixun and Yu, Jeffrey Xu and Li, Jia},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671982},
  pages     = {1725–1735},
  title     = {ZeroG: Investigating cross-dataset zero-shot transferability in graphs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving robustness of hyperbolic neural networks by
lipschitz analysis. <em>KDD</em>, 1713–1724. (<a
href="https://doi.org/10.1145/3637528.3671875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hyperbolic neural networks (HNNs) are emerging as a promising tool for representing data embedded in non-Euclidean geometries, yet their adoption has been hindered by challenges related to stability and robustness. In this work, we conduct a rigorous Lipschitz analysis for HNNs and propose using Lipschitz regularization as a novel strategy to enhance their robustness. Our comprehensive investigation spans both the Poincar\&#39;{e} ball model and the hyperboloid model, establishing Lipschitz bounds for HNN layers. Importantly, our analysis provides detailed insights into the behavior of the Lipschitz bounds as they relate to feature norms, particularly distinguishing between scenarios where features have unit norms and those with large norms. Further, we study regularization using the derived Lipschitz bounds. Our empirical validations demonstrate consistent improvements in HNN robustness against noisy perturbations.},
  archive   = {C_KDD},
  author    = {Li, Yuekang and Mao, Yidan and Yang, Yifei and Zou, Dongmian},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671875},
  pages     = {1713–1724},
  title     = {Improving robustness of hyperbolic neural networks by lipschitz analysis},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward structure fairness in dynamic graph embedding: A
trend-aware dual debiasing approach. <em>KDD</em>, 1701–1712. (<a
href="https://doi.org/10.1145/3637528.3671848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent studies successfully learned static graph embeddings that are structurally fair by preventing the effectiveness disparity of high- and low-degree vertex groups in downstream graph mining tasks. However, achieving structure fairness in dynamic graph embedding remains an open problem. Neglecting degree changes in dynamic graphs will significantly impair embedding effectiveness without notably improving structure fairness. This is because the embedding performance of high-degree and low-to-high-degree vertices will significantly drop close to the generally poorer embedding performance of most slightly changed vertices in the long-tail part of the power-law distribution. We first identify biased structural evolutions in a dynamic graph based on the evolving trend of vertex degree and then propose FairDGE, the first structurally Fair Dynamic Graph Embedding algorithm. FairDGE learns biased structural evolutions by jointly embedding the connection changes among vertices and the long-short-term evolutionary trend of vertex degrees. Furthermore, a novel dual debiasing approach is devised to encode fair embeddings contrastively, customizing debiasing strategies for different biased structural evolutions. This innovative debiasing strategy breaks the effectiveness bottleneck of embeddings without notable fairness loss. Extensive experiments demonstrate that FairDGE achieves simultaneous improvement in the effectiveness and fairness of embeddings.},
  archive   = {C_KDD},
  author    = {Li, Yicong and Yang, Yu and Cao, Jiannong and Liu, Shuaiqi and Tang, Haoran and Xu, Guandong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671848},
  pages     = {1701–1712},
  title     = {Toward structure fairness in dynamic graph embedding: A trend-aware dual debiasing approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bi-objective contract allocation for guaranteed delivery
advertising. <em>KDD</em>, 1691–1700. (<a
href="https://doi.org/10.1145/3637528.3671752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Contemporary systems of Guaranteed Delivery (GD) advertising work with two different stages, namely, the offline selling stage and the online serving stage. The former deals with contract allocation, and the latter fulfills the impression allocation of signed contracts. Existing work usually handles these two stages separately. For example, contracts are formulated offline without concerning practical situations in the online serving stage. Therefore, we address in this paper a bi-objective contract allocation for GD advertising, which maximizes the impressions, i.e., Ad resource assignments, allocated for the new incoming advertising orders, and at the same time, controls the balance in the inventories. Since the proposed problem is high dimensional and heavily constrained, we design an efficient local search that focuses on the two objectives alternatively. The experimental results indicate that our algorithm outperforms multi-objective evolutionary algorithms and Gurobi, the former of which is commonly applied for multi-objective optimization and the latter of which is a well-known competitive commercial tool.},
  archive   = {C_KDD},
  author    = {Li, Yan and Huang, Yundu and Mao, Wuyang and Ye, Furong and He, Xiang and Zu, Zhonglin and Cai, Shaowei},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671752},
  pages     = {1691–1700},
  title     = {Bi-objective contract allocation for guaranteed delivery advertising},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). InLN: Knowledge-aware incremental leveling network for
dynamic advertising. <em>KDD</em>, 1679–1690. (<a
href="https://doi.org/10.1145/3637528.3672032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In today&#39;s fast-paced world, advertisers are increasingly demanding real-time and accurate personalized ad delivery based on dynamic preference modeling, which emphasizes the temporality existing in both user preference and product characteristics. Meanwhile, with the development of graph neural networks (GNNs), E-commerce knowledge graphs (KG) with rich semantic relatedness are invoked to improve accuracy and provide appropriate explanations to encourage advertisers&#39; willingness to invest in ad expenses. However, it is still challenging for existing methods to comprehensively consider both time-series interactions and graph-structured knowledge triples in a unified model, i.e., the case in knowledge-aware dynamic advertising. The interaction graph between users and products changes rapidly over time, while the knowledge in KG remains relatively stable. This results in an uneven distribution of temporal and semantic information, causing existing GNNs to fail in this scenario. In this work, we quantitatively define the above phenomenon as temporal unevenness and introduce the Incremental Leveling Network (InLN) with three novel techniques: the periodic-focusing window for node-level dynamic modeling, the biased temporal walk for subgraph-level dynamic modeling and the incremental leveling mechanism for KG updating. Verified by comprehensive and intensive experiments, InLN outperforms nine baseline models in three tasks by substantial margins, reaching up to a 9.9\% improvement and averaging a 5.7\% increase.},
  archive   = {C_KDD},
  author    = {Li, Xujia and Peng, Jingshu and Chen, Lei},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672032},
  pages     = {1679–1690},
  title     = {InLN: Knowledge-aware incremental leveling network for dynamic advertising},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-distilled disentangled learning for counterfactual
prediction. <em>KDD</em>, 1667–1678. (<a
href="https://doi.org/10.1145/3637528.3671782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The advancements in disentangled representation learning significantly enhance the accuracy of counterfactual predictions by granting precise control over instrumental variables, confounders, and adjustable variables. An appealing method for achieving the independent separation of these factors is mutual information minimization, a task that presents challenges in numerous machine learning scenarios, especially within high-dimensional spaces. To circumvent this challenge, we propose the Self-Distilled Disentanglement framework, referred to as SD2. Grounded in information theory, it ensures theoretically sound independent disentangled representations without intricate mutual information estimator designs for high-dimensional representations. Our comprehensive experiments, conducted on both synthetic and real-world datasets, confirms the effectiveness of our approach in facilitating counterfactual inference in the presence of both observed and unobserved confounders.},
  archive   = {C_KDD},
  author    = {Li, Xinshu and Gong, Mingming and Yao, Lina},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671782},
  pages     = {1667–1678},
  title     = {Self-distilled disentangled learning for counterfactual prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Predicting long-term dynamics of complex networks via
identifying skeleton in hyperbolic space. <em>KDD</em>, 1655–1666. (<a
href="https://doi.org/10.1145/3637528.3671968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning complex network dynamics is fundamental for understanding, modeling, and controlling real-world complex systems. Though great efforts have been made to predict the future states of nodes on networks, the capability of capturing long-term dynamics remains largely limited. This is because they overlook the fact that long-term dynamics in complex network are predominantly governed by their inherent low-dimensional manifolds, i.e., skeletons. Therefore, we propose the &amp;lt;u&amp;gt;D&amp;lt;/u&amp;gt;ynamics-&amp;lt;u&amp;gt;I&amp;lt;/u&amp;gt;nvariant &amp;lt;u&amp;gt;Sk&amp;lt;/u&amp;gt;eleton Neural &amp;lt;u&amp;gt;Net&amp;lt;/u&amp;gt;work (DiskNet), which identifies skeletons of complex networks based on the renormalization group structure in hyperbolic space to preserve both topological and dynamics properties. Specifically, we first condense complex networks with various dynamics into simple skeletons through physics-informed hyperbolic embeddings. Further, we design graph neural ordinary differential equations to capture the condensed dynamics on the skeletons. Finally, we recover the skeleton networks and dynamics to the original ones using a degree-based super-resolution module. Extensive experiments across three representative dynamics as well as five real-world and two synthetic networks demonstrate the superior performances of the proposed DiskNet, which outperforms the state-of-the-art baselines by an average of 10.18\% in terms of long-term prediction accuracy. Code for reproduction is available at: https://github.com/tsinghua-fib-lab/DiskNet.},
  archive   = {C_KDD},
  author    = {Li, Ruikun and Wang, Huandong and Piao, Jinghua and Liao, Qingmin and Li, Yong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671968},
  pages     = {1655–1666},
  title     = {Predicting long-term dynamics of complex networks via identifying skeleton in hyperbolic space},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ITPNet: Towards instantaneous trajectory prediction for
autonomous driving. <em>KDD</em>, 1643–1654. (<a
href="https://doi.org/10.1145/3637528.3671681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trajectory prediction of moving traffic agents is crucial for the safety of autonomous vehicles, whereas previous approaches usually rely on sufficiently long-observed trajectory (e.g., 2 seconds) to predict the future trajectory of the agents. However, in many real-world scenarios, it is not realistic to collect adequate observed locations for moving agents, leading to the collapse of most prediction models. For instance, when a moving car suddenly appears and is very close to an autonomous vehicle because of the obstruction, it is quite necessary for the autonomous vehicle to quickly and accurately predict the future trajectories of the car with limited observed trajectory locations. In light of this, we focus on investigating the task of instantaneous trajectory prediction, i.e., two observed locations are available during inference. To this end, we put forward a general and plug-and-play instantaneous trajectory prediction approach, called ITPNet. Specifically, we propose a backward forecasting mechanism to reversely predict the latent feature representations of unobserved historical trajectories of the agent based on its two observed locations and then leverage them as complementary information for future trajectory prediction. Meanwhile, due to the inevitable existence of noise and redundancy in the predicted latent feature representations, we further devise a Noise Redundancy Reduction Former (NRRFormer) module, which aims to filter out noise and redundancy from unobserved trajectories and integrate the filtered features and observed features into a compact query representation for future trajectory predictions. In essence, ITPNet can be naturally compatible with existing trajectory prediction models, enabling them to gracefully handle the case of instantaneous trajectory prediction. Extensive experiments on the Argoverse and nuScenes datasets demonstrate ITPNet outperforms the baselines by a large margin and shows its efficacy with different trajectory prediction models.},
  archive   = {C_KDD},
  author    = {Li, Rongqing and Li, Changsheng and Li, Yuhang and Li, Hanjie and Chen, Yi and Yuan, Ye and Wang, Guoren},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671681},
  pages     = {1643–1654},
  title     = {ITPNet: Towards instantaneous trajectory prediction for autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SimDiff: Simple denoising probabilistic latent diffusion
model for data augmentation on multi-modal knowledge graph.
<em>KDD</em>, 1631–1642. (<a
href="https://doi.org/10.1145/3637528.3671769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we address the challenges of data augmentation in Multi-Modal Knowledge Graphs (MMKGs), a relatively under-explored area. We propose a novel diffusion-based generative model, the Simple Denoising Probabilistic Latent Diffusion Model (SimDiff). SimDiff is capable of handling different data modalities including the graph topology in a unified manner by the same diffusion model in the latent space. It enhances the utilization of multi-modal data and encourage the multi-modal fusion and reduces the dependency on limited training data. We validate our method in downstream Entity Alignment (EA) tasks in MMKGs, demonstrating that even when using only half of the seed entities in training, our methods can still achieve superior performance. This work contributes to the field by providing a new data generation or augmentation method for MMKGs, potentially paving the way for more effective use of MMKGs in various applications. Code is made available at https://github.com/ranlislz/SimDiff.},
  archive   = {C_KDD},
  author    = {Li, Ran and Di, Shimin and Chen, Lei and Zhou, Xiaofang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671769},
  pages     = {1631–1642},
  title     = {SimDiff: Simple denoising probabilistic latent diffusion model for data augmentation on multi-modal knowledge graph},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Privileged knowledge state distillation for reinforcement
learning-based educational path recommendation. <em>KDD</em>, 1621–1630.
(<a href="https://doi.org/10.1145/3637528.3671872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Educational recommendation seeks to suggest knowledge concepts that match a learner&#39;s ability, thus facilitating a personalized learning experience. In recent years, reinforcement learning (RL) methods have achieved considerable results by taking the encoding of the learner&#39;s exercise log as the state and employing an RL-based agent to make suitable recommendations. However, these approaches suffer from handling the diverse and dynamic learner&#39;s knowledge states. In this paper, we introduce the privileged feature distillation technique and propose the P rivileged K nowledge S tate D istillation (PKSD ) framework, allowing the RL agent to leverage the &quot;actual&#39;&#39; knowledge state as privileged information in the state encoding to help tailor recommendations to meet individual needs. Concretely, our PKSD takes the privileged knowledge states together with the representations of the exercise log for the state representations during training. And through distillation, we transfer the ability to adapt to learners to aknowledge state adapter. During inference, theknowledge state adapter would serve as the estimated privileged knowledge states instead of the real one since it is not accessible. Considering that there are strong connections among the knowledge concepts in education, we further propose to collaborate the graph structure learning for concepts into our PKSD framework. This new approach is termed GEPKSD (Graph-Enhanced PKSD). As our method is model-agnostic, we evaluate PKSD and GEPKSD by integrating them with five different RL bases on four public simulators, respectively. Our results verify that PKSD can consistently improve the recommendation performance with various RL methods, and our GEPKSD could further enhance the effectiveness of PKSD in all the simulations.},
  archive   = {C_KDD},
  author    = {Li, Qingyao and Xia, Wei and Yin, Li&#39;ang and Jin, Jiarui and Yu, Yong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671872},
  pages     = {1621–1630},
  title     = {Privileged knowledge state distillation for reinforcement learning-based educational path recommendation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Causal subgraph learning for generalizable inductive
relation prediction. <em>KDD</em>, 1610–1620. (<a
href="https://doi.org/10.1145/3637528.3671972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inductive relation reasoning in knowledge graphs aims at predicting missing triplets involving unseen entities and/or unseen relations. While subgraph-based methods that reason about the local structure surrounding a candidate triplet have shown promise, they often fall short in accurately modeling the causal dependence between a triplet&#39;s subgraph and its ground-truth label. This limitation typically results in a susceptibility to spurious correlations caused by confounders, adversely affecting generalization capabilities. Herein, we introduce a novel front-door adjustment-based approach designed to learn the causal relationship between subgraphs and their ground-truth labels, specifically for inductive relation prediction. We conceptualize the semantic information of subgraphs as a mediator and employ a graph data augmentation mechanism to create augmented subgraphs. Furthermore, we integrate a fusion module and a decoder within the front-door adjustment framework, enabling the estimation of the mediator&#39;s combination with augmented subgraphs. We also introduce the reparameterization trick in the fusion model to enhance model robustness. Extensive experiments on widely recognized benchmark datasets demonstrate the proposed method&#39;s superiority in inductive relation prediction, particularly for tasks involving unseen entities and unseen relations. Additionally, the subgraphs reconstructed by our decoder offer valuable insights into the model&#39;s decision-making process, enhancing transparency and interpretability.},
  archive   = {C_KDD},
  author    = {Li, Mei and Liu, Xiaoguang and Ji, Hua and Zheng, Shuangjia},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671972},
  pages     = {1610–1620},
  title     = {Causal subgraph learning for generalizable inductive relation prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Label learning method based on tensor projection.
<em>KDD</em>, 1599–1609. (<a
href="https://doi.org/10.1145/3637528.3671671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-view clustering method based on anchor graph has been widely concerned due to its high efficiency and effectiveness. In order to avoid post-processing, most of the existing anchor graph-based methods learn bipartite graphs with connected components. However, such methods have high requirements on parameters, and in some cases it may not be possible to obtain bipartite graphs with clear connected components. To end this, we propose a label learning method based on tensor projection (LLMTP). Specifically, we project anchor graph into the label space through an orthogonal projection matrix to obtain cluster labels directly. Considering that the spatial structure information of multi-view data may be ignored to a certain extent when projected in different views separately, we extend the matrix projection transformation to tensor projection, so that the spatial structure information between views can be fully utilized. In addition, we introduce the tensor Schatten p-norm regularization to make the clustering label matrices of different views as consistent as possible. Extensive experiments have proved the effectiveness of the proposed method.},
  archive   = {C_KDD},
  author    = {Li, Jing and Gao, Quanxue and Wang, Qianqian and Deng, Cheng and Xie, Deyan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671671},
  pages     = {1599–1609},
  title     = {Label learning method based on tensor projection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Physics-informed neural ODE for post-disaster mobility
recovery. <em>KDD</em>, 1587–1598. (<a
href="https://doi.org/10.1145/3637528.3672027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Urban mobility undergoes a profound decline in the aftermath of a disaster, subsequently exhibiting a complex recovery trajectory. Effectively capturing and predicting this dynamic recovery process holds paramount importance for devising more efficient post-disaster recovery strategies, such as resource allocation to areas with protracted recovery periods. Existing models for post-disaster mobility recovery predominantly employ basic mathematical methods, which are strongly based on simplifying assumptions, and their limited parameters restrict their capacity to fully capture the mobility recovery patterns. In response to this gap, we introduce the Coupled Dynamic Graph ODE Network (CDGON) to model the intricate dynamics of post-disaster mobility recovery. Our model seamlessly integrates existing physical knowledge pertaining to post-disaster mobility recovery and incorporates the nuanced interactions between intra-regional and inter-regional population flows. Extensive experimental results demonstrate the efficiency of our model in capturing the dynamic recovery patterns of urban population mobility in post-disaster scenarios, surpassing the capabilities of current dynamic graph prediction models.},
  archive   = {C_KDD},
  author    = {Li, Jiahao and Wang, Huandong and Chen, Xinlei},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672027},
  pages     = {1587–1598},
  title     = {Physics-informed neural ODE for post-disaster mobility recovery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Debiased recommendation with noisy feedback. <em>KDD</em>,
1576–1586. (<a href="https://doi.org/10.1145/3637528.3671915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ratings of a user to most items in recommender systems are usually missing not at random (MNAR), largely because users are free to choose which items to rate. To achieve unbiased learning of the prediction model under MNAR data, three typical solutions have been proposed, including error-imputation-based (EIB), inverse-propensity-scoring (IPS), and doubly robust (DR) methods. However, these methods ignore an alternative form of bias caused by the inconsistency between the observed ratings and the users&#39; true preferences, also known as noisy feedback or outcome measurement errors (OME), e.g., due to public opinion or low-quality data collection process. In this work, we study intersectional threats to the unbiased learning of the prediction model from data MNAR and OME in the collected data. First, we design OME-EIB, OME-IPS, and OME-DR estimators, which largely extend the existing estimators to combat OME in real-world recommendation scenarios. Next, we theoretically prove the unbiasedness and generalization bound of the proposed estimators. We further propose an alternate denoising training approach to achieve unbiased learning of the prediction model under MNAR data with OME. Extensive experiments are conducted on three real-world datasets and one semi-synthetic dataset to show the effectiveness of our proposed approaches. The code is available at https://github.com/haoxuanli-pku/KDD24-OME-DR.},
  archive   = {C_KDD},
  author    = {Li, Haoxuan and Zheng, Chunyuan and Wang, Wenjie and Wang, Hao and Feng, Fuli and Zhou, Xiao-Hua},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671915},
  pages     = {1576–1586},
  title     = {Debiased recommendation with noisy feedback},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Truthful bandit mechanisms for repeated two-stage ad
auctions. <em>KDD</em>, 1565–1575. (<a
href="https://doi.org/10.1145/3637528.3671813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Online advertising platforms leverage a two-stage auction architecture to deliver personalized ads to users with low latency. The first stage efficiently selects a small subset of promising candidates out of the complete pool of ads. In the second stage, an auction is conducted within the subset to determine the winning ad for display, using click-through-rate predictions from the second-stage machine learning model. In this work, we investigate the online learning process of the first-stage subset selection policy, while ensuring game-theoretic properties in repeated two-stage ad auctions. Specifically, we model the problem as designing a combinatorial bandit mechanism with a general reward function, as well as additional requirements of truthfulness and individual rationality (IR). We establish an O(T) regret lower bound for truthful bandit mechanisms, which demonstrates the challenge of simultaneously achieving allocation efficiency and truthfulness. To circumvent this impossibility result, we introduce truthful α-approximation oracles and evaluate the bandit mechanism through α-approximation regret. Two mechanisms are proposed, both of which are ex-post truthful and ex-post IR. The first mechanism is an explore-then-commit mechanism with regret O(T2/3 ), and the second mechanism achieves an improved O(log T /ΔΦ2) regret where ΔΦ is a distribution-dependent gap, but requires additional assumptions on the oracles and information about the strategic bidders.},
  archive   = {C_KDD},
  author    = {Li, Haoming and Liu, Yumou and Zheng, Zhenzhe and Zhang, Zhilin and Xu, Jian and Wu, Fan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671813},
  pages     = {1565–1575},
  title     = {Truthful bandit mechanisms for repeated two-stage ad auctions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic neural dowker network: Approximating persistent
homology in dynamic directed graphs. <em>KDD</em>, 1554–1564. (<a
href="https://doi.org/10.1145/3637528.3671980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Persistent homology, a fundamental technique within Topological Data Analysis (TDA), captures structural and shape characteristics of graphs, yet encounters computational difficulties when applied to dynamic directed graphs. This paper introduces the Dynamic Neural Dowker Network (DNDN), a novel framework specifically designed to approximate the results of dynamic Dowker filtration, aiming to capture the high-order topological features of dynamic directed graphs. Our approach creatively uses line graph transformations to produce both source and sink line graphs, highlighting the shared neighbor structures that Dowker complexes focus on. The DNDN incorporates a Source-Sink Line Graph Neural Network (SSLGNN) layer to effectively capture the neighborhood relationships among dynamic edges. Additionally, we introduce an innovative duality edge fusion mechanism, ensuring that the results for both the sink and source line graphs adhere to the duality principle intrinsic to Dowker complexes. Our approach is validated through comprehensive experiments on real-world datasets, demonstrating DNDN&#39;s capability not only to effectively approximate dynamic Dowker filtration results but also to perform exceptionally in dynamic graph classification tasks.},
  archive   = {C_KDD},
  author    = {Li, Hao and Jiang, Hao and Jiajun, Fan and Ye, Dongsheng and Du, Liang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671980},
  pages     = {1554–1564},
  title     = {Dynamic neural dowker network: Approximating persistent homology in dynamic directed graphs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable multitask learning using gradient-based estimation
of task affinity. <em>KDD</em>, 1542–1553. (<a
href="https://doi.org/10.1145/3637528.3671835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multitask learning is a widely used paradigm for training models on diverse tasks, with applications ranging from graph neural networks to language model fine-tuning. Since tasks may interfere with each other, a key notion for modeling their relationships is task affinity. This includes pairwise task affinity, computed among pairs of tasks, and higher-order affinity, computed among subsets of tasks. Naively computing either of them requires repeatedly training on data pooled from various task combinations, which is computationally intensive. We present a new algorithm Grad-TAG that can estimate task affinities without this repeated training. The key idea of Grad-TAG is to train a &quot;base&quot; model for all tasks and then use a linearization technique to estimate the loss of any other model with a specific task combination. The linearization works by computing a gradient-based first-order approximation of the loss, using low-dimensional projections of gradients as features in a logistic regression trained to predict labels for the specific task combination. We show theoretically that the linearized model can provably approximate the loss when the gradient-based approximation is accurate, and also empirically verify that on several large models. Then, given the estimated task affinity matrix, we design a semi-definite program for clustering to group similar tasks that maximize the average density of clusters. We evaluate Grad-TAG&#39;s performance across seven datasets, including multi-label classification on graphs, and instruction fine-tuning of language models. Our results show that our task affinity estimates are within 2.7\% distance of the true affinities while needing only 3\% of FLOPs compared to full training. On our largest graph with 21M edges and 500 labeling tasks, our algorithm delivers an estimate accurate to within 5\% of the true affinities, while using only 112.3 GPU hours. Our results show that Grad-TAG achieves excellent performance and runtime tradeoffs compared to existing approaches.},
  archive   = {C_KDD},
  author    = {Li, Dongyue and Sharma, Aneesh and Zhang, Hongyang R.},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671835},
  pages     = {1542–1553},
  title     = {Scalable multitask learning using gradient-based estimation of task affinity},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RecExplainer: Aligning large language models for explaining
recommendation models. <em>KDD</em>, 1530–1541. (<a
href="https://doi.org/10.1145/3637528.3671802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recommender systems are widely used in online services, with embedding-based models being particularly popular due to their expressiveness in representing complex signals. However, these models often function as a black box, making them less transparent and reliable for both users and developers. Recently, large language models (LLMs) have demonstrated remarkable intelligence in understanding, reasoning, and instruction following. This paper presents the initial exploration of using LLMs as surrogate models to explaining black-box recommender models. The primary concept involves training LLMs to comprehend and emulate the behavior of target recommender models. By leveraging LLMs&#39; own extensive world knowledge and multi-step reasoning abilities, these aligned LLMs can serve as advanced surrogates, capable of reasoning about observations. Moreover, employing natural language as an interface allows for the creation of customizable explanations that can be adapted to individual user preferences. To facilitate an effective alignment, we introduce three methods: behavior alignment, intention alignment, and hybrid alignment. Behavior alignment operates in the language space, representing user preferences and item information as text to mimic the target model&#39;s behavior; intention alignment works in the latent space of the recommendation model, using user and item representations to understand the model&#39;s behavior; hybrid alignment combines both language and latent spaces. Comprehensive experiments conducted on three public datasets show that our approach yields promising results in understanding and mimicking target models, producing high-quality, high-fidelity, and distinct explanations. Our code is available at https://github.com/microsoft/RecAI.},
  archive   = {C_KDD},
  author    = {Lei, Yuxuan and Lian, Jianxun and Yao, Jing and Huang, Xu and Lian, Defu and Xie, Xing},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671802},
  pages     = {1530–1541},
  title     = {RecExplainer: Aligning large language models for explaining recommendation models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Layer-wise adaptive gradient norm penalizing method for
efficient and accurate deep learning. <em>KDD</em>, 1518–1529. (<a
href="https://doi.org/10.1145/3637528.3671728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sharpness-aware minimization (SAM) is known to improve the generalization performance of neural networks. However, it is not widely used in real-world applications yet due to its expensive model perturbation cost. A few variants of SAM have been proposed to tackle such an issue, but they commonly do not alleviate the cost noticeably. In this paper, we propose a lightweight layer-wise gradient norm penalizing method that tackles the expensive computational cost of SAM while maintaining its superior generalization performance. Our study empirically proves that the gradient norm of the whole model can be effectively suppressed by penalizing the gradient norm of only a few critical layers. We also theoretically show that such a partial model perturbation does not harm the convergence rate of SAM, allowing them to be safely adapted in real-world applications. To demonstrate the efficacy of the proposed method, we perform extensive experiments comparing the proposed method to mini-batch SGD and the conventional SAM using representative computer vision and language modeling benchmarks.},
  archive   = {C_KDD},
  author    = {Lee, Sunwoo},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671728},
  pages     = {1518–1529},
  title     = {Layer-wise adaptive gradient norm penalizing method for efficient and accurate deep learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SLADE: Detecting dynamic anomalies in edge streams without
labels via self-supervised learning. <em>KDD</em>, 1506–1517. (<a
href="https://doi.org/10.1145/3637528.3671845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To detect anomalies in real-world graphs, such as social, email, and financial networks, various approaches have been developed. While they typically assume static input graphs, most real-world graphs grow over time, naturally represented as edge streams. In this context, we aim to achieve three goals: (a) instantly detecting anomalies as they occur, (b) adapting to dynamically changing states, and (c) handling the scarcity of dynamic anomaly labels.In this paper, we propose SLADE (&amp;lt;u&amp;gt;S&amp;lt;/u&amp;gt;elf-supervised &amp;lt;u&amp;gt;L&amp;lt;/u&amp;gt;earning for &amp;lt;u&amp;gt;A&amp;lt;/u&amp;gt;nomaly &amp;lt;u&amp;gt;D&amp;lt;/u&amp;gt;etection in &amp;lt;u&amp;gt;E&amp;lt;/u&amp;gt;dge Streams) for rapid detection of dynamic anomalies in edge streams, without relying on labels. SLADE detects the shifts of nodes into abnormal states by observing deviations in their interaction patterns over time. To this end, it trains a deep neural network to perform two self-supervised tasks: (a) minimizing drift in node representations and (b) generating long-term interaction patterns from short-term ones. Failure in these tasks for a node signals its deviation from the norm. Notably, the neural network and tasks are carefully designed so that all required operations can be performed in constant time (w.r.t. the graph size) in response to each new edge in the input stream. In dynamic anomaly detection across four real-world datasets, SLADE outperforms nine competing methods, even those leveraging label supervision. Our code and datasets are available at https://github.com/jhsk777/SLADE.},
  archive   = {C_KDD},
  author    = {Lee, Jongha and Kim, Sunwoo and Shin, Kijung},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671845},
  pages     = {1506–1517},
  title     = {SLADE: Detecting dynamic anomalies in edge streams without labels via self-supervised learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continual collaborative distillation for recommender system.
<em>KDD</em>, 1495–1505. (<a
href="https://doi.org/10.1145/3637528.3671924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Knowledge distillation (KD) has emerged as a promising technique for addressing the computational challenges associated with deploying large-scale recommender systems. KD transfers the knowledge of a massive teacher system to a compact student model, to reduce the huge computational burdens for inference while retaining high accuracy. The existing KD studies primarily focus on one-time distillation in static environments, leaving a substantial gap in their applicability to real-world scenarios dealing with continuously incoming users, items, and their interactions. In this work, we delve into a systematic approach to operating the teacher-student KD in a non-stationary data stream. Our goal is to enable efficient deployment through a compact student, which preserves the high performance of the massive teacher, while effectively adapting to continuously incoming data. We propose &amp;lt;u&amp;gt;C&amp;lt;/u&amp;gt;ontinual &amp;lt;u&amp;gt;C&amp;lt;/u&amp;gt;ollaborative &amp;lt;u&amp;gt;D&amp;lt;/u&amp;gt;istillation (CCD) framework, where both the teacher and the student continually and collaboratively evolve along the data stream. CCD facilitates the student in effectively adapting to new data, while also enabling the teacher to fully leverage accumulated knowledge. We validate the effectiveness of CCD through extensive quantitative, ablative, and exploratory experiments on two real-world datasets. We expect this research direction to contribute to narrowing the gap between existing KD studies and practical applications, thereby enhancing the applicability of KD in real-world systems.},
  archive   = {C_KDD},
  author    = {Lee, Gyuseok and Kang, SeongKu and Kweon, Wonbin and Yu, Hwanjo},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671924},
  pages     = {1495–1505},
  title     = {Continual collaborative distillation for recommender system},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ShapeFormer: Shapelet transformer for multivariate time
series classification. <em>KDD</em>, 1484–1494. (<a
href="https://doi.org/10.1145/3637528.3671862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multivariate time series classification (MTSC) has attracted significant research attention due to its diverse real-world applications. Recently, exploiting transformers for MTSC has achieved state-of-the-art performance. However, existing methods focus on generic features, providing a comprehensive understanding of data, but they ignore class-specific features crucial for learning the representative characteristics of each class. This leads to poor performance in the case of imbalanced datasets or datasets with similar overall patterns but differing in minor class-specific details. In this paper, we propose a novel Shapelet Transformer (ShapeFormer), which comprises class-specific and generic transformer modules to capture both of these features. In the class-specific module, we introduce the discovery method to extract the discriminative subsequences of each class (i.e. shapelets) from the training set. We then propose a Shapelet Filter to learn the difference features between these shapelets and the input time series. We found that the difference feature for each shapelet contains important class-specific features, as it shows a significant distinction between its class and others. In the generic module, convolution filters are used to extract generic features that contain information to distinguish among all classes. For each module, we employ the transformer encoder to capture the correlation between their features. As a result, the combination of two transformer modules allows our model to exploit the power of both types of features, thereby enhancing the classification performance. Our experiments on 30 UEA MTSC datasets demonstrate that ShapeFormer has achieved the highest accuracy ranking compared to state-of-the-art methods. The code is available at https://github.com/xuanmay2701/shapeformer.},
  archive   = {C_KDD},
  author    = {Le, Xuan-May and Luo, Ling and Aickelin, Uwe and Tran, Minh-Tuan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671862},
  pages     = {1484–1494},
  title     = {ShapeFormer: Shapelet transformer for multivariate time series classification},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ReCTSi: Resource-efficient correlated time series imputation
via decoupled pattern learning and completeness-aware attentions.
<em>KDD</em>, 1474–1483. (<a
href="https://doi.org/10.1145/3637528.3671816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Imputation of Correlated Time Series (CTS) is essential in data preprocessing for many tasks, particularly when sensor data is often incomplete. Deep learning has enabled sophisticated models that improve CTS imputation by capturing temporal and spatial patterns. However, deep models often incur considerable consumption of computational resources and thus cannot be deployed in resource-limited settings. This paper presents ReCTSi (Resource-efficient CTS imputation), a method that adopts a new architecture for decoupled pattern learning in two phases: (1) the Persistent Pattern Extraction phase utilizes a multi-view learnable codebook mechanism to identify and archive persistent patterns common across different time series, enabling rapid pattern retrieval during inference. (2) the Transient Pattern Adaptation phase introduces completeness-aware attention modules that allocate attention to the complete and hence more reliable data segments. Extensive experimental results show that ReCTSi achieves state-of-the-art imputation accuracy while consuming much fewer computational resources than the leading existing model, consuming only 0.004\% of the FLOPs for inference compared to its closest competitor. The blend of high accuracy and very low resource consumption makes ReCTSi the currently best method for resource-limited scenarios. The related code is available at https://github.com/ryanlaics/RECTSI.},
  archive   = {C_KDD},
  author    = {Lai, Zhichen and Zhang, Dalin and Li, Huan and Zhang, Dongxiang and Lu, Hua and Jensen, Christian S.},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671816},
  pages     = {1474–1483},
  title     = {ReCTSi: Resource-efficient correlated time series imputation via decoupled pattern learning and completeness-aware attentions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient topology-aware data augmentation for high-degree
graph neural networks. <em>KDD</em>, 1463–1473. (<a
href="https://doi.org/10.1145/3637528.3671765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, graph neural networks (GNNs) have emerged as a potent tool for learning on graph-structured data and won fruitful successes in varied fields. The majority of GNNs follow the message-passing paradigm, where representations of each node are learned by recursively aggregating features of its neighbors. However, this mechanism brings severe over-smoothing and efficiency issues over high-degree graphs (HDGs), wherein most nodes have dozens (or even hundreds) of neighbors, such as social networks, transaction graphs, power grids, etc. Additionally, such graphs usually encompass rich and complex structure semantics, which are hard to capture merely by feature aggregations in GNNs.Motivated by the above limitations, we propose TADA, an efficient and effective front-mounted data augmentation framework for GNNs on HDGs. Under the hood, TADA includes two key modules: (i) feature expansion with structure embeddings, and (ii) topology- and attribute-aware graph sparsification. The former obtains augmented node features and enhanced model capacity by encoding the graph structure into high-quality structure embeddings with our highly-efficient sketching method. Further, by exploiting task-relevant features extracted from graph structures and attributes, the second module enables the accurate identification and reduction of numerous redundant/noisy edges from the input graph, thereby alleviating over-smoothing and facilitating faster feature aggregations over HDGs. Empirically, algo considerably improves the predictive performance of mainstream GNN models on 8 real homophilic/heterophilic HDGs in terms of node classification, while achieving efficient training and inference processes.},
  archive   = {C_KDD},
  author    = {Lai, Yurui and Lin, Xiaoyang and Yang, Renchi and Wang, Hongtao},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671765},
  pages     = {1463–1473},
  title     = {Efficient topology-aware data augmentation for high-degree graph neural networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Compact decomposition of irregular tensors for data
compression: From sparse to dense to high-order tensors. <em>KDD</em>,
1451–1462. (<a href="https://doi.org/10.1145/3637528.3671846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An irregular tensor is a collection of matrices with different numbers of rows. Real-world data from diverse domains, including medical and stock data, are effectively represented as irregular tensors due to the inherent variations in data length. For their analysis, various tensor decomposition methods (e.g., PARAFAC2) have been devised. While they are expected to be effective in compressing large-scale irregular tensors, akin to regular tensor decomposition methods, our analysis reveals that their compression performance is limited due to the larger number of first mode factor matrices.In this work, we propose accurate and compact decomposition methods for lossy compression of irregular tensors. First, we propose Light-IT, which unifies all first mode factor matrices into a single matrix, dramatically reducing the size of compressed outputs. Second, motivated by the success of Tucker decomposition in regular tensor compression, we extend Light-IT to Light-IT++ to enhance its expressive power and thus reduce compression error. Finally, we generalize both methods to handle irregular tensors of any order and leverage the sparsity of tensors for acceleration.Extensive experiments on 6 real-world datasets demonstrate that our methods are (a) Compact: their compressed output is up to 37\texttimes{} smaller than that of the most concise baseline, (b) Accurate: our methods are up to 5\texttimes{} more accurate, with smaller compressed output, than the most accurate baseline, and (c) Versatile: our methods are effective for sparse, dense, and higher-order tensors.},
  archive   = {C_KDD},
  author    = {Kwon, Taehyung and Ko, Jihoon and Jung, Jinhong and Jang, Jun-Gi and Shin, Kijung},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671846},
  pages     = {1451–1462},
  title     = {Compact decomposition of irregular tensors for data compression: From sparse to dense to high-order tensors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Max-min diversification with asymmetric distances.
<em>KDD</em>, 1440–1450. (<a
href="https://doi.org/10.1145/3637528.3671757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One of the most well-known and simplest models for diversity maximization is the Max-Min Diversification (MMD) model, which has been extensively studied in the data mining and database literature. In this paper, we initiate the study of the Asymmetric Max-Min Diversification (AMMD) problem. The input is a positive integer k and a complete digraph over n vertices, together with a nonnegative distance function over the edges obeying the directed triangle inequality. The objective is to select a set of k vertices, which maximizes the smallest pairwise distance between them. AMMD reduces to the well-studied MMD problem in case the distances are symmetric, and has natural applications to query result diversification, web search, and facility location problems. Although the MMD problem admits a simple 1/2-approximation by greedily selecting the next-furthest point, this strategy fails for AMMD and it remained unclear how to design good approximation algorithms for AMMD.We propose a combinatorial 1/(6k)-approximation algorithm for AMMD by leveraging connections with the Maximum Antichain problem. We discuss several ways of speeding up the algorithm and compare its performance against heuristic baselines on real-life and synthetic datasets.},
  archive   = {C_KDD},
  author    = {Kumpulainen, Iiro and Adriaens, Florian and Tatti, Nikolaj},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671757},
  pages     = {1440–1450},
  title     = {Max-min diversification with asymmetric distances},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attacking graph neural networks with bit flips: Weisfeiler
and leman go indifferent. <em>KDD</em>, 1428–1439. (<a
href="https://doi.org/10.1145/3637528.3671890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Prior attacks on graph neural networks have focused on graph poisoning and evasion, neglecting the network&#39;s weights and biases. For convolutional neural networks, however, the risk arising from bit flip attacks is well recognized. We show that the direct application of a traditional bit flip attack to graph neural networks is of limited effectivity. Hence, we discuss the Injectivity Bit Flip Attack, the first bit flip attack designed specifically for graph neural networks. Our attack targets the learnable neighborhood aggregation functions in quantized message passing neural networks, degrading their ability to distinguish graph structures and impairing the expressivity of the Weisfeiler-Leman test. We find that exploiting mathematical properties specific to certain graph neural networks significantly increases their vulnerability to bit flip attacks. The Injectivity Bit Flip Attack can degrade the maximal expressive Graph Isomorphism Networks trained on graph property prediction datasets to random output by flipping only a small fraction of the network&#39;s bits, demonstrating its higher destructive power compared to traditional bit flip attacks transferred from convolutional neural networks. Our attack is transparent, motivated by theoretical insights and confirmed by extensive empirical results.},
  archive   = {C_KDD},
  author    = {Kummer, Lorenz and Moustafa, Samir and Schrittwieser, Sebastian and Gansterer, Wilfried and Kriege, Nils},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671890},
  pages     = {1428–1439},
  title     = {Attacking graph neural networks with bit flips: Weisfeiler and leman go indifferent},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LeMon: Automating portrait generation for zero-shot story
visualization with multi-character interactions. <em>KDD</em>,
1418–1427. (<a href="https://doi.org/10.1145/3637528.3671850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Zero-Shot Story Visualization (ZSV) seeks to depict textual narratives through a sequence of images without relying on pre-existing text-image pairs for training. In this paper, we address the challenge of automated multi-character ZSV, aiming to create distinctive yet compatible character portraits for high-quality story visualization without the need of manual human interventions. Our study is motivated by the limitation of current ZSV approaches that necessitate inefficient manual collection of external images as initial character portraits and suffer from low-quality story visualization, especially with multi-character interactions, when the portraits are not well initiated. To overcome these issues, we develop LeMon, an LLM enhanced Multi-Character Zero-Shot Visualization framework that automates character portrait initialization and supports iterative portrait refinement by exploring the semantic content of the story. In particular, we design an LLM-based portrait generation strategy that matches the story characters with external movie characters, and leverage the matched resources as in-context learning (ICL) samples for LLMs to accurately initialize the character portraits. We then propose a graph-based Text2Image diffusion model that constructs a character interaction graph from the story to iteratively refine the character portraits by maximizing the distinctness of different characters while minimizing their incompatibility in the multi-character story visualization. Our evaluation results show that LeMon outperforms existing ZSV approaches in generating high-quality visualizations for stories across various types with multiple interacted characters. Our code is available at https://github.com/arxrean/LLM-LeMon.},
  archive   = {C_KDD},
  author    = {Kou, Ziyi and Pei, Shichao and Zhang, Xiangliang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671850},
  pages     = {1418–1427},
  title     = {LeMon: Automating portrait generation for zero-shot story visualization with multi-character interactions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OntoType: Ontology-guided and pre-trained language model
assisted fine-grained entity typing. <em>KDD</em>, 1407–1417. (<a
href="https://doi.org/10.1145/3637528.3671745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fine-grained entity typing (FET), which assigns entities in text with context-sensitive, fine-grained semantic types, is a basic but important task for knowledge extraction from unstructured text. FET has been studied extensively in natural language processing and typically relies on human-annotated corpora for training, which is costly and difficult to scale. Recent studies explore the utilization of pre-trained language models (PLMs) as a knowledge base to generate rich and context-aware weak supervision for FET. However, a PLM still requires direction and guidance to serve as a knowledge base as they often generate a mixture of rough and fine-grained types, or tokens unsuitable for typing. In this study, we vision that an ontology provides a semantics-rich, hierarchical structure, which will help select the best results generated by multiple PLM models and head words. Specifically, we propose a novel annotation-free, ontology-guided FET method, OntoType, which follows a type ontological structure, from coarse to fine, ensembles multiple PLM prompting results to generate a set of type candidates, and refines its type resolution, under the local context with a natural language inference model. Our experiments on the Ontonotes, FIGER, and NYT datasets using their associated ontological structures demonstrate that our method outperforms the state-of-the-art zero-shot fine-grained entity typing methods as well as a typical LLM method, ChatGPT. Our error analysis shows that refinement of the existing ontology structures will further improve fine-grained entity typing.},
  archive   = {C_KDD},
  author    = {Komarlu, Tanay and Jiang, Minhao and Wang, Xuan and Han, Jiawei},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671745},
  pages     = {1407–1417},
  title     = {OntoType: Ontology-guided and pre-trained language model assisted fine-grained entity typing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large language models meet collaborative filtering: An
efficient all-round LLM-based recommender system. <em>KDD</em>,
1395–1406. (<a href="https://doi.org/10.1145/3637528.3671931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collaborative filtering recommender systems (CF-RecSys) have shown successive results in enhancing the user experience on social media and e-commerce platforms. However, as CF-RecSys struggles under cold scenarios with sparse user-item interactions, recent strategies have focused on leveraging modality information of user/items (e.g., text or images) based on pre-trained modality encoders and Large Language Models (LLMs). Despite their effectiveness under cold scenarios, we observe that they underperform simple traditional collaborative filtering models under warm scenarios due to the lack of collaborative knowledge. In this work, we propose an efficient All-round LLM-based Recommender system, called A-LLMRec, that excels not only in the cold scenario but also in the warm scenario. Our main idea is to enable an LLM to directly leverage the collaborative knowledge contained in a pre-trained state-of-the-art CF-RecSys so that the emergent ability of the LLM as well as the high-quality user/item embeddings that are already trained by the state-of-the-art CF-RecSys can be jointly exploited. This approach yields two advantages: (1) model-agnostic, allowing for integration with various existing CF-RecSys, and (2) efficiency, eliminating the extensive fine-tuning typically required for LLM-based recommenders. Our extensive experiments on various real-world datasets demonstrate the superiority of A-LLMRec in various scenarios, including cold/warm, few-shot, cold user, and cross-domain scenarios. Beyond the recommendation task, we also show the potential of A-LLMRec in generating natural language outputs based on the understanding of the collaborative knowledge by performing a favorite genre prediction task. Our code is available at https://github.com/ghdtjr/A-LLMRec.},
  archive   = {C_KDD},
  author    = {Kim, Sein and Kang, Hongseok and Choi, Seungyoon and Kim, Donghyun and Yang, Minchul and Park, Chanyoung},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671931},
  pages     = {1395–1406},
  title     = {Large language models meet collaborative filtering: An efficient all-round LLM-based recommender system},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast and accurate domain adaptation for irregular tensor
decomposition. <em>KDD</em>, 1383–1394. (<a
href="https://doi.org/10.1145/3637528.3671670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Given an irregular tensor from a newly emerging domain, how can we quickly and accurately capture its patterns utilizing existing irregular tensors in multiple domains? The problem is of great importance for various tasks such as finding patterns of a new disease using pre-existing diseases data. This is challenging as new target tensors have limited information due to their recent emergence. Thus, carefully utilizing the existing source tensors for analyzing the target tensor is helpful. PARAFAC2 decomposition is a strong tool for finding the patterns of irregular tensors, and the patterns are used in many applications such as missing value prediction and anomaly detection. However, previous PARAFAC2-based works cannot adaptably handle newly emerging target tensors utilizing the source tensors.In this work, we propose Meta-P2, a fast and accurate domain adaptation method for irregular tensor decomposition. Meta-P2 generates a meta factor matrix from the multiple source domains, by domain adaptation and meta-update steps. Meta-P2 quickly and accurately finds the patterns of the new irregular tensor utilizing the meta factor matrix. Extensive experiments on real-world datasets show that Meta-P2 achieves the best performance in various downstream tasks including missing value prediction and anomaly detection tasks.},
  archive   = {C_KDD},
  author    = {Kim, Junghun and Park, Ka Hyun and Jang, Jun-Gi and Kang, U},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671670},
  pages     = {1383–1394},
  title     = {Fast and accurate domain adaptation for irregular tensor decomposition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CAFO: Feature-centric explanation on time series
classification. <em>KDD</em>, 1372–1382. (<a
href="https://doi.org/10.1145/3637528.3671724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In multivariate time series (MTS) classification, finding the important features (e.g., sensors) for model performance is crucial yet challenging due to the complex, high-dimensional nature of MTS data, intricate temporal dynamics, and the necessity for domain-specific interpretations. Current explanation methods for MTS mostly focus on time-centric explanations, apt for pinpointing important time periods but less effective in identifying key features. This limitation underscores the pressing need for a feature-centric approach, a vital yet often overlooked perspective that complements time-centric analysis. To bridge this gap, our study introduces a novel feature-centric explanation and evaluation framework for MTS, named CAFO (Channel Attention and Feature Orthgonalization). CAFO employs a convolution-based approach with channel attention mechanisms, incorporating a depth-wise separable channel attention module (DepCA) and a QR decomposition-based loss for promoting feature-wise orthogonality. We demonstrate that this orthogonalization enhances the separability of attention distributions, thereby refining and stabilizing the ranking of feature importance. This improvement in feature-wise ranking enhances our understanding of feature explainability in MTS. Furthermore, we develop metrics to evaluate global and class-specific feature importance. Our framework&#39;s efficacy is validated through extensive empirical analyses on two major public benchmarks and real-world datasets, both synthetic and self-collected, specifically designed to highlight class-wise discriminative features. The results confirm CAFO&#39;s robustness and informative capacity in assessing feature importance in MTS classification tasks. This study not only advances the understanding of feature-centric explanations in MTS but also sets a foundation for future explorations in feature-centric explanations. The codes are available at https://github.com/eai-lab/CAFO.},
  archive   = {C_KDD},
  author    = {Kim, Jaeho and Hahn, Seok-Ju and Hwang, Yoontae and Lee, Junghye and Lee, Seulki},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671724},
  pages     = {1372–1382},
  title     = {CAFO: Feature-centric explanation on time series classification},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gandalf: Learning label-label correlations in extreme
multi-label classification via label features. <em>KDD</em>, 1360–1371.
(<a href="https://doi.org/10.1145/3637528.3672063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Extreme Multi-label Text Classification (XMC) involves learning a classifier that can assign an input with a subset of most relevant labels from millions of label choices. Recent works in this domain have increasingly focused on a symmetric problem setting where both input instances and label features are short-text in nature. Short-text XMC with label features has found numerous applications in areas such as query-to-ad-phrase matching in search ads, title-based product recommendation, prediction of related searches. In this paper, we propose Gandalf, a novel approach which makes use of a label co-occurrence graph to leverage label features as additional data points to supplement the training distribution. By exploiting the characteristics of the short-text XMC problem, it leverages the label features to construct valid training instances, and uses the label graph for generating the corresponding soft-label targets, hence effectively capturing the label-label correlations. Surprisingly, models trained on these new training instances, although being less than half of the original dataset, can outperform models trained on the original dataset, particularly on the PSP@k metric for tail labels. With this insight, we aim to train existing XMC algorithms on both, the original and new training instances, leading to an average 5\% relative improvements for 6 state-of-the-art algorithms across 4 benchmark datasets consisting of up to 1.3M labels. Gandalf can be applied in a plug-and-play manner to various methods and thus forwards the state-of-the-art in the domain, without incurring any additional computational overheads. Code has been open-sourced at www.github.com/xmc-aalto/InceptionXML.},
  archive   = {C_KDD},
  author    = {Kharbanda, Siddhant and Gupta, Devaansh and Schultheis, Erik and Banerjee, Atmadeep and Hsieh, Cho-Jui and Babbar, Rohit},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672063},
  pages     = {1360–1371},
  title     = {Gandalf: Learning label-label correlations in extreme multi-label classification via label features},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Masked LoGoNet: Fast and accurate 3D image analysis for
medical domain. <em>KDD</em>, 1348–1359. (<a
href="https://doi.org/10.1145/3637528.3672069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Standard modern machine-learning-based imaging methods have faced challenges in medical applications due to the high cost of dataset construction and, thereby, the limited labeled training data available. Additionally, upon deployment, these methods are usually used to process a large volume of data on a daily basis, imposing a high maintenance cost on medical facilities. In this paper, we introduce a new neural network architecture, termed LoGoNet, with a tailored self-supervised learning (SSL) method to mitigate such challenges. LoGoNet integrates a novel feature extractor within a U-shaped architecture, leveraging Large Kernel Attention (LKA) and a dual encoding strategy to capture both long-range and short-range feature dependencies adeptly. This is in contrast to existing methods that rely on increasing network capacity to enhance feature extraction. This combination of novel techniques in our model is especially beneficial in medical image segmentation, given the difficulty of learning intricate and often irregular body organ shapes, such as the spleen. Complementary, we propose a novel SSL method tailored for 3D images to compensate for the lack of large labeled datasets. Our method combines masking and contrastive learning techniques within a multi-task learning framework and is compatible with both Vision Transformer (ViT) and CNN-based models. We demonstrate the efficacy of our methods in numerous tasks across two standard datasets (i.e., BTCV and MSD). Benchmark comparisons with eight state-of-the-art models highlight LoGoNet&#39;s superior performance in both inference time and accuracy. Code available at: https://github.com/aminK8/Masked-LoGoNet.},
  archive   = {C_KDD},
  author    = {Karimi Monsefi, Amin and Karisani, Payam and Zhou, Mengxi and Choi, Stacey and Doble, Nathan and Ji, Heng and Parthasarathy, Srinivasan and Ramnath, Rajiv},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672069},
  pages     = {1348–1359},
  title     = {Masked LoGoNet: Fast and accurate 3D image analysis for medical domain},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bivariate decision trees: Smaller, interpretable, more
accurate. <em>KDD</em>, 1336–1347. (<a
href="https://doi.org/10.1145/3637528.3671903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Univariate decision trees, commonly used since the 1950s, predict by asking questions about a single feature in each decision node. While they are interpretable, they often lack competitive predictive accuracy due to their inability to model feature correlations. Multivariate (oblique) trees use multiple features in each node, capturing high-dimensional correlations better, but sometimes they can be difficult to interpret. We advocate for a model that strikes a useful middle ground: bivariate decision trees, which use two features in each node. This typically produces trees that not only are more accurate than univariate trees, but much smaller, which offsets the small increase in node complexity and keeps them interpretable. They also help data mining by constructing new features that are useful for discrimination, and by providing a form of supervised, hierarchical 2D visualization that reveals patterns such as clusters or linear structure. We give two new algorithms to learn bivariate trees: a fast one based on CART; and a slower one based on alternating optimization with a feature regularization term, which produces the best trees while still scaling to large datasets.},
  archive   = {C_KDD},
  author    = {Kairgeldin, Rasul and Carreira-Perpi\~{n}\&#39;{a}n, Miguel \&#39;{A}.},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671903},
  pages     = {1336–1347},
  title     = {Bivariate decision trees: Smaller, interpretable, more accurate},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sketch-based replay projection for continual learning.
<em>KDD</em>, 1325–1335. (<a
href="https://doi.org/10.1145/3637528.3671714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Continual learning closely emulates human learning, which allows a model to learn from a stream of tasks sequentially without forgetting previously learned knowledge. Replay-based continual learning methods mitigate forgetting and improve performance by reintroducing data belonging to old tasks, however a replay method&#39;s performance may deteriorate when the reintroduced data does not effectively represent all experienced data. To address this concern, we propose the Sketch-based Replay Projection (SRP) method to capture and retain the original data stream&#39;s distribution within stored memory. SRP augments existing replay frameworks and introduces a two-fold approach. First, we develop a sketch-based sample selection technique to approximate feature distributions within distinct tasks, thereby capturing a wide distribution of examples for subsequent replay. Second, we propose a data compression method which projects examples into a reduced-dimensional space while preserving inter-example relationships and emphasizing inter-class disparities, encouraging diverse representations of each class while maintaining memory requirements similar to existing replay methodologies. Our experimental results demonstrate that SRP enhances replay diversity and improves the performance of existing replay models.},
  archive   = {C_KDD},
  author    = {Julian, Jack and Koh, Yun Sing and Bifet, Albert},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671714},
  pages     = {1325–1335},
  title     = {Sketch-based replay projection for continual learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RCTD: Reputation-constrained truth discovery in sybil attack
crowdsourcing environment. <em>KDD</em>, 1313–1324. (<a
href="https://doi.org/10.1145/3637528.3671803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sybil attacks are a prevalent concern within the realm of crowdsourcing, underscoring the significance of quality control in this domain. Truth discovery has been extensively studied to deduce the most trustworthy information from conflicting data based on the principle that reliable workers yield reliable answers. However, existing truth discovery approaches overlook the metric of workers&#39; reputations, e.g., workers&#39; historical approval rates on crowdsourcing platforms, despite being inflated and noisy, they offer a rough indication of workers&#39; ability. In this paper, we first refine the approval rate using Wilson Lower Bound to enhance its confidence, and then mitigate its noise and inflation through a method based on ranking similarity. Specifically, we propose a method called RCTD (Reputation-Constrained Truth Discovery), which introduces a similarity metric between the rankings of workers&#39; weights and the refined approval rates. This metric serves as a penalizing factor in the objective function of the truth discovery, restricting workers&#39; weights to avoid excessively deviating from their historical reputation during the weight estimation process. We solve the objective function by introducing the block coordinate descent coupled with heuristics approach method. Experimental results on real-world datasets demonstrate that our approach achieves more accurate inference of true results in the Sybil attack environment compared to the state-of-the-art methods.},
  archive   = {C_KDD},
  author    = {Jin, Xing and Gong, Zhihai and Jiang, Jiuchuan and Wang, Chao and Zhang, Jian and Wang, Zhen},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671803},
  pages     = {1313–1324},
  title     = {RCTD: Reputation-constrained truth discovery in sybil attack crowdsourcing environment},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Killing two birds with one stone: Cross-modal reinforced
prompting for graph and language tasks. <em>KDD</em>, 1301–1312. (<a
href="https://doi.org/10.1145/3637528.3671742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, Graph Neural Networks (GNNs) and Large Language Models (LLMs) have exhibited remarkable capability in addressing different graph learning and natural language tasks, respectively. Motivated by this, integrating LLMs with GNNs has been increasingly studied to acquire transferable knowledge across modalities, which leads to improved empirical performance in language and graph domains. However, existing studies mainly focused on a single-domain scenario by designing complicated integration techniques to manage multimodal data effectively. Therefore, a concise and generic learning framework for multi-domain tasks, i.e., graph and language domains, is highly desired yet remains under-exploited due to two major challenges. First, the language corpus of downstream tasks differs significantly from graph data, making it hard to bridge the knowledge gap between modalities. Second, not all knowledge demonstrates immediate benefits for downstream tasks, potentially introducing disruptive noise to context-sensitive models like LLMs. To tackle these challenges, we propose a novel plug-and-play framework for incorporating a lightweight cross-domain prompting method into both language and graph learning tasks. Specifically, we first convert the textual input into a domain-scalable prompt, which not only preserves the semantic and logical contents of the textual input, but also highlights related graph information as external knowledge for different domains. Then, we develop a reinforcement learning-based method to learn the optimal edge selection strategy for useful knowledge extraction, which profoundly sharpens the multi-domain model capabilities. In addition, we introduce a joint multi-view optimization module to regularize agent-level collaborative learning across two domains. Finally, extensive empirical justifications over 23 public and synthetic datasets demonstrate that our approach can be applied to diverse multi-domain tasks more accurately, robustly, and reasonably, and improve the performances of the state-of-the-art graph and language models in different learning paradigms.},
  archive   = {C_KDD},
  author    = {Jiang, Wenyuan and Wu, Wenwei and Zhang, Le and Yuan, Zixuan and Xiang, Jian and Zhou, Jingbo and Xiong, Hui},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671742},
  pages     = {1301–1312},
  title     = {Killing two birds with one stone: Cross-modal reinforced prompting for graph and language tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatic multi-task learning framework with neural
architecture search in recommendations. <em>KDD</em>, 1290–1300. (<a
href="https://doi.org/10.1145/3637528.3671715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-task learning (MTL), which aims to make full use of knowledge contained in multiple tasks to enhance overall performance and efficiency, has been broadly applied in recommendations. The main challenge for MTL models is negative transfer. Existing MTL models, mainly built on the Mixture-of-Experts (MoE) structure, seek enhancements in performance through feature selection and specific expert sharing mode design. However, one expert sharing mode may not be universally applicable due to the complex correlations and diverse demands among various tasks. Additionally, homogeneous expert architectures in such models further limit their performance. To address these issues, in this paper, we propose an innovative automatic MTL framework, AutoMTL, leveraging neural architecture search (NAS) to design optimal expert architectures and sharing modes. The Dual-level Expert Sharing mode and Architecture Navigator (DESAN) search space of AutoMTL can not only efficiently explore expert sharing modes and feature selection schemes but also focus on the architectures of expert subnetworks. Along with this, we introduce an efficient Progressively Discretizing Differentiable Architecture Search (PD-DARTS) algorithm for search space exploration. Extensive experiments demonstrate that AutoMTL can consistently outperform state-of-the-art, human-crafted MTL models. Moreover, the insights obtained from the discovered architectures provide valuable guidance for building new multi-task recommendation models.},
  archive   = {C_KDD},
  author    = {Jiang, Shen and Zhu, Guanghui and Wang, Yue and Yuan, Chunfeng and Huang, Yihua},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671715},
  pages     = {1290–1300},
  title     = {Automatic multi-task learning framework with neural architecture search in recommendations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mutual distillation extracting spatial-temporal knowledge
for lightweight multi-channel sleep stage classification. <em>KDD</em>,
1279–1289. (<a href="https://doi.org/10.1145/3637528.3671981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sleep stage classification has important clinical significance for the diagnosis of sleep-related diseases. To pursue more accurate sleep stage classification, multi-channel sleep signals are widely used due to the rich spatial-temporal information contained. However, it leads to a great increment in the size and computational costs, which constrain the application of multi-channel sleep models on hardware devices. Knowledge distillation is an effective way to compress models, yet existing knowledge distillation methods cannot fully extract and transfer the spatial-temporal knowledge in the multi-channel sleep signals. To solve the problem, we propose a general knowledge distillation framework for multi-channel sleep stage classification called spatial-temporal mutual distillation. Based on the spatial relationship of human body and the temporal transition rules of sleep signals, the spatial and temporal modules are designed to extract the spatial-temporal knowledge, thus help the lightweight student model learn the rich spatial-temporal knowledge from large-scale teacher model. The mutual distillation framework transfers the spatial-temporal knowledge mutually. Teacher model and student model can learn from each other, further improving the student model. The results on the ISRUC-III and MASS-SS3 datasets show that our proposed framework compresses the sleep models effectively with minimal performance loss and achieves the state-of-the-art performance compared to the baseline methods.},
  archive   = {C_KDD},
  author    = {Jia, Ziyu and Wang, Haichao and Liu, Yucheng and Jiang, Tianzi},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671981},
  pages     = {1279–1289},
  title     = {Mutual distillation extracting spatial-temporal knowledge for lightweight multi-channel sleep stage classification},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FairMatch: Promoting partial label learning by unlabeled
samples. <em>KDD</em>, 1269–1278. (<a
href="https://doi.org/10.1145/3637528.3671685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies the semi-supervised partial label learning (SSPLL) problem, which aims to improve the partial label learning (PLL) by leveraging unlabeled samples. Both the existing SSPLL methods and the semi-supervised learning methods exploit the information in unlabeled samples by selecting high-confidence unlabeled samples as the pseudo labels based on the maximum value of the model output. However, the scarcity of labeled samples and the ambiguity from partial labels skew this strategy towards an unfair selection of high-confidence samples on each class, most notably during the initial phases of training, resulting in slower training and performance degradation. In this paper, we propose a novel method FairMatch, which adopts a learning state aware self-adaptive threshold for selecting the same number of high-confidence samples on each class, and uses augmentation consistency to incorporate the unlabeled samples to promote PLL. In addition, we adopt the candidate label disambiguation to utilize the partial labeled samples and mix up the partial labeled samples and the selected high-confidence unlabeled samples to prevent the model from overfitting on partial label samples. FairMatch can achieve maximum accuracy improvements of 9.53\%, 4.9\%, and 16.45\% on CIFAR-10, CIFAR-100, and CIFAR-100H, respectively. The codes can be found at https://github.com/jhjiangSEU/FairMatch.},
  archive   = {C_KDD},
  author    = {Jiang, Jiahao and Jia, Yuheng and Liu, Hui and Hou, Junhui},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671685},
  pages     = {1269–1278},
  title     = {FairMatch: Promoting partial label learning by unlabeled samples},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MemMap: An adaptive and latent memory structure for dynamic
graph learning. <em>KDD</em>, 1257–1268. (<a
href="https://doi.org/10.1145/3637528.3672060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dynamic graph learning has attracted much attention in recent years due to the fact that most of the real-world graphs are dynamic and evolutionary. As a result, many dynamic learning methods have been proposed to cope with the changes of node states over time. Among these studies, a critical issue is how to update the representations of nodes when new temporal events are observed. In this paper, we provide a novel memory structure - Memory Map (MemMap) for this problem. MemMap is an adaptive and evolutionary latent memory space, where each cell corresponds to an evolving &quot;topic&quot; of the dynamic graph. Moreover, the representation of a node is generated from its semantically correlated memory cells, rather than linked neighbors of the node. We have conducted experiments on real-world datasets and compared our method with the SOTA ones. It can be concluded that: 1) By constructing an adaptive and evolving memory structure during the dynamic learning process, our method can capture the dynamic graph changes, and the learned MemMap is actually a compact evolving structure organized according to the latent &quot;topics&quot; of the graph nodes. 2) Our research suggests that it is a more effective and efficient way to generate node representations from a latent semantic space (like MemMap in our method) than from directly connected neighbors (like most of the previous graph learning methods). The reason is that the number of memory cells in latent space could be much smaller than the number of nodes in a real-world graph, and the representation learning process could well balance the global and local message passing by leveraging the semantic similarity of graph nodes via the correlated memory cells.},
  archive   = {C_KDD},
  author    = {Ji, Shuo and Liu, Mingzhe and Sun, Leilei and Liu, Chuanren and Zhu, Tongyu},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672060},
  pages     = {1257–1268},
  title     = {MemMap: An adaptive and latent memory structure for dynamic graph learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tensorized unaligned multi-view clustering with multi-scale
representation learning. <em>KDD</em>, 1246–1256. (<a
href="https://doi.org/10.1145/3637528.3671689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Unaligned Multi-view Clustering (UMC) problem is currently receiving widespread attention, focusing on clustering unaligned multi-view data generated in real-world applications. Although some algorithms have emerged to address this issue, there still exist the following drawbacks: 1) The fully unknown correspondence of samples across views can significantly limit the exploration of consistent clustering structure. 2) The fixed representation space makes it difficult to mine the comprehensive information in the original data. 3) Unbiased tensor rank approximation is desired to capture the high-order correlation among different views. To address these issues, we proposed a novel UMC framework termed Tensorized Unaligned Multi-view Clustering with Multi-scale Representation Learning (TUMCR). Specifically, TUMCR designs a multi-scale representation learning and alignment framework, which constructs multi-scale representation spaces to comprehensively explore the unknown correspondence across views. Then, a tensorial multi-scale fusion module is proposed to fuse multi-scale representations and explore the high-order correlation hidden in different views, which utilizes the Enhanced Tensor Rank (ETR) to learn the low-rank structure. Furthermore, TUMCR is solved by an efficient algorithm with good convergence. Extensive experiments on different types of datasets demonstrate the effectiveness and superiority of our TUMCR compared with state-of-the-art methods. Our code is publicly available at: https://github.com/jijintian/TUMCR.},
  archive   = {C_KDD},
  author    = {Ji, Jintian and Feng, Songhe and Li, Yidong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671689},
  pages     = {1246–1256},
  title     = {Tensorized unaligned multi-view clustering with multi-scale representation learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Addressing prediction delays in time series forecasting: A
continuous GRU approach with derivative regularization. <em>KDD</em>,
1234–1245. (<a href="https://doi.org/10.1145/3637528.3671969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Time series forecasting has been an essential field in many different application areas, including economic analysis, meteorology, and so forth. The majority of time series forecasting models are trained using the mean squared error (MSE). However, this training based on MSE causes a limitation known as prediction delay. The prediction delay, which implies the ground-truth precedes the prediction, can cause serious problems in a variety of fields, e.g., finance and weather forecasting --- as a matter of fact, predictions succeeding ground-truth observations are not practically meaningful although their MSEs can be low. This paper proposes a new perspective on traditional time series forecasting tasks and introduces a new solution to mitigate the prediction delay. We introduce a continuous-time gated recurrent unit (GRU) based on the neural ordinary differential equation (NODE) which can supervise explicit time-derivatives. We generalize the GRU architecture in a continuous-time manner and minimize the prediction delay through our time-derivative regularization. Our method outperforms in metrics such as MSE, Dynamic Time Warping (DTW) and Time Distortion Index (TDI). In addition, we demonstrate the low prediction delay of our method in a variety of datasets.},
  archive   = {C_KDD},
  author    = {Jhin, Sheo Yon and Kim, Seojin and Park, Noseong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671969},
  pages     = {1234–1245},
  title     = {Addressing prediction delays in time series forecasting: A continuous GRU approach with derivative regularization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On (normalised) discounted cumulative gain as an off-policy
evaluation metric for top-n recommendation. <em>KDD</em>, 1222–1233. (<a
href="https://doi.org/10.1145/3637528.3671687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Approaches to recommendation are typically evaluated in one of two ways: (1) via a (simulated) online experiment, often seen as the gold standard, or (2) via some offline evaluation procedure, where the goal is to approximate the outcome of an online experiment. Several offline evaluation metrics have been adopted in the literature, inspired by ranking metrics prevalent in the field of Information Retrieval. (Normalised) Discounted Cumulative Gain (nDCG) is one such metric that has seen widespread adoption in empirical studies, and higher (n)DCG values have been used to present new methods as the state-of-the-art in top-n recommendation for many years.Our work takes a critical look at this approach, and investigates when we can expect such metrics to approximate the gold standard outcome of an online experiment. We formally present the assumptions that are necessary to consider DCG an unbiased estimator of online reward and provide a derivation for this metric from first principles, highlighting where we deviate from its traditional uses in IR. Importantly, we show that normalising the metric renders it inconsistent, in that even when DCG is unbiased, ranking competing methods by their normalised DCG can invert their relative order. Through a correlation analysis between off- and on-line experiments conducted on a large-scale recommendation platform, we show that our unbiased DCG estimates strongly correlate with online reward, even when some of the metric&#39;s inherent assumptions are violated. This statement no longer holds for its normalised variant, suggesting that nDCG&#39;s practical utility may be limited.},
  archive   = {C_KDD},
  author    = {Jeunen, Olivier and Potapov, Ivan and Ustimenko, Aleksei},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671687},
  pages     = {1222–1233},
  title     = {On (Normalised) discounted cumulative gain as an off-policy evaluation metric for top-n recommendation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FreQuant: A reinforcement-learning based adaptive portfolio
optimization with multi-frequency decomposition. <em>KDD</em>,
1211–1221. (<a href="https://doi.org/10.1145/3637528.3671668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {How can we leverage inherent frequency features of stock signals for effective portfolio optimization? Portfolio optimization in the domain of finance revolves around strategically allocating assets to maximize returns. Recent advancements highlight the efficacy of deep learning and reinforcement learning (RL) in capturing temporal asset patterns for portfolio optimization. However, previous methodologies focusing on time-domain often fail to detect sudden market shifts and abrupt events because their models are overly tailored to prevalent patterns, resulting in significant losses.In this paper, we propose FreQuant (Adaptive Portfolio Optimization via Multi-&amp;lt;u&amp;gt;Fre&amp;lt;/u&amp;gt;quency &amp;lt;u&amp;gt;Quant&amp;lt;/u&amp;gt;itative Analysis), an effective deep RL framework for portfolio optimization that fully operates in the frequency domain, tackling the limitations of time domain-focused models. By bringing the analysis into the frequency domain with the Discrete Fourier Transform, our framework captures both prominent and subtle market frequencies, enhancing its adaptability and stability in response to market shifts. This approach allows FreQuant to adeptly identify primary asset patterns while also effectively responding to less common and abrupt market events, providing a more accurate and comprehensive asset representation. Empirical validation on diverse real-world trading datasets underscores the remarkable performance of FreQuant, showing its superiority in terms of profitability. Notably, FreQuant achieves up to 2.1x higher Annualized Rate of Return and 2.9x higher Portfolio Value than the best-performing competitors.},
  archive   = {C_KDD},
  author    = {Jeon, Jihyeong and Park, Jiwon and Park, Chanhee and Kang, U},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671668},
  pages     = {1211–1221},
  title     = {FreQuant: A reinforcement-learning based adaptive portfolio optimization with multi-frequency decomposition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Promoting fairness and priority in selecting k-winners using
IRV. <em>KDD</em>, 1199–1210. (<a
href="https://doi.org/10.1145/3637528.3671735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate the problem of finding winner(s) given a large number of users&#39; (voters&#39;) preferences casted as ballots, one from each of the m users, where each ballot is a ranked order of preference of up to ℓ out of n items (candidates). Given a group protected attribute with k different values and a priority that imposes a selection order among these groups, the goal is to satisfy the priority order and select a winner per group that is most representative. It is imperative that at times the original users&#39; preferences may require further manipulation to meet these fairness and priority requirement. We consider manipulation by modifications and formalize the margin finding problem under modification problem. We study the suitability of Instant Run-off Voting (IRV) as a preference aggregation method and demonstrate its advantages over positional methods. We present a suite of technical results on the hardness of the problem, design algorithms with theoretical guarantees and further investigate efficiency opportunities. We present exhaustive experimental evaluations using multiple applications and large-scale datasets to demonstrate the effectiveness of IRV, and efficacy of our designed solutions qualitatively and scalability-wise.},
  archive   = {C_KDD},
  author    = {Islam, Md Mouinul and Vahidi, Soroush and Schieber, Baruch and Basu Roy, Senjuti},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671735},
  pages     = {1199–1210},
  title     = {Promoting fairness and priority in selecting k-winners using IRV},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient discovery of time series motifs under both length
differences and warping. <em>KDD</em>, 1188–1198. (<a
href="https://doi.org/10.1145/3637528.3671726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Over the past two decades, time series motif discovery has become a crucial subroutine for many time series data mining tasks; concurrently, it has been established that Dynamic Time Warping (DTW) outperforms other similarity measures like Euclidean Distance in most scenarios. Against this backdrop, a DTW motif discovery algorithm was recently developed; however, it is confined to working with fixed-length subsequences. In this work, we propose a novel approach that allows us to find motifs under both length differences and warping. Our algorithm exploits a promising time series representation called Spikelets and introduces the first lower bound for DTW in the Spikelet space. Extensive empirical studies demonstrate that our method scales effectively across various real-world datasets and efficiently identifies DTW motif pairs of different lengths.},
  archive   = {C_KDD},
  author    = {Imamura, Makoto and Nakamura, Takaaki},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671726},
  pages     = {1188–1198},
  title     = {Efficient discovery of time series motifs under both length differences and warping},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uplift modelling via gradient boosting. <em>KDD</em>,
1177–1187. (<a href="https://doi.org/10.1145/3637528.3672019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Gradient Boosting machine learning ensemble algorithm, well-known for its proficiency and superior performance in intricate machine learning tasks, has encountered limited success in the realm of uplift modeling. Uplift modeling is a challenging task that necessitates a known target for the precise computation of the training gradient. The prevailing two-model strategies, which separately model treatment and control outcomes, are encumbered with limitations as they fail to directly tackle the uplift problem.This paper presents an innovative approach to uplift modeling that employs Gradient Boosting. Unlike previous works, our algorithm utilizes multioutput boosting model and calculates the uplift gradient based on intermediate surrogate predictions and directly models the concealed target. This method circumvents the requirement for a known target and addresses the uplift problem more effectively than existing solutions.Moreover, we broaden the scope of this solution to encompass multitreatment settings, thereby enhancing its applicability. This novel approach not only overcomes the limitations of the traditional two-model strategies but also paves the way for more effective and efficient uplift modeling using Gradient Boosting.},
  archive   = {C_KDD},
  author    = {Ibragimov, Bulat and Vakhrushev, Anton},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672019},
  pages     = {1177–1187},
  title     = {Uplift modelling via gradient boosting},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learn together stop apart: An inclusive approach to ensemble
pruning. <em>KDD</em>, 1166–1176. (<a
href="https://doi.org/10.1145/3637528.3672018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Gradient Boosting is a leading learning method that builds ensembles and adapts their sizes to particular tasks, consistently delivering top-tier results across various applications. However, determining the optimal number of models in the ensemble remains a critical yet underexplored aspect. Traditional approaches assume a universal ensemble size effective for all data points, which may not always hold true due to data heterogeneity.This paper introduces an adaptive approach to early stopping in Gradient Boosting, addressing data heterogeneity by assigning different stop moments to different data regions at inference time while still training a common ensemble on the entire dataset. We propose two methods: Direct Supervised Partition (DSP) and Indirect Supervised Partition (ISP). The DSP method uses a decision tree to partition the data based on learning curves, while ISP leverages the dataset&#39;s geometric and target distribution characteristics.An effective validation protocol is developed to determine the optimal number of early stopping regions or detect when the heterogeneity assumption does not hold. Experiments using state-of-the-art implementations of Gradient Boosting, LightGBM, and CatBoost, on standard benchmarks demonstrate that our methods enhance model precision by up to 2\%, underscoring the significance of this research direction. This approach does not increase computational complexity and can be easily integrated into existing learning pipelines.},
  archive   = {C_KDD},
  author    = {Ibragimov, Bulat and Gusev, Gleb},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672018},
  pages     = {1166–1176},
  title     = {Learn together stop apart: An inclusive approach to ensemble pruning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RC-mixup: A data augmentation strategy against noisy data
for regression tasks. <em>KDD</em>, 1155–1165. (<a
href="https://doi.org/10.1145/3637528.3671993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of robust data augmentation for regression tasks in the presence of noisy data. Data augmentation is essential for generalizing deep learning models, but most of the techniques like the popular Mixup are primarily designed for classification tasks on image data. Recently, there are also Mixup techniques that are specialized to regression tasks like C-Mixup. In comparison to Mixup, which takes linear interpolations of pairs of samples, C-Mixup is more selective in which samples to mix based on their label distances for better regression performance. However, C-Mixup does not distinguish noisy versus clean samples, which can be problematic when mixing and lead to suboptimal model performance. At the same time, robust training has been heavily studied where the goal is to train accurate models against noisy data through multiple rounds of model training. We thus propose our data augmentation strategy RC-Mixup, which tightly integrates C-Mixup with multi-round robust training methods for a synergistic effect. In particular, C-Mixup improves robust training in identifying clean data, while robust training provides cleaner data to C-Mixup for it to perform better. A key advantage of RC-Mixup is that it is data-centric where the robust model training algorithm itself does not need to be modified, but can simply benefit from data mixing. We show in our experiments that RC-Mixup significantly outperforms C-Mixup and robust training baselines on noisy data benchmarks and can be integrated with various robust training methods.},
  archive   = {C_KDD},
  author    = {Hwang, Seong-Hyeon and Kim, Minsu and Whang, Steven Euijong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671993},
  pages     = {1155–1165},
  title     = {RC-mixup: A data augmentation strategy against noisy data for regression tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EntropyStop: Unsupervised deep outlier detection with loss
entropy. <em>KDD</em>, 1143–1154. (<a
href="https://doi.org/10.1145/3637528.3671943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unsupervised Outlier Detection (UOD) is an important data mining task. With the advance of deep learning, deep Outlier Detection (OD) has received broad interest. Most deep UOD models are trained exclusively on clean datasets to learn the distribution of the normal data, which requires huge manual efforts to clean the real-world data if possible. Instead of relying on clean datasets, some approaches directly train and detect on unlabeled contaminated datasets, leading to the need for methods that are robust to such challenging conditions. Ensemble methods emerged as a superior solution to enhance model robustness against contaminated training sets. However, the training time is greatly increased by the ensemble mechanism.In this study, we investigate the impact of outliers on training, aiming to halt training on unlabeled contaminated datasets before performance degradation. Initially, we noted that blending normal and anomalous data causes AUC fluctuations-a label-dependent measure of detection accuracy. To circumvent the need for labels, we propose a zero-label entropy metric named Loss Entropy for loss distribution, enabling us to infer optimal stopping points for training without labels. Meanwhile, a negative correlation between entropy metric and the label-based AUC score is demonstrated by theoretical proofs. Based on this, an automated early-stopping algorithm called EntropyStop is designed to halt training when loss entropy suggests the maximum model detection capability. We conduct extensive experiments on ADBench (including 47 real datasets), and the overall results indicate that AutoEncoder (AE) enhanced by our approach not only achieves better performance than ensemble AEs but also requires under 2\% of training time. Lastly, loss entropy and EntropyStop are evaluated on other deep OD models, exhibiting their broad potential applicability.},
  archive   = {C_KDD},
  author    = {Huang, Yihong and Zhang, Yuang and Wang, Liping and Zhang, Fan and Lin, Xuemin},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671943},
  pages     = {1143–1154},
  title     = {EntropyStop: Unsupervised deep outlier detection with loss entropy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Can modifying data address graph domain adaptation?
<em>KDD</em>, 1131–1142. (<a
href="https://doi.org/10.1145/3637528.3672058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph neural networks (GNNs) have demonstrated remarkable success in numerous graph analytical tasks. Yet, their effectiveness is often compromised in real-world scenarios due to distribution shifts, limiting their capacity for knowledge transfer across changing environments or domains. Recently, Unsupervised Graph Domain Adaptation (UGDA) has been introduced to resolve this issue. UGDA aims to facilitate knowledge transfer from a labeled source graph to an unlabeled target graph. Current UGDA efforts primarily focus on model-centric methods, such as employing domain invariant learning strategies and designing model architectures. However, our critical examination reveals the limitations inherent to these model-centric methods, while a data-centric method allowed to modify the source graph provably demonstrates considerable potential. This insight motivates us to explore UGDA from a data-centric perspective. By revisiting the theoretical generalization bound for UGDA, we identify two data-centric principles for UGDA: alignment principle and rescaling principle. Guided by these principles, we propose GraphAlign, a novel UGDA method that generates a small yet transferable graph. By exclusively training a GNN on this new graph with classic Empirical Risk Minimization (ERM), GraphAlign attains exceptional performance on the target graph. Extensive experiments under various transfer scenarios demonstrate the GraphAlign outperforms the best baselines by an average of 2.16\%, training on the generated graph as small as 0.25~1\% of the original training graph.},
  archive   = {C_KDD},
  author    = {Huang, Renhong and Xu, Jiarong and Jiang, Xin and An, Ruichuan and Yang, Yang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672058},
  pages     = {1131–1142},
  title     = {Can modifying data address graph domain adaptation?},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prompt perturbation in retrieval-augmented generation based
large language models. <em>KDD</em>, 1119–1130. (<a
href="https://doi.org/10.1145/3637528.3671932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The robustness of large language models (LLMs) becomes increasingly important as their use rapidly grows in a wide range of domains. Retrieval-Augmented Generation (RAG) is considered as a means to improve the trustworthiness of text generation from LLMs. However, how the outputs from RAG-based LLMs are affected by slightly different inputs is not well studied. In this work, we find that the insertion of even a short prefix to the prompt leads to the generation of outputs far away from factually correct answers. We systematically evaluate the effect of such prefixes on RAG by introducing a novel optimization technique called Gradient Guided Prompt Perturbation (GGPP). GGPP achieves a high success rate in steering outputs of RAG-based LLMs to targeted wrong answers. It can also cope with instructions in the prompts requesting to ignore irrelevant context. We also exploit LLMs&#39; neuron activation difference between prompts with and without GGPP perturbations to give a method that improves the robustness of RAG-based LLMs through a highly effective detector trained on neuron activation triggered by GGPP generated prompts. Our evaluation on open-sourced LLMs demonstrates the effectiveness of our methods.},
  archive   = {C_KDD},
  author    = {Hu, Zhibo and Wang, Chen and Shu, Yanfeng and Paik, Hye-Young and Zhu, Liming},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671932},
  pages     = {1119–1130},
  title     = {Prompt perturbation in retrieval-augmented generation based large language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Privacy-preserved neural graph databases. <em>KDD</em>,
1108–1118. (<a href="https://doi.org/10.1145/3637528.3671678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the era of large language models (LLMs), efficient and accurate data retrieval has become increasingly crucial for the use of domain-specific or private data in the retrieval augmented generation (RAG). Neural graph databases (NGDBs) have emerged as a powerful paradigm that combines the strengths of graph databases (GDBs) and neural networks to enable efficient storage, retrieval, and analysis of graph-structured data which can be adaptively trained with LLMs. The usage of neural embedding storage and Complex neural logical Query Answering (CQA) provides NGDBs with generalization ability. When the graph is incomplete, by extracting latent patterns and representations, neural graph databases can fill gaps in the graph structure, revealing hidden relationships and enabling accurate query answering. Nevertheless, this capability comes with inherent trade-offs, as it introduces additional privacy risks to the domain-specific or private databases. Malicious attackers can infer more sensitive information in the database using well-designed queries such as from the answer sets of where Turing Award winners born before 1950 and after 1940 lived, the living places of Turing Award winner Hinton are probably exposed, although the living places may have been deleted in the training stage due to the privacy concerns. In this work, we propose a privacy-preserved neural graph database (P-NGDB) framework to alleviate the risks of privacy leakage in NGDBs. We introduce adversarial training techniques in the training stage to enforce the NGDBs to generate indistinguishable answers when queried with private information, enhancing the difficulty of inferring sensitive information through combinations of multiple innocuous queries. Extensive experimental results on three datasets show that our framework can effectively protect private information in the graph database while delivering high-quality public answers responses to queries. The code is available at https://github.com/HKUST-KnowComp/PrivateNGDB.},
  archive   = {C_KDD},
  author    = {Hu, Qi and Li, Haoran and Bai, Jiaxin and Wang, Zihao and Song, Yangqiu},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671678},
  pages     = {1108–1118},
  title     = {Privacy-preserved neural graph databases},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Is aggregation the only choice? Federated learning via
layer-wise model recombination. <em>KDD</em>, 1096–1107. (<a
href="https://doi.org/10.1145/3637528.3671722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although Federated Learning (FL) enables global model training across clients without compromising their raw data, due to the unevenly distributed data among clients, existing Federated Averaging (FedAvg)-based methods suffer from the problem of low inference performance. Specifically, different data distributions among clients lead to various optimization directions of local models. Aggregating local models usually results in a low-generalized global model, which performs worse on most of the clients. To address the above issue, inspired by the observation from a geometric perspective that a well-generalized solution is located in a flat area rather than a sharp area, we propose a novel and heuristic FL paradigm named FedMR (Federated Model Recombination). The goal of FedMR is to guide the recombined models to be trained towards a flat area. Unlike conventional FedAvg-based methods, in FedMR, the cloud server recombines collected local models by shuffling each layer of them to generate multiple recombined models for local training on clients rather than an aggregated global model. Since the area of the flat area is larger than the sharp area, when local models are located in different areas, recombined models have a higher probability of locating in a flat area. When all recombined models are located in the same flat area, they are optimized towards the same direction. We theoretically analyze the convergence of model recombination. Experimental results show that, compared with state-of-the-art FL methods, FedMR can significantly improve the inference accuracy without exposing the privacy of each client.},
  archive   = {C_KDD},
  author    = {Hu, Ming and Yue, Zhihao and Xie, Xiaofei and Chen, Cheng and Huang, Yihao and Wei, Xian and Lian, Xiang and Liu, Yang and Chen, Mingsong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671722},
  pages     = {1096–1107},
  title     = {Is aggregation the only choice? federated learning via layer-wise model recombination},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RoutePlacer: An end-to-end routability-aware placer with
graph neural network. <em>KDD</em>, 1085–1095. (<a
href="https://doi.org/10.1145/3637528.3671895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Placement is a critical and challenging step of modern chip design, with routability being an essential indicator of placement quality. Current routability-oriented placers typically apply an iterative two-stage approach, wherein the first stage generates a placement solution, and the second stage provides non-differentiable routing results to heuristically improve the solution quality. This method hinders jointly optimizing the routability aspect during placement. To address this problem, this work introduces RoutePlacer, an end-to-end routability-aware placement method. It trains RouteGNN, a customized graph neural network, to efficiently and accurately predict routability by capturing and fusing geometric and topological representations of placements. Well-trained RouteGNN then serves as a differentiable approximation of routability, enabling end-to-end gradient-based routability optimization. In addition, RouteGNN can improve two-stage placers as a plug-and-play alternative to external routers. Our experiments on DREAMPlace, an open-source AI4EDA platform, show that RoutePlacer can reduce Total Overflow by up to 16\% while maintaining routed wirelength, compared to the state-of-the-art; integrating RouteGNN within two-stage placers leads to a 44\% reduction in Total Overflow without compromising wirelength.},
  archive   = {C_KDD},
  author    = {Hou, Yunbo and Ye, Haoran and Zhang, Yingxue and Xu, Siyuan and Song, Guojie},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671895},
  pages     = {1085–1095},
  title     = {RoutePlacer: An end-to-end routability-aware placer with graph neural network},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Budgeted multi-armed bandits with asymmetric confidence
intervals. <em>KDD</em>, 1073–1084. (<a
href="https://doi.org/10.1145/3637528.3671833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the stochastic Budgeted Multi-Armed Bandit (MAB) problem, where a player chooses from K arms with unknown expected rewards and costs. The goal is to maximize the total reward under a budget constraint. A player thus seeks to choose the arm with the highest reward-cost ratio as often as possible. Current approaches for this problem have several issues, which we illustrate. To overcome them, we propose a new upper confidence bound (UCB) sampling policy, \o{}mega-UCB, that uses asymmetric confidence intervals. These intervals scale with the distance between the sample mean and the bounds of a random variable, yielding a more accurate and tight estimation of the reward-cost ratio compared to our competitors. We show that our approach has sublinear instance-dependent regret in general and logarithmic regret for parameter ρ ≥ 1, and that it outperforms existing policies consistently in synthetic and real settings.},
  archive   = {C_KDD},
  author    = {Heyden, Marco and Arzamasov, Vadim and Fouch\&#39;{e}, Edouard and B\&quot;{o}hm, Klemens},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671833},
  pages     = {1073–1084},
  title     = {Budgeted multi-armed bandits with asymmetric confidence intervals},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Double correction framework for denoising recommendation.
<em>KDD</em>, 1062–1072. (<a
href="https://doi.org/10.1145/3637528.3671692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As its availability and generality in online services, implicit feedback is more commonly used in recommender systems. However, implicit feedback usually presents noisy samples in real-world recommendation scenarios (such as misclicks or non-preferential behaviors), which will affect precise user preference learning. To overcome the noisy samples problem, a popular solution is based on dropping noisy samples in the model training phase, which follows the observation that noisy samples have higher training losses than clean samples. Despite the effectiveness, we argue that this solution still has limits. (1) High training losses can result from model optimization instability or hard samples, not just noisy samples. (2) Completely dropping of noisy samples will aggravate the data sparsity, which lacks full data exploitation.To tackle the above limitations, we propose a Double Correction Framework for Denoising Recommendation (DCF), which contains two correction components from views of more precise sample dropping and avoiding more sparse data. In the sample dropping correction component, we use the loss value of the samples over time to determine whether it is noise or not, increasing dropping stability. Instead of averaging directly, we use the damping function to reduce the bias effect of outliers. Furthermore, due to the higher variance exhibited by hard samples, we derive a lower bound for the loss through concentration inequality to identify and reuse hard samples. In progressive label correction, we iteratively re-label highly deterministic noisy samples and retrain them to further improve performance. Finally, extensive experimental results on three datasets and four backbones demonstrate the effectiveness and generalization of our proposed framework.},
  archive   = {C_KDD},
  author    = {He, Zhuangzhuang and Wang, Yifan and Yang, Yonghui and Sun, Peijie and Wu, Le and Bai, Haoyue and Gong, Jinqi and Hong, Richang and Zhang, Min},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671692},
  pages     = {1062–1072},
  title     = {Double correction framework for denoising recommendation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model-agnostic random weighting for out-of-distribution
generalization. <em>KDD</em>, 1050–1061. (<a
href="https://doi.org/10.1145/3637528.3671762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite the encouraging successes in numerous applications, machine learning methods grounded on the i.i.d. assumption often experience performance deterioration when confronted with the distribution shift between training and test data. This challenge has instigated recent research endeavors focusing on out-of-distribution (OOD) generalization. A particularly pervasive and intricate OOD problem is to enhance the model&#39;s generalization ability by training it on samples drawn from a single environment. In response to the problem, we propose a simple model-agnostic method tailored for a practical OOD scenario in this paper. Our approach centers on pursuing robust weighted empirical risks, utilizing randomly shifted training distributions derived through a specific sample-based weighting strategy. Furthermore, we theoretically establish that the expected risk of the shifted training distribution can bound the expected risk of the test distribution. This theoretical foundation ensures the improved prediction performance of our method when employed in uncertain test distributions. Extensive experiments conducted on diverse real-world datasets affirm the effectiveness of our method, highlighting its potential to address the distribution shifts in machine learning applications.},
  archive   = {C_KDD},
  author    = {He, Yue and Tian, Pengfei and Xu, Renzhe and Shen, Xinwei and Zhang, Xingxuan and Cui, Peng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671762},
  pages     = {1050–1061},
  title     = {Model-agnostic random weighting for out-of-distribution generalization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient local search algorithm for large GD advertising
inventory allocation with multilinear constraints. <em>KDD</em>,
1040–1049. (<a href="https://doi.org/10.1145/3637528.3671811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Guaranteed Delivery (GD) advertising is a crucial component of the online advertising industry, and the allocation of inventory in GD advertising is an important procedure that influences directly the ability of the publisher to fulfill the requirements and increase its revenues. Nowadays, as the requirements of advertisers become more and more diverse and fine-grained, the focus ratio requirement, which states that the portion of allocated impressions of a designated contract on focus media among all possible media should be greater than another contract, often appears in business scenarios. However, taking these requirements into account brings hardness for the GD advertising inventory allocation as the focus ratio requirements involve non-convex multilinear constraints. Existing methods which rely on the convex properties are not suitable for processing this problem, while mathematical programming or constraint-based heuristic solvers are unable to produce high-quality solutions within the time limit. Therefore, we propose a local search framework to address this challenge. It incorporates four new operators designed for handling multilinear constraints and a two-mode algorithmic architecture. Experimental results demonstrate that our algorithm is able to compute high-quality allocations with better business metrics compared to the state-of-the-art mathematical programming or constraint based heuristic solvers. Moreover, our algorithm is able to handle the general multilinear constraints and we hope it could be used to solve other problems in GD advertising with similar requirements.},
  archive   = {C_KDD},
  author    = {He, Xiang and Mao, Wuyang and Xu, Zhenghang and Gu, Yuanzhe and Huang, Yundu and Zu, Zhonglin and Wang, Liang and Zhao, Mengyu and Zou, Mengchuan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671811},
  pages     = {1040–1049},
  title     = {An efficient local search algorithm for large GD advertising inventory allocation with multilinear constraints},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A unified core structure in multiplex networks: From finding
the densest subgraph to modeling user engagement. <em>KDD</em>,
1028–1039. (<a href="https://doi.org/10.1145/3637528.3672011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many complex systems, the interactions between objects span multiple aspects. Multiplex networks are accurate paradigms to model such systems, where each edge is associated with a type. A key graph mining primitive is extracting dense subgraphs, and this has led to interesting notions such as k-cores, known as building blocks of complex networks. Despite recent attempts to extend the notion of core to multiplex networks, existing studies suffer from a subset of the following limitations: They (1) force all nodes to exhibit their high degree in the same set of relation types while in multiplex networks some connection types can be noisy for some nodes, (2) either require high computational cost or miss the complex information of multiplex networks, and (3) assume the same importance for all relation types. We introduce Score, a novel and unifying family of dense structures in multiplex networks that uses a function S(.) to summarize the degree vector of each node. We then discuss how one can choose a proper S(.) from the data. To demonstrate the usefulness of Scores, we focus on finding the densest subgraph as well as modeling user engagement in multiplex networks. We present a new density measure in multiplex networks and discuss its advantages over existing density measures. We show that the problem of finding the densest subgraph in multiplex networks is NP-hard and design an efficient approximation algorithm based on Scores. Finally, we present a new mathematical model of user engagement in the presence of different relation types. Our experiments shows the efficiency and effectiveness of our algorithms and supports the proposed mathematical model of user engagement.},
  archive   = {C_KDD},
  author    = {Hashemi, Farnoosh and Behrouz, Ali},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672011},
  pages     = {1028–1039},
  title     = {A unified core structure in multiplex networks: From finding the densest subgraph to modeling user engagement},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Expander hierarchies for normalized cuts on graphs.
<em>KDD</em>, 1016–1027. (<a
href="https://doi.org/10.1145/3637528.3671978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Expander decompositions of graphs have significantly advanced the understanding of many classical graph problems and led to numerous fundamental theoretical results. However, their adoption in practice has been hindered due to their inherent intricacies and large hidden factors in their asymptotic running times. Here, we introduce the first practically efficient algorithm for computing expander decompositions and their hierarchies and demonstrate its effectiveness and utility by incorporating it as the core component in a novel solver for the normalized cut graph clustering objective.Our extensive experiments on a variety of large graphs show that our expander-based algorithm outperforms state-of-the-art solvers for normalized cut with respect to solution quality by a large margin on a variety of graph classes such as citation, e-mail, and social networks or web graphs while remaining competitive in running time.},
  archive   = {C_KDD},
  author    = {Hanauer, Kathrin and Henzinger, Monika and M\&quot;{u}nk, Robin and R\&quot;{a}cke, Harald and V\&quot;{o}tsch, Maximilian},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671978},
  pages     = {1016–1027},
  title     = {Expander hierarchies for normalized cuts on graphs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adapting job recommendations to user preference drift with
behavioral-semantic fusion learning. <em>KDD</em>, 1004–1015. (<a
href="https://doi.org/10.1145/3637528.3671759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Job recommender systems are crucial for aligning job opportunities with job-seekers in online job-seeking. However, users tend to adjust their job preferences to secure employment opportunities continually, which limits the performance of job recommendations. The inherent frequency of preference drift poses a challenge to promptly and precisely capture user preferences. To address this issue, we propose a novel session-based framework, BISTRO, to timely model user preference through fusion learning of semantic and behavioral information. Specifically, BISTRO is composed of three stages: 1) coarse-grained semantic clustering, 2) fine-grained job preference extraction, and 3) personalized top-k job recommendation. Initially, BISTRO segments the user interaction sequence into sessions and leverages session-based semantic clustering to achieve broad identification of person-job matching. Subsequently, we design a hypergraph wavelet learning method to capture the nuanced job preference drift. To mitigate the effect of noise in interactions caused by frequent preference drift, we innovatively propose an adaptive wavelet filtering technique to remove noisy interaction. Finally, a recurrent neural network is utilized to analyze session-based interaction for inferring personalized preferences. Extensive experiments on three real-world offline recruitment datasets demonstrate the significant performances of our framework. Significantly, BISTRO also excels in online experiments, affirming its effectiveness in live recruitment settings. This dual success underscores the robustness and adaptability of BISTRO. The source code is available at https://github.com/Applied-Machine-Learning-Lab/BISTRO.},
  archive   = {C_KDD},
  author    = {Han, Xiao and Zhu, Chen and Hu, Xiao and Qin, Chuan and Zhao, Xiangyu and Zhu, Hengshu},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671759},
  pages     = {1004–1015},
  title     = {Adapting job recommendations to user preference drift with behavioral-semantic fusion learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AnyLoss: Transforming classification metrics into loss
functions. <em>KDD</em>, 992–1003. (<a
href="https://doi.org/10.1145/3637528.3672017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many evaluation metrics can be used to assess the performance of models in binary classification tasks. However, most of them are derived from a confusion matrix in a non-differentiable form, making it very difficult to generate a differentiable loss function that could directly optimize them. The lack of solutions to bridge this challenge not only hinders our ability to solve difficult tasks, such as imbalanced learning, but also requires the deployment of computationally expensive hyperparameter search processes in model selection. In this paper, we propose a general-purpose approach that transforms any confusion matrix-based metric into a loss function, AnyLoss, that is available in optimization processes. To this end, we use an approximation function to make a confusion matrix represented in a differentiable form, and this approach enables any confusion matrix-based metric to be directly used as a loss function. The mechanism of the approximation function is provided to ensure its operability and the differentiability of our loss functions is proved by suggesting their derivatives. We conduct extensive experiments under diverse neural networks with many datasets, and we demonstrate their general availability to target any confusion matrix-based metrics. Our method, especially, shows outstanding achievements in dealing with imbalanced datasets, and its competitive learning speed, compared to multiple baseline models, underscores its efficiency.},
  archive   = {C_KDD},
  author    = {Han, Doheon and Moniz, Nuno and Chawla, Nitesh V.},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672017},
  pages     = {992–1003},
  title     = {AnyLoss: Transforming classification metrics into loss functions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Binder: Hierarchical concept representation through order
embedding of binary vectors. <em>KDD</em>, 980–991. (<a
href="https://doi.org/10.1145/3637528.3671793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For natural language understanding and generation, embedding concepts using an order-based representation is an essential task. Unlike traditional point vector based representation, an order-based representation imposes geometric constraints on the representation vectors for explicitly capturing various semantic relationships that may exist between a pair of concepts. In existing literature, several approaches on order-based embedding have been proposed, mostly focusing on capturing hierarchical relationships; examples include vectors in Euclidean space, complex, Hyperbolic, order, and Box Embedding. Box embedding creates region-based rich representation of concepts, but along the process it sacrifices simplicity, requiring a custom-made optimization scheme for learning the representation. Hyperbolic embedding improves embedding quality by exploiting the ever-expanding property of Hyperbolic space, but it also suffers from the same fate as box embedding as gradient descent like optimization is not simple in the Hyperbolic space. In this work, we propose Binder, a novel approach for order-based representation. Binder uses binary vectors for embedding, so the embedding vectors are compact with an order of magnitude smaller footprint than other methods. Binder uses a simple and efficient optimization scheme for learning representation vectors with a linear time complexity. Our comprehensive experimental results show that Binder is very accurate, yielding competitive results on the representation task. But Binder stands out from its competitors on the transitive closure link prediction task as it can learn concept embeddings just from the direct edges, whereas all existing order-based approaches rely on the indirect edges. In particular, Binder achieves a whopping 70\% higher F1-score than the second best method (98.6\% vs 29\%) in our largest dataset, WordNet Nouns (743,241 edges), when using only direct edges during training.},
  archive   = {C_KDD},
  author    = {Gyurek, Croix and Talukder, Niloy and Hasan, Mohammad Al},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671793},
  pages     = {980–991},
  title     = {Binder: Hierarchical concept representation through order embedding of binary vectors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HiFGL: A hierarchical framework for cross-silo cross-device
federated graph learning. <em>KDD</em>, 968–979. (<a
href="https://doi.org/10.1145/3637528.3671660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Federated Graph Learning (FGL) has emerged as a promising way to learn high-quality representations from distributed graph data with privacy preservation. Despite considerable efforts have been made for FGL under either cross-device or cross-silo paradigm, how to effectively capture graph knowledge in a more complicated cross-silo cross-device environment remains an under-explored problem. However, this task is challenging because of the inherent hierarchy and heterogeneity of decentralized clients, diversified privacy constraints in different clients, and the cross-client graph integrity requirement. To this end, in this paper, we propose a Hierarchical Federated Graph Learning (HiFGL) framework for cross-silo cross-device FGL. Specifically, we devise a unified hierarchical architecture to safeguard federated GNN training on heterogeneous clients while ensuring graph integrity. Moreover, we propose a Secret Message Passing (SecMP) scheme to shield unauthorized access to subgraph-level and node-level sensitive information simultaneously. Theoretical analysis proves that HiFGL achieves multi-level privacy preservation with complexity guarantees. Extensive experiments on real-world datasets validate the superiority of the proposed framework against several baselines. Furthermore, HiFGL&#39;s versatile nature allows for its application in either solely cross-silo or cross-device settings, further broadening its utility in real-world FGL applications.},
  archive   = {C_KDD},
  author    = {Guo, Zhuoning and Yao, Duanyi and Yang, Qiang and Liu, Hao},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671660},
  pages     = {968–979},
  title     = {HiFGL: A hierarchical framework for cross-silo cross-device federated graph learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ranking with slot constraints. <em>KDD</em>, 956–967. (<a
href="https://doi.org/10.1145/3637528.3672000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Rankings are increasingly used as part of human decision-making processes to most effectively allocate reviewing resources. Many of these processes have complex constraints, and we identify slot constraints as a model for a wide range of application problems -- from college admission with limited slots for different majors, to composing a stratified cohort of eligible participants in a medical trial. In this paper, we formalize the slot-constrained ranking problem as producing a ranking that maximizes the number of filled slots if candidates are evaluated by a human decision maker for slot eligibility in the order of the ranking. We show that naive adaptations of the Probability Ranking Principle (PRP) can be highly sub-optimal for slot-constrained ranking problems, and we devise a new ranking algorithm, called MatchRank. MatchRank generalizes the PRP, and it subsumes the PRP as a special case when there are no slot constraints. Our theoretical analysis shows that MatchRank has a strong approximation guarantee without any independence assumptions between slots or candidates. Furthermore, we show how MatchRank can be implemented efficiently. Beyond the theoretical guarantees, empirical evaluations show that MatchRank can provide substantial improvements over a range of synthetic and real-world tasks.},
  archive   = {C_KDD},
  author    = {Guo, Wentao and Wang, Andrew and Thymes, Bradon and Joachims, Thorsten},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672000},
  pages     = {956–967},
  title     = {Ranking with slot constraints},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Consistency and discrepancy-based contrastive tripartite
graph learning for recommendations. <em>KDD</em>, 944–955. (<a
href="https://doi.org/10.1145/3637528.3672056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tripartite graph-based recommender systems markedly diverge from traditional models by recommending unique combinations such as user groups and item bundles. Despite their effectiveness, these systems exacerbate the long-standing cold-start problem in traditional recommender systems, because any number of user groups or item bundles can be formed among users or items. To address this issue, we introduce a &amp;lt;u&amp;gt;C&amp;lt;/u&amp;gt;onsistency and &amp;lt;u&amp;gt;D&amp;lt;/u&amp;gt;iscrepancy-based graph contrastive learning method for tripartite graph-based &amp;lt;u&amp;gt;R&amp;lt;/u&amp;gt;ecommendation (CDR). This approach leverages two novel meta-path-based metrics-consistency and discrepancy-to capture nuanced, implicit associations between the recommended objects and the recommendees. These metrics, indicative of high-order similarities, can be efficiently calculated with infinite graph convolutional networks (GCN) layers under a multi-objective optimization framework, using the limit theory of GCN. Additionally, we introduce a novel Contrastive Divergence (CD) loss, which can seamlessly integrate the consistency and discrepancy metrics into the contrastive objective as the positive and contrastive supervision signals to learn node representations, enhancing the pairwise ranking of recommended objects and proving particularly valuable in severe cold-start scenarios. Extensive experiments demonstrate the effectiveness of the proposed CDR. The code is released at https://github.com/foodfaust/CDR.},
  archive   = {C_KDD},
  author    = {Guo, Linxin and Zhu, Yaochen and Gao, Min and Tao, Yinghui and Yu, Junliang and Chen, Chen},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672056},
  pages     = {944–955},
  title     = {Consistency and discrepancy-based contrastive tripartite graph learning for recommendations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Investigating out-of-distribution generalization of GNNs: An
architecture perspective. <em>KDD</em>, 932–943. (<a
href="https://doi.org/10.1145/3637528.3671792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph neural networks (GNNs) have exhibited remarkable performance under the assumption that test data comes from the same distribution of training data. However, in real-world scenarios, this assumption may not always be valid. Consequently, there is a growing focus on exploring the Out-of-Distribution (OOD) problem in the context of graphs. Most existing efforts have primarily concentrated on improving graph OOD generalization from two model-agnostic perspectives: data-driven methods and strategy-based learning. However, there has been limited attention dedicated to investigating the impact of well-known GNN model architectures on graph OOD generalization, which is orthogonal to existing research. In this work, we provide the first comprehensive investigation of OOD generalization on graphs from an architecture perspective, by examining the common building blocks of modern GNNs. Through extensive experiments, we reveal that both the graph self-attention mechanism and the decoupled architecture contribute positively to graph OOD generalization. In contrast, we observe that the linear classification layer tends to compromise graph OOD generalization capability. Furthermore, we provide in-depth theoretical insights and discussions to underpin these discoveries. These insights have empowered us to develop a novel GNN backbone model, DGat, designed to harness the robust properties of both graph self-attention mechanism and the decoupled architecture. Extensive experimental results demonstrate the effectiveness of our model under graph OOD, exhibiting substantial and consistent enhancements across various training strategies. Our codes are available at https://github.com/KaiGuo20/DGAT **REMOVE 2nd URL**://github.com/KaiGuo20/DGAT.},
  archive   = {C_KDD},
  author    = {Guo, Kai and Wen, Hongzhi and Jin, Wei and Guo, Yaming and Tang, Jiliang and Chang, Yi},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671792},
  pages     = {932–943},
  title     = {Investigating out-of-distribution generalization of GNNs: An architecture perspective},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Topology-driven multi-view clustering via tensorial refined
sigmoid rank minimization. <em>KDD</em>, 920–931. (<a
href="https://doi.org/10.1145/3637528.3672070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Benefiting from the effective exploitation of the high-order correlations across multiple views, tensor-based multi-view clustering (TMVC) has garnered considerable attention in recent years. Nevertheless, prior TMVC techniques commonly involve assembling multiple view-specific spatial similarity graphs into a three-dimensional tensor, overlooking the intrinsic topological structure essential for precise clustering of data within a manifold. Additionally, mainstream techniques are constrained by equally shrinking all singular values to recover a low-rank tensor, limiting their capacity to distinguish significant variations among different singular values. In this investigation, we present an innovative TMVC framework termed toPology-driven multi-view clustering viA refined teNsorial sigmoiD rAnk minimization (PANDA ). Specifically, PANDA extracts view-specific topological structures from Euclidean graphs and intricately integrates them into a low-rank three-dimensional tensor, facilitating the concurrent utilization of intra-view topological connectivity and inter-view high-order correlations. Moreover, we develop a refined sigmoid function as the tighter surrogate to tensor rank, enabling the exploration of significant information of heterogeneous singular values. Meanwhile, the topological structures are merged into a unified structure with varying weights, associated with a connectivity constraint, empowering the significant divergence among views and the explicit cluster structure of the target graph are simultaneously leveraged. Extensive experiments demonstrate the superiority of PANDA, outperforming SOTA methods.},
  archive   = {C_KDD},
  author    = {Gu, Zhibin and Li, Zhendong and Feng, Songhe},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672070},
  pages     = {920–931},
  title     = {Topology-driven multi-view clustering via tensorial refined sigmoid rank minimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An energy-centric framework for category-free
out-of-distribution node detection in graphs. <em>KDD</em>, 908–919. (<a
href="https://doi.org/10.1145/3637528.3671939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph neural networks have garnered notable attention for effectively processing graph-structured data. Prevalent models prioritize improving in-distribution (IND) data performance, frequently overlooking the risks from potential out-of-distribution (OOD) nodes during training and inference. In real-world graphs, the automated network construction can introduce noisy nodes from unknown distributions. Previous research into OOD node detection, typically referred to as entropy-based methods, calculates OOD measurements from the prediction entropy alongside category classification training. However, the nodes in the graph might not be pre-labeled with specific categories, rendering entropy-based OOD detectors inapplicable in such category-free situations. To tackle this issue, we propose an energy-centric density estimation framework for OOD node detection, referred to as EnergyDef. Within this framework, we introduce an energy-based GNN to compute node energies that act as indicators of node density and reveal the OOD uncertainty of nodes. Importantly, EnergyDef can efficiently identify OOD nodes with low-resource OOD node annotations, achieved by sampling hallucinated nodes via Langevin Dynamics and structure estimation, along with training through Contrastive Divergence. Our comprehensive experiments on real-world datasets substantiate that our framework markedly surpasses state-of-the-art methods in terms of detection quality, even under conditions of scarce or entirely absent OOD node annotations.},
  archive   = {C_KDD},
  author    = {Gong, Zheng and Sun, Ying},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671939},
  pages     = {908–919},
  title     = {An energy-centric framework for category-free out-of-distribution node detection in graphs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A population-to-individual tuning framework for adapting
pretrained LM to on-device user intent prediction. <em>KDD</em>,
896–907. (<a href="https://doi.org/10.1145/3637528.3671984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile devices, especially smartphones, can support rich functions and have developed into indispensable tools in daily life. With the rise of generative AI services, smartphones can potentially transform into personalized assistants, anticipating user needs and scheduling services accordingly. Predicting user intents on smartphones, and reflecting anticipated activities based on past interactions and context, remains a pivotal step towards this vision. Existing research predominantly focuses on specific domains, neglecting the challenge of modeling diverse event sequences across dynamic contexts. Leveraging pre-trained language models (PLMs) offers a promising avenue, yet adapting PLMs to on-device user intent prediction presents significant challenges. To address these challenges, we propose PITuning, a Population-to-Individual Tuning framework. PITuning enhances common pattern extraction through dynamic event-to-intent transition modeling and addresses long-tailed preferences via adaptive unlearning strategies. Experimental results on real-world datasets demonstrate PITuning&#39;s superior intent prediction performance, highlighting its ability to capture long-tailed preferences and its practicality for on-device prediction scenarios.},
  archive   = {C_KDD},
  author    = {Gong, Jiahui and Ding, Jingtao and Meng, Fanjin and Chen, Guilong and Chen, Hong and Zhao, Shen and Lu, Haisheng and Li, Yong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671984},
  pages     = {896–907},
  title     = {A population-to-individual tuning framework for adapting pretrained LM to on-device user intent prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical neural constructive solver for real-world TSP
scenarios. <em>KDD</em>, 884–895. (<a
href="https://doi.org/10.1145/3637528.3672053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing neural constructive solvers for routing problems have predominantly employed transformer architectures, conceptualizing the route construction as a set-to-sequence learning task. However, their efficacy has primarily been demonstrated on entirely random problem instances that inadequately capture real-world scenarios. In this paper, we introduce realistic Traveling Salesman Problem (TSP) scenarios relevant to industrial settings and derive the following insights: (1) The optimal next node (or city) to visit often lies within proximity to the current node, suggesting the potential benefits of biasing choices based on current locations. (2) Effectively solving the TSP requires robust tracking of unvisited nodes and warrants succinct grouping strategies. Building upon these insights, we propose integrating a learnable choice layer inspired by Hypernetworks to prioritize choices based on the current location, and a learnable approximate clustering algorithm inspired by the Expectation-Maximization algorithm to facilitate grouping the unvisited cities. Together, these two contributions form a hierarchical approach towards solving the realistic TSP by considering both immediate local neighbourhoods and learning an intermediate set of node representations. Our hierarchical approach yields superior performance compared to both classical and recent transformer models, showcasing the efficacy of the key designs.},
  archive   = {C_KDD},
  author    = {Goh, Yong Liang and Cao, Zhiguang and Ma, Yining and Dong, Yanfei and Dupty, Mohammed Haroon and Lee, Wee Sun},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672053},
  pages     = {884–895},
  title     = {Hierarchical neural constructive solver for real-world TSP scenarios},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PATE: Proximity-aware time series anomaly evaluation.
<em>KDD</em>, 872–883. (<a
href="https://doi.org/10.1145/3637528.3671971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Evaluating anomaly detection algorithms in time series data is critical as inaccuracies can lead to flawed decision-making in various domains where real-time analytics and data-driven strategies are essential. Traditional performance metrics assume iid data and fail to capture the complex temporal dynamics and specific characteristics of time series anomalies, such as early and delayed detections. We introduce Proximity-Aware Time series anomaly Evaluation (PATE), a novel evaluation metric that incorporates the temporal relationship between prediction and anomaly intervals. PATE uses proximity-based weighting considering buffer zones around anomaly intervals, enabling a more detailed and informed assessment of a detection. Using these weights, PATE computes a weighted version of the area under the Precision and Recall curve. Our experiments with synthetic and real-world datasets show the superiority of PATE in providing more sensible and accurate evaluations than other evaluation metrics. We also tested several state-of-the-art anomaly detectors across various benchmark datasets using the PATE evaluation scheme. The results show that a common metric like Point-Adjusted F1 Score fails to characterize the detection performances well, and that PATE is able to provide a more fair model comparison. By introducing PATE, we redefine the understanding of model efficacy that steers future studies toward developing more effective and accurate detection models.},
  archive   = {C_KDD},
  author    = {Ghorbani, Ramin and Reinders, Marcel J.T. and Tax, David M.J.},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671971},
  pages     = {872–883},
  title     = {PATE: Proximity-aware time series anomaly evaluation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online preference weight estimation algorithm with vanishing
regret for car-hailing in road network. <em>KDD</em>, 863–871. (<a
href="https://doi.org/10.1145/3637528.3671664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Car-hailing services play an important role in the modern transportation system, and the utilities of the service providers highly depend on the efficiency of route planning algorithms. A widely adopted route planning framework is to assign weights to roads and compute the routes with the shortest path algorithms. Existing techniques of weight-assigning often focus on the traveling time and length of the roads, but cannot incorporate with the preferences of the passengers (users). In this paper, a set of preference weight estimation models is employed to capture the users&#39; preferences over paths in car-hailing with their historical choices. Since the user preferences may vary dynamically over time, it is a challenging task to make real-time decisions over the models. The main technical contribution of this paper is to propose an online learning-based preference weight chasing (PWC) algorithm to solve this problem. The worst-case performance of PWC is analyzed with the metric regret, and it is proved that PWC has a vanishing regret, which means that the time-averaged loss concerning the fixed in-hindsight best model tends to zero. Experiments based on real-world datasets are conducted to verify the effectiveness and efficiency of our algorithm. The code is available at https://github.com/GaoYucen/PWC.},
  archive   = {C_KDD},
  author    = {Gao, Yucen and Zhu, Zhehao and Ma, Mingqian and Gao, Fei and Gao, Hui and Shi, Yangguang and Gao, Xiaofeng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671664},
  pages     = {863–871},
  title     = {Online preference weight estimation algorithm with vanishing regret for car-hailing in road network},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph condensation for open-world graph learning.
<em>KDD</em>, 851–862. (<a
href="https://doi.org/10.1145/3637528.3671917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The burgeoning volume of graph data presents significant computational challenges in training graph neural networks (GNNs), critically impeding their efficiency in various applications. To tackle this challenge, graph condensation (GC) has emerged as a promising acceleration solution, focusing on the synthesis of a compact yet representative graph for efficiently training GNNs while retaining performance. Despite the potential to promote scalable use of GNNs, existing GC methods are limited to aligning the condensed graph with merely the observed static graph distribution. This limitation significantly restricts the generalization capacity of condensed graphs, particularly in adapting to dynamic distribution changes. In real-world scenarios, however, graphs are dynamic and constantly evolving, with new nodes and edges being continually integrated. Consequently, due to the limited generalization capacity of condensed graphs, applications that employ GC for efficient GNN training end up with sub-optimal GNNs when confronted with evolving graph structures and distributions in dynamic real-world situations. To overcome this issue, we propose open-world graph condensation (OpenGC), a robust GC framework that integrates structure-aware distribution shift to simulate evolving graph patterns and exploit the temporal environments for invariance condensation. This approach is designed to extract temporal invariant patterns from the original graph, thereby enhancing the generalization capabilities of the condensed graph and, subsequently, the GNNs trained on it. Furthermore, to support the periodic re-condensation and expedite condensed graph updating in life-long graph learning, OpenGC reconstructs the sophisticated optimization scheme with kernel ridge regression and non-parametric graph convolution, significantly accelerating the condensation process while ensuring the exact solutions. Extensive experiments on both real-world and synthetic evolving graphs demonstrate that OpenGC outperforms state-of-the-art (SOTA) GC methods in adapting to dynamic changes in open-world graph environments.},
  archive   = {C_KDD},
  author    = {Gao, Xinyi and Chen, Tong and Zhang, Wentao and Li, Yayong and Sun, Xiangguo and Yin, Hongzhi},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671917},
  pages     = {851–862},
  title     = {Graph condensation for open-world graph learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Policy-based bayesian active causal discovery with deep
reinforcement learning. <em>KDD</em>, 839–850. (<a
href="https://doi.org/10.1145/3637528.3671705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Causal discovery with observational and interventional data plays an important role in numerous fields. Due to the costly and potentially risky nature of intervention experiments, selecting informative interventions is critical in real-world situations. Several recent works introduce Bayesian active learning to select interventions that maximize the expected information gain about the underlying causal relationship at each optimization step. However, there are still some limitations within these methods: (1) Local optimality. With multiple intervention experiments, selecting optimal intervention myopically at each step may drop into the local optimal point. (2) Expensive time cost. Optimizing the most informative intervention at each step is time-consuming and not suitable for adaptive experiments with strict inference speed requirements. In this study, we propose a novel method called Reinforcement Learning-based Causal Bayesian Experimental Design (RL-CBED) to reduce the risk of local optimality and accelerate intervention selection inference. Specifically, we formulate the active causal discovery problem as a partially observable Markov decision process (POMDP). We design an information gain-based sparse reward function and then improve it to a dense reward function, providing fine-grained feedback to help the RL policy learn more quickly in complex environments. Moreover, we theoretically prove that the Q-function estimator can be learned using only trajectories sampled from the prior, which can significantly reduce the time cost of training process, enabling the real-world application of our method. Extensive experiments on both synthetic and real world-inspired semi-synthetic datasets demonstrate the effectiveness of our proposed method.},
  archive   = {C_KDD},
  author    = {Gao, Heyang and Sun, Zexu and Yang, Hao and Chen, Xu},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671705},
  pages     = {839–850},
  title     = {Policy-based bayesian active causal discovery with deep reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Federated graph learning with structure proxy alignment.
<em>KDD</em>, 827–838. (<a
href="https://doi.org/10.1145/3637528.3671717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Federated Graph Learning (FGL) aims to learn graph learning models over graph data distributed in multiple data owners, which has been applied in various applications such as social recommendation and financial fraud detection. Inherited from generic Federated Learning (FL), FGL similarly has the data heterogeneity issue where the label distribution may vary significantly for distributed graph data across clients. For instance, a client can have the majority of nodes from a class, while another client may have only a few nodes from the same class. This issue results in divergent local objectives and impairs FGL convergence for node-level tasks, especially for node classification. Moreover, FGL also encounters a unique challenge for the node classification task: the nodes from a minority class in a client are more likely to have biased neighboring information, which prevents FGL from learning expressive node embeddings with Graph Neural Networks (GNNs). To grapple with the challenge, we propose FedSpray, a novel FGL framework that learns local class-wise structure proxies in the latent space and aligns them to obtain global structure proxies in the server. Our goal is to obtain the aligned structure proxies that can serve as reliable, unbiased neighboring information for node classification. To achieve this, FedSpray trains a global feature-structure encoder and generates unbiased soft targets with structure proxies to regularize local training of GNN models in a personalized way. We conduct extensive experiments over four datasets, and experiment results validate the superiority of FedSpray compared with other baselines. Our code is available at https://github.com/xbfu/FedSpray.},
  archive   = {C_KDD},
  author    = {Fu, Xingbo and Chen, Zihan and Zhang, Binchi and Chen, Chen and Li, Jundong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671717},
  pages     = {827–838},
  title     = {Federated graph learning with structure proxy alignment},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DIET: Customized slimming for incompatible networks in
sequential recommendation. <em>KDD</em>, 816–826. (<a
href="https://doi.org/10.1145/3637528.3671669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Due to the continuously improving capabilities of mobile edges, recommender systems start to deploy models on edges to alleviate network congestion caused by frequent mobile requests. Several studies have leveraged the proximity of edge-side to real-time data, fine-tuning them to create edge-specific models. Despite their significant progress, these methods require substantial on-edge computational resources and frequent network transfers to keep the model up to date. The former may disrupt other processes on the edge to acquire computational resources, while the latter consumes network bandwidth, leading to a decrease in user satisfaction. In response to these challenges, we propose a customizeD slImming framework for incompatiblE neTworks(DIET). DIET deploys the same generic backbone (potentially incompatible for a specific edge) to all devices. To minimize frequent bandwidth usage and storage consumption in personalization, DIET tailors specific subnets for each edge based on its past interactions, learning to generate slimming subnets(diets) within incompatible networks for efficient transfer. It also takes the inter-layer relationships into account, empirically reducing inference time while obtaining more suitable diets. We further explore the repeated modules within networks and propose a more storage-efficient framework, DIETING, which utilizes a single layer of parameters to represent the entire network, achieving comparably excellent performance. The experiments across four state-of-the-art datasets and two widely used models demonstrate the superior accuracy in recommendation and efficiency in transmission and storage of our framework.},
  archive   = {C_KDD},
  author    = {Fu, Kairui and Zhang, Shengyu and Lv, Zheqi and Chen, Jingyuan and Li, Jiwei},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671669},
  pages     = {816–826},
  title     = {DIET: Customized slimming for incompatible networks in sequential recommendation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AutoXPCR: Automated multi-objective model selection for time
series forecasting. <em>KDD</em>, 806–815. (<a
href="https://doi.org/10.1145/3637528.3672057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automated machine learning (AutoML) streamlines the creation of ML models, but few specialized methods have approached the challenging domain of time series forecasting. Deep neural networks (DNNs) often deliver state-of-the-art predictive performance for forecasting data, however these models are also criticized for being computationally intensive black boxes. As a result, when searching for the &quot;best&quot; model, it is crucial to also acknowledge other aspects, such as interpretability and resource consumption. In this paper, we propose AutoXPCR - a novel method that produces DNNs for forecasting under consideration of multiple objectives in an automated and explainable fashion. Our approach leverages meta-learning to estimate any model&#39;s performance along PCR criteria, which encompass (P)redictive error, (C)omplexity, and (R)esource demand. Explainability is addressed on multiple levels, as AutoXPCR pro-vides by-product explanations of recommendations and allows to interactively control the desired PCR criteria importance and trade-offs. We demonstrate the practical feasibility AutoXPCR across 108 forecasting data sets from various domains. Notably, our method outperforms competing AutoML approaches - on average, it only requires 20\% of computation costs for recommending highly efficient models with 85\% of the empirical best quality.},
  archive   = {C_KDD},
  author    = {Fischer, Raphael and Saadallah, Amal},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672057},
  pages     = {806–815},
  title     = {AutoXPCR: Automated multi-objective model selection for time series forecasting},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Communication-efficient multi-service mobile traffic
prediction by leveraging cross-service correlations. <em>KDD</em>,
794–805. (<a href="https://doi.org/10.1145/3637528.3671730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile traffic prediction plays a crucial role in enabling efficient network management and service provisioning. Traditional prediction approaches treat different mobile application services (such as Uber, Facebook, Twitter, etc) as isolated entities, neglecting potential correlation among them. Moreover, such isolated prediction methods necessitate the uploading of historical traffic data from all regions to forecast city-wide traffic, resulting in consuming substantial bandwidth resources and risking prediction failure in the event of data loss in specific regions. To address these challenges, we propose a novel Cross-service Attention-based Spatial-Temporal Graph Convolutional Network (CsASTGCN) for precise and communication-efficient multi-service mobile traffic prediction. Our methodology allows each mobile service to transmit the traffic data of only a fraction of regions for city-wide traffic prediction of all mobile services, which reduces the resource consumption caused by data transmission. Specifically, the sparse traffic data are initially transmitted to the cloud server and the masked graph autoencoder is utilized to roughly reconstruct the traffic volume for regions with missing data. Subsequently, a cross-service attention-based predictor is designed to calculate the data correlation among different mobile services within the same region. Considering the constantly emerging mobile services, we incorporate a novel model-based adaptive transfer learning scheme to extract valuable knowledge from the existing models and expedite the training of a new model for a new service without training from scratch, thereby enhancing the scalability of our framework. Extensive experiments conducted on a large-scale real-world mobile traffic dataset demonstrate that our model greatly outperforms the existing schemes, enhancing both the communication-efficiency and robustness of large-scale multi-service traffic prediction.},
  archive   = {C_KDD},
  author    = {Feng, Zhiying and Wu, Qiong and Chen, Xu},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671730},
  pages     = {794–805},
  title     = {Communication-efficient multi-service mobile traffic prediction by leveraging cross-service correlations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SensitiveHUE: Multivariate time series anomaly detection by
enhancing the sensitivity to normal patterns. <em>KDD</em>, 782–793. (<a
href="https://doi.org/10.1145/3637528.3671919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unsupervised anomaly detection in multivariate time series (MTS) has always been a challenging problem, and the modeling based on reconstruction has garnered significant attention. The insensitivity of these methods towards normal patterns poses challenges in distinguishing between normal and abnormal points. Firstly, the general reconstruction strategies may exhibit limited sensitivity to spatio-temporal dependencies, and their performance remains largely unaffected by such dependencies. Secondly, most methods fail to model the heteroscedastic uncertainty in MTS, hindering their abilities to derive a distinguishable criterion. For instance, normal data with high noise levels may lead to detection failure due to excessively high reconstruction errors. In this work, we emphasize the necessity of sensitivity to normal patterns, which could improve the discrimination between normal and abnormal points remarkably. To this end, we propose SensitiveHUE, a probabilistic network by implementing both reconstruction and heteroscedastic uncertainty estimation. Its core includes a statistical feature removal strategy to ensure the dependency sensitive property, and a novel MTS-NLL loss for modeling the normal patterns in important regions. Experimental results demonstrate that SensitiveHUE exhibits nontrivial sensitivity to normal patterns and outperforms the existing state-of-the-art alternatives by a large margin. Code is publicly available at this URLfootnotehttp://github.com/yuesuoqingqiu/SensitiveHUE.},
  archive   = {C_KDD},
  author    = {Feng, Yuye and Zhang, Wei and Fu, Yao and Jiang, Weihao and Zhu, Jiang and Ren, Wenqi},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671919},
  pages     = {782–793},
  title     = {SensitiveHUE: Multivariate time series anomaly detection by enhancing the sensitivity to normal patterns},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Influence maximization via graph neural bandits.
<em>KDD</em>, 771–781. (<a
href="https://doi.org/10.1145/3637528.3671983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider a ubiquitous scenario in the study of Influence Maximization (IM), in which there is limited knowledge about the topology of the diffusion network. We set the IM problem in a multi-round diffusion campaign, aiming to maximize the number of distinct users that are influenced. Leveraging the capability of bandit algorithms to effectively balance the objectives of exploration and exploitation, as well as the expressivity of neural networks, our study explores the application of neural bandit algorithms to the IM problem. We propose the framework IM-GNB (Influence Maximization with Graph Neural Bandits), where we provide an estimate of the users&#39; probabilities of being influenced by influencers (also known as diffusion seeds). This initial estimate forms the basis for constructing both an exploitation graph and an exploration one. Subsequently, IM-GNB handles the exploration-exploitation tradeoff, by selecting seed nodes in real-time using Graph Convolutional Networks (GCN), in which the pre-estimated graphs are employed to refine the influencers&#39; estimated rewards in each contextual setting. Through extensive experiments on two large real-world datasets, we demonstrate the effectiveness of IM-GNB compared with other baseline methods, significantly improving the spread outcome of such diffusion campaigns, when the underlying network is unknown.},
  archive   = {C_KDD},
  author    = {Feng, Yuting and Tan, Vincent Y.F. and Cautis, Bogdan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671983},
  pages     = {771–781},
  title     = {Influence maximization via graph neural bandits},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ROTAN: A rotation-based temporal attention network for
time-specific next POI recommendation. <em>KDD</em>, 759–770. (<a
href="https://doi.org/10.1145/3637528.3671809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The next Point-of-interest recommendation has attracted extensive research interest recently, which predicts users&#39; subsequent movements. The main challenge is how to effectively capture users&#39; personalized sequential transitions in check-in trajectory, and various methods have been developed. However, most existing studies ignore the temporal information when conducting the next POI recommendation. To fill this gap, we investigate a time-specific next POI recommendation task, which additionally incorporates the target time information. We propose a brand new Time2Rotation technique to capture the temporal information. Different from conventional methods, we represent timeslots as rotation vectors and then perform the rotation operations. Based on the Time2Rotation technique, we propose a novel rotation-based temporal attention network, namely ROTAN, for the time-specific next POI recommendation task. The ROTAN begins by building a collaborative POI transition graph, capturing the asymmetric temporal influence in sequential transitions. After that, it incorporates temporal information into the modeling of individual check-in trajectories, extracting separate representations for user preference and POI influence to reflect their distinct temporal patterns. Lastly, the target time is integrated to generate recommendations. Extensive experiments are conducted on three real-world datasets, which demonstrates the advantages of the proposed Time2Rotation technique and ROTAN recommendation model.},
  archive   = {C_KDD},
  author    = {Feng, Shanshan and Meng, Feiyu and Chen, Lisi and Shang, Shuo and Ong, Yew Soon},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671809},
  pages     = {759–770},
  title     = {ROTAN: A rotation-based temporal attention network for time-specific next POI recommendation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GAugLLM: Improving graph contrastive learning for
text-attributed graphs with large language models. <em>KDD</em>,
747–758. (<a href="https://doi.org/10.1145/3637528.3672035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work studies self-supervised graph learning for text-attributed graphs (TAGs) where nodes are represented by textual attributes. Unlike traditional graph contrastive methods that perturb the numerical feature space and alter the graph&#39;s topological structure, we aim to improve view generation through language supervision. This is driven by the prevalence of textual attributes in real applications, which complement graph structures with rich semantic information. However, this presents challenges because of two major reasons. First, text attributes often vary in length and quality, making it difficulty to perturb raw text descriptions without altering their original semantic meanings. Second, although text attributes complement graph structures, they are not inherently well-aligned. To bridge the gap, we introduce GAugLLM, a novel framework for augmenting TAGs. It leverages advanced large language models like Mistral to enhance self-supervised graph learning. Specifically, we introduce a mixture-of-prompt-expert technique to generate augmented node features. This approach adaptively maps multiple prompt experts, each of which modifies raw text attributes using prompt engineering, into numerical feature space. Additionally, we devise a collaborative edge modifier to leverage structural and textual commonalities, enhancing edge augmentation by examining or building connections between nodes. Empirical results across five benchmark datasets spanning various domains underscore our framework&#39;s ability to enhance the performance of leading contrastive methods (e.g., BGRL, GraphCL, and GBT) as a plug-in tool. Notably, we observe that the augmented features and graph structure can also enhance the performance of standard generative methods (e.g., GraphMAE and S2GAE), as well as popular graph neural networks (e.g., GCN and GAT). The open-sourced implementation of our GAugLLM is available at https://github.com/NYUSHCS/GAugLLM.},
  archive   = {C_KDD},
  author    = {Fang, Yi and Fan, Dongzhe and Zha, Daochen and Tan, Qiaoyu},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672035},
  pages     = {747–758},
  title     = {GAugLLM: Improving graph contrastive learning for text-attributed graphs with large language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Label shift correction via bidirectional marginal
distribution matching. <em>KDD</em>, 735–746. (<a
href="https://doi.org/10.1145/3637528.3671867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Due to the timeliness and uncertainty of data acquisition, label shift, which assumes that the source (training) and target (test) label distributions differ, occurs with the changing environment and reduces the generalization ability of traditional models. To correct the label shift, existing methods estimate the true label distribution by prediction of target data from a source classifier, which results in high variance, especially with large label shift. In this paper, we tackle this problem by proposing a novel approach termed as Label Shift Correction via Bidirectional Marginal Distribution Matching (BMDM). Our approach matchs the label and feature marginal distributions simultaneously to ensure the stability of estimated class proportions. We prove theoretically that there is a unique optimal solution, i.e., true target label distribution, for our approach under mild conditions, and an efficient optimization strategy is also proposed. On this basis, in multi-shot scenario where label distribution changes continuously, we extend BMDM by designing a new distribution matching mechanism and constructing a regularization term that constrains the direction of label distribution change. Extensive experimental results validate the effectiveness of our approach over existing state-of-the-arts methods.},
  archive   = {C_KDD},
  author    = {Fan, Ruidong and Ouyang, Xiao and Tao, Hong and Hou, Chenping},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671867},
  pages     = {735–746},
  title     = {Label shift correction via bidirectional marginal distribution matching},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CAT: Interpretable concept-based taylor additive models.
<em>KDD</em>, 723–734. (<a
href="https://doi.org/10.1145/3637528.3672020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As an emerging interpretable technique, Generalized Additive Models (GAMs) adopt neural networks to individually learn non-linear functions for each feature, which are then combined through a linear model for final predictions. Although GAMs can explain deep neural networks (DNNs) at the feature level, they require large numbers of model parameters and are prone to overfitting, making them hard to train and scale. Additionally, in real-world datasets with many features, the interpretability of feature-based explanations diminishes for humans. To tackle these issues, recent research has shifted towards concept-based interpretable methods. These approaches try to integrate concept learning as an intermediate step before making predictions, explaining the predictions in terms of human-understandable concepts. However, these methods require domain experts to extensively label concepts with relevant names and their ground-truth values. In response, we propose CAT, a novel interpretable Concept-bAsed Taylor additive model to simplify this process. CAT does not require domain experts to annotate concepts and their ground-truth values. Instead, it only requires users to simply categorize input features into broad groups, which can be easily accomplished through a quick metadata review. Specifically, CAT first embeds each group of input features into one-dimensional high-level concept representation, and then feeds the concept representations into a new white-box Taylor Neural Network (TaylorNet). The TaylorNet aims to learn the non-linear relationship between the inputs and outputs using polynomials. Evaluation results across multiple benchmarks demonstrate that CAT can outperform or compete with the baselines while reducing the need of extensive model parameters. Importantly, it can effectively explain model predictions through high-level concepts. Source code is available at github.com/vduong143/CAT-KDD-2024.},
  archive   = {C_KDD},
  author    = {Duong, Viet and Wu, Qiong and Zhou, Zhengyi and Zhao, Hongjue and Luo, Chenxiang and Zavesky, Eric and Yao, Huaxiu and Shao, Huajie},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672020},
  pages     = {723–734},
  title     = {CAT: Interpretable concept-based taylor additive models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Auctions with LLM summaries. <em>KDD</em>, 713–722. (<a
href="https://doi.org/10.1145/3637528.3672022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study an auction setting in which bidders bid for placement of their content within a summary generated by a large language model (LLM), e.g., an ad auction in which the display is a summary paragraph of multiple ads. This generalizes the classic ad settings such as position auctions to an LLM generated setting, which allows us to handle general display formats. We propose a novel factorized framework in which an auction module and an LLM module work together via a prediction model to provide welfare maximizing summary outputs in an incentive compatible manner. We provide a theoretical analysis of this framework and synthetic experiments to demonstrate the feasibility and validity of the system together with welfare comparisons.},
  archive   = {C_KDD},
  author    = {Dubey, Avinava and Feng, Zhe and Kidambi, Rahul and Mehta, Aranyak and Wang, Di},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672022},
  pages     = {713–722},
  title     = {Auctions with LLM summaries},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pre-training identification of graph winning tickets in
adaptive spatial-temporal graph neural networks. <em>KDD</em>, 701–712.
(<a href="https://doi.org/10.1145/3637528.3671912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a novel method to significantly enhance the computational efficiency of Adaptive Spatial-Temporal Graph Neural Networks (ASTGNNs) by introducing the concept of the Graph Winning Ticket (GWT), derived from the Lottery Ticket Hypothesis (LTH). By adopting a pre-determined star topology as a GWT prior to training, we balance edge reduction with efficient information propagation, reducing computational demands while maintaining high model performance. Both the time and memory computational complexity of generating adaptive spatial-temporal graphs is significantly reduced from O(N2) to O(N). Our approach streamlines the ASTGNN deployment by eliminating the need for exhaustive training, pruning, and retraining cycles, and demonstrates empirically across various datasets that it is possible to achieve comparable performance to full models with substantially lower computational costs. Specifically, our approach enables training ASTGNNs on the largest scale spatial-temporal dataset using a single A6000 equipped with 48 GB of memory, overcoming the out-of-memory issue encountered during original training and even achieving state-of-the-art performance. Furthermore, we delve into the effectiveness of the GWT from the perspective of spectral graph theory, providing substantial theoretical support. This advancement not only proves the existence of efficient sub-networks within ASTGNNs but also broadens the applicability of the LTH in resource-constrained settings, marking a significant step forward in the field of graph neural networks. Code is available at https://anonymous.4open.science/r/paper-1430.},
  archive   = {C_KDD},
  author    = {Duan, Wenying and Fang, Tianxiang and Rao, Hong and He, Xiaoxi},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671912},
  pages     = {701–712},
  title     = {Pre-training identification of graph winning tickets in adaptive spatial-temporal graph neural networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reserving-masking-reconstruction model for self-supervised
heterogeneous graph representation. <em>KDD</em>, 689–700. (<a
href="https://doi.org/10.1145/3637528.3671719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-supervised Heterogeneous Graph Representation (SSHGRL) learning is widely used in data mining. The latest SSHGRL methods normally use metapaths to describe the heterogeneous information (multiple relations and node types) to learn the heterogeneous graph representation and achieve impressive results. However, establishing metapaths requires lofty computational costs that are too high for the medium and large graphs. To this end, this paper proposes a Reserving-Masking-Reconstruction (RMR) model that can fully consider heterogeneous information without relying on the metapaths. In detail, we propose a reserving method to reserve to-be-masked nodes&#39; (target nodes) information before graph masking. Second, we split the reserved graph into relation subgraphs according to the type of relations that require much less computational overheads than metapath. Then, the target nodes in each relation subgraph are randomly masked with minimal topology information loss. After, a novel reconstruction method is proposed to reconstruct the masked nodes on different relation subgraphs to establish the self-supervised signal. The proposed method requires low computational complexity and can establish a self-supervised signal without deeply changing the graph topology. Experimental results show the proposed method achieves state-of-the-art records on medium and large-scale heterogeneous graphs and competitive records on small-scale heterogeneous graphs. The code is available at https://github.com/DuanhaoranCC/RMR.},
  archive   = {C_KDD},
  author    = {Duan, Haoran and Xie, Cheng and Li, Linyu},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671719},
  pages     = {689–700},
  title     = {Reserving-masking-reconstruction model for self-supervised heterogeneous graph representation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Disentangled multi-interest representation learning for
sequential recommendation. <em>KDD</em>, 677–688. (<a
href="https://doi.org/10.1145/3637528.3671800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, much effort has been devoted to modeling users&#39; multi-interests (aka multi-faceted preferences) based on their behaviors, aiming to accurately capture users&#39; complex preferences. Existing methods attempt to model each interest of users through a distinct representation, but these multi-interest representations easily collapse into similar ones due to a lack of effective guidance. In this paper, we propose a generic multi-interest method for sequential recommendation, achieving disentangled representation learning of diverse interests technically and theoretically. To alleviate the collapse issue of multi-interests, we propose to conduct item partition guided by their likelihood of being co-purchased in a global view. It can encourage items in each group to focus on a discriminated interest, thus achieving effective disentangled learning of multi-interests. Specifically, we first prove the theoretical connection between item partition and spectral clustering, demonstrating its effectiveness in alleviating item-level and facet-level collapse issues that hinder existing disentangled methods. To efficiently optimize this problem, we then propose a Markov Random Field (MRF)-based method that samples small-scale sub-graphs from two separate MRFs, thus it can be approximated with a cross-entropy loss and optimized through contrastive learning. Finally, we perform multi-task learning to seamlessly align item partition learning with multi-interest modeling for more accurate recommendation. Experiments on three real-world datasets show that our method significantly outperforms state-of-the-art methods and can flexibly integrate with existing multi-interest models as a plugin to enhance their performances.},
  archive   = {C_KDD},
  author    = {Du, Yingpeng and Wang, Ziyan and Sun, Zhu and Ma, Yining and Liu, Hongzhi and Zhang, Jie},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671800},
  pages     = {677–688},
  title     = {Disentangled multi-interest representation learning for sequential recommendation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DisCo: Towards harmonious disentanglement and collaboration
between tabular and semantic space for recommendation. <em>KDD</em>,
666–676. (<a href="https://doi.org/10.1145/3637528.3672008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recommender systems play important roles in various applications such as e-commerce, social media, etc. Conventional recommendation methods usually model the collaborative signals within the tabular representation space. Despite the personalization modeling and the efficiency, the latent semantic dependencies are omitted. Methods that introduce semantics into recommendation then emerge, injecting knowledge from the semantic representation space where the general language understanding are compressed. However, existing semantic-enhanced recommendation methods focus on aligning the two spaces, during which the representations of the two spaces tend to get close while the unique patterns are discarded and not well explored. In this paper, we propose DisCo to Disentangle the unique patterns from the two representation spaces and Collaborate the two spaces for recommendation enhancement, where both the specificity and the consistency of the two spaces are captured. Concretely, we propose 1) a dual-side attentive network to capture the intra-domain patterns and the inter-domain patterns, 2) a sufficiency constraint to preserve the task-relevant information of each representation space and filter out the noise, and 3) a disentanglement constraint to avoid the model from discarding the unique information. These modules strike a balance between disentanglement and collaboration of the two representation spaces to produce informative pattern vectors, which could serve as extra features and be appended to arbitrary recommendation backbones for enhancement. Experiment results validate the superiority of our method against different models and the compatibility of DisCo over different backbones. Various ablation studies and efficiency analysis are also conducted to justify each model component.},
  archive   = {C_KDD},
  author    = {Du, Kounianhua and Chen, Jizheng and Lin, Jianghao and Xi, Yunjia and Wang, Hangyu and Dai, Xinyi and Chen, Bo and Tang, Ruiming and Zhang, Weinan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672008},
  pages     = {666–676},
  title     = {DisCo: Towards harmonious disentanglement and collaboration between tabular and semantic space for recommendation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Representation learning of temporal graphs with structural
roles. <em>KDD</em>, 654–665. (<a
href="https://doi.org/10.1145/3637528.3671854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Temporal graph representation learning has drawn considerable attention in recent years. Most existing works mainly focus on modeling local structural dependencies of temporal graphs. However, underestimating the inherent global structural role information in many real-world temporal graphs inevitably leads to sub-optimal graph representations. To overcome this shortcoming, we propose a novel &amp;lt;u&amp;gt;R&amp;lt;/u&amp;gt;ole-based &amp;lt;u&amp;gt;T&amp;lt;/u&amp;gt;emporal &amp;lt;u&amp;gt;G&amp;lt;/u&amp;gt;raph &amp;lt;u&amp;gt;C&amp;lt;/u&amp;gt;onvolution &amp;lt;u&amp;gt;N&amp;lt;/u&amp;gt;etwork (RTGCN) that fully leverages the global structural role information in temporal graphs. Specifically, RTGCN can effectively capture the static global structural roles by using hypergraph convolution neural networks. To capture the evolution of nodes&#39; structural roles, we further design structural role-based gated recurrent units. Finally, we integrate structural role proximity in our objective function to preserve global structural similarity, further promoting temporal graph representation learning. Experimental results on multiple real-world datasets demonstrate that RTGCN consistently outperforms state-of-the-art temporal graph representation learning methods by significant margins in various temporal link prediction and node classification tasks. Specifically, RTGCN achieves AUC improvement of up to 5.1\% for link prediction and F1 improvement of up to 6.2\% for new link prediction. In addition, RTGCN achieves AUC improvement up to 4.6\% for node classification and 2.7\% for structural role classification.},
  archive   = {C_KDD},
  author    = {Du, Huaming and Shi, Long and Chen, Xingyan and Zhao, Yu and Zhang, Hegui and Yang, Carl and Zhuang, Fuzhen and Kou, Gang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671854},
  pages     = {654–665},
  title     = {Representation learning of temporal graphs with structural roles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimated judge reliabilities for weighted
bradley-terry-luce are not reliable. <em>KDD</em>, 642–653. (<a
href="https://doi.org/10.1145/3637528.3671907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There are many applications for which we want to learn a latent scale for subjective properties, such as the excitement of a photo or the legibility of a font; however, obtaining human-labeled data is costly and time-consuming. One oft-used method for acquiring these labels, despite the cost being quadratic in the number of items, is the method of pairwise comparisons since this method minimizes the effect of biases and generally can be used effectively outside of a controlled environment.Crowdsourcing appears to be a panacea since online platforms provide affordable access to numerous people, but these participants, judges, vary in diligence and expertise. Several methods have been proposed to assign weights to judges based on their responses relative to everyone else, the goal being to reduce exposure to poor performers, hopefully upgrading the quality of the data.Our research focuses on two natural extensions to the Bradley-Terry-Luce formulation of scaling that jointly optimize for both scale value and judge weights. While both methods appear to perform at least as well as the unweighted formulation on average with well-behaved judges, we report a previously unknown flaw, revealing that the resultant judge weights should not be interpreted as reliabilities. Consequently, these values should not be leveraged for decisions about the judges, such as for active sampling or to validate the participant pool.},
  archive   = {C_KDD},
  author    = {Dreher, Andrew F. and Vouga, Etienne and Fussell, Donald S.},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671907},
  pages     = {642–653},
  title     = {Estimated judge reliabilities for weighted bradley-terry-luce are not reliable},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heterogeneity-informed meta-parameter learning for
spatiotemporal time series forecasting. <em>KDD</em>, 631–641. (<a
href="https://doi.org/10.1145/3637528.3671961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Spatiotemporal time series forecasting plays a key role in a wide range of real-world applications. While significant progress has been made in this area, fully capturing and leveraging spatiotemporal heterogeneity remains a fundamental challenge. Therefore, we propose a novel Heterogeneity-Informed Meta-Parameter Learning scheme. Specifically, our approach implicitly captures spatiotemporal heterogeneity through learning spatial and temporal embeddings, which can be viewed as a clustering process. Then, a novel spatiotemporal meta-parameter learning paradigm is proposed to learn spatiotemporal-specific parameters from meta-parameter pools, which is informed by the captured heterogeneity. Based on these ideas, we develop a &amp;lt;u&amp;gt;H&amp;lt;/u&amp;gt;eterogeneity-&amp;lt;u&amp;gt;I&amp;lt;/u&amp;gt;nformed Spatiotemporal &amp;lt;u&amp;gt;M&amp;lt;/u&amp;gt;eta-&amp;lt;u&amp;gt;Net&amp;lt;/u&amp;gt;work (HimNet) for spatiotemporal time series forecasting. Extensive experiments on five widely-used benchmarks demonstrate our method achieves state-of-the-art performance while exhibiting superior interpretability. Our code is available at &amp;lt;u&amp;gt;https://github.com/XDZhelheim/HimNet&amp;lt;/u&amp;gt;.},
  archive   = {C_KDD},
  author    = {Dong, Zheng and Jiang, Renhe and Gao, Haotian and Liu, Hangchen and Deng, Jinliang and Wen, Qingsong and Song, Xuan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671961},
  pages     = {631–641},
  title     = {Heterogeneity-informed meta-parameter learning for spatiotemporal time series forecasting},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IDEA: A flexible framework of certified unlearning for graph
neural networks. <em>KDD</em>, 621–630. (<a
href="https://doi.org/10.1145/3637528.3671744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph Neural Networks (GNNs) have been increasingly deployed in a plethora of applications. However, the graph data used for training may contain sensitive personal information of the involved individuals. Once trained, GNNs typically encode such information in their learnable parameters. As a consequence, privacy leakage may happen when the trained GNNs are deployed and exposed to potential attackers. Facing such a threat, machine unlearning for GNNs has become an emerging technique that aims to remove certain personal information from a trained GNN. Among these techniques, certified unlearning stands out, as it provides a solid theoretical guarantee of the information removal effectiveness. Nevertheless, most of the existing certified unlearning methods for GNNs are only designed to handle node and edge unlearning requests. Meanwhile, these approaches are usually tailored for either a specific design of GNN or a specially designed training objective. These disadvantages significantly jeopardize their flexibility. In this paper, we propose a principled framework named IDEA to achieve flexible and certified unlearning for GNNs. Specifically, we first instantiate four types of unlearning requests on graphs, and then we propose an approximation approach to flexibly handle these unlearning requests over diverse GNNs. We further provide theoretical guarantee of the effectiveness for the proposed approach as a certification. Different from existing alternatives, IDEA is not designed for any specific GNNs or optimization objectives to perform certified unlearning, and thus can be easily generalized. Extensive experiments on real-world datasets demonstrate the superiority of IDEA in multiple key perspectives.},
  archive   = {C_KDD},
  author    = {Dong, Yushun and Zhang, Binchi and Lei, Zhenyu and Zou, Na and Li, Jundong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671744},
  pages     = {621–630},
  title     = {IDEA: A flexible framework of certified unlearning for graph neural networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised alignment of hypergraphs with different scales.
<em>KDD</em>, 609–620. (<a
href="https://doi.org/10.1145/3637528.3671955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {People usually interact in groups, and such groups may appear on different platforms. For instance, people often create various group chats on messaging apps (e.g., Facebook Messenger and WhatsApp) to communicate with families, friends, or colleagues. How do we identify the same people across the two platforms based on the information about the groups? This gives rise to the hypergraph alignment problem, whose objective is to find the correspondences between the sets of nodes of two hypergraphs. In a hypergraph, a node represents a person, and each hyperedge represents a group of several people. In addition, the two sets of hyperedges in the two hypergraphs can vary significantly in scales as people may use different apps at different time periods.In this work, we propose and tackle the problem of unsupervised hypergraph alignment. Given two hypergraphs with potentially different scales and without any side information or prior ground-truth correspondences, we develop \O{}urMethod, a learning framework, to find node correspondences across the two hypergraphs. \O{}urMethod directly addresses each challenge of the problem. In particular, it (a) extracts node features from the hypergraph topology, (b) employs contrastive learning, as a &quot;supervised pseudo-alignment&#39;&#39; task to pre-train the learning model (c) applies topological augmentation to help a generative adversarial network to align the two embedding spaces from the two hypergraphs. The purpose of augmentation is to add virtual hyperedges from one hypergraph in order to the other to resolve the scale difference and share information across the two hypergraphs. Our extensive experiments on 12 real-world datasets demonstrate the significant and consistent superiority of \O{}urMethod over the baseline approaches.},
  archive   = {C_KDD},
  author    = {Do, Manh Tuan and Shin, Kijung},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671955},
  pages     = {609–620},
  title     = {Unsupervised alignment of hypergraphs with different scales},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing on-device LLM inference with historical
cloud-based LLM interactions. <em>KDD</em>, 597–608. (<a
href="https://doi.org/10.1145/3637528.3671679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many billion-scale large language models (LLMs) have been released for resource-constraint mobile devices to provide local LLM inference service when cloud-based powerful LLMs are not available. However, the capabilities of current on-device LLMs still lag behind those of cloud-based LLMs, and how to effectively and efficiently enhance on-device LLM inference becomes a practical requirement. We thus propose to collect the user&#39;s historical interactions with the cloud-based LLM and build an external datastore on the mobile device for enhancement using nearest neighbors search. Nevertheless, the full datastore improves the quality of token generation at the unacceptable expense of much slower generation speed. To balance performance and efficiency, we propose to select an optimal subset of the full datastore within the given size limit, the optimization objective of which is proven to be submodular. We further design an offline algorithm, which selects the subset after the construction of the full datastore, as well as an online algorithm, which performs selection over the stream and can be flexibly scheduled. We theoretically analyze the performance guarantee and the time complexity of the offline and the online designs to demonstrate effectiveness and scalability. We finally take three ChatGPT related dialogue datasets and four different on-device LLMs for evaluation. Evaluation results show that the proposed designs significantly enhance LLM performance in terms of perplexity while maintaining fast token generation speed. Practical overhead testing on the smartphone reveal the efficiency of on-device datastore subset selection from memory usage and computation overhead.},
  archive   = {C_KDD},
  author    = {Ding, Yucheng and Niu, Chaoyue and Wu, Fan and Tang, Shaojie and Lyu, Chengfei and Chen, Guihai},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671679},
  pages     = {597–608},
  title     = {Enhancing on-device LLM inference with historical cloud-based LLM interactions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast unsupervised deep outlier model selection with
hypernetworks. <em>KDD</em>, 585–596. (<a
href="https://doi.org/10.1145/3637528.3672003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep neural network based Outlier Detection (DOD) has seen a recent surge of attention thanks to the many advances in deep learning. In this paper, we consider a critical-yet-understudied challenge with unsupervised DOD, that is, effective hyperparameter (HP) tuning/model selection. While several prior work report the sensitivity of OD models to HP settings, the issue is ever so critical for the modern DOD models that exhibit a long list of HPs. We introduce HYPER for tuning DOD models, tackling two fundamental challenges: (1) validation without supervision (due to lack of labeled outliers), and (2) efficient search of the HP/model space (due to exponential growth in the number of HPs). A key idea is to design and train a novel hypernetwork (HN) that maps HPs onto optimal weights of the main DOD model. In turn, HYPER capitalizes on a single HN that can dynamically generate weights for many DOD models (corresponding to varying HPs), which offers significant speed-up. In addition, it employs meta-learning on historical OD tasks with labels to train a proxy validation function, likewise trained with our proposed HN efficiently. Extensive experiments on different OD tasks show that HYPER achieves competitive performance against 8 baselines with significant efficiency gains.},
  archive   = {C_KDD},
  author    = {Ding, Xueying and Zhao, Yue and Akoglu, Leman},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672003},
  pages     = {585–596},
  title     = {Fast unsupervised deep outlier model selection with hypernetworks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Divide and denoise: Empowering simple models for robust
semi-supervised node classification against label noise. <em>KDD</em>,
574–584. (<a href="https://doi.org/10.1145/3637528.3671798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph neural networks (GNNs) based on message passing have achieved remarkable performance in graph machine learning. By combining it with the power of pseudo labeling, one can further push forward the performance on the task of semi-supervised node classification. However, most existing works assume that the training node labels are purely noise-free, while this strong assumption usually does not hold in practice. GNNs will overfit the noisy training labels and the adverse effects of mislabeled nodes can be exaggerated by being propagated to the remaining nodes through the graph structure, exacerbating the model failure. Worse still, the noisy pseudo labels could also largely undermine the model&#39;s reliability without special treatment. In this paper, we revisit the role of (1) message passing and (2) pseudo labels in the studied problem and try to address two denoising subproblems from the model architecture and algorithm perspective, respectively. Specifically, we first develop a label-noise robust GNN that discards the coupled message-passing scheme. Despite its simple architecture, this learning backbone prevents overfitting to noisy labels and also inherently avoids the noise propagation issue. Moreover, we propose a novel reliable graph pseudo labeling algorithm that can effectively leverage the knowledge of unlabeled nodes while mitigating the adverse effects of noisy pseudo labels. Based on those novel designs, we can attain exceptional effectiveness and efficiency in solving the studied problem. We conduct extensive experiments on benchmark datasets for semi-supervised node classification with different levels of label noise and show new state-of-the-art performance. The code is available at https://github.com/DND-NET/DND-NET.},
  archive   = {C_KDD},
  author    = {Ding, Kaize and Ma, Xiaoxiao and Liu, Yixin and Pan, Shirui},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671798},
  pages     = {574–584},
  title     = {Divide and denoise: Empowering simple models for robust semi-supervised node classification against label noise},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unraveling block maxima forecasting models with
counterfactual explanation. <em>KDD</em>, 562–573. (<a
href="https://doi.org/10.1145/3637528.3671923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Disease surveillance, traffic management, and weather forecasting are some of the key applications that could benefit from block maxima forecasting of a time series as the extreme block maxima values often signify events of critical importance such as disease outbreaks, traffic gridlock, and severe weather conditions. As the use of deep neural network models for block maxima forecasting increases, so does the need for explainable AI methods that could unravel the inner workings of such black box models. To fill this need, this paper presents a novel counterfactual explanation framework for block maxima forecasting models. Unlike existing methods, our proposed framework, DiffusionCF, combines deep anomaly detection with a conditional diffusion model to identify unusual patterns in the time series that could help explain the forecasted extreme block maxima. Experimental results on several real-world datasets demonstrate the superiority of DiffusionCF over other baseline methods when evaluated according to various metrics, particularly their informativeness and closeness. Our data and codes are available at https://github.com/yue2023cs/DiffusionCF.},
  archive   = {C_KDD},
  author    = {Deng, Yue and Galib, Asadullah Hill and Tan, Pang-Ning and Luo, Lifeng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671923},
  pages     = {562–573},
  title     = {Unraveling block maxima forecasting models with counterfactual explanation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explanatory model monitoring to understand the effects of
feature shifts on performance. <em>KDD</em>, 550–561. (<a
href="https://doi.org/10.1145/3637528.3671959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Monitoring and maintaining machine learning models are among the most critical challenges in translating recent advances in the field into real-world applications. However, current monitoring methods lack the capability of provide actionable insights answering the question of why the performance of a particular model really degraded. In this work, we propose a novel approach to explain the behavior of a black-box model under feature shifts by attributing an estimated performance change to interpretable input characteristics. We refer to our method that combines concepts from Optimal Transport and Shapley Values as Explanatory Performance Estimation (XPE). We analyze the underlying assumptions and demonstrate the superiority of our approach over several baselines on different data sets across various data modalities such as images, audio, and tabular data. We also indicate how the generated results can lead to valuable insights, enabling explanatory model monitoring by revealing potential root causes for model deterioration and guiding toward actionable countermeasures.},
  archive   = {C_KDD},
  author    = {Decker, Thomas and Koebler, Alexander and Lebacher, Michael and Thon, Ingo and Tresp, Volker and Buettner, Florian},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671959},
  pages     = {550–561},
  title     = {Explanatory model monitoring to understand the effects of feature shifts on performance},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AGS-GNN: Attribute-guided sampling for graph neural
networks. <em>KDD</em>, 538–549. (<a
href="https://doi.org/10.1145/3637528.3671940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose AGS-GNN, a novel attribute-guided sampling algorithm for Graph Neural Networks (GNNs). AGS-GNN exploits the node features and the connectivity structure of a graph while simultaneously adapting for both homophily and heterophily in graphs. In homophilic graphs, vertices of the same class are more likely to be adjacent, but vertices of different classes tend to be adjacent in heterophilic graphs. GNNs have been successfully applied to homophilic graphs, but their utility to heterophilic graphs remains challenging. The state-of-the-art GNNs for heterophilic graphs use the full neighborhood of a node instead of sampling it, and hence do not scale to large graphs and are not inductive. We develop dual-channel sampling techniques based on feature-similarity and feature-diversity to select subsets of neighbors for a node that capture adaptive information from homophilic and heterophilic neighborhoods. Currently, AGS-GNN is the only algorithm that explicitly controls homophily in the sampled subgraph through similar and diverse neighborhood samples. For diverse neighborhood sampling, we employ submodularity, a novel contribution in this context. We pre-compute the sampling distribution in parallel, achieving the desired scalability. Using an extensive dataset consisting of 35 small (&amp;lt; 100K nodes) and large (- 100K nodes) homophilic and heterophilic graphs, we demonstrate the superiority of AGS-GNN compared to the state-of-the-art approaches. AGS-GNN achieves test accuracy comparable to the best-performing heterophilic GNNs, even outperforming methods that use the entire graph for node classification. AGS-GNN converges faster than methods that sample neighborhoods randomly, and can be incorporated into existing GNN models that employ node or graph sampling.},
  archive   = {C_KDD},
  author    = {Das, Siddhartha Shankar and Ferdous, S M and Halappanavar, Mahantesh M. and Serra, Edoardo and Pothen, Alex},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671940},
  pages     = {538–549},
  title     = {AGS-GNN: Attribute-guided sampling for graph neural networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural retrievers are biased towards LLM-generated content.
<em>KDD</em>, 526–537. (<a
href="https://doi.org/10.1145/3637528.3671882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, the emergence of large language models (LLMs) has revolutionized the paradigm of information retrieval (IR) applications, especially in web search, by generating vast amounts of human-like texts on the Internet. As a result, IR systems in the LLM era are facing a new challenge: the indexed documents are now not only written by human beings but also automatically generated by the LLMs. How these LLM-generated documents influence the IR systems is a pressing and still unexplored question. In this work, we conduct a quantitative evaluation of IR models in scenarios where both human-written and LLM-generated texts are involved. Surprisingly, our findings indicate that neural retrieval models tend to rank LLM-generated documents higher. We refer to this category of biases in neural retrievers towards the LLM-generated content as the source bias. Moreover, we discover that this bias is not confined to the first-stage neural retrievers, but extends to the second-stage neural re-rankers. Then, in-depth analyses from the perspective of text compression indicate that LLM-generated texts exhibit more focused semantics with less noise, making it easier for neural retrieval models to semantic match. To mitigate the source bias, we also propose a plug-and-play debiased constraint for the optimization objective, and experimental results show its effectiveness. Finally, we discuss the potential severe concerns stemming from the observed source bias and hope our findings can serve as a critical wake-up call to the IR community and beyond. To facilitate future explorations of IR in the LLM era, the constructed two new benchmarks are available at https://github.com/KID-22/Source-Bias.},
  archive   = {C_KDD},
  author    = {Dai, Sunhao and Zhou, Yuqi and Pang, Liang and Liu, Weihao and Hu, Xiaolin and Liu, Yong and Zhang, Xiao and Wang, Gang and Xu, Jun},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671882},
  pages     = {526–537},
  title     = {Neural retrievers are biased towards LLM-generated content},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fairness in streaming submodular maximization subject to a
knapsack constraint. <em>KDD</em>, 514–525. (<a
href="https://doi.org/10.1145/3637528.3671778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Submodular optimization has been identified as a powerful tool for many data mining applications, where a representative subset of moderate size needs to be extracted from a large-scale dataset. In scenarios where data points possess sensitive attributes such as age, gender, or race, it becomes imperative to integrate fairness measures into submodular optimization to mitigate bias and discrimination. In this paper, we study the fundamental problem of fair submodular maximization subject to a knapsack constraint and propose the first streaming algorithm for it with provable performance guarantees for both monotone and non-monotone submodular functions. As a byproduct, we also propose a streaming algorithm for submodular maximization subject to a partition matroid and a knapsack constraint, significantly improving the performance bounds achieved by previous work. We conduct extensive experiments on real-world applications such as movie recommendation, image summarization, and maximum coverage in social networks. The experimental results strongly demonstrate the superiority of our proposed algorithms in terms of both fairness and utility.},
  archive   = {C_KDD},
  author    = {Cui, Shuang and Han, Kai and Tang, Shaojie and Li, Feng and Luo, Jun},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671778},
  pages     = {514–525},
  title     = {Fairness in streaming submodular maximization subject to a knapsack constraint},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging pedagogical theories to understand student
learning process with graph-based reasonable knowledge tracing.
<em>KDD</em>, 502–513. (<a
href="https://doi.org/10.1145/3637528.3671853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Knowledge tracing (KT) is a crucial task in intelligent education, focusing on predicting students&#39; performance on given questions to trace their evolving knowledge. The advancement of deep learning in this field has led to deep-learning knowledge tracing (DLKT) models that prioritize high predictive accuracy. However, many existing DLKT methods overlook the fundamental goal of tracking students&#39; dynamical knowledge mastery. These models do not explicitly model knowledge mastery tracing processes or yield unreasonable results that educators find difficulty to comprehend and apply in real teaching scenarios. In response, our research conducts a preliminary analysis of mainstream KT approaches to highlight and explain such unreasonableness. We introduce GRKT, a graph-based reasonable knowledge tracing method to address these issues. By leveraging graph neural networks, our approach delves into the mutual influences of knowledge concepts, offering a more accurate representation of how the knowledge mastery evolves throughout the learning process. Additionally, we propose a fine-grained and psychological three-stage modeling process as knowledge retrieval, memory strengthening, and knowledge learning/forgetting, to conduct a more reasonable knowledge tracing process. Comprehensive experiments demonstrate that GRKT outperforms eleven baselines across three datasets, not only enhancing predictive accuracy but also generating more reasonable knowledge tracing results. This makes our model a promising advancement for practical implementation in educational settings. The source code is available at https://github.com/JJCui96/GRKT.},
  archive   = {C_KDD},
  author    = {Cui, Jiajun and Qian, Hong and Jiang, Bo and Zhang, Wei},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671853},
  pages     = {502–513},
  title     = {Leveraging pedagogical theories to understand student learning process with graph-based reasonable knowledge tracing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Relevance meets diversity: A user-centric framework for
knowledge exploration through recommendations. <em>KDD</em>, 490–501.
(<a href="https://doi.org/10.1145/3637528.3671949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Providing recommendations that are both relevant and diverse is a key consideration of modern recommender systems. Optimizing both of these measures presents a fundamental trade-off, as higher diversity typically comes at the cost of relevance, resulting in lower user engagement. Existing recommendation algorithms try to resolve this trade-off by combining the two measures, relevance and diversity, into one aim and then seeking recommendations that optimize the combined objective, for a given number of items. Traditional approaches, however, do not consider the user interaction with the suggested items. In this paper, we put the user at the central stage, and build on the interplay between relevance, diversity, and user behavior. In contrast to applications where the goal is solely to maximize engagement, we focus on scenarios aiming at maximizing the total amount of knowledge encountered by the user. We use diversity as a surrogate for the amount of knowledge obtained by the user while interacting with the system, and we seek to maximize diversity. We propose a probabilistic user-behavior model in which users keep interacting with the recommender system as long as they receive relevant suggestions, but they may stop if the relevance of the recommended items drops. Thus, for a recommender system to achieve a high-diversity measure, it will need to produce recommendations that are both relevant and diverse. Finally, we propose a novel recommendation strategy that combines relevance and diversity by a copula function. We conduct an extensive evaluation of the proposed methodology over multiple datasets, and we show that our strategy outperforms several state-of-the-art competitors. Our implementation is publicly available at https://github.com/EricaCoppolillo/EXPLORE.},
  archive   = {C_KDD},
  author    = {Coppolillo, Erica and Manco, Giuseppe and Gionis, Aristides},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671949},
  pages     = {490–501},
  title     = {Relevance meets diversity: A user-centric framework for knowledge exploration through recommendations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient exploration of the rashomon set of rule-set
models. <em>KDD</em>, 478–489. (<a
href="https://doi.org/10.1145/3637528.3671818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Today, as increasingly complex predictive models are developed, simple rule sets remain a crucial tool to obtain interpretable predictions and drive high-stakes decision making. However, a single rule set provides a partial representation of a learning task. An emerging paradigm in interpretable machine learning aims at exploring the Rashomon set of all models exhibiting near-optimal performance. Existing work on Rashomon-set exploration focuses on exhaustive search of the Rashomon set for particular classes of models, which can be a computationally challenging task. On the other hand, exhaustive enumeration leads to redundancy that often is not necessary, and a representative sample or an estimate of the size of the Rashomon set is sufficient for many applications. In this work, we propose, for the first time, efficient methods to explore the Rashomon set of rule-set models with or without exhaustive search. Extensive experiments demonstrate the effectiveness of the proposed methods in a variety of scenarios.},
  archive   = {C_KDD},
  author    = {Ciaperoni, Martino and Xiao, Han and Gionis, Aristides},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671818},
  pages     = {478–489},
  title     = {Efficient exploration of the rashomon set of rule-set models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Iterative weak learnability and multiclass AdaBoost.
<em>KDD</em>, 466–477. (<a
href="https://doi.org/10.1145/3637528.3671842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose an efficient boosting algorithm for multiclass classification, called AdaBoost.Iter, that extends SAMME and AdaBoost. The algorithm iteratively applies the weak learnability condition of SAMME to eliminate classes to find the correct classificiation. The iterative weak learnability is a sufficient and necessary condition for boostability, but it is also easier to validate than the EOR criterion of AdaBoost.MM citeMukherjeeSchapire2013. We show that the training error of AdaBoost.Iter vanishes at the exponential rate, while the generalization error converges to zero at the same rate as AdaBoost. AdaBoost.Iter numerically outperforms SAMME and achieves performance comparable to AdaBoost.MM on benchmark datasets.},
  archive   = {C_KDD},
  author    = {Cho, In-Koo and Libgober, Jonathan A. and Ding, Cheng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671842},
  pages     = {466–477},
  title     = {Iterative weak learnability and multiclass AdaBoost},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing contrastive learning on graphs with node
similarity. <em>KDD</em>, 456–465. (<a
href="https://doi.org/10.1145/3637528.3671898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph Neural Networks (GNN) have proven successful for graph-related tasks. However, many GNNs methods require labeled data, which is challenging to obtain. To tackle this, graph contrastive learning (GCL) have gained attention. GCL learns by contrasting similar nodes (positives) and dissimilar nodes (negatives). Current GCL methods, using data augmentation for positive samples and random selection for negative samples, can be sub-optimal due to limited positive samples and the possibility of false-negative samples. In this study, we propose an enhanced objective addressing these issues. We first introduce an ideal objective with all positive and no false-negative samples, then transform it probabilistically based on sampling distributions. We next model these distributions with node similarity and derive an enhanced objective. Comprehensive experiments have shown the effectiveness of the proposed enhanced objective for a broad set of GCL models.},
  archive   = {C_KDD},
  author    = {Chi, Hongliang and Ma, Yao},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671898},
  pages     = {456–465},
  title     = {Enhancing contrastive learning on graphs with node similarity},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Retrieval-augmented hypergraph for multimodal social media
popularity prediction. <em>KDD</em>, 445–455. (<a
href="https://doi.org/10.1145/3637528.3672041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurately predicting the popularity of multimodal user-generated content (UGC) is fundamental for many real-world applications such as online advertising and recommendation. Existing approaches generally focus on limited contextual information within individual UGCs, yet overlook the potential benefit of exploiting meaningful knowledge in relevant UGCs. In this work, we propose RAGTrans, an aspect-aware retrieval-augmented multi-modal hypergraph transformer that retrieves pertinent knowledge from a multi-modal memory bank and enhances UGC representations via neighborhood knowledge aggregation on multi-model hypergraphs. In particular, we initially retrieve relevant multimedia instances from a large corpus of UGCs via the aspect information and construct a knowledge-enhanced hypergraph based on retrieved relevant instances. This allows capturing meaningful contextual information across the data. We then design a novel bootstrapping hypergraph transformer on multimodal hypergraphs to strengthen UGC representations across modalities via customizing a propagation algorithm to effectively diffuse information across nodes and edges. Additionally, we propose a user-aware attention-based fusion module to comprise the enriched UGC representations for popularity prediction. Extensive experiments on real-world social media datasets demonstrate that RAGTrans outperforms state-of-the-art popularity prediction models across settings.},
  archive   = {C_KDD},
  author    = {Cheng, Zhangtao and Zhang, Jienan and Xu, Xovee and Trajcevski, Goce and Zhong, Ting and Zhou, Fan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672041},
  pages     = {445–455},
  title     = {Retrieval-augmented hypergraph for multimodal social media popularity prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Resurrecting label propagation for graphs with heterophily
and label noise. <em>KDD</em>, 433–444. (<a
href="https://doi.org/10.1145/3637528.3671774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Label noise is a common challenge in large datasets, as it can significantly degrade the generalization ability of deep neural networks. Most existing studies focus on noisy labels in computer vision; however, graph models encompass both node features and graph topology as input, and become more susceptible to label noise through message-passing mechanisms. Recently, only a few works have been proposed to tackle the label noise on graphs. One significant limitation is that they operate under the assumption that the graph exhibits homophily and that the labels are distributed smoothly. However, real-world graphs can exhibit varying degrees of heterophily, or even be dominated by heterophily, which results in the inadequacy of the current methods.In this paper, we study graph label noise in the context of arbitrary heterophily, with the aim of rectifying noisy labels and assigning labels to previously unlabeled nodes. We begin by conducting two empirical analyses to explore the impact of graph homophily on graph label noise. Following observations, we propose a efficient algorithm, denoted as R2LP. Specifically, R2LP is an iterative algorithm with three steps: (1) reconstruct the graph to recover the homophily property, (2) utilize label propagation to rectify the noisy labels, (3) select high-confidence labels to retain for the next iteration. By iterating these steps, we obtain a set of &#39;&#39;correct&#39;&#39; labels, ultimately achieving high accuracy in the node classification task. The theoretical analysis is also provided to demonstrate its remarkable denoising effect. Finally, we perform experiments on ten benchmark datasets with different levels of graph heterophily and various types of noise. In these experiments, we compare the performance of R2LP against ten typical baseline methods. Our results illustrate the superior performance of the proposed \o{}urs. The code and data of this paper can be accessed at: https://github.com/cy623/R2LP.git.},
  archive   = {C_KDD},
  author    = {Cheng, Yao and Shan, Caihua and Shen, Yifei and Li, Xiang and Luo, Siqiang and Li, Dongsheng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671774},
  pages     = {433–444},
  title     = {Resurrecting label propagation for graphs with heterophily and label noise},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Co-neighbor encoding schema: A light-cost structure encoding
method for dynamic link prediction. <em>KDD</em>, 421–432. (<a
href="https://doi.org/10.1145/3637528.3671770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Structure encoding has proven to be the key feature to distinguishing links in a graph. However, Structure encoding in the temporal graph keeps changing as the graph evolves, repeatedly computing such features can be time-consuming due to the high-order subgraph construction. We develop the Co-Neighbor Encoding Schema (CNES) to address this issue. Instead of recomputing the feature by the link, CNES stores information in the memory to avoid redundant calculations. Besides, unlike the existing memory-based dynamic graph learning method that stores node hidden states, we introduce a hashtable-based memory to compress the adjacency matrix for efficient structure feature construction and updating with vector computation in parallel. Furthermore, CNES introduces a Temporal-Diverse Memory to generate long-term and short-term structure encoding for neighbors with different structural information. A dynamic graph learning framework, Co-Neighbor Encoding Network (CNE-N), is proposed using the aforementioned techniques. Extensive experiments on thirteen public datasets verify the effectiveness and efficiency of the proposed method.},
  archive   = {C_KDD},
  author    = {Cheng, Ke and Linzhi, Peng and Ye, Junchen and Sun, Leilei and Du, Bowen},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671770},
  pages     = {421–432},
  title     = {Co-neighbor encoding schema: A light-cost structure encoding method for dynamic link prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DyGKT: Dynamic graph learning for knowledge tracing.
<em>KDD</em>, 409–420. (<a
href="https://doi.org/10.1145/3637528.3671773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Knowledge Tracing aims to assess student learning states by predicting their performance in answering questions. Different from the existing research which utilizes fixed-length learning sequence to obtain the student states and regards KT as a static problem, this work is motivated by three dynamical characteristics: 1) The scales of students answering records are constantly growing; 2) The semantics of time intervals between the records vary; 3) The relationships between students, questions and concepts are evolving. The three dynamical characteristics above contain the great potential to revolutionize the existing knowledge tracing methods. Along this line, we propose a Dynamic Graph-based Knowledge Tracing model, namely DyGKT. In particular, a continuous-time dynamic question-answering graph for knowledge tracing is constructed to deal with the infinitely growing answering behaviors, and it is worth mentioning that it is the first time dynamic graph learning technology is used in this field. Then, a dual time encoder is proposed to capture long-term and short-term semantics among the different time intervals. Finally, a multiset indicator is utilized to model the evolving relationships between students, questions, and concepts via the graph structural feature. Numerous experiments are conducted on five real-world datasets, and the results demonstrate the superiority of our model. All the used resources are publicly available at https://github.com/PengLinzhi/DyGKT.},
  archive   = {C_KDD},
  author    = {Cheng, Ke and Peng, Linzhi and Wang, Pengyang and Ye, Junchen and Sun, Leilei and Du, Bowen},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671773},
  pages     = {409–420},
  title     = {DyGKT: Dynamic graph learning for knowledge tracing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conformal counterfactual inference under hidden confounding.
<em>KDD</em>, 397–408. (<a
href="https://doi.org/10.1145/3637528.3671976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Personalized decision making requires the knowledge of potential outcomes under different treatments, and confidence intervals about the potential outcomes further enrich this decision-making process and improve its reliability in high-stakes scenarios. Predicting potential outcomes along with its uncertainty in a counterfactual world poses the foundamental challenge in causal inference. Existing methods that construct confidence intervals for counterfactuals either rely on the assumption of strong ignorability that completely ignores hidden confounders, or need access to un-identifiable lower and upper bounds that characterize the difference between observational and interventional distributions. In this paper, to overcome these limitations, we first propose a novel approach wTCP-DR based on transductive weighted conformal prediction, which provides confidence intervals for counterfactual outcomes with marginal converage guarantees, even under hidden confounding. With less restrictive assumptions, our approach requires access to a fraction of interventional data (from randomized controlled trials) to account for the covariate shift from observational distributoin to interventional distribution. Theoretical results explicitly demonstrate the conditions under which our algorithm is strictly advantageous to the naive method that only uses interventional data. Since transductive conformal prediction is notoriously costly, we propose wSCP-DR, a two-stage variant of wTCP-DR, based on split conformal prediction with same marginal coverage guarantees but at a significantly lower computational cost. After ensuring valid intervals on counterfactuals, it is straightforward to construct intervals for individual treatment effects (ITEs). We demonstrate our method across synthetic and real-world data, including recommendation systems, to verify the superiority of our methods compared against state-of-the-art baselines in terms of both coverage and efficiency. Our code can be found at https://github.com/rguo12/KDD24-Conformal.},
  archive   = {C_KDD},
  author    = {Chen, Zonghao and Guo, Ruocheng and Ton, Jean-Francois and Liu, Yang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671976},
  pages     = {397–408},
  title     = {Conformal counterfactual inference under hidden confounding},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Shopping trajectory representation learning with
pre-training for e-commerce customer understanding and recommendation.
<em>KDD</em>, 385–396. (<a
href="https://doi.org/10.1145/3637528.3671747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Understanding customer behavior is crucial for improving service quality in large-scale E-commerce. This paper proposes C-STAR, a new framework that learns compact representations from customer shopping journeys, with good versatility to fuel multiple downstream customer-centric tasks. We define the notion of shopping trajectory that encompasses customer interactions at the level of product categories, capturing the overall flow of their browsing and purchase activities. C-STAR excels at modeling both inter-trajectory distribution similarity-the structural similarities between different trajectories, and intra-trajectory semantic correlation-the semantic relationships within individual ones. This coarse-to-fine approach ensures informative trajectory embeddings for representing customers. To enhance embedding quality, we introduce a pre-training strategy that captures two intrinsic properties within the pre-training data. Extensive evaluation on large-scale industrial and public datasets demonstrates the effectiveness of C-STAR across three diverse customer-centric tasks. These tasks empower customer profiling and recommendation services for enhancing personalized shopping experiences on our E-commerce platform.},
  archive   = {C_KDD},
  author    = {Chen, Yankai and Truong, Quoc-Tuan and Shen, Xin and Li, Jin and King, Irwin},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671747},
  pages     = {385–396},
  title     = {Shopping trajectory representation learning with pre-training for E-commerce customer understanding and recommendation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Maximum-entropy regularized decision transformer with reward
relabelling for dynamic recommendation. <em>KDD</em>, 376–384. (<a
href="https://doi.org/10.1145/3637528.3671750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning-based recommender systems have recently gained popularity. However, due to the typical limitations of simulation environments (e.g., data inefficiency), most of the work cannot be broadly applied in all domains. To counter these challenges, recent advancements have leveraged offline reinforcement learning methods, notable for their data-driven approach utilizing offline datasets. A prominent example of this is the Decision Transformer. Despite its popularity, the Decision Transformer approach has inherent drawbacks, particularly evident in recommendation methods based on it. This paper identifies two key shortcomings in existing Decision Transformer-based methods: a lack of stitching capability and limited effectiveness in online adoption. In response, we introduce a novel methodology named Max-Entropy enhanced Decision Transformer with Reward Relabeling for Offline RLRS (EDT4Rec). Our approach begins with a max entropy perspective, leading to the development of a max-entropy enhanced exploration strategy. This strategy is designed to facilitate more effective exploration in online environments. Additionally, to augment the model&#39;s capability to stitch sub-optimal trajectories, we incorporate a unique reward relabeling technique. To validate the effectiveness and superiority of EDT4Rec, we have conducted comprehensive experiments across six real-world offline datasets and in an online simulator.},
  archive   = {C_KDD},
  author    = {Chen, Xiaocong and Wang, Siyu and Yao, Lina},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671750},
  pages     = {376–384},
  title     = {Maximum-entropy regularized decision transformer with reward relabelling for dynamic recommendation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hate speech detection with generalizable target-aware
fairness. <em>KDD</em>, 365–375. (<a
href="https://doi.org/10.1145/3637528.3671821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To counter the side effect brought by the proliferation of social media platforms, hate speech detection (HSD) plays a vital role in halting the dissemination of toxic online posts at an early stage. However, given the ubiquitous topical communities on social media, a trained HSD classifier can easily become biased towards specific targeted groups (e.g.,female andblack people), where a high rate of either false positive or false negative results can significantly impair public trust in the fairness of content moderation mechanisms, and eventually harm the diversity of online society. Although existing fairness-aware HSD methods can smooth out some discrepancies across targeted groups, they are mostly specific to a narrow selection of targets that are assumed to be known and fixed. This inevitably prevents those methods from generalizing to real-world use cases where new targeted groups constantly emerge (e.g., new forums created on Reddit) over time. To tackle the defects of existing HSD practices, we propose &amp;lt;u&amp;gt;Ge&amp;lt;/u&amp;gt;neralizable &amp;lt;u&amp;gt;t&amp;lt;/u&amp;gt;arget-aware &amp;lt;u&amp;gt;Fair&amp;lt;/u&amp;gt;ness (GetFair), a new method for fairly classifying each post that contains diverse and even unseen targets during inference. To remove the HSD classifier&#39;s spurious dependence on target-related features, GetFair trains a series of filter functions in an adversarial pipeline, so as to deceive the discriminator that recovers the targeted group from filtered post embeddings. To maintain scalability and generalizability, we innovatively parameterize all filter functions via a hypernetwork. Taking a target&#39;s pretrained word embedding as input, the hypernetwork generates the weights used by each target-specific filter on-the-fly without storing dedicated filter parameters. In addition, a novel semantic gap alignment scheme is imposed on the generation process, such that the produced filter function for an unseen target is rectified by its semantic affinity with existing targets used for training. Finally, experiments are conducted on two benchmark HSD datasets, showing advantageous performance of GetFair on out-of-sample targets among baselines.},
  archive   = {C_KDD},
  author    = {Chen, Tong and Wang, Danny and Liang, Xurong and Risius, Marten and Demartini, Gianluca and Yin, Hongzhi},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671821},
  pages     = {365–375},
  title     = {Hate speech detection with generalizable target-aware fairness},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GraphWiz: An instruction-following language model for graph
computational problems. <em>KDD</em>, 353–364. (<a
href="https://doi.org/10.1145/3637528.3672010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large language models (LLMs) have achieved impressive success across various domains, but their capability in understanding and resolving complex graph problems is less explored. To bridge this gap, we introduce GraphInstruct, a novel instruction-tuning dataset aimed at enabling language models to tackle a broad spectrum of graph problems through explicit reasoning paths. Utilizing GraphInstruct, we build GraphWiz, an open-source language model capable of solving various graph computational problems while generating clear reasoning processes. To further enhance the model&#39;s performance and reliability, we integrate the Direct Preference Optimization (DPO) framework within the graph problem-solving context. The improved model, GraphWiz-DPO, achieves an average accuracy of 65\% across nine tasks with different complexity levels, surpassing GPT-4 which has an average accuracy of 43.8\%. Our study also investigates the relationship between training data volume and model performance, emphasizing the risk of overfitting as data volume increases. Additionally, we explore the transferability of the proposed model across different tasks and datasets, demonstrating its robust zero-shot generalization capability. GraphWiz offers a new blueprint and valuable insights for developing LLMs specialized in graph reasoning and problem-solving.},
  archive   = {C_KDD},
  author    = {Chen, Nuo and Li, Yuhan and Tang, Jianheng and Li, Jia},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672010},
  pages     = {353–364},
  title     = {GraphWiz: An instruction-following language model for graph computational problems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Calibration of time-series forecasting: Detecting and
adapting context-driven distribution shift. <em>KDD</em>, 341–352. (<a
href="https://doi.org/10.1145/3637528.3671926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent years have witnessed the success of introducing deep learning models to time series forecasting. From a data generation perspective, we illustrate that existing models are susceptible to distribution shifts driven by temporal contexts, whether observed or unobserved. Such context-driven distribution shift (CDS) introduces biases in predictions within specific contexts and poses challenges for conventional training paradigms. In this paper, we introduce a universal calibration methodology for the detection and adaptation of CDS with a trained model. To this end, we propose a novel CDS detector, termed the &quot;residual-based CDS detector&quot; or &quot;Reconditionor&quot;, which quantifies the model&#39;s vulnerability to CDS by evaluating the mutual information between prediction residuals and their corresponding contexts. A high Reconditionor score indicates a severe susceptibility, thereby necessitating model adaptation. In this circumstance, we put forth a straightforward yet potent adapter framework for model calibration, termed the &quot;sample-level contextualized adapter&quot; or &quot;SOLID&quot;. This framework involves the curation of a contextually similar dataset to the provided test sample and the subsequent fine-tuning of the model&#39;s prediction layer with a limited number of steps. Our theoretical analysis demonstrates that this adaptation strategy can achieve an optimal bias-variance trade-off. Notably, our proposed Reconditionor and SOLID are model-agnostic and readily adaptable to a wide range of models. Extensive experiments show that SOLID consistently enhances the performance of current forecasting models on real-world datasets, especially on cases with substantial CDS detected by the proposed Reconditionor, thus validating the effectiveness of the calibration approach.},
  archive   = {C_KDD},
  author    = {Chen, Mouxiang and Shen, Lefei and Fu, Han and Li, Zhuo and Sun, Jianling and Liu, Chenghao},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671926},
  pages     = {341–352},
  title     = {Calibration of time-series forecasting: Detecting and adapting context-driven distribution shift},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explicit and implicit modeling via dual-path transformer for
behavior set-informed sequential recommendation. <em>KDD</em>, 329–340.
(<a href="https://doi.org/10.1145/3637528.3671755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sequential recommendation (SR) and multi-behavior sequential recommendation (MBSR) both come from real-world scenarios. Compared with SR, MBSR takes into account the dependencies of different behaviors. We find that most existing works on MBSR are studied in the context of e-commerce scenarios. In terms of the data format of the behavior types, we observe that the conventional label-formatted data carries limited information and is inadequate for scenarios like social media. With this observation, we introducebehavior set and extend MBSR to behavior set-informed sequential recommendation (BSSR). In BSSR, behavior dependencies become more complex and personalized, and user interest arousal may lack explicit contextual associations. To delve into the dynamics inhered within a behavior set and adaptively tailor recommendation lists upon its variability, we propose a novel solution called Explicit and Implicit modeling via Dual-Path Transformer (EIDP) for BSSR. Our EIDP adopts a dual-path architecture, distinguishing between explicit modeling path (EMP) and implicit modeling path (IMP) based on whether to directly incorporate the behavior representations. EMP features the personalized behavior set-wise transition pattern extractor (PBS-TPE) as its core component. It couples behavioral representations with both the items and positions to explore intra-behavior dynamics within a behavior set at a fine granularity. IMP utilizes light multi-head self-attention blocks (L-MSAB) as encoders under specific behavior types. The obtained multi-view representations are then aggregated by cross-behavior attention fusion (CBAF), using the behavior set of the next time step as a guidance to extract collaborative semantics at the behavioral level. Extensive experiments on two real-world datasets demonstrate the effectiveness of our EIDP. We release the implementation code at: https://github.com/OshiNoCSMA/EIDP.},
  archive   = {C_KDD},
  author    = {Chen, Ming and Pan, Weike and Ming, Zhong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671755},
  pages     = {329–340},
  title     = {Explicit and implicit modeling via dual-path transformer for behavior set-informed sequential recommendation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Profiling urban streets: A semi-supervised prediction model
based on street view imagery and spatial topology. <em>KDD</em>,
319–328. (<a href="https://doi.org/10.1145/3637528.3671918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the expansion and growth of cities, profiling urban areas with the advent of multi-modal urban datasets (e.g., points-of-interest and street view imagery) has become increasingly important in urban planing and management. Particularly, street view images have gained popularity for understanding the characteristics of urban areas due to its abundant visual information and inherent correlations with human activities. In this study, we define a street segment represented by multiple street view images as the minimum spatial unit for analysis and predict its functional and socioeconomic indicators, which presents several challenges in modeling spatial distributions of images on a street and the spatial topology (adjacency) of streets. Meanwhile, Large Language Models are capable of understanding imagery data based on its extraordinary knowledge base and unveil a remarkable opportunity for profiling streets with images. In view of the challenges and opportunity, we present a semi-supervised Urban Street Profiling Model (USPM) based on street view imagery and spatial adjacency of urban streets. Specifically, given a street with multiple images, we first employ a newly designed spatial context-based contrastive learning method to generate feature vectors of images and then apply the LSTM-based fusion method to encode multiple images on a street to yield the street visual representation; we then create the descriptions of street scenes for street view images based on the SPHINX (a large language model) and produce the street textual representation; finally, we build an urban street graph based on spatial topology (adjacency) and employ a semi-supervised graph learning algorithm to further encode the street representations for prediction. We conduct thorough experiments with real-world datasets to assess the proposed USPM. The experimental results demonstrate that USPM considerably outperforms baseline methods in two urban prediction tasks.},
  archive   = {C_KDD},
  author    = {Chen, Meng and Li, Zechen and Huang, Weiming and Gong, Yongshun and Yin, Yilong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671918},
  pages     = {319–328},
  title     = {Profiling urban streets: A semi-supervised prediction model based on street view imagery and spatial topology},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large language model-driven meta-structure discovery in
heterogeneous information network. <em>KDD</em>, 307–318. (<a
href="https://doi.org/10.1145/3637528.3671965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Heterogeneous information networks (HIN) have gained increasing popularity in recent years for capturing complex relations between diverse types of nodes. Meta-structures are proposed as a useful tool to identify the important patterns in HINs, but hand-crafted meta-structures pose significant challenges for scaling up, drawing wide research attention towards developing automatic search algorithms. Previous efforts primarily focused on searching for meta-structures with good empirical performance, overlooking the importance of human comprehensibility and generalizability. To address this challenge, we draw inspiration from the emergent reasoning abilities of large language models (LLMs). We propose ReStruct, a meta-structure search framework that integrates LLM reasoning into the evolutionary procedure. ReStruct uses a grammar translator to encode the meta-structures into natural language sentences, and leverages the reasoning power of LLMs to evaluate their semantic feasibility. Besides, ReStruct also employs performance-oriented evolutionary operations. These two competing forces allow ReStruct to jointly optimize the semantic explainability and empirical performance of meta-structures. Furthermore, ReStruct contains a differential LLM explainer to generate and refine natural language explanations for the discovered meta-structures by reasoning through the search history. Experiments on eight representative HIN datasets demonstrate that ReStruct achieves state-of-the-art performance in both recommendation and node classification tasks. Moreover, a survey study involving 73 graduate students shows that the discovered meta-structures and generated explanations by ReStruct are substantially more comprehensible. Our code and questionnaire are available at https://github.com/LinChen-65/ReStruct.},
  archive   = {C_KDD},
  author    = {Chen, Lin and Xu, Fengli and Li, Nian and Han, Zhenyu and Wang, Meng and Li, Yong and Hui, Pan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671965},
  pages     = {307–318},
  title     = {Large language model-driven meta-structure discovery in heterogeneous information network},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). QGRL: Quaternion graph representation learning for
heterogeneous feature data clustering. <em>KDD</em>, 297–306. (<a
href="https://doi.org/10.1145/3637528.3671839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Clustering is one of the most commonly used techniques for unsupervised data analysis. As real data sets are usually composed of numerical and categorical features that are heterogeneous in nature, the heterogeneity in the distance metric and feature coupling prevents deep representation learning from achieving satisfactory clustering accuracy. Currently, supervised Quaternion Representation Learning (QRL) has achieved remarkable success in efficiently learning informative representations of coupled features from multiple views derived endogenously from the original data. To inherit the advantages of QRL for unsupervised heterogeneous feature representation learning, we propose a deep QRL model that works in an encoder-decoder manner. To ensure that the implicit couplings of heterogeneous feature data can be well characterized by representation learning, a hierarchical coupling encoding strategy is designed to convert the data set into an attributed graph to be the input of QRL. We also integrate the clustering objective into the model training to facilitate a joint optimization of the representation and clustering. Extensive experimental evaluations illustrate the superiority of the proposed Quaternion Graph Representation Learning (QGRL) method in terms of clustering accuracy and robustness to various data sets composed of arbitrary combinations of numerical and categorical features. The source code is opened at https://github.com/Juny-Chen/QGRL.git.},
  archive   = {C_KDD},
  author    = {Chen, Junyang and Ji, Yuzhu and Zou, Rong and Zhang, Yiqun and Cheung, Yiu-ming},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671839},
  pages     = {297–306},
  title     = {QGRL: Quaternion graph representation learning for heterogeneous feature data clustering},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Can a deep learning model be a sure bet for tabular
prediction? <em>KDD</em>, 288–296. (<a
href="https://doi.org/10.1145/3637528.3671893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Data organized in tabular format is ubiquitous in real-world applications, and users often craft tables with biased feature definitions and flexibly set prediction targets of their interests. Thus, a rapid development of a robust, effective, dataset-versatile, user-friendly tabular prediction approach is highly desired. While Gradient Boosting Decision Trees (GBDTs) and existing deep neural networks (DNNs) have been extensively utilized by professional users, they present several challenges for casual users, particularly: (i) the dilemma of model selection due to their different dataset preferences, and (ii) the need for heavy hyperparameter searching, failing which their performances are deemed inadequate. In this paper, we delve into this question: Can we develop a deep learning model that serves as a sure bet solution for a wide range of tabular prediction tasks, while also being user-friendly for casual users? We delve into three key drawbacks of deep tabular models, encompassing: (P1) lack of rotational variance property, (P2) large data demand, and (P3) over-smooth solution. We propose ExcelFormer, addressing these challenges through a semi-permeable attention module that effectively constrains the influence of less informative features to break the DNNs&#39; rotational invariance property (for P1), data augmentation approaches tailored for tabular data (for P2), and attentive feedforward network to boost the model fitting capability (for P3). These designs collectively make ExcelFormer a sure bet solution for diverse tabular datasets. Extensive and stratified experiments conducted on real-world datasets demonstrate that our model outperforms previous approaches across diverse tabular data prediction tasks, and this framework can be friendly to casual users, offering ease of use without the heavy hyperparameter tuning. The codes are available at https://github.com/whatashot/excelformer.},
  archive   = {C_KDD},
  author    = {Chen, Jintai and Yan, Jiahuan and Chen, Qiyuan and Chen, Danny Z. and Wu, Jian and Sun, Jimeng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671893},
  pages     = {288–296},
  title     = {Can a deep learning model be a sure bet for tabular prediction?},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable algorithm for finding balanced subgraphs with
tolerance in signed networks. <em>KDD</em>, 278–287. (<a
href="https://doi.org/10.1145/3637528.3671674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Signed networks, characterized by edges labeled as either positive or negative, offer nuanced insights into interaction dynamics beyond the capabilities of unsigned graphs. Central to this is the task of identifying the maximum balanced subgraph, crucial for applications like polarized community detection in social networks and portfolio analysis in finance. Traditional models, however, are limited by an assumption of perfect partitioning, which fails to mirror the complexities of real-world data. Addressing this gap, we introduce an innovative generalized balanced subgraph model that incorporates tolerance for imbalance. Our proposed region-based heuristic algorithm, tailored for this NP -hard problem, strikes a balance between low time complexity and high-quality outcomes. Comparative experiments validate its superior performance against leading solutions, delivering enhanced effectiveness (notably larger subgraph sizes) and efficiency (achieving up to 100\texttimes{} speedup) in both traditional and generalized contexts.},
  archive   = {C_KDD},
  author    = {Chen, Jingbang and Mang, Qiuyang and Zhou, Hangrui and Peng, Richard and Gao, Yu and Ma, Chenhao},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671674},
  pages     = {278–287},
  title     = {Scalable algorithm for finding balanced subgraphs with tolerance in signed networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cluster-wide task slowdown detection in cloud system.
<em>KDD</em>, 266–277. (<a
href="https://doi.org/10.1145/3637528.3671936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Slow task detection is a critical problem in cloud operation and maintenance since it is highly related to user experience and can bring substantial liquidated damages. Most anomaly detection methods detect it from a single-task aspect. However, considering millions of concurrent tasks in large-scale cloud computing clusters, it becomes impractical and inefficient. Moreover, single-task slowdowns are very common and do not necessarily indicate a malfunction of a cluster due to its violent fluctuation nature in a virtual environment. Thus, we shift our attention to cluster-wide task slowdowns by utilizing the duration time distribution of tasks across a cluster, so that the computation complexity is not relevant to the number of tasks. The task duration time distribution often exhibits compound periodicity and local exceptional fluctuations over time. Though transformer-based methods are one of the most powerful methods to capture these time series normal variation patterns, we empirically find and theoretically explain the flaw of the standard attention mechanism in reconstructing subperiods with low amplitude when dealing with compound periodicity. To tackle these challenges, we propose SORN (i.e., &amp;lt;u&amp;gt;S&amp;lt;/u&amp;gt;kimming &amp;lt;u&amp;gt;O&amp;lt;/u&amp;gt;ff subperiods in descending amplitude order and &amp;lt;u&amp;gt;R&amp;lt;/u&amp;gt;econstructing &amp;lt;u&amp;gt;N&amp;lt;/u&amp;gt;on-slowing fluctuation), which consists of a Skimming Attention mechanism to reconstruct the compound periodicity and a Neural Optimal Transport module to distinguish cluster-wide slowdowns from other exceptional fluctuations. Furthermore, since anomalies in the training set are inevitable in a practical scenario, we propose a picky loss function, which adaptively assigns higher weights to reliable time slots in the training set. Extensive experiments demonstrate that SORN outperforms state-of-the-art methods on multiple real-world industrial datasets.},
  archive   = {C_KDD},
  author    = {Chen, Feiyi and Zhang, Yingying and Fan, Lunting and Liang, Yuxuan and Pang, Guansong and Wen, Qingsong and Deng, Shuiguang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671936},
  pages     = {266–277},
  title     = {Cluster-wide task slowdown detection in cloud system},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Harm mitigation in recommender systems under user preference
dynamics. <em>KDD</em>, 255–265. (<a
href="https://doi.org/10.1145/3637528.3671925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider a recommender system that takes into account the interplay between recommendations, the evolution of user interests, and harmful content. We model the impact of recommendations on user behavior, particularly the tendency to consume harmful content. We seek recommendation policies that establish a tradeoff between maximizing click-through rate (CTR) and mitigating harm. We establish conditions under which the user profile dynamics have a stationary point, and propose algorithms for finding an optimal recommendation policy at stationarity. We experiment on a semi-synthetic movie recommendation setting initialized with real data and observe that our policies outperform baselines at simultaneously maximizing CTR and mitigating harm.},
  archive   = {C_KDD},
  author    = {Chee, Jerry and Kalyanaraman, Shankar and Ernala, Sindhu Kiranmai and Weinsberg, Udi and Dean, Sarah and Ioannidis, Stratis},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671925},
  pages     = {255–265},
  title     = {Harm mitigation in recommender systems under user preference dynamics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hierarchical context augmentation method to improve
retrieval-augmented LLMs on scientific papers. <em>KDD</em>, 243–254.
(<a href="https://doi.org/10.1145/3637528.3671847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Scientific papers of a large scale on the Internet encompass a wealth of data and knowledge, attracting the attention of numerous researchers. To fully utilize these knowledge, Retrieval-Augmented Large Language Models (LLMs) usually leverage large-scale scientific corpus to train and then retrieve relevant passages from external memory to improve generation, which have demonstrated outstanding performance. However, existing methods can only capture one-dimension fragmented textual information without incorporating hierarchical structural knowledge, eg. the deduction relationship of abstract and main body, which makes it difficult to grasp the central thought of papers. To tackle this problem, we propose a hierarchical context augmentation method, which helps Retrieval-Augmented LLMs to autoregressively learn the structure knowledge of scientific papers. Specifically, we utilize the document tree to represent the hierarchical relationship of a paper and enhance the structure information of scientific context from three aspects: scale, format and global information. First, we think each top-bottom path of document tree is a logical independent context, which can be used to largely increase the scale of extracted structural corpus. Second, we propose a novel label-based format to represent the structure of context in textual sequences, unified between training and inference. Third, we introduce the global information of retrieved passages to further enhance the structure of context. Extensive experiments on three scientific tasks show that the proposed method significantly improves the performance of Retrieval-Augmented LLMs on all tasks. Besides, our method achieves start-of-art performance in Question Answer task and outperforms ChatGPT. Moreover, it also brings considerate gains with irrelevant retrieval passages, illustrating its effectiveness on practical application scenarios.},
  archive   = {C_KDD},
  author    = {Che, Tian-Yi and Mao, Xian-Ling and Lan, Tian and Huang, Heyan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671847},
  pages     = {243–254},
  title     = {A hierarchical context augmentation method to improve retrieval-augmented LLMs on scientific papers},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Path-based explanation for knowledge graph completion.
<em>KDD</em>, 231–242. (<a
href="https://doi.org/10.1145/3637528.3671683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph Neural Networks (GNNs) have achieved great success in Knowledge Graph Completion (KGC) by modelling how entities and relations interact in recent years. However, the explanation of the predicted facts has not caught the necessary attention. Proper explanations for the results of GNN-based KGC models increase model transparency and help researchers develop more reliable models. Existing practices for explaining KGC tasks rely on instance/subgraph-based approaches, while in some scenarios, paths can provide more user-friendly and interpretable explanations. Nonetheless, the methods for generating path-based explanations for KGs have not been well-explored. To address this gap, we propose Power-Link, the first path-based KGC explainer that explores GNN-based models. We design a novel simplified graph-powering technique, which enables the generation of path-based explanations with a fully parallelisable and memory-efficient training scheme. We further introduce three new metrics for quantitative evaluation of the explanations, together with a qualitative human evaluation. Extensive experiments demonstrate that Power-Link outperforms the SOTA baselines in interpretability, efficiency, and scalability. The code is available at https://github.com/OUTHIM/power-link},
  archive   = {C_KDD},
  author    = {Chang, Heng and Ye, Jiangnan and Lopez-Avila, Alejo and Du, Jinhua and Li, Jia},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671683},
  pages     = {231–242},
  title     = {Path-based explanation for knowledge graph completion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DiffusionE: Reasoning on knowledge graphs via
diffusion-based graph neural networks. <em>KDD</em>, 222–230. (<a
href="https://doi.org/10.1145/3637528.3671997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph Neural Networks (GNNs) have demonstrated powerful capabilities in reasoning within Knowledge Graphs (KGs), gathering increasing attention. Our idea stems from the observation that the prior work typically employs hand-designed or sample-designed paradigms in the process of message propagation, engaging a set of adjacent entities at each step of propagation. As a result, such methods struggle with the increasing number of entities involved as propagation steps extend. Moreover, they neglect the message interactions between adjacent entities and propagation relations in KG reasoning, leading to semantic inconsistency during the message aggregation phase. To address these issues, we introduce a novel knowledge graph embedding method through a diffusion process, termed DiffusionE. Specifically, we reformulate the message propagation in knowledge reasoning as a diffusion process, regarding the message semantics as the diffusion signal. In this sense, guided by semantic information, messages can be transmitted between nodes effectively and adaptively. Furthermore, the theoretical analysis suggests our method can leverage an optimal diffusivity for message propagation in the semantic interactions of KGs. It shows that DiffusionE effectively leverages message interactions between entities and propagation relations, ensuring semantic consistency in KG reasoning. Comprehensive experiments reveal that our method attains state-of-the-art performance compared to prior work on several well-established benchmarks.},
  archive   = {C_KDD},
  author    = {Cao, Zongsheng and Li, Jing and Wang, Zigan and Li, Jinliang},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671997},
  pages     = {222–230},
  title     = {DiffusionE: Reasoning on knowledge graphs via diffusion-based graph neural networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tackling instance-dependent label noise with class rebalance
and geometric regularization. <em>KDD</em>, 211–221. (<a
href="https://doi.org/10.1145/3637528.3671707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In label-noise learning, accurately identifying the transition matrix is crucial for developing statistically consistent classifiers. This task is complicated by instance-dependent noise, which introduces identifiability challenges in the absence of stringent assumptions. Existing methods use neural networks to estimate the transition matrix by initially extracting confident clean instances. However, this extraction process is hindered by severe inter-class imbalance and a bias toward selecting unambiguous intra-class instances, leading to a distorted understanding of noise patterns. To tackle these challenges, our paper introduces a Class Rebalance and Geometric Regularization-based Framework (CRGR). CRGR employs a smoothed, noise-tolerant reweighting mechanism to equilibrate inter-class representation, thereby mitigating the risk of model overfitting to dominant classes. Additionally, recognizing that instances with similar characteristics often exhibit parallel noise patterns, we propose that the transition matrix should mirror the similarity of the feature space. This insight promotes the inclusion of ambiguous instances in training, serving as a form of geometric regularization. Such a strategy enhances the model&#39;s ability to navigate diverse noise patterns and strengthens its generalization capabilities. By addressing both inter-class and intra-class biases, CRGR offers a more balanced and robust classification model. Extensive experiments on both synthetic and real-world datasets demonstrate CRGR&#39;s superiority over existing state-of-the-art methods, significantly boosting classification accuracy and showcasing its effectiveness in handling instance-dependent noise.},
  archive   = {C_KDD},
  author    = {Cao, Shuzhi and Ruan, Jianfei and Dong, Bo and Shi, Bin},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671707},
  pages     = {211–221},
  title     = {Tackling instance-dependent label noise with class rebalance and geometric regularization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FoRAG: Factuality-optimized retrieval augmented generation
for web-enhanced long-form question answering. <em>KDD</em>, 199–210.
(<a href="https://doi.org/10.1145/3637528.3672065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Retrieval Augmented Generation (RAG) has become prevalent in question-answering (QA) tasks due to its ability of utilizing search engine to enhance the quality of long-form question-answering (LFQA). Despite the emergence of various open source methods and web-enhanced commercial systems such as Bing Chat, two critical problems remain unsolved, i.e., the lack of factuality and clear logic in the generated long-form answers. In this paper, we remedy these issues via a systematic study on answer generation in web-enhanced LFQA. Specifically, we first propose a novel outline-enhanced generator to achieve clear logic in the generation of multifaceted answers and construct two datasets accordingly. Then we propose a factuality optimization method based on a carefully designed doubly fine-grained RLHF framework, which contains automatic evaluation and reward modeling in different levels of granularity. Our generic framework comprises conventional fine-grained RLHF methods as special cases. Extensive experiments verify the superiority of our proposed Factuality-optimized RAG (FoRAG) method on both English and Chinese benchmarks. In particular, when applying our method to Llama2-7B-chat, the derived model FoRAG-L-7B outperforms WebGPT-175B in terms of three commonly used metrics (i.e., coherence, helpfulness, and factuality), while the number of parameters is much smaller (only 1/24 of that of WebGPT-175B). Our datasets and models are made publicly available for better reproducibility.https://huggingface.co/forag \l{}abelfootnote_dataset_url},
  archive   = {C_KDD},
  author    = {Cai, Tianchi and Tan, Zhiwen and Song, Xierui and Sun, Tao and Jiang, Jiyan and Xu, Yunqi and Zhang, Yinger and Gu, Jinjie},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672065},
  pages     = {199–210},
  title     = {FoRAG: Factuality-optimized retrieval augmented generation for web-enhanced long-form question answering},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Popularity-aware alignment and contrast for mitigating
popularity bias. <em>KDD</em>, 187–198. (<a
href="https://doi.org/10.1145/3637528.3671824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collaborative Filtering~(CF) typically suffers from the significant challenge of popularity bias due to the uneven distribution of items in real-world datasets. This bias leads to a significant accuracy gap between popular and unpopular items. It not only hinders accurate user preference understanding but also exacerbates the Matthew effect in recommendation systems. To alleviate popularity bias, existing efforts focus on emphasizing unpopular items or separating the correlation between item representations and their popularity. Despite the effectiveness, existing works still face two persistent challenges: (1) how to extract common supervision signals from popular items to improve the unpopular item representations, and (2) how to alleviate the representation separation caused by popularity bias. In this work, we conduct an empirical analysis of popularity bias and propose &amp;lt;u&amp;gt;P&amp;lt;/u&amp;gt;opularity-&amp;lt;u&amp;gt;A&amp;lt;/u&amp;gt;ware &amp;lt;u&amp;gt;A&amp;lt;/u&amp;gt;lignment and &amp;lt;u&amp;gt;C&amp;lt;/u&amp;gt;ontrast (PAAC) to address two challenges. Specifically, we use the common supervisory signals modeled in popular item representations and propose a novel popularity-aware supervised alignment module to learn unpopular item representations. Additionally, we suggest re-weighting the contrastive learning loss to mitigate the representation separation from a popularity-centric perspective. Finally, we validate the effectiveness and rationale of PAAC in mitigating popularity bias through extensive experiments on three real-world datasets. Our code is available at https://github.com/miaomiao-cai2/KDD2024-PAAC.},
  archive   = {C_KDD},
  author    = {Cai, Miaomiao and Chen, Lei and Wang, Yifan and Bai, Haoyue and Sun, Peijie and Wu, Le and Zhang, Min and Wang, Meng},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671824},
  pages     = {187–198},
  title     = {Popularity-aware alignment and contrast for mitigating popularity bias},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Where have you been? A study of privacy risk for
point-of-interest recommendation. <em>KDD</em>, 175–186. (<a
href="https://doi.org/10.1145/3637528.3671758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As location-based services (LBS) have grown in popularity, more human mobility data has been collected. The collected data can be used to build machine learning (ML) models for LBS to enhance their performance and improve overall experience for users. However, the convenience comes with the risk of privacy leakage since this type of data might contain sensitive information related to user identities, such as home/work locations. Prior work focuses on protecting mobility data privacy during transmission or prior to release, lacking the privacy risk evaluation of mobility data-based ML models. To better understand and quantify the privacy leakage in mobility data-based ML models, we design a privacy attack suite containing data extraction and membership inference attacks tailored for point-of-interest (POI) recommendation models, one of the most widely used mobility data-based ML models. These attacks in our attack suite assume different adversary knowledge and aim to extract different types of sensitive information from mobility data, providing a holistic privacy risk assessment for POI recommendation models. Our experimental evaluation using two real-world mobility datasets demonstrates that current POI recommendation models are vulnerable to our attacks. We also present unique findings to understand what types of mobility data are more susceptible to privacy attacks. Finally, we evaluate defenses against these attacks and highlight future directions and challenges.},
  archive   = {C_KDD},
  author    = {Cai, Kunlin and Zhang, Jinghuai and Hong, Zhiqing and Shand, William and Wang, Guang and Zhang, Desheng and Chi, Jianfeng and Tian, Yuan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671758},
  pages     = {175–186},
  title     = {Where have you been? a study of privacy risk for point-of-interest recommendation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Making temporal betweenness computation faster and restless.
<em>KDD</em>, 163–174. (<a
href="https://doi.org/10.1145/3637528.3671825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Buss et al [KDD 2020] recently proved that the problem of computing the betweenness of all nodes of a temporal graph is computationally hard in the case of foremost and fastest paths, while it is solvable in time O(n3T2) in the case of shortest and shortest foremost paths, where n is the number of nodes and T is the number of distinct time steps. A new algorithm for temporal betweenness computation is introduced in this paper. In the case of shortest and shortest foremost paths, it requires O(n + M) space and runs in time O(nM)=O(n3T), where M is the number of temporal edges, thus significantly improving the algorithm of Buss et al in terms of time complexity (note that T is usually large). Experimental evidence is provided that our algorithm performs between twice and almost 250 times better than the algorithm of Buss et al. Moreover, we were able to compute the exact temporal betweenness values of several large temporal graphs with over a million of temporal edges. For such size, only approximate computation was possible by using the algorithm of Santoro and Sarpe [WWW 2022]. Maybe more importantly, our algorithm extends to the case of restless walks (that is, walks with waiting constraints in each node), thus providing a polynomial-time algorithm (with complexity O(nM)) for computing the temporal betweenness in the case of several different optimality criteria. Such restless computation was known only for the shortest criterion (Rymar et al [JGAA 2023]), with complexity O(n2MT2). We performed an extensive experimental validation by comparing different waiting constraints and different optimisation criteria. Moreover, as a case study, we investigate six public transit networks including Berlin, Rome, and Paris. Overall we find a general consistency between the different variants of betweenness centrality. However, we do measure a sensible influence of waiting constraints, and note some cases of low correlation for certain pairs of criteria in some networks.},
  archive   = {C_KDD},
  author    = {Brunelli, Filippo and Crescenzi, Pierluigi and Viennot, Laurent},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671825},
  pages     = {163–174},
  title     = {Making temporal betweenness computation faster and restless},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning the covariance of treatment effects across many
weak experiments. <em>KDD</em>, 153–162. (<a
href="https://doi.org/10.1145/3637528.3672034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When primary objectives are insensitive or delayed, experimenters may instead focus on proxy metrics derived from secondary outcomes. For example, technology companies often infer the long-term impacts of product interventions from their effects on short-term user engagement signals. We consider the meta-analysis of many historical experiments to learn the covariance of treatment effects on these outcomes, which can support the construction of such proxies. Even when experiments are plentiful, if treatment effects are weak, the covariance of estimated treatment effects across experiments can be highly biased. We overcome this with techniques inspired by weak instrumental variable analysis. We show that Limited Information Maximum Likelihood (LIML) learns a parameter equivalent to fitting total least squares to a transformation of the scatterplot of treatment effects, and that Jackknife Instrumental Variables Estimation (JIVE) learns another parameter computable from the average of Jackknifed covariance matrices across experiments. We also present a total covariance estimator for the latter estimand under homoskedasticity, which is equivalent to a k-class estimator. We show how these parameters can be used to construct unbiased proxy metrics under various structural models. Lastly, we discuss the real-world application of our methods at Netflix.},
  archive   = {C_KDD},
  author    = {Bibaut, Aur\&#39;{e}lien and Chou, Winston and Ejdemyr, Simon and Kallus, Nathan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672034},
  pages     = {153–162},
  title     = {Learning the covariance of treatment effects across many weak experiments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FaultInsight: Interpreting hyperscale data center host
faults. <em>KDD</em>, 141–152. (<a
href="https://doi.org/10.1145/3637528.3672051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Operating and maintaining hyperscale data centers involving millions of service hosts has been an extremely intricate task to tackle for top Internet companies. Incessant system failures cost operators countless hours of browsing through performance metrics to diagnose the underlying root cause to prevent the recurrence. Although many state-of-the-art (SOTA) methods have used time-series causal discovery to construct causal relationships among anomalous metrics, they only focus on homogeneous service-level performance metrics and fail to yield useful insights on heterogeneous host-level metrics. To address the challenge, this study presents FaultInsight, a highly interpretable deep causal host fault diagnosing framework that offers diagnostic insights from various perspectives to reduce human effort in troubleshooting. We evaluate FaultInsight using dozens of incidents collected from our production environment. FaultInsight provides markedly better root cause identification accuracy than SOTA baselines in our incident dataset. It also shows outstanding advantages in terms of deployability in real production systems. Our engineers are deeply impressed by FaultInsight&#39;s ability to interpret incidents from multiple perspectives, helping them quickly understand the mechanism behind the faults.},
  archive   = {C_KDD},
  author    = {Bi, Tingzhu and Yang, Zhang and Pan, Yicheng and Zhang, Yu and Ma, Meng and Jiang, Xinrui and Han, Linlin and Wang, Feng and Liu, Xian and Wang, Ping},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672051},
  pages     = {141–152},
  title     = {FaultInsight: Interpreting hyperscale data center host faults},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evading community detection via counterfactual neighborhood
search. <em>KDD</em>, 131–140. (<a
href="https://doi.org/10.1145/3637528.3671896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Community detection techniques are useful for social media platforms to discover tightly connected groups of users who share common interests. However, this functionality often comes at the expense of potentially exposing individuals to privacy breaches by inadvertently revealing their tastes or preferences. Therefore, some users may wish to preserve their anonymity and opt out of community detection for various reasons, such as affiliation with political or religious organizations, without leaving the platform. In this study, we address the challenge of community membership hiding, which involves strategically altering the structural properties of a network graph to prevent one or more nodes from being identified by a given community detection algorithm. We tackle this problem by formulating it as a constrained counterfactual graph objective, and we solve it via deep reinforcement learning. Extensive experiments demonstrate that our method outperforms existing baselines, striking the best balance between accuracy and cost.},
  archive   = {C_KDD},
  author    = {Bernini, Andrea and Silvestri, Fabrizio and Tolomei, Gabriele},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671896},
  pages     = {131–140},
  title     = {Evading community detection via counterfactual neighborhood search},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph mamba: Towards learning on graphs with state space
models. <em>KDD</em>, 119–130. (<a
href="https://doi.org/10.1145/3637528.3672044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph Neural Networks (GNNs) have shown promising potential in graph representation learning. The majority of GNNs define a local message-passing mechanism, propagating information over the graph by stacking multiple layers. These methods, however, are known to suffer from two major limitations: over-squashing and poor capturing of long-range dependencies. Recently, Graph Transformers (GTs) emerged as a powerful alternative to Message-Passing Neural Networks (MPNNs). GTs, however, have quadratic computational cost, lack inductive biases on graph structures, and rely on complex Positional Encodings (PE). In this paper, we show that while Transformers, complex message-passing, and PE are sufficient for good performance in practice, neither is necessary. Motivated by the recent success of State Space Models (SSMs), we present Graph Mamba Networks (GMNs), a framework for a new class of GNNs based on selective SSMs. We discuss the new challenges when adapting SSMs to graph-structured data, and present four required steps to design GMNs, where we choose (1) Neighborhood Tokenization, (2) Token Ordering, (3) Architecture of SSM Encoder, and (4) Local Encoding. We provide theoretical justification for the power of GMNs, and experimentally show that GMNs attain an outstanding performance in various benchmark datasets. The code is available in this link.},
  archive   = {C_KDD},
  author    = {Behrouz, Ali and Hashemi, Farnoosh},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672044},
  pages     = {119–130},
  title     = {Graph mamba: Towards learning on graphs with state space models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved active covering via density-based space
transformation. <em>KDD</em>, 107–118. (<a
href="https://doi.org/10.1145/3637528.3671794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we study active covering, a variant of the active-learning problem that involves labeling (or identifying) all of the examples with a positive label. We propose a couple of algorithms, namely Density-Adjusted Non-Adaptive (DANA) learner and Density-Adjusted Adaptive (DAA) learner, that query the labels according to a distance function that is adjusted by the density function. Under mild assumptions, we prove that our algorithms discover all of the positive labels while querying only a sublinear number of examples from the support of negative labels for constant-dimensional spaces (see Theorems 5 and 6). Our experiments show that our champion algorithm DAA consistently improves over the prior work on some standard benchmark datasets, including those used by the previous work, as well as a couple of data sets on credit card fraud. For instance, when measuring performance using AUC, our algorithm is the best in 25 out of 27 experiments over 7 different datasets.},
  archive   = {C_KDD},
  author    = {Bateni, MohammadHossein and Esfandiari, Hossein and HosseinGhorban, Samira and Montaseri, Alipasha},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671794},
  pages     = {107–118},
  title     = {Improved active covering via density-based space transformation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Meta clustering of neural bandits. <em>KDD</em>, 95–106. (<a
href="https://doi.org/10.1145/3637528.3671691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The contextual bandit has been identified as a powerful framework to formulate the recommendation process as a sequential decision-making process, where each item is regarded as an arm and the objective is to minimize the regret of T rounds. In this paper, we study a new problem, Clustering of Neural Bandits, by extending previous work to the arbitrary reward function, to strike a balance between user heterogeneity and user correlations in the recommender system. To solve this problem, we propose a novel algorithm called M-CNB, which utilizes a meta-learner to represent and rapidly adapt to dynamic clusters, along with an informative Upper Confidence Bound (UCB)-based exploration strategy. We provide an instance-dependent performance guarantee for the proposed algorithm that withstands the adversarial context, and we further prove the guarantee is at least as good as state-of-the-art (SOTA) approaches under the same assumptions. In extensive experiments conducted in both recommendation and online classification scenarios, M-CNB outperforms SOTA baselines. This shows the effectiveness of the proposed approach in improving online recommendation and online classification performance.},
  archive   = {C_KDD},
  author    = {Ban, Yikun and Qi, Yunzhe and Wei, Tianxin and Liu, Lihui and He, Jingrui},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671691},
  pages     = {95–106},
  title     = {Meta clustering of neural bandits},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards robust information extraction via binomial
distribution guided counterpart sequence. <em>KDD</em>, 83–94. (<a
href="https://doi.org/10.1145/3637528.3672067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Information extraction (IE) aims to extract meaningful structured tuples from unstructured text. Existing studies usually utilize a pre-trained generative language model that rephrases the original sentence into a target sequence, which can be easily decoded as tuples. However, traditional evaluation metrics treat a slight error within the tuple as an entire prediction failure, which is unable to perceive the correctness extent of a tuple. For this reason, we first propose a novel IE evaluation metric called Matching Score to evaluate the correctness of the predicted tuples in more detail. Moreover, previous works have ignored the effects of semantic uncertainty when focusing on the generation of the target sequence. We argue that leveraging the built-in semantic uncertainty of language models is beneficial for improving its robustness. In this work, we propose &amp;lt;u&amp;gt;B&amp;lt;/u&amp;gt;inomial distribution guided &amp;lt;u&amp;gt;c&amp;lt;/u&amp;gt;ounterpart &amp;lt;u&amp;gt;s&amp;lt;/u&amp;gt;equence (BCS) method, which is a model-agnostic approach. Specifically, we propose to quantify the built-in semantic uncertainty of the language model by bridging all local uncertainties with the whole sequence. Subsequently, with the semantic uncertainty and Matching Score, we formulate a unique binomial distribution for each local decoding step. By sampling from this distribution, a counterpart sequence is obtained, which can be regarded as a semantic complement to the target sequence. Finally, we employ the Kullback-Leibler divergence to align the semantics of the target sequence and its counterpart. Extensive experiments on 14 public datasets over 5 information extraction tasks demonstrate the effectiveness of our approach on various methods. Our code and dataset are available at https://github.com/byinhao/BCS.},
  archive   = {C_KDD},
  author    = {Bai, Yinhao and Zhao, Yuhua and Han, Zhixin and Gao, Hang and Xue, Chao and Hu, Mengting},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672067},
  pages     = {83–94},
  title     = {Towards robust information extraction via binomial distribution guided counterpart sequence},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Understanding inter-session intentions via complex logical
reasoning. <em>KDD</em>, 71–82. (<a
href="https://doi.org/10.1145/3637528.3671808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Understanding user intentions is essential for improving product recommendations, navigation suggestions, and query reformulations. However, user intentions can be intricate, involving multiple sessions and attribute requirements connected by logical operators such as And, Or, and Not. For instance, a user may search for Nike or Adidas running shoes across various sessions, with a preference for purple. In another example, a user may have purchased a mattress in a previous session and is now looking for a matching bed frame without intending to buy another mattress. Existing research on session understanding has not adequately addressed making product or attribute recommendations for such complex intentions. In this paper, we present the task of logical session complex query answering (LS-CQA), where sessions are treated as hyperedges of items, and we frame the problem of complex intention understanding as an LS-CQA task on an aggregated hypergraph of sessions, items, and attributes. This is a unique complex query answering task with sessions as ordered hyperedges. We also introduce a new model, the Logical Session Graph Transformer (LSGT), which captures interactions among items across different sessions and their logical connections using a transformer structure. We analyze the expressiveness of LSGT and prove the permutation invariance of the inputs for the logical operators. By evaluating LSGT on three datasets, we demonstrate that it achieves state-of-the-art results.},
  archive   = {C_KDD},
  author    = {Bai, Jiaxin and Luo, Chen and Li, Zheng and Yin, Qingyu and Song, Yangqiu},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671808},
  pages     = {71–82},
  title     = {Understanding inter-session intentions via complex logical reasoning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semi-supervised learning for time series collected at a low
sampling rate. <em>KDD</em>, 59–70. (<a
href="https://doi.org/10.1145/3637528.3672033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although time-series classification has many applications in healthcare and manufacturing, the high cost of data collection and labeling hinders its widespread use. To reduce data collection and labeling costs while maintaining high classification accuracy, we propose a novel problem setting, called semi-supervised learning with low-sampling-rate time series, in which the majority of time series are collected at a low sampling rate and are unlabeled whereas the minority of time series are collected at a high sampling rate and are labeled. For this novel problem scenario, we develop the SemiTSR framework equipped with the super-resolution module and the semi-supervised learning module. Here, low-sampling-rate time series are upsampled precisely, taking periodicity and trend at each timestamp into account, and both labeled and unlabeled high-sampling-rate time series are utilized for training. In particular, consistency regularization between artificially downsampled time series derived from an original high-sampling-rate time series is effective at overcoming limited sampling rates. We demonstrate that SemiTSR significantly outperforms conventional semi-supervised learning techniques by assuring high classification accuracy with low-sampling-rate time series.},
  archive   = {C_KDD},
  author    = {Bae, Minyoung and Shin, Yooju and Nam, Youngeun and Lee, Young Seop and Lee, Jae-Gil},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672033},
  pages     = {59–70},
  title     = {Semi-supervised learning for time series collected at a low sampling rate},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A learned generalized geodesic distance function-based
approach for node feature augmentation on graphs. <em>KDD</em>, 49–58.
(<a href="https://doi.org/10.1145/3637528.3671858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Geodesic distances on manifolds have numerous applications in image processing, computer graphics and computer vision. In this work, we introduce an approach called &#39;LGGD&#39; (Learned Generalized Geodesic Distances). This method involves generating node features by learning a generalized geodesic distance function through a training pipeline that incorporates training data, graph topology and the node content features. The strength of this method lies in the proven robustness of the generalized geodesic distances to noise and outliers. Our contributions encompass improved performance in node classification tasks, competitive results with state-of-the-art methods on real-world graph datasets, the demonstration of the learnability of parameters within the generalized geodesic equation on graph, and dynamic inclusion of new labels.},
  archive   = {C_KDD},
  author    = {Azad, Amitoz and Fang, Yuan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671858},
  pages     = {49–58},
  title     = {A learned generalized geodesic distance function-based approach for node feature augmentation on graphs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statistical models of top-k partial orders. <em>KDD</em>,
39–48. (<a href="https://doi.org/10.1145/3637528.3672014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many contexts involving ranked preferences, agents submit partial orders over available alternatives. Statistical models often treat these as marginal in the space of total orders, but this approach overlooks information contained in the list length itself. In this work, we introduce and taxonomize approaches for jointly modeling distributions over top-k partial orders and list lengths k, considering two classes of approaches: composite models that view a partial order as a truncation of a total order, and augmented ranking models that model the construction of the list as a sequence of choice decisions, including the decision to stop. For composite models, we consider three dependency structures for joint modeling of order and truncation length. For augmented ranking models, we consider different assumptions on how the stop-token choice is modeled. Using data consisting of partial rankings from San Francisco school choice and San Francisco ranked choice elections, we evaluate how well the models predict observed data and generate realistic synthetic datasets. We find that composite models, explicitly modeling length as a categorical variable, produce synthetic datasets with accurate length distributions, and an augmented model with position-dependent item utilities jointly models length and preferences in the training data best, as measured by negative log loss. Methods from this work have significant implications on the simulation and evaluation of real-world social systems that solicit ranked preferences.},
  archive   = {C_KDD},
  author    = {Awadelkarim, Amel and Ugander, Johan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672014},
  pages     = {39–48},
  title     = {Statistical models of top-k partial orders},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Resilient k-clustering. <em>KDD</em>, 29–38. (<a
href="https://doi.org/10.1145/3637528.3671888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of resilient clustering in the metric setting where one is interested in designing algorithms that return high quality solutions that preserve the clustering structure under perturbations of the input points. Our first contribution is to introduce a formal notion of algorithmic resiliency for clustering problems that, roughly speaking, requires an algorithm to have similar outputs on close inputs. Then, we notice that classic algorithms have weak resiliency guarantees and develop new algorithms for fundamental clustering problems such as k-center, k-median, and k-means. Finally, we complement our results with an experimental analysis showing the effectiveness of our techniques on real-world instances.},
  archive   = {C_KDD},
  author    = {Ahmadian, Sara and Bateni, MohammadHossein and Esfandiari, Hossein and Lattanzi, Silvio and Monemizadeh, Morteza and Norouzi-Fard, Ashkan},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671888},
  pages     = {29–38},
  title     = {Resilient k-clustering},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Approximating memorization using loss surface geometry for
dataset pruning and summarization. <em>KDD</em>, 17–28. (<a
href="https://doi.org/10.1145/3637528.3671985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The sustainable training of modern neural network models represents an open challenge. Several existing methods approach this issue by identifying a subset of relevant data samples from the full training data to be used in model optimization with the goal of matching the performance of the full data training with that of the subset data training. Our work explores using memorization scores to find representative and atypical samples. We demonstrate that memorization-aware dataset summarization improves the subset construction performance. However, computing memorization scores is notably resource-intensive. To this end, we propose a novel method that leverages the discrepancy between sharpness-aware minimization and stochastic gradient descent to capture data points atypicality. We evaluate our metric over several efficient approximation functions for memorization scores - namely proxies -, empirically showing superior correlation and effectiveness. We explore the causes behind our approximation quality, highlighting how typical data points trigger a flatter loss landscape compared to atypical ones. Extensive experiments confirm the effectiveness of our proxy for dataset pruning and summarization tasks, surpassing state-of-the-art approaches both on canonical setups - where atypical data points benefit performance - and few-shot learning scenarios-where atypical data points can be detrimental.},
  archive   = {C_KDD},
  author    = {Agiollo, Andrea and Kim, Young In and Khanna, Rajiv},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671985},
  pages     = {17–28},
  title     = {Approximating memorization using loss surface geometry for dataset pruning and summarization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GEO: Generative engine optimization. <em>KDD</em>, 5–16. (<a
href="https://doi.org/10.1145/3637528.3671900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The advent of large language models (LLMs) has ushered in a new paradigm of search engines that use generative models to gather and summarize information to answer user queries. This emerging technology, which we formalize under the unified framework of generative engines (GEs), can generate accurate and personalized responses, rapidly replacing traditional search engines like Google and Bing. Generative Engines typically satisfy queries by synthesizing information from multiple sources and summarizing them using LLMs. While this shift significantly improvesuser utility and generative search engine traffic, it poses a huge challenge for the third stakeholder -- website and content creators. Given the black-box and fast-moving nature of generative engines, content creators have little to no control over when and how their content is displayed. With generative engines here to stay, we must ensure the creator economy is not disadvantaged. To address this, we introduce Generative Engine Optimization (GEO), the first novel paradigm to aid content creators in improving their content visibility in generative engine responses through a flexible black-box optimization framework for optimizing and defining visibility metrics. We facilitate systematic evaluation by introducing GEO-bench, a large-scale benchmark of diverse user queries across multiple domains, along with relevant web sources to answer these queries. Through rigorous evaluation, we demonstrate that GEO can boost visibility by up to 40\% in generative engine responses. Moreover, we show the efficacy of these strategies varies across domains, underscoring the need for domain-specific optimization methods. Our work opens a new frontier in information discovery systems, with profound implications for both developers of generative engines and content creators.},
  archive   = {C_KDD},
  author    = {Aggarwal, Pranjal and Murahari, Vishvak and Rajpurohit, Tanmay and Kalyan, Ashwin and Narasimhan, Karthik and Deshpande, Ameet},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3671900},
  pages     = {5–16},
  title     = {GEO: Generative engine optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Empower an end-to-end scalable and interpretable data
science ecosystem using statistics, AI and domain science. <em>KDD</em>,
3–4. (<a href="https://doi.org/10.1145/3637528.3672194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The data science ecosystem encompasses data fairness, statistical, ML and AI methods and tools, interpretable data analysis and results, and trustworthy decision-making. Rapid advancements in AI have revolutionized data utilization and enabled machines to learn from data more effectively. Statistics, as the science of learning from data while accounting for uncertainty, plays a pivotal role in addressing complex real-world problems and facilitating trustworthy decision-making. In this talk, I will discuss the challenges and opportunities involved in building an end-to-end scalable and interpretable data science ecosystem using the analysis of whole genome sequencing studies and biobanks that integrates statistics, ML/AI, and genomic and health science as an example. Biobanks collect whole genome data, electronic health records and epidemiological data. I will illustrate key points using the analysis of multi-ancestry whole genome sequencing studies and biobanks by discussing a few scalable and interpretable statistical and ML/AI methods, tools and data science resources.Specifically, first, data fairness and diversity is a critical pillar of a trustworthy data science ecosystem. About 85+\% of genome wide association study samples in the last 15 years are European, resulting in disparity in genetic research. I will discuss the community effort on improving diversity in genetic studies in the last 10 years. I will present trans-ancestry polygenic risk scores (PRS) using millions of common genetic variants across the genome by leveraging large GWAS sample sizes of European and smaller sample sizes of under-represented populations for predicting disease risk using transfer learning and genetic association summary statistics. The performance of deep learning methods for PRS will also be discussed. Second, scalability in cloud platforms is critical for large scale affordable analysis for multi-ancestry biobanks and whole genome studies. I will discuss improving scalability in cloud-computing using interpretable sparsity via FastSparseGRM.To build an interpretable and powerful end-to-end ecosystem of rare variant analysis of large scale whole genome sequencing studies and biobanks, I will first introduce FAVOR, a multi-faceted variant functional annotation database and portal of all possible 9 billions of variants across the whole genome. I will discuss FAVOR-GPT, a LLM interface of the FAVOR functional annotation database to improve user experience for navigating FAVOR and performing variant functional annotation query and variant functional summary statistics calculations. I will also discuss FAVORannotator which can be used to functionally annotate any whole genome sequencing studies. I will also discuss STAAR and STAAR and STAARpipeline, the WGS rare variant analysis pipeline that boosts the power of WGS rare variant association analysis by dynamically incorporating multi-faceted variant functional annotations. Extension of incorporating single-cell data in WGS analysis will also be discussed. I will also discuss ensemble methods that improve the power of rare variant association tests.Cloud-deployment of these resources and tools in several ecosystems will be presented, such as RAP for the UK biobank, AnVIL for the NHGRI Genome Sequencing Program and All of Us, and BioData Catalyst for the NHLBI Trans-omics Precision Medine Program (TOPMed). This talk aims to ignite proactive and thought-provoking discussions, foster collaboration, and cultivate open-minded approaches to advance scientific discovery.},
  archive   = {C_KDD},
  author    = {Lin, Xihong},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672194},
  pages     = {3–4},
  title     = {Empower an end-to-end scalable and interpretable data science ecosystem using statistics, AI and domain science},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AI for nature: From science to impact. <em>KDD</em>, 2. (<a
href="https://doi.org/10.1145/3637528.3672192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Computation has fundamentally changed the way we study nature. New data collection technologies, such as GPS, high-definition cameras, autonomous vehicles under water, on the ground, and in the air, genotyping, acoustic sensors, and crowdsourcing, are generating data about life on the planet that are orders of magnitude richer than any previously collected. Yet, our ability to extract insight from this data lags substantially behind our ability to collect it.The need for understanding is more urgent than ever and the challenges are great. We are in the middle of the 6th extinction, losing the planet&#39;s biodiversity at an unprecedented rate and scale. In many cases, we do not even have the basic numbers of what species we are losing, which impacts our ability to understand biodiversity loss drivers, predict the impact on ecosystems, and implement policy. From the basic science perspective, the new data opens the possibility of understanding function of traits of organisms and ecosystems, which is critical for biologists to predict effects of environmental change or genetic manipulation and to understand the significance of patterns in the four-billion-year evolutionary history of life.The key to unlocking the potential of this data are machine learning (ML) and artificial intelligence (AI) methods, which are already beginning to have significant impacts on research across ecology and conservation. AI can turn data into high resolution information source about living organisms, enabling scientific inquiry, conservation, and policy decisions.The talk introduces a new field of science, imageomics, and presents a vision and examples of AI as a trustworthy partner both in science and biodiversity conservation, discussing opportunities and challenges.},
  archive   = {C_KDD},
  author    = {Berger-Wolf, Tanya Y.},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672192},
  pages     = {2},
  title     = {AI for nature: From science to impact},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From word-prediction to complex skills: Compositional
thinking and metacognition in LLMs. <em>KDD</em>, 1. (<a
href="https://doi.org/10.1145/3637528.3672193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The talk will present evidence that today&#39;s large language models (LLMs) display somewhat deeper &quot;understanding&#39;&#39; than one would naively expect. 1. When asked to solve a task by combining a set of k simpler skills (&quot;test of compositional capability&quot;), they are able to do so despite not having seen the same combination of skills during their training.2. They demonstrate ability to reason about of their own learning processes, which is analogous to &quot;metacognitive knowledge&quot;[Flavel&#39;76] in humans. For instance, given examples of an evaluation task, they can produce a catalog of suitably named skills that are relevant for solving each example of that task. Furthermore, this catalog of skills is meaningful, in the sense that incorporating it into training pipelines improves performance (including of other unrelated LLMs) on that task. We discuss mechanisms by which such complex understanding could arise (including a theory by [Arora,Goyal&#39;23] that tries to explain (a)) and also give examples of how to leverage LLM meta knowledge to improve LLM training pipelines as well as evaluations.},
  archive   = {C_KDD},
  author    = {Arora, Sanjeev},
  booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3637528.3672193},
  pages     = {1},
  title     = {From word-prediction to complex skills: Compositional thinking and metacognition in LLMs},
  year      = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
