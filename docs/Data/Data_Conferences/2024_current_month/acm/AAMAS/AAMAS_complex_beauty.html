<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>AAMAS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="aamas---460">AAMAS - 460</h2>
<ul>
<li><details>
<summary>
(2024). A survey of multi-agent deep reinforcement learning with
communication. <em>AAMAS</em>, 2845–2847. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Communication is an effective mechanism for coordinating the behaviors of multiple agents, broadening their views of the environment, and to support their collaborations. In the field of multi-agent deep reinforcement learning (MADRL), agents can improve the overall learning performance and achieve their objectives through communication. Agents can communicate various types of messages, either to all agents or to specific agent groups, or conditioned on specific constraints. With the growing body of research work in MADRL with communication (Comm-MADRL), there is a lack of a systematic and structural approach to distinguish and classify existing Comm-MADRL approaches. In this paper, we survey recent works in the Comm-MADRL field and consider various aspects of communication that can play a role in designing and developing multi-agent reinforcement learning systems. With these aspects in mind, we propose 9 dimensions along which Comm-MADRL approaches can be analyzed, developed, and compared. By projecting existing works into the multi-dimensional space, we discover interesting trends. We also propose some novel directions for designing future Comm-MADRL systems through exploring possible combinations of the dimensions.},
  archive   = {C_AAMAS},
  author    = {Zhu, Changxi and Dastani, Mehdi and Wang, Shihan},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2845–2847},
  title     = {A survey of multi-agent deep reinforcement learning with communication},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663308},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Extended abstract: Price of anarchy of traffic assignment
with exponential cost functions. <em>AAMAS</em>, 2842–2844. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper is an extended abstract version of &quot;Price of Anarchy of Traffic Assignment with Exponential Cost Functions[5]&quot;. We study a routing game where vehicles, selfish agents, independently choose routes to minimize travel delays from road congestion. We focus on exponential latency functions, unlike prior research using polynomial functions like BPR. We calculate a tight upper bound for the price of anarchy and compare it with the BPR function. Results indicate that the exponential function has a lower upper bound for traffic volumes below road capacity than the BPR function. Numerical analysis using real-world data shows that the exponential function closely approximates road latency with even tighter parameters, resulting in a relatively lower upper bound.},
  archive   = {C_AAMAS},
  author    = {Qiao, Jianglin and de Jonge, Dave and Zhang, Dongmo and Simoff, Simeon and Sierra, Carles and Du, Bo},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2842–2844},
  title     = {Extended abstract: Price of anarchy of traffic assignment with exponential cost functions},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663307},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Combining theory of mind and abductive reasoning in
agent-oriented programming. <em>AAMAS</em>, 2839–2841. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Theory of Mind capabilities, i.e. the capacity to adopt and reason from the perspective of others. By combining the Theory of Mind of TomAbd agents with abductive reasoning, agents can infer explanations for the behaviour of others, which they can incorporate into their own decision-making. We have implemented the TomAbd agent model and successfully tested its performance in the cooperative board game Hanabi.},
  archive   = {C_AAMAS},
  author    = {Montes, Nieves and Luck, Michael and Osman, Nardine and Rodrigues, Odinaldo and Sierra, Carles},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2839–2841},
  title     = {Combining theory of mind and abductive reasoning in agent-oriented programming},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663306},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward a normative approach for resilient multiagent
systems: A summary. <em>AAMAS</em>, 2836–2838. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We model a multiagent system (MAS) in socio-technical terms, combining a social layer consisting of norms with a technical layer consisting of actions that the agents execute. We express stakeholder needs to ensure that a MAS demonstrates resilience, allowing it to recover effectively from failures within a brief timeframe. This extended abstract presents a framework that computes probabilistic and temporal guarantees on whether the underlying requirements are met or, if failed, recovered. An important contribution of the framework is that it shows how the social and technical layers can be modeled jointly to enable the construction of resilient systems of autonomous agents. This paper facilitates specification refinement through methodological guidelines, emphasizing joint modeling of social and technical layers. We demonstrate our framework using a manufacturing scenario with competing public, industrial, and environmental requirements. This is an extended abstract of our JAAMAS paper available online[11].},
  archive   = {C_AAMAS},
  author    = {Mahala, Geeta and Kafali, Ozgur and Dam, Hoa Khanh and Ghose, Aditya and Singh, Munindar P.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2836–2838},
  title     = {Toward a normative approach for resilient multiagent systems: A summary},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663305},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Extended abstract of diffusion auction design with
transaction costs. <em>AAMAS</em>, 2833–2835. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study multi-unit diffusion auctions powered by intermediated markets, where all transactions are processed by intermediaries and incur certain costs. The classic Vickrey-Clarke-Groves (VCG) mechanism within the scenario can obtain the maximum social welfare, but it can lead to a deficit for the seller. To address the revenue issue, we develop two deficit reduction strategies and further propose a family of diffusion auctions, called Critical Neighborhood Auctions (CNA). The CNA not only maximizes the social welfare, but also achieves a (non-negative) revenue that is no less than the revenue given by the VCG mechanism with/without intermediaries. This is the first set of diffusion auctions with welfare and revenue advantages that can handle multiple items and transaction costs.},
  archive   = {C_AAMAS},
  author    = {Li, Bin and Hao, Dong and Zhao, Dengji},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2833–2835},
  title     = {Extended abstract of diffusion auction design with transaction costs},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663304},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A summary of online markov decision processes with
non-oblivious strategic adversary. <em>AAMAS</em>, 2830–2832. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study a novel setting in Online Markov Decision Processes (OMDPs) where the loss function is chosen by a non-oblivious strategic adversary who follows a no-external regret algorithm. In this setting, we first demonstrate that MDP-Expert, an existing algorithm that works well with oblivious adversaries can still apply and achieve a policy regret bound of O (√Tlog(L) + τ2√ T log(|A|)) where L is the size of adversary&#39;s pure strategy set and |A| denotes the size of agent&#39;s action space. Considering real-world games where the support size of a NE is small, we further propose a new algorithm: MDP-Online Oracle Expert (MDP-OOE), that achieves a policy regret bound of O (√Tlog(L)&amp;lt; + τ 2 √ Tk log(k)) where k depends only on the support size of the NE. MDP-OOE leverages the key benefit of Double Oracle in game theory and thus can solve games with prohibitively large action space. Finally, to better understand the learning dynamics of no-regret methods, under the same setting of no-external regret adversary in OMDPs, we introduce an algorithm that achieves last-round convergence result to a NE. To our best knowledge, this is first work leading to the last iteration result in OMDPs.},
  archive   = {C_AAMAS},
  author    = {Dinh, Le Cong and Mguni, David Henry and Tran-Thanh, Long and Wang, Jun and Yang, Yaodong},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2830–2832},
  title     = {A summary of online markov decision processes with non-oblivious strategic adversary},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663303},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A summary of the RGS⊕: An RDF graph synchronization system
for collaborative robotics. <em>AAMAS</em>, 2827–2829. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the context of collaborative robotics, distributed situation awareness is essential for supporting collective intelligence in teams of robots and human agents, where it can be used for both individual and collective decision support. This is particularly important in applications pertaining to emergency rescue and crisis management. During operational missions, data and knowledge is gathered incrementally and in different ways by heterogeneous robots and humans. This paper aims to describe an RDF Graph Synchronization System called RGS⊕. It is assumed that a dynamic set of agents provide or retrieve knowledge stored in their local RDF Graphs which are continuously synchronized between agents. The RGS⊕ System was designed to handle unreliable communication and does not depend on a static centralized infrastructure.},
  archive   = {C_AAMAS},
  author    = {Berger, Cyrille and Doherty, Patrick and Rudol, Piotr and Wzorek, Mariusz},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2827–2829},
  title     = {A summary of the RGS⊕: An RDF graph synchronization system for collaborative robotics},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663302},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generating and choosing organizations for multi-agent
systems. <em>AAMAS</em>, 2824–2826. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The design of organizations is a complex and laborious task. It is the subject of recent studies, which define models to automatically perform this task. However, existing models constrain the space of possible solutions by requiring a priori definitions of organizational roles and usually are not suitable for planning resource use. This paper presents GoOrg [1], a model that uses as input a set of goals and a set of available agents to generate different arrangements of organizational structures made up of synthesized organizational positions. The most distinguishing characteristics of GoOrg are the use of organizational positions instead of roles and that positions are automatically synthesized rather than required as a priori defined inputs. These features allow for the planning of organizational resources at design time and increase the chance of finding feasible solutions. This paper also introduces a model extension that illustrates how GoOrg can be extended to suit a specific domain.},
  archive   = {C_AAMAS},
  author    = {Amaral, Cleber J. and H\&quot;{u}bner, Jomi F. and Cranefield, Stephen},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2824–2826},
  title     = {Generating and choosing organizations for multi-agent systems},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663301},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pgeon applied to overcooked-AI to explain agents’ behaviour.
<em>AAMAS</em>, 2821–2823. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Policy Graphs (PGs) are a method for representing the behaviour of opaque agents by observing them in the environment and producing graphs where the state and action spaces are discretised into predicates. We present pgeon, a Python library that demonstrates the effectiveness of PGs in providing explanations for the behaviour of agents and we showcase it by applying it to a multi-agent cooperative environment: Overcooked-AI. This library illustrates how PGs can create transparent and explainable surrogate agents that closely mimic the behavior of the original agents. These features can help improving trust in environments where humans and AI systems collaborate by improving the explainability of all agents, even opaque or human.},
  archive   = {C_AAMAS},
  author    = {Tormos, Adrian and Gimenez-Abalos, Victor and V\&#39;{a}zquez-Salceda, Javier and Alvarez-Napagao, Sergio},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2821–2823},
  title     = {Pgeon applied to overcooked-AI to explain agents&#39; behaviour},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663299},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Engaging the elderly in exercise with agents: A gamified
stationary bike system for sarcopenia management. <em>AAMAS</em>,
2818–2820. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a portable, gamified exercise system with an embedded agent, specifically designed to aid the elderly in lower-body workouts using stationary bikes. The system integrates a custom-made Internet of Things (IoT) sensing unit, a gamified application, and an agent-embedded backend platform. By leveraging real-time feedback along with historical user data, the agent actively contributes to exercise safety and adherence by customizing the intensity of workouts and managing break periods. This novel approach aims to make cycling exercise for sarcopenia prevention and intervention more engaging and effective, promoting regular participation and potentially improving health outcomes.},
  archive   = {C_AAMAS},
  author    = {Qiu, Yang and Chen, Ping and Zhang, Huiguo and Huang, Bo and Wang, Di and Shen, Zhiqi},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2818–2820},
  title     = {Engaging the elderly in exercise with agents: A gamified stationary bike system for sarcopenia management},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663298},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SMT4SMTL: A tool for SMT-based satisfiability checking of
SMTL. <em>AAMAS</em>, 2815–2817. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present SMT4SMTL - the first tool for deciding the bounded satisfiability of Metric Temporal Logic (MTL) and the existential fragment of Strategic Metric Temporal Logic (SMTL), interpreted over timed multi-agent systems represented by networks of timed automata. The tool combines Satisfiability Modulo Theories (SMT) techniques and Parametric Bounded Model Checking algorithms.},
  archive   = {C_AAMAS},
  author    = {Niewiadomski, Artur and Nazarczuk, Maciej and Przychodzki, Mateusz and Kacprzak, Magdalena and Penczek, Wojciech and Zbrzezny, Andrzej},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2815–2817},
  title     = {SMT4SMTL: A tool for SMT-based satisfiability checking of SMTL},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663297},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). STV+KH: Towards practical verification of strategic ability
for knowledge and information flow. <em>AAMAS</em>, 2812–2814. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an expanded version of our tool STV for model checking of strategic abilities. The new version adds support for knowledge and uncertainty operators, thus enabling the verification of properties such as privacy, anonymity, and strategic information flow. All of that is available through a web interface, with no need to install or configure the software by the user.},
  archive   = {C_AAMAS},
  author    = {Kami\&#39;{n}ski, Mateusz and Kurpiewski, Damian and Jamroga, Wojciech},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2812–2814},
  title     = {STV+KH: Towards practical verification of strategic ability for knowledge and information flow},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663296},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conversational language models for human-in-the-loop
multi-robot coordination. <em>AAMAS</em>, 2809–2811. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the increasing prevalence and diversity of robots interacting in the real world, there is need for flexible, on-the-fly planning and cooperation. Large Language Models are starting to be explored in a multimodal setup for communication, coordination, and planning in robotics. Existing approaches generally use a single agent building a plan, or have multiple homogeneous agents coordinating for a simple task. We present a decentralised, dialogical approach in which a team of agents with different abilities plans solutions through peer-to-peer and human-robot discussion. We suggest that argument-style dialogues are an effective way to facilitate adaptive use of each agent&#39;s abilities within a cooperative team. Two robots discuss how to solve a cleaning problem set by a human, define roles, and agree on paths they each take. Each step can be interrupted by a human advisor and agents check their plans with the human. Agents then execute this plan in the real world, collecting rubbish from people in each room. Our implementation uses text at every step, maintaining transparency and effective human-multi-robot interaction.},
  archive   = {C_AAMAS},
  author    = {Hunt, William and Godfrey, Toby and Soorati, Mohammad D.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2809–2811},
  title     = {Conversational language models for human-in-the-loop multi-robot coordination},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663295},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Naphtha cracking center scheduling optimization using
multi-agent reinforcement learning. <em>AAMAS</em>, 2806–2808. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Naphtha Cracking Center (NCC) is central to petrochemical feedstock production through the intricate process. It consists of receipt stage for unloading naphtha, blending stage for mixing naphtha, and furnace stage for producing marketable products. It is crucial to make an optimal schedule for NCC for profitability and efficiency. Traditionally managed by human experts, challenges arise in predicting complex chemical reactions and navigating real-world complexities. To address these issues, this paper aims to develop autonomous NCC operation using multi-agent reinforcement learning, where each agent is responsible for each stage and collaborates to achieve common objectives, while adhering to real-world constraints. We developed an online web service to allow the staff in LG Chem Daesan NCC facility to obtain an NCC schedule in real-time, and the staff are now operating the facility based on schedules generated by the online web service.},
  archive   = {C_AAMAS},
  author    = {Hong, Sunghoon and Yoon, Deunsol and Jung, Whiyoung and Lee, Jinsang and Yoo, Hyundam and Ham, Jiwon and Jung, Suhyun and Moon, Chanwoo and Jung, Yeontae and Lee, Kanghoon and Lim, Woohyung and Jeon, Somin and Lee, Myounggu and Hong, Sohui and Lee, Jaesang and Jang, Hangyoul and Kwak, Changhyun and Park, Jeonghyeon and Kang, Changhoon and Kim, Jungki},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2806–2808},
  title     = {Naphtha cracking center scheduling optimization using multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663294},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A symbolic sequential equilibria solver for game theory
explorer. <em>AAMAS</em>, 2803–2805. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present the first implemented symbolic solver for sequential equilibria in general finite imperfect information games.},
  archive   = {C_AAMAS},
  author    = {Graf, Moritz and Engesser, Thorsten and Nebel, Bernhard},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2803–2805},
  title     = {A symbolic sequential equilibria solver for game theory explorer},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663293},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Imitation learning datasets: A toolkit for creating
datasets, training agents and benchmarking. <em>AAMAS</em>, 2800–2802.
(<a href="https://dl.acm.org/doi/10.5555/3635637.3663292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Imitation learning field requires expert data to train agents in a task. Most often, this learning approach suffers from the absence of available data, which results in techniques being tested on its dataset. Creating datasets is a cumbersome process requiring researchers to train expert agents from scratch, record their interactions and test each benchmark method with newly created data. Moreover, creating new datasets for each new technique results in a lack of consistency in the evaluation process since each dataset can drastically vary in state and action distribution. In response, this work aims to address these issues by creating Imitation Learning Datasets, a toolkit that allows for: (i) curated expert policies with multithreaded support for faster dataset creation; (ii) readily available datasets and techniques with precise measurements; and (iii) sharing implementations of common imitation learning techniques. Demonstration link: https://nathangavenski.github.io/#/il-datasets-video},
  archive   = {C_AAMAS},
  author    = {Gavenski, Nathan and Luck, Michael and Rodrigues, Odinaldo},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2800–2802},
  title     = {Imitation learning datasets: A toolkit for creating datasets, training agents and benchmarking},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663292},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). End to end camera only drone detection and tracking demo
within a multi-agent framework with a CNN-LSTM model for range
estimation. <em>AAMAS</em>, 2797–2799. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an end-to-end camera-only drone tracking approach in a multi-agent framework. We show implementation and simulation of such a system and test the tracking components utilizing a CNN-LSTM model for range estimation tested on real data. A video of the demo is available at this link (https://drive.google.com/file/d/1AlV89lgfi5nqwZCHTXC0pLm6BsZh_cBk/view?usp=drive_link).},
  archive   = {C_AAMAS},
  author    = {de Rochechouart, Maxence and Abu Zitar, Raed and El Fallah Seghrouchni, Amal and Barbaresco, Frederic},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2797–2799},
  title     = {End to end camera only drone detection and tracking demo within a multi-agent framework with a CNN-LSTM model for range estimation},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663291},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EVtonomy: A personalised route planner for electric
vehicles. <em>AAMAS</em>, 2794–2796. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the continuing growth of the electric vehicle (EV) market, planning long road trips should be a seamless and hassle-free experience for EV owners. Dedicated EV route planning apps have emerged recently as indispensable assistants providing essential mapping and data services. However, EV owners still face a number of challenges when planning their routes to prevent unnecessary delays or expenses. These challenges are not yet fully addressed with current EV planning apps. This paper introduces EVtonomy, an app that assigns an intelligent agent to each driver capable of planning personalised journeys. Specifically, the agent provides routes and charging stop recommendations aligned with the EV owner&#39;s individual preferences in terms of trip duration, including both driving time and the time spent charging the car, along with the total charging costs.},
  archive   = {C_AAMAS},
  author    = {Augustin, Alexandry and Shafipour, Elnaz and Stein, Sebastian},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2794–2796},
  title     = {EVtonomy: A personalised route planner for electric vehicles},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663290},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advancing sample efficiency and explainability in
multi-agent reinforcement learning. <em>AAMAS</em>, 2791–2793. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-Agent Reinforcement Learning (MARL) holds promise for complex real-world applications but faces challenges in sample efficiency and policy explainability. My dissertation aims to address these critical barriers, advancing MARL towards more practical and interpretable systems. To boost sample efficiency, it is crucial for agents to effectively learn from and generalize past experiences. We propose a meta-exploration technique to train meta-exploration policies that exploit the joint state-action space structure from meta-training tasks. This approach can be integrated with any off-policy MARL algorithm to improve learning efficiency. Complementing the efficiency gain, my research also focuses on augmenting the explainability of neural network policies&#39; decision-making processes using techniques such as decision-tree extraction from MARL networks. In this extended abstract, I will summarize my research so far and outline promising future directions to further the deployability of MARL in complex real-world environments.},
  archive   = {C_AAMAS},
  author    = {Zhang, Zhicheng},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2791–2793},
  title     = {Advancing sample efficiency and explainability in multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663288},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Allocating resources with imperfect information.
<em>AAMAS</em>, 2788–2790. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The distribution of resources is a critical issue that impacts all aspects of the Internet and society, with fairness playing a key role.Current standard algorithms for distribution typically measure fairness through methods based on envy or proportionality, requiring precise numerical values.However, there is a clear discrepancy between how these algorithms are intended to work in theory and their application in real-world situations. This is because users often do not have exact information about the resources and struggle to assign a numerical value to them.Our goal is to explore settings where agents do not take exact numeric values as input. In this framework, we do not assume that individuals can specify exact numerical values for resources. Instead, we assume that each agent has an ordinal preference for the items. That is, given two items, an agent can identify which is better, without assigning cardinal values to them. We consider new criteria for fairness in this setting, and discuss about their achievability in this article. Besides, we investigate the Probabilistic Serial mechanism where agents also only provides ordinal ranking over items, and particularly research on the incentive ratio of the mechanism.},
  archive   = {C_AAMAS},
  author    = {Xing, Shiji},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2788–2790},
  title     = {Allocating resources with imperfect information},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663287},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Autonomous skill acquisition for robots using graduated
learning. <em>AAMAS</em>, 2785–2787. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Skill acquisition is among the most remarkable aspects of human intelligence. It involves discovering purposeful behavioural modules, retaining them as skills, honing them through practice, and applying them in unforeseen circumstances [11]. Skill acquisition underlies our ability to choose to spend time and energy on the mastery of particular tasks and draw upon previous experience to solve more complex problems over time with less cognitive effort[10]. If endowed with continual skill acquisition, robots can autonomously improve their skills over time, where learning at one stage of development is a foundation for future learning [23]. It could unlock new possibilities for physical automation with general-purpose robots, just as general-purpose computer processors ushered in the information age [24, 33]. In this work, we propose a novel approach called Graduated Learning, where we ask a robot to acquire new manipulation and locomotion skills repeatedly, using time-delineated experiences of attempts at those skills (i.e., episodes) and some store of previously acquired knowledge (e.g., weights of a neural network). Our proposed approach chooses the order in which an agent learns these skills since the progressive manner in which they are developed plays a vital role in developing a final skill set.},
  archive   = {C_AAMAS},
  author    = {Vasan, Gautham},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2785–2787},
  title     = {Autonomous skill acquisition for robots using graduated learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663286},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian model-free deep reinforcement learning.
<em>AAMAS</em>, 2782–2784. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Exploration in reinforcement learning remains a difficult challenge. In order to drive exploration, ensembles with randomized prior functions have recently been popularized to quantify uncertainty in the value model. However these ensembles have no theoretical reason to resemble the actual Bayesian posterior, which is known to provide strong performance in theory under certain conditions. In this thesis work, we view training ensembles from the perspective of Sequential Monte Carlo, a Monte Carlo method that approximates a sequence of distributions with a set of particles, and propose an algorithm that exploits both the practical flexibility of ensembles and theory of the Bayesian paradigm. We incorporate this method into a standard DQN agent and experimentally show qualitatively good uncertainty quantification and improved exploration capabilities over a regular ensemble. In the future, we will investigate the impact of likelihood and prior choices in Bayesian model-free reinforcement learning methods.},
  archive   = {C_AAMAS},
  author    = {van der Vaart, Pascal R.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2782–2784},
  title     = {Bayesian model-free deep reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663285},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributive and temporal fairness in algorithmic collective
decision-making. <em>AAMAS</em>, 2779–2781. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {From dividing parliamentary seats after a national election, to scheduling conference activities for an international AI conference, or deciding how to split public budget for city-wide projects, numerous real-life scenarios necessitates a group of individuals collectively reaching a desirable outcome through a preference aggregation process. In recent years, algorithms have been deployed in many scenarios to aid humans in such collective decision-making processes, with the goal of achieving fair outcomes efficiently. My work looks at the design and analysis of algorithms for various collective decision-making settings, including (i) indivisible resource allocation in the presence of strategic agents with different entitlements, (ii) multiwinner elections with temporal considerations, and (iii) the division of time and money when agents have cardinal preferences.},
  archive   = {C_AAMAS},
  author    = {Teh, Nicholas},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2779–2781},
  title     = {Distributive and temporal fairness in algorithmic collective decision-making},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663284},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Formal and natural language assisted curriculum generation
for reinforcement learning agents. <em>AAMAS</em>, 2776–2778. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement Learning (RL) has proven successful in learning behaviors for artificial agents and robots when the transition dynamics of the environment are unknown. Despite this progress, many sequential decision making tasks are prohibitively expensive to learn. For my research, I intend to utilize and synthesize existing symbolic knowledge available to supplement the RL techniques for improved efficiency and faster learning progress. This symbolic information be in the form of formal language specifications (such as LTL) or in the form of natural language derived using Large Language Models (LLMs). I have developed various methods and frameworks that propose novel techniques in the curriculum learning domain to improve the learning efficiency of RL agents. I further want to implement these techniques on physical manipulator robot and show its efficacy for solving problems in the real world.},
  archive   = {C_AAMAS},
  author    = {Shukla, Yash},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2776–2778},
  title     = {Formal and natural language assisted curriculum generation for reinforcement learning agents},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663283},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cooperative multi-agent reinforcement learning in convention
reliant environments. <em>AAMAS</em>, 2773–2775. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There has been a substantial increase in interest in the field of Reinforcement Learning (RL), particularly that of using it to solve problems involving cooperation between many different agents, examples include self driving cars, robot assistants and robots in warehouses. Multi-Agent Reinforcement Learning (MARL) has been used with varying levels of success in these cooperative environments enabling two or more agents to be trained to work collaboratively toward a common goal. It has been established that training agents in self-play (SP) can achieve emergent behaviours in which agents adopt different conventions to solve a problem, however a mismatch in convention could lead to sub-optimal or even disastrous results. For instance, in driving, adherence to a unified convention, such as driving on the left or the right, is crucial to prevent collisions. This work introduces a strategy to address convention mismatches by creating a population of agents with diverse conventions and learns to identify which convention should be adopted for a given group of agents.},
  archive   = {C_AAMAS},
  author    = {Shipton, Jarrod},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2773–2775},
  title     = {Cooperative multi-agent reinforcement learning in convention reliant environments},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663282},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalizing objective-specification in markov decision
processes. <em>AAMAS</em>, 2767–2769. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this thesis, we address general utility Markov decision processes (GUMDPs), which generalize the standard Markov decision processes (MDPs) framework for decision-making by considering a broader range of objective functions that depend on the occupancy induced by a given policy. We aim to study GUMDPs from a theoretical perspective and develop new algorithms to solve GUMDPs by leveraging optimization techniques. We also aim to better understand how objective specification in GUMDPs compares to that of MDPs, further studying the connections between the two frameworks for sequential decision-making. We hope that, by achieving the proposed goals, the contributions of this thesis can lay down the foundations supporting the future development and deployment of agents that take advantage of the diverse set of objectives that can be encoded with GUMDPs.},
  archive   = {C_AAMAS},
  author    = {Santos, Pedro P.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2767–2769},
  title     = {Generalizing objective-specification in markov decision processes},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663281},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predicting and protecting the cognitive health of operators
in isolated, confined, and extreme environments. <em>AAMAS</em>,
2764–2766. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Operators&#39; cognitive performance is often critical for success and safety in isolated, confined, and extreme (ICE) environments such as spaceflight and wilderness medicine. Future autonomous systems may leverage predictions of cognitive states to improve human-system performance. Current approaches of estimating cognitive states, such as surveys or behavioral measures, are obtrusive, task-specific, or cannot be used in real-time. Physiological modeling, where biosignals are used to predict operator cognitive states, has the potential to overcome these limitations. My research develops predictive models of cognitive states and investigates cognitive health in ICE environments, aiming to inform adaptive autonomous systems and mitigate health and performance decrement.},
  archive   = {C_AAMAS},
  author    = {Richardson, Erin E.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2764–2766},
  title     = {Predicting and protecting the cognitive health of operators in isolated, confined, and extreme environments},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663280},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging interpretable human models to personalize AI
interventions for behavior change. <em>AAMAS</em>, 2761–2763. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many important areas of behavior change, such as wellness or education, are frictionful; they require individuals to expend effort over a long period of time with little immediate gratification. Because of this, humans often act sub-optimally with respect to their stated long-term goal. Here, an artificial intelligence (AI) agent can provide personalized behavioral interventions to correct human policies. The AI must personalize rapidly (before the individual has a chance to disengage) and interpretably, to aid our scientific understanding of the behavioral interventions. This work focuses on crafting small, interpretable models of the human that capture the mechanism behind the human agent&#39;s sub-optimal policies. These human models provide the AI with enough inductive bias to quickly learn intervention policies for each individual it encounters.},
  archive   = {C_AAMAS},
  author    = {Nofshin, Eura},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2761–2763},
  title     = {Leveraging interpretable human models to personalize AI interventions for behavior change},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663279},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interactive control and decision-making for multi-robots
systems. <em>AAMAS</em>, 2758–2760. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In order to achieve the ultimate goal of harmonious human-robot co-existence, the key is to build autonomous robots that can safely interact with humans for collaboration and coordination, as well as demonstrate reliable behavior that is acceptable to humans. These two requirements slightly differ from each other, with the former addressing the safety and functionality of robots as task performers, and the latter emphasizing the social compliance of robots as entities in society. In this abstract, I will outline my efforts towards enhancing the safety and reliability of interactive robot autonomy from three progressively advancing perspectives, 1) self-level autonomy, aiming to develop reactive behavior that ensures safety for individual robots when encountering non-cooperative agents, 2) peer-level autonomy, emphasizing the establishment of a safe interaction mechanism within an diverse and unconnected multi-robot system, and 3) human-involved autonomy, highlighting the consideration of human factors in the decision-making process for the design of multi-robot systems.},
  archive   = {C_AAMAS},
  author    = {Lyu, Yiwei},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2758–2760},
  title     = {Interactive control and decision-making for multi-robots systems},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663278},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive decision-making in non-stationary markov decision
processes. <em>AAMAS</em>, 2755–2757. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This research addresses a critical and largely unresolved challenge in the field of sequential decision-making: operating effectively in non-stationary environments. These environments are characterized by exogenously-driven changes over time, introducing significant uncertainties in decision-making processes. The urgency lies in devising strategies for optimal decision-making and planning amidst these unpredictable conditions. Central to my research is the concept of &#39;anytime&#39; decision-making. This approach involves leveraging dynamically learned models that not only mirror the current environmental state but also anticipate its potential evolution. The focus is on how an agent adapts its decision-making process in an ever-changing environment. A key contribution of my work is the exploration of adaptive decision-making strategies employed by an agent whose objectives fluctuate between performance optimization and safety prioritization. This is particularly challenging in dynamic environments where traditional static decision-making models fall short. The paper concludes by presenting future research directions. These aims are to enhance the understanding of adaptive decision-making in non-stationary environments, thereby advancing the field in this complex and constantly evolving area.},
  archive   = {C_AAMAS},
  author    = {Luo, Baiting},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2755–2757},
  title     = {Adaptive decision-making in non-stationary markov decision processes},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663277},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Building trustworthy human-centric autonomous systems via
explanations. <em>AAMAS</em>, 2752–2754. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous systems suffer from people&#39;s mistrust, as these systems rely on highly accurate yet inscrutable black box methods that are not amenable to safety guarantees nor common sense understanding. As a result, we see the erosion of accountability, human oversight, and contestation. In an attempt to build transparency, I advocate the use of model-specific, interactive, intelligible, and causally-grounded explanations for autonomous systems that take the human factor into account. I proposed a simulation-based conversational and causal framework for explaining sequential decision-making. The method, which is called CEMA, satisfies the previous four criteria without sacrificing the performance of complex models. I verified the benefits of CEMA via extensive quantitative and qualitative evaluation involving a large user study and autonomous driving. However, future work remains. To build a trustworthy autonomous system, CEMA needs to provide explanations that accurately calibrate people&#39;s trust according to the capabilities of the system. Towards this end, I hope to exploit prior knowledge in large language models to extend CEMA into a trust calibration system that uses conversations and explanations to adjust people&#39;s trust appropriately.},
  archive   = {C_AAMAS},
  author    = {Gyevnar, Balint},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2752–2754},
  title     = {Building trustworthy human-centric autonomous systems via explanations},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663276},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient continuous space BeliefMDP solutions for
navigation and active sensing. <em>AAMAS</em>, 2749–2751. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous robot teams have the potential to revolutionize the way we approach many problems, ranging from transportation to active sensing for weather science. However, to accomplish these missions, the robots must operate in environments with more threats and uncertainty than current autonomous systems can handle. The Belief Markov Decision Process framework (BeliefMDP) is a systematic and robust mathematical framework that can be used to obtain policies for these agents while reasoning over different kinds of uncertainties in the environment. Since computing optimal policies for a BeliefMDP exactly is intractable, this doctoral proposal focuses on solving them approximately by leveraging tree search techniques and guiding them using smart heuristics and learning algorithms for long-horizon continuous space problems.},
  archive   = {C_AAMAS},
  author    = {Gupta, Himanshu},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2749–2751},
  title     = {Efficient continuous space BeliefMDP solutions for navigation and active sensing},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663275},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large learning agents: Towards continually aligned robots
with scale in RL. <em>AAMAS</em>, 2746–2748. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the field of deep reinforcement learning significant progress has been made, but it seems we are missing the power of the scaling laws evident in large language models. This research aims to pioneer the development of large learning agents (LLAs) that can take advantage of efficient scaling. We focus on creating agents that generalize strongly, quickly adapt to continuously changing environments, and integrate the reinforcements received through human feedback. We believe that this is a key step towards the long-term vision for continually aligned and intelligent agents.},
  archive   = {C_AAMAS},
  author    = {Grooten, Bram},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2746–2748},
  title     = {Large learning agents: Towards continually aligned robots with scale in RL},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663274},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards building autonomous AI agents and robots for open
world environments. <em>AAMAS</em>, 2743–2745. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The shift of AI agents from controlled laboratory environments to real-world applications, such as autonomous vehicles and service robots, demands robust algorithms for navigating the intricacies of open-world scenarios. While traditional AI agents show proficiency in predictable, closed-world settings, their performance often diminishes in the dynamic and unforeseen conditions of real-world environments. My dissertation focuses on developing methods, frameworks, and domains that push the boundaries of open-world problem-solving in AI agents and robots. The central thesis question explores how AI agents can rapidly learn and adapt in open-world settings while tackling long-horizon, complex tasks. My work proposes integrative frameworks that combine reinforcement learning with symbolic planning, enabling on-the-fly adaptation of agents. Furthermore, we also propose environments designed for developing and assessing agent architectures adept at handling novelty. These advancements in open-world learning are pivotal in enhancing adaptability, speed, and robustness in AI agents and robots, laying a},
  archive   = {C_AAMAS},
  author    = {Goel, Shivam},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2743–2745},
  title     = {Towards building autonomous AI agents and robots for open world environments},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663273},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward explainable agent behaviour. <em>AAMAS</em>,
2740–2742. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Agents are a special kind of AI-based software in that they interact in complex environments and have increased potential for emergent behaviour, even in isolation. Explaining such behaviour is key to deploying trustworthy AI, but the increasing complexity and opaqueness of agents makes this hard. Beyond narrow-task and instant-based goals, agents may exhibit durative behaviour and be required to have planning or deliberative capabilities, or even to reason over other&#39;s behaviours. This precludes machine learning explainability -i.e. explanations over single predictions or actions- from giving complete and useful explanations. There is a need for extending explainability tools. We split the capabilities of agents into several levels, each more abstract, and produce explanations by climbing these levels: from actions, tellic (ends), deliberation, and more. The first two have been solved through frequentist models (Policy-Graphs), and the third is work in progress. We intend to extend this work by adding components for explaining epistemology, agent-agent interaction, norms and values.},
  archive   = {C_AAMAS},
  author    = {Gimenez-Abalos, Victor},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2740–2742},
  title     = {Toward explainable agent behaviour},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663272},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scaling up cooperative multi-agent reinforcement learning
systems. <em>AAMAS</em>, 2737–2739. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cooperative multi-agent reinforcement learning methods aim to learn effective collaborative behaviours of multiple agents performing complex tasks. However, existing MARL methods are commonly proposed for fairly small-scale multi-agent benchmark problems, wherein both the number of agents and the length of the time horizons are typically restricted. My initial work investigates hierarchical controls of multi-agent systems, where a unified overarching framework coordinates multiple smaller multi-agent subsystems, tackling complex, long-horizon tasks that involve multiple objectives. Addressing another critical need in the field, my research introduces a comprehensive benchmark for evaluating MARL methods in long-horizon, multi-agent, and multi-objective scenarios. This benchmark aims to fill the current gap in the MARL community for assessing methodologies in more complex and realistic scenarios. My dissertation would focus on proposing and evaluating methods for scaling up multi-agent systems in two aspects: structural-wise increasing the number of reinforcement learning agents and temporal-wise extending the planning horizon and complexity of problem domains that agents are deployed in.},
  archive   = {C_AAMAS},
  author    = {Geng, Minghong},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2737–2739},
  title     = {Scaling up cooperative multi-agent reinforcement learning systems},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663271},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Negotiation strategies for combining partials deals in
one-to-many negotiations. <em>AAMAS</em>, 2734–2736. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Efficient automated negotiation is not trivial in one-to-many negotiations with partial deals, where a negotiation agent is challenged with a difficult task to plan and oversee multiple interconnected negotiations. A decision or deal made in one negotiation can affect subgoals in other subnegotiations, so substrategies in different subnegotiations should be well aligned to achieve a common goal. The interconnected nature of subnegotiations and the uncertain course of opponent actions makes this setting a complex challenge.We study the challenges faced in a one-to-many context with partial deals and explore their theoretical properties in combination with experimental research. We specifically discuss the challenges for protocol design and negotiation strategies.},
  archive   = {C_AAMAS},
  author    = {Florijn, Tamara C.P.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2734–2736},
  title     = {Negotiation strategies for combining partials deals in one-to-many negotiations},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663270},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The multi-agent system based on LLM for online discussions.
<em>AAMAS</em>, 2731–2733. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The considerable improvement on the Internet and the corresponding applications leads to the result of online discussions becoming far more popular and significant than any other method for people to communicate with each other and reach a consensus. Meanwhile, the incredible improvement in Large Language Models (LLM) has promoted the performance of LLM-based agents in text understanding and content generation capabilities. The research objective of the PhD thesis is to build democratic discussion environments, with three main issues existing right now: 1) Large-scale discussions tend to be complicated, 2) Rumours and misinformation bring negative effects to the discussions, and 3) Direct democratic discussions are complex and time-consuming. This extended abstract introduces the efforts that have been made to address those issues, with the introduction of the potential directions in the future.},
  archive   = {C_AAMAS},
  author    = {Dong, Yihan},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2731–2733},
  title     = {The multi-agent system based on LLM for online discussions},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663269},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Communication and generalization in multi-agent learning.
<em>AAMAS</em>, 2728–2730. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Significant challenges exist in robustly interacting and communicating with a diverse array of agents, especially in intricate settings like autonomous driving where AI agents and humans coexist. This work approaches these challenges from three perspectives: generalization of agent policies, development of communication-supporting representations, and interactions between humans and AI agents using natural language. We provide an overview of preliminary achievements in each area and outline proposed research focusing on enhancing cooperative driving through natural language communication, aiming to comprehensively address these complex multi-agent interaction challenges.},
  archive   = {C_AAMAS},
  author    = {Cui, Jiaxun},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2728–2730},
  title     = {Communication and generalization in multi-agent learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663268},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Emergence of linguistic conventions in multi-agent systems
through situated communicative interactions. <em>AAMAS</em>, 2725–2727.
(<a href="https://dl.acm.org/doi/10.5555/3635637.3663267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The field of emergent communication investigates the emergence of shared linguistic conventions among autonomous agents engaged in cooperative tasks that require communication. Conventions that arise through self-organisation are known to be more robust, flexible, and adaptive, and it removes the need for hand-crafting communication protocols. In my PhD research, I investigate how artificial agents can co-construct such conventions of linguistic structures in reference-based tasks. This problem is tackled using the language game experimental paradigm which aims to model the processes underlying the emergence and evolution of human languages. My primary contribution thus far introduces a novel methodology for the language game paradigm in the emergent setting. Using the methodology, agents can establish through self-organisation an emergent language that enables them to refer to arbitrary entities in their environment using single-word utterances. For the first time, the methodology is directly applicable to any dataset that describes entities in terms of continuously-valued features. The next phase in my research is to move from single-word utterances to multi-word utterances through the emergence of grammatical structures.},
  archive   = {C_AAMAS},
  author    = {Botoko Ekila, J\&#39;{e}r\^{o}me},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2725–2727},
  title     = {Emergence of linguistic conventions in multi-agent systems through situated communicative interactions},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663267},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Abstraction in non-monotonic reasoning. <em>AAMAS</em>,
2722–2724. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Abstraction emerges as a valuable method across diverse domains of Artificial Intelligence (AI), particularly in the field of knowledge representation and reasoning. Intuitively, abstraction maps a complicated structure to a simpler version of it. That reduces the computational complexity of the task being considered, as it provides us with the ability to focus on the parts of the problem that are relevant to the solution. In our view, such a tool can also have potential in the field of non-monotonic reasoning. Non-monotonicity is a crucial notion as it is very common when reasoning over defeasible knowledge. Adding new entries to our current knowledge, oftentimes results in restricting the conclusions that we can draw. For this form of reasoning we use certain formalisms, such as computational argumentation and Logic Programming (LP), that help us capture non-monotonicity. However, interpreting these formalisms faces hardships due to the large structures that might occur when representing the problem in question. Hence, coming up with ways to manage these structures easier is necessary. Recently, abstraction was shown to be a promising tool when dealing with Argumentation Frameworks (AFs) as well as with LP. AFs are frameworks with graph-like structure, whose nodes represent arguments with no internal structure, while edges stand for conflicts among the arguments. In our research we focus on continuing in this direction by employing structured frameworks such as Assumption-Based Argumentation Frameworks (ABAFs). Subsequently, we will extend our research to similar formalisms such as LP.},
  archive   = {C_AAMAS},
  author    = {Apostolakis, Iosif},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2722–2724},
  title     = {Abstraction in non-monotonic reasoning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663266},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Utility-based reinforcement learning: Unifying
single-objective and multi-objective reinforcement learning.
<em>AAMAS</em>, 2717–2721. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Research in multi-objective reinforcement learning (MORL) has introduced the utility-based paradigm, which makes use of both environmental rewards and a function that defines the utility derived by the user from those rewards. In this paper we extend this paradigm to the context of single-objective reinforcement learning (RL), and outline multiple potential benefits including the ability to perform multi-policy learning across tasks relating to uncertain objectives, risk-aware RL, discounting, and safe RL. We also examine the algorithmic implications of adopting a utility-based approach.},
  archive   = {C_AAMAS},
  author    = {Vamplew, Peter and Foale, Cameron and Hayes, Conor F. and Mannion, Patrick and Howley, Enda and Dazeley, Richard and Johnson, Scott and K\&quot;{a}llstr\&quot;{o}m, Johan and Ramos, Gabriel and Radulescu, Roxana and R\&quot;{o}pke, Willem and Roijers, Diederik M.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2717–2721},
  title     = {Utility-based reinforcement learning: Unifying single-objective and multi-objective reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663264},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explainable agents (XAg) by design. <em>AAMAS</em>,
2712–2716. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The likes of ChatGPT has propelled the use of AI techniques beyond our community&#39;s expectations. Along with this, the fear of AI has also risen, in particular around the ability, or lack thereof, of the AI system to explain its behaviours. Explainability is a key element of building trust and an important issue for our community. In this paper we advocate for agents that are explainable-by-design, that is, explainability is built into the development of agents rather than an afterthought. We propose key features of an explainable agent (XAg) system and propose a general framework that enables explainability. We advocate the use of design patterns to develop XAgs and propose a general design pattern that can be used for any agent architecture. We instantiate our framework for goal-based agents and implement the framework for the SARL agent programming language coupled with a state-of-the-art event management system. We make a call to the developers of other agent programming languages (APLs) in our community to follow suit by instantiating the general framework we propose into their APL, perhaps even enhancing the framework we present. We also propose an open repository of design patterns and examples for agent systems. If nothing else, we hope this paper will inspire further work on XAg from the design perspective as it is critical that multi agent systems are explainable by design!},
  archive   = {C_AAMAS},
  author    = {Rodriguez, Sebastian and Thangarajah, John},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2712–2716},
  title     = {Explainable agents (XAg) by design},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663263},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The cognitive hourglass: Agent abstractions in the large
models era. <em>AAMAS</em>, 2706–2711. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in AI are driving an unprecedented and fast-paced development of myriads of powerful agent tools and applications, mostly based on generative AI technologies such as Large Language/Multi-modal/Agent Models. However, despite many proposals in that direction, the lack of a sound set of usable engineering abstractions hinders the possibility of methodically engineering complex agent-based applications, also due to the gap between cognitive agent-based concepts and LLMs&#39; behavioural patterns. We argue that such a set of abstractions should constitute the &quot;narrow neck&quot; of an indispensable &quot;cognitive hourglass&quot;: a level of abstraction that is meant to be useful for humans to understand/design/control agents and MAS, regardless of the specific AI technologies adopted at the implementation level and of the specific application context. Here, we elaborate on the idea of the cognitive hourglass, motivate its need, sketch its envisioned architecture, and identify the research challenges for its realisation.},
  archive   = {C_AAMAS},
  author    = {Ricci, Alessandro and Mariani, Stefano and Zambonelli, Franco and Burattini, Samuele and Castelfranchi, Cristiano},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2706–2711},
  title     = {The cognitive hourglass: Agent abstractions in the large models era},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663262},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Selecting representative bodies: An axiomatic view.
<em>AAMAS</em>, 2701–2705. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As the world&#39;s democratic institutions are challenged by dissatisfied citizens, political scientists and computer scientists have proposed and analyzed various (innovative) methods to select representative bodies, a crucial task in every democracy. However, a unified framework to analyze and compare different selection mechanisms is largely missing. To address this gap, we advocate employing concepts and tools from computational social choice to devise a model in which different selection mechanisms can be formalized. Such a model would allow for conceptualizing and evaluating desirable representation axioms. We make the first step in this direction by proposing a unifying mathematical formulation of different selection mechanisms as well as various social-choice-inspired axioms such as proportionality and monotonicity.},
  archive   = {C_AAMAS},
  author    = {Revel, Manon and Boehmer, Niclas and Colley, Rachael and Brill, Markus and Faliszewski, Piotr and Elkind, Edith},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2701–2705},
  title     = {Selecting representative bodies: An axiomatic view},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663261},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards sustainable human-agent teams: A framework for
understanding human-agent team dynamics. <em>AAMAS</em>, 2696–2700. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human-agent teamwork is a promising research stream with great potential to impact society. Research on collaborative AI and human-agent interaction has tackled the problem from several perspectives, but we argue that a focus on teams as a unit and a model for human-agent team dynamics is missing. Such a focus is particularly relevant if we aim at involving agents as active team members and at building sustainable teams over time. A team perspective on human-agent collaboration requires new models that pose challenges for AI and humans alike. AI needs new models to build an understanding of team level variables, such as team structure and cohesion, to be able to monitor the team and act on the team beyond performing the task. Humans, in turn, need to be able to incorporate agents as team members in their mental models of teamwork and integrate them into team processes. Such human-agent team dynamics models should be built taking into account four different levels: individual, interpersonal, team, and organisational. We believe that to fulfill this vision we need to bring together the different fields of AI and social sciences.},
  archive   = {C_AAMAS},
  author    = {Prada, Rui and Homan, Astrid C. and van Kleef, Gerben A.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2696–2700},
  title     = {Towards sustainable human-agent teams: A framework for understanding human-agent team dynamics},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663260},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Designing artificial reasoners for communication.
<em>AAMAS</em>, 2690–2695. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In order to endow a conversational agent with sophisticated social intelligence, machine learning (which is prominent in LLM-based systems like Chat-GPT) is not enough. Logic-based reasoning and decision-making is needed. We need formal languages as well as reasoning and planning algorithms based on them for modeling and endowing the agent with intentional communication, theory of mind, explanatory capability and norm compliance. We identify some requirements that such languages should satisfy as well as a number of challenges regarding their combination and their integration with machine learning methods.},
  archive   = {C_AAMAS},
  author    = {Lorini, Emiliano},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2690–2695},
  title     = {Designing artificial reasoners for communication},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663259},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive incentive engineering in citizen-centric AI.
<em>AAMAS</em>, 2684–2689. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Adaptive incentives are a valuable tool shown to improve the efficiency of complex multiagent systems and could produce win-win situations for all stakeholders. However, their application usage is very limited, partly due to a significant gap between the literature and practice. We argue that overcoming this gap requires addressing four open research challenges. First, the dynamic, volatile and uncertain nature of environments needs to be fully considered. Second, social factors including user acceptance, fairness, ethical considerations and trust have to match end users&#39; expectations and needs. Third, the evaluation of mechanisms and systems has to be robust and focused on real-world outcomes and stakeholder requirements. Finally, all this has to be built on a reliable theoretical foundation. In order to overcome these open challenges in adaptive incentive engineering, tools from the fields of mechanism design and game theory can be used. This will help to achieve the opportunities adaptive incentives can provide to real-world practical environments, producing better AI systems for the benefit of all.},
  archive   = {C_AAMAS},
  author    = {Koohy, Behrad and Buermann, Jan and Yazdanpanah, Vahid and Briggs, Pamela and Pschierer-Barnfather, Paul and Gerding, Enrico and Stein, Sebastian},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2684–2689},
  title     = {Adaptive incentive engineering in citizen-centric AI},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663258},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Empowering BDI agents with generalised decision-making.
<em>AAMAS</em>, 2679–2683. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While research on software agents has long focused on explicit agent communication, there is comparatively less effort on implicit communication between agents via recognising each other&#39;s intentions and desires for understanding their decision-making reasoning process. Since most human communication is not explicit, we aim to outline a research agenda to help endow autonomous agents with analogous coordination capabilities. In this paper, we formalise a framework that empowers the decision-making process of BDI agents in adversarial and cooperative environments by casting them as generalised planners using Theory of Mind. Our formalisation uses the fundamental philosophical properties of the BDI model and its reasoning process to outline a broad research agenda in agents&#39; research.},
  archive   = {C_AAMAS},
  author    = {Fraga Pereira, Ramon and Meneguzzi, Felipe},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2679–2683},
  title     = {Empowering BDI agents with generalised decision-making},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663257},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Going beyond mono-mission earth observation: Using the
multi-agent paradigm to federate multiple missions. <em>AAMAS</em>,
2674–2678. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We envision a multi-agent federation layer to coordinate systems composed of independent Earth observation missions. The goal of this federation is to allow clients requesting acquisitions of large areas to easily access several constellations of satellites and communication sites to compose and download their acquisitions, in a reduced time compared to conventional uncoordinated requests. We identify several scientific tracks and challenges related to agent-based approaches such as coordination, planning and learning, to implement to two key federation functions: (i) multi-mission coverage feasibility and dispatching and (ii) communication site booking.},
  archive   = {C_AAMAS},
  author    = {Farges, Jean-Loup and Perotto, Filipo and Picard, Gauthier and Pralet, C\&#39;{e}dric and de Lussy, Cyrille and Guerra, Jonathan and Pavero, Philippe and Planchou, Fabrice},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2674–2678},
  title     = {Going beyond mono-mission earth observation: Using the multi-agent paradigm to federate multiple missions},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663256},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-deal negotiation. <em>AAMAS</em>, 2668–2673. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Negotiating multiple deals is an essential day-to-day activity for many businesses. Procurement, for instance, typically represents one of the largest expense items for businesses worldwide. Today, 95\% of European businesses are still negotiating their goods and services entirely unaided by computers, which has been shown to lead to significantly less efficient outcomes, increased costs, and highly labor-intensive processes. Enabling the automation of general-purpose multi-deal negotiations would therefore have an enormous impact on the competitiveness and profitability of businesses world-wide. However, currently available algorithms are not yet capable of performing multiple complex and interdependent negotiations at the same time. This so far underexplored research challenge calls for solutions and methods beyond the state-of-the-art research in auctions, game theory, or bilateral negotiation. It requires new asynchronous negotiation strategies that can reach multiple interdependent deals, as well as novel mathematical coordination mechanisms that are able to steer pro-actively toward a desirable aggregate outcome. The challenges of multi-deal negotiation call for 1) a mathematical model and protocol for multi-deal negotiation algorithms that can strike multiple partial deals with multiple partners; 2) coordination techniques for making optimal trade-offs regarding expected agreement utility; and 3) multi-deal negotiation strategies that can provide online probability estimates of the expected outcome. Altogether, such a research endeavor would deliver the fundamental underpinnings for general-purpose multi-deal negotiation algorithms, thereby paving the way for future systems for domains ranging from procurement and energy to ethics and transportation.},
  archive   = {C_AAMAS},
  author    = {Baarslag, Tim},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2668–2673},
  title     = {Multi-deal negotiation},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663255},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Budget-feasible egalitarian allocation of conflicting jobs.
<em>AAMAS</em>, 2659–2667. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Allocating conflicting jobs among individuals while respecting a budget constraint for each individual is an optimization problem that arises in various real-world scenarios. In this paper, we consider the situation where each individual derives some satisfaction from each job. We focus on finding a feasible allocation of conflicting jobs that maximize egalitarian cost, i.e. the satisfaction of the individual who is worst-off. To the best of our knowledge, this is the first paper to combine egalitarianism, budget-feasibility, and conflict-freeness in allocations. We provide a systematic study of the computational complexity of finding budget-feasible conflict-free egalitarian allocation and show that our problem generalizes a large number of classical optimization problems. Therefore, unsurprisingly, our problem is NP-Hard even for two individuals and when there is no conflict between any jobs. We show that the problem admits algorithms when studied in the realm of approximation algorithms and parameterized algorithms with a host of natural parameters that match and in some cases improve upon the running time of known algorithms.},
  archive   = {C_AAMAS},
  author    = {Gupta, Sushmita and Jain, Pallavi and Mohanapriya, A and Tripathi, Vikash},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2659–2667},
  title     = {Budget-feasible egalitarian allocation of conflicting jobs},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663253},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MAGNets: Micro-architectured group neural networks.
<em>AAMAS</em>, 2650–2658. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement Learning (RL) algorithms have successfully achieved human-like performances in complex environments like games, autonomous vehicles, and industrial robots. However, the Deep Neural Networks (DNNs) used to approximate large Deep Reinforcement Learning (DRL) policies are resource-hungry and opaque. This limits the applicability of DRL in safety-critical applications running on resource-constrained platforms. On the other hand, on inspecting the design of most multi-output safety critical embedded control applications, it may be observed that such systems often derive each output based on some artifacts, which are, in turn, derived from input variables. Given such dependencies of internal system states on inputs, one may argue that each of these derived artifacts can be approximated by a smaller network in a multi-network DRL setting. In this work, we propose Micro Architecture Group Neural Networks (MAGNets) that can distill the learning of a large DRL network into multiple small neural networks. Using several OpenAI Gym environments, we show that existing verification tools can be used to verify the output of MAGNets while preserving the performance of a large neural policy. We also report our gains in network compactness, which directly impacts the execution latency and applicability in edge devices.},
  archive   = {C_AAMAS},
  author    = {Dey, Sumanta and Gangopadhyay, Briti and Dasgupta, Pallb and Dey, Soumyajit},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2650–2658},
  title     = {MAGNets: Micro-architectured group neural networks},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663252},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Fair and efficient division of a discrete cake with
switching utility loss. <em>AAMAS</em>, 2641–2649. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cake cutting is a widely studied model for allocating resources with temporal or spatial structures among agents. Recently, a new line of research has emerged that focuses on the discrete variant, where the resources are indivisible and connected by a path. In some real-world applications, the resources are interdependent, and dividing the cake may reduce their effectiveness. In this paper, we introduce a model that captures the effect of division as switching utility loss and investigate the tradeoff between fairness and efficiency for various settings. Specifically, we measure fairness and efficiency using the popular notions of envy-freeness up to one item (EF1) and social welfare, respectively. The goal of our study is to understand how much social welfare must be sacrificed to ensure EF1 allocations and design polynomial-time algorithms that can compute EF1 allocations with the best possible social welfare guarantee.},
  archive   = {C_AAMAS},
  author    = {Chen, Zheng and Li, Bo and Li, Minming and Zhang, Guochuan},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2641–2649},
  title     = {Fair and efficient division of a discrete cake with switching utility loss},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663251},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Regret-based defense in adversarial reinforcement learning.
<em>AAMAS</em>, 2633–2640. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep Reinforcement Learning (DRL) policies are vulnerable to adversarial noise in observations, which can have disastrous consequences in safety-critical environments. For instance, a self-driving car receiving adversarially perturbed sensory observations about traffic signs (e.g., a stop sign physically altered to be perceived as a speed limit sign) can be fatal. Leading existing approaches for making RL algorithms robust to an observation-perturbing adversary have focused on (a) regularization approaches that make expected value objectives robust by adding adversarial loss terms; or (b) employing &quot;maximin&#39;&#39; (i.e., maximizing the minimum value) notions of robustness. While regularization approaches are adept at reducing the probability of successful attacks, their performance drops significantly when an attack is successful. On the other hand, maximin objectives, while robust, can be extremely conservative. To this end, we focus on optimizing a well-studied robustness objective, namely regret. To ensure the solutions provided are not too conservative, we optimize an approximation of regret using three different methods. We demonstrate that our methods outperform existing best approaches for adversarial RL problems across a variety of standard benchmarks from literature.},
  archive   = {C_AAMAS},
  author    = {Belaire, Roman and Varakantham, Pradeep and Nguyen, Thanh and Lo, David},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2633–2640},
  title     = {Regret-based defense in adversarial reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663250},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Controlling delegations in liquid democracy. <em>AAMAS</em>,
2624–2632. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In liquid democracy, agents can either vote directly or delegate their vote to a different agent of their choice. This results in a power structure in which certain agents possess more voting weight than others. As a result, it opens up certain possibilities of vote manipulation, including control and bribery, that do not exist in standard voting scenarios of direct democracy. Here we formalize a certain kind of election control - in which an external agent may change certain delegation arcs - and study the computational complexity of the corresponding combinatorial problem.},
  archive   = {C_AAMAS},
  author    = {Alouf-Heffetz, Shiri and Inamdar, Tanmay and Jain, Pallavi and Talmon, Nimrod and Hiren, Yash More},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2624–2632},
  title     = {Controlling delegations in liquid democracy},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663249},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VMFER: Von mises-fisher experience resampling based on
uncertainty of gradient directions for policy improvement of
actor-critic algorithms. <em>AAMAS</em>, 2621–2623. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement Learning (RL) is a widely employed technique in decision-making problems, encompassing two fundamental operations -- policy evaluation and policy improvement. Actor-critic algorithms dominate the field of RL, but there is a challenge in improving their learning efficiency. To address this, ensemble critics are often employed to enhance policy evaluation efficiency. However, when using multiple critics, the actor in the policy improvement process can obtain different gradients. Previous studies have combined these gradients without considering their disagreements. Therefore, optimizing the policy improvement process is crucial to enhance the learning efficiency of actor-critic algorithms. This study focuses on investigating the impact of gradient disagreements caused by ensemble critics on policy improvement. We introduce the concept of uncertainty of gradient directions as a means to measure the disagreement among gradients utilized in the policy improvement process. Through measuring the disagreement among gradients, we find that transitions with lower uncertainty of gradient directions are more reliable in the policy improvement process. Building on this analysis, we propose a method called von Mises-Fisher Experience Resampling (vMFER), which optimizes the policy improvement process by resampling transitions and assigning higher confidence to transitions with lower uncertainty of gradient directions. Our experiments on Mujoco robotic control tasks and robotic arm tasks with sparse rewards demonstrate that vMFER significantly outperforms the benchmark and is particularly well-suited for ensemble structures in RL.},
  archive   = {C_AAMAS},
  author    = {Zhu, Yiwen and Liu, Jinyi and Wei, Wenya and Fu, Qianyi and Hu, Yujing and Fang, Zhou and An, Bo and Hao, Jianye and Lv, Tangjie and Fan, Changjie},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2621–2623},
  title     = {VMFER: Von mises-fisher experience resampling based on uncertainty of gradient directions for policy improvement of actor-critic algorithms},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663247},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards zero shot learning in restless multi-armed bandits.
<em>AAMAS</em>, 2618–2620. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Restless multi-arm bandits (RMABs), a class of resource allocation problems with broad application in areas such as healthcare, online advertising, and anti-poaching, have recently been studied from a multi-agent reinforcement learning perspective. Prior RMAB research suffers from several limitations, e.g., it fails to adequately address continuous states, and requires retraining from scratch when arms opt-in and opt-out over time, a common challenge in many real world applications. We propose a neural network-based pre-trained model that has general zero-shot ability on a wide range of previously unseen RMABs.},
  archive   = {C_AAMAS},
  author    = {Zhao, Yunfan and Behari, Nikhil and Hughes, Edward and Zhang, Edwin and Nagaraj, Dheeraj and Tuyls, Karl and Taneja, Aparna and Tambe, Milind},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2618–2620},
  title     = {Towards zero shot learning in restless multi-armed bandits},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663246},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bootstrapped policy learning: Goal shaping for efficient
task-oriented dialogue policy learning. <em>AAMAS</em>, 2615–2617. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement Learning (RL) shows promise in optimizing task-oriented dialogue policies, but addressing the challenge of reward sparsity remains challenging. Curriculum learning offers an effective solution by strategically training dialogue policies from simple to complex, facilitating a smooth knowledge transition across varied goal complexities. However, these methods typically assume that goal difficulty will increase gradually to adapt to difficult goals over time. In complex environments lacking intermediate goals, attaining smooth knowledge transitions becomes tricky. This paper proposes a novel Bootstrapped Policy Learning (BPL) framework that adaptively tailors a curriculum for each complex goal through goal shaping, which consists of progressively challenging subgoals. Goal shaping comprises goal decomposition and evolution, breaking complex goals into solvable subgoals and progressively increasing subgoal difficulty as the policy improves. BPL harmoniously combines these aspects, enabling smooth knowledge transitions from simple to complex goals, thereby enhancing task-oriented dialogue policy learning efficiency. Our experiments demonstrate the effectiveness of BPL in two complex dialogue environments.},
  archive   = {C_AAMAS},
  author    = {Zhao, Yangyang and Dastani, Mehdi and Wang, Shihan},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2615–2617},
  title     = {Bootstrapped policy learning: Goal shaping for efficient task-oriented dialogue policy learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663245},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). JDRec: Practical actor-critic framework for online
combinatorial recommender system. <em>AAMAS</em>, 2612–2614. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the realm of online recommendation systems, the Combinatorial Recommender (CR) system stands out for its unique approach. It presents users with a list of items on a result page, where user behavior is simultaneously influenced by contextual information and the items listed. Formulated as a combinatorial optimization problem, the objective of the CR system is to maximize the recommendation reward across the entire list of items. Despite the significant potential of CR systems, developing a practical and efficient model remains substantial challenges. These challenges stem from the dynamic nature of online environments and the pressing need for personalized recommendations. To tackle these challenges, we decompose the overarching problem into two sub-problems: list generation and list evaluation. We propose novel and pragmatic model architectures for each sub-problem aiming to concurrently enhance both effectiveness and efficiency. To further adapt the CR system to online scenarios, we integrate a bootstrap algorithm into an actor-critic reinforcement framework. This innovative approach called JD Recommender System (JDRec) is designed to continuously refine the recommendation mode through sustained user interaction, ensuring the system&#39;s adaptability and relevance. The proposed JDRec framework, tested through rigorous offline and online experiments, has shown promising results. It has been successfully deployed in online JD recommendation systems, yielding a notable improvement in click-through rate by 2.6\% and augmenting the total value of the platform by 5.03\%. Besides, we release the large scale dataset used in our work to facilitate further research.},
  archive   = {C_AAMAS},
  author    = {Zhao, Xin and Li, Jiaxin and Fang, Zhiwei and Guo, Yuchen and Zhao, Jinyuan and He, Jie and Chen, Wenlong and Peng, Changping and Ding, Guiguang},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2612–2614},
  title     = {JDRec: Practical actor-critic framework for online combinatorial recommender system},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663244},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ENOTO: Improving offline-to-online reinforcement learning
with q-ensembles. <em>AAMAS</em>, 2609–2611. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Offline reinforcement learning (RL) is a learning paradigm where an agent learns from a fixed dataset of experience. However, learning solely from a static dataset can limit the performance due to the lack of exploration. To overcome it, offline-to-online RL combines offline pre-training with online fine-tuning, which enables the agent to further refine its policy by interacting with the environment in real-time. Despite its benefits, existing offline-to-online RL methods suffer from performance degradation and slow improvement during the online phase. To tackle these challenges, we propose a novel framework called ENsemble-based Offline-To-Online (ENOTO) RL. By increasing the number of Q-networks, we seamlessly bridge offline pre-training and online fine-tuning without degrading performance. Moreover, to expedite online performance enhancement, we appropriately loosen the pessimism of Q-value estimation and incorporate ensemble-based exploration mechanisms into our framework. Experimental results demonstrate that ENOTO can substantially improve the training stability, learning efficiency, and final performance of existing offline RL methods during online fine-tuning on a range of locomotion tasks, significantly outperforming existing offline-to-online RL methods.},
  archive   = {C_AAMAS},
  author    = {Zhao, Kai and Hao, Jianye and Ma, Yi and Liu, Jinyi and Zheng, Yan and Meng, Zhaopeng},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2609–2611},
  title     = {ENOTO: Improving offline-to-online reinforcement learning with Q-ensembles},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663243},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distance-aware attentive framework for multi-agent
collaborative perception in presence of pose error. <em>AAMAS</em>,
2606–2608. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent collaborative perception exchanges information to promote holistic perception, especially for remote and invisible areas that are limited by detection range and occlusion. Due to imperfect localization in practice, it usually suffers from pose estimation error, which can cause spatial message misalignment and performance degradation. Unlike most existing methods using additional module or procedure to correct pose error, we propose a novel framework, DistAtt, to suppress pose error and mine useful information simultaneously. It mainly consists of distance-aware feature sampling and cross-agent feature aggregation. The former utilizes diverse pooling kernels to downsample the intermediate features to different multiple granularities, and the latter utilizes specially designed attention mechanism to learn the most critical information. Furthermore, it adopts compensation strategy for more stable optimization. Experimental results show that DistAtt significantly suppresses the effect of localization noise and achieves outperformed performance when pose error exists.},
  archive   = {C_AAMAS},
  author    = {Zhao, Binyu and Zhang, Wei and Zou, Zhaonian},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2606–2608},
  title     = {Distance-aware attentive framework for multi-agent collaborative perception in presence of pose error},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663242},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decentralized competing bandits in many-to-one matching
markets. <em>AAMAS</em>, 2603–2605. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Two-sided matching is a classic and well-studied problem. As the participants are usually not aware of the accurate preferences towards the other side, the model of competing bandits characterizes the process of learning uncertainty through interactions in one-to-one matching markets. However, it does not apply to many cases, such as the online labor market where employers may have multiple vacancies. Thus, in this paper, we study the generalized problem of competing bandits in many-to-one matching markets and focus on the fully decentralized setting. We propose an algorithm and show that it achieves O(log T) regret compared with the optimal stable matching, for the first time without restricted assumptions on preferences and observability in previous literature.},
  archive   = {C_AAMAS},
  author    = {Zhang, Yirui and Fang, Zhixuan},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2603–2605},
  title     = {Decentralized competing bandits in many-to-one matching markets},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663241},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal diffusion auctions. <em>AAMAS</em>, 2600–2602. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Diffusion auction design is a new trend in mechanism design for which the main goal is to incentivize existing buyers to invite their neighbors on a social network, to join an auction. With more buyers, a diffusion auction will be able to receive higher revenue. Existing studies have proposed many diffusion auctions to attract more buyers, but the seller&#39;s revenue is not optimized. In this study, we investigate what optimal revenue the seller can achieve by attracting more buyers. Different from the traditional setting, the revenue can be achieved highly relies on the structure of the network. We propose a class of mechanisms, where for any given structure, an optimal diffusion mechanism can be found. Moreover, we show that an optimal mechanism that handles all structures does not exist. Therefore, we also propose mechanisms that have bounded approximations of the optimal revenue in all structures.},
  archive   = {C_AAMAS},
  author    = {Zhang, Yao and Zheng, Shanshan and Zhao, Dengji},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2600–2602},
  title     = {Optimal diffusion auctions},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663240},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mutual information as intrinsic reward of reinforcement
learning agents for on-demand ride pooling. <em>AAMAS</em>, 2597–2599.
(<a href="https://dl.acm.org/doi/10.5555/3635637.3663239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The emergence of on-demand ride pooling services allows each vehicle to serve multiple passengers at a time, thus increasing drivers&#39; income and enabling passengers to travel at lower prices than taxi/car on-demand services. Although on-demand ride pooling services can bring so many benefits, ride pooling services need a well-defined matching strategy to maximize the benefits for all parties (passengers, drivers, aggregation companies and environment), especially the regional dispatching of vehicles has a significant impact on matching and revenue. Existing algorithms often only consider revenue maximization, which makes it difficult for requests with unusual distribution to get rides. How to increase revenue while ensuring a reasonable assignment of requests brings a challenge to ride pooling service companies (aggregation companies). In this paper, we propose a framework for vehicle dispatching for ride pooling tasks, which splits the city into discrete dispatching regions and uses the reinforcement learning (RL) algorithm to dispatch vehicles in these regions. We also consider the mutual information (MI) between vehicle and request distribution as the intrinsic reward of the RL algorithm to improve the correlation between their distributions, thus ensuring the possibility of getting a ride for unusually distributed requests. In experimental results on a real-world taxi dataset, we demonstrate that our framework can significantly increase revenue up to an average of 3\% over the existing best on-demand ride pooling method.},
  archive   = {C_AAMAS},
  author    = {Zhang, Xianjie and Sun, Jiahao and Gong, Chen and Wang, Kai and Cao, Yifei and Chen, Hao and Liu, Yu},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2597–2599},
  title     = {Mutual information as intrinsic reward of reinforcement learning agents for on-demand ride pooling},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663239},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large language model assissted multi-agent dialogue for
ontology alignment. <em>AAMAS</em>, 2594–2596. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ontology alignment is critical in cross-domain integration; however, it typically necessitates the involvement of a human domain-expert, which can make the task costly. Although a variety of machine-learning approaches have been proposed that can simplify this task by learning the patterns from experts, such techniques are still susceptible to domain knowledge updates that could potentially change the patterns and lead to extra expert involvement. The use of Large Language Models (LLMs) has demonstrated a general cognitive ability, which has the potential to assist ontology alignment from the cognition level, thus obviating the need for costly expert involvement. However, the process by which the output of LLMs is generated can be opaque and thus the reliability and interpretability of such models is not always predictable. This paper proposes a dialogue model, in which multiple agents negotiate the correspondence between two knowledge sets with the support from an LLM. We demonstrate that this approach not only reduces the need for the involvement of a domain expert for ontology alignment, but that the results are interpretable despite the use of LLMs.},
  archive   = {C_AAMAS},
  author    = {Zhang, Shiyao and Dong, Yuji and Zhang, Yichuan and Payne, Terry R. and Zhang, Jie},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2594–2596},
  title     = {Large language model assissted multi-agent dialogue for ontology alignment},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663238},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Auto-encoding adversarial imitation learning.
<em>AAMAS</em>, 2591–2593. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning (RL) provides a powerful framework for decision-making, but its application in practice often requires a carefully designed reward function. Adversarial Imitation Learning (AIL) sheds light on automatic policy acquisition without access to the reward signal from the environment. In this work, we propose Auto-Encoding Adversarial Imitation Learning (AEAIL), a robust and scalable AIL framework. To induce expert policies from demonstrations, AEAIL utilizes the reconstruction error of an auto-encoder as a reward signal, which provides more information for optimizing policies than the prior discriminator-based ones. Subsequently, we use the derived objective functions to train the auto-encoder and the agent policy. Experiments show that our AEAIL performs superior compared to state-of-the-art methods on both state and image based environments. More importantly, AEAIL shows much better robustness when the expert demonstrations are noisy.},
  archive   = {C_AAMAS},
  author    = {Zhang, Kaifeng and Zhao, Rui and Zhang, Ziming and Gao, Yang},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2591–2593},
  title     = {Auto-encoding adversarial imitation learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663237},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bellman momentum on deep reinforcement learning.
<em>AAMAS</em>, 2588–2590. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The sable point may pretend to be optimal and will further degrade the asymptotical performance of the whole training task.We try to solve this problem by seeking more aspects to prepare effective policy regularization, which will provide better policy exploration when faced with suboptimal stable points.As we know, the action is a multidimensional vector with each element as a random variable, so their probabilities compose a vector that can indicate some direction, which is exactly the information we can utilize.},
  archive   = {C_AAMAS},
  author    = {Zhang, Huihui},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2588–2590},
  title     = {Bellman momentum on deep reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663236},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PADDLE: Logic program guided policy reuse in deep
reinforcement learning. <em>AAMAS</em>, 2585–2587. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning new skills through previous experience is regular in human life, which is the core idea of Transfer Reinforcement Learning (TRL). TRL requires the agent to learn when and which source policy is the best to reuse as the target task&#39;s policy and how to reuse the source policy. Most TRL methods learn, transfer, and reuse black-box policies, which is hard to explain: 1) when to reuse, 2) which source policy is effective, and reduces transfer efficiency. In this paper, we propose a novel TRL method called ProgrAm gui DeD poLicy rEuse (PADDLE). PADDLE can measure the logic similarities between tasks and transfer knowledge which reflects the logic behind the target task. To achieve this, we propose a hybrid decision model that synthesizes high-level logic programs and learns low-level DRL policy to learn source tasks. Second, we propose a transferability metric that can measure the logic similarity between the target task and source tasks. Last, we combine it with the low-level policy similarity to select the appropriate source policy as the guiding policy for the target task. Experimental results show that PADDLE can effectively select the appropriate source tasks to guide learning on the target task, outperforming black-box TRL methods.},
  archive   = {C_AAMAS},
  author    = {Zhang, Hao and Yang, Tianpei and Zheng, Yan and Hao, Jianye and Taylor, Matthew E.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2585–2587},
  title     = {PADDLE: Logic program guided policy reuse in deep reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663235},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MATLight: Traffic signal coordinated control algorithm based
on heterogeneous-agent mirror learning with transformer. <em>AAMAS</em>,
2582–2584. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In order to better handle the issue of real-time multi-intersection traffic signal coordinated control, we expect that multi-agent decision-making can benefit from the advantages of large sequence models. In this paper, we propose a method for multi-intersection traffic signal coordinated control based on heterogeneous-agent mirror learning and Transformer to sequential multi-agent cooperative decision. First, multi-intersection traffic signal control is modeled as a sequential problem based on the heterogeneous-agent mirror learning framework. We convert real-time multi-intersection traffic signal control into a multi-agent sequential decision-making process. It completely capitalizes on the surprising connection between the multi-agent reinforcement learning decision process and sequential model prediction. And it provides strong theoretical guarantees. Then the Transformer sequence model is used to cleverly implement the sequential update scheme to learn the optimal traffic signal coordination control strategy online with a new training paradigm. The proposed method has theoretical policy promotion and convergence, alleviates the credit assignment problem in the process of multi-intersection traffic signal coordinated control, reduces the complexity of the joint policy optimization, and improves the learning efficiency of few-shot samples. We used LibSignal, a unified framework for traffic signal control tasks, for comparison testing. According to experimental results, our method can significantly improve the efficiency and performance of few-shot online learning, outperform the baseline methods in both network-level and arterial coordination, and simplify the complexity of algorithm implementation.},
  archive   = {C_AAMAS},
  author    = {Zhang, Haipeng and Wang, Zhiwen and Li, Na},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2582–2584},
  title     = {MATLight: Traffic signal coordinated control algorithm based on heterogeneous-agent mirror learning with transformer},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663234},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Overview of t-DGR: A trajectory-based deep generative replay
method for continual learning in decision making. <em>AAMAS</em>,
2579–2581. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep generative replay has emerged as a promising approach for continual learning in decision-making tasks. This approach addresses the problem of catastrophic forgetting by leveraging the generation of trajectories from previously encountered tasks to augment the current dataset. However, existing deep generative replay methods for continual learning rely on autoregressive models, which suffer from compounding errors in the generated trajectories. In this extended abstract, we summarize a simple, scalable, and non-autoregressive method for continual learning in decision-making tasks using a generative model that generates task samples conditioned on the trajectory timestep. We evaluate our method on Continual World benchmarks and find that our approach achieves state-of-the-art performance on the average success rate metric among continual learning methods. Code and a preprint of a complete paper with full details are available at https://github.com/WilliamYue37/t-DGR.},
  archive   = {C_AAMAS},
  author    = {Yue, William and Liu, Bo and Stone, Peter},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2579–2581},
  title     = {Overview of t-DGR: A trajectory-based deep generative replay method for continual learning in decision making},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663233},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Solving offline 3D bin packing problem with large-sized bin
via two-stage deep reinforcement learning. <em>AAMAS</em>, 2576–2578.
(<a href="https://dl.acm.org/doi/10.5555/3635637.3663232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing Deep Reinforcement Learning (DRL) algorithms address the 3D Bin Packing Problem (3D-BPP) by decomposing the packing action into three sub-stages. However, this three-stage scheme makes it necessary for information to be passed between sub-networks, which may increase the computational cost of training and inference. This paper proposes a two-stage DRL algorithm, combining index and orientation into a single sub-stage to simplify learning. Additionally, a Bidirectional Cooperative Packing (BCP) method is introduced to compress the action space during position selection while retaining exploration capability. The experimental results show that the two-stage DRL algorithm, which incorporates BCP, achieves 0.3\%-1.7\% improvement in space utilization compared to the currently best-performing algorithm.},
  archive   = {C_AAMAS},
  author    = {Yin, Hao and Chen, Fan and He, Hongjie},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2576–2578},
  title     = {Solving offline 3D bin packing problem with large-sized bin via two-stage deep reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663232},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward socially friendly autonomous driving using
multi-agent deep reinforcement learning. <em>AAMAS</em>, 2573–2575. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We develop a novel multi-agent driving simulation framework (SFDPO) so that socially friendly driving behaviors can be acquired by agents through multi-agent reinforcement learning. We model personal and social driving behaviors in the driver model to reflect human driving goals and preferences. We make a game-theoretic assumption on fair compromised solution concepts to find an equilibrium solution under conflicts in complex interactive scenarios. A meta-policy optimization method is adopted to leverage personal and social driving behaviors in terms of personalized loss and socialized loss to achieve a balanced Pareto optimal solution between the socially friendly and personal preference driving goals.},
  archive   = {C_AAMAS},
  author    = {Yeh, Jhih-Ching and Soo, Von-Wun},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2573–2575},
  title     = {Toward socially friendly autonomous driving using multi-agent deep reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663231},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual role AoI-based incentive mechanism for HD map
crowdsourcing. <em>AAMAS</em>, 2570–2572. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A high-quality fresh high-definition (HD) map is vital in enhancing transportation efficiency and safety in autonomous driving. Vehicle-based crowdsourcing offers a promising approach for updating HD maps. However, recruiting crowdsourcing vehicles involves making the challenging tradeoff between the HD map freshness and recruitment cost. Existing studies on HD map crowdsourcing often (1) prioritize maximizing spatial coverage, and (2) overlook the dual role of crowdsourcing vehicles in HD maps, as vehicles serve both as contributors and customers of HD maps. This motivates us to propose the Dual-Role Age of Information (AoI) based Incentive Mechanism (DRAIM) to address these issues. DRAIM aims to achieve the company&#39;s tradeoff between freshness and recruitment cost.},
  archive   = {C_AAMAS},
  author    = {Ye, Wentao and Liu, Bo and Luo, Yuan and Huang, Jianwei},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2570–2572},
  title     = {Dual role AoI-based incentive mechanism for HD map crowdsourcing},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663230},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the complexity of candidates-embedded multiwinner voting
under the hausdorff function. <em>AAMAS</em>, 2567–2569. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study candidates-embedded approval-based multiwinner voting. In this model, we are given a metric undefined on the set of candidates, and voters are free to approve or disapprove any candidates. The task is to select a k-committee that either minimizes the sum of distances from the committee to all votes (utilitarian rules) or minimizes the maximum distance from the committee to any vote (egalitarian rules). The distance from a committee to a vote is measured by certain set-to-set functions derived from undefined. Previous works have considered the min, the max, and the sum functions. This paper examines the Hausdorff function. We show that in general computing winners under the Hausdorff function is hard, but we also derive several polynomial-time algorithms for certain special cases.},
  archive   = {C_AAMAS},
  author    = {Yang, Yongjie},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2567–2569},
  title     = {On the complexity of candidates-embedded multiwinner voting under the hausdorff function},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663229},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Truthful and stable one-sided matching on networks.
<em>AAMAS</em>, 2564–2566. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Diffusion one-sided matching aims at incentivizing more participants to match so as to improve overall matching result. Existing works have tried to add constraints on Top Trading Cycles to obtain the incentive, but it only works in trees. In this paper, we first propose a mechanism named Swap With Neighbors (SWN), which can work in any graph structure and intuitively satisfy incentive compatibility and the tightest stability (first defined here) in the new setting. Then we find a natural improvement of SWN called Leave and Share which not only reaches the same properties as SWN but also provides an obvious efficiency difference.},
  archive   = {C_AAMAS},
  author    = {Yang, Tianyi and Zhai, Yuxiang and Zhao, Dengji and Song, Xinwei and Li, Miao},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2564–2566},
  title     = {Truthful and stable one-sided matching on networks},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663228},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Population-aware online mirror descent for mean-field games
by deep reinforcement learning. <em>AAMAS</em>, 2561–2563. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mean Field Games (MFGs) have the ability to handle large-scale multi-agent systems, but learning Nash equilibria in MFGs remains a challenging task. In this paper, we propose a deep reinforcement learning (DRL) algorithm that achieves population-dependent Nash equilibrium without the need for averaging or sampling from history, inspired by Munchausen RL and Online Mirror Descent. Through the design of an additional inner-loop replay buffer, the agents can effectively learn to achieve Nash equilibrium from any distribution, mitigating catastrophic forgetting. The resulting policy can be applied to various initial distributions. Numerical experiments on four canonical examples demonstrate our algorithm has better convergence properties than SOTA algorithms, in particular a DRL version of Fictitious Play for population-dependent policies.},
  archive   = {C_AAMAS},
  author    = {Wu, Zida and Lauri\`{e}re, Mathieu and Chua, Samuel Jia Cong and Geist, Matthieu and Pietquin, Olivier and Mehta, Ankur},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2561–2563},
  title     = {Population-aware online mirror descent for mean-field games by deep reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663227},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Competitive analysis of online facility open problem.
<em>AAMAS</em>, 2558–2560. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate an online cost minimization problem of serving requests in a tree of facilities, referred to as the Online Facility Open Problem (Online FOP). To address this problem, we propose the Anchor-Barrier Algorithm (ABA), a threshold-based algorithm applicable to any tree and any cost assignment, which can work in a distributed manner for scalability. We conduct the competitive analysis and show that ABA&#39;s achieves the optimal competitive ratio Height + 2, where Height is the height of the facility tree.},
  archive   = {C_AAMAS},
  author    = {Wu, Binghan and Bao, Wei and Zhou, Bing},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2558–2560},
  title     = {Competitive analysis of online facility open problem},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663226},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Potential games on cubic splines for multi-agent motion
planning of autonomous agents. <em>AAMAS</em>, 2555–2557. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an algorithm to solve for local Nash Equilibrium trajectories in the multi-agent motion planning problem for self-interested agents. Our method models the problem as a concurrent game where each agent&#39;s action consists of choosing a cubic spline defined by a set of waypoints. We observe that with certain kinds of cost functions, the resulting game has the structure of a potential game which is guaranteed to reach an equilibrium even when each agent myopically improves their own cost without considering the costs of other agents. Our algorithm uses simultaneous gradient descent with independent per-agent step sizes to converge to local Nash Equilibrium trajectories. We demonstrate the algorithm can scale to very long horizons through simulated experiments in the electric vertical take-off and landing vehicles (eVTOL) domain.},
  archive   = {C_AAMAS},
  author    = {Williams, Sam and Deshmukh, Jyotirmoy},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2555–2557},
  title     = {Potential games on cubic splines for multi-agent motion planning of autonomous agents},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663225},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforcement nash equilibrium solver. <em>AAMAS</em>,
2552–2554. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Nash Equilibrium (NE) is the canonical solution concept of game theory, which provides an elegant tool to understand the rationalities. Computing NE in two- or multi-player general-sum games is PPAD-Complete. Therefore, in this work, we propose REinforcement Nash Equilibrium Solver (RENES), which trains a single policy to modify the games with different sizes and applies the solvers on the modified games where the obtained solution is evaluated on the original games. Specifically, our contributions are threefold. i) We represent the games as α-rank response graphs and leverage graph neural network (GNN) to handle the games with different sizes as inputs; ii) We use tensor decomposition, e.g., canonical polyadic (CP), to make the dimension of modifying actions fixed for games with different sizes; iii) We train the modifying strategy for games with the widely-used proximal policy optimization (PPO) and apply the solvers to solve the modified games, where the obtained solution is evaluated on original games. Extensive experiments on large-scale normal-form games show that our method can further improve the approximation of NE of different solvers, i.e., α-rank, CE, FP and PRD, and can be generalized to unseen games.},
  archive   = {C_AAMAS},
  author    = {Wang, Xinrun and Yang, Chang and Li, Shuxin and Li, Pengdeng and Huang, Xiao and Chan, Hau and An, Bo},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2552–2554},
  title     = {Reinforcement nash equilibrium solver},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663224},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decision market based learning for multi-agent contextual
bandit problems. <em>AAMAS</em>, 2549–2551. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Information is often stored in a distributed and proprietary form, and agents who own this information are often self-interested and require incentives to reveal it. Suitable mechanisms are required to elicit and aggregate such distributed information for decision-making. In this study, we use simulations to investigate the use of decision markets as mechanisms in a multi-agent learning system to aggregate distributed information for decision-making in a contextual bandit problem.},
  archive   = {C_AAMAS},
  author    = {Wang, Wenlong and Pfeiffer, Thomas},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2549–2551},
  title     = {Decision market based learning for multi-agent contextual bandit problems},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663223},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the utility of external agent intention predictor for
human-AI coordination. <em>AAMAS</em>, 2546–2548. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reaching a consensus on the team plans is vital to human-AI coordination. We suggest incorporating external models to assist humans in understanding the intentions of AI agents when the AI has no explainable plan to communicate. In this paper, we propose a two-stage paradigm that first trains a Theory of Mind (ToM) model from collected offline trajectories of the target agent and utilizes the model in the process of human-AI collaboration by real-timely displaying the future action predictions of the target agent. We further implement a transformer-based predictor as the ToM model and develop an extended online human-AI collaboration platform for experiments. Experimental results validate that our ToM model can significantly improve team performance, demonstrating the potential of our paradigm in human-AI collaboration.},
  archive   = {C_AAMAS},
  author    = {Wang, Chenxu and Chen, Zilong and Liu, Huaping},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2546–2548},
  title     = {On the utility of external agent intention predictor for human-AI coordination},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663222},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Detecting anomalous agent decision sequences based on
offline imitation learning. <em>AAMAS</em>, 2543–2545. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Anomaly detection in decision-making sequences is a challenging problem due to the complexity of normality representation learning, the sequential nature of the task and the difficulty of real-world implementation. In this work, we propose extracting two behaviour features: action optimality and sequential association to detect anomalous behaviour. Our offline imitation learning model is an adaptation of behavioural cloning with a transformer policy network, where we modify the training process to learn a Q function and a state value function from normal trajectories.},
  archive   = {C_AAMAS},
  author    = {Wang, Chen and Erfani, Sarah and Alpcan, Tansu and Leckie, Christopher},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2543–2545},
  title     = {Detecting anomalous agent decision sequences based on offline imitation learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663221},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Clique analysis and bypassing in continuous-time
conflict-based search. <em>AAMAS</em>, 2540–2542. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study symmetry-breaking enhancements for Continuous-Time Conflict-Based Search (CCBS), a solver for continuous-time MAPF. Resolving conflict symmetries in MAPF can require an exponential amount of work. We adapt known symmetry-breaking enhancements from unit-cost domains for CCBS. We then improve upon these to produce a new state of the art algorithm: CCBS with disjoint k-partite cliques (CCBS+DK). Finally, we show empirically that CCBS+DK solves for up to 20\% more agents in the same amount of time when compared to previous state of the art.},
  archive   = {C_AAMAS},
  author    = {Walker, Thayne T. and Sturtevant, Nathan R. and Felner, Ariel},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2540–2542},
  title     = {Clique analysis and bypassing in continuous-time conflict-based search},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663220},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explaining sequences of actions in multi-agent deep
reinforcement learning models. <em>AAMAS</em>, 2537–2539. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a method to explain MADRL agents&#39; behaviors by abstracting their actions into high-level strategies. Particularly, a spatio-temporal neural network model is applied to encode the agents&#39; sequences of actions as memory episodes wherein an aggregating memory retrieval can generalize them into a concise abstract representation of collective strategies. To assess the effectiveness of our method, we applied it to explain the actions of QMIX MADRL agents playing a StarCraft Multi-agent Challenge (SMAC) video game. A user study on the perceived explainability of the extracted strategies indicates that our method can provide comprehensible explanations at various levels of granularity.},
  archive   = {C_AAMAS},
  author    = {Wai, Khaing Phyo and Geng, Minghong and Pateria, Shubham and Subagdja, Budhitama and Tan, Ah-Hwee},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2537–2539},
  title     = {Explaining sequences of actions in multi-agent deep reinforcement learning models},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663219},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the existence of EFX under picky or non-differentiative
agents. <em>AAMAS</em>, 2534–2536. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we consider the fair division of indivisible goods under arguably the strongest envy-based fairness notion of envy-free up to any item (EFX). Extending the long line of work on special cases of additive valuations, we show existence of EFX for the following two cases: (i) instances where agents are very picky, i.e., each agent likes at most four items positively. (ii) ternary instances where the value of an agent for an item is 0, a, or b for 0 &amp;lt; a &amp;lt; b ≤ 2a. In both cases, the existence is shown by designing an efficient algorithm to find an EFX allocation.},
  archive   = {C_AAMAS},
  author    = {Viswanathan, Maya and Mehta, Ruta},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2534–2536},
  title     = {On the existence of EFX under picky or non-differentiative agents},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663218},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Understanding the impact of promotions on consumer behavior.
<em>AAMAS</em>, 2531–2533. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Marketing is a complex tool companies use to publicize new products and build consumer loyalty. However, cost-effective understanding and prediction of marketing campaign influence on consumers&#39; behavior are necessary to maintain an effective business strategy. To understand the impact of the diversity of profiles and human behaviors, it is necessary to supplement aggregated solutions with the design of granular, individual-centered Agent-Based Models suitable for describing behavioral diversity. In this article, we propose a new model that reproduces customer loyalty as an emergent phenomenon while also demonstrating the effects of price wars on consumer loyalty. The model facilitates measuring the increase in sales during discounts, the drop in competitors&#39; sales, the negative effects of discount repetition and also complex phenomenon as decoy effect. Introducing a new product, a &quot;decoy&quot;, in a competitive category can raise the sales of an existing product.},
  archive   = {C_AAMAS},
  author    = {Vanderlynden, Jarod and Mathieu, Philippe and Warlop, Romain},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2531–2533},
  title     = {Understanding the impact of promotions on consumer behavior},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663217},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian ensembles for exploration in deep q-learning.
<em>AAMAS</em>, 2528–2530. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Exploration in reinforcement learning remains a difficult challenge. In order to drive exploration, ensembles with randomized prior functions have recently been popularized to quantify uncertainty in the value model. There is no theoretical reason for these ensembles to resemble the actual posterior, however. In this work, we view training ensembles from the perspective of Sequential Monte Carlo, a Monte Carlo method that approximates a sequence of distributions with a set of particles. In particular, we propose an algorithm that exploits both the practical flexibility of ensembles and theory of the Bayesian paradigm. We incorporate this method into a standard Deep Q-learning agent (DQN) and experimentally show qualitatively good uncertainty quantification and improved exploration capabilities over a regular ensemble.},
  archive   = {C_AAMAS},
  author    = {van der Vaart, Pascal R. and Yorke-Smith, Neil and Spaan, Matthijs T. J.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2528–2530},
  title     = {Bayesian ensembles for exploration in deep Q-learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663216},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Embracing relational reasoning in multi-agent actor-critic.
<em>AAMAS</em>, 2525–2527. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Relational reasoning has become an important concept in machine learning and has seen notable progress in its methods like graph neural networks, which highlight the value of capturing intricate relational patterns. While it has shown promise in single-agent reinforcement learning, its potential in the multi-agent landscape remains largely uncharted. Our work aims to bridge this gap, demonstrating the advantages of integrating deep relational learning into multi-agent reinforcement learning. We do so by introducing an actor-critic architecture for centralized learning and decentralized execution that uses relational graph neural networks to imbue a spatial inductive bias. Empirical results highlight improved sample efficiency and asymptotic performance against strong baselines in cooperative tasks with significant spatial complexity.},
  archive   = {C_AAMAS},
  author    = {Utke, Sharlin and Houssineau, Jeremie and Montana, Giovanni},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2525–2527},
  title     = {Embracing relational reasoning in multi-agent actor-critic},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663215},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint intrinsic motivation for coordinated exploration in
multi-agent deep reinforcement learning. <em>AAMAS</em>, 2522–2524. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent deep reinforcement learning (MADRL) often struggles to learn strongly coordinated tasks, as performance depends not only on one agent&#39;s behavior but rather on the joint behavior of multiple agents. In this context, a group of agents can benefit from actively exploring different joint strategies to determine the most efficient one. In this paper, we propose an approach for rewarding strategies where agents collectively exhibit novel behaviors. We present JIM (Joint Intrinsic Motivation), a multi-agent intrinsic motivation method that rewards joint trajectories based on a centralized measure of novelty. We show how JIM can be used to improve state-of-the-art MADRL methods in a highly coordinated task, demonstrating the crucial role of coordinated exploration.},
  archive   = {C_AAMAS},
  author    = {Toquebiau, Maxime and Bredeche, Nicolas and Benamar, Fa\&quot;{\i}z and Jun, Jae-Yun},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2522–2524},
  title     = {Joint intrinsic motivation for coordinated exploration in multi-agent deep reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663214},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reducing systemic risk in financial networks through
donations. <em>AAMAS</em>, 2519–2521. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We examine the extent to which rescue strategies within a banking system can reduce systemic risk. We focus on donations from solvent banks to banks in distress, which can in principle reduce losses and prevent default cascades. We build an agent-based model to simulate the ensuing strategic game on a randomly generated financial network, where nodes represent banks and edges represent interbank liabilities. Each bank independently decides whether to rescue (and whom) to maximise their payoffs. We analyse the rescue strategies adopted by the banks at equilibrium, using empirical game-theoretic analysis. Our results show that donations can indeed reduce systemic risk when the equilibrium strategy profile is adopted. Individual donations can benefit multiple banks in the network. Our results also indicate that lower default costs and small-variance liabilities tend to decrease the incentives to donate. We furthermore examine the impact of the banks&#39; rationality on the effects of rescue, finding that banks behaving rationally use their funds for rescues more efficiently than banks that behave irrationally.},
  archive   = {C_AAMAS},
  author    = {Tong, Jinyun and De Keijzer, Bart and Ventre, Carmine},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2519–2521},
  title     = {Reducing systemic risk in financial networks through donations},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663213},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Consensus of nonlinear multi-agent systems with semi-markov
switching under DoS attacks. <em>AAMAS</em>, 2516–2518. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Denial of Service (DoS) attacks will destroy the communication channels between agents. How to reduce the impact of DoS attacks onsystem consensus of nonlinear multi-agent systems (MASs) with semi-Markov switching (SMS) is an important problem that has appeared in many applications. Existing work on consensus of nonlinear MAS with switching under DoS attacks can be divided into two categories: Markov switching (MS) and Semi-Markov Switching (SMS). There are many studies on MS, but very few on SMS. This is because the dwell time of the SMS obeys a more general probability distribution, which will bring new challenges to analysis. This paper proposes a novel approach that adopts a dynamic event-triggered strategy to reduce the frequency of control signals to complete the consensus on nonlinear MASs with SMS under DoS attacks. We use multiple Lyapunov functions established by stochastic techniques, and obtain sufficient conditions for MAS mean square consensus under aperiodic DoS attacks. The effectiveness of our strategy is verified by simulation results.},
  archive   = {C_AAMAS},
  author    = {Tian, Sheng and Shen, Hong and Tian, Yuan and Tian, Hui},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2516–2518},
  title     = {Consensus of nonlinear multi-agent systems with semi-markov switching under DoS attacks},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663212},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Game transformations that preserve nash equilibria or
best-response sets. <em>AAMAS</em>, 2513–2515. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the full version of this paper, we investigate under which conditions normal-form games are (guaranteed) to be strategically equivalent. First, we show for N-player games (N ≥ 3) that (a) it is NP-hard to decide whether a given strategy is a best response to some strategy profile of the opponents, and that (b) it is co-NP-hard to decide whether two games have the same best-response sets.We then turn our attention to equivalence-preserving game transformations. It is a widely used fact that a positive affine (linear) transformation of the utility payoffs neither changes the best-response sets nor the Nash equilibrium set. We investigate which other game transformations also possess either of the following two properties when being applied to an arbitrary N-player game (N ≥ 2): (i) The Nash equilibrium set stays the same; (ii) The best-response sets stay the same.For game transformations that operate player-wise and strategy-wise, we prove that (i) implies (ii) and that transformations with property (ii) must be positive affine. The resulting equivalence chain highlights the special status of positive affine transformations among all the transformation procedures that preserve key game-theoretic characteristics.},
  archive   = {C_AAMAS},
  author    = {Tewolde, Emanuel and Conitzer, Vincent},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2513–2515},
  title     = {Game transformations that preserve nash equilibria or best-response sets},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663211},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Persuasion by shaping beliefs about multidimensional
features of a thing. <em>AAMAS</em>, 2510–2512. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Research has demonstrated the effectiveness of personalization in persuasive agents, recommendation agents, and nudge agents. Ultimate personalization targets the presentation of information tailored to an individual&#39;s nuanced beliefs and utilities, rather than relying on broad attributes such as personality traits, age, or gender. Multi-attribute utility theory suggests that the utility of a thing is determined by the sum of the utilities given to its various features. In our research, we developed a method to enhance the personal utility of a thing by addressing and manipulating people&#39;s beliefs about the features of a thing. We conducted an experiment (n=197) to verify whether the proposed method can increase the participants&#39; utility of a fully autonomous vehicle, as a target of persuasion. Among 13 propositions (features) that constitute the concept of fully autonomous vehicles, in a semi-structured dialog, a virtual agent presented counter-propositions to the top propositions that each participant assigned the most negative utilities. Before and after the dialog, the monetary value of fully autonomous vehicles, the desire to ride them, and the social obligation to accept them were measured. The results showed that the proposed method improved the social obligation to accept fully autonomous vehicles more than the baseline method and the non-personalized method, but had no effect on the monetary value and the desire to ride them. This suggests that personalized belief manipulation may not be effective in enhancing the &quot;want to&quot; desire or utility of a thing, but may only improve the thought of &quot;ought to do&quot;.},
  archive   = {C_AAMAS},
  author    = {Terada, Kazunori and Noma, Yasuo and Hattori, Masanori},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2510–2512},
  title     = {Persuasion by shaping beliefs about multidimensional features of a thing},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663210},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unifying regret and state-action space coverage for
effective unsupervised environment design. <em>AAMAS</em>, 2507–2509.
(<a href="https://dl.acm.org/doi/10.5555/3635637.3663209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unsupervised Environment Design (UED) employs interactive training between a teacher agent and a student agent to train generally-capable student agents. Existing UED methods primarily rely on regret to progressively introduce curriculum complexity for the student but often overlook the importance of environment novelty - a critical element for enhancing an agent&#39;s exploration and generalization capabilities. There is a substantial lack of investigating the effects of environment novelty in UED. This paper addresses this gap by introducing the GMM-based Evaluation of Novelty In Environments (GENIE) framework. GENIE quantifies environment novelty within the UED paradigm by using Gaussian Mixture Models. To assess GENIE&#39;s effectiveness in quantifying novelty and driving exploration, we integrate it with ACCEL, the state-of-the-art UED algorithm. Empirical results demonstrate the superior zero-shot performance of this extended approach over existing UED algorithms, including its predecessor. By providing a means to quantify environment novelty, GENIE lays the groundwork for future UED algorithms to unify novelty-driven exploration and regret-driven exploitation in curriculum generation.},
  archive   = {C_AAMAS},
  author    = {Teoh Jing Xiang, Jayden and Li, Wenjun and Varakantham, Pradeep},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2507–2509},
  title     = {Unifying regret and state-action space coverage for effective unsupervised environment design},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663209},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neurological based timing mechanism for reinforcement
learning. <em>AAMAS</em>, 2504–2506. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The inherently time-dependent dynamics which underly the neuronal spiking communication, are ubquitous throughout brain, and yet are not fully understood. Likewise time-based mechanisms are underdeveloped in the field of Machine and Reinforcement Learning (RL) [7]. The complexity-rich and multi-dimensional dynamics observed in the brain offer potential advancements in Machine Learning (ML), and development of Artificial Generalized Intelligence.It is in our interests to model known time-mechanisms of neuronal spiking communication, and reproduce the emergent properties of complex timing and learning in assemblies. If neuronal temporal dynamics can be understood, a new field of possibilities will open for in-situ models which learn in complex real-time environments. A key challenge for these models is correctly identifying associations of actions and stimulus at variable time separations. In this article, we bring the flexible time representation mechanisms from neuroscience to the field of automata and RL, to explore its potential.},
  archive   = {C_AAMAS},
  author    = {Tarlton, Michael J. and Mello, Gustavo B. and Yazidi, Anis},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2504–2506},
  title     = {Neurological based timing mechanism for reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663208},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fuzzy clustered federated learning under mixed data
distributions. <em>AAMAS</em>, 2501–2503. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Federated learning (FL) is deeply troubled by non-independent and identically distributed (non-IID) data, leading to suboptimal training results. Clustered FL partitions clients&#39; unique data into different clusters to reduce the heterogeneity among clients. Current approaches are unable to eliminate the impact of data heterogeneity and provide personalized models to client devices. By assuming the clients&#39; data can be divided into different data distributions, we propose a novel fuzzy clustered FL method. We partition the client&#39;s data and generate a personalized model for each client. The experiments demonstrated that our method achieved excellent results. In the case of N clusters, our method achieved a communication cost reduction of 1/N compared to the SOTA methods, while improving performance by 10.4\% on CIFAR-10.},
  archive   = {C_AAMAS},
  author    = {Tang, Peng and Wang, Lifan and Qiu, Weidong and Huang, Zheng and Wang, Qiangmin},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2501–2503},
  title     = {Fuzzy clustered federated learning under mixed data distributions},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663207},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HiMAP: Learning heuristics-informed policies for large-scale
multi-agent pathfinding. <em>AAMAS</em>, 2498–2500. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large-scale multi-agent pathfinding (MAPF) presents significant challenges in several areas. As systems grow in complexity with a multitude of autonomous agents operating simultaneously, efficient and collision-free coordination becomes paramount. Traditional algorithms often fall short in scalability, especially in intricate scenarios. Reinforcement Learning (RL) has shown potential to address the intricacies of MAPF; however, it has also been shown to struggle with scalability, demanding intricate implementation, lengthy training, and often exhibiting unstable convergence, limiting its practical application. In this paper, we introduce Heuristics-Informed Multi-Agent Pathfinding (HiMAP), a novel scalable approach that employs imitation learning with heuristic guidance in a decentralized manner. We train on small-scale instances using a heuristic policy as a teacher that maps each single agent observation information to an action probability distribution. During pathfinding, we adopt several inference techniques to improve performance. With a simple training scheme and implementation, HiMAP demonstrates competitive results in terms of success rate and scalability in the field of imitation-learning-only MAPF, showing the potential of imitation-learning-only MAPF equipped with inference techniques.},
  archive   = {C_AAMAS},
  author    = {Tang, Huijie and Berto, Federico and Ma, Zihan and Hua, Chuanbo and Ahn, Kyuree and Park, Jinkyoo},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2498–2500},
  title     = {HiMAP: Learning heuristics-informed policies for large-scale multi-agent pathfinding},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663206},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pure nash equilibria in weighted congestion games with
complementarities and beyond. <em>AAMAS</em>, 2495–2497. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Congestion games offer a primary model of non-cooperative games, and a number of generalizations have been proposed, such as weighted congestion games, congestion games with mixed costs, and congestion games with complementarities. Our main contribution is a proof of the existence of pure Nash equilibria in weighted matroid congestion games with complementarities and their further generalization, under a simplified assumption. Some extensions of previous results on congestion games with mixed costs are also presented.},
  archive   = {C_AAMAS},
  author    = {Takazawa, Kenjiro},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2495–2497},
  title     = {Pure nash equilibria in weighted congestion games with complementarities and beyond},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663205},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Efficient size-based hybrid algorithm for optimal coalition
structure generation. <em>AAMAS</em>, 2492–2494. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Coalition Structure Generation (CSG) involves dividing agents into coalitions in such a way as to coordinate them into solving problems together efficiently. In this paper, we revisit the CSG problem and propose a new search method that introduces an offline phase to speed up the search process, where the best coalition sets to search are preprocessed. These sets are calculated only once regardless of the coalition values and can be reused each time a CSG instance is to be solved. Then our search in the online phase combines dynamic programming with integer partition-based search in a novel way.},
  archive   = {C_AAMAS},
  author    = {Taguelmimt, Redha and Aknine, Samir and Boukredera, Djamila and Changder, Narayan and Sandholm, Tuomas},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2492–2494},
  title     = {Efficient size-based hybrid algorithm for optimal coalition structure generation},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663204},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A multiagent path search algorithm for large-scale
coalition structure generation. <em>AAMAS</em>, 2489–2491. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Coalition structure generation (CSG) is a critical problem in multiagent systems, involving the optimal partitioning of agents into disjoint coalitions to maximize social welfare. This paper introduces SALDAE, a novel multiagent path finding algorithm for CSG on a coalition structure graph. SALDAE employs various heuristics and strategies for efficient search, making it an anytime algorithm suitable for handling large-scale problems.},
  archive   = {C_AAMAS},
  author    = {Taguelmimt, Redha and Aknine, Samir and Boukredera, Djamila and Changder, Narayan and Sandholm, Tuomas},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2489–2491},
  title     = {A multiagent path search algorithm for large-scale coalition structure generation},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663203},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ethical markov decision processes with moral worth as
rewards. <em>AAMAS</em>, 2486–2488. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose an expressive framework for specifying ethical behaviours, called Ethical Markov Decision Processes (E-MDPs) that extends classical MDPs with the explicit representation of moral values - positive or negative - that the agent&#39;s decisions may promote or demote.},
  archive   = {C_AAMAS},
  author    = {Stojanovski, Mihail and Bourdache, Nadjet and Bonnet, Gr\&#39;{e}gory and Mouaddib, Abdel-Illah},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2486–2488},
  title     = {Ethical markov decision processes with moral worth as rewards},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663202},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decent-BRM: Decentralization through block reward
mechanisms. <em>AAMAS</em>, 2483–2485. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Proof-of-Work (PoW) is a consensus algorithm where miners solve cryptographic puzzles to mine blocks and obtain the reward specified through the underlying blockchain&#39;s Block Reward Mechanism (BRM). Rewards from mining the block have a high associated risk (variance). Miners form mining pools to reduce this risk. The formation of mining pools leads to centralization in PoW blockchains. We study the role of BRMs in forming mining pools and propose a novel BRM that disincentivizes the formation of mining pools. For our analysis, we model the system as a two-player game between (1) an incoming miner and (2) the existing PoW blockchain system.We categorize BRMs into (a) Memoryless - history independent and (b) Retentive BRMs. We show the impossibility of designing a Memoryless BRM that disincentivizes mining pool formation. We propose a novel retentive BRM - Decent-BRM, which incentivizes incoming miners to perform solo mining (leading to a decentralized PoW blockchain) over forming mining pools.},
  archive   = {C_AAMAS},
  author    = {Srivastava, Varul and Gujar, Sujit},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2483–2485},
  title     = {Decent-BRM: Decentralization through block reward mechanisms},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663201},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid participatory budgeting: Divisible, indivisible, and
beyond. <em>AAMAS</em>, 2480–2482. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Participatory budgeting (PB) has been receiving significant attention lately both in theory and practice. PB is broadly classified into two categories: divisible PB and indivisible PB. Divisible PB imposes no constraint on the amount allocated to each project, whereas the indivisible PB assumes that each project is associated with a cost and the project must either be funded in full or not funded. In this work, we propose a rich PB model that encompasses many settings of PB as special cases. Some of such settings include the case where some projects are divisible and some are indivisible and the case where the cost of each project may belong to a continuum range of values. We propose various welfare and fairness objectives and verify the computational complexity of each of them. We prove experimentally that even the computationally hard objectives become tractable in practice. Also, we propose greedy approximation algorithms for such objectives and prove that our algorithms achieve nearly optimal solutions on real world PB datasets.},
  archive   = {C_AAMAS},
  author    = {Sreedurga, Gogulapati},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2480–2482},
  title     = {Hybrid participatory budgeting: Divisible, indivisible, and beyond},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663200},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unlocking the potential of machine ethics with
explainability. <em>AAMAS</em>, 2477–2479. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Roughly speaking, the research field of machine ethics deals with devising behavioral constraints on computational systems to ensure restricted, morally acceptable behavior. The potential benefits of researching machine ethics are substantial, encompassing contributions to ethical AI development and the societal impact of computational systems. However, there are genuine concerns and risks associated with this research (e.g., the potential to undermine human autonomy) that must be carefully considered.In this article, we will explore the question of whether it is worthwhile to conduct research in machine ethics, given the potential demerits and challenges involved. Central to our study is the proposition that explainability, such as is being explored in connection with explainable artificial intelligence (XAI), can serve as a powerful tool to augment the advantages of machine ethics research, mitigate its disadvantages, and create unique advantages of its own. Overall, we conclude that the study of machine ethics is worthwhile, especially when it is supported by research on explainability.},
  archive   = {C_AAMAS},
  author    = {Speith, Timo},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2477–2479},
  title     = {Unlocking the potential of machine ethics with explainability},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663199},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fairness of exposure in online restless multi-armed bandits.
<em>AAMAS</em>, 2474–2476. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Restless multi-armed bandits (RMABs) generalize the multi-armed bandits where each arm exhibits Markovian behavior and transitions according to their transition dynamics. Solutions to RMAB exist for both offline and online cases. However, they do not consider the distribution of pulls among the arms. Studies have shown that optimal policies lead to unfairness, where some arms are not exposed enough. Existing works in fairness in RMABs focus heavily on the offline case, which diminishes their application in real-world scenarios where the environment is largely unknown. In the online scenario, we propose the first fair RMAB framework, where each arm receives pulls in proportion to its merit. We define the merit of an arm as a function of its stationary reward distribution. We prove that our algorithm achieves sublinear fairness regret in the single pull case O(√TIn T), with T being the total number of episodes. Empirically, we show that our algorithm performs well in the multi-pull scenario as well.},
  archive   = {C_AAMAS},
  author    = {Sood, Archit and Jain, Shweta and Gujar, Sujit},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2474–2476},
  title     = {Fairness of exposure in online restless multi-armed bandits},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663198},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fairness and privacy guarantees in federated contextual
bandits. <em>AAMAS</em>, 2471–2473. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies the contextual multi-armed bandit problem with fairness and privacy guarantees in a federated setting. It proposes a collaborative algorithm, Fed-FairX-LinUCB, that achieves sub-linear fairness regret and can be adapted to ensure differential privacy. The key challenge is designing a communication protocol that balances privacy and regret. The proposed protocol achieves both sub-linear fairness regret and effective use of privacy budget. Experiments validates the efficacy of both Fed-FairX-LinUCB and its private counterpart, Priv-FairX-LinUCB.},
  archive   = {C_AAMAS},
  author    = {Solanki, Sambhav and Gujar, Sujit and Jain, Shweta},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2471–2473},
  title     = {Fairness and privacy guarantees in federated contextual bandits},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663197},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fairness and cooperation between independent reinforcement
learners through indirect reciprocity. <em>AAMAS</em>, 2468–2470. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In a multi-agent setting, altruistic cooperation is costly yet socially desirable. As such, agents adapting through independent reinforcement learning struggle to converge to efficient, cooperative policies. Indirect reciprocity (IR) constitutes a possible mechanism to encourage cooperation by introducing reputations, social norms and the possibility that agents reciprocate based on past actions. IR has been mainly studied in homogeneous populations. In this paper, we introduce a model that allows for both reputation and group-based cooperation, and analyse how specific social norms (i.e. rules to assign reputations) can lead to varying levels of cooperation and fairness. We investigate how a finite population of independent Q-learning agents perform under different social norms. We observe that while norms such as Stern-Judging sustain both cooperation and fairness in populations of learning agents, other norms used to judge in- or out-group interactions can lead to unfair outcomes.},
  archive   = {C_AAMAS},
  author    = {Smit, Jacobus and Santos, Fernando P.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2468–2470},
  title     = {Fairness and cooperation between independent reinforcement learners through indirect reciprocity},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663196},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OPEx: A large language model-powered framework for embodied
instruction following. <em>AAMAS</em>, 2465–2467. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Embodied Instruction Following (EIF) is crucial for understanding natural language in a practical context, requiring agents to follow verbal instructions for complex tasks. Traditionally, EIF relies heavily on expert annotations for learning, which are costly and sometimes unattainable. Recent research shows Large Language Models (LLMs) can use their reasoning ability to help in EIF with minimal examples, but applying LLMs directly faces issues like hallucinations and partially observable environment. To bridge the gap, we introduce OPEx, a new LLM-based method for EIF that needs far less specific data. OPEx uses three LLMs for different roles: observing to gather environment data, planning by breaking down instructions, and executing tasks with learned skills. Our tests reveal OPEx significantly outperforms the FILM baseline, with 90\% less training data for planning tasks and achieving up to 38\% performance gain when FILM is trained on identical data.},
  archive   = {C_AAMAS},
  author    = {Shi, Haochen and Sun, Zhiyuan and Yuan, Xingdi and C\^{o}t\&#39;{e}, Marc-Alexandre and Liu, Bang},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2465–2467},
  title     = {OPEx: A large language model-powered framework for embodied instruction following},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663195},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cournot queueing games with applications to mobility
systems. <em>AAMAS</em>, 2462–2464. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce a novel class of non-cooperative games, in which a closed network of queues is shared between multiple players. Each player receives a reward based on the throughput of their jobs, while they incur a cost that varies with the number of jobs they submit. Closed queueing networks are a commonly used stochastic formalism that model a variety of real-world situations, and this paper presents an application to competitive vehicle-sharing systems. In our vehicle-sharing system model, each provider receives revenue from each trip, but pays a cost based on the total number of vehicles. The core technical results of this paper include conditions that guarantee the existence of a Nash equilibrium, and an efficient equilibrium-finding algorithm. We apply this model to a case study of a hypothetical vehicle-sharing system in Oslo. The results indicate that adding a single competitor can increase the number of trips taken by up to 14.1\%, adding two competitors can increase the amount by up to 18.9\%, and a highly competitive market can increase this by up to 30\%.},
  archive   = {C_AAMAS},
  author    = {Sheldon, Matthew and Paccagnan, Dario and Casale, Giuliano},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2462–2464},
  title     = {Cournot queueing games with applications to mobility systems},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663194},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Balanced and incentivized learning with limited shared
information in multi-agent multi-armed bandit. <em>AAMAS</em>,
2459–2461. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent multi-armed bandit (MAMAB) is a classic collaborative learning model and has gained much attention in recent years. However, existing studies do not consider the case where an agent may refuse to share all her information with others, e.g., when some of the data contains personal privacy. In this paper, we propose a novel limited shared information multi-agent multi-armed bandit (LSI-MAMAB) model in which each agent only shares the information that she is willing to share, and propose the Balanced-ETC algorithm to help multiple agents collaborate efficiently with limited shared information. Our analysis shows that Balanced-ETC is asymptotically optimal, and its average regret (on each agent) approaches a constant when there are sufficient agents involved. Moreover, to encourage agents to participate in this collaborative learning, an incentive mechanism is proposed to make sure each agent can benefit from the collaboration system. Finally, we present experimental results to validate our theoretical results.},
  archive   = {C_AAMAS},
  author    = {Shao, Junning and Wang, Siwei and Fang, Zhixuan},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2459–2461},
  title     = {Balanced and incentivized learning with limited shared information in multi-agent multi-armed bandit},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663193},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Geospatial active search for preventing evictions.
<em>AAMAS</em>, 2456–2458. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Evictions are a threat to housing stability and a major concern for many cities. An open question is whether data-driven methods can enhance door-to-door outreach programs to target at-risk tenants. We model this problem using a new framework we term geospatial active search. Geospatial Active Search integrates visual information such as satellite imagery along with tabular data such as property and neighborhood-level information to create an online exploration plan. We develop an approach for the implementation of Geospatial Active Search in St. Louis to find properties containing tenants who will have an eviction filed against them.},
  archive   = {C_AAMAS},
  author    = {Sarkar, Anindya and DiChristofano, Alex and Das, Sanmay and Fowler, Patrick J. and Jacobs, Nathan and Vorobeychik, Yevgeniy},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2456–2458},
  title     = {Geospatial active search for preventing evictions},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663192},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Centralized training with hybrid execution in multi-agent
reinforcement learning. <em>AAMAS</em>, 2453–2455. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce hybrid execution in multi-agent reinforcement learning (MARL), a new paradigm in which agents aim to successfully complete cooperative tasks with arbitrary communication levels at execution time by taking advantage of information-sharing among the agents. Under hybrid execution, the communication level can range from a setting in which no communication is allowed between agents (fully decentralized), to a setting featuring full communication (fully centralized), but the agents do not know beforehand which communication level they will encounter at execution time. To formalize our setting, we define a new class of multi-agent partially observable Markov decision processes (POMDPs) that we name hybrid-POMDPs, which explicitly model a communication process between the agents.},
  archive   = {C_AAMAS},
  author    = {Santos, Pedro P. and Carvalho, Diogo S. and Vasco, Miguel and Sardinha, Alberto and Santos, Pedro A. and Paiva, Ana and Melo, Francisco S.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2453–2455},
  title     = {Centralized training with hybrid execution in multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663191},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Social identities and responsible agency. <em>AAMAS</em>,
2450–2452. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Social identities play an important role in the dynamics of human societies, and it can be argued that some sense of identification with a larger cause or idea plays a critical role in making humans act responsibly. Often social activists strive to get populations to identify with some cause or notion--like green energy, diversity, etc. in order to bring about desired social changes. We explore the problem of designing computational models for social identities in the context of autonomous AI agents. For this, we propose an agent model that enables agents to identify with certain notions and show how this affects collective outcomes. We also contrast between associations of identity with rational preferences. The proposed model is simulated in an application context of urban mobility, where we show how changes in social identity affect mobility patterns and collective outcomes.},
  archive   = {C_AAMAS},
  author    = {Sama, Karthik and Deshmukh, Jayati and Srinivasa, Srinath},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2450–2452},
  title     = {Social identities and responsible agency},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663190},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Source detection in networks using the stationary
distribution of a markov chain. <em>AAMAS</em>, 2447–2449. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Nowadays, the diffusion of information through social networks is a powerful phenomenon. One common way to model diffusions in social networks is the Independent Cascade (IC) model. Given a set of infected nodes according to the IC model, a natural problem is the source detection problem, in which the goal is to identify the unique node that has started the diffusion. Maximum Likelihood Estimation (MLE) is a common approach for tackling the source detection problem, but it is computationally hard.In this work, we propose an efficient method for the source detection problem under the MLE approach, which is based on computing the stationary distribution of a Markov chain. Using simulations, we demonstrate the effectiveness of our method compared to other state-of-the-art methods from the literature, both on random and real-world networks.},
  archive   = {C_AAMAS},
  author    = {Sabato, Yael and Azaria, Amos and Hazon, Noam},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2447–2449},
  title     = {Source detection in networks using the stationary distribution of a markov chain},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663189},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). JaxMARL: Multi-agent RL environments and algorithms in JAX.
<em>AAMAS</em>, 2444–2446. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Benchmarks play an important role in the development of machine learning algorithms, with reinforcement learning (RL) research having been heavily influenced by the available environments. However, RL environments are traditionally run on the CPU, limiting their scalability with typical academic compute. Recent advancements in JAX have enabled the wider use of hardware acceleration to overcome these computational hurdles, enabling massively parallel RL training pipelines and environments. This is particularly useful for multi-agent reinforcement learning (MARL) research. First of all, multiple agents must be considered at each environment step, adding computational burden, and secondly, the sample complexity is increased due to non-stationarity, decentralised partial observability, or other MARL challenges. In this paper, we present JaxMARL, the first open-source code base that combines ease-of-use with GPU enabled efficiency, and supports a large number of commonly used MARL environments as well as popular baseline algorithms. When considering wall clock time, our experiments show that per-run our JAX-based training pipeline is up to 12500x faster than existing approaches. \%This enables efficient and thorough evaluations, with the potential to alleviate the evaluation crisis of the field. We also introduce and benchmark SMAX, a vectorised, simplified version of the popular StarCraft Multi-Agent Challenge, which removes the need to run the StarCraft II game engine. This not only enables GPU acceleration, but also provides a more flexible MARL environment, unlocking the potential for self-play, meta-learning, and other future applications in MARL. We provide code at https://github.com/flairox/jaxmarl.},
  archive   = {C_AAMAS},
  author    = {Rutherford, Alexander and Ellis, Benjamin and Gallici, Matteo and Cook, Jonathan and Lupu, Andrei and Ingvarsson, Gar\dh{}ar and Willi, Timon and Khan, Akbir and Schroeder de Witt, Christian and Souly, Alexandra and Bandyopadhyay, Saptarashmi and Samvelyan, Mikayel and Jiang, Minqi and Lange, Robert and Whiteson, Shimon and Lacerda, Bruno and Hawes, Nick and Rockt\&quot;{a}schel, Tim and Lu, Chris and Foerster, Jakob},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2444–2446},
  title     = {JaxMARL: Multi-agent RL environments and algorithms in JAX},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663188},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The selfishness level of social dilemmas. <em>AAMAS</em>,
2441–2443. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A key contributor to the success of modern societies is humanity&#39;s innate ability to meaningfully cooperate. Game-theoretic reasoning shows however, that an individual&#39;s amenity to cooperation is directly linked with the mechanics of the scenario at hand. Social dilemmas constitute a subset of such scenarios where players are caught in a dichotomy between the decision to cooperate, prioritising collective welfare, or defect, prioritising their own welfare. In this work, we study such games through the lens of &#39;selfishness level&#39;, a standard game-theoretic metric which quantifies the extent to which a game&#39;s payoffs incentivize self-directed behaviours. Using this framework, we derive the conditions under which SDs can be resolved and, additionally, produce a first-step towards extending this metric to Markov games. Finally, we present an empirical analysis indicating the positive effects of selfishness level directed mechanisms in such environments.},
  archive   = {C_AAMAS},
  author    = {Roesch, Stefan and Leonardos, Stefanos and Du, Yali},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2441–2443},
  title     = {The selfishness level of social dilemmas},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663187},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Psychophysiological models of cognitive states can be
operator-agnostic. <em>AAMAS</em>, 2438–2440. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-time prediction of a person&#39;s trust (T), mental workload (W), and situation awareness (SA) can improve safety and performance in operational environments. We develop psychophysiological models of TWSA both with and without operator-specific demographic information available to them and we assess the impact of this constraint on the models&#39; performance. We demonstrate functional model fit (Adjusted R2: T = 0.67, W = 0.65, and SA = 0.88) and predictive ability with operator-agnostic models, assessed via leave-one-participant-out cross validation (Q2: T = 0.58, W = 0.45, and SA = 0.79). Our findings help establish the viability of operator-agnostic psychophysiological models of TWSA which could be used to inform an autonomous agent or manage multi-agent teams.},
  archive   = {C_AAMAS},
  author    = {Richardson, Erin E. and Buchner, Savannah L. and Kintz, Jacob R. and Clark, Torin K. and Anderson, Allison P.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2438–2440},
  title     = {Psychophysiological models of cognitive states can be operator-agnostic},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663186},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BAR nash equilibrium and application to blockchain design.
<em>AAMAS</em>, 2435–2437. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel solution concept, called BAR Nash Equilibrium (BARNE) and applies it to analyse the Verifier&#39;s dilemma, a fundamental problem in blockchain. Our solution concept adapts the Nash equilibrium (NE) to accommodate interactions among Byzantine, altruistic and rational agents, which became known as the BAR setting in the literature. We prove the existence of BARNE in a large class of games and introduce two natural refinements, global and local stability. Using this equilibrium and its refinements, we analyse the free-rider problem in the context of Byzantine consensus. We demonstrate that by incorporating fines and forced errors into a standard quorum-based blockchain protocol, we can effectively reestablish honest behavior as a globally stable BARNE.},
  archive   = {C_AAMAS},
  author    = {Reynouard, Maxime and Gorelkina, Olga and Laraki, Rida},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2435–2437},
  title     = {BAR nash equilibrium and application to blockchain design},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663185},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Banzhaf power in hierarchical games. <em>AAMAS</em>,
2432–2434. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Banzhaf Power Index (BPI) is a method of measuring the power of voters in determining the outcome of a voting game. Some voting games exhibit a hierarchical structure, including the US electoral college and ensemble learning methods; we call such games hierarchical voting games. It is generally understood that BPI in hierarchical voting games can be computed via a recursive decomposition of the hierarchy, which can substantially reduce the calculation&#39;s complexity. We identify a key (previously undocumented) assumption on which this decomposition is based, namely balance, meaning one group of voters has enough votes to win whenever the complementary group of voters does not, and vice versa. We then introduce a generalization of BPI that we call Extended BPI (EBPI) for all voting games, including those that are not balanced, which simplifies to BPI in balanced games. We show that BPI in unbalanced hierarchical voting games decomposes in terms of EBPI at each level in the hierarchy, which yields computational savings analogous to those achieved in the balanced case. As a sample application, we take advantage of the compositionality of language, and model the impact of individual words on a sentence&#39;s sentiment as a voting game. As the complement of a phrase in a sentence does not necessarily have the opposite sentiment, this voting game is unbalanced and requires our decomposition of BPI in terms of EBPI. Our results suggest that EBPI is an effective proxy for BPI (because the meaning of a sentence is not always 100\% compositional), and demonstrate a dramatic improvement in run time.},
  archive   = {C_AAMAS},
  author    = {Randolph, John and Greenwald, Amy and Goktas, Denizalp},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2432–2434},
  title     = {Banzhaf power in hierarchical games},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663184},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GOV-REK: Governed reward engineering kernels for designing
robust multi-agent reinforcement learning systems. <em>AAMAS</em>,
2429–2431. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For multi-agent reinforcement learning (MARL) systems, the problem task often involves massive problem-specific reward engineering effort. This effort is usually not directly transferable to other problems; worse, this problem is further exacerbated for sparse reward scenarios. We propose GOVerned Reward Engineering Kernels (GOV-REK), which dynamically assign reward distributions to agents in MARLs during the learning stage. We also introduce governance kernels, which exploit the underlying structure in either state or joint action space for assigning meaningful agent rewards. We demonstrate, using a Hyperband-like problem-agnostic algorithm, that this approach successfully learns to solve different MARL problems by iteratively exploring multiple reward models.},
  archive   = {C_AAMAS},
  author    = {Rana, Ashish and Oesterle, Michael and Brinkmann, Jannik},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2429–2431},
  title     = {GOV-REK: Governed reward engineering kernels for designing robust multi-agent reinforcement learning systems},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663183},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Emergent dominance hierarchies in reinforcement learning
agents. <em>AAMAS</em>, 2426–2428. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern Reinforcement Learning (RL) algorithms are able to outperform humans in a wide variety of tasks. Multi-agent reinforcement learning (MARL) settings present additional challenges around cooperation in mixed-motive groups. Social conventions and norms, often inspired by human institutions, are used as tools for striking the balance between individual and group objectives. We examine a fundamental social convention that underlies cooperation in animal and human societies: dominance hierarchies.We adapt the ethological theory of dominance hierarchies to artificial agents, borrowing established terminology and definitions. We provide an environment we call Chicken Coop, and we demonstrate that populations of RL agents in that environment can invent, learn, enforce, and transmit a dominance hierarchy to new populations. The dominance hierarchies that emerge in it have a similar structure to those studied in chickens, mice, fish, and other species.},
  archive   = {C_AAMAS},
  author    = {Rachum, Ram and Nakar, Yonatan and Tomlinson, Bill and Alon, Nitay and Mirsky, Reuth},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2426–2428},
  title     = {Emergent dominance hierarchies in reinforcement learning agents},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663182},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fully independent communication in multi-agent reinforcement
learning. <em>AAMAS</em>, 2423–2425. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Several recent works have focused on communication approaches in Multi-Agent Reinforcement Learning (MARL). However, the multiple proposed communication methods might still be too complex and not easily transferable to more practical contexts. One of the reasons is due to the use of the famous parameter sharing trick. In this paper, we investigate how independent learners in MARL that do not share parameters can communicate. We demonstrate that this setting might incur into some problems, to which we propose a new learning scheme as a solution. Our results show that, despite the challenges, independent agents can still learn communication strategies following our method. Additionally, we use this method to investigate how communication in MARL is affected by different network capacities, both for sharing and not sharing parameters.},
  archive   = {C_AAMAS},
  author    = {Pina, Rafael and De Silva, Varuna and Artaud, Corentin and Liu, Xiaolan},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2423–2425},
  title     = {Fully independent communication in multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663181},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal majority rules and quantitative condorcet properties
of setwise kemeny voting schemes. <em>AAMAS</em>, 2420–2422. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Kemeny problem consists of computing consensus rankings of an election with respect to the Kemeny voting rule, admits important applications in biology and computational social choice [1, 2, 4, 6]. The problem was generalized recently via an interesting setwise approach by Gilbert et al. [9, 10] where not only pairwise comparisons but also the discordance between the winners of subsets of three candidates are also taken into account. We elaborate an exhaustive list of quantified axiomatic properties such as the Condorcet and Smith criteria, the 5/6-majority rule, and the Unanimity property of the 3-wise Kemeny rule. Since the 3-wise Kemeny problem is NP-hard, our results also provide some of the first useful search space reduction techniques by determining the relative orders of pairs of alternatives. Our works suggest similar interesting properties of higher setwise Kemeny voting schemes which justify the more expensive computational cost than the classical Kemeny scheme. We also establish optimal quantitative extensions of the Unanimity property and the well-known 3/4-majority rule of Betzler et al. [4] for the classical Kemeny problem.},
  archive   = {C_AAMAS},
  author    = {Phung, Xuan Kien and Hamel, Sylvie},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2420–2422},
  title     = {Optimal majority rules and quantitative condorcet properties of setwise kemeny voting schemes},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663180},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decision making in non-stationary environments with
policy-augmented search. <em>AAMAS</em>, 2417–2419. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sequential decision-making is challenging in non-stationary environments, where the environment in which an agent operates can change over time. Policies learned before execution become stale when the environment changes, and relearning takes time and computational effort. Online search, on the other hand, can return sub-optimal actions when there are limitations on allowed runtime. In this paper, we introduce Policy-Augmented Monte Carlo tree search (PA-MCTS), which combines action-value estimates from an out-of-date policy with an online search using an up-to-date model of the environment. We prove several theoretical results about PA-MCTS. We also compare and contrast our approach with AlphaZero, another hybrid planning approach, and Deep Q Learning on several OpenAI Gym environments and show that PA-MCTS outperforms these baselines.},
  archive   = {C_AAMAS},
  author    = {Pettet, Ava and Zhang, Yunuo and Luo, Baiting and Wray, Kyle and Baier, Hendrik and Laszka, Aron and Dubey, Abhishek and Mukhopadhyay, Ayan},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2417–2419},
  title     = {Decision making in non-stationary environments with policy-augmented search},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663179},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incentive-based MARL approach for commons dilemmas in
property-based environments. <em>AAMAS</em>, 2414–2416. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose ORAA, a novel online incentive algorithm that guides agents in a property-based MARL domain to act sustainably with a common pool of resources. ORAA, uses our proposed P-MADDPG model to learn and make decisions over the decentralised agents. We test our solutions in our novel domain, the &quot;Pollinators&#39; Game&#39;&#39;, which simulates a property-based MARL scenario and its incentivisation dynamics. We show significant improvement in the incentives&#39; cost-efficiency when using learned models that approximate the behaviour of each agent instead of simulating their true models.},
  archive   = {C_AAMAS},
  author    = {Pelcner, Lukasz and Aparecido do Carmo Alves, Matheus and Soriano Marcolino, Leandro and Harrison, Paula and Atkinson, Peter},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2414–2416},
  title     = {Incentive-based MARL approach for commons dilemmas in property-based environments},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663178},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DCT: Dual channel training of action embeddings for
reinforcement learning with large discrete action spaces.
<em>AAMAS</em>, 2411–2413. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to learn robust policies while generalizing over large discrete action spaces is an open challenge for intelligent systems, especially in noisy environments that face the curse of dimensionality. In this paper, we present a novel framework to efficiently learn action embeddings that simultaneously allow us to reconstruct the original action as well as to predict the expected future state. We describe an encoder-decoder architecture for action embeddings with a dual channel loss that balances between action reconstruction and state prediction accuracy. We use the trained decoder in conjunction with a standard reinforcement learning algorithm that produces actions in the embedding space. Our architecture is able to outperform two competitive baselines in two diverse environments: a 2D maze environment with more than 4000 discrete noisy actions, and a product recommendation task that uses real-world e-commerce transaction data. Empirical results show that the model results in cleaner action embeddings, and the improved representations help learn better policies with earlier convergence.},
  archive   = {C_AAMAS},
  author    = {Pathakota, Pranavi and Meisheri, Hardik and Khadilkar, Harshad},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2411–2413},
  title     = {DCT: Dual channel training of action embeddings for reinforcement learning with large discrete action spaces},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663177},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sentimental agents: Combining sentiment analysis and
non-bayesian updating for cooperative decision-making. <em>AAMAS</em>,
2408–2410. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With ongoing exploration of Large Language Model (LLM)-based multi-agent systems, it is becoming increasingly important to understand and interpret the dynamics of agent interactions and their beliefs, particularly when designed to emulate diverse roles and perspectives or to engage in debates. At present, there are no unified solutions that can systematically interpret and analyze the beliefs and interactions of these agents. This study introduces Sentimental Agents, a framework designed to support decision-making by providing multiple perspectives on a topic. Agents within this framework are equipped with a mental model of self, articulated in natural language. We have integrated sentiment analysis with a non-Bayesian updating mechanism to interpret and analyze the agents&#39; beliefs and interactions systematically. A collective viewpoint is achieved when the update is marginal. We have adapted this framework for a simulated scenario in the Human Resource domain, implementing a conceptual tool known as the Artificial Board of Advisors (ABA). A key focus of this simulation is the application of ABA in the assessment of candidates for roles, showcasing its potential application in a theoretical HR recruiting environment.},
  archive   = {C_AAMAS},
  author    = {Orner, Daniele and Ondula, Elizabeth Akinyi and Mumero Mwangi, Nick and Goyal, Richa},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2408–2410},
  title     = {Sentimental agents: Combining sentiment analysis and non-bayesian updating for cooperative decision-making},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663176},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ontological modeling and reasoning for comparison and
contrastive narration of robot plans. <em>AAMAS</em>, 2405–2407. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This extended abstract focuses on an approach to modeling and reasoning about the comparison of competing plans, so that robots can later explain the divergent result. First, the need for a novel ontological model that empowers robots to formalize and reason about plan divergences is identified. Then, a new ontological theory is proposed to facilitate the classification of plans (e.g., the shortest, the safest, the closest to human preferences, etc.). Finally, the limitations of a baseline algorithm for ontology-based explanatory narration are examined, and a novel algorithm is introduced to leverage the divergent knowledge between plans, enabling the construction of contrastive narratives. An empirical evaluation is conducted to assess the quality of the explanations provided by the proposed algorithm, which outperforms the baseline method.},
  archive   = {C_AAMAS},
  author    = {Olivares-Alarcos, Alberto and Foix, Sergi and Borr\`{a}s, J\&#39;{u}lia and Canal, Gerard and Aleny\`{a}, Guillem},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2405–2407},
  title     = {Ontological modeling and reasoning for comparison and contrastive narration of robot plans},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663175},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MA-MIX: Value function decomposition for cooperative
multiagent reinforcement learning based on multi-head attention
mechanism. <em>AAMAS</em>, 2402–2404. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-Agent Deep Reinforcement Learning (MADRL) is a research field that combines deep learning and multi-agent reinforcement learning. In complex tasks, a single agent often finds it difficult to complete the task independently, thus requiring cooperation and communication between agents. However, communication between agents remains a key issue in multi-agent cooperative reinforcement learning. To address this issue, we propose a new method called Multi-Head Attention Mixing Network (MA-MIX), which aims to solve key challenges in multi-agent systems. MA-MIX is based on the multi-head attention mechanism and innovatively applied to agent networks, effectively solving the problem of information exchange and cooperation in multi-agent systems. We compared MA-MIX with traditional QMIX algorithms and other baseline algorithms. The experimental results show that MA-MIX has superior performance under the StarCraft Multi-Agent Challenge (SMAC) environment.},
  archive   = {C_AAMAS},
  author    = {Niu, Yu and Zhao, Hengxu and Yu, Lei},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2402–2404},
  title     = {MA-MIX: Value function decomposition for cooperative multiagent reinforcement learning based on multi-head attention mechanism},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663174},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging sub-optimal data for human-in-the-loop
reinforcement learning. <em>AAMAS</em>, 2399–2401. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To create useful reinforcement learning (RL) agents, step zero is to design a suitable reward function that captures the nuances of the task. However, reward engineering can be a difficult and time-consuming process. Instead, human-in-the-loop (HitL) RL approaches allow agents to learn reward functions from human feedback. Despite recent successes, many of the HitL RL methods still require numerous human interactions to learn successful reward functions. To that end, this work introduces Sub-optimal Data Pre-training, SDP, a method that leverages reward-free, sub-optimal data to improve the feedback efficiency of HitL RL algorithms. We demonstrate that SDP can significantly improve over state-of-the-art HitL RL algorithms in three DMControl environments.},
  archive   = {C_AAMAS},
  author    = {Muslimani, Calarina and Taylor, Matthew E.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2399–2401},
  title     = {Leveraging sub-optimal data for human-in-the-loop reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663173},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simulated robotic soft body manipulation. <em>AAMAS</em>,
2396–2398. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The performance of intelligent agents manipulating a soft body object depends on the agent&#39;s understanding of the execution environment. Hence, by keeping the agent fixed and changing the environment, the difference between environments can be measured. However, this becomes complicated when dealing with agents that learn in each environment.We propose a framework for evaluating the influence of the simulated soft bodies (and related object models) on the reinforcement learning algorithms&#39; performance. The change in algorithm behavior is quantified between different environments, and the correlation of behavioral difference is measured via statistical analysis. An evaluation case is presented on PyBullet and MuJoCo physics simulation environments with DDPG, PPO, TD3 and SAC algorithms.},
  archive   = {C_AAMAS},
  author    = {Mir, Glareh and Beetz, Michael},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2396–2398},
  title     = {Simulated robotic soft body manipulation},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663172},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continual depth-limited responses for computing
counter-strategies in sequential games. <em>AAMAS</em>, 2393–2395. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In zero-sum games, the optimal strategy is well-defined by the Nash equilibrium. However, it is overly conservative when playing against suboptimal opponents and it can not exploit their weaknesses. Limited look-ahead game solving in imperfect-information games allows superhuman play in massive real-world games such as Poker, Liar&#39;s Dice, and Scotland Yard. However, since they approximate Nash equilibrium, they tend to only win slightly against weak opponents. We propose theoretically sound methods combining limited look-ahead solving with an opponent model, in order to 1) approximate a best response in large games or 2) compute a robust response with control over the robustness of the response. Both methods can compute the response in real time to previously unseen strategies. We present theoretical guarantees of our methods. We show that existing robust response methods do not work combined with limited look-ahead solving of the shelf, and we propose a novel solution for the issue. Our algorithm performs significantly better than multiple baselines in smaller games and outperforms state-of-the-art methods against SlumBot.},
  archive   = {C_AAMAS},
  author    = {Milec, David and Kub\&#39;{\i}\v{c}ek, Ond\v{r}ej and Lis\&#39;{y}, Viliam},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2393–2395},
  title     = {Continual depth-limited responses for computing counter-strategies in sequential games},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663171},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fairness in repeated house allocation. <em>AAMAS</em>,
2390–2392. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This article considers a house allocation setting?where exactly one object has to be assigned to each agent-in a repeated context, where the same allocation problem is decided multiple times while taking previous decisions into account. Since fairness can be rarely achieved in a one-shot decision, we study whether fairness over time can be reached. In particular, we introduce several fairness criteria and investigate whether they can be satisfied in our repeated house allocation setting. While we show that most related decision problems are hard in general, we identify restricted positive cases.},
  archive   = {C_AAMAS},
  author    = {Micheel, Karl Jochen and Wilczynski, Ana\&quot;{e}lle},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2390–2392},
  title     = {Fairness in repeated house allocation},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663170},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing search and rescue capabilities in hazardous
communication-denied environments through path-based sensors with
backtracking. <em>AAMAS</em>, 2387–2389. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Prior work on path-based sensors has assumed that each agent&#39;s path is determined before the agent departs and cannot be changed mid-trip. We consider how an agent might adjust its path in response to new information that it gathers en route. Mid-trip path adjustment is non-trivial because it can increase the number of locations at which a missing agent may have been destroyed (from an external observer&#39;s point-of-view). We solve this issue by employing backtracking as a particular form of mid-trip path adjustment that avoids the issue of additional potential destruction locations.},
  archive   = {C_AAMAS},
  author    = {Mendelsohn, Alexander and Sofge, Donald and Otte, Michael},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2387–2389},
  title     = {Enhancing search and rescue capabilities in hazardous communication-denied environments through path-based sensors with backtracking},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663169},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Shield decentralization for safe reinforcement learning in
general partially observable multi-agent environments. <em>AAMAS</em>,
2384–2386. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As reinforcement learning (RL) is increasingly used in safety-critical systems, it is important to restrict RL agents to only take safe actions. Shielding is a promising approach to this task; however, in multi-agent domains, shielding has previously been restricted to environments where all agents observe the same information. Most real-world tasks do not satisfy this strong assumption. We discuss the theoretical foundations of multi-agent shielding in environments with general partial observability and develop a novel shielding method which is effective in such domains. Through a series of experiments, we show that agents that use our shielding method are able to safely and successfully solve a variety of RL tasks, including tasks in which prior methods cannot be applied.},
  archive   = {C_AAMAS},
  author    = {Melcer, Daniel and Amato, Christopher and Tripakis, Stavros},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2384–2386},
  title     = {Shield decentralization for safe reinforcement learning in general partially observable multi-agent environments},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663168},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Projection-optimal monotonic value function factorization in
multi-agent reinforcement learning. <em>AAMAS</em>, 2381–2383. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Value function factorization has emerged as the prevalent method for cooperative multi-agent reinforcement learning under the centralized training and decentralized execution paradigm. Many of these algorithms ensure the coherence between joint and local action selections for decentralized decision-making by factorizing the optimal joint action-value function using a monotonic mixing function of agent utilities. Despite this, utilizing monotonic mixing functions also induces representational limitations, and finding the optimal projection of an unconstrained mixing function onto monotonic function classes remains an open problem. In this paper, we propose QPro, which casts this optimal projection problem for value function factorization as regret minimization over projection weights of different transitions. This optimization problem can be relaxed and solved using the Lagrangian multiplier method to obtain the optimal projection weights in a closed form, where we narrow the gap between optimal and restricted monotonic mixing functions by minimizing the policy regret of expected returns, thereby enhancing the monotonic value function factorization. Our experiments demonstrate the effectiveness of our method, indicating improved performance in environments with non-monotonic value functions.},
  archive   = {C_AAMAS},
  author    = {Mei, Yongsheng and Zhou, Hanhan and Lan, Tian},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2381–2383},
  title     = {Projection-optimal monotonic value function factorization in multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663167},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-level aggregation with delays and stochastic arrivals.
<em>AAMAS</em>, 2378–2380. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In online Multi-Level Aggregation (MLA) with delays, the input is an edge-weighted rooted tree T and a sequence of requests arriving at its vertices (with each vertex representing an independent agent) that need to be served in an online manner. Each request r is characterized by two parameters: its arrival time t(r) and its location l(r) (a vertex). Once r arrives, we can either serve it immediately or postpone this action until any time later. We can serve several pending requests at the same time, paying a service cost equal to the weight of the subtree that contains the locations of all the requests served and the root of T. Postponing the service of a request r to time t generates an additional delay cost of t -t(r). The goal is to serve all requests in an online manner such that the total cost (i.e., the total sum of service and delay costs) is minimized. The MLA problem is a generalization of several well-studied problems, including the TCP Acknowledgment (depth 1), Joint Replenishment (depth 2), and Multi-Level Message Aggregation (arbitrary depth). This problem has only been studied in an adversarial model thus far, and the current best algorithm for this problem achieves a competitive ratio of O(d2), where d denotes the depth of the tree. We study a stochastic version of MLA where the requests follow a Poisson arrival process. We present a deterministic online algorithm that achieves a constant ratio of expectations, meaning that the ratio between the expected costs of the solution generated by our algorithm and the optimal offline solution is bounded by a constant.},
  archive   = {C_AAMAS},
  author    = {Mari, Mathieu and Pawundefinedowski, Michaundefined and Ren, Runtian and Sankowski, Piotr},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2378–2380},
  title     = {Multi-level aggregation with delays and stochastic arrivals},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663166},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Time-constrained restless multi-armed bandits with
applications to city service scheduling. <em>AAMAS</em>, 2375–2377. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Municipalities maintain critical infrastructure through inspections, both proactive and in response to complaints. For example, the Chicago Department of Public Health (CDPH) periodically inspects 7000 food establishments to maintain the safety of food bought, sold, or prepared for public consumption. Restless multi-armed bandits (RMABs) appear to be a useful tool for optimizing the scheduling of inspections, as the schedule aims to keep as many establishments in the &quot;passing&#39;&#39; state subject to an action limit per period. However, a key challenge arises: satisfying timing and frequency constraints. Municipal agencies often provide an inspection window to each establishment (e.g., a two-week period where an inspection will occur) and guarantee the minimum frequency of inspection (e.g., once per year). We develop an extension to Whittle index-based systems for RMABs that can guarantee both action window constraints and minimum frequencies. Briefly, we take a Whittle index-based view, enforcing window constraints by integrating the window structure into individual MDPs, and frequency constraints through a higher-level scheduling algorithm that aims to maximize the Whittle index. We demonstrate our methods&#39; performance and scalability in experiments using synthetic and real data (with 7000 establishments inspected per year). Not only does our approach enforce constraints more effectively than naive methods, but it also achieves higher rewards, up to 20\%.},
  archive   = {C_AAMAS},
  author    = {Mao, Yi and Perrault, Andrew},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2375–2377},
  title     = {Time-constrained restless multi-armed bandits with applications to city service scheduling},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663165},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Opinion diffusion on society graphs based on approval
ballots. <em>AAMAS</em>, 2372–2374. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A society graph, as considered by [Faliszewski et al., IJCAI 2018], is a graph corresponding to an election instance where every possible ranking is a node, and the weight of such a node is given by the number of voters whose vote correspond to the said ranking. A natural diffusion process on this graph is defined, and an immediate question that emerges is whether there is a diffusion path that leads to a particular candidate winning according to a certain voting rule-this turns out to be NP-complete.In this contribution, we consider the setting when votes are approval ballots, as opposed to rankings-and we consider both the possible and necessary winner problems. We demonstrate that it is possible to efficiently determine if a candidate is a possible winner (i.e, if there exists a diffusion path along which a given candidate wins the election) if the underlying society graph is a star (i.e, tree of diameter at most two), while the problem is NP-complete for trees of diameter d for d &amp;gt; 2. Analogously, we show that it is possible to efficiently determine if a candidate is a necessary winner (i.e, a winner for every possible diffusion path) if the underlying society graph is a star, while the problem is coNP-complete for trees of diameter d for d &amp;gt; 2. We also show the following results on structured graphs for the possible winner problem: the problem is strongly NP-complete on a disjoint union of paths, and on trees of constant diameter. We also report preliminary experiments from an ILP-based implementation.},
  archive   = {C_AAMAS},
  author    = {Madathil, Jayakrishnan and Misra, Neeldhara and More, Yash},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2372–2374},
  title     = {Opinion diffusion on society graphs based on approval ballots},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663164},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards understanding how to reduce generalization gap in
visual reinforcement learning. <em>AAMAS</em>, 2369–2371. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It is vital to learn a generalizable policy in visual reinforcement learning (RL). Many algorithms are proposed to handle this problem while none of them theoretically show what affects the generalization gap and why their methods work. In this paper, we bridge this issue by theoretically answering the key factors that contribute to the generalization gap when the testing environment has distractors. Our theories indicate that minimizing the representation distance between training and testing environments is the most critical. Our theoretical results are supported by the empirical evidence in the DMControl Generalization Benchmark.},
  archive   = {C_AAMAS},
  author    = {Lyu, Jiafei and Wan, Le and Li, Xiu and Lu, Zongqing},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2369–2371},
  title     = {Towards understanding how to reduce generalization gap in visual reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663163},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simple k-crashing plan with a good approximation ratio.
<em>AAMAS</em>, 2366–2368. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A project is considered as an activity-on-edge network (AOE network, which is a directed acyclic graph) N, where each activity / job of the project is an edge. Some jobs must be finished before others can be started, as described by the topology structure of N. It is known that job ji in normal speed would take bi days to be finished after it is started, and hence the (normal) duration of the project N, denoted by d(N), is determined, which equals the length of the critical path (namely, the longest path) of N.To speed up the project, the manager can crash a few jobs (namely, reduce the length of the corresponding edges) by investing extra resources into that job. However, the time for completing ji has a lower bound due to technological limits - it requires at least ai days to be completed. Following the convention, assume that the duration of a job has a linear relation with the extra resources put into this job; equivalently, there is a parameter ci (slope), so that shortening ji by d(0 ≤ d ≤ bi-ai) days costs ci⋅ d resources.Given project N and an integer k ≥ 1, the k-crashing problem asks the minimum cost to speed up the project by k days.In this paper, we present a simple solution with the approximation ratio 1 over 1 + … + 1 over k. For simplicity, we focus on the linear case throughout the paper, but our proofs are still correct for the convex case, where shortening an edge becomes more difficult after a previous shortening.},
  archive   = {C_AAMAS},
  author    = {Luo, Ruixi and Jin, Kai and Ye, Zelin},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2366–2368},
  title     = {Simple k-crashing plan with a good approximation ratio},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663162},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient collaboration with unknown agents: Ignoring
similar agents without checking similarity. <em>AAMAS</em>, 2363–2365.
(<a href="https://dl.acm.org/doi/10.5555/3635637.3663161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ad hoc teamwork (AHT) is concerned with developing an AI agent who learns to collaborate with different previously unseen partners. We consider a setting where the AI agent is provided with a hypothesis set of partners&#39; policies. Several online algorithms that take the hypothesis set as input can be applied to solve the AHT problem. One way to speed up these online learning algorithms is to eliminate the redundant policies, i.e., partner models sharing the same collaborating policy, from the hypothesis set. Nevertheless, we show whether this elimination should be applied depends on the learning algorithm used by the AI agent. Specifically, we identify a property of a learning algorithm: redundancy-aware. When the learning algorithm is redundancy-aware, redundancy elimination is unnecessary. In other words, redundancy-aware algorithms can ignore similar agents in the hypothesis set. We demonstrate through an example that an online algorithm with redundancy-aware property exists when the hypothesis set contains the true partner policy. We test our approach on a team Markov game of two players. Comparative numerical analyses reveal that the redundancy-aware algorithm outperforms other standard no-regret learning algorithms including upper confidence bound (UCB), Q-learning with UCB exploration, and the optimistic posterior sampling algorithm when the set of partner policies contains many redundant policies.},
  archive   = {C_AAMAS},
  author    = {Li, Yansong and Han, Shuo},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2363–2365},
  title     = {Efficient collaboration with unknown agents: Ignoring similar agents without checking similarity},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663161},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From explicit communication to tacit cooperation: A novel
paradigm for cooperative MARL. <em>AAMAS</em>, 2360–2362. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Centralized training with decentralized execution (CTDE) is a widely used learning paradigm that has achieved significant success in complex tasks. Drawing inspiration from human team cooperative learning, we propose a novel paradigm that facilitates a gradual shift from explicit communication to tacit cooperation. In the initial training stage, we promote cooperation by sharing relevant information among agents and concurrently reconstructing this information using each agent&#39;s local trajectory in a self-supervised way. We then combine the explicitly communicated information with the reconstructed information to obtain mixed information. Throughout the training process, we progressively decrease the proportion of explicitly communicated information, facilitating a seamless transition to fully decentralized execution without communication.},
  archive   = {C_AAMAS},
  author    = {Li, Dapeng and Xu, Zhiwei and Zhang, Bin and Zhou, Guangchong and Zhang, Zeren and Fan, Guoliang},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2360–2362},
  title     = {From explicit communication to tacit cooperation: A novel paradigm for cooperative MARL},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663160},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ELA: Exploited level augmentation for offline learning in
zero-sum games. <em>AAMAS</em>, 2357–2359. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Offline learning derives effective policies from expert demonstrators&#39; datasets without direct interaction. While recent research consider dataset characteristics like expertise level or multiple demonstrators, a distinct approach is necessary in zero-sum games, where outcomes significantly depend on the opponent&#39;s strategy. In this study, we introduce a novel approach using unsupervised learning techniques to estimate the exploited level (EL) of each trajectory from the offline dataset of zero-sum games made by diverse demonstrators. The estimated EL is then integrated into offline learning to maximize the influence of the dominant strategy. Our method enables interpretable EL estimation in multiple zero-sum games, effectively identifying dominant strategies. Also, EL augmented offline learning significantly enhances the imitation and offline reinforcement learning algorithms in zero-sum games.},
  archive   = {C_AAMAS},
  author    = {Lei, Shiqi and Lee, Kanghoon and Li, Linjing and Park, Jinkyoo and Li, Jiachen},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2357–2359},
  title     = {ELA: Exploited level augmentation for offline learning in zero-sum games},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663159},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Which games are unaffected by absolute commitments?
<em>AAMAS</em>, 2354–2356. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We identify a subtle security issue that impacts mechanism design in scenarios in which agents can absolutely commit to strategies, by which the strategy of an agent may depend on the commitments made by the other agents. This changes fundamental game-theoretic assumptions by inducing a meta-game of choosing which strategies to commit to. We say that a game that is unaffected by such commitments is Stackelberg resilient and show that computing it is intractable in general, though it can be computed efficiently for two-player games of perfect information. We show the intuitive, but technically non-trivial result, that if a game is resilient when some number of players can make commitments, it is also resilient when these commitments are available to fewer players. We demonstrate the non-triviality of Stackelberg resilience by analyzing two escrow mechanisms from the literature. These mechanisms have the same intended functionality, but we show that only one is Stackelberg resilient. Our model is particularly relevant in Web3 scenarios, where these commitments can be realized by the automated and irrevocable nature of smart contracts, and highlights an important issue in ensuring the secure design of Web3. In particular, our work suggests that smart contracts already deployed on major blockchains may be susceptible to these attacks.},
  archive   = {C_AAMAS},
  author    = {Landis, Daji and Schwartzbach, Nikolaj I.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2354–2356},
  title     = {Which games are unaffected by absolute commitments?},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663158},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A SAT-based approach for argumentation dynamics.
<em>AAMAS</em>, 2351–2353. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the realm of multi-agent systems, argumentative dialogues for persuasion and negotiation involve autonomous agents exchanging arguments, necessitating continual re-evaluation of argument acceptability. This study introduces a novel approach using modern SAT solving techniques to dynamically reassess the acceptability status of arguments, aligning with various classical semantics. Our method uses the assumption mechanism in SAT solvers, distinguished by minimal assumptions, ensuring practicality.},
  archive   = {C_AAMAS},
  author    = {Lagniez, Jean-Marie and Lonca, Emmanuel and Mailly, Jean-Guy},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2351–2353},
  title     = {A SAT-based approach for argumentation dynamics},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663157},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Guided exploration in reinforcement learning via monte carlo
critic optimization. <em>AAMAS</em>, 2348–2350. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In reinforcement learning (RL), the class of deep deterministic off-policy algorithms is effectively applied to solve challenging continuous control problems. Current approaches commonly utilize random noise as an exploration method, which has several drawbacks, including the need for manual adjustment for a given task and the absence of exploratory calibration during the training process. We address these challenges by proposing a novel guided exploration method that uses an ensemble of Monte Carlo Critics for calculating exploratory action correction. The proposed method enhances the traditional exploration scheme by dynamically adjusting exploration. Subsequently, we present a novel algorithm that leverages the proposed exploratory module for both policy and critic modification. The presented algorithm demonstrates superior performance compared to modern RL algorithms across a variety of problems in the DMControl suite.},
  archive   = {C_AAMAS},
  author    = {Kuznetsov, Igor},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2348–2350},
  title     = {Guided exploration in reinforcement learning via monte carlo critic optimization},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663156},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fair scheduling of indivisible chores. <em>AAMAS</em>,
2345–2347. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of fairly assigning a set of discrete tasks (or chores) among a set of agents with additive valuations. Each chore is associated with an arrival time and a deadline, and each agent can perform at most one chore at any given time. The goal is to find a fair and efficient schedule of the chores, where fairness pertains to satisfying envy-freeness up to one chore (EF1) and efficiency pertains to maximality (i.e., no unallocated chore can be feasibly assigned to any agent). Our main result is a polynomial-time algorithm for computing an EF1 and maximal schedule for two agents under monotone valuations when the conflict constraints constitute an arbitrary interval graph. The algorithm uses a coloring technique in interval graphs that may be of independent interest. For an arbitrary number of agents, we provide an algorithm for finding a fair schedule under identical dichotomous valuations when the constraints constitute a path graph. We also rule out the existence of schedules satisfying stronger fairness and efficiency properties, including envy-freeness up to any chore (EFX) together with maximality and EF1 together with Pareto optimality.},
  archive   = {C_AAMAS},
  author    = {Kumar, Yatharth and Equbal, Sarfaraz and Gurjar, Rohit and Nath, Swaprava and Vaish, Rohit},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2345–2347},
  title     = {Fair scheduling of indivisible chores},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663155},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep hawkes process for high-frequency market making.
<em>AAMAS</em>, 2342–2344. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {High-frequency market making is a liquidity-providing trading strategy that simultaneously generates many bids and asks for a security at ultra-low latency while maintaining a relatively neutral position. The strategy makes a profit from the bid-ask spread for every buy and sell transaction, against the risk of adverse selection, uncertain execution and inventory risk. We design realistic simulations of limit order markets and develop a high-frequency market making strategy in which agents process order book information to post the optimal price, order type and execution time. By introducing the Deep Hawkes process to the high-frequency market making strategy, we allow a feedback loop to be created between order arrival and the state of the limit order book, together with self- and cross-excitation effects. Our high-frequency market making strategy accounts for the cancellation of orders that influence order queue position, profitability, bid-ask spread and the value of the order. The experimental results show that our trading agent outperforms the baseline strategy, which uses a probability density estimate of the fundamental price. We investigate the effect of cancellations on market quality and the agent&#39;s profitability. We validate how closely the simulation framework approximates reality by reproducing stylised facts from the empirical analysis of the simulated order book data.},
  archive   = {C_AAMAS},
  author    = {Kumar, Pankaj},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2342–2344},
  title     = {Deep hawkes process for high-frequency market making},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663154},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Difference of convex functions programming for policy
optimization in reinforcement learning. <em>AAMAS</em>, 2339–2341. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We formulate the problem of optimizing an agent&#39;s policy within the Markov decision process (MDP) model as a difference-of-convex functions (DC) program. The DC perspective enables optimizing the policy iteratively where each iteration constructs an easier-to-optimize lower bound on the value function using the well known concave-convex procedure. We show that several popular policy gradient based deep RL algorithms (both for discrete and continuous state, action spaces, and stochastic/deterministic policies) such as actor-critic, deterministic policy gradient (DPG), and soft actor critic (SAC) can be derived from the DC perspective. Additionally, the DC formulation enables more sample efficient learning approaches by exploiting the structure of the value function lower bound, and when the policy has a simpler parametric form, allows using efficient nonlinear programming solvers. Furthermore, we show that the DC perspective extends easily to constrained RL and partially observable and multiagent settings. Such connections provide new insight on previous algorithms, and also help develop new algorithms for RL.},
  archive   = {C_AAMAS},
  author    = {Kumar, Akshat},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2339–2341},
  title     = {Difference of convex functions programming for policy optimization in reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663153},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Electric vehicle routing for emergency power supply with
deep reinforcement learning. <em>AAMAS</em>, 2336–2338. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To maintain telecom services even during power outages, maintaining the power of the base stations is essential. Here, we consider a solution where Electric Vehicles (EVs) go around to directly supply their power to the base stations whose power is continuously decreasing. The goal is to find EV routes that minimize both total travel distance and the number of downed base stations. In this paper, we formulate this routing as a new variant of the Electric Vehicle Routing Problem (EVRP) and propose a solver that combines a rule-based vehicle selector and a reinforcement learning-based node selector. We evaluate our solver on synthetic datasets and real datasets. The results show that our solver outperforms baselines in terms of the objective value and computational time. See https://ntt-dkiku.github.io/rl-evrpeps for details (full paper, code, visualization, etc).},
  archive   = {C_AAMAS},
  author    = {Kikuta, Daisuke and Ikeuchi, Hiroki and Tajiri, Kengo and Toyama, Yuta and Nakamura, Masaki and Nakano, Yuusuke},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2336–2338},
  title     = {Electric vehicle routing for emergency power supply with deep reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663152},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GLIDE-RL: Grounded language instruction through
DEmonstration in RL. <em>AAMAS</em>, 2333–2335. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A critical capability in the development of complex human-AI collaborative systems is the ability of AI agents to understand the natural language and perform tasks accordingly.},
  archive   = {C_AAMAS},
  author    = {Kharyal, Chaitanya and Gottipati, Sai Krishna and Sinha, Tanmay and Das, Srijita and Taylor, Matthew E.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2333–2335},
  title     = {GLIDE-RL: Grounded language instruction through DEmonstration in RL},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663151},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decentralized safe control for multi-robot navigation in
dynamic environments with limited sensing. <em>AAMAS</em>, 2330–2332.
(<a href="https://dl.acm.org/doi/10.5555/3635637.3663150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Our research addresses the challenging multi-agent safe control problem where agents must reach their goals while avoiding collisions. Avoidance constraints are enforced within a limited sensing field, adding practical relevance to the problem. We propose a novel approach based on tractable Control Lyapunov Function (CLF)-based Quadratic Programs (QPs) for individual agents, enabling goal tracking while considering the dynamics of the obstacles in their limited sensing range. Our framework is highly adaptable, accommodating a large number of agents and ensuring scalability. Extensive experiments with differential drive robots illustrate the computational efficiency and scalability of our approach, even in highly occluded environments with large number of robots.},
  archive   = {C_AAMAS},
  author    = {Khan, Saad and Baranwal, Mayank and Sukumar, Srikant},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2330–2332},
  title     = {Decentralized safe control for multi-robot navigation in dynamic environments with limited sensing},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663150},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contiguous allocation of binary valued indivisible items on
a path. <em>AAMAS</em>, 2327–2329. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of allocating indivisible binary-valued items on a path among agents. The objective is to find a fair and efficient allocation in which each agent&#39;s bundle forms a contiguous block on the path. We demonstrate that deciding whether every item can be allocated to an agent who wants it is NP-complete. Consequently, we provide fixed-parameter tractable (FPT) algorithms for maximizing utilitarian social welfare, with respect to the optimum value and the number of agents. Additionally, we present a 2-approximation algorithm for the special case when the maximum utility is equal to the number of items. Furthermore, we establish that deciding whether the maximum egalitarian social welfare is at least 2 or at most 1 is an NP-complete problem. We also explore the case where the order of the blocks of items allocated to the agents is predetermined. In this case, we show that both maximum utilitarian social welfare and egalitarian social welfare can be computed in polynomial time. However, we determine that checking the existence of an EF1 allocation is NP-complete.},
  archive   = {C_AAMAS},
  author    = {Kawase, Yasushi and Roy, Bodhayan and Sanpui, Mohammad Azharuddin},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2327–2329},
  title     = {Contiguous allocation of binary valued indivisible items on a path},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663149},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the computational complexity of quasi-variational
inequalities and multi-leader-follower games. <em>AAMAS</em>, 2324–2326.
(<a href="https://dl.acm.org/doi/10.5555/3635637.3663148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a computational version of the generalized quasi-variational inequality problem and study its computational complexity, in particular proving that it is PPAD-complete. We also consider applications to multi-leader-follower games, a domain traditionally marked by the absence of general solutions. However, through the use of relaxation techniques, we obtain versions of these problems which may be formulated in terms of quasi-variational inequalities, allowing us to obtain PPAD-completeness for such games.},
  archive   = {C_AAMAS},
  author    = {Kapron, Bruce M. and Samieefar, Koosha},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2324–2326},
  title     = {On the computational complexity of quasi-variational inequalities and multi-leader-follower games},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663148},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TIMAT: Temporal information multi-agent transformer.
<em>AAMAS</em>, 2321–2323. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many specific tasks, training models with Multi-Agent Reinforcement Learning (MARL) to solve a task often leads to overfitting to the training environment. When dealing with multi-task, models specialized for a single task often fail to generalize, and retraining models often implies the consumption of computational resources. Therefore, it is necessary to establish a pre-trained model that can be quickly deployed in an online environment. Therefore, we propose temporal information multi-agent transformer (TIMAT) based on the transformer that extracts temporal information and models MARL as Sequence Models (SM). The advantage of this framework is that it can handle time information of arbitrary length and any number of agents regardless of the type, which greatly enhances the generalization ability of the model.},
  archive   = {C_AAMAS},
  author    = {Kang, Qitong and Wang, Fuyong and Liu, Zhongxin and Chen, Zengqiang},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2321–2323},
  title     = {TIMAT: Temporal information multi-agent transformer},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663147},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NPPP-completeness of control by adding players to change the
penrose-banzhaf power index in weighted voting games. <em>AAMAS</em>,
2318–2320. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Weighted voting games are an important class of simple games that can be compactly represented and have many real-world applications. Rey and Rothe [14] introduced the notion of structural control by adding players to or deleting them from weighted voting games, with the goal to either change or maintain a given player&#39;s power in a given game with respect to the (probabilistic) Penrose-Banzhaf power index [4] or the Shapley-Shubik power index [17]. For control by adding players, they showed PP-hardness as the best known lower bound and an upper bound of NPPP, where PP is &quot;probabilistic polynomial time.&quot; We optimally improve their results by establishing NPPP-hardness (and thus NPPP-completeness) of all problems related to the Penrose-Banzhaf index and for the problem of maintaining the Shapley-Shubik index when players are added.},
  archive   = {C_AAMAS},
  author    = {Kaczmarek, Joanna and Rothe, J\&quot;{o}rg},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2318–2320},
  title     = {NPPP-completeness of control by adding players to change the penrose-banzhaf power index in weighted voting games},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663146},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual-policy-guided offline reinforcement learning with
optimal stopping. <em>AAMAS</em>, 2315–2317. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Policy-guided offline reinforcement learning (POR) decomposes the offline reinforcement learning (offline RL) problem into goal estimation and goal-conditioned execution subproblems, leading to improved performance. However, we reveal that the preciseness of the estimated goal massively affects the performance and robustness of the trained goal-conditioned policy. To overcome this problem, we propose an offline RL model with dual guide-policies to improve the preciseness of the goal and reduce the variance. The proposed dual-policy-guided offline RL (Dual POR) adopts an integrating function, which balances the goals predicted by two guide-policies to obtain a refined goal. Moreover, we employ the optimal stopping strategy to schedule the training process, which dramatically shortens the training process and improves the generalization. The proposed Dual POR achieves state-of-the-art performance on the D4RL datasets with reduced variances. The improvements in high-complexity tasks are even significant, which indicates the potential of the proposed Dual POR in real-world applications.},
  archive   = {C_AAMAS},
  author    = {Jiang, Weibo and Li, Shaohui and Li, Zhi and Ke, Yuxin and Jiang, Zhizhuo and Li, Yaowen and Liu, Yu},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2315–2317},
  title     = {Dual-policy-guided offline reinforcement learning with optimal stopping},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663145},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Strategic routing and scheduling for evacuations.
<em>AAMAS</em>, 2312–2314. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Evacuation planning is an essential part of disaster management where the goal is to relocate people under imminent danger to safety. Although government authorities often prescribe routes and schedule, evacuees generally behave as self-interested agents and may choose their actions in a selfish manner. It is crucial to understand the degree of inefficiency this can cause to the evacuation process. In this paper, we present a strategic routing and scheduling game (Evacuation Planning Game, EPG), where evacuees choose their route and time of departure. We prove that every instance of EPG has at least one pure strategy Nash equilibrium. We then present a polynomial time algorithm (Sequential Action Algorithm, SAA), for finding equilibria in a given instance. We also provide bounds on how bad an equilibrium state can be compared to a socially optimal state. Finally, we use Harris County of Houston, Texas as our study area and construct a game instance for it. Our results show that, SAA can efficiently find equilibria in this instance that have social objective close to the optimal value.},
  archive   = {C_AAMAS},
  author    = {Islam, Kazi Ashik and Chen, Da Qi and Marathe, Madhav and Mortveit, Henning and Swarup, Samarth and Vullikanti, Anil},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2312–2314},
  title     = {Strategic routing and scheduling for evacuations},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663144},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Computing nash equilibria in multidimensional congestion
games. <em>AAMAS</em>, 2309–2311. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study pure-strategy Nash equilibrium (PSNE) computation in k-dimensional congestion games (k-DCGs) where the weights or demands of the players are k-dimensional vectors. We first show that deciding the existence of a PSNE in a k-DCG is NP-complete even for games when players have binary and unit demand vectors. We then focus on computing PSNE for k-DCGs and their variants with general, linear, and exponential cost functions. For general cost functions (potentially non-monotonic), we provide the first configuration-space framework to find a PSNE if one exists. For linear and exponential cost functions, we provide potential function-based algorithms to find a PSNE. These algorithms run in polynomial time under certain assumptions. We also study structured demands and cost functions, giving polynomial-time algorithms to compute PSNE for several cases. For general cost functions, we give a constructive proof of existence for an (α, β)-PSNE (for certain α and β), where α and β are multiplicative and additive approximation factors, respectively.},
  archive   = {C_AAMAS},
  author    = {Irfan, Mohammad T. and Chan, Hau and Soundy, Jared},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2309–2311},
  title     = {Computing nash equilibria in multidimensional congestion games},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663143},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distribution of chores with information asymmetry.
<em>AAMAS</em>, 2306–2308. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One well-regarded fairness notion in dividing indivisible chores is envy-freeness up to one item (EF1), which requires that pairwise envy can be eliminated by the removal of a single item. While an EF1 and Pareto optimal (PO) allocation of goods can always be found via well-known algorithms, even the existence of such solutions for chores remains open, to date. We take an epistemic approach to identify such allocations utilizing information asymmetry by introducing dubious chores - items that inflict no cost on receiving agents but are perceived to be costly by others. On a technical level, dubious chores provide a more fine-grained approximation of envy-freeness than EF1. We show that finding allocations with minimal number of dubious chores is computationally hard. Nonetheless, we prove the existence of envy-free and fractional PO allocations for n agents with only 2n-2 dubious chores and strengthen it to n-1 dubious chores in four special classes of valuations.},
  archive   = {C_AAMAS},
  author    = {Hosseini, Hadi and Kavner, Joshua and W\k{a}s, Tomasz and Xia, Lirong},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2306–2308},
  title     = {Distribution of chores with information asymmetry},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663142},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Addressing permutation challenges in multi-agent
reinforcement learning. <em>AAMAS</em>, 2303–2305. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In Reinforcement Learning, deep neural networks play a crucial role, especially in Multi-Agent Systems. Owing to information from multiple sources, the challenge lies in handling input permutations efficiently, causing sample inefficiency and delayed convergence. Traditional approaches treat each permutation source as individual nodes for inference. Our novel approach integrates an attention mechanism, allowing us to capture temporal dependencies and contextually align inputs. The attention mechanism enhances the alignment process, allowing for improved information processing. Empirical evaluations on SMAC environments demonstrate superior performance compared to baselines, achieving a higher win rate on 68\% of test evaluations.},
  archive   = {C_AAMAS},
  author    = {Hazra, Somnath and Dasgupta, Pallab and Dey, Soumyajit},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2303–2305},
  title     = {Addressing permutation challenges in multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663141},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Taking agent-based social simulation to the next level using
exascale computing: Potential use-cases, capacity requirements and
threats. <em>AAMAS</em>, 2300–2302. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Exascale computing (1018 FLOPS) officially arrived in 2022 when Oak Ridge&#39;s Frontier achieved that performance benchmark, and other countries seek their own exascale capabilities. High-end computing is typically used by the natural sciences, but empirical Agent-Based Social Simulation (ABSS) is a social science application. Empirical ABSS has a long history, but was prominent during the Covid crisis. In future crises, policy options could be evaluated within rapid policy design windows using exascale computing. We report on a group model-building exercise, co-constructing a causal loop model, to explore visions of the potential of exascale computing in ABSS, identifying potential use cases, capabilities, capacity requirements and threats.},
  archive   = {C_AAMAS},
  author    = {Hare, Matt and Salt, Doug and Colasanti, Ric and Milton, Richard and Batty, Mike and Heppenstall, Alison and Polhill, Gary},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2300–2302},
  title     = {Taking agent-based social simulation to the next level using exascale computing: Potential use-cases, capacity requirements and threats},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663140},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards socially-acceptable multi-criteria resolution of the
4D-contracts repair problem. <em>AAMAS</em>, 2297–2299. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We use multi-agent systems to solve conflicts between drone flight paths (4D contracts) in urban traffic, installing safety and service quality. Intuitive corrective actions are chosen considering delay, quality, and energy. We explore different algorithms based on graph search, auctions, and distributed optimization for decision-making and action evaluation. We test these in a simulated surveillance scenario with unforeseen emergency trajectories.},
  archive   = {C_AAMAS},
  author    = {Hamadi, Youssef and Picard, Gauthier},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2297–2299},
  title     = {Towards socially-acceptable multi-criteria resolution of the 4D-contracts repair problem},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663139},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforcement learning for question answering in programming
domain using public community scoring as a human feedback.
<em>AAMAS</em>, 2294–2296. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study explores improving GPT Neo 125M in programming-focused Community Question Answering (CQA) using Reinforcement Learning from Human Feedback (RLHF) and Stack Overflow scores. We utilized two reward model training strategies with Proximal Policy Optimization (PPO), achieving enhancements comparable to GPT Neo&#39;s 2.7B model. The research introduces an auxiliary scoring mechanism, revealing the limitations of traditional linguistic metrics for programming responses. It highlights the need for domain-specific evaluation methods and the challenges in applying RLHF to programming CQA, contributing to the advancement of Large Language Models (LLMs) with human feedback.},
  archive   = {C_AAMAS},
  author    = {Gorbatovski, Alexey and Kovalchuk, Sergey},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2294–2296},
  title     = {Reinforcement learning for question answering in programming domain using public community scoring as a human feedback},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663138},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging approximate model-based shielding for
probabilistic safety guarantees in continuous environments.
<em>AAMAS</em>, 2291–2293. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Shielding is a popular technique for achieving safe reinforcement learning (RL). However, classical shielding approaches come with quite restrictive assumptions making them difficult to deploy in complex environments, particularly those with continuous state or action spaces. In this paper we extend the more versatile approximate model-based shielding (AMBS) framework to the continuous setting. In particular we use Safety Gym as our test-bed, allowing for a more direct comparison of AMBS with popular constrained RL algorithms. We also provide strong probabilistic safety guarantees for the continuous setting. In addition, we propose two novel penalty techniques that directly modify the policy gradient, which empirically provide more stable convergence in our experiments.},
  archive   = {C_AAMAS},
  author    = {Goodall, Alexander W. and Belardinelli, Francesco},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2291–2293},
  title     = {Leveraging approximate model-based shielding for probabilistic safety guarantees in continuous environments},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663137},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Indirect credit assignment in a multiagent system.
<em>AAMAS</em>, 2288–2290. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning in a multiagent system requires structural credit assignment to distill system performance into agent-specific feedback. Fitness shaping methods largely isolate agent credit, but struggle when an agent&#39;s actions do not directly affect system feedback. This work introduces D-Indirect, a fitness shaping method that gives credit for both direct actions and actions that have an indirect impact on the system&#39;s performance. We demonstrate the effectiveness of D-Indirect in a simulated shepherding scenario and our results show that learning with D-Indirect significantly outperforms learning with the standard difference evaluation and the system evaluation when agents indirectly impact system performance.},
  archive   = {C_AAMAS},
  author    = {Gonzalez, Everardo and Viswanathan, Siddarth and Tumer, Kagan},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2288–2290},
  title     = {Indirect credit assignment in a multiagent system},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663136},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Facility location games with task allocation.
<em>AAMAS</em>, 2285–2287. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Facility location games have been studied extensively but most are about locating facilities given agents&#39; profiles. However, in some real-life scenarios, the facility&#39;s location may be fixed already. When there are multiple facilities the strategic agents will always go to the closest one, resulting in the remote facilities unused. In this paper, we introduce the model that includes two facilities and n rational agents. There is one task at each facility to be done. Each agent will select one task and aims to minimize the amount of work assigned to her. Our goal is to design the allocation rules to achieve social optimality, i.e., every Nash equilibrium guarantees that every task can be completed. We show that no allocation rule can achieve social optimality without positive/negative incentives. For negative incentives, we propose a class of allocation rules with dummy work, where social optimality can be achieved, and no worker does the dummy work. For positive incentives, we first give a simple rule that achieves social optimality and propose a more complex rule to achieve the minimum subsidy.},
  archive   = {C_AAMAS},
  author    = {Gong, Zifan and Li, Minming and Zhou, Houyu},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2285–2287},
  title     = {Facility location games with task allocation},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663135},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Risk-sensitive multi-agent reinforcement learning in network
aggregative markov games. <em>AAMAS</em>, 2282–2284. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Classical multi-agent reinforcement learning (MARL) assumes risk neutrality and complete objectivity for agents. However, in settings where agents need to consider or model human economic or social preferences, a notion of risk must be incorporated into the RL optimization problem. This will be of greater importance in MARL where other human or non-human agents are involved, possibly with their own risk-sensitive policies. In this work, we consider risk-sensitive and non-cooperative MARL with cumulative prospect theory (CPT), a non-convex risk measure and a generalization of coherent measures of risk. CPT is capable of explaining loss aversion in humans and their tendency to overestimate/underestimate small/large probabilities. We propose a distributed sampling-based actor-critic (AC) algorithm with CPT risk for network aggregative Markov games (NAMGs), which we call Distributed Nested CPT-AC. Under a set of assumptions, we prove the convergence of the algorithm to a subjective notion of Markov perfect Nash equilibrium in NAMGs. The experimental results show that subjective CPT policies obtained by our algorithm can be different from the risk-neutral ones, and agents with a higher loss aversion are more inclined to socially isolate themselves in an NAMG.},
  archive   = {C_AAMAS},
  author    = {Ghaemi, Hafez and Kebriaei, Hamed and Ramezani Moghaddam, Alireza and Nili Ahmadabadi, Majid},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2282–2284},
  title     = {Risk-sensitive multi-agent reinforcement learning in network aggregative markov games},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663134},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Benchmarking MARL on long horizon sequential multi-objective
tasks. <em>AAMAS</em>, 2279–2281. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current MARL benchmarks fall short in simulating realistic scenarios, particularly those involving long action sequences with sequential tasks and multiple conflicting objectives. Addressing this gap, we introduce Multi-Objective SMAC (MOSMAC), a novel MARL benchmark tailored to assess MARL methods on tasks with varying time horizons and multiple objectives. Each MOSMAC task contains one or multiple sequential subtasks. Agents are required to simultaneously balance between two objectives - combat and navigation - to successfully complete each subtask. Our evaluation of nine state-of-the-art MARL algorithms reveals that MOSMAC presents substantial challenges to many state-of-the-art MARL methods and effectively fills a critical gap in existing benchmarks for both single-objective and multi-objective MARL research.},
  archive   = {C_AAMAS},
  author    = {Geng, Minghong and Pateria, Shubham and Subagdja, Budhitama and Tan, Ah-Hwee},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2279–2281},
  title     = {Benchmarking MARL on long horizon sequential multi-objective tasks},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663133},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Behaviour modelling of social animals via causal structure
discovery and graph neural networks. <em>AAMAS</em>, 2276–2278. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Better understanding the natural world is a crucial task with a wide range of applications. In environments with close proximity between humans and animals, such as zoos, it is essential to better understand the causes behind animal behaviour to predict unusual changes, mitigate their detrimental effects and increase the well-being of animals. However, the complex social behaviours of mammalian groups remain largely unexplored. In this work, we propose a method to build behavioural models using causal structure discovery and graph neural networks for time series. We apply this method to a mob of meerkats in a zoo environment and study its ability to predict future actions and model the behaviour distribution at an individual-level and at a group level. We show that our method can match and outperform standard deep learning architectures and generate more realistic data, while using fewer parameters and providing increased interpretability.},
  archive   = {C_AAMAS},
  author    = {Gendron, Ga\&quot;{e}l and Chen, Yang and Rogers, Mitchell and Liu, Yiping and Azhar, Mihailo and Heidari, Shahrokh and Valdez, David Arturo Soriano and Knowles, Kobe and O&#39;Leary, Padriac and Eyre, Simon and Witbrock, Michael and Dobbie, Gillian and Liu, Jiamou and Delmas, Patrice},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2276–2278},
  title     = {Behaviour modelling of social animals via causal structure discovery and graph neural networks},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663132},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Combinatorial client-master multiagent deep reinforcement
learning for task offloading in mobile edge computing. <em>AAMAS</em>,
2273–2275. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep reinforcement learning (DRL) is gaining popularity in task-offloading problems because it can adapt to dynamic changes and minimize online computational complexity. However, the various types of continuous and discrete resource constraints on user devices (UDs) and mobile edge computing (MEC) servers pose challenges to the design of an efficient DRL-based task-offloading strategy. Existing DRL-based task-offloading algorithms focus on the constraints of the UDs, assuming the availability of enough storage resources on the server. Moreover, existing multiagent DRL (MADRL)-based task-offloading algorithms are homogeneous agents and consider homogeneous constraints as a penalty in their reward function. In this work, we propose a novel combinatorial client-master MADRL (CCM_MADRL) algorithm for task offloading in mobile edge computing (CCM_MADRL_MEC) that allows UDs to decide their resource requirements and the server to make a combinatorial decision based on the UDs&#39; requirements. CCM_MADRL_MEC is the first MADRL approach in task offloading to consider server storage capacity in addition to the constraints of the UDs. By taking advantage of the combinatorial action selection, CCM_MADRL_MEC has shown superior convergence over existing benchmark and heuristic algorithms.},
  archive   = {C_AAMAS},
  author    = {Gebrekidan, Tesfay Zemuy and Stein, Sebastian and Norman, Timothy J.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2273–2275},
  title     = {Combinatorial client-master multiagent deep reinforcement learning for task offloading in mobile edge computing},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663131},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Synthesizing social laws with ATL conditions.
<em>AAMAS</em>, 2270–2272. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a formalism called SLAM (Social Laws on ATL Models) for defining social laws. Such social laws can constrain the behaviour of a multi-agent system. Importantly, these social laws can use any ATL formula as the condition under which an action is allowed. We show that the synthesis problem for these social laws is NP-complete. This generalizes a known result that synthesis of social laws that use only Boolean conditions is NP-complete.},
  archive   = {C_AAMAS},
  author    = {Galimullin, Rustam and Kuijer, Louwe B.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2270–2272},
  title     = {Synthesizing social laws with ATL conditions},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663130},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Aleatoric predicates: Reasoning about marbles.
<em>AAMAS</em>, 2267–2269. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Aleatoric Logic is the logic of dice, where Boolean propositions are replaced by independent probabilistic events. In a first order extension of this notion, Aleatoric predicates are applied to domain elements selected via independent probabilistic events. An analogy for this is the classic marbles in an urn problem, where we might ask the probability of drawing three marbles of the same colour from an urn, or drawing only black marbles from an urn until a red marble is drawn. This paper formalises a syntax and semantics for propositions built from aleatoric predicates, and discusses how these predicates give a representation of an agent&#39;s beliefs that come through experience.},
  archive   = {C_AAMAS},
  author    = {French, Tim},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2267–2269},
  title     = {Aleatoric predicates: Reasoning about marbles},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663129},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A negotiator’s backup plan: Optimal concessions with a
reservation value. <em>AAMAS</em>, 2264–2266. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automated negotiation is a well-known mechanism for autonomous agents to reach agreements. To realize beneficial agreements quickly, it is key to employ a good bidding strategy. When a negotiating agent has a good back-up plan, i.e., a high reservation value, failing to reach an agreement is not necessarily disadvantageous. Thus, the agent can adopt a risk-seeking strategy, aiming for outcomes with a higher utilities.Accordingly, this paper develops an optimal bidding strategy called MIA-RVelous for bilateral negotiations with private reservation values. The proposed greedy algorithm finds the optimal bid sequence given the agent&#39;s beliefs about the opponent in O(n2D) time, with D the maximum number of rounds and n the number of outcomes. The results obtained here can pave the way to realizing effective concurrent negotiations, given that concurrent negotiations can serve as a (probabilistic) backup plan.},
  archive   = {C_AAMAS},
  author    = {Florijn, Tamara C.P. and Yolum, Pinar and Baarslag, Tim},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2264–2266},
  title     = {A negotiator&#39;s backup plan: Optimal concessions with a reservation value},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663128},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Influence-focused asymmetric island model. <em>AAMAS</em>,
2261–2263. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning good joint-behaviors is challenging in multiagent settings due to the inherent non-stationarity: agents adapt their policies and act simultaneously. This is aggravated when the agents are asymmetric (agents have distinct capabilities and objectives) and must learn complementary behaviors required to work as a team. The Asymmetric Island Model partially addresses this by independently optimizing class-specific and team-wide behaviors. However, optimizing class-specific behaviors in isolation can produce egocentric behaviors that yield sub-optimal inter-class behaviors. This work introduces the Influence-Focused Asymmetric Island model (IF-AIM), a hierarchical framework that explicitly reinforces inter-class behaviors by optimizing class-specific behaviors conditioned on the expectation of behaviors of the complementary agent classes. An experiment in the harvest environment highlights the effectiveness of our method in optimizing adaptable inter-class behaviors.},
  archive   = {C_AAMAS},
  author    = {Festa, Andrew and Dixit, Gaurav and Tumer, Kagan},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2261–2263},
  title     = {Influence-focused asymmetric island model},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663127},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deceptive path planning via reinforcement learning with
graph neural networks. <em>AAMAS</em>, 2258–2260. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deceptive path planning (DPP) is the problem of designing a path that hides its true goal from an outside observer. Existing methods for DPP rely on unrealistic assumptions, such as global state observability and perfect model knowledge, and therefore do not generalize to unseen problem instances, lack scalability to realistic problem sizes, and preclude both on-the-fly tunability of deception levels and real-time adaptivity to changing environments. In this paper, we propose a reinforcement learning (RL)-based scheme that overcomes these issues. Through extensive experimentation we show that, without additional fine-tuning, at test time the resulting policies successfully generalize, scale, enjoy tunable levels of deception, and adapt in real-time to changes in the environment.},
  archive   = {C_AAMAS},
  author    = {Fatemi, Michael Y. and Suttle, Wesley A. and Sadler, Brian M.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2258–2260},
  title     = {Deceptive path planning via reinforcement learning with graph neural networks},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663126},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Strategic cost selection in participatory budgeting.
<em>AAMAS</em>, 2255–2257. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study strategic behavior of project proposers in the context of participatory budgeting. We assume that the votes are fixed and known and the proposers want to set as high project prices as possible, provided that their projects get selected and the prices are not below the minimum costs of their delivery. We study the existence of Nash equilibria in such games. Furthermore, we report an experimental study of the games we propose.},
  archive   = {C_AAMAS},
  author    = {Faliszewski, Piotr and Janeczko, \L{}ukasz and Kaczmarczyk, Andrzej and Lisowski, Grzegorz and Skowron, Piotr and Szufa, Stanisundefinedaw},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2255–2257},
  title     = {Strategic cost selection in participatory budgeting},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663125},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attention graph for multi-robot social navigation with deep
reinforcement learning. <em>AAMAS</em>, 2252–2254. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present MultiSoc, a new method for learning multi-agent socially aware navigation strategies using RL. Inspired by recent works on multi-agent deep RL, our method leverages graph-based representation of agent interactions, combining the positions and fields of view of entities (pedestrians and agents). Each agent uses a model based on two Graph Neural Networks combined with attention mechanisms. First an edge-selector produces a sparse graph, then a crowd coordinator applies node attention to produce a graph representing the influence of each entity on the others. This is incorporated into a model-free RL framework to learn multi-agent policies. We evaluate our approach on simulation and provide a series of experiments in a set of various conditions that conclude that our method learns faster than social navigation deep RL mono-agent techniques, and enables efficient multi-agent implicit coordination in challenging crowd navigation with multiple heterogeneous humans. A full description and analysis of this work is available in the full paper version [2].},
  archive   = {C_AAMAS},
  author    = {Escudie, Erwan and Matignon, Laetitia and Saraydaryan, Jacques},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2252–2254},
  title     = {Attention graph for multi-robot social navigation with deep reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663124},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Computational theory of mind with abstractions for effective
human-agent collaboration. <em>AAMAS</em>, 2249–2251. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Empowering artificially intelligent agents with capabilities that humans use regularly is crucial to enable effective human-agent collaboration. One of these crucial capabilities is the modeling of Theory of Mind (ToM) reasoning: the human ability to reason about the mental content of others such as their beliefs, desires, and goals. However, it is generally impractical to track all individual mental attitudes of all other individuals and for many practical situations not even necessary. Hence, what is important is to capture enough information to create an approximate model that is effective and flexible. Accordingly, this paper proposes a computational ToM mechanism based on abstracting beliefs and knowledge into higher-level human concepts, called abstractions, similar to the ones that guide humans to effectively interact with each other (e.g., trust). We develop an agent architecture based on epistemic logic to formalize the computational dynamics of ToM reasoning. We identify important challenges regarding effective maintenance of abstractions and accurate use of ToM reasoning and demonstrate how our approach addresses these challenges over multiagent simulations.},
  archive   = {C_AAMAS},
  author    = {Erdogan, Emre and Verbrugge, Rineke and Yolum, P\i{}nar},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2249–2251},
  title     = {Computational theory of mind with abstractions for effective human-agent collaboration},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663123},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Verifying proportionality in temporal voting.
<em>AAMAS</em>, 2246–2248. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study a model of multiwinner voting where candidates are selected sequentially in rounds over a time horizon. Prior work has adapted popular notions of justified representation as well as voting rules that provide strong representation guarantees from the standard single-round multiwinner election case to the temporal setting. In our work, we focus on the complexity of verifying whether a given outcome is proportional. We show that the temporal setting is strictly harder than the standard single-round model of multiwinner voting, but identify natural special cases that enable efficient verification.},
  archive   = {C_AAMAS},
  author    = {Elkind, Edith and Obraztsova, Svetlana and Teh, Nicholas},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2246–2248},
  title     = {Verifying proportionality in temporal voting},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663122},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pruning neural networks using cooperative game theory.
<em>AAMAS</em>, 2243–2245. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce Game Theoretic Assisted Pruning (GTAP), a method that utlizes power indices from cooperative game theory to efficiently prune deep neural networks without compromising their predictive performance. GTAP identifies and removes less impactful neurons based on their contribution to the network&#39;s performance, streamlining the model&#39;s size and computational load. Our empirical evaluations show that GTAP outperforms traditional pruning techniques, achieving a better balance between model compactness and accuracy across multiple types of neural networks.},
  archive   = {C_AAMAS},
  author    = {Diaz-Ortiz, Mauricio and Kempinski, Benjamin and Cornelisse, Daphne and Bachrach, Yoram and Kachman, Tal},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2243–2245},
  title     = {Pruning neural networks using cooperative game theory},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663121},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comparison of the myerson value and the position value.
<em>AAMAS</em>, 2240–2242. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the realm of graph-restricted games, the underlying network structure plays a pivotal role, enforcing a key constraint: communication between agents is only feasible if a valid path connecting them exists within the network. This constraint significantly influences the dynamics and strategies, particularly in value allocation scenarios among connected agents.Among various contributions to the allocation rules for such network-centric scenarios, Myerson&#39;s pioneering work stands out [4, 5]. Named after him, the Myerson value represents an adaptation of the Shapley value [7]. Another prominent concept in this domain is the position value. Both serve as solution concepts, offering distinct perspectives, with the Myerson value focusing on agents and the position value [1, 6] on links between agents.We provide an axiomatic characterization of the Myerson value based on two fundamental axioms. Expanding our investigation, a subtle modification of the first axiom leads to a characterization of the position value. This extension enables comparing these value operators, highlighting their essential distinctions and similarities.},
  archive   = {C_AAMAS},
  author    = {Derya, Ayse Mutlu},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2240–2242},
  title     = {A comparison of the myerson value and the position value},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663120},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluation of robustness of off-road autonomous driving
segmentation against adversarial attacks: A dataset-centric study.
<em>AAMAS</em>, 2237–2239. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The study explores the vulnerability of semantic segmentation models to adversarial input perturbations in the domain of off-road autonomous driving. Existing studies have primarily concentrated on enhancing model&#39;s robustness via architectural modifications along-with using noisy images during training. On the contrary, little attention has been paid to investigating the impact of datasets on the adversarial attacks. Our study aims to address this gap by examining the impact of non-robust features in off-road datasets and comparing the effects of adversarial attacks on different segmentation network architectures. To enable this, a robust dataset is created consisting of only robust features and training the networks on this robustified dataset. We present both qualitative and quantitative analysis of our findings. The code is publicly available at (https://github.com/rohtkumar/adversarial_attacks_on_segmentation)},
  archive   = {C_AAMAS},
  author    = {Deoli, Pankaj and Kumar, Rohit and Vierling, Axel and Berns, Karsten},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2237–2239},
  title     = {Evaluation of robustness of off-road autonomous driving segmentation against adversarial attacks: A dataset-centric study},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663119},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attila: A negotiating agent for the game of diplomacy, based
on purely symbolic a.i. <em>AAMAS</em>, 2234–2236. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The board game of Diplomacy is considered one of the most challenging test cases for automated negotiation. While many bots have been developed for this game, very few of them are able to negotiate successfully, and the ones that do have been trained on large data sets of human example games. This makes it hard to apply the same techniques to other games or negotiation scenarios for which no human knowledge is (yet) available. Furthermore, since those bots were trained using deep learning, they are essentially black-boxes for which it is hard to understand how they work. So, these bots do not help us much in gaining a better understanding of strong negotiation techniques. We therefore present a new Diplomacy bot, called Attila, that is purely based on symbolic A.I. techniques. It makes use of an existing oracle for the tactical part of the game, called the &#39;D-Brane Tactical Module&#39; (DBTM). We explain how the DBTM can be converted into a search algorithm for automated negotiation and we present experiments that show that Attila strongly outperforms several state-of-the-art Diplomacy bots.},
  archive   = {C_AAMAS},
  author    = {de Jonge, Dave and Rodriguez Cima, Laura},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2234–2236},
  title     = {Attila: A negotiating agent for the game of diplomacy, based on purely symbolic A.I.},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663118},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning for population-dependent controls in mean
field control problems with common noise. <em>AAMAS</em>, 2231–2233. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose several approaches to learn the optimal population-dependent controls in order to solve mean field control problems (MFC). Such policies enable us to solve MFC problems with forms of common noises at a level of generality that was not covered by existing methods. We analyze rigorously the theoretical convergence of the proposed approximation algorithms. Of particular interest for its simplicity of implementation is the N-particle approximation. The effectiveness and the flexibility of our algorithms is supported by numerical experiments comparing several combinations of distribution approximation techniques and neural network architectures. We use three different benchmark problems from the literature: a systemic risk model, a price impact model, and a crowd motion model. We first show that our proposed algorithms converge to the correct solution in an explicitly solvable MFC problem. Then, we show that population-dependent controls outperform state-dependent controls. Along the way, we show that specific neural network architectures can improve the learning further.},
  archive   = {C_AAMAS},
  author    = {Dayanikli, G\&quot;{o}k\c{c}e and Lauri\`{e}re, Mathieu and Zhang, Jiacheng},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2231–2233},
  title     = {Deep learning for population-dependent controls in mean field control problems with common noise},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663117},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). No transaction fees? No problem! Achieving fairness in
transaction fee mechanism design. <em>AAMAS</em>, 2228–2230. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The recently proposed Transaction Fee Mechanism (TFM) literature studies the strategic interaction between the miner of a block and the transaction creators (or users) in a blockchain. In a TFM, the miner includes transactions that maximize its utility while users submit fees for a slot in the block. The existing TFM literature focuses on satisfying standard incentive properties - which may limit widespread adoption. We argue that a TFM is &quot;fair&quot; to the transaction creators if it satisfies specific notions, namely Zero-fee Transaction Inclusion and Monotonicity. First, we prove that one generally cannot ensure both these properties and prevent a miner&#39;s strategic manipulation. We also show that existing TFMs either do not satisfy these notions or do so at a high cost to the miners&#39; utility. As such, we introduce a novel TFM using on-chain randomness - rTFM. We prove that rTFM guarantees incentive compatibility for miners and users while satisfying our novel fairness notions.},
  archive   = {C_AAMAS},
  author    = {Damle, Sankarshan and Srivastava, Varul and Gujar, Sujit},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2228–2230},
  title     = {No transaction fees? no problem! achieving fairness in transaction fee mechanism design},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663116},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analyzing crowdfunding of public projects under dynamic
beliefs. <em>AAMAS</em>, 2225–2227. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the last decade, social planners have used crowdfunding to raise funds for public projects. As these public projects are non-excludable, the beneficiaries may free-ride. Thus, there is a need to design incentive mechanisms for such strategic agents to contribute to the project. The existing mechanisms, like PPR or PPRx, assume that the agent&#39;s beliefs about the project getting funded do not change over time, i.e., their beliefs are static. Researchers highlight that unless appropriately incentivized, the agents defer their contributions in static settings, leading to a &quot;race&#39;&#39; to contribute at the deadline. In this work, we model the evolution of agents&#39; beliefs as a random walk. We study PPRx - an existing mechanism for the static belief setting - in this dynamic belief setting and refer to it as PPRx-DB for readability. We prove that in PPRx-DB, the project is funded at equilibrium. More significantly, we prove that under certain conditions on agent&#39;s belief evolution, agents will contribute as soon as they arrive at the mechanism. Thus, we believe that by incorporating dynamic belief evolution in analysis, the social planner may mitigate the concern of race conditions in many mechanisms.},
  archive   = {C_AAMAS},
  author    = {Damle, Sankarshan and Gujar, Sujit},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2225–2227},
  title     = {Analyzing crowdfunding of public projects under dynamic beliefs},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663115},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inferring lewisian common knowledge using theory of mind
reasoning in a forward-chaining rule engine. <em>AAMAS</em>, 2222–2224.
(<a href="https://dl.acm.org/doi/10.5555/3635637.3663114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a practical technique for inferring common knowledge based on the approach of David Lewis, who identified three conditions that are sufficient for information about the world and other agents&#39; reasoning mechanisms to lead to chains of iterated mutual knowledge. We consider agents with theory-of-mind rules that model other agents&#39; beliefs. We prove that only two levels of nested models of other agents are necessary to achieve common knowledge. We illustrate this approach with an implemented scenario involving information on monuments in a public forum.},
  archive   = {C_AAMAS},
  author    = {Cranefield, Stephen and Srivathsan, Sriashalya and Pitt, Jeremy},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2222–2224},
  title     = {Inferring lewisian common knowledge using theory of mind reasoning in a forward-chaining rule engine},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663114},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Near-optimal online resource allocation in the random-order
model. <em>AAMAS</em>, 2219–2221. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of allocating either divisible or indivisible items (goods or chores) among a set of agents, where the items arrive online, one at a time. Each agent&#39;s non-negative value for an item is set by an adversary upon the item&#39;s arrival. Our focus is on a unifying algorithmic framework for finding online allocations that treats both fairness and economic efficiency. For this sake, we aim to optimize the generalized means of agents&#39; received values, covering a spectrum of welfare functions including average utilitarian welfare and egalitarian welfare. In the traditional adversarial model, where items arrive in an arbitrary order, no algorithm can give a decent approximation to welfare in the worst case. To escape from this strong lower bound, we consider the random-order model, where items arrive in a uniformly random order. This model provides us with a major breakthrough: we devise algorithms that guarantee a nearly-optimal competitive ratio for certain welfare functions, if the welfare obtained by the optimal allocation is sufficiently large. We prove that our results are almost tight: if the optimal solution&#39;s welfare is strictly below a certain threshold, then no nearly-optimal algorithm exists, even in the random-order model.},
  archive   = {C_AAMAS},
  author    = {Cohen, Saar and Agmon, Noa},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2219–2221},
  title     = {Near-optimal online resource allocation in the random-order model},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663113},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A reinforcement learning framework for studying group and
individual fairness. <em>AAMAS</em>, 2216–2218. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning is a commonly used technique for optimising objectives in decision support systems for complex problem solving. When these systems affect individuals or groups, it is essential to reflect on fairness. As absolute fairness is in practice not achievable, we propose a framework which allows to balance distinct fairness notions along with the primary objective. To this end, we formulate group and individual fairness in sequential fairness notions. First, we present an extended Markov decision process, undefinedMDP, that is explicitly aware of individuals and groups. Next, we formalise fairness notions in terms of this undefinedMDP which allows us to evaluate the primary objective along with the fairness notions that are important to the user, taking a multi-objective reinforcement learning approach. To evaluate our framework, we consider two scenarios that require distinct aspects of the performance-fairness trade-off: job hiring and fraud detection. The objectives in job hiring are to compose strong teams, while providing equal treatment to similar individual applicants and to groups in society. The trade-off in fraud detection is the necessity of detecting fraudulent transactions, while distributing the burden for customers of checking transactions fairly. In this framework, we further explore the influence of distance metrics on individual fairness and highlight the impact of the history size on the fairness calculations and the obtainable fairness through exploration.},
  archive   = {C_AAMAS},
  author    = {Cimpean, Alexandra and Jonker, Catholijn and Libin, Pieter and Now\&#39;{e}, Ann},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2216–2218},
  title     = {A reinforcement learning framework for studying group and individual fairness},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663112},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Minimizing negative side effects in cooperative multi-agent
systems using distributed coordination. <em>AAMAS</em>, 2213–2215. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous agents in real-world environments may encounter undesirable outcomes or negative side effects (NSEs) when working collaboratively alongside other agents. We frame the challenge of minimizing NSEs in a multi-agent setting as a lexicographic decentralized Markov decision process in which we assume independence of rewards and transitions with respect to the primary assigned tasks, but allowing negative side effects to create a form of dependence among the agents. We present a lexicographic Q-learning approach to mitigate the NSEs using human feedback models while maintaining near-optimality with respect to the assigned tasks-up to some given slack. Our empirical evaluation across two domains demonstrates that our collaborative approach effectively mitigates NSEs, outperforming non-collaborative methods.},
  archive   = {C_AAMAS},
  author    = {Choudhury, Moumita and Saisubramanian, Sandhya and Zhang, Hao and Zilberstein, Shlomo},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2213–2215},
  title     = {Minimizing negative side effects in cooperative multi-agent systems using distributed coordination},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663111},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal task assignment and path planning using
conflict-based search with precedence and temporal constraints.
<em>AAMAS</em>, 2210–2212. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper examines the Task Assignment and Path Finding with Precedence and Temporal Constraints (TAPF-PTC) problem. We augment Conflict-Based Search (CBS) to generate task assignments and collision-free paths that adhere to precedence and temporal constraints for agents to maximize a user-defined objective.},
  archive   = {C_AAMAS},
  author    = {Chong, Yu Quan and Li, Jiaoyang and Sycara, Katia},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2210–2212},
  title     = {Optimal task assignment and path planning using conflict-based search with precedence and temporal constraints},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663110},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modelling the dynamics of subjective identity in allocation
games. <em>AAMAS</em>, 2207–2209. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Allocation games are zero-sum games that model the distribution of resources among multiple agents. In this paper, we explore the interplay between an elastic sense of subjective identity and its impact on notions of fairness in allocation. An elastic sense of identity in agents is known to lead to responsible decision-making in non-cooperative, non-zero-sum games like Prisoners&#39; Dilemma. It thus is a desirable way to model autonomous agents. However, when it comes to allocation, an elastic sense of identity is shown to exacerbate inequities in allocation, giving no rational incentive for agents to act fairly towards one another. This leads us to argue that fairness needs to be an innate characteristic of autonomous agency. To illustrate this, we implement the well-known Ultimatum Game between two agents, where their elastic sense of self (represented by γ) and a sense of fairness (represented by τ) are both varied. We study the points at which agents find it no longer rational to identify with the other agent, and uphold their sense of fairness, and vice versa. Such a study also helps us discern the subtle difference between responsibility and fairness in the context of allocation games.},
  archive   = {C_AAMAS},
  author    = {Chhabra, Janvi and Deshmukh, Jayati and Srinivasa, Srinath},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2207–2209},
  title     = {Modelling the dynamics of subjective identity in allocation games},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663109},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cognizing and imitating robotic skills via a dual
cognition-action architecture. <em>AAMAS</em>, 2204–2206. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Enabling robots to effectively learn and imitate expert skills in long-horizon tasks remains challenging. Hierarchical imitation learning (HIL) approaches have made strides but often fall short in complex scenarios due to their reliance on self-exploration. This paper introduces a novel approach inspired by the human skill acquisition process, proposing a Cognition-Action-based Robotic Skill Imitation Learning (CasIL) framework. CasIL integrates human cognitive priors for task decomposition into a dual-layer architecture, enhancing robots&#39; ability to cognize and imitate essential skills from expert demonstrations. Our experiments across four RLbench tasks demonstrate CasIL&#39;s superior performance, robustness, and generalizability in skill imitation compared to related methods.},
  archive   = {C_AAMAS},
  author    = {Chen, Zixuan and Ji, Ze and Liu, Shuyang and Huo, Jing and Chen, Yiyu and Gao, Yang},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2204–2206},
  title     = {Cognizing and imitating robotic skills via a dual cognition-action architecture},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663108},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantifying agent interaction in multi-agent reinforcement
learning for cost-efficient generalization. <em>AAMAS</em>, 2201–2203.
(<a href="https://dl.acm.org/doi/10.5555/3635637.3663107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generalization in Multi-agent Reinforcement Learning (MARL) is challenging. Introducing a diverse set of co-play agents typically boosts the agent&#39;s generalization to unseen co-players. However, the extent to which an agent is influenced by co-players varies across scenarios and environments; thus, the improvement in generalization introduced by diversifying co-players also varies. In this work, we introduces Level of Influence (LoI), a novel metric measuring the interaction intensity among agents within a given scenario and environment. We show that LoI can effectively predict the disparities in the benefits of diversifying co-player distribution across scenarios, offering insights into optimizing training cost for varied situations. The code is available at: https://github.com/ThomasChen98/Level-of-Influence.},
  archive   = {C_AAMAS},
  author    = {Chen, Yuxin and Tang, Chen and Tian, Ran and Li, Chenran and Li, Jinning and Tomizuka, Masayoshi and Zhan, Wei},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2201–2203},
  title     = {Quantifying agent interaction in multi-agent reinforcement learning for cost-efficient generalization},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663107},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mastering robot control through point-based reinforcement
learning with pre-training. <em>AAMAS</em>, 2198–2200. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual-based Reinforcement Learning (RL) has gained prominence in robotics decision-making due to its significant potential. However, the prevalent utilization of images in visual-based RL lacks explicit descriptions of object structures and spatial configurations in scenes, thereby limiting the overall efficiency and robustness of RL in robot control. Additionally, training an RL policy solely using visual observations from scratch is typically sample-inefficient, rendering it impractical for real-world application. To address these challenges, this paper proposes a novel method, called Pre-training on Point-based RL (P2RL), which takes the point cloud representations of scenes as states and preserves the intricate spatial details between objects. To further enhance efficiency, we leverage the pre-training method to bolster the perception ability of the network. Key factors in the pre-training process are systematically examined to optimize downstream RL training. Experimental results demonstrate the superior robustness and efficiency of P2RL compared to the state-of-the-art image-based RL method, especially in evaluations involving untrained scenes.},
  archive   = {C_AAMAS},
  author    = {Chen, Yihong and Wang, Cong and Yang, Tianpei and Wang, Meng and Chen, Yingfeng and Zhou, Jifei and Zhao, Chaoyi and Zhang, Xinfeng and Zhao, Zeng and Fan, Changjie and Hu, Zhipeng and Xiong, Rong and Zeng, Long},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2198–2200},
  title     = {Mastering robot control through point-based reinforcement learning with pre-training},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663106},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ANOTO: Improving automated negotiation via offline-to-online
reinforcement learning. <em>AAMAS</em>, 2195–2197. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automated negotiation is a crucial component for establishing cooperation and collaboration within multi-agent systems.While reinforcement learning (RL)-based negotiating agents have achieved remarkable success in various scenarios, they still face limitations due to certain assumptions on which they are based.In this work, we proposes a novel approach called ANOTO to improve the negotiating agents&#39; ability via offline-to-online RL.ANOTO enables a negotiating agent (1) to communicate with opponents using an end-to-end strategy that covers all negotiation actions, (2) to learn negotiation strategies from historical offline data without requiring active interactions, and (3) to enhance the optimization process during the online phase, facilitating rapid and stable performance improvements for the learned offline strategies.Experimental results, based on a number of negotiation scenarios and recent winning agents from the Automated Negotiating Agents Competitions (ANAC), are provided.},
  archive   = {C_AAMAS},
  author    = {Chen, Siqi and Zhao, Jianing and Zhao, Kai and Weiss, Gerhard and Zhang, Fengyun and Su, Ran and Dong, Yang and Li, Daqian and Lei, Kaiyou},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2195–2197},
  title     = {ANOTO: Improving automated negotiation via offline-to-online reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663105},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cutsets and EF1 fair division of graphs. <em>AAMAS</em>,
2192–2194. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A connected graph G = (V, E) provides a natural context for importing the connectivity requirement of fair division from the continuous world into the discrete one. Each of n agents is allocated a share of G&#39;s vertex set V. These n shares partition V, with each required to induce a connected subgraph. Agents use their own valuation functions to determine the non-negative numerical values of the shares, which then determine whether the allocation is fair in some specified sense. Applications include the problem of dividing cities connected by a road network when each party wishes to drive among its allocated cities without leaving its territory.We introduce graph cutsets - forbidden substructures which block allocations that are fair in the EF1 (envy-free up to one item) sense. Two parameters - gap and valence - determine blocked values of n. If G contains a cutset of gap k ≥ 2 and valence in the interval [n - k + 1, n - 1], then allocations that are CEF1 (connected EF1) fail to exist for n agents with certain CM (common monotone) valuations; an elementary cutset yields such a failure even for CA (common additive) valuations. Additionally, we provide an example (Graph GIII in Figure 1) which excludes both cutsets of gap at least two and CEF1 divisions for three agents even with CA valuations. We show that it is NP-complete to determine whether cutsets exist. Finally, for some graphs G we can, in combination with some new positive results, pin down G&#39;s spectrum - the list of exactly which values of n do/do not guarantee CEF1 allocations. Examples suggest a conjectured common spectral pattern for all graphs.},
  archive   = {C_AAMAS},
  author    = {Chen, Jiehua and Zwicker, William S.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2192–2194},
  title     = {Cutsets and EF1 fair division of graphs},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663104},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HLG: Bridging human heuristic knowledge and deep
reinforcement learning for optimal agent performance. <em>AAMAS</em>,
2189–2191. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Training an optimal policy in deep reinforcement learning (DRL) remains a significant challenge due to the pitfalls of inefficient sampling in dynamic environments with sparse rewards. In this paper, we proposed a Human Local Guide (HLG) incorporating high-level human knowledge and local policies to guide DRL agents to achieve optimal performance. HLG deployed the heuristic rules from human knowledge in differential decision trees and then injected them into neural networks, which can continuously improve the suboptimal global policy till the optimal level. Our developed HLG includes action guides based on a policy-switching mechanism and adaptive action guides inspired by an approximate policy evaluation scheme through a perturbation model to optimise policy further. Our proposed HLG outperforms PPO and PROLONET with at least 25\% improvement in training efficiency and exploration capability based on MinGrid environments with sparse reward signals. This implies that HLG has a significant potential to continuously assist the DRL agent in achieving optimal policy in dynamic and complex environments.},
  archive   = {C_AAMAS},
  author    = {Chen, Bin and Cao, Zehong},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2189–2191},
  title     = {HLG: Bridging human heuristic knowledge and deep reinforcement learning for optimal agent performance},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663103},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Agent-based triangle counting and its applications in
anonymous graphs. <em>AAMAS</em>, 2186–2188. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Triangle counting in graphs is a fundamental problem with a diverse application domain. In this paper, we propose a solution to the triangle counting problem in an anonymous graph using autonomous mobile agents. We further use the triangle count to address the Truss Decomposition problem which involves finding maximal sub-graphs with strong interconnections. Truss decomposition helps in identifying maximal, highly interconnected sub-graphs, or trusses, within a network. Additionally, the triangle count is also used to compute two important metrics - Triangle Centrality and Local Clustering Coefficient for the nodes of the graph. Our goal is to devise algorithms that effectively solve these problems minimizing both the overall time complexity and the memory usage at each agent.},
  archive   = {C_AAMAS},
  author    = {Chand, Prabhat Kumar and Das, Apurba and Molla, Anisur Rahaman},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2186–2188},
  title     = {Agent-based triangle counting and its applications in anonymous graphs},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663102},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Anytime multi-agent path finding using operation parallelism
in large neighborhood search. <em>AAMAS</em>, 2183–2185. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-Agent Path Finding (MAPF) is the problem of finding a set of collision-free paths for multiple agents in a shared environment while improving the solution quality. The state-of-the-art anytime MAPF algorithm is based on Large Neighborhood Search (MAPF-LNS), which is a combinatorial search algorithm that iteratively destroys and repairs a subset of collision-free paths. In this paper, we propose Destroy-Repair Operation Parallelism for MAPF-LNS (DROP-LNS), a parallel framework that performs multiple destroy and repair operations concurrently to explore more regions of the search space and improve the solution quality. Unlike MAPF-LNS, DROP-LNS is able to exploit multiple threads during the search. The results show that DROP-LNS outperforms the state-of-the-art anytime MAPF algorithms, namely MAPF-LNS and LaCAM*, with respect to solution quality when terminated at the same runtime.},
  archive   = {C_AAMAS},
  author    = {Chan, Shao-Hung and Chen, Zhe and Lin, Dian-Lun and Zhang, Yue and Harabor, Daniel and Koenig, Sven and Huang, Tsung-Wei and Phan, Thomy},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2183–2185},
  title     = {Anytime multi-agent path finding using operation parallelism in large neighborhood search},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663101},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mechanism design for reducing agent distances to prelocated
facilities. <em>AAMAS</em>, 2180–2182. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider a variant of facility location problems where the facility is prelocated at a specific position to serve the agents who are located on a real line. Because the facility cannot be relocated due to various constraints (e.g., construction costs and requirements), the social planner considers the structural modification problem of adding short-cut edges to the real line (e.g., shuttles between pairs of locations) for improving the accessibility or reducing costs of the agents to the facility, where the cost of an agent is measured by their shortest distance to the facility possibly using the short-cut edges. We focus on the mechanism design aspects of the problems where the agents&#39; locations are private. We propose several strategy-proof mechanisms that elicit true agent locations and minimize the total or maximum cost of agents. We provide approximation ratios for these mechanisms and lower bounds on the approximation ratios for total or maximum cost.},
  archive   = {C_AAMAS},
  author    = {Chan, Hau and Fu, Xinliang and Li, Minming and Wang, Chenhao},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2180–2182},
  title     = {Mechanism design for reducing agent distances to prelocated facilities},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663100},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Non stationary bandits with periodic variation.
<em>AAMAS</em>, 2177–2179. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In numerous real-world scenarios, we encounter periodic patterns in the dynamics of non-stationary data. Unfortunately, current approaches to addressing non-stationary bandit problems overlook the valuable potential offered by the presence of periodicity. In response, we introduce SW-PUCB, a novel sliding window algorithm explicitly designed to exploit periodicity in bandit arms, surpassing the performance of the conventional UCB approach when dealing with perfectly periodic bandit environments. Recognizing that perfect periodicity is seldom encountered in real-world setting, we further present SW-NPUCB, another sliding window algorithm tailored to data exhibiting near-periodic characteristics. Lastly, we demonstrate the practical efficacy of our algorithms through comprehensive experimentation, on synthetically generated data. By bench-marking against existing non-stationary bandit techniques, we emphasize the superiority of our approaches.},
  archive   = {C_AAMAS},
  author    = {Chakraborty, Titas and Shettiwar, Parth},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2177–2179},
  title     = {Non stationary bandits with periodic variation},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663099},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). User-centric explanation strategies for interactive
recommenders. <em>AAMAS</em>, 2174–2176. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the pervasive usage of recommendation systems across various domains, there is a growing need for transparent and convincing interactions to build a rapport with the system users. Incorporating explainability into recommendation systems has become a promising strategy to bolster user trust and sociability. This study centers on recommendation systems that leverage varying explainability techniques to cultivate trust by delivering comprehensible customized explanations for the given recommendations. Accordingly, we propose two explanation methods aligning with a cluster-based recommendation strategy.},
  archive   = {C_AAMAS},
  author    = {Buzcu, Berk and Kuru, Emre and Aydogan, Reyhan},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2174–2176},
  title     = {User-centric explanation strategies for interactive recommenders},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663098},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Who gets the maximal extractable value? A dynamic sharing
blockchain mechanism. <em>AAMAS</em>, 2171–2173. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Maximal Extractable Value (MEV) has emerged as a new frontier in the design of blockchain systems. MEV refers to any excess value that a block producer can realize by manipulating the ordering of transactions. In this paper, we propose to make the MEV extraction rate part of the protocol design space. Our aim is to leverage this parameter to maintain a healthy balance between block producers (who need to be compensated for the service they provide) and users (who need to feel encouraged to transact). We design a dynamic mechanism which updates the MEV extraction rate with the goal of stabilizing it at a target value. We analyse the evolution of this dynamic mechanism under various market conditions and provide formal guarantees about its long-term performance. The main takeaway from our work is that the proposed system exhibits desirable behavior (near-optimal performance) even when it operates in out of equilibrium conditions that are often met in practice. Our work establishes, the first to our knowledge, dynamic framework for the integral problem of MEV sharing between extractors and users.},
  archive   = {C_AAMAS},
  author    = {Braga, Pedro and Chionas, Georgios and Krysta, Piotr and Leonardos, Stefanos and Piliouras, Georgios and Ventre, Carmine},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2171–2173},
  title     = {Who gets the maximal extractable value? a dynamic sharing blockchain mechanism},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663097},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decentralised emergence of robust and adaptive linguistic
conventions in populations of autonomous agents grounded in continuous
worlds. <em>AAMAS</em>, 2168–2170. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a methodology for establishing linguistic conventions in populations of autonomous agents in a fully decentralised manner. As agents take part in local communicative interactions, they gradually establish a common conceptual system and vocabulary that enables them to communicate about arbitrary entities in their continuous environment. Apart from introducing the methodology, we also present six experiments that showcase the robustness of the methodology against sensor defects, its ability to handle noisy observations and uncalibrated sensors, and its suitability for continual learning.},
  archive   = {C_AAMAS},
  author    = {Botoko Ekila, J\&#39;{e}r\^{o}me and Nevens, Jens and Verheyen, Lara and Beuls, Katrien and Van Eecke, Paul},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2168–2170},
  title     = {Decentralised emergence of robust and adaptive linguistic conventions in populations of autonomous agents grounded in continuous worlds},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663096},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Factored MDP based moving target defense with dynamic threat
modeling. <em>AAMAS</em>, 2165–2167. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Moving Target Defense (MTD) has emerged as a proactive defense framework to counteract ever-changing cyber threats. Existing approaches often make assumptions about attacker-side knowledge and behavior, potentially resulting in suboptimal defense. This paper introduces a novel MTD approach, leveraging a Markov Decision Process (MDP) model that eliminates the need for prior knowledge about attacker intentions or payoffs. Our framework seamlessly integrates real-time attacker responses into the defender&#39;s MDP using a dynamic Bayesian network. We use a factored MDP model to enable a more comprehensive and realistic representation of the system having multiple switchable aspects and also accommodate incremental updates of an attack response predictor as new attack data emerges, ensuring adaptive defense. Empirical evaluations demonstrate the approach&#39;s effectiveness in uncertain scenarios with evolving as well as unknown attack landscapes.},
  archive   = {C_AAMAS},
  author    = {Bose, Megha and Paruchuri, Praveen and Kumar, Akshat},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2165–2167},
  title     = {Factored MDP based moving target defense with dynamic threat modeling},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663095},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fair allocation of conflicting courses under additive
utilities. <em>AAMAS</em>, 2162–2164. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate the problem of fair allocation of indivisible items when certain item pairs conflict, where conflicts are represented by an interval graph. In this setting, no two conflicting items may be allocated to the same agent. Our problem has practical applications specifically for course allocation where students are agents and course seats are items; courses may have conflicting schedules. We devise algorithms for finding fair, specifically envy-freeness up to one item (EF1), allocations of courses to students in the most general setting: when students have non-uniform, non-identical, additive utility functions. In this extended abstract, we provide one of the algorithms that finds a EF1 solution under identical utilities, implying that, for any course, all students have the same utility.},
  archive   = {C_AAMAS},
  author    = {Biswas, Arpita and Ke, Yiduo and Khuller, Samir and Liu, Quanquan C.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2162–2164},
  title     = {Fair allocation of conflicting courses under additive utilities},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663094},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gaze supervision for mitigating causal confusion in driving
agents. <em>AAMAS</em>, 2159–2161. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Imitation Learning (IL) algorithms show promise in learning human-level driving behavior, but they often suffer from &quot;causal confusion,&quot; a phenomenon where the lack of explicit inference of the underlying causal structure can result in misattribution of the relative importance of scene elements, especially pronounced in complex scenarios like urban driving with abundant information per time step. Our key idea is that while driving, human drivers naturally exhibit an easily obtained, continuous signal that is highly correlated with causal elements of the state space: eye gaze. We collect human driver demonstrations in a CARLA-based VR driving simulator, allowing us to capture eye gaze in the same simulation environment commonly used in prior work. Further, we propose a method to use gaze-based supervision to mitigate causal confusion in driving IL agents -- exploiting the relative importance of gazed-at and not-gazed-at scene elements for driving decision-making. We present quantitative results demonstrating the promise of gaze-based supervision improving the driving performance of IL agents.},
  archive   = {C_AAMAS},
  author    = {Biswas, Abhijat and Pardhi, Badal Arun and Chuck, Caleb and Holtz, Jarrett and Niekum, Scott and Admoni, Henny and Allievi, Alessandro},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2159–2161},
  title     = {Gaze supervision for mitigating causal confusion in driving agents},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663093},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decentralized control of distributed manipulators: An
information diffusion approach. <em>AAMAS</em>, 2156–2158. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a self-organizing decentralized controller employing an information diffusion mechanism to govern the behavior of surface-based distributed manipulators. These systems utilize independently controlled actuators arranged in a grid for precise positioning and orientation of objects. The proposed approach is demonstrated in a simulated virtual environment with a generic distributed manipulator. The system&#39;s self-organization capabilities are evaluated through experiments involving objects of varying sizes and shapes. The results show the robustness, fault tolerance, and performance of the system. The approach&#39;s high level of abstraction makes it versatile for different actuation principles and sensing devices, focusing on the essential information and module capabilities needed for the task.},
  archive   = {C_AAMAS},
  author    = {Bessone, Nicolas and Zahadat, Payam and Stoy, Kasper},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2156–2158},
  title     = {Decentralized control of distributed manipulators: An information diffusion approach},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663092},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Computing balanced solutions for large international kidney
exchange schemes when cycle length is unbounded. <em>AAMAS</em>,
2153–2155. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In kidney exchange programmes (KEP) patients may swap their incompatible donors leading to cycles of kidney transplants. Countries try to merge their national patient-donor pools leading to international KEPs (IKEPs). Long-term stability of an IKEP can be achieved through a credit-based system. The goal is to find, in each round, an optimal solution that closely approximates this target allocation. We provide both theoretical and experimental results for the case where the cycle length is unbounded.},
  archive   = {C_AAMAS},
  author    = {Benedek, M\&#39;{a}rton and Bir\&#39;{o}, P\&#39;{e}ter and Cs\&#39;{a}ji, Gergely and Johnson, Matthew and Paulusma, Dani\&quot;{e}l and Ye, Xin},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2153–2155},
  title     = {Computing balanced solutions for large international kidney exchange schemes when cycle length is unbounded},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663091},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive discounting of training time attacks.
<em>AAMAS</em>, 2150–2152. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Among the most insidious attacks on Reinforcement Learning (RL) solutions are training-time attacks (TTAs) that create loopholes and backdoors in the learned behaviour. Not limited to a simple disruption, constructive TTAs (C-TTAs) are now available, where the attacker forces a specific, target behaviour upon a training RL agent (victim). However, even state-of-the-art C-TTAs focus on target behaviours that could be naturally adopted by the victim if not for a particular feature of the environment dynamics, which C-TTAs exploit. In this work, we show that a C-TTA is possible even when the target behaviour is un-adoptable by the victim (in the default/un-attacked environment) due to both environment dynamics as well as due to the behaviour&#39;s non-optimality w.r.t. the victim&#39;s objective(s). To find efficient attacks in this context, we develop a specialised flavour of the DDPG algorithm, which we term γDDPG, that learns this stronger version of C-TTA. γDDPG dynamically alters the attack policy planning horizon based on the victim&#39;s current behaviour. This improves effort distribution throughout the attack timeline and reduces the effect of uncertainty that the attacker has about the victim. To demonstrate the features of our method and better relate the results to prior research, we borrow a 3D grid domain from a state-of-the-art C-TTA for our experiments. The full paper is available at &quot;bit.ly/AdaptiveDiscountingofTTA&quot;.},
  archive   = {C_AAMAS},
  author    = {Bector, Ridhima and Aradhya, Abhay and Quek, Chai and Rabinovich, Zinovi},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2150–2152},
  title     = {Adaptive discounting of training time attacks},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663090},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Concurrency model of BDI programming frameworks: Why should
we control it? <em>AAMAS</em>, 2147–2149. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We provide a taxonomy of concurrency models for BDI frameworks, elicited by analysing state-of-the-art technologies, and aimed at helping both BDI designers and developers in making informed decisions. Comparison among BDI technologies w.r.t. concurrency models reveals heterogeneous support, and low customisability.},
  archive   = {C_AAMAS},
  author    = {Baiardi, Martina and Burattini, Samuele and Ciatto, Giovanni and Pianini, Danilo and Omicini, Andrea and Ricci, Alessandro},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2147–2149},
  title     = {Concurrency model of BDI programming frameworks: Why should we control it?},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663089},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Metric distortion under public-spirited voting.
<em>AAMAS</em>, 2144–2146. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate the impact of public-spirited voting on the distortion in the metric framework. We employ the public-spirited model proposed by Flanigan et. al. [3] (EC&#39;23) to model the public-spirited behavior of the agents and evaluate the distortion of different voting rules, including Plurality, Borda, Copeland, Veto, k-approval, and Plurality Veto. We establish a lower bound for any voting rule operating within the metric framework with public-spirited voters. Additionally, we present lower and upper bounds on the distortion associated with these voting rules within the public-spirited model. Among these voting rules, we show that, in the case of public-spirited voting where all voters exhibit identical behavior, the distortion of PluralityVeto matches the general lower bound.},
  archive   = {C_AAMAS},
  author    = {Bagheridelouee, Amirreza and Nilipour, Marzie and Seddighin, Masoud and Shamsipour, Maziar},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2144–2146},
  title     = {Metric distortion under public-spirited voting},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663088},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Entropy seeking constrained multiagent reinforcement
learning. <em>AAMAS</em>, 2141–2143. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multiagent Reinforcement Learning (MARL) has been successfully applied to domains requiring close coordination among many agents. However, real-world tasks require safety specifications that are not generally considered by MARL algorithms. In this work, we introduce an Entropy Seeking Constrained (ESC) approach aiming to learn safe cooperative policies for multiagent systems. Unlike previous methods, ESC considers safety specifications while maximizing state-visitation entropy, addressing the exploration issues of constrained-based solutions.},
  archive   = {C_AAMAS},
  author    = {Aydeniz, Ayhan Alp and Marchesini, Enrico and Amato, Christopher and Tumer, Kagan},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2141–2143},
  title     = {Entropy seeking constrained multiagent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663087},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MiKe: Task scheduling for UAV-based parcel delivery.
<em>AAMAS</em>, 2138–2140. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unmanned aerial vehicle (UAV) networks represent an ecological alternative to truck-based delivery systems, especially in urban areas prone to traffic congestion. In this paper, we formalize MiKe as the problem of assigning deliveries to a fleet of drones to minimize the time makespan. We solve this problem with a polynomial-time algorithm, P&amp;amp;D, which outperforms other state-of-the-art solutions.},
  archive   = {C_AAMAS},
  author    = {Arrigoni, Viviana and Attenni, Giulio and Bartolini, Novella and Finelli, Matteo and Maselli, Gaia},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2138–2140},
  title     = {MiKe: Task scheduling for UAV-based parcel delivery},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663086},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Liquid democracy for low-cost ensemble pruning.
<em>AAMAS</em>, 2135–2137. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We show that there is a strong connection between ensemble learning and a delegative voting paradigm, liquid democracy, which can be leveraged to reduce ensemble training costs. We present an incremental training procedure that removes redundant classifiers from an ensemble via delegation. By carefully selecting the underlying delegation mechanism weight-centralization among classifiers is avoided, leading to higher accuracy than some boosting methods with a significantly lower cost than training a full ensemble. This work serves as an exemplar of how ideas from computational social choice can be applied to problems in nontraditional domains.},
  archive   = {C_AAMAS},
  author    = {Armstrong, Ben and Larson, Kate},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2135–2137},
  title     = {Liquid democracy for low-cost ensemble pruning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663085},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Abstracting assumptions in structured argumentation.
<em>AAMAS</em>, 2132–2134. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work we apply a form of existential abstraction on the prominent structured approach of assumption-based argumentation (ABA) via clustering assumptions, leading to simplified argumentation scenarios supporting explainability. We present ways of interpreting clustered ABA frameworks, look at use cases, and provide an interactive automated tool that obtains faithful clusterings.},
  archive   = {C_AAMAS},
  author    = {Apostolakis, Iosif and Saribatur, Zeynep G. and Wallner, Johannes P.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2132–2134},
  title     = {Abstracting assumptions in structured argumentation},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663084},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bounding consideration probabilities in consider-then-choose
ranking models. <em>AAMAS</em>, 2129–2131. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A common theory of choice posits that individuals make choices in two steps, first selecting a subset of the alternatives to consider before making a choice from the resulting consideration set. However, inferring unobserved consideration sets (or item consideration probabilities) in this &quot;consider then choose&#39;&#39; setting poses significant challenges: even simple models of consideration with strong independence assumptions are not identifiable, even if item utilities are known. We consider a natural extension of consider-then-choose models to a top-k ranking setting, where we assume rankings are constructed according to a Plackett-Luce model after sampling a consideration set. While item consideration probabilities remain non-identified in this setting, we prove that knowledge of item utilities allows us to infer bounds on the relative sizes of consideration probabilities. Additionally, given a bound on the expected consideration set size, we derive absolute upper and lower bounds on item consideration probabilities. We also provide an algorithm to tighten those bounds on consideration probabilities by propagating inferred constraints. Thus, we show that we can learn useful information about consideration probabilities despite their non-identifiability. We demonstrate our methods on a dataset from a psychology experiment with two different ranking tasks (one with fixed consideration sets and one with unknown consideration sets). This combination of data allows us to estimate utilities and then learn about unknown consideration probabilities using our bounds.},
  archive   = {C_AAMAS},
  author    = {Aoki-Sherwood, Ben and Bregou, Catherine and Liben-Nowell, David and Tomlinson, Kiran and Zeng, Thomas},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2129–2131},
  title     = {Bounding consideration probabilities in consider-then-choose ranking models},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663083},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Charging electric vehicles fairly and efficiently.
<em>AAMAS</em>, 2126–2128. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motivated by electric vehicle (EV) charging, we formulate the problem of and efficient allocation of a divisible resource among agents that arrive and depart over time and consume the resource at different rates. The agents (EVs) derive utility from the amount of charge gained, which depends on their own charging rate as well as that of the charging outlet. The goal is to allocate charging time at different outlets among the EVs such that the final allocation is envy-free, pareto optimal, and in certain contexts, group-strategyproof. The differences in the charging rates of the outlets and the EVs, and a continuous time-window where the arrivals and departures occur make this a non-trivial combinatorial optimization problem. We show possibilities and impossibilities of achieving a combination of properties such as envy-freeness, pareto optimality, leximin, and group-strategyproofness under different operational settings, e.g., when the EVs have (dis)similar charging technology, or when there are one or more dissimilar charging outlets. We complement the positive existence results with polynomial-time algorithms.},
  archive   = {C_AAMAS},
  author    = {Anandanarayanan, Ramsundar and Nath, Swaprava and Vaish, Rohit},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2126–2128},
  title     = {Charging electric vehicles fairly and efficiently},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663082},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantum circuit design: A reinforcement learning challenge.
<em>AAMAS</em>, 2123–2125. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To assess the prospects of using reinforcement learning (RL) for selecting and parameterizing quantum gates to build viable circuit architectures, we introduce the quantum circuit designer (QCD). By considering quantum control a decision-making problem, we strive to profit from advanced RL exploration mechanisms to overcome the need for granular specification and hand-crafted architectures. To evaluate current state-of-the-art RL algorithms, we define generic objectives that arise from quantum architecture search and circuit optimization. Those evaluation results reveal challenges inherent to learning optimal quantum control.},
  archive   = {C_AAMAS},
  author    = {Altmann, Philipp and B\&quot;{a}rligea, Adelina and Stein, Jonas and K\&quot;{o}lle, Michael and Gabor, Thomas and Phan, Thomy and Linnhof-Popien, Claudia},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2123–2125},
  title     = {Quantum circuit design: A reinforcement learning challenge},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663081},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Approximately fair allocation of indivisible items with
random valuations. <em>AAMAS</em>, 2120–2122. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we consider the problem of fairly allocating a set of indivisible items to agents, who have additive and random valuations for the bundles of items they receive. The valuations that each agent has for all items are independent and bounded, and their realizations are only revealed after allocating the items. The goal is to determine an allocation that minimizes, in expectation, the maximum envy that an agent has for the bundle assigned to each other, without knowing in advance the realization of the random valuations.We first show how to compute in polynomial time and deterministically an allocation that guarantees an expected maximum envy of at most O(w√ln(n)m/n), where n is the number of agents, m is the number of items and w is the maximum valuation for each item. Furthermore, we show that the above bound cannot be improved, that is, there is an instance for which the expected maximum envy of any allocation is at least Ω(w√ln(n)m/n). Finally, we resort to randomized algorithms that return (random) allocations satisfying further efficiency guarantees, such as ex-ante envy-freeness and ex-ante Pareto optimality. If we relax the constraint of ex-ante Pareto optimality, we provide an algorithm that still works without knowing the probability distributions of agent valuations.},
  archive   = {C_AAMAS},
  author    = {Aloisio, Alessandro and Bil\`{o}, Vittorio and Caruso, Antonio Mario and Flammini, Michele and Vinci, Cosimo},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2120–2122},
  title     = {Approximately fair allocation of indivisible items with random valuations},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663080},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On general epistemic abstract argumentation frameworks.
<em>AAMAS</em>, 2117–2119. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Epistemic Abstract Argumentation Framework (EAAF) extends Dung&#39;s framework (AAF)-a central formalism in AI for modeling disputes among agents-by allowing the representation of epistemic knowledge. EAAF augments AAF with strong and weak epistemic attacks whose intuitive meaning is that an attacked argument is epistemically accepted only if the attacking argument is possibly or certainly rejected, respectively. So far, the semantics of EAAF has been defined only for a restricted class of frameworks, namely acyclic EAAF, where epistemic attacks do not occur in any cycle. In this paper, we provide an intuitive semantics for (general) EAAF that naturally extends that for AAF as well as that for acyclic EAAF.},
  archive   = {C_AAMAS},
  author    = {Alfano, Gianvincenzo and Greco, Sergio and Parisi, Francesco and Trubitsyna, Irina},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2117–2119},
  title     = {On general epistemic abstract argumentation frameworks},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663079},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Actual trust in multiagent systems. <em>AAMAS</em>,
2114–2116. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study how trust can be established in multiagent systems where human and AI agents collaborate. We propose a computational notion of actual trust, emphasising the modelling of trust based on agents&#39; capacity to deliver tasks in prospect. Unlike reputation-based trust, we consider the specific setting in which agents interact and model a forward-looking notion of trust. We provide a conceptual analysis of actual trust&#39;s characteristics and highlight relevant trust verification tools. By advancing the understanding and verification of trust in collaborative systems, we contribute to responsible and trustworthy human-AI interactions, enhancing reliability in various domains.},
  archive   = {C_AAMAS},
  author    = {Akintunde, Michael and Yazdanpanah, Vahid and Salehi Fathabadi, Asieh and Cirstea, Corina and Dastani, Mehdi and Moreau, Luc},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2114–2116},
  title     = {Actual trust in multiagent systems},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663078},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Defining deception in decision making. <em>AAMAS</em>,
2111–2113. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the growing capabilities of machine learning systems, particularly those that communicate or interact with humans, there is an increased risk of systems that can easily deceive and manipulate people. Preventing unintended deception and manipulation therefore represents an important challenge for creating aligned AI systems. To approach this challenge in a principled way, we first need to define deception formally. In this work, we present a concrete definition of deception under the formalism of rational decision making in partially observed Markov decision processes. We propose a general regret theory of deception under which the degree of deception can be quantified in terms of the actor&#39;s beliefs, actions, and utility. We instantiate these principles as reward terms for communication agents, and study the degree to which the behavior aligns with human judgments about deception. We hope our work will represent a step toward systems that aim to avoid deception, and detection mechanisms to identify deceptive agents.},
  archive   = {C_AAMAS},
  author    = {Abdulhai, Marwa and Carroll, Micah and Svegliato, Justin and Dragan, Anca and Levine, Sergey},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2111–2113},
  title     = {Defining deception in decision making},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663077},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Maximising the influence of temporary participants in
opinion formation. <em>AAMAS</em>, 2104–2110. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {DeGroot-style opinion formation presumes a continuous interaction among agents of a social network. Hence, it cannot handle agents external to the social network that interact only temporarily with the permanent ones. Many real-world organisations and individuals fall into such a category. For instance, a company tries to persuade as many as possible to buy its products and, due to various constraints, can only exert its influence for a limited amount of time. We propose a variant of the DeGroot model that allows an external agent to interact with the permanent ones for a preset period of time. We obtain several insights on maximising an external agent&#39;s influence in opinion formation by analysing and simulating the variant.},
  archive   = {C_AAMAS},
  author    = {Zhuang, Zhiqiang and Wang, Kewen and Wang, Zhe and Wang, Junhu and Yang, Yinong},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2104–2110},
  title     = {Maximising the influence of temporary participants in opinion formation},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663075},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pragmatic instruction following and goal assistance via
cooperative language-guided inverse planning. <em>AAMAS</em>, 2094–2103.
(<a href="https://dl.acm.org/doi/10.5555/3635637.3663074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {People often give instructions whose meaning is ambiguous without further context, expecting that their actions or goals will disambiguate their intentions. How can we build assistive agents that follow such instructions in a flexible, context-sensitive manner? This paper introduces cooperative language-guided inverse plan search (CLIPS), a Bayesian agent architecture for pragmatic instruction following and goal assistance. Our agent assists a human by modeling them as a cooperative planner who communicates joint plans to the assistant, then performs multimodal Bayesian inference over the human&#39;s goal from actions and language, using large language models (LLMs) to evaluate the likelihood of an instruction given a hypothesized plan. Given this posterior, our assistant acts to minimize expected goal achievement cost, enabling it to pragmatically follow ambiguous instructions and provide effective assistance even when uncertain about the goal. We evaluate these capabilities in two cooperative planning domains (Doors, Keys \&amp;amp; Gems and VirtualHome), finding that CLIPS significantly outperforms GPT-4V, LLM-based literal instruction following and unimodal inverse planning in both accuracy and helpfulness, while closely matching the inferences and assistive judgments provided by human raters.},
  archive   = {C_AAMAS},
  author    = {Zhi-Xuan, Tan and Ying, Lance and Mansinghka, Vikash and Tenenbaum, Joshua B.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2094–2103},
  title     = {Pragmatic instruction following and goal assistance via cooperative language-guided inverse planning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663074},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MESA: Cooperative meta-exploration in multi-agent learning
through exploiting state-action space structure. <em>AAMAS</em>,
2085–2093. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent reinforcement learning (MARL) algorithms often struggle to find strategies close to Pareto optimal Nash Equilibrium, owing largely to the lack of efficient exploration. The problem is exacerbated in sparse-reward settings, caused by the larger variance exhibited in policy learning. This paper introduces MESA, a novel meta-exploration method for cooperative multi-agent learning. It learns to explore by first identifying the agents&#39; high-rewarding joint state-action subspace from training tasks and then learning a set of diverse exploration policies to &quot;cover&quot; the subspace. These trained exploration policies can be integrated with any off-policy MARL algorithm for test-time tasks. We first showcase MESA&#39;s advantage in a multi-step matrix game. Furthermore, experiments show that with learned exploration policies, MESA achieves significantly better performance in sparse-reward tasks in several multi-agent particle environments and multi-agent MuJoCo environments, and exhibits the ability to generalize to more challenging tasks at test time.},
  archive   = {C_AAMAS},
  author    = {Zhang, Zhicheng and Liang, Yancheng and Wu, Yi and Fang, Fei},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2085–2093},
  title     = {MESA: Cooperative meta-exploration in multi-agent learning through exploiting state-action space structure},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663073},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Memory-based resilient control against non-cooperation in
multi-agent flocking. <em>AAMAS</em>, 2075–2084. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inspired by natural flocking behaviors, researchers aim to develop a distributed control approach for artificial agents to mimic these behaviors. The main challenge lies in maintaining the resilience of the artificial flock, as some agents inevitably display non-cooperative behavior, thereby deviating from the flocking objective. Existing control approaches, especially those based on learning algorithm, are susceptible to forgetting issues that non-cooperative agents can exploit to disrupt the flock formation. To address this problem, this study introduces a memory-based resilient control approach that strategically analyzes historical data across three distinct time scales (long, short, and periodic). The implementation of a long short periodic-term memory (LSP) algorithm employs accumulative discounted credibility evaluated by Q-learning to recognize long-term non-cooperation, utilizes a filtering rule to establish a trusted set excluding short-term non-cooperation, and integrates fast Fourier transform to refine the trusted set against periodic inconsistency. We assess the effectiveness of this approach through extensive experiments. The results highlight the potential and advantages of using LSP in flocking, enhancing the resilience of multi-agent flocking against complex non-cooperative threats.},
  archive   = {C_AAMAS},
  author    = {Zhang, Mingyue and Li, Nianyu and Li, Jialong and Liao, Jiachun and Liu, Jiamou},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2075–2084},
  title     = {Memory-based resilient control against non-cooperation in multi-agent flocking},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663072},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human goal recognition as bayesian inference: Investigating
the impact of actions, timing, and goal solvability. <em>AAMAS</em>,
2066–2074. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Goal recognition is a fundamental cognitive process that enables individuals to infer intentions based on available cues. Current goal recognition algorithms often take only observed actions as input, but here we use a Bayesian framework to explore the role of actions, timing, and goal solvability in goal recognition. We analyze human responses to goal-recognition problems in the Sokoban domain, and find that actions are assigned most importance, but that timing and solvability also influence goal recognition in some cases, especially when actions are uninformative. We leverage these findings to develop a goal recognition model that matches human inferences more closely than do existing algorithms. Our work provides new insight into human goal recognition and takes a step towards more human-like AI models.},
  archive   = {C_AAMAS},
  author    = {Zhang, Chenyuan and Kemp, Charles and Lipovetzky, Nir},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2066–2074},
  title     = {Human goal recognition as bayesian inference: Investigating the impact of actions, timing, and goal solvability},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663071},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Majority-based preference diffusion on social networks.
<em>AAMAS</em>, 2057–2065. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study a majority based preference diffusion model in which the members of a social network update their preferences based on those of their connections. Consider an undirected graph where each node has a strict linear order over a set of α alternatives. At each round, a node randomly selects two adjacent alternatives and updates their relative order with the majority view of its neighbors.We bound the convergence time of the process in terms of the number of nodes/edges and α. Furthermore, we study the minimum cost to ensure that a desired alternative will &quot;win&#39;&#39; the process, where occupying each position in a preference order of a node has a cost. We prove tight bounds on the minimum cost for general graphs and graphs with strong expansion properties.Furthermore, we investigate a more light-weight process where each node chooses one of its neighbors uniformly at random and copies its order fully with some fixed probability and remains unchanged otherwise. We characterize the convergence properties of this process, namely convergence time and stable states, using Martingale and reversible Markov chain analysis.Finally, we present the outcomes of our experiments conducted on different synthetic random graph models and graph data from online social platforms. These experiments not only support our theoretical findings, but also shed some light on some other fundamental problems, such as designing powerful countermeasures.},
  archive   = {C_AAMAS},
  author    = {Zehmakan, Ahad N.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2057–2065},
  title     = {Majority-based preference diffusion on social networks},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663070},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Viral marketing in social networks with competing products.
<em>AAMAS</em>, 2047–2056. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Consider a directed network where each node is either red (using the red product), blue (using the blue product), or uncolored (undecided). Then in each round, an uncolored node chooses red (resp. blue) with some probability proportional to the number of its red (resp. blue) out-neighbors.What is the best strategy to maximize the expected final number of red nodes given the budget to select k red seed nodes? After proving that this problem is computationally hard, we provide a polynomial time approximation algorithm with the best possible approximation guarantee, building on the monotonicity and submodularity of the objective function and exploiting the Monte Carlo method. Furthermore, our experiments on various real-world and synthetic networks demonstrate that our proposed algorithm outperforms other algorithms.Additionally, we investigate the convergence time of the aforementioned process both theoretically and experimentally. In particular, we prove several tight bounds on the convergence time in terms of different graph parameters, such as the number of nodes/edges, maximum out-degree and diameter, by developing novel proof techniques.},
  archive   = {C_AAMAS},
  author    = {Zehmakan, Ahad N. and Zhou, Xiaotian and Zhang, Zhongzhi},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2047–2056},
  title     = {Viral marketing in social networks with competing products},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663069},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). When is mean-field reinforcement learning tractable and
relevant? <em>AAMAS</em>, 2038–2046. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mean-field reinforcement learning has become a popular theoretical framework for efficiently approximating large-scale multi-agent reinforcement learning (MARL) problems exhibiting symmetry. However, questions remain regarding the applicability of mean-field approximations: in particular, their approximation accuracy of real-world systems and conditions under which they become computationally tractable. We establish explicit finite-agent bounds for how well the MFG solution approximates the true N-player game for two popular mean-field solution concepts. Furthermore, for the first time, we establish explicit lower bounds indicating that MFGs are poor or uninformative at approximating N-player games assuming only Lipschitz dynamics and rewards. Finally, we analyze the computational complexity of solving MFGs with only Lipschitz properties and prove that they are in the class of PPAD -complete problems conjectured to be intractable, similar to general sum N player games. Our theoretical results underscore the limitations of MFGs and complement and justify existing work by proving difficulty in the absence of common theoretical assumptions.},
  archive   = {C_AAMAS},
  author    = {Yardim, Batuhan and Goldman, Artur and He, Niao},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2038–2046},
  title     = {When is mean-field reinforcement learning tractable and relevant?},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663068},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Risk-aware constrained reinforcement learning with
non-stationary policies. <em>AAMAS</em>, 2029–2037. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Constrained reinforcement learning (RL) algorithms have attracted extensive attentions nowadays to tackle sequential decision-making problems that contain constraints defined under various risk measures. However, most works only search policies within the stationary policy class and fail to capture a simple intuition: adjust the action-selecting distribution at each state according to the accumulated cost so far. In this work, we design a novel quantile-level-driven policy class to fully realize such intuition, within which each policy additionally takes the quantile level of the accumulated cost as input. Such quantile level is obtained via a novel Invertible Backward Distributional Critic (IBDC) framework, which utilizes invertible function approximators to estimate the accumulated cost distribution and outputs the required quantile level with their inverse forms. Further, the estimated accumulated cost distribution also helps to decompose the challenging trajectory-level constraints into state-level constraints, and Risk-Aware Constrained RL (RAC) algorithm is designed then to solve the decomposed problem with Lagrangian multipliers. Experimental results in various environments validate the effectiveness of RAC versus state-of-the-art baselines.},
  archive   = {C_AAMAS},
  author    = {Yang, Zhaoxing and Jin, Haiming and Tang, Yao and Fan, Guiyun},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2029–2037},
  title     = {Risk-aware constrained reinforcement learning with non-stationary policies},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663067},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Whom to trust? Elective learning for distributed gaussian
process regression. <em>AAMAS</em>, 2020–2028. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces an innovative approach to enhance distributed cooperative learning using Gaussian process (GP) regression in multi-agent systems (MASs). The key contribution of this work is the development of an elective learning algorithm, namely prior-aware elective distributed GP (Pri-GP), which empowers agents with the capability to selectively request predictions from neighboring agents based on their trustworthiness. The proposed Pri-GP effectively improves individual prediction accuracy, especially in cases where the prior knowledge of an agent is incorrect. Moreover, it eliminates the need for computationally intensive variance calculations for determining aggregation weights in distributed GP. Furthermore, we establish a prediction error bound within the Pri-GP framework, ensuring the reliability of predictions, which is regarded as a crucial property in safety-critical MAS applications.},
  archive   = {C_AAMAS},
  author    = {Yang, Zewen and Dai, Xiaobing and Dubey, Akshat and Hirche, Sandra and Hattab, Georges},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2020–2028},
  title     = {Whom to trust? elective learning for distributed gaussian process regression},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663066},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multimodal pretrained models for verifiable sequential
decision-making: Planning, grounding, and perception. <em>AAMAS</em>,
2011–2019. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently developed pretrained models can encode rich world knowledge expressed in multiple modalities, such as text and images. However, the outputs of these models cannot be integrated into algorithms to solve sequential decision-making tasks. We develop an algorithm that utilizes the knowledge from pretrained models to construct and verify controllers for sequential decision-making tasks, and to ground these controllers to task environments through visual observations with formal guarantees. In particular, the algorithm queries a pretrained model with a user-provided, text-based task description and uses the model&#39;s output to construct an automaton-based controller that encodes the model&#39;s task-relevant knowledge. It allows formal verification of whether the knowledge encoded in the controller is consistent with other independently available knowledge, which may include abstract information on the environment or user-provided specifications. Next, the algorithm leverages the vision and language capabilities of pretrained models to link the observations from the task environment to the text-based control logic from the controller (e.g., actions and conditions that trigger the actions). We propose a mechanism to provide probabilistic guarantees on whether the controller satisfies the user-provided specifications under perceptual uncertainties. We demonstrate the algorithm&#39;s ability to construct, verify, and ground automaton-based controllers through a suite of real-world tasks, including daily life and robot manipulation tasks.},
  archive   = {C_AAMAS},
  author    = {Yang, Yunhao and Neary, Cyrus and Topcu, Ufuk},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2011–2019},
  title     = {Multimodal pretrained models for verifiable sequential decision-making: Planning, grounding, and perception},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663065},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatic curriculum for unsupervised reinforcement
learning. <em>AAMAS</em>, 2002–2010. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unsupervised reinforcement learning (URL) relies on carefully designed training objectives rather than task rewards to learn general skills. However, we lack quantitative evaluation metrics for URL but mainly rely on visualizations of trajectories for comparison. Moreover, most URL methods choose to optimize a single training objective, which may hinder later-stage learning and the development of new skills. To bridge these gaps, we first introduce a combination of metrics that can evaluate diverse properties of URL. We show that balancing these metrics in URL leads to better performance and trajectories with empirical evidence and theoretical insights. Next, we develop an automatic curriculum that uses a non-stationary multi-armed bandit algorithm to select URL objectives for different learning episodes, resulting in a balanced improvement on all the metrics. Extensive experiments in different environments demonstrate the advantages of our method in achieving promising and balanced performance on multiple metrics when compared to recent URL methods.},
  archive   = {C_AAMAS},
  author    = {Yang, Yucheng and Zhou, Tianyi and Han, Lei and Fang, Meng and Pechenizkiy, Mykola},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2002–2010},
  title     = {Automatic curriculum for unsupervised reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663064},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attention-based priority learning for limited time
multi-agent path finding. <em>AAMAS</em>, 1993–2001. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Solving large-scale Multi-Agent Path Finding (MAPF) within a limited time remains an open challenge, despite its importance for many robotic applications. Recent learning-based methods scale better than conventional ones, but remain suboptimal and often exhibit low success rates within a limited time on large-scale instances. These limitations often stem from their black-box nature. In this study, we propose a hybrid approach that incorporates prioritized planning with learning-based methods to explicitly address these challenges. We formulate prioritized planning as a Markov Decision Process and introduce a reinforcement learning-based prioritized planning paradigm. In doing so, we develop a novel Synthetic Score-based Attention Network (S2AN) to learn conflict/blocking relationships among agents, and deliver blocking-free priorities. By integrating priority mechanisms and leveraging a new attention-based neural network for enhanced multi-agent cooperative strategies, our method enhances solution completeness while trading off scalability and maintains linear time complexity, thus offering a robust avenue for large-scale MAPF tasks. Comparisons demonstrate its superiority over current learning-based methods in terms of solution quality, completeness, and reachability within limited time constraints, especially in large-scale scenarios. Moreover, an extensive set of numerical results reveals superior completeness compared to restricted-time Priority-Based Search (PBS) and Priority Inheritance with Backtracking (PIBT) in medium to large-scale obstacle-dense scenarios.},
  archive   = {C_AAMAS},
  author    = {Yang, Yibin and Fan, Mingfeng and He, Chengyang and Wang, Jianqiang and Huang, Heye and Sartoretti, Guillaume},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1993–2001},
  title     = {Attention-based priority learning for limited time multi-agent path finding},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663063},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Successively pruned q-learning: Using self q-function to
reduce the overestimation. <em>AAMAS</em>, 1984–1992. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It&#39;s well-known that the Q-learning algorithm suffers the overestimation owing to using the maximum state-action value as an approximation of the maximum expected state-action value. Double Q-learning and other algorithms have been proposed as efficient solutions to alleviate the overestimation. However, these proposed methods intend to utilize multiple Q-functions to reduce the overestimation and ignore the information of single Q-function. In this paper, 1) we reinterpret the update process of Q-learning, build a more precise model compatible with previous model. 2) We propose a novel and simple method to control the maximum bias by employing the information of single Q-function. 3) Our method not only balances between the overestimation and the underestimation, but also attains the minimum bias under proper hyper-parameters. 4) Moreover, it can be naturally generalized to the discrete control domain and continuous control tasks. We reveal that our algorithms outperform Double DQN and other algorithms on some representative games and some classical off-policy actor-critic algorithms can also gain benefits from our method.},
  archive   = {C_AAMAS},
  author    = {Xue, Zhaolin and Zhang, Lihua and Dong, Zhiyan},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1984–1992},
  title     = {Successively pruned Q-learning: Using self Q-function to reduce the overestimation},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663062},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning to schedule online tasks with bandit feedback.
<em>AAMAS</em>, 1975–1983. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Online task scheduling serves an integral role for task-intensive applications in cloud computing and crowdsourcing. Optimal scheduling can enhance system performance, typically measured by the reward-to-cost ratio, under some task arrival distribution. On one hand, both reward and cost are dependent on task context (e.g., evaluation metric) and remain black-box in practice. These render reward and cost hard to model thus unknown before decision making. On the other hand, task arrival behaviors remain sensitive to factors like unpredictable system fluctuation whereby a prior estimation or the conventional assumption of arrival distribution (e.g., Poisson) may fail. This implies another practical yet often neglected challenge, i.e., uncertain task arrival distribution. Towards effective scheduling under a stationary environment with various uncertainties, we propose a double-optimistic learning based Robbins-Monro (DOL-RM) algorithm. Specifically, DOL-RM integrates a learning module that incorporates optimistic estimation for reward-to-cost ratio and a decision module that utilizes the Robbins-Monro method to implicitly learn task arrival distribution while making scheduling decisions. Theoretically, DOL-RM achieves a sub-linear regret of O(T3/4), which is the first result for online task scheduling under uncertain task arrival distribution and unknown reward and cost. Our numerical results in a synthetic experiment and a real-world application demonstrate the effectiveness of DOL-RM in achieving the best cumulative reward-to-cost ratio compared with other state-of-the-art baselines.},
  archive   = {C_AAMAS},
  author    = {Xu, Yongxin and Wang, Shangshang and Guo, Hengquan and Liu, Xin and Shao, Ziyu},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1975–1983},
  title     = {Learning to schedule online tasks with bandit feedback},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663061},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Safeguard privacy for minimal data collection with
trustworthy autonomous agents. <em>AAMAS</em>, 1966–1974. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ensuring digital privacy necessitates users giving well-considered consent to online service providers for data usage, creating an unsustainable and error-prone decision load. Software privacy agents can help make data consent decisions on behalf of users, but a compromised agent could be more detrimental than the absence of such an agent. In response, we employ trustworthy autonomous agents to safeguard users&#39; privacy at the stage of data collection. Drawing upon General Data Protection Regulation (GDPR) principles, notably data minimisation, our autonomous agent guarantees that GDPR-reflected privacy requirements are met through strong proof. We provide a computational encoding of a typical data collection scenario-where data are requested and decisions are made about these requests-as a cognitive agent that makes decisions based on how an agent&#39;s beliefs and goals lead to particular choices. Importantly, our approach provides verifiable assurance about decisions made by these cognitive agents through formal verification, supporting both simultaneous (data requested at the same time) and sequential (data requested one after the other) situations. We provide a templated implementation of these privacy agents and a small example of a mobile app serves to illustrate how a privacy agent can be designed in practice. An in-depth evaluation is given to demonstrate its computational practicality in making privacy decisions in real time and its computational complexity in verifying them. This approach represents a promising step towards trustworthy computational stewardship in data management.},
  archive   = {C_AAMAS},
  author    = {Xu, Mengwei and Dennis, Louise A. and Mustafa, Mustafa A.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1966–1974},
  title     = {Safeguard privacy for minimal data collection with trustworthy autonomous agents},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663060},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Collaborative deep reinforcement learning for solving
multi-objective vehicle routing problems. <em>AAMAS</em>, 1956–1965. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing deep reinforcement learning (DRL) methods for multi-objective vehicle routing problems (MOVRPs) typically decompose an MOVRP into subproblems with respective preferences and then train policies to solve corresponding subproblems. However, such a paradigm is still less effective in tackling the intricate interactions among subproblems, thus holding back the quality of the Pareto solutions. To counteract this limitation, we introduce a collaborative deep reinforcement learning method. We first propose a preference-based attention network (PAN) that allows the DRL agents to reason out solutions to subproblems in parallel, where a shared encoder learns the instance embedding and a decoder is tailored for each agent by preference intervention to construct respective solutions. Then, we design a collaborative active search (CAS) to further improve the solution quality, which updates only a part of the decoder parameters per instance during inference. In the CAS process, we also explicitly foster the interactions of neighboring DRL agents by imitation learning, empowering them to exchange insights of elite solutions to similar subproblems. Extensive results on random and benchmark instances verified the efficacy of PAN and CAS, which is particularly pronounced on the configurations (i.e., problem sizes or node distributions) beyond the training ones. Our code is available at https://github.com/marmotlab/PAN-CAS.},
  archive   = {C_AAMAS},
  author    = {Wu, Yaoxin and Fan, Mingfeng and Cao, Zhiguang and Gao, Ruobin and Hou, Yaqing and Sartoretti, Guillaume},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1956–1965},
  title     = {Collaborative deep reinforcement learning for solving multi-objective vehicle routing problems},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663059},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive evolutionary reinforcement learning algorithm with
early termination strategy. <em>AAMAS</em>, 1947–1955. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Evolutionary reinforcement learning algorithms (ERLs), which combine evolutionary algorithms (EAs) with reinforcement learning (RL), have demonstrated significant success in enhancing RL performance. However, most ERLs rely heavily on Gaussian mutation operators to generate new individuals. When the standard deviation is too large or small, this approach will result in the production of poor or highly similar offspring. Such outcomes can be detrimental to the learning process of the RL agent, as too many poor or similar experiences are generated by these individuals. In order to alleviate these issues, this paper proposes an Adaptive Evolutionary Reinforcement Learning (AERL) method that adaptively adjusts both the standard deviation and the evaluation process. By tracking the performance of new individuals, AERL maintains the mutation strength within a suitable range without the need for additional gradient computations. Moreover, the proposed AERL approach early terminates unnecessary evaluations and discards experiences arising from poor individuals, thereby resulting in enhanced learning efficiency. Empirical assessments conducted on a variety of continuous control problems demonstrate the effectiveness of the AERL method.},
  archive   = {C_AAMAS},
  author    = {Wu, Xiaoqiang and Zhu, Qingling and Lin, Qiuzhen and Chen, Weineng and Li, Jianqiang},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1947–1955},
  title     = {Adaptive evolutionary reinforcement learning algorithm with early termination strategy},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663058},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). New algorithms for distributed fair k-center clustering:
Almost accurate as sequential algorithms. <em>AAMAS</em>, 1938–1946. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fair clustering problems have been paid lots of attention recently. In this paper, we study the k-Center problem under the group fairness and data summarization fairness constraints, denoted as Group Fair k-Center (GFkC) and Data Summarization Fair k-Center (DSFkC), respectively, in the massively parallel computational (MPC) distributed model. The previous best results for the above two problems in the MPC model are a 9-approximation with violation 7 (WWW 2022) and a (17+ε)-approximation without fairness violation (ICML 2020), respectively. In this paper, we obtain a (3+ε)-approximation with violation 1 for the GFkC problem in the MPC model, which is almost as accurate as the best known approximation ratio 3 with violation 1 for the sequential algorithm of the GFkC problem. Moreover, for the DSFkC problem in the MPC model, we obtain a (4+ε)-approximation without fairness violation, which is very close to the best known approximation ratio 3 for the sequential algorithm of the DSFkC problem. Empirical experiments show that our distributed algorithms perform better than existing state-of-the-art distributed methods for the above two problems.},
  archive   = {C_AAMAS},
  author    = {Wu, Xiaoliang and Feng, Qilong and Huang, Ziyun and Xu, Jinhui and Wang, Jianxin},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1938–1946},
  title     = {New algorithms for distributed fair k-center clustering: Almost accurate as sequential algorithms},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663057},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-robot motion and task planning in automotive
production using controller-based safe reinforcement learning.
<em>AAMAS</em>, 1928–1937. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Using synthesis- and AI-planning-based approaches, recent works investigated methods to support engineers with the automation of design, planning, and execution of multi-robot cells. However, real-time constraints and stochastic processes were not well covered due, e.g., to the high abstraction level of the problem modeling, and these methods do not scale well. In this paper, using probabilistic model checking, we construct a controller and integrate it with reinforcement learning approaches to synthesize the most efficient and correct multi-robot task schedules. Statistical Model Checking (SMC) is applied for system requirement verification. Our method is aware of uncertainties and considers robot movement times, interruption times, and stochastic interruptions that can be learned during multi-robot cell operations. We developed a model-at-runtime that integrates the execution of the production cell and optimizes its performance using a controller-based AI system. For this purpose and to derive the best policy, we implemented and compared AI-based methods, namely, Monte Carlo Tree Search, a heuristic AI-planning technique, and Q-learning, a model-free reinforcement learning method. Our results show that our methodology can choose time-efficient task sequences that consequently improve the cycle time and efficiently adapt to stochastic events, e.g., robot interruptions. Moreover, our approach scales well compared to previous investigations using SMC, which did not reveal any violation of the requirements.},
  archive   = {C_AAMAS},
  author    = {Wete, Eric and Greenyer, Joel and Kudenko, Daniel and Nejdl, Wolfgang},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1928–1937},
  title     = {Multi-robot motion and task planning in automotive production using controller-based safe reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663056},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards generalizability of multi-agent reinforcement
learning in graphs with recurrent message passing. <em>AAMAS</em>,
1919–1927. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph-based environments pose unique challenges to multi-agent reinforcement learning. In decentralized approaches, agents operate within a given graph and make decisions based on partial or outdated observations. The size of the observed neighborhood limits the generalizability to different graphs and affects the reactivity of agents, the quality of the selected actions, and the communication overhead. This work focuses on generalizability and resolves the trade-off in observed neighborhood size with a continuous information flow in the whole graph. We propose a recurrent message-passing model that iterates with the environment&#39;s steps and allows nodes to create a global representation of the graph by exchanging messages with their neighbors. Agents receive the resulting learned graph observations based on their location in the graph. Our approach can be used in a decentralized manner at runtime and in combination with a reinforcement learning algorithm of choice. We evaluate our method across 1000 diverse graphs in the context of routing in communication networks and find that it enables agents to generalize and adapt to changes in the graph.},
  archive   = {C_AAMAS},
  author    = {Weil, Jannis and Bao, Zhenghua and Abboud, Osama and Meuser, Tobias},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1919–1927},
  title     = {Towards generalizability of multi-agent reinforcement learning in graphs with recurrent message passing},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663055},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed online rollout for multivehicle routing in
unmapped environments. <em>AAMAS</em>, 1910–1918. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider a generalization of the well-known multivehicle routing problem: given a network, a set of agents occupying a subset of its nodes, and a set of tasks, we seek a minimum cost sequence of movements subject to the constraint that each task is visited by some agent at least once. The classical version of this problem assumes a central computational server that observes the entire state of the system perfectly and directs individual agents according to a centralized control scheme. In contrast, we assume that there is no centralized server and that each agent is an individual processor with no a priori knowledge of the underlying network, including the locations of tasks and other agents. Moreover, our agents possess strictly local communication and sensing capabilities (restricted to a fixed radius), aligning more closely with several real-world multiagent applications. We present a fully distributed, online, and scalable reinforcement learning algorithm for this problem whereby agents self-organize into local clusters to which they independently apply a multiagent rollout scheme. We demonstrate empirically via extensive simulations that there exists a critical sensing radius beyond which the distributed rollout algorithm begins to improve over a greedy base policy. This critical sensing radius grows proportionally to the log-star function of the size of the network and is therefore a small constant in practice. Our decentralized reinforcement learning algorithm achieves approximately a factor of two cost improvement over the base policy for a range of radii between two and three times the critical sensing radius. In addition, we observe in simulations that our distributed algorithm requires exponentially fewer computational resources than the centralized approach, at only a small constant factor detriment in cost. We validate our algorithm through physical robot experiments in continuous space and show that the resulting behavior reflects the discrete simulations.},
  archive   = {C_AAMAS},
  author    = {Weber, Jamison W. and Giriyan, Dhanush R. and Parkar, Devendra R. and Bertsekas, Dimitri P. and Richa, Andr\&#39;{e}a W.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1910–1918},
  title     = {Distributed online rollout for multivehicle routing in unmapped environments},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663054},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The reasons that agents act: Intention and instrumental
goals. <em>AAMAS</em>, 1901–1909. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Intention is an important and challenging concept in AI. It is important because it underlies many other concepts we care about, such as agency, manipulation, legal responsibility, and blame. However, ascribing intent to AI systems is contentious, and there is no universally accepted theory of intention applicable to AI agents. We operationalise the intention with which an agent acts, relating to the reasons it chooses its decision. We introduce a formal definition of intention in structural causal influence models, grounded in the philosophy literature on intent and applicable to real-world machine learning systems. Through a number of examples and results, we show that our definition captures the intuitive notion of intent and satisfies desiderata set-out by past work. In addition, we show how our definition relates to past concepts, including actual causality, and the notion of instrumental goals, which is a core idea in the literature on safe AI agents. Finally, we demonstrate how our definition can be used to infer the intentions of reinforcement learning agents and language models from their behaviour.},
  archive   = {C_AAMAS},
  author    = {Ward, Francis Rhys and MacDermott, Matt and Belardinelli, Francesco and Toni, Francesca and Everitt, Tom},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1901–1909},
  title     = {The reasons that agents act: Intention and instrumental goals},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663053},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalized response objectives for strategy exploration in
empirical game-theoretic analysis. <em>AAMAS</em>, 1892–1900. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the policy-space response oracle (PSRO) framework, strategy sets defining an empirical game are iteratively extended by computing each player&#39;s best response to a target profile. The method for selecting a target profile is called a meta-strategy solver (MSS), and a variety of MSSs have been proposed and analyzed for their effectiveness in exploring the strategy space. Here we investigate an alternative means to control strategy exploration: setting the response objective (RO) employed in deriving a strategy for a given target profile. In evaluating effectiveness of strategy exploration, we consider not only rate of convergence to a solution, but also the quality of solution(s) captured by the evolving empirical game. We perform our study first in the domain of sequential bargaining games, comparing the standard RO based on own payoff with others that incorporate other players&#39; payoffs. We find that an RO based on the bargaining solution and a social welfare related RO can lead to identifying equilibrium outcomes with significantly higher social welfare than the standard objective. For other proposed ROs, experiments demonstrate that they can differentially affect the makeup and value of solutions for different players. We further test PSRO with generalized ROs in large attack-graph games and a cooperative game Hanabi. We observe a similar impact and effectiveness of our ROs on strategy exploration. Finally, we establish a theoretical relationship between PSRO with generalized ROs and generalized weakened fictitious play in particular settings and a connection between the social welfare related RO with Berge equilibrium.},
  archive   = {C_AAMAS},
  author    = {Wang, Yongzhao and Wellman, Michael P.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1892–1900},
  title     = {Generalized response objectives for strategy exploration in empirical game-theoretic analysis},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663052},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Positive intra-group externalities in facility location.
<em>AAMAS</em>, 1883–1891. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study facility location games with multiple groups in one dimension where an agent&#39;s utility is not only decided by the distance from the facility but also by their group members. The positive effect of the interactions within a group is captured by positive intra-group externalities. Our goal is to design a mechanism that is non-manipulable and respects unanimity while (approximately) optimizing an objective function. We consider three types of manipulation, misreporting only the location, misreporting only the group membership, and misreporting both, under two social objectives, the social utility and the minimum utility. For both objectives, we achieve nearly tight bounds by either designing new mechanisms or extending the existing mechanisms in terms of the first two types of manipulation. As to the negative result, we show that strategyproofness and unanimity are incompatible when each agent can misreport both the location and the group membership, which is independent of the objective functions.},
  archive   = {C_AAMAS},
  author    = {Wang, Ying and Zhou, Houyu and Li, Minming},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1883–1891},
  title     = {Positive intra-group externalities in facility location},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663051},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal flash loan fee function with respect to leverage
strategies. <em>AAMAS</em>, 1874–1882. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate two decentralized methods for leveraging assets: Firstly, investors recurrently commit their target assets as collateral to secure loans, subsequently reinvesting the borrowed funds in the same assets. Secondly, investors pledge their assets once but are required to promptly borrow from a lender and repay the borrowed amount. This model is exemplified by recent Ethereum investment strategies, where investors must weigh the trade-off between gas fees associated with multiple pledging processes and fees charged by the lender, known as the Flash Loan project. Our comprehensive analysis encompasses game theory dynamics, determining optimal strategies for self-interested investors and deriving a unique non-linear optimal fee structure for Flash Loans. This structure remains incentive-compatible, guarding against Sybil attacks and other deviations. Empirical results, under varying environmental parameters, consistently demonstrate the superior revenue performance of our optimal fee structure compared to the commonly used linear fee model within the Flash Loan project.},
  archive   = {C_AAMAS},
  author    = {Wang, Chenmin and Li, Peng and Zeng, Yulong and Fan, Xuepeng},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1874–1882},
  title     = {Optimal flash loan fee function with respect to leverage strategies},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663050},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MABL: Bi-level latent-variable world model for
sample-efficient multi-agent reinforcement learning. <em>AAMAS</em>,
1865–1873. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent reinforcement learning (MARL) methods often suffer from high sample complexity, limiting their use in real-world problems where data is sparse or expensive to collect. Although latent-variable world models have been employed to address this issue by generating abundant synthetic data for MARL training, most of these models cannot encode vital global information available during training into their latent states, which hampers learning efficiency. The few exceptions that incorporate global information assume centralized execution of their learned policies, which is impractical in many applications with partial observability.We propose a novel model-based MARL algorithm, MABL (Multi-Agent Bi-Level world model), that learns a bi-level latent-variable world model from high-dimensional inputs. Unlike existing models, MABL is capable of encoding essential global information into the latent states during training while guaranteeing the decentralized execution of learned policies. For each agent, MABL learns a global latent state at the upper level, which is used to inform the learning of an agent latent state at the lower level. During execution, agents exclusively use lower-level latent states and act independently. Crucially, MABL can be combined with any model-free MARL algorithm for policy learning. In our empirical evaluation with complex discrete and continuous multi-agent tasks including SMAC, Flatland, and MAMuJoCo, MABL surpasses SOTA multi-agent latent-variable world models in both sample efficiency and overall performance.},
  archive   = {C_AAMAS},
  author    = {Venugopal, Aravind and Milani, Stephanie and Fang, Fei and Ravindran, Balaraman},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1865–1873},
  title     = {MABL: Bi-level latent-variable world model for sample-efficient multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663049},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enabling BDI agents to reason on a dynamic action repertoire
in hypermedia environments. <em>AAMAS</em>, 1856–1864. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomy requires adaptability and persistence in pursuing long-term objectives within evolving contexts. While BDI agents can cope with dynamic and uncertain environments, their adaptability is typically constrained by static action repertoires, known a priori by designers. Through Semantic Web and Web of Things technologies, machine-readable action descriptions can be discovered in hypermedia environments, and updated dynamically to mirror the evolving landscape of actions offered by real-world environments. This paper proposes the integration of action-oriented BDI reasoning with signifiers that reveal information about action possibilities that may appear, disappear, or be modified in a hypermedia environment at any time. We extend the means-end reasoning of BDI agents with a mechanism for resolving signifiers discovered at run time into actions, which enables agents to adjust their decision-making based on action possibilities advertised in the environment. We evaluate our approach through experiments, where Jason agents discover signifiers expressed with available Web ontologies. The results demonstrate that our signifier resolution mechanism enhances action reasoning at run time towards effective goal achievement in dynamic and unknown environments.},
  archive   = {C_AAMAS},
  author    = {Vachtsevanou, Danai and de Lima, Bruno and Ciortea, Andrei and H\&quot;{u}bner, Jomi Fred and Mayer, Simon and Lem\&#39;{e}e, J\&#39;{e}r\&#39;{e}my},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1856–1864},
  title     = {Enabling BDI agents to reason on a dynamic action repertoire in hypermedia environments},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663048},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reducing optimism bias in incomplete cooperative games.
<em>AAMAS</em>, 1847–1855. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cooperative game theory has diverse applications in contemporary artificial intelligence, including domains like interpretable machine learning, resource allocation, and collaborative decision-making. However, specifying a cooperative game entails assigning values to exponentially many coalitions, and obtaining even a single value can be resource-intensive in practice. Yet simply leaving certain coalition values undisclosed introduces ambiguity regarding individual contributions to the collective grand coalition. This ambiguity often leads to players holding overly optimistic expectations, stemming from either inherent biases or strategic considerations, frequently resulting in collective claims exceeding the actual grand coalition value. In this paper, we present a framework aimed at optimizing the sequence for revealing coalition values, with the overarching goal of efficiently closing the gap between players&#39; expectations and achievable outcomes in cooperative games. Our contributions are threefold: (i) we study the individual players&#39; optimistic completions of games with missing coalition values along with the arising gap, and investigate its analytical characteristics that facilitate more efficient optimization; (ii) we develop methods to minimize this gap over classes of games with a known prior by disclosing values of additional coalitions in both offline and online fashion; and (iii) we empirically demonstrate the algorithms&#39; performance in practical scenarios, together with an investigation into the typical order of revealing coalition values.},
  archive   = {C_AAMAS},
  author    = {\&#39;{U}radn\&#39;{\i}k, Filip and Sychrovsk\&#39;{y}, David and Cern\&#39;{y}, Jakub and Cern\&#39;{y}, Martin},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1847–1855},
  title     = {Reducing optimism bias in incomplete cooperative games},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663047},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Norm enforcement with a soft touch: Faster emergence,
happier agents. <em>AAMAS</em>, 1837–1846. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A multiagent system is a society of autonomous agents whose interactions can be regulated via social norms. In general, the norms of a society are not hardcoded but emerge from the agents&#39; interactions. Specifically, how the agents in a society react to each other&#39;s behavior and respond to the reactions of others determines which norms emerge in the society. We think of these reactions by an agent to the satisfactory or unsatisfactory behaviors of another agent as communications from the first agent to the second agent. Understanding these communications is a kind of social intelligence: these communications provide natural drivers for norm emergence by pushing agents toward certain behaviors, which can become established as norms. Whereas it is well-known that sanctioning can lead to the emergence of norms, we posit that a broader kind of social intelligence can prove more effective in promoting cooperation in a multiagent system.Accordingly, we develop Nest, a framework that models social intelligence via a wider variety of communications and understanding of them than in previous work. To evaluate Nest, we develop a simulated pandemic environment and conduct simulation experiments to compare Nest with baselines considering a combination of three kinds of social communication: sanction, tell, and hint.We find that societies formed of Nest agents achieve norms faster. Moreover, Nest agents effectively avoid undesirable consequences, which are negative sanctions and deviation from goals, and yield higher satisfaction for themselves than baseline agents despite requiring only an equivalent amount of information.},
  archive   = {C_AAMAS},
  author    = {Tzeng, Sz-Ting and Ajmeri, Nirav and Singh, Munindar P.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1837–1846},
  title     = {Norm enforcement with a soft touch: Faster emergence, happier agents},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663046},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assessing fairness of residential dynamic pricing for
electricity using active learning with agent-based simulation.
<em>AAMAS</em>, 1827–1836. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Extreme weather events and fast-paced adoption of green energy technologies have led to new challenges in demand-side management, maintaining grid reliability, and fulfilling variable consumer demands One of the effective ways to address these difficulties is by introducing economic incentives - replacing the flat rate tariffs with dynamic tariffs. However, dynamic pricing schemes need to be designed carefully to consider fairness and benefits for consumers as well as power companies. This paper describes an ML-based simulation framework for exploring two fairness constructs of dynamic pricing for residential electricity with behavioral agent-based models based on social theory combined with active learning. As an example, we simulate behavior adaptations in response to changes in electricity prices to study cost savings through monthly bills and peak demand reduction in synthetic household agents in a Time Of Use (TOU) pricing scheme in Virginia, USA. Further, we can show that there exists a region in the parameter space that corresponds to a fair TOU pricing scheme for both entities: all income-stratified communities and power companies.},
  archive   = {C_AAMAS},
  author    = {Thorve, Swapna and Mortveit, Henning and Vullikanti, Anil and Marathe, Madhav and Swarup, Samarth},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1827–1836},
  title     = {Assessing fairness of residential dynamic pricing for electricity using active learning with agent-based simulation},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663045},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards efficient auction design with ROI constraints.
<em>AAMAS</em>, 1818–1826. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Online advertising stands as a significant revenue source of the Internet. Recently, the trend among advertisers tilting towards the use of auto-bidding tools has heralded the emergence of a new model of bidders operating with constraints related to return on investment (ROI). However, most of the current research on ROI-constrained bidders in auction design only focuses on either the ROI constraints or values of bidders being private, while it is more practical to keep them both private in reality. Designing a truthful mechanism for bidders with both private values and ROI constraints introduces complexities because of the characteristics of designing mechanisms with multiple parameters. To remedy this, we divide bidders into binary classes: the traditional utility maximizers (UMs) who can be viewed as having an ROI constraint of 1, and the ROI-constrained bidders (RBs) who share a fixed ROI constraint denoted as γ. This framework retains the essence of multi-parameter mechanism design but transitions this into a more tractable form. Then we introduce a novel auction mechanism, cleverly combining the conventional VCG mechanism and an existing mechanism for public ROI-constrained bidders which is called Cavallo&#39;s mechanism. Our mechanism can achieve an approximation ratio of 3 over 2 on social welfare. Additionally, we unearth new insights into the limitations posed by ROI constraints. When the ROI constraint γ exceeds 2, the lower bound of social welfare is 5 over 4; when it falls below 2, the lower bound becomes 3+γ over 2+3γ-γ2.},
  archive   = {C_AAMAS},
  author    = {Tang, Xinyu and Lv, Hongtao and Gao, Yingjie and Wu, Fan and Liu, Lei and Cui, Lizhen},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1818–1826},
  title     = {Towards efficient auction design with ROI constraints},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663044},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the transit obfuscation problem. <em>AAMAS</em>,
1809–1817. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Concealing an intermediate point on a route or visible from a route is an important goal in some transportation and surveillance scenarios. This paper studies the Transit Obfuscation Problem, the problem of traveling from some start location to an end location while &quot;covering&quot; a specific transit point that needs to be concealed from adversaries. We propose the notion of transit anonymity, a quantitative guarantee of the anonymity of a specific transit point, even with a powerful adversary with full knowledge of the path planning algorithm. We propose and evaluate planning/search algorithms that satisfy this anonymity criterion.},
  archive   = {C_AAMAS},
  author    = {Takahashi, Hideaki and Fukunaga, Alex},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1809–1817},
  title     = {On the transit obfuscation problem},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663043},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Allocating contiguous blocks of indivisible chores fairly:
revisited. <em>AAMAS</em>, 1800–1808. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Resource allocation is a fundamental problem in multi-agent systems, with two key factors to consider: fairness and efficiency. The concept of the &quot;price of fairness&#39;&#39; helps in the understanding of efficiency loss under fairness constraints. Among the diverse resource allocation settings, cake cutting stands out as a prominent model. Recently, H\&quot;{o} hne and van Stee [Inf. Comput., 2021] examined a variation of this model in which the cake represents indivisible chores, with each agent requiring a connected piece of the chores. H\&quot;{o} hne and van Stee provided upper and lower bounds on the price of fairness when fairness is measured by envy-freeness and proportionality. However, in the case of indivisible items, achieving envy-free and proportional allocations is difficult, rendering these bounds insufficient for a comprehensive understanding of the true trade-off between fairness and efficiency. In this paper, we revisit the same problem and consider fairness notions that are satisfiable, including proportionality up to one item, and maximin share fairness. By presenting tight bounds on the price of fairness with respect to these notions, we complete the picture of fairness and efficiency trade-off.},
  archive   = {C_AAMAS},
  author    = {Sun, Ankang and Li, Bo},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1800–1808},
  title     = {Allocating contiguous blocks of indivisible chores fairly: Revisited},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663042},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-agent alternate q-learning. <em>AAMAS</em>, 1791–1799.
(<a href="https://dl.acm.org/doi/10.5555/3635637.3663041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Decentralized learning has shown great promise for cooperative multi-agent reinforcement learning (MARL). However, non-stationarity remains a significant challenge in fully decentralized learning. In the paper, we tackle the non-stationarity problem in the simplest and fundamental way and propose multi-agent alternate Q-learning (MA2QL), where agents take turns updating their Q-functions by Q-learning. MA2QL is a minimalist approach to fully decentralized cooperative MARL but is theoretically grounded. We prove that when each agent guarantees ε-convergence at each turn, their joint policy converges to a Nash equilibrium. In practice, MA2QL only requires minimal changes to independent Q-learning (IQL). We empirically evaluate MA2QL on a variety of cooperative multi-agent tasks. Results show MA2QL consistently outperforms IQL, which verifies the effectiveness of MA2QL, despite such minimal changes.},
  archive   = {C_AAMAS},
  author    = {Su, Kefan and Zhou, Siyuan and Jiang, Jiechuan and Gan, Chuang and Wang, Xiangjun and Lu, Zongqing},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1791–1799},
  title     = {Multi-agent alternate Q-learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663041},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Algorithmic filtering, out-group stereotype, and
polarization on social media. <em>AAMAS</em>, 1782–1790. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The introduction of social media websites touted the idea of global communication, exposing users to a worldwide audience and a diverse range of experiences, opinions, and debates. Unfortunately, studies have shown that social networks have instead contributed to growing levels of polarization in society across a wide variety of issues. Social media websites employ algorithmic filtering strategies to drive engagement, which can lead to the formation of filter bubbles and increased levels of polarization. In this paper, we introduce features of affective polarization --- feelings towards one&#39;s in-group and out-group --- into an opinion dynamics model. Specifically, we show that incorporating a negative out-group stereotype into the opinion dynamics model (1) affects the level of polarization present among agents in the network; (2) changes the effectiveness of algorithmic filtering strategies; and (3) is exacerbated by the presence of extremists in the network. Hence, the inclusion of an affective group mechanism in opinion dynamics modeling provides novel insights into the effects of algorithmic filtering strategies on the extremity of opinions in social networks.},
  archive   = {C_AAMAS},
  author    = {Springsteen, Jean and Yeoh, William and Christenson, Dino},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1782–1790},
  title     = {Algorithmic filtering, out-group stereotype, and polarization on social media},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663040},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Boosting studies of multi-agent reinforcement learning on
google research football environment: The past, present, and future.
<em>AAMAS</em>, 1772–1781. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Even though Google Research Football (GRF) was initially benchmarked and studied as a single-agent environment in its original paper [19], recent years have witnessed an increasing focus on its multi-agent nature by researchers utilizing it as a testbed for Multi-Agent Reinforcement Learning (MARL), especially in the cooperative scenarios. However, the absence of standardized environment settings and unified evaluation metrics for multi-agent scenarios hampers the consistent understanding of various studies. Furthermore, the challenging 5 vs 5 and 11 vs 11 full-game scenarios have received limited thorough examination due to their substantial training complexities. To address these gaps, this paper extends the original environment by not only standardizing the environment settings and benchmarking cooperative learning algorithms across different scenarios, including the most challenging full-game scenarios, but also by discussing approaches to enhance football AI from diverse perspectives and introducing related research tools for learning beyond multi-agent cooperation. Specifically, we provide a distributed and asynchronous population-based self-play framework with diverse pre-trained policies for faster training, two football-specific analytical tools for deeper investigation, and an online leaderboard for broader evaluation. The overall expectation of this work is to advance the study of Multi-Agent Reinforcement Learning both on and with Google Research Football environment, with the ultimate goal of deploying these technologies to real-world applications, such as sports analysis.},
  archive   = {C_AAMAS},
  author    = {Song, Yan and Jiang, He and Zhang, Haifeng and Tian, Zheng and Zhang, Weinan and Wang, Jun},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1772–1781},
  title     = {Boosting studies of multi-agent reinforcement learning on google research football environment: The past, present, and future},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663039},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On dealing with false beliefs and maintaining KD45n
property. <em>AAMAS</em>, 1763–1771. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motivated by counterintuitive results of the update of a Kripke structure using an update model, such as the inability to correct false beliefs of agents or the creation of incoherent state of beliefs of agents, this paper explores a novel methodology for updating a Kripke structure using an update model. The paper shows that the new definition helps agents correct their false beliefs when they are full observers of a sensing action or a truthful announcement. Furthermore, the paper presents a sufficient condition for update models that guarantees that the resulting Kripke structure maintains the KD45n property of the original Kripke structure if the update model is also KD45n. In particular, the majority of update models recently described in the literature for reasoning about knowledge and beliefs of agents in multi-agent domains satisfy such sufficient condition. This implies that the KD45n property will be maintained after the execution of an action sequence if the initial Kripke structure is KD45n.These results can help guide the design of update models for compound actions in applications dealing with knowledge and beliefs; they can also be used by epistemic planners that employ update models to correct agents&#39; false beliefs.},
  archive   = {C_AAMAS},
  author    = {Son, Tran Cao and Pham, Loc and Pontelli, Enrico},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1763–1771},
  title     = {On dealing with false beliefs and maintaining KD45n property},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663038},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Frugal actor-critic: Sample efficient off-policy deep
reinforcement learning using unique experiences. <em>AAMAS</em>,
1754–1762. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Efficient utilization of the replay buffer plays a significant role in the off-policy actor-critic reinforcement learning (RL) algorithms used for model-free control policy synthesis for complex dynamical systems. We propose a method for achieving sample efficiency, which focuses on selecting unique samples and adding them to the replay buffer during the exploration with the goal of reducing the buffer size and maintaining the independent and identically distributed (IID) nature of the samples. Our method is based on selecting an important subset of the set of state variables from the experiences encountered during the initial phase of random exploration, partitioning the state space into a set of abstract states based on the selected important state variables, and finally selecting the experiences with unique state-reward combination by using a kernel density estimator. We formally prove that the off-policy actor-critic algorithm incorporating the proposed method for unique experience accumulation converges faster than the vanilla off-policy actor-critic algorithm. Furthermore, we evaluate our method by comparing it with two state-of-the-art actor-critic RL algorithms on several continuous control benchmarks available in the Gym environment. Experimental results demonstrate that our method achieves a significant reduction in the size of the replay buffer for all the benchmarks while achieving either faster convergent or better reward accumulation compared to the baseline algorithms.},
  archive   = {C_AAMAS},
  author    = {Singh, Nikhil Kumar and Saha, Indranil},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1754–1762},
  title     = {Frugal actor-critic: Sample efficient off-policy deep reinforcement learning using unique experiences},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663037},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PAS: Probably approximate safety verification of
reinforcement learning policy using scenario optimization.
<em>AAMAS</em>, 1745–1753. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the advancement of machine learning based automation in the current digital world, the problem of safety verification of such systems is becoming crucial, especially in safety-critical domains like self-driving cars, robotics, etc. Reinforcement learning (RL) is an emerging machine learning technique with many applications, including in safety-critical domains. The classical safety verification approach of making a binary decision on determining whether a system is safe or unsafe is particularly challenging for an RL system. Such an approach generally requires prior knowledge about the system, e.g., the transition model of the system, the set of unsafe states in the environment, etc., which are typically unavailable in a standard RL setting. Instead, this paper addresses the safety verification problem from a quantitative safety perspective, i.e., we quantify the safe behavior of the policy in terms of probability. We formulate the safety verification problem as a chance-constrained optimization using the technique of barrier certificate. We then use a sampling based approach called scenario optimization to solve the chance-constrained problem, which gives the desired probabilistic guarantee on the safe behavior of the policy. Our extensive empirical evaluation shows the validity and robustness of our approach in three RL domains.},
  archive   = {C_AAMAS},
  author    = {Singh, Arambam James and Easwaran, Arvind},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1745–1753},
  title     = {PAS: Probably approximate safety verification of reinforcement learning policy using scenario optimization},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663036},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LgTS: Dynamic task sampling using LLM-generated sub-goals
for reinforcement learning agents. <em>AAMAS</em>, 1736–1744. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advancements in reasoning abilities of Large Language Models (LLM) has promoted their usage in problems that require high-level planning for artificial agents. However, current techniques that utilize LLMs for such planning tasks make certain key assumptions such as, access to datasets that permit finetuning, meticulously engineered prompts that only provide relevant and essential information to the LLM, and most importantly, a deterministic approach to allow execution of the LLM responses either in the form of existing policies or plan operators. In this work, we propose LgTS (LLM-guided Teacher-Student learning), a novel approach that explores the planning abilities of LLMs to provide a graphical representation of the sub-goals to a reinforcement learning (RL) agent that does not have access to the transition dynamics of the environment. The RL agent uses Teacher-Student learning algorithm to learn a set of successful policies for reaching the goal state from the start state while simultaneously minimizing the number of environmental interactions. Unlike previous methods that utilize LLMs, our approach does not assume access to a fine-tuned LLM, nor does it require pre-trained policies that achieve the sub-goals proposed by the LLM. Through experiments on a gridworld based DoorKey domain and a search-and-rescue inspired domain, we show that a LLM-proposed graphical structure for sub-goals combined with a Teacher-Student RL algorithm achieves sample-efficient policies.},
  archive   = {C_AAMAS},
  author    = {Shukla, Yash and Gao, Wenchang and Sarathy, Vasanth and Velasquez, Alvaro and Wright, Robert and Sinapov, Jivko},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1736–1744},
  title     = {LgTS: Dynamic task sampling using LLM-generated sub-goals for reinforcement learning agents},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663035},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Relaxed exploration constrained reinforcement learning.
<em>AAMAS</em>, 1727–1735. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This research introduces a novel setting for reinforcement learning with constraints, termed Relaxed Exploration Constrained Reinforcement Learning (RECRL). Similar to standard constrained reinforcement learning (CRL), the objective in RECRL is to discover a policy that maximizes the environmental return while adhering to a predefined set of constraints. However, in some real-world settings, it is possible to train the agent in a setting that does not require strict adherence to the constraints, as long as the agent adheres to them once deployed. To model such settings, we introduce RECRL, which explicitly incorporates an initial training phase where the constraints are relaxed, enabling the agent to explore the environment more freely. Subsequently, during deployment, the agent is obligated to fully satisfy all constraints. To address RECRL problems, we introduce a curriculum-based approach called CLiC, designed to enhance the exploration of existing CRL algorithms during the training phase and facilitate convergence towards a policy that satisfies the full set of constraints by the end of training. Empirical evaluations demonstrate that CLiC yields policies with significantly higher returns during deployment compared to training solely under the strict set of constraints. The code is available at https://github.com/Shperb/RECRL.},
  archive   = {C_AAMAS},
  author    = {Shperberg, Shahaf S. and Liu, Bo and Stone, Peter},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1727–1735},
  title     = {Relaxed exploration constrained reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663034},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling cognitive biases in decision-theoretic planning for
active cyber deception. <em>AAMAS</em>, 1718–1726. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an approach to modeling and exploiting cognitive biases of cyber attackers in planning for active deception. Sophisticated cyber attacks are primarily orchestrated by human actors. Hence, we focus on the human aspect of the attacker&#39;s decision-making process. Humans deviate from rational decision-making due to various cognitive biases. Here, we focus on fundamental attribution error (FAE) and confirmation bias and their role in cyber deception because these biases contribute to humans being deceived. We use the decision-theoretic planning framework of finitely-nested factored I-POMDP (I-POMDPχ), which allows us to explicitly model FAE in multi-agent settings and build cognitive models of the attackers. We show how these biases impact their beliefs as they act and obtain more information about the environment and the adversary. The tractability of the I-POMDPχ also allows for modeling agents at a higher strategy level where the optimal policy relies on induction and exploitation of these biases. Hence, we also present an I-POMDPχ-based rational defender agent that can model the attacker&#39;s beliefs under the influence of FAE and confirmation bias from a higher strategic level, and exploit them. Our experiments in simulated interactions show that the I-POMDPχ-based defender agent can induce FAE in an attacker to distort the attacker&#39;s beliefs. Consequently, the defender agent can exploit the attacker&#39;s cognitive biases to extend the duration of the attack to facilitate the attacker&#39;s intent recognition in a controlled environment. Our work provides a general decision-theoretic formulation of FAE and confirmation bias, and demonstrates its role in planning for agent-based active cyber deception.},
  archive   = {C_AAMAS},
  author    = {Shinde, Aditya and Doshi, Prashant},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1718–1726},
  title     = {Modeling cognitive biases in decision-theoretic planning for active cyber deception},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663033},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Battlefield transfers in coalitional blotto games.
<em>AAMAS</em>, 1710–1717. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In competitive resource allocation environments, agents often choose to form alliances; however, for some agents, doing so may not always be beneficial. Is there a method of forming alliances that always reward each of their members? We study this question using the framework of the coalitional Blotto game, in which two players compete against a common adversary by allocating their budgeted resources across disjoint sets of valued battlefields. On any given battlefield, the agent that allocates a greater amount of resources wins the corresponding battlefield value. Existing work has shown the surprising result that in certain game instances, if one player donates a portion of their budget to the other player, then both players win larger amounts in their separate competitions against the adversary. However, this transfer-based method of alliance formation is not always mutually beneficial, which motivates the search for alternate strategies. In this vein, we study a new method of alliance formation referred to as a joint transfer, whereby players publicly transfer battlefields and budgets between one another before they engage in their separate competitions against the adversary. We show that in almost all game instances, there exists a mutually beneficial joint transfer that strictly increases the payoff of each player.},
  archive   = {C_AAMAS},
  author    = {Shah, Vade and Marden, Jason R.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1710–1717},
  title     = {Battlefield transfers in coalitional blotto games},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663032},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient public health intervention planning using
decomposition-based decision-focused learning. <em>AAMAS</em>,
1701–1709. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The declining participation of beneficiaries over time is a key concern in public health programs. A popular strategy for improving retention is to have health workers &#39;intervene&#39; on beneficiaries at risk of dropping out. However, the availability and time of these health workers are limited resources. As a result, there has been a line of research on optimizing these limited intervention resources using Restless Multi-Armed Bandits (RMABs). The key technical barrier to using this framework in practice lies in the need to estimate the beneficiaries&#39; RMAB parameters from historical data. Recent research has shown that Decision-Focused Learning (DFL), which focuses on maximizing the beneficiaries&#39; adherence rather than predictive accuracy, improves the performance of intervention targeting using RMABs. Unfortunately, these gains come at a high computational cost because of the need to solve and evaluate the RMAB in each DFL training step. In this paper, we provide a principled way to exploit the structure of RMABs to speed up intervention planning by cleverly decoupling the planning for different beneficiaries. We use real-world data from an Indian NGO, ARMMAN, to show that our approach is up to two orders of magnitude faster than the state-of-the-art approach while also yielding superior model performance. This would enable the NGO to scale up deployments using DFL to potentially millions of mothers, ultimately advancing progress toward UNSDG 3.1.},
  archive   = {C_AAMAS},
  author    = {Shah, Sanket and Suggala, Arun and Tambe, Milind and Taneja, Aparna},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1701–1709},
  title     = {Efficient public health intervention planning using decomposition-based decision-focused learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663031},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Value alignment in participatory budgeting. <em>AAMAS</em>,
1692–1700. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Participatory budgeting empowers citizens to take an active role in shaping their government&#39;s policies by influencing the allocation of a limited budget. In this process, citizens file various proposals and then collectively decide which ones should receive funding through a voting system. While participatory budgets have garnered significant attention in research and practice, one aspect so far overlooked is the ethical dimension of the proposals. Thus, beyond just gauging citizen preferences, we propose also to consider how these initiatives align with the government&#39;s core values. Specifically, we apply optimisation techniques to solve a multi-criteria decision problem that considers both citizen support and value alignment when choosing the proposals to fund. We illustrate our method in two real case studies and analyse how we can combine both criteria in an egalitarian way that does not necessarily compromise the will of citizens and may encourage governments to broaden the objectives and increase the allocated budget.},
  archive   = {C_AAMAS},
  author    = {Serramia, Marc and Lopez-Sanchez, Maite and Rodriguez-Aguilar, Juan A. and Moretti, Stefano},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1692–1700},
  title     = {Value alignment in participatory budgeting},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663030},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-user norm consensus. <em>AAMAS</em>, 1683–1691. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many agents act in environments with multiple human users, from care robots to smart assistants. When interacting in multi-user environments it is paramount that these agents act as all users expect. However, it is not always possible to have well-defined collective preferences, nor to easily infer them from individual preferences. This is especially true in fast changing environments, like a device placed in a public space where users can enter and exit freely. In response, this paper proposes a model to represent individual preferences about the behaviour of an agent and a mechanism to find multi-user consensuses over these preferences. Norms can then be generated to ensure that when the agent follows them it will act according to the preferences of all users. We formalise what a consensus norm is and what properties the set of consensus norms should satisfy (i.e. generate the minimum number of norms while maximising the coverage of user preferences). We provide an optimisation approach to find this set of norms and show that our approach satisfies the aforementioned properties.},
  archive   = {C_AAMAS},
  author    = {Serramia, Marc and Criado, Natalia and Luck, Michael},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1683–1691},
  title     = {Multi-user norm consensus},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663029},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IDIL: Imitation learning of intent-driven expert behavior.
<em>AAMAS</em>, 1673–1682. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When faced with accomplishing a task, human experts exhibit intentional behavior. Their unique intents shape their plans and decisions, resulting in experts demonstrating diverse behaviors to accomplish the same task. Due to the uncertainties encountered in the real world and their bounded rationality, experts sometimes adjust their intents, which in turn influences their behaviors during task execution. This paper introduces IDIL, a novel imitation learning algorithm to mimic these diverse intent-driven behaviors of experts. Iteratively, our approach estimates expert intent from heterogeneous demonstrations and then uses it to learn an intent-aware model of their behavior. Unlike contemporary approaches, IDIL is capable of addressing sequential tasks with high-dimensional state representations, while sidestepping the complexities and drawbacks associated with adversarial training (a mainstay of related techniques). Our empirical results suggest that the models generated by IDIL either match or surpass those produced by recent imitation learning benchmarks in metrics of task performance. Moreover, as it creates a generative model, IDIL demonstrates superior performance in intent inference metrics, crucial for human-agent interactions, and aptly captures a broad spectrum of expert behaviors.},
  archive   = {C_AAMAS},
  author    = {Seo, Sangwon and Unhelkar, Vaibhav},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1673–1682},
  title     = {IDIL: Imitation learning of intent-driven expert behavior},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663028},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CORE: Towards scalable and efficient causal discovery with
reinforcement learning. <em>AAMAS</em>, 1664–1672. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Causal discovery is the challenging task of inferring causal structure from data. Motivated by Pearl&#39;s Causal Hierarchy (PCH), which tells us that passive observations alone are not enough to distinguish correlation from causation, there has been a recent push to incorporate interventions into machine learning research. Reinforcement learning provides a convenient framework for such an active approach to learning. This paper presents CORE, a deep reinforcement learning-based approach for causal discovery and intervention planning. CORE learns to sequentially reconstruct causal graphs from data while learning to perform informative interventions. Our results demonstrate that CORE generalizes to unseen graphs and efficiently uncovers causal structures. Furthermore, CORE scales to larger graphs with up to 10 variables and outperforms existing approaches in structure estimation accuracy and sample efficiency. All relevant code and supplementary material can be found at https://github.com/sa-and/CORE.},
  archive   = {C_AAMAS},
  author    = {Sauter, Andreas and Botteghi, Nicol\`{o} and Acar, Erman and Plaat, Aske},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1664–1672},
  title     = {CORE: Towards scalable and efficient causal discovery with reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663027},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Computing optimal commitments to strategies and
outcome-conditional utility transfers. <em>AAMAS</em>, 1654–1663. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Prior work [14,15] has studied the computational complexity of computing optimal strategies to commit to in Stackelberg or leadership games, where a leader commits to a strategy which is then observed by one or more followers. We extend this setting to one in which the leader can additionally commit to outcome-conditional utility transfers. In this setting, we characterize the computational complexity of finding optimal commitments for normal-form and Bayesian games. We find a mix of polynomial time algorithms and NP-hardness results. Then, we allow the leader to additionally commit to a signaling scheme based on her action, inducing a correlated equilibrium. In this variant, optimal commitments can be computed efficiently for arbitrarily many players.},
  archive   = {C_AAMAS},
  author    = {Sauerberg, Nathaniel and Oesterheld, Caspar},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1654–1663},
  title     = {Computing optimal commitments to strategies and outcome-conditional utility transfers},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663026},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The triangles of dishonesty: Modelling the evolution of
lies, bullshit, and deception in agent societies. <em>AAMAS</em>,
1645–1653. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Misinformation and disinformation in agent societies can be spread due to the adoption of dishonest communication. Recently, this phenomenon has been exacerbated by advances in AI technologies. One way to understand dishonest communication is to model it from an agent-oriented perspective. In this paper we model dishonesty games considering the existing literature on lies, bullshit, and deception, three prevalent but distinct forms of dishonesty. We use an evolutionary agent-based replicator model to simulate dishonesty games and show the differences between the three types of dishonest communication under two different sets of assumptions: agents are either self-interested (payoff maximizers) or competitive (relative payoff maximizers). We show that: (i) truth-telling is not stable in the face of lying, but that interrogation helps drive truth-telling in the self-interested case but not the competitive case; (ii) that in the competitive case, agents stop bullshitting and start truth-telling, but this is not stable; (iii) that deception can only dominate in the competitive case, and that truth-telling is a saddle point in which agents realise deception can provide better payoffs.},
  archive   = {C_AAMAS},
  author    = {Sarkadi, Stefan and Lewis, Peter R.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1645–1653},
  title     = {The triangles of dishonesty: Modelling the evolution of lies, bullshit, and deception in agent societies},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663025},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-agent diagnostics for robustness via illuminated
diversity. <em>AAMAS</em>, 1630–1644. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the rapidly advancing field of multi-agent systems, ensuring robustness in unfamiliar and adversarial settings is crucial. Notwithstanding their outstanding performance in familiar environments, these systems often falter in new situations due to overfitting during the training phase. This is especially pronounced in settings where both cooperative and competitive behaviours are present, encapsulating a dual nature of overfitting and generalisation challenges. To address this issue, we present Multi-Agent Diagnostics for Robustness via Illuminated Diversity (MADRID), a novel approach for generating diverse adversarial scenarios that expose strategic vulnerabilities in pre-trained multi-agent policies. Leveraging the concepts from open-ended learning, MADRID navigates the vast space of adversarial settings, employing a target policy&#39;s regret to gauge the vulnerabilities of these settings. We evaluate the effectiveness of MADRID on the 11vs11 version of Google Research Football, one of the most complex environments for multi-agent reinforcement learning. Specifically, we employ MADRID for generating a diverse array of adversarial settings for TiZero, the state-of-the-art approach which &quot;masters&quot; the game through 45 days of training on a large-scale distributed infrastructure. We expose key shortcomings in TiZero&#39;s tactical decision-making, underlining the crucial importance of rigorous evaluation in multi-agent systems.},
  archive   = {C_AAMAS},
  author    = {Samvelyan, Mikayel and Paglieri, Davide and Jiang, Minqi and Parker-Holder, Jack and Rockt\&quot;{a}schel, Tim},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1630–1644},
  title     = {Multi-agent diagnostics for robustness via illuminated diversity},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663024},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design patterns for explainable agents (XAg).
<em>AAMAS</em>, 1621–1629. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to explain the behaviour of the AI systems is a key aspect of building trust, especially for autonomous agent systems - how does one trust an agent whose behaviour can not be explained? In this work, we advocate the use of design patterns for developing explainable-by-design agents (XAg), to ensure explainability is an integral feature of agent systems rather than an &quot;add-on&quot; feature. We present TriQPAN (Trigger, Query, Process, Action and Notify), a design pattern for XAg. TriQPAN can be used to explain behaviours of any agent architecture and we show how this can be done to explain decisions such as why the agent chose to pursue a particular goal, why or why didn&#39;t the agent choose a particular plan to achieve a goal, and so on. We term these queries as direct queries. Our framework also supports temporal correlation queries such as asking a search and rescue drone, &quot;which locations did you visit and why?&quot;. We implemented TriQPAN in the SARL agent language, built-in to the goal reasoning engine, affording developers XAg with minimal overhead. The implementation will be made available for public use. We describe that implementation and apply it to two case studies illustrating the explanations produced, in practice.},
  archive   = {C_AAMAS},
  author    = {Rodriguez, Sebastian and Thangarajah, John and Davey, Andrew},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1621–1629},
  title     = {Design patterns for explainable agents (XAg)},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663023},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interactively learning the user’s utility for best-arm
identification in multi-objective multi-armed bandits. <em>AAMAS</em>,
1611–1620. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many real-world problems have multiple, conflicting objectives. Without knowing the utility function of the decision maker, one must extensively learn all Pareto-efficient trade-offs to make sure that the true preferred policy is included in the learned set. Because such thorough exploration can be expensive (especially in high-dimensional multi-objective problems), a possible alternative is to allow some form of interaction with the decision maker as to gain some information about the utility function. In particular, in this work we assume that limited queries can be made to the policy maker to gather some information about the true utility function, concurrently to the search process being carried out. Improving our knowledge over the utility function narrows the search-space of the optimal policy. In turn, this results in more relevant trade-offs used to query the decision maker. Thus, correctly timing the queries is crucial to maximize information gain. We refer to this setting as fixed-budget best-arm identification for multi-objective multi-armed bandits, which adds to the traditional arm-pull actions a separate query-action that can be taken instead, where both actions have fixed but separate budgets. We propose Monte-Carlo Bayesian Utility Learning (MCBUL), a method based on Monte-Carlo planning that is able to optimize the timing of query-actions w.r.t. the arm-pull actions. We show that MCBUL significantly improves the chances of finding the optimal policy compared to baselines that interact with the decision maker at fixed intervals.},
  archive   = {C_AAMAS},
  author    = {Reymond, Mathieu and Bargiacchi, Eugenio and Roijers, Diederik M. and Now\&#39;{e}, Ann},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1611–1620},
  title     = {Interactively learning the user&#39;s utility for best-arm identification in multi-objective multi-armed bandits},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663022},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online decentralised mechanisms for dynamic ridesharing.
<em>AAMAS</em>, 1602–1610. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ridesharing services promise an exciting new future for urban mobility. A carefully designed ridesharing system will decrease congestion levels and increase air quality. However, an effective system needs to capture online demand, where users do not schedule their trips in advance, but instead appear and ask for a ride right away. Such online demands require rerouting to be efficient. We call this setting with online demands and available rerouting &quot;dynamic ridesharing&#39;&#39; and propose a market-based mechanism where the prospective riders are provided with a menu of choices between several available cars. Our algorithm incentivises users to share their rides and guarantees riders&#39; utility by properly compensating riders whose routes change during their journey. We provide numerical results, comparing our algorithm against natural benchmarks representing real-world ridesharing services for several cases and with respect to efficiency, fairness, and environmental impact.},
  archive   = {C_AAMAS},
  author    = {Protopapas, Nicos and Yazdanpanah, Vahid and Gerding, Enrico H. and Stein, Sebastian},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1602–1610},
  title     = {Online decentralised mechanisms for dynamic ridesharing},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663021},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust knowledge extraction from large language models using
social choice theory. <em>AAMAS</em>, 1593–1601. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large-language models (LLMs) can support a wide range of applications like conversational agents, creative writing or general query answering. However, they are ill-suited for query answering in high-stake domains like medicine because they are typically not robust - even the same query can result in different answers when prompted multiple times. In order to improve the robustness of LLM queries, we propose using ranking queries repeatedly and to aggregate the queries using methods from social choice theory. We study ranking queries in diagnostic settings like medical and fault diagnosis and discuss how the Partial Borda Choice function from the literature can be applied to merge multiple query results. We discuss some additional interesting properties in our setting and evaluate the robustness of our approach empirically.},
  archive   = {C_AAMAS},
  author    = {Potyka, Nico and Zhu, Yuqicheng and He, Yunjie and Kharlamov, Evgeny and Staab, Steffen},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1593–1601},
  title     = {Robust knowledge extraction from large language models using social choice theory},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663020},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Atlas-x equity financing: Unlocking new methods to securely
obfuscate axe inventory data based on differential privacy.
<em>AAMAS</em>, 1585–1592. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Banks publish daily a list of available securities/assets (axe list) to selected clients to help them effectively locate Long (buy) or Short (sell) trades at reduced financing rates. This reduces costs for the bank, as the list aggregates the bank&#39;s internal firm inventory per asset for all clients of long as well as short trades. However, this is somewhat problematic: (1) the bank&#39;s inventory is revealed; (2) trades of clients who contribute to the aggregated list, particularly those deemed large, are revealed to other clients. Clients conducting sizable trades with the bank and possessing a portion of the aggregated asset exceeding 50\% are considered to be concentrated clients. This could potentially reveal a trading concentrated client&#39;s activity to their competitors, thus providing an unfair advantage over the market.Atlas-X Axe Obfuscation, powered by new differential private methods, enables a bank to obfuscate its published axe list on a daily basis while under continual observation, thus maintaining an acceptable inventory Profit and Loss (P&amp;amp;L) cost pertaining to the noisy obfuscated axe list while reducing the clients&#39; trading activity leakage. Our main differential private innovation is a differential private aggregator for streams (time series data) of both positive and negative integers under continual observation.For the last two years, the Atlas-X system has been live in production across three major regions-USA, Europe, and Asia-at J.P. Morgan, a major financial institution. To our knowledge, it is the first differential privacy solution to be deployed in the financial sector. We also report benchmarks of our algorithm based on (anonymous) real and synthetic data to showcase the quality of our obfuscation and its success in production.},
  archive   = {C_AAMAS},
  author    = {Polychroniadou, Antigoni and Ciprianni, Gabriele and Hua, Richard and Balch, Tucker},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1585–1592},
  title     = {Atlas-X equity financing: Unlocking new methods to securely obfuscate axe inventory data based on differential privacy},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663019},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simultaneously achieving group exposure fairness and
within-group meritocracy in stochastic bandits. <em>AAMAS</em>,
1576–1584. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing approaches to fairness in stochastic multi-armed bandits (MAB) primarily focus on exposure guarantee to individual arms. When arms are naturally grouped by certain attribute(s), we propose Bi-Level Fairness, which considers two levels of fairness. At the first level, Bi-Level Fairness guarantees a certain minimum exposure to each group. To address the unbalanced allocation of pulls to individual arms within a group, we consider meritocratic fairness at the second level, which ensures that each arm is pulled according to its merit within the group. Our work shows that we can adapt a UCB-based algorithm to achieve a Bi-Level Fairness by providing (i) anytime Group Exposure Fairness guarantees and (ii) ensuring individual-level Meritocratic Fairness within each group. We first show that one can decompose regret bounds into two components: (a) regret due to anytime group exposure fairness and (b) regret due to meritocratic fairness within each group. Our proposed algorithm BF-UCB balances these two regrets optimally to achieve the upper bound of O(√T) on regret; T being the stopping time. With the help of simulated experiments, we further show that BF-UCB achieves sub-linear regret; provides better group and individual exposure guarantees compared to existing algorithms; and does not result in a significant drop in reward with respect to UCB algorithm, which does not impose any fairness constraint.},
  archive   = {C_AAMAS},
  author    = {Pokhriyal, Subham and Jain, Shweta and Ghalme, Ganesh and Dhamal, Swapnil and Gujar, Sujit},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1576–1584},
  title     = {Simultaneously achieving group exposure fairness and within-group meritocracy in stochastic bandits},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663018},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Single-winner voting with alliances: Avoiding the spoiler
effect. <em>AAMAS</em>, 1567–1575. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the setting of single-winner elections with ordinal preferences where candidates might be members of alliances (which may correspond to e.g., political parties, factions, or coalitions). However, we do not assume that candidates from the same alliance are necessarily adjacent in voters&#39; rankings. In such a case, every classical voting rule is vulnerable to the spoiler effect, i.e., the presence of a candidate may harm his or her alliance. We therefore introduce a new idea of alliance-aware voting rules which extend the classical ones. We show that our approach is superior both to using classical cloneproof voting rules and to running primaries within alliances before the election. We introduce several alliance-aware voting rules and show that they satisfy the most desirable standard properties of their classical counterparts as well as newly introduced axioms for the model with alliances which, e.g., exclude the possibility of the spoiler effect. Our rules have natural definitions and are simple enough to explain to be used in practice.},
  archive   = {C_AAMAS},
  author    = {Pierczynski, Grzegorz and Szufa, Stanislaw},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1567–1575},
  title     = {Single-winner voting with alliances: Avoiding the spoiler effect},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663017},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Confidence-based curriculum learning for multi-agent path
finding. <em>AAMAS</em>, 1558–1566. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A wide range of real-world applications can be formulated as Multi-Agent Path Finding (MAPF) problem, where the goal is to find collision-free paths for multiple agents with individual start and goal locations. State-of-the-art MAPF solvers are mainly centralized and depend on global information, which limits their scalability and flexibility regarding changes or new maps that would require expensive replanning. Multi-agent reinforcement learning (MARL) offers an alternative way by learning decentralized policies that can generalize over a variety of maps. While there exist some prior works that attempt to connect both areas, the proposed techniques are heavily engineered and very complex due to the integration of many mechanisms that limit generality and are expensive to use. We argue that much simpler and general approaches are needed to bring the areas of MARL and MAPF closer together with significantly lower costs. In this paper, we propose Confidence-based Auto-Curriculum for Team Update Stability (CACTUS) as a lightweight MARL approach to MAPF. CACTUS defines a simple reverse curriculum scheme, where the goal of each agent is randomly placed within an allocation radius around the agent&#39;s start location. The allocation radius increases gradually as all agents improve, which is assessed by a confidence-based measure. We evaluate CACTUS in various maps of different sizes, obstacle densities, and numbers of agents. Our experiments demonstrate better performance and generalization capabilities than state-of-the-art MARL approaches with less than 600,000 trainable parameters, which is less than 5\% of the neural network size of current MARL approaches to MAPF.},
  archive   = {C_AAMAS},
  author    = {Phan, Thomy and Driscoll, Joseph and Romberg, Justin and Koenig, Sven},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1558–1566},
  title     = {Confidence-based curriculum learning for multi-agent path finding},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663016},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Monitored markov decision processes. <em>AAMAS</em>,
1549–1557. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In reinforcement learning (RL), an agent learns to perform a task by interacting with an environment and receiving feedback (a numerical reward) for its actions. However, the assumption that rewards are always observable is often not applicable in real-world problems. For example, the agent may need to ask a human to supervise its actions or activate a monitoring system to receive feedback. There may even be a period of time before rewards become observable, or a period of time after which rewards are no longer given. In other words, there are cases where the environment generates rewards in response to the agent&#39;s actions but the agent cannot observe them. In this paper, we formalize a novel but general RL framework - Monitored MDPs - where the agent cannot always observe rewards. We discuss the theoretical and practical consequences of this setting, show challenges raised even in toy environments, and propose algorithms to begin to tackle this novel setting. This paper introduces a powerful new formalism that encompasses both new and existing problems and lays the foundation for future research.},
  archive   = {C_AAMAS},
  author    = {Parisi, Simone and Mohammedalamen, Montaser and Kazemipour, Alireza and Taylor, Matthew E. and Bowling, Michael},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1549–1557},
  title     = {Monitored markov decision processes},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663015},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving mobile maternal and child health care programs:
Collaborative bandits for time slot selection. <em>AAMAS</em>,
1540–1548. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Maternal and child health is a global priority, reflected in the UN Sustainable Development Goal 3.1. Mobile health (mHealth) programs, using automated voice messages, are a vital tool for NGOs to disseminate health information in underserved communities. However, these programs face challenges: limited beneficiary phone access and unknown time preferences hinder timely outreach, leading to poor engagement. We address this by formulating the time preference inference problem as a multi-agent multi-armed bandit optimization problem, where beneficiaries are modeled as agents, and time slots as arms. We introduce a novel online collaborative filtering framework that infers preferred time slots by collaborating across beneficiaries to quickly identify their preferred time slots.To highlight the scope and impact of this problem, we are working with Kilkari, the world&#39;s largest maternal and child mHealth program serving millions in India every week. Kilkari faces substantial reattempt costs to improve call answer rates. Through extensive experiments on real-world data obtained from Kilkari, we demonstrate that our collaborative bandit framework significantly outperforms both existing policies used by the NGO, and popular non-collaborative bandit algorithms (e.g., Upper Confidence Bound), both in terms of number of call retries, saving critical bandwidth that enables wider outreach, and by rapidly learning optimal time slots, improving beneficiary engagement and retention.},
  archive   = {C_AAMAS},
  author    = {Pal, Soumyabrata and Tambe, Milind and Suggala, Arun and Shanmugam, Karthikeyan and Taneja, Aparna},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1540–1548},
  title     = {Improving mobile maternal and child health care programs: Collaborative bandits for time slot selection},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663014},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A computational framework of human values. <em>AAMAS</em>,
1531–1539. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There is an increasing recognition of the need to engineer AI that respects and embodies human values. The value alignment problem, which identifies that need, has led to a growing body of research that investigates value learning, the aggregation of individual values into the values of groups, the alignment of norms with values, and the design of other computational mechanisms that reason over values in general. Yet despite these efforts, no foundational, computational model of human values has been proposed. In response, we propose a model for the computational representation of human values that builds upon a sustained body of research from social psychology.},
  archive   = {C_AAMAS},
  author    = {Osman, Nardine and d&#39;Inverno, Mark},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1531–1539},
  title     = {A computational framework of human values},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663013},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Emergent cooperation under uncertain incentive alignment.
<em>AAMAS</em>, 1521–1530. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Understanding the emergence of cooperation in systems of computational agents is crucial for the development of effective cooperative AI. Interaction among individuals in real-world settings are often sparse and occur within a broad spectrum of incentives, which often are only partially known. In this work, we explore how cooperation can arise among reinforcement learning agents in scenarios characterised by infrequent encounters, and where agents face uncertainty about the alignment of their incentives with those of others. To do so, we train the agents under a wide spectrum of environments ranging from fully competitive, to fully cooperative, to mixed-motives. Under this type of uncertainty we study the effects of mechanisms, such as reputation and intrinsic rewards, that have been proposed in the literature to foster cooperation in mixed-motives environments. Our findings show that uncertainty substantially lowers the agents&#39; ability to engage in cooperative behaviour, when that would be the best course of action. In this scenario, the use of effective reputation mechanisms and intrinsic rewards boosts the agents&#39; capability to act nearly-optimally in cooperative environments, while greatly enhancing cooperation in mixed-motive environments as well.},
  archive   = {C_AAMAS},
  author    = {Orzan, Nicole and Acar, Erman and Grossi, Davide and R\u{a}dulescu, Roxana},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1521–1530},
  title     = {Emergent cooperation under uncertain incentive alignment},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663012},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning and sustaining shared normative systems via
bayesian rule induction in markov games. <em>AAMAS</em>, 1510–1520. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A universal feature of human societies is the adoption of systems of rules and norms in the service of cooperative ends. How can we build learning agents that do the same, so that they may flexibly cooperate with the human institutions they are embedded in? We hypothesize that agents can achieve this by assuming there exists a shared set of norms that most others comply with while pursuing their individual desires, even if they do not know the exact content of those norms. By assuming shared norms, a newly introduced agent can infer the norms of an existing population from observations of compliance and violation. Furthermore, groups of agents can converge to a shared set of norms, even if they initially diverge in their beliefs about what the norms are. This in turn enables the stability of the normative system: since agents can bootstrap common knowledge of the norms, this leads the norms to be widely adhered to, enabling new entrants to rapidly learn those norms. We formalize this framework in the context of Markov games and demonstrate its operation in a multi-agent environment via approximately Bayesian rule induction of obligative and prohibitive norms. Using our approach, agents are able to rapidly learn and sustain a variety of cooperative institutions, including resource management norms and compensation for pro-social labor, promoting collective welfare while still allowing agents to act in their own interests.},
  archive   = {C_AAMAS},
  author    = {Oldenburg, Ninell and Zhi-Xuan, Tan},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1510–1520},
  title     = {Learning and sustaining shared normative systems via bayesian rule induction in markov games},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663011},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Engineering LaCAM*: Towards real-time, large-scale, and
near-optimal multi-agent pathfinding. <em>AAMAS</em>, 1501–1509. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the challenges of real-time, large-scale, and near-optimal multi-agent pathfinding (MAPF) through enhancements to the recently proposed LaCAM* algorithm. LaCAM* is a scalable search-based algorithm that guarantees the eventual finding of optimal solutions for cumulative transition costs. While it has demonstrated remarkable planning success rates, surpassing various state-of-the-art MAPF methods, its initial solution quality is far from optimal, and its convergence speed to the optimum is slow. To overcome these limitations, this paper introduces several improvement techniques, partly drawing inspiration from other MAPF methods. We provide empirical evidence that the fusion of these techniques significantly improves the solution quality of LaCAM*, thus further pushing the boundaries of MAPF algorithms.},
  archive   = {C_AAMAS},
  author    = {Okumura, Keisuke},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1501–1509},
  title     = {Engineering LaCAM*: Towards real-time, large-scale, and near-optimal multi-agent pathfinding},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663010},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RAISE the bar: Restriction of action spaces for improved
social welfare and equity in traffic management. <em>AAMAS</em>,
1492–1500. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Restriction-based governance has recently been proposed as an alternative to reward shaping for achieving system-level goals in competitive multi-agent systems. In this work, we apply these two approaches to the domain of traffic management, specifically investigating their efficacy and fairness. Our results show that edge restrictions in congested traffic networks are superior to dynamic pricing with regard to equity (i.e., equal treatment of agents) while achieving comparable travel-time improvements. We argue that the former metric, as an adequate proxy for fairness, can be crucial for the quality and acceptance of a governance scheme, particularly when human agents are affected.},
  archive   = {C_AAMAS},
  author    = {Oesterle, Michael and Grams, Tim and Bartelt, Christian and Stuckenschmidt, Heiner},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1492–1500},
  title     = {RAISE the bar: Restriction of action spaces for improved social welfare and equity in traffic management},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663009},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforcement learning interventions on boundedly rational
human agents in frictionful tasks. <em>AAMAS</em>, 1482–1491. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many important behavior changes are frictionful; they require individuals to expend effort over a long period with little immediate gratification. Here, an artificial intelligence (AI) agent can provide personalized interventions to help individuals stick to their goals. In these settings, the AI agent must personalize rapidly (before the individual disengages) and interpretably, to help us understand the behavioral interventions. In this paper, we introduce Behavior Model Reinforcement Learning (BMRL), a framework in which an AI agent intervenes on the parameters of a Markov Decision Process (MDP) belonging to a boundedly rational human agent. Our formulation of the human decision-maker as a planning agent allows us to attribute undesirable human policies (ones that do not lead to the goal) to their maladapted MDP parameters, such as an extremely low discount factor. Furthermore, we propose a class of tractable human models that captures fundamental behaviors in frictionful tasks. Introducing a notion of MDP equivalence specific to BMRL, we theoretically and empirically show that AI planning with our human models can lead to helpful policies on a wide range of more complex, ground-truth humans.},
  archive   = {C_AAMAS},
  author    = {Nofshin, Eura and Swaroop, Siddharth and Pan, Weiwei and Murphy, Susan and Doshi-Velez, Finale},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1482–1491},
  title     = {Reinforcement learning interventions on boundedly rational human agents in frictionful tasks},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663008},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Solution-oriented agent-based models generation with
verifier-assisted iterative in-context learning. <em>AAMAS</em>,
1473–1481. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Agent-based models (ABMs) stand as an essential paradigm for proposing and validating hypothetical solutions or policies aimed at addressing challenges posed by complex systems and achieving various objectives. This process demands labor-intensive endeavors and multidisciplinary expertise. Large language models (LLMs) encapsulating cross-domain knowledge and programming proficiency could potentially alleviate the difficulty of this process. However, LLMs excel in handling sequential information, making it challenging for analyzing the intricate interactions and nonlinear dynamics inherent in ABMs. Additionally, due to the lack of self-evaluation capability of LLMs, relying solely on LLMs is insufficient to effectively accomplish this process. In this paper, we present SAGE, a general solution-oriented ABM generation framework designed for automatic modeling and generating solutions for targeted problems. Unlike approaches reliant on expert handcrafting or resource-intensive neural network training, SAGE establishes a verifier-assisted iterative in-context learning process employing large language models (LLMs) to leverages their inherent cross-domain knowledge for tackling intricate demands from diverse domain scenarios. In SAGE, we introduce an semi-structured conceptual representation expliciting the intricate structures of ABMs and an objective representation to guide LLMs in modeling scenarios and proposing hypothetical solutions through in-context learning. To ensure the model executability and solution feasibility, SAGE devises a two-level verifier with chain-of-thought prompting tailored to the complex interactions and non-linear dynamics of ABMs, driving the iterative generation optimization. Moreover, we construct an evaluation dataset of solution-oriented ABMs from open sources. It contains practical models across various domains, completed with scenario descriptions and executable agent-based solutions. Evaluations by various LLMs demonstrate that SAGE leads to an average improvement of 18.7\% in modeling quality and 38.1\% in solution generation effectiveness. This work advances our understanding and ability in tackling complex real-world challenges across diverse domains through the application of ABM methodologies.},
  archive   = {C_AAMAS},
  author    = {Niu, Tong and Zhang, Weihao and Zhao, Rong},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1473–1481},
  title     = {Solution-oriented agent-based models generation with verifier-assisted iterative in-context learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663007},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bootstrapping linear models for fast online adaptation in
human-agent collaboration. <em>AAMAS</em>, 1463–1472. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Agents that assist people need to have well-initialized policies that can adapt quickly to align with their partners&#39; reward functions. Initializing policies to maximize performance with unknown partners can be achieved by bootstrapping nonlinear models using imitation learning over large, offline datasets. Such policies can require prohibitive computation to fine-tune in-situ and therefore may miss critical run-time information about a partner&#39;s reward function as expressed through their immediate behavior. In contrast, online logistic regression using low-capacity models performs rapid inference and fine-tuning updates and thus can make effective use of immediate in-task behavior for reward function alignment. However, these low-capacity models cannot be bootstrapped as effectively by offline datasets and thus have poor initializations. We propose BLR-HAC, Bootstrapped Logistic Regression for Human Agent Collaboration, which bootstraps large nonlinear models to learn the parameters of a low-capacity model which then uses online logistic regression for updates during collaboration. We test BLR-HAC in a simulated surface rearrangement task and demonstrate that it achieves higher zero-shot accuracy than shallow methods and takes far less computation to adapt online while still achieving similar performance to fine-tuned, large nonlinear models. For code, please see our project page https://sites.google.com/view/blr-hac.},
  archive   = {C_AAMAS},
  author    = {Newman, Benjamin A. and Paxton, Chris and Kitani, Kris and Admoni, Henny},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1463–1472},
  title     = {Bootstrapping linear models for fast online adaptation in human-agent collaboration},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663006},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mixed-initiative human-robot teaming under suboptimality
with online bayesian adaptation. <em>AAMAS</em>, 1454–1462. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For effective human-agent teaming, robots and other artificial intelligence (AI) agents must infer their human partner&#39;s abilities and behavioral response patterns and adapt accordingly. Most prior works make the unrealistic assumption that one or more teammates can act near-optimally. In real-world collaboration, humans and autonomous agents can be suboptimal, especially when each only has partial domain knowledge. In this work, we develop computational modeling and optimization techniques for enhancing the performance of human-agent teams, where both the human and the robotic agent have asymmetric capabilities and act suboptimally due to incomplete environmental knowledge. We adopt an online Bayesian approach that enables a robot to infer people&#39;s willingness to comply with its assistance in a sequential decision-making game. Our user studies show that user preferences and team performance vary with robot intervention styles, and our approach for mixed-initiative collaboration enhances objective team performance (p&amp;lt;.001) and subjective measures, such as user&#39;s trust (p&amp;lt;.001) and perceived likeability of the robot (p&amp;lt;.001).},
  archive   = {C_AAMAS},
  author    = {Natarajan, Manisha and Xue, Chunyue and van Waveren, Sanne and Feigh, Karen and Gombolay, Matthew},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1454–1462},
  title     = {Mixed-initiative human-robot teaming under suboptimality with online bayesian adaptation},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663005},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rethinking out-of-distribution detection for reinforcement
learning: Advancing methods for evaluation and detection.
<em>AAMAS</em>, 1445–1453. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While reinforcement learning (RL) algorithms have been successfully applied across numerous sequential decision-making problems, their generalization to unforeseen testing environments remains a significant concern. In this paper, we study the problem of out-of-distribution (OOD) detection in RL, which focuses on identifying situations at test time that RL agents have not encountered in their training environments. We first propose a clarification of terminology for OOD detection in RL, which aligns it with the literature from other machine learning domains. We then present new benchmark scenarios for OOD detection, which introduce anomalies with temporal autocorrelation into different components of the agent-environment loop. We argue that such scenarios have been understudied in the current literature, despite their relevance to real-world situations. Confirming our theoretical predictions, our experimental results suggest that state-of-the-art OOD detectors are not able to identify such anomalies. To address this problem, we propose a novel method for OOD detection, which we call DEXTER (Detection via Extraction of Time Series Representations). By treating environment observations as time series data, DEXTER extracts salient time series features, and then leverages an ensemble of isolation forest algorithms to detect anomalies. We find that DEXTER can reliably identify anomalies across benchmark scenarios, exhibiting superior performance compared to both state-of-the-art OOD detectors and high-dimensional changepoint detectors adopted from statistics.},
  archive   = {C_AAMAS},
  author    = {Nasvytis, Linas and Sandbrink, Kai and Foerster, Jakob and Franzmeyer, Tim and Schroeder de Witt, Christian},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1445–1453},
  title     = {Rethinking out-of-distribution detection for reinforcement learning: Advancing methods for evaluation and detection},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663004},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Linking vision and multi-agent communication through visible
light communication using event cameras. <em>AAMAS</em>, 1436–1444. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Various robots, rovers, drones, and other agents of mass-produced products are expected to encounter scenes where they intersect and collaborate in the near future. In such multi-agent systems, individual identification and communication play crucial roles. In this paper, we explore camera-based visible light communication using event cameras to tackle this problem. An event camera captures the events occurring in regions with changes in brightness and can be utilized as a receiver for visible light communication, leveraging its high temporal resolution. Generally, agents with identical appearances in mass-produced products are visually indistinguishable when using conventional CMOS cameras. Therefore, linking visual information with information acquired through conventional radio communication is challenging. We empirically demonstrate the advantages of a visible light communication system employing event cameras and LEDs for visual individual identification over conventional CMOS cameras with ArUco marker recognition. In the simulation, we also verified scenarios where our event camera-based visible light communication outperforms conventional radio communication in situations with visually indistinguishable multi-agents. Finally, our newly implemented multi-agent system verifies its functionality through physical robot experiments.},
  archive   = {C_AAMAS},
  author    = {Nakagawa, Haruyuki and Miyatani, Yoshitaka and Kanezaki, Asako},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1436–1444},
  title     = {Linking vision and multi-agent communication through visible light communication using event cameras},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663003},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Policy optimization using horizon regularized advantage to
improve generalization in reinforcement learning. <em>AAMAS</em>,
1427–1435. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we focus on improving the generalization performance of a reinforcement learning (RL) agent in diverse environments. We observe that in environments created under the Contextual Markov Decision Process (CMDP), where an environment&#39;s dynamics and attribute distribution change across contexts, the generated episodes are highly stochastic and unpredictable. To improve generalization in such scenarios, we present Horizon Regularized Advantage (HRA) estimation that enables robustness to the underlying uncertainty of episode duration. Using three challenging RL generalization benchmarks Procgen, Crafter, and Minigrid we demonstrate that our proposed approach outperforms the Proximal Policy Optimization (PPO) baseline that uses classical single exponential discounting-based advantage estimate. We also incorporate HRA into another generalization-specific approach (APDAC), and the results indicate further improvement in APDAC&#39;s generalization ability. This denotes the effectiveness of our approach as a generic component that can be incorporated into any policy gradient method to aid generalization.},
  archive   = {C_AAMAS},
  author    = {Nafi, Nasik Muhammad and Ali, Raja Farrukh and Hsu, William and Duong, Kevin and Vick, Mason},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1427–1435},
  title     = {Policy optimization using horizon regularized advantage to improve generalization in reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663002},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PI-NeuGODE: Physics-informed graph neural ordinary
differential equations for spatiotemporal trajectory prediction.
<em>AAMAS</em>, 1418–1426. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It is challenging to predict a group of individuals&#39; spatiotemporal trajectories in continuous time and space, due to various environmental and intrinsic factors. Especially, social dynamics such as driving or crowding behaviors could be hard to predict due to heterogeneous and complex mapping from high-dimensional inputs to an output driven by the decision-making processes of other agents. To tackle this challenge, neural ordinary differential equations (neural ODEs) have been developed to predict continuous-time long-term dynamics with constant memory cost and high computational efficiency. Furthermore, scientific communities have developed a rich set of physics models to describe how individuals interactively make decisions. With a rapidly growing trend of employing physics-informed deep learning (PIDL) for dynamical systems in science and engineering, its application to social dynamics is understudied. This paper aims to develop an integrated framework, named &quot;PI-NeuGODE,&quot; that encodes physics models, complemented by symbolic regression, into neural ODEs. In the proposed model, physics informs the training of neural ODEs, while neural ODEs guide knowledge discovery. Symbolic regression is used to uncover physics knowledge from complex data. We further use graph neural networks to learn the topological interaction of individuals. The proposed method is tested on two applications, human driving and platooning, as well as crowding, which demonstrate the algorithmic accuracy and efficiency against baselines including existing social deep learning models.},
  archive   = {C_AAMAS},
  author    = {Mo, Zhaobin and Fu, Yongjie and Di, Xuan},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1418–1426},
  title     = {PI-NeuGODE: Physics-informed graph neural ordinary differential equations for spatiotemporal trajectory prediction},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663001},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Observer-aware planning with implicit and explicit
communication. <em>AAMAS</em>, 1409–1417. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3663000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a computational model designed for planning both implicit and explicit communication of intentions, goals, and desires. Building upon previous research focused on implicit communication of intention via actions, our model seeks to strategically influence an observer&#39;s belief using both the agent&#39;s actions and explicit messages. We show that our proposed model can be considered to be a special case of general multi-agent problems with explicit communication under certain assumptions. Since the mental state of the observer depends on histories, computing a policy for the proposed model amounts to optimizing a non-Markovian objective, which we show to be intractable in the worst case. To mitigate this challenge, we propose a technique based on splitting domain and communication actions during planning. We conclude with experimental evaluations of the proposed approach that illustrate its effectiveness},
  archive   = {C_AAMAS},
  author    = {Miura, Shuwa and Zilberstein, Shlomo},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1409–1417},
  title     = {Observer-aware planning with implicit and explicit communication},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3663000},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating district-based election surveys with synthetic
dirichlet likelihood. <em>AAMAS</em>, 1400–1408. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In district-based multi-party elections, electors cast votes in their respective districts. In each district, the party with maximum votes wins the corresponding &quot;seat&quot; in the governing body. Election Surveys try to predict the election outcome (vote shares and seat shares of parties) by querying a random sample of electors. However, the survey results are often inconsistent with the actual results, which could be due to multiple reasons. The aim of this work is to estimate a posterior distribution over the possible outcomes of the election, given one or more survey results. This is achieved using a prior distribution over vote shares, election models to simulate the complete election from the vote share, and survey models to simulate survey results from a complete election. The desired posterior distribution over the space of possible outcomes is constructed using Synthetic Dirichlet Likelihoods, whose parameters are estimated from Monte Carlo sampling of elections using the election models. We further show the same approach can also use be used to evaluate the surveys - whether they were biased or not, based on the true outcome once it is known. Our work offers the first-ever probabilistic model to analyze district-based election surveys. We illustrate our approach with extensive experiments on real and simulated data of district-based political elections in India.},
  archive   = {C_AAMAS},
  author    = {Mitra, Adway and Dey, Palash},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1400–1408},
  title     = {Evaluating district-based election surveys with synthetic dirichlet likelihood},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662999},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TaxAI: A dynamic economic simulator and benchmark for
multi-agent reinforcement learning. <em>AAMAS</em>, 1390–1399. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Taxation and government spending are crucial tools for governments to promote economic growth and maintain social equity. However, the difficulty in accurately predicting the dynamic strategies of diverse self-interested households presents a challenge for governments to implement effective tax policies. Given its proficiency in modeling other agents in partially observable environments and adaptively learning to find optimal policies, Multi-Agent Reinforcement Learning (MARL) is highly suitable for solving dynamic games between the government and numerous households. Although MARL shows more potential than traditional methods such as the genetic algorithm and dynamic programming, there is a lack of large-scale multi-agent reinforcement learning economic simulators. Therefore, we propose a MARL environment, named TaxAI, for dynamic games involving N households, government, firms, and financial intermediaries based on the Bewley-Aiyagari economic model. Our study benchmarks 2 traditional economic methods with 7 MARL methods on TaxAI, demonstrating the effectiveness and superiority of MARL algorithms. Moreover, TaxAI&#39;s scalability in simulating dynamic interactions between the government and 10,000 households, coupled with real-data calibration, grants it a substantial improvement in scale and reality over existing simulators. Therefore, TaxAI is the most realistic economic simulator for optimal tax policy, which aims to generate feasible recommendations for governments and individuals.},
  archive   = {C_AAMAS},
  author    = {Mi, Qirui and Xia, Siyu and Song, Yan and Zhang, Haifeng and Zhu, Shenghao and Wang, Jun},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1390–1399},
  title     = {TaxAI: A dynamic economic simulator and benchmark for multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662998},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Containing the spread of a contagion on a tree.
<em>AAMAS</em>, 1381–1389. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Contact tracing can be thought of as a race between two processes: an infection process and a tracing process. In this paper, we study a simple model of infection spreading on a tree, and a tracer who stabilizes one node at a time. We focus on the question, how should the tracer choose nodes to stabilize so as to prevent the infection from spreading further? We study simple policies, which prioritize nodes based on time, infectiousness, or probability of generating new contacts.},
  archive   = {C_AAMAS},
  author    = {Meister, Michela and Kleinberg, Jon},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1381–1389},
  title     = {Containing the spread of a contagion on a tree},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662997},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Network agency: An agent-based model of forced migration
from ukraine. <em>AAMAS</em>, 1372–1380. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Individuals in social systems are embedded in collective decision-making hierarchies, such as households, neighborhoods, communities, organizations, etc. The locus of agency in such systems is dispersed across the system, and can variously be viewed as individual, distributed, and shared agency. Here we propose a general notion of network agency that subsumes these descriptions and also allows for integrating related notions, such as peer influence. In our view, the social system can be seen as a multi-layer network, where each layer corresponds to different aggregations of the underlying units, representing different kinds of perception and decision-making. We illustrate this general framework with an agent-based model of the ongoing forced migration from Ukraine. In our model, individuals perceive hazards (conflict events), but decisions to migrate are taken at the household level, where peer influence from other households in the neighborhood is also taken into account. We present this model in detail to elucidate our concept of network agency. We also calibrate the model with data on daily refugee flows and show that our model is able to estimate the scale of the daily refugee flow from Ukraine for the first two months with a Root Mean Squared Percentage Error (RMSPE) of 0.24, outperforming state-of-the-art, which had an RMSPE of 0.77. Moreover, our model also captures the daily trend of outflow with a Pearson Correlation Coefficient (PCC) of 0.98. We also perform sensitivity analysis of the model and analyze the significant parameters of the model, which in turn tells us how different agencies are significant in different contexts.},
  archive   = {C_AAMAS},
  author    = {Mehrab, Zakaria and Stundal, Logan and Swarup, Samarth and Venaktramanan, Srinivasan and Lewis, Bryan and Mortveit, Henning and Barrett, Christopher and Pandey, Abhishek and Wells, Chad and Galvani, Alison and Singer, Burton and Leblang, David and Colwell, Rita and Marathe, Madhav},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1372–1380},
  title     = {Network agency: An agent-based model of forced migration from ukraine},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662996},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PDiT: Interleaving perception and decision-making
transformers for deep reinforcement learning. <em>AAMAS</em>, 1363–1371.
(<a href="https://dl.acm.org/doi/10.5555/3635637.3662995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Designing better deep networks and better reinforcement learning (RL) algorithms are both important for deep RL. This work studies the former. Specifically, the Perception and Decision-making Interleaving Transformer (PDiT) network is proposed, which cascades two Transformers in a very natural way: the perceiving one focuses on the environmental perception by processing the observation at the patch level, whereas the deciding one pays attention to the decision-making by conditioning on the history of the desired returns, the perceiver&#39;s outputs, and the actions. Such a network design is generally applicable to a lot of deep RL settings, e.g., both the online and offline RL algorithms under environments with either image observations, proprioception observations, or hybrid image-language observations. Extensive experiments show that PDiT can not only achieve superior performance than strong baselines in different settings but also extract explainable feature representations. Our code is available at https://github.com/maohangyu/PDiT.},
  archive   = {C_AAMAS},
  author    = {Mao, Hangyu and Zhao, Rui and Li, Ziyue and Xu, Zhiwei and Chen, Hao and Chen, Yiqun and Zhang, Bin and Xiao, Zhen and Zhang, Junge and Yin, Jiangjin},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1363–1371},
  title     = {PDiT: Interleaving perception and decision-making transformers for deep reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662995},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian behavioural model estimation for live crowd
simulation. <em>AAMAS</em>, 1355–1362. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The increasing availability of real-time crowd observation data enables the development of agent-based crowd simulations that incorporate real-time data feeds, i.e. live crowd simulations , which achieve accurate crowd forecasting for real-time interventions for improved and safer mobility. Various approaches for live crowd simulations have recently been proposed. However, existing methods cannot offer long forecasting lead times, which are crucial for planning and implementing timely interventions. To address this issue, we develop a Bayesian behavioural model estimation for live crowd simulations that sequentially estimates the underlying behavioural model assumed behind the observed crowd flows. In real crowds, although apparent behaviours change over time, an invariable rule that determines behaviour often exists behind the crowd. The developed method estimates the underlying invariable behavioural model and provides reliable long-term crowd flow forecasting. The experimental results show that the developed method can accurately forecast long-term crowd flows using aggregate observations, whereas the state-of-the-art forecasting method fails to provide reliable forecasting when the apparent behavioural tendency changes. We also demonstrate that the developed method can provide forecasting results that are sufficient to consider interventions, even in the presence of a data-model mismatch.},
  archive   = {C_AAMAS},
  author    = {Makinoshima, Fumiyasu and Takahashi, Tetsuro and Oishi, Yusuke},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1355–1362},
  title     = {Bayesian behavioural model estimation for live crowd simulation},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662994},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explaining the behavior of POMDP-based agents through the
impact of counterfactual information. <em>AAMAS</em>, 1346–1354. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we consider AI agents operating in Partially Observable Markov Decision Processes (POMDPs)-a widely-used framework for sequential decision making with incomplete state information. Agents operating with partial information take actions not only to advance their underlying goals but also to seek information and reduce uncertainty. Despite rapid progress in explainable AI, research on separating information-driven vs. goal-driven behaviors remains sparse. To address this gap, we introduce a novel explanation generation framework called Sequential Information Probing (SIP), to investigate the direct impact of state information, or its absence, on agent behavior. To quantify the impact we also propose two metrics under this SIP framework called Value of Information (VoI) and Influence of Information (IoI). We then theoretically derive several properties of these metrics. Finally, we present several experiments, including a case study on an autonomous vehicle, that illustrate the efficacy of our method.},
  archive   = {C_AAMAS},
  author    = {Mahmud, Saaduddin and Vazquez-Chanlatte, Marcell and Witwicki, Stefan and Zilberstein, Shlomo},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1346–1354},
  title     = {Explaining the behavior of POMDP-based agents through the impact of counterfactual information},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662993},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attacking multi-player bandits and how to robustify them.
<em>AAMAS</em>, 1337–1345. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motivated by cognitive radios, stochastic Multi-Player Multi-Armed Bandits has been extensively studied in recent years. In this setting, each player pulls an arm, and receives a reward corresponding to the arm if there is no collision, namely the arm was selected by one single player. Otherwise, the player receives no reward if collision occurs. In this paper, we consider the presence of malicious players (or attackers) who obstruct the cooperative players (or defenders) from maximizing their rewards, by deliberately colliding with them. We provide the first decentralized and robust algorithm RESYNC for defenders whose performance deteriorates gracefully as O(C) as the number of collisions C from the attackers increases. We show that this algorithm is order-optimal by proving a lower bound which scales as Omega(C). This algorithm is agnostic to the algorithm used by the attackers and agnostic to the number of collisions C faced from attackers.},
  archive   = {C_AAMAS},
  author    = {Mahesh, Shivakumar and Rangi, Anshuka and Xu, Haifeng and Tran-Thanh, Long},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1337–1345},
  title     = {Attacking multi-player bandits and how to robustify them},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662992},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mixed-initiative bayesian sub-goal optimization in
hierarchical reinforcement learning. <em>AAMAS</em>, 1328–1336. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In hierarchical reinforcement learning, human expertise is often involved in defining sub-goals that decompose the final objective into relevant sub-tasks. However, existing approaches with human-defined sub-goals often lack crucial information about their correlations, limiting their applicability in environments with multiple parallel tasks or mutually conflicting solutions. To address this issue, we propose a mixed-initiative Bayesian sub-goal optimization algorithm that combines human expertise with AI automated reasoning to identify reasonable sub-goals. Our algorithm employs a probabilistic graphical model to capture the correlations among the candidate sub-goals and refine the encoded knowledge to reduce the introduced biases. We conduct experiments in high-dimensional environments with both discrete and continuous controls. In comparison with relevant baselines, our algorithm can achieve better performance in effectively solving problems with multiple selectable solutions. We have empirically demonstrated that our approach is robust against varying levels of human knowledge and expertise, consistently converging to optimal hierarchical policies even amidst misleading or conflicting human guidance.},
  archive   = {C_AAMAS},
  author    = {Ma, Haozhe and Vo, Thanh Vinh and Leong, Tze-Yun},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1328–1336},
  title     = {Mixed-initiative bayesian sub-goal optimization in hierarchical reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662991},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Covert planning against imperfect observers. <em>AAMAS</em>,
1319–1327. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Covert planning refers to a class of constrained planning problems where an agent aims to accomplish a task with minimal information leaked to a passive observer to avoid detection. However, existing methods of covert planning often consider deterministic environments or do not exploit the observer&#39;s imperfect information. This paper studies how covert planning can leverage the coupling of stochastic dynamics and the observer&#39;s imperfect observation to achieve optimal task performance without being detected. Specifically, we employ a Markov decision process to model the interaction between the agent and its stochastic environment, and a partial observation function to capture the leaked information to a passive observer. Assuming the observer employs hypothesis testing to detect if the observation deviates from a nominal policy, the covert planning agent aims to maximize the total discounted reward while keeping the probability of being detected as an adversary below a given threshold. We prove that finite-memory policies are more powerful than Markovian policies in covert planning. Then, we develop a primal-dual proximal policy gradient method with a two-time-scale update to compute a (locally) optimal covert policy. We demonstrate the effectiveness of our methods using a stochastic gridworld example. Our experimental results illustrate that the proposed method computes a policy that maximizes the adversary&#39;s expected reward without violating the detection constraint, and empirically demonstrates how the environmental noises can influence the performance of the covert policies.},
  archive   = {C_AAMAS},
  author    = {Ma, Haoxiang and Shi, Chongyang and Han, Shuo and Dorothy, Michael R. and Fu, Jie},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1319–1327},
  title     = {Covert planning against imperfect observers},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662990},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Oh, now i see what you want: Learning agent models with
internal states from observations. <em>AAMAS</em>, 1310–1318. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning behavior models of other agents from observations is challenging because agents typically do not act based on observable states alone, but usually take their internal, for external agents unobservable, states such as desires, motivations, preferences, and others into account. Consequently, methods that only use observational states for modeling other agents&#39; behaviors are insufficient for capturing and predicting agent behavior, especially for agents with rich internal processes. We propose a novel approach to online agent model learning that works incrementally with limited data, provides fine-grained and interpretable descriptions of the agent&#39;s behavior, and, most importantly, is able to hypothesize agent-internal states to better explain observed behavioral trajectories. We show in various proof-of-concept experiments that our method avoids the pitfalls of common agent-modeling strategies when agent-internal states govern behavior and is able to build accurate and interpretable behavior models. We also discuss how the method can work in conjunction with existing approaches (e.g., for goal recognition) to facilitate better modeling of open-world agents.},
  archive   = {C_AAMAS},
  author    = {Lymperopoulos, Panagiotis and Scheutz, Matthias},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1310–1318},
  title     = {Oh, now i see what you want: Learning agent models with internal states from observations},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662989},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Act as you learn: Adaptive decision-making in non-stationary
markov decision processes. <em>AAMAS</em>, 1301–1309. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A fundamental challenge in sequential decision-making is dealing with non-stationary environments, where exogenous environmental conditions change over time. Such problems are traditionally modeled as non-stationary Markov decision processes (NS-MDP). However, existing approaches for decision-making in NS-MDPs have two major shortcomings: first, they assume that the updated environmental dynamics at the current time are known (although future dynamics can change); and second, planning is largely pessimistic, i.e., the agent acts &quot;safely&#39;&#39; to account for the non-stationary evolution of the environment. We argue that both these assumptions are invalid in practice-updated environmental conditions are rarely known, and as the agent interacts with the environment, it can learn about the updated dynamics and avoid being pessimistic, at least in states whose dynamics it is confident about. We present a heuristic search algorithm called Adaptive Monte Carlo Tree Search (ADA-MCTS) that addresses these challenges. We show that the agent can learn the updated dynamics of the environment over time and then act as it learns, i.e., if the agent is in a region of the state space about which it has updated knowledge, it can avoid being pessimistic. To quantify &quot;updated knowledge,&#39;&#39; we disintegrate the aleatoric and epistemic uncertainty in the agent&#39;s updated belief and show how the agent can use these estimates for decision-making. We compare the proposed approach with multiple state-of-the-art approaches in decision-making across multiple well-established open-source problems and empirically show that our approach is faster and more adaptive without sacrificing safety.},
  archive   = {C_AAMAS},
  author    = {Luo, Baiting and Zhang, Yunuo and Dubey, Abhishek and Mukhopadhyay, Ayan},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1301–1309},
  title     = {Act as you learn: Adaptive decision-making in non-stationary markov decision processes},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662988},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A task-driven multi-UAV coalition formation mechanism.
<em>AAMAS</em>, 1292–1300. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the rapid advancement of UAV technology, the problem of UAV coalition formation has become a hotspot. Therefore, designing task-driven multi-UAV coalition formation mechanism has become a challenging problem. However, existing coalition formation mechanisms suffer from low relevance between UAVs and task requirements, resulting in overall low coalition utility and unstable coalition structures. To address these problems, this paper proposed a novel multi-UAV coalition network collaborative task completion model, considering both coalition work capacity and task-requirement relationships. This model stimulated the formation of coalitions that match task requirements by using a revenue function based on the coalition&#39;s revenue threshold. Subsequently, an algorithm for coalition formation based on marginal utility was proposed. Specifically, the algorithm utilized Shapley value to achieve fair utility distribution within the coalition, evaluated coalition values based on marginal utility preference order, and achieved stable coalition partition through a limited number of iterations. Additionally, we theoretically proved that this algorithm has Nash equilibrium solution. Finally, experimental results demonstrated that the proposed algorithm, compared to currently classical algorithms, not only forms more stable coalitions but also further enhances the overall utility of coalitions effectively.},
  archive   = {C_AAMAS},
  author    = {Lu, Xinpeng and Song, Heng and Ma, Huailing and Zhu, Junwu},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1292–1300},
  title     = {A task-driven multi-UAV coalition formation mechanism},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662987},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DuaLight: Enhancing traffic signal control by leveraging
scenario-specific and scenario-shared knowledge. <em>AAMAS</em>,
1283–1291. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning has been revolutionizing the traditional traffic signal control task, showing promising power to relieve congestion and improve efficiency. However, the existing methods lack effective learning mechanisms capable of absorbing dynamic information inherent to a specific scenario and universally applicable dynamic information across various scenarios. Moreover, within each specific scenario, they fail to fully capture the essential empirical experiences about how to coordinate between neighboring and target intersections, leading to sub-optimal system-wide outcomes.Viewing these issues, we propose DuaLight, which aims to leverage both the experiential information within a single scenario and the generalizable information across various scenarios for enhanced decision-making. Specifically, DuaLight introduces a scenario-specific experiential weight module with two learnable parts: Intersection-wise and Feature-wise, guiding how to adaptively utilize neighbors and input features for each scenario, thus providing a more fine-grained understanding of different intersections. Furthermore, we implement a scenario-shared Co-Train module to facilitate the learning of generalizable dynamics information across different scenarios. Empirical results on both real-world and synthetic scenarios show DuaLight achieves competitive performance across various metrics, offering a promising solution to alleviate traffic congestion, with 3-7\% improvements. The code is available under https://github.com/lujiaming-12138/DuaLight.},
  archive   = {C_AAMAS},
  author    = {Lu, Jiaming and Ruan, Jingqing and Jiang, Haoyuan and Li, Ziyue and Mao, Hangyu and Zhao, Rui},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1283–1291},
  title     = {DuaLight: Enhancing traffic signal control by leveraging scenario-specific and scenario-shared knowledge},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662986},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Safe reinforcement learning with free-form natural language
constraints and pre-trained language models. <em>AAMAS</em>, 1274–1282.
(<a href="https://dl.acm.org/doi/10.5555/3635637.3662985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safe reinforcement learning (RL) agents accomplish given tasks while adhering to specific constraints. Employing constraints expressed via easily-understandable human language offers considerable potential for real-world applications due to its accessibility and non-reliance on domain expertise. Previous safe RL methods with natural language constraints typically adopt a recurrent neural network, which leads to limited capabilities when dealing with various forms of human language input. Furthermore, these methods often require a ground-truth cost function, necessitating domain expertise for the conversion of language constraints into a well-defined cost function that determines constraint violation. To address these issues, we proposes to use pre-trained language models (LM) to facilitate RL agents&#39; comprehension of natural language constraints and allow them to infer costs for safe policy learning. Through the use of pre-trained LMs and the elimination of the need for a ground-truth cost, our method enhances safe policy learning under a diverse set of human-derived free-form natural language constraints. Experiments on grid-world navigation and robot control show that the proposed method can achieve strong performance while adhering to given constraints. The usage of pre-trained LMs allows our method to comprehend complicated constraints and learn safe policies without the need for ground-truth cost at any stage of training or evaluation. Extensive ablation studies are conducted to demonstrate the efficacy of each part of our method.},
  archive   = {C_AAMAS},
  author    = {Lou, Xingzhou and Zhang, Junge and Wang, Ziyan and Huang, Kaiqi and Du, Yali},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1274–1282},
  title     = {Safe reinforcement learning with free-form natural language constraints and pre-trained language models},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662985},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uncoupled learning of differential stackelberg equilibria
with commitments. <em>AAMAS</em>, 1265–1273. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In multi-agent problems requiring a high degree of cooperation, success often depends on the ability of the agents to adapt to each other&#39;s behavior. A natural solution concept in such settings is the Stackelberg equilibrium, in which the &quot;leader&quot; agent selects the strategy that maximizes its own payoff given that the &quot;follower&quot; agent will choose their best response to this strategy. Recent work has extended this solution concept to two-player differentiable games, such as those arising from multi-agent deep reinforcement learning, in the form of the differential Stackelberg equilibrium. While this previous work has presented learning dynamics which converge to such equilibria, these dynamics are &quot;coupled&quot; in the sense that the learning updates for the leader&#39;s strategy require some information about the follower&#39;s payoff function. As such, these methods cannot be applied to truly decentralised multi-agent settings, particularly ad hoc cooperation, where each agent only has access to its own payoff function. In this work we present &quot;uncoupled&quot; learning dynamics based on zeroth-order gradient estimators, in which each agent&#39;s strategy update depends only on their observations of the other&#39;s behavior. We analyze the convergence of these dynamics in general-sum games, and prove that they converge to differential Stackelberg equilibria under the same conditions as previous coupled methods. Furthermore, we present an online mechanism by which symmetric learners can negotiate leader-follower roles. We conclude with a discussion of the implications of our work for multi-agent reinforcement learning and ad hoc collaboration more generally.},
  archive   = {C_AAMAS},
  author    = {Loftin, Robert and \c{C}elikok, Mustafa Mert and van Hoof, Herke and Kaski, Samuel and Oliehoek, Frans A.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1265–1273},
  title     = {Uncoupled learning of differential stackelberg equilibria with commitments},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662984},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GraphSAID: Graph sampling via attention based integer
programming method. <em>AAMAS</em>, 1256–1264. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graphs are extensively employed in data mining and machine learning owing to their remarkable ability to model real-world objects and their relationships. However, as graphs scale up, they present several challenges. To tackle these issues, graph sampling methods have gained popularity by selecting a representative subgraph within a given budget. However, most graph sampling approaches rely on graph-structure information and cannot simultaneously consider node feature interaction and selection bias to perform graph sampling. Given the recent success of the attention mechanism in model training, it is worth investigating its potential to enhance graph sampling methods and overcome their challenges. The primary objective of this work is to establish a novel connection between the learned attention and the graph sampling problem using the Integer Programming method. To accomplish this, we propose a novel solution, GraphSAID, which utilizes an attention learning stage to generate initial node-level attention, followed by an aggregation stage to compute connected component scores that are independent of the budget. Finally, the Integer Programming method is employed to optimize an objective function that considers both the budget value and the user-defined selection bias. Empirical results on 1 synthesized and 3 real-world graph datasets demonstrate its superior performance. Additionally, we showcase the ease with which selection bias (user control) can be incorporated into GraphSAID to further improve performance.},
  archive   = {C_AAMAS},
  author    = {Liu, Ziqi and Liu, Laurence},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1256–1264},
  title     = {GraphSAID: Graph sampling via attention based integer programming method},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662983},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural population learning beyond symmetric zero-sum games.
<em>AAMAS</em>, 1247–1255. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study computationally efficient methods for finding equilibria in n-player general-sum games, specifically ones that afford complex visuomotor skills. We show how existing methods would struggle in this setting, either computationally or in theory. We then introduce NeuPL-JPSRO, a neural population learning algorithm that benefits from transfer learning of skills and converges to a Coarse Correlated Equilibrium (CCE) of the game. We show empirical convergence in a suite of OpenSpiel games, validated rigorously by exact game solvers. We then deploy NeuPL-JPSRO to complex domains, where our approach enables adaptive coordination in a MuJoCo control domain and skill transfer in capture-the-flag. Our work shows that equilibrium convergent population learning can be implemented at scale and in generality, paving the way towards solving real-world games between heterogeneous players with mixed motives.},
  archive   = {C_AAMAS},
  author    = {Liu, Siqi and Marris, Luke and Lanctot, Marc and Piliouras, Georgios and Leibo, Joel Z and Heess, Nicolas},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1247–1255},
  title     = {Neural population learning beyond symmetric zero-sum games},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662982},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 2D-ptr: 2D array pointer network for solving the
heterogeneous capacitated vehicle routing problem. <em>AAMAS</em>,
1238–1246. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The heterogeneous capacitated vehicle routing problem (HCVRP) aims to optimize the routes of heterogeneous vehicles with capacity constraints to serve a set of customers with demands. Existing learning-based methods for solving HCVRP have the problem of weak generalization ability, which means that well-trained model cannot adapt well to new scenarios with different vehicle or customer numbers. To address this issue, by modeling the simultaneous decision-making of multiple agents as a sequence of consecutive actions in real time, we propose a pointer network extension model, which includes a static encoder and a dynamic encoder to map the current situation to node embeddings and vehicle embeddings, respectively. For each element in the consecutive actions sequence, the decoder of our model uses the probability distribution obtained from node embeddings and vehicle embeddings as a 2D array pointer to select a tuple from the combinations of vehicles and nodes (customers and depot). We call this architecture a 2D Array Pointer network (2D-Ptr). Instead of planning paths based on the priority order of vehicles, 2D-Ptr plans paths based on the priority order of actions. In addition, 2D-Ptr consists of a series of carefully designed attention modules, entitling the model to be generalizable in the scenarios where additional vehicles (or customers) are introduced or existing vehicles (or customers) are removed. We empirically test 2D-Ptr and show its capability for producing near-optimal solutions through cooperative actions. 2D-Ptr delivers competitive performance against the state-of-the-art baselines, and can solve arbitrary instances of the HCVRP without requiring re-training.},
  archive   = {C_AAMAS},
  author    = {Liu, Qidong and Liu, Chaoyue and Niu, Shaoyao and Long, Cheng and Zhang, Jie and Xu, Mingliang},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1238–1246},
  title     = {2D-ptr: 2D array pointer network for solving the heterogeneous capacitated vehicle routing problem},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662981},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A trajectory perspective on the role of data sampling
techniques in offline reinforcement learning. <em>AAMAS</em>, 1229–1237.
(<a href="https://dl.acm.org/doi/10.5555/3635637.3662980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, offline reinforcement learning (RL) algorithms have gained considerable attention. However, the role of data sampling techniques in offline RL has been somewhat overlooked, despite their potential to enhance online RL performance. Recent research in offline RL indicates that applying sampling techniques directly to state-transitions does not consistently improve performance. Therefore, to better leverage limited offline trajectory data, we investigate the impact of data sampling processes on offline RL algorithms from a trajectory perspective. In this paper, we introduce a memory technique, (Prioritized) Trajectory Replay (TR/PTR), to facilitate trajectory data storage and sampling. Building on TR, we delve into the potential of trajectory backward sampling, a method that has already proven effective in online RL, in the offline RL domain. Furthermore, to improve the sampling efficiency, we examine the influence of prioritized sampling based on various trajectory priority metrics on offline training. Integrating with existing algorithms, our findings demonstrate that data sampling and updates based on vanilla TR can contribute to more stable training. Also, our proposed 13 trajectory priority metrics for PTR exhibit outstanding performance on their respective applicable types of dataset, with the best-case scenario resulting in performance improvements exceeding 25\%. These performance gains are achieved at a slight extra cost during the data sampling process, highlighting the significant advantages of trajectory-based data sampling for offline RL.},
  archive   = {C_AAMAS},
  author    = {Liu, Jinyi and Ma, Yi and Hao, Jianye and Hu, Yujing and Zheng, Yan and Lv, Tangjie and Fan, Changjie},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1229–1237},
  title     = {A trajectory perspective on the role of data sampling techniques in offline reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662980},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LLM-powered hierarchical language agent for real-time
human-AI coordination. <em>AAMAS</em>, 1219–1228. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {AI agents powered by Large Language Models (LLMs) have made significant advances, enabling them to assist humans in diverse complex tasks and leading to a revolution in human-AI coordination. LLM-powered agents typically require invoking LLM APIs and employing artificially designed complex prompts, which results in high inference latency. While this paradigm works well in scenarios with minimal interactive demands, such as code generation, it is unsuitable for highly interactive and real-time applications, such as gaming. Traditional gaming AI often employs small models or reactive policies, enabling fast inference but offering limited task completion and interaction abilities. In this work, we consider Overcooked as our testbed where players could communicate with natural language and cooperate to serve orders. We propose a Hierarchical Language Agent (HLA) for human-AI coordination that provides both strong reasoning abilities while keeping real-time execution. In particular, HLA adopts a hierarchical framework and comprises three modules: a proficient LLM, referred to as Slow Mind, for intention reasoning and language interaction, a lightweight LLM, referred to as Fast Mind, for generating macro actions, and a reactive policy, referred to as Executor, for transforming macro actions into atomic actions. Human studies show that HLA outperforms other baseline agents, including slow-mind-only agents and fast-mind-only agents, with stronger cooperation abilities, faster responses, and more consistent language communications.},
  archive   = {C_AAMAS},
  author    = {Liu, Jijia and Yu, Chao and Gao, Jiaxuan and Xie, Yuqing and Liao, Qingmin and Wu, Yi and Wang, Yu},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1219–1228},
  title     = {LLM-powered hierarchical language agent for real-time human-AI coordination},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662979},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Progression with probabilities in the situation calculus:
Representation and succinctness. <em>AAMAS</em>, 1210–1218. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Progression in the Situation Calculus is perhaps one of the most extensively studied cases of updating logical theories over a sequence of actions. While it generally requires second-order logic, several useful first-order and tractable cases have been identified. Recently, there has been an interest in studying the progression of probabilistic knowledge bases expressed using degrees of belief on first-order formulas. However, although a few results exist, they do not provide much clarity about how this progression can be computed or represented in a feasible manner. In this paper, we address this problem for the first time. We first examine the progression of a probabilistic knowledge base (PKB) in a world-level representation; in particular, we show that such a representation is closed under progression for any local-effect actions with quantifier-free contexts. We also propose a more succinct representation of the probabilistic knowledge base, i.e. factored-representation PKB. For this type of PKB, we study the conditions for progression to remain succinct.},
  archive   = {C_AAMAS},
  author    = {Liu, Daxin and Belle, Vaishak},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1210–1218},
  title     = {Progression with probabilities in the situation calculus: Representation and succinctness},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662978},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Policy-regularized offline multi-objective reinforcement
learning. <em>AAMAS</em>, 1201–1209. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we aim to utilize only offline trajectory data to train a policy for multi-objective RL. We extend the offline policy-regularized method, a widely-adopted approach for single-objective offline RL problems, into the multi-objective setting in order to achieve the above goal. However, such methods face a new challenge in offline MORL settings, namely thepreference-inconsistent demonstration problem. We propose two solutions to this problem: 1) filtering out preference-inconsistent demonstrations via approximating behavior preferences, and 2) adopting regularization techniques with high policy expressiveness. Moreover, we integrate the preference-conditioned scalarized update method into policy-regularized offline RL, in order to simultaneously learn a set of policies using a single policy network, thus reducing the computational cost induced by the training of a large number of individual policies for various preferences. Finally, we introduce Regularization Weight Adaptation to dynamically determine appropriate regularization weights for arbitrary target preferences during deployment. Empirical results on various multi-objective datasets demonstrate the capability of our approach in solving offline MORL problems.},
  archive   = {C_AAMAS},
  author    = {Lin, Qian and Yu, Chao and Liu, Zongkai and Wu, Zifan},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1201–1209},
  title     = {Policy-regularized offline multi-objective reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662977},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Episodic reinforcement learning with expanded state-reward
space. <em>AAMAS</em>, 1192–1200. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Empowered by deep neural networks, deep reinforcement learning (DRL) has demonstrated tremendous empirical successes in various domains, including games, health care, and autonomous driving. Despite these advancements, DRL is still identified as data-inefficient as effective policies demand vast numbers of environmental samples. Recently, episodic control (EC)-based model-free DRL methods enable sample efficiency by recalling past experiences from episodic memory. However, existing EC-based methods suffer from the limitation of potential misalignment between the state and reward spaces for neglecting the utilization of (past) retrieval states with extensive information, which probably causes inaccurate value estimation and degraded policy performance. To tackle this issue, we introduce an efficient EC-based DRL framework with expanded state-reward space, where the expanded states used as the input and the expanded rewards used in the training both contain historical and current information. To be specific, we reuse the historical states retrieved by EC as part of the input states and integrate the retrieved MC-returns into the immediate reward in each interactive transition. As a result, our method is able to simultaneously achieve the full utilization of retrieval information and the better evaluation of state values by a Temporal Difference (TD) loss. Empirical results on challenging Box2d and Mujoco tasks demonstrate the superiority of our method over a recent sibling method and common baselines. Further, we also verify our method&#39;s effectiveness in alleviating Q-value overestimation by additional experiments of Q-value comparison.},
  archive   = {C_AAMAS},
  author    = {Liang, Dayang and Zhang, Yaru and Liu, Yunlong},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1192–1200},
  title     = {Episodic reinforcement learning with expanded state-reward space},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662976},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A complete landscape for the price of envy-freeness.
<em>AAMAS</em>, 1183–1191. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the efficiency of fair allocations using the well-studied price of fairness concept, which quantitatively measures the worst-case efficiency loss when imposing fairness constraints. Previous works provided partial results on the price of fairness with well-known fairness notions such as envy-freeness up to one good (EF1) and envy-freeness up to any good (EFX). In this paper, we give a complete characterization for the price of envy-freeness in various settings. In particular, we first consider the two-agent case under the indivisible-goods setting and present tight ratios for the price of EF1 (for scaled utility) and EFX (for unscaled utility), which resolve questions left open in the literature. Next, we consider the mixed goods setting which concerns a mixture of both divisible and indivisible goods. We focus on envy-freeness for mixed goods (EFM), which generalizes both envy-freeness and EF1, as well as its strengthening called envy-freeness up to any good for mixed goods (EFXM), which generalizes envy-freeness and EFX. To this end, we settle the price of EFM and EFXM by providing a complete picture of tight bounds for two agents and asymptotically tight bounds for n agents, for both scaled and unscaled utilities.},
  archive   = {C_AAMAS},
  author    = {Li, Zihao and Liu, Shengxin and Lu, Xinhang and Tao, Biaoshuai and Tao, Yichen},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1183–1191},
  title     = {A complete landscape for the price of envy-freeness},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662975},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Developing a multi-agent and self-adaptive framework with
deep reinforcement learning for dynamic portfolio risk management.
<em>AAMAS</em>, 1174–1182. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep or reinforcement learning (RL) approaches have been adapted as reactive agents to quickly learn and respond with new investment strategies for portfolio management under the highly turbulent financial market environments in recent years. In many cases, due to the very complex correlations among various financial sectors, and the fluctuating trends in different financial markets, a deep or reinforcement learning based agent can be biased in maximising the total returns of the newly formulated investment portfolio while neglecting its potential risks under the turmoil of various market conditions in the global or regional sectors. Accordingly, a multi-agent and self-adaptive framework namely the MASA is proposed in which a sophisticated multi-agent reinforcement learning (RL) approach is adopted through two cooperating and reactive agents to carefully and dynamically balance the trade-off between the overall portfolio returns and their potential risks. Besides, a very flexible and proactive agent as the market observer is integrated into the MASA framework to provide some additional information on the estimated market trends as valuable feedbacks for multi-agent RL approach to quickly adapt to the ever-changing market conditions. The obtained empirical results clearly reveal the potential strengths of our proposed MASA framework based on the multi-agent RL approach against many well-known RL-based approaches on the challenging data sets of the CSI 300, Dow Jones Industrial Average and S&amp;amp;P 500 indexes over the past 10 years. More importantly, our proposed MASA framework shed lights on many possible directions for future investigation.},
  archive   = {C_AAMAS},
  author    = {Li, Zhenglong and Tam, Vincent and Yeung, Kwan L.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1174–1182},
  title     = {Developing a multi-agent and self-adaptive framework with deep reinforcement learning for dynamic portfolio risk management},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662974},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Factor graph neural network meets max-sum: A real-time route
planning algorithm for massive-scale trips. <em>AAMAS</em>, 1165–1173.
(<a href="https://dl.acm.org/doi/10.5555/3635637.3662973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Global route planning (GRP) is a typical combinatorial optimization problem that has been solved for a variety of industrial purposes, such as traffic flow management, network routing, and conflict prevention. The goal of the GRP is to find a route for each trip query such that all queries have a minimum global travel time. The GRP problem is NP-hard and computationally challenging, even for medium-sized instances. However, in real-world GRP applications, such as Google Maps-based vehicle route guidance systems, there are always massive-scale trips issued simultaneously, and real-time response is required. Existing mathematical programming-based exact methods and heuristics struggle to balance the extremes of optimality and scalability. Considering that many closed-related GRP instances must be solved repeatedly, this paper explores a deep learning approach to learn real-time and efficient solutions for GRP. This paper first proposes a novel route-query factor graph (RQ-FG) to model the GRP problem, where the message-passing damped Max-sum (DMS) algorithm can be exploited to generate high-quality approximate solutions. A hybrid pruning method is proposed to accelerate solving the DMS. We further devise a route-query factor graph neural network (RQ-FGNN) based on the RQ-FG, which has the ability to return solutions in milliseconds. Experiments demonstrate that our method can generate high-quality solutions in massive-scale GRP instances in real-time.},
  archive   = {C_AAMAS},
  author    = {Li, Yixuan and Wang, Wanyuan and Xu, Weiyi and Deng, Yanchen and Wu, Weiwei},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1165–1173},
  title     = {Factor graph neural network meets max-sum: A real-time route planning algorithm for massive-scale trips},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662973},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Context-aware communication for multi-agent reinforcement
learning. <em>AAMAS</em>, 1156–1164. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Effective communication protocols in multi-agent reinforcement learning (MARL) are critical to fostering cooperation and enhancing team performance. To leverage communication, many previous works have proposed to compress local information into a single message and broadcast it to all reachable agents. This simplistic messaging mechanism, however, may fail to provide adequate, critical, and relevant information to individual agents, especially in severely bandwidth-limited scenarios. This motivates us to develop context-aware communication schemes for MARL, aiming to deliver personalized messages to different agents. Our communication protocol, named CACOM, consists of two stages. In the first stage, agents exchange coarse representations in a broadcast fashion, providing context for the second stage. Following this, agents utilize attention mechanisms in the second stage to selectively generate messages personalized for the receivers. Furthermore, we employ the learned step size quantization (LSQ) technique for message quantization to reduce the communication overhead. To evaluate the effectiveness of CACOM, we integrate it with both actor-critic and value-based MARL algorithms. Empirical results on cooperative benchmark tasks demonstrate that CACOM provides evident performance gains over baselines under communication-constrained scenarios. The code is publicly available at https://github.com/LXXXXR/CACOM.},
  archive   = {C_AAMAS},
  author    = {Li, Xinran and Zhang, Jun},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1156–1164},
  title     = {Context-aware communication for multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662972},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Grasper: A generalist pursuer for pursuit-evasion problems.
<em>AAMAS</em>, 1147–1155. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pursuit-evasion games (PEGs) model interactions between a team of pursuers and an evader in graph-based environments such as urban street networks. Recent advancements have demonstrated the effectiveness of the pre-training and fine-tuning paradigm in Policy-Space Response Oracles (PSRO) to improve scalability in solving large-scale PEGs. However, these methods primarily focus on specific PEGs with fixed initial conditions that may vary substantially in real-world scenarios, which significantly hinders the applicability of the traditional methods. To address this issue, we introduce Grasper, a GeneRAlist purSuer for Pursuit-Evasion pRoblems, capable of efficiently generating pursuer policies tailored to specific PEGs. Our contributions are threefold: First, we present a novel architecture that offers high-quality solutions for diverse PEGs, comprising critical components such as (i) a graph neural network (GNN) to encode PEGs into hidden vectors, and (ii) a hypernetwork to generate pursuer policies based on these hidden vectors. As a second contribution, we develop an efficient three-stage training method involving (i) a pre-pretraining stage for learning robust PEG representations through self-supervised graph learning techniques like graph masked auto-encoder (GraphMAE), (ii) a pre-training stage utilizing heuristic-guided multi-task pre-training (HMP) where heuristic-derived reference policies (e.g., through Dijkstra&#39;s algorithm) regularize pursuer policies, and (iii) a fine-tuning stage that employs PSRO to generate pursuer policies on designated PEGs. Finally, we perform extensive experiments on synthetic and real-world maps, showcasing Grasper&#39;s significant superiority over baselines in terms of solution quality and generalizability. We demonstrate that Grasper provides a versatile approach for solving pursuit-evasion problems across a broad range of scenarios, enabling practical deployment in real-world situations.},
  archive   = {C_AAMAS},
  author    = {Li, Pengdeng and Li, Shuxin and Wang, Xinrun and Cern\&#39;{y}, Jakub and Zhang, Youzhi and McAleer, Stephen and Chan, Hau and An, Bo},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1147–1155},
  title     = {Grasper: A generalist pursuer for pursuit-evasion problems},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662971},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Normalization enhances generalization in visual
reinforcement learning. <em>AAMAS</em>, 1137–1146. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in visual reinforcement learning (RL) have led to impressive success in handling complex tasks. However, these methods have demonstrated limited generalization capability to visual disturbances, which poses a significant challenge to their real-world application and adaptability. Though normalization techniques have demonstrated huge success in supervised and unsupervised learning, their applications in visual RL are still scarce. In this paper, we explore the potential benefits of integrating normalization into visual RL methods with respect to generalization performance. We find that, perhaps surprisingly, incorporating suitable normalization techniques is sufficient to enhance the generalization capabilities, without any additional special design. We utilize the combination of two normalization techniques, CrossNorm and SelfNorm, for generalizable visual RL. Extensive experiments are conducted on DMControl Generalization Benchmark, CARLA, and ProcGen Benchmark to validate the effectiveness of our method. We show that our method significantly improves generalization capability while only marginally affecting sample efficiency. In particular, when integrated with DrQ-v2, our method enhances the test performance of DrQ-v2 on CARLA across various scenarios, from 14\% of the training performance to 97\%. Our project page: https://sites.google.com/view/norm-generalization-vrl/home},
  archive   = {C_AAMAS},
  author    = {Li, Lu and Lyu, Jiafei and Ma, Guozheng and Wang, Zilin and Yang, Zhenjie and Li, Xiu and Li, Zhiheng},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1137–1146},
  title     = {Normalization enhances generalization in visual reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662970},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bounding the incentive ratio of the probabilistic serial
rule. <em>AAMAS</em>, 1128–1136. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Probabilistic Serial (PS) is a well-studied allocation rule used for distributing resources among multiple agents. Although it satisfies certain notable fairness and welfare properties, it is not truthful. This means that agents have incentives to misreport their preferences in order to influence the allocation in their favor. An interesting research question is to understand the extent to which an agent can gain from manipulation. A widely-accepted concept employed for this exploration is the incentive ratio, defined as the supreme ratio, across all instances of the problem, between the utility an agent obtains by employing an optimal manipulation strategy and the utility they receive when being truthful. Wang et al. [AAAI, 2020] examined the incentive ratio of PS for the setting when the number of items m equals the number of agents n and proved that the incentive ratio is 1.5. In this paper, we study the general scenario in which m and n can be arbitrary. We prove that in this case, the tight incentive ratio of PS is 2- 1 over 2n/i&amp;gt;-1.},
  archive   = {C_AAMAS},
  author    = {Li, Bo and Sun, Ankang and Xing, Shiji},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1128–1136},
  title     = {Bounding the incentive ratio of the probabilistic serial rule},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662969},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Coalition formation with bounded coalition size.
<em>AAMAS</em>, 1119–1127. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many situations when people are assigned to coalitions, the utility of each person depends on the friends in her coalition. Additionally, in many situations, the size of each coalition should be bounded. This paper studies such coalition formation scenarios in both weighted and unweighted settings. Since finding a partition that maximizes the utilitarian social welfare is computationally hard, we provide a polynomial-time approximation algorithm. We also investigate the existence and the complexity of finding stable partitions. Namely, we show that the Contractual Strict Core (CSC) is never empty, but the Strict Core (SC) of some games is empty. Finding partitions that are in the CSC is computationally easy, but even deciding whether an SC of a given game exists is NP-hard. In the unweighted setting, we show that when the coalition size is bounded by 3 the core is never empty, and we present a polynomial time algorithm for finding a member of the core. However, for the weighted setting, the core may be empty, and we prove that deciding whether there exists a core is NP-hard.},
  archive   = {C_AAMAS},
  author    = {Levinger, Chaya and Hazon, Noam and Simola, Sofia and Azaria, Amos},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1119–1127},
  title     = {Coalition formation with bounded coalition size},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662968},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning partner selection rules that sustain cooperation in
social dilemmas with the option of opting out. <em>AAMAS</em>,
1110–1118. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study populations of self-interested agents playing a 2-person repeated Prisoner&#39;s Dilemma game, with each player having the option of opting out of the interaction and choosing to be randomly assigned to another partner instead. The partner selection component makes these games akin to random matching, where defection is known to take over the entire population. Results in the literature have shown that, when forcing agents to obey a set partner selection rule known as Out-for-Tat, where defectors are systematically being broken ties with, cooperation can be sustained in the long run. In this paper, we remove this assumption and study agents that learn both action- and partner-selection strategies. Through multi-agent reinforcement learning, we show that cooperation can be sustained without forcing agents to play predetermined strategies. Our simulations show that agents are capable of learning in-game strategies by themselves, such as Tit-for-Tat. What is more, they are also able to simultaneously discover cooperation-sustaining partner selection rules, notably Out-for-Tat, as well as other new rules that make cooperation prevail.},
  archive   = {C_AAMAS},
  author    = {Leung, Chin-wing and Turrini, Paolo},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1110–1118},
  title     = {Learning partner selection rules that sustain cooperation in social dilemmas with the option of opting out},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662967},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The stochastic evolutionary dynamics of softmax policy
gradient in games. <em>AAMAS</em>, 1101–1109. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The theoretical underpinnings of multi-agent learning have recently attracted much attention. In this paper, we study the learning dynamics of the softmax policy gradient (PG) algorithm in multi-agent environments in the context of evolutionary game theory. We revisit the previous analyses based on mean dynamics and observe that previous models fail to characterize the effect of stochasticity. To this end, we propose a stochastic dynamics model to analyse the learning dynamics of PG under symmetric games. We model the parameter dynamics of the learning agent as a multidimensional Wiener process. Applying the It\^{o}&#39;s lemma, we obtain the corresponding policy dynamics for the agent. From that, we study the convergence behaviour of the policy dynamics under the self-play training scheme for learning in games. We work out the sufficient conditions for the stochastic stability of the pure Nash equilibrium strategy, and we evaluate the sufficient conditions for the existence of stationary distribution for strictly stable games. Moreover, we express the dynamics of the parameter distribution with the Fokker-Planck equation. In the experiments, we demonstrate that our stochastic dynamics model always provides a significantly more accurate description of the actual learning dynamics than the mean dynamics model across different games and settings.},
  archive   = {C_AAMAS},
  author    = {Leung, Chin-wing and Hu, Shuyue and Leung, Ho-fung},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1101–1109},
  title     = {The stochastic evolutionary dynamics of softmax policy gradient in games},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662966},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Policy learning for off-dynamics RL with deficient support.
<em>AAMAS</em>, 1093–1100. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement Learning (RL) can effectively learn complex policies. However, learning these policies often demands extensive trial-and-error interactions with the environment. In many real-world scenarios, this approach is not practical due to the high costs of data collection and safety concerns. As a result, a common strategy is to transfer a policy trained in a low-cost, rapid source simulator to a real-world target environment. However, this process poses challenges. Simulators, no matter how advanced, cannot perfectly replicate the intricacies of the real world, leading to dynamics discrepancies between the source and target environments. Past research posited that the source domain must encompass all possible target transitions, a condition we term full support. However, expecting full support is often unrealistic, especially in scenarios where significant dynamics discrepancies arise. In this paper, our emphasis shifts to addressing large dynamics mismatch adaptation. We move away from the stringent full support condition of earlier research, focusing instead on crafting an effective policy for the target domain. Our proposed approach is simple but effective. It is anchored in the central concepts of the skewing and extension of source support towards target support to mitigate support deficiencies. Through comprehensive testing on a varied set of benchmarks, our method&#39;s efficacy stands out, showcasing notable improvements over previous techniques.},
  archive   = {C_AAMAS},
  author    = {Le Pham Van, Linh and The Tran, Hung and Gupta, Sunil},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1093–1100},
  title     = {Policy learning for off-dynamics RL with deficient support},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662965},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Beyond surprise: Improving exploration through surprise
novelty. <em>AAMAS</em>, 1084–1092. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a new computing model for intrinsic rewards in reinforcement learning that addresses the limitations of existing surprise-driven explorations. The reward is the novelty of the surprise rather than the surprise norm. We estimate the surprise novelty as retrieval errors of a memory network wherein the memory stores and reconstructs surprises. Our surprise memory (SM) augments the capability of surprise-based intrinsic motivators, maintaining the agent&#39;s interest in exciting exploration while reducing unwanted attraction to unpredictable or noisy observations. Our experiments demonstrate that the SM combined with various surprise predictors exhibits efficient exploring behaviors and significantly boosts the final performance in sparse reward environments, including Noisy-TV, navigation and challenging Atari games.},
  archive   = {C_AAMAS},
  author    = {Le, Hung and Do, Kien and Nguyen, Dung and Venkatesh, Svetha},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1084–1092},
  title     = {Beyond surprise: Improving exploration through surprise novelty},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662964},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Proportional fairness in obnoxious facility location.
<em>AAMAS</em>, 1075–1083. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the obnoxious facility location problem (in which agents prefer the facility location to be far from them) and propose a hierarchy of distance-based proportional fairness concepts for the problem. These fairness axioms ensure that groups of agents at the same location are guaranteed to be a distance from the facility proportional to their group size. We consider deterministic and randomized mechanisms, and compute tight bounds on the price of proportional fairness. In the deterministic setting, we show that our proportional fairness axioms are incompatible with strategy proofness, and prove asymptotically tight epsilon-price of anarchy and stability bounds for proportionally fair welfare-optimal mechanisms. In the randomized setting, we identify proportionally fair and strategyproof mechanisms that give an expected welfare within a constant factor of the optimal welfare. Finally, we prove existence results for two extensions to our model.},
  archive   = {C_AAMAS},
  author    = {Lam, Alexander and Aziz, Haris and Li, Bo and Ramezani, Fahimeh and Walsh, Toby},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1075–1083},
  title     = {Proportional fairness in obnoxious facility location},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662963},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Higher order reasoning under intent uncertainty reinforces
the hobbesian trap. <em>AAMAS</em>, 1066–1074. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Civilisations in the universe face the difficulty of communicating and trying to understand others&#39; intentions. Moreover, advanced civilisations could develop weapons to pre-emptively eliminate any civilisations that present a future threat - this is known as the Hobbesian trap. Here, we present a multi-agent simulation model to investigate conditions for such pre-emptive attacks. We design a novel algorithm for solving Interactive Partially Observable Markov Decision Processes (I-POMDPs) with continuous state and observation spaces; it enables civilisations to perform higher-order reasoning. The algorithm builds a nested hierarchy of search forests using Monte Carlo simulations, determining updated beliefs by weighting existing particles. Our experiments reveal interesting insights into the behaviour of rational civilisations under varying levels of reasoning, morality and uncertainty. We find that selfish civilisations always create a war-like universe. Even good, universalist civilisations can initiate pre-emptive attacks if they are uncertain about others&#39; intentions. Finally, our findings have important implications for international peace and security and may explain persistent conflicts and the fragility of ceasefires. Under such conditions a well-coordinated international approach, facilitated by international alliances such as the United Nations, is paramount.},
  archive   = {C_AAMAS},
  author    = {Kuusela, Otto and Roy, Debraj},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1066–1074},
  title     = {Higher order reasoning under intent uncertainty reinforces the hobbesian trap},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662962},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Approximating APS under submodular and XOS valuations with
binary marginals. <em>AAMAS</em>, 1057–1065. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of fairly dividing indivisible goods among a set of agents under the fairness notion of Any Price Share (APS). APS is known to dominate the widely studied Maximin share (MMS). Since an exact APS allocation may not exist, the focus has traditionally been on the computation of approximate APS allocations. [4] studied the problem under additive valuations, and asked (i) how large can the APS value be compared to the MMS value? and (ii) what guarantees can one achieve beyond additive functions. We partly answer these questions by considering valuations beyond additive, namely submodular and XOS functions, with binary marginals. For the submodular functions with binary marginals, also known as matroid rank functions (MRFs), we show that APS is exactly equal to MMS. Consequently, following [5] we show that an exact APS allocation exists and can be computed efficiently while maximizing the social welfare. Complementing this result, we show that it is NP-hard to compute the APS value within a factor of 5/6 for submodular valuations with three distinct marginals of {0, 1/2, 1.}We then consider binary XOS functions, which are immediate generalizations of binary submodular functions in the complement free hierarchy. In contrast to the MRFs setting, MMS and APS values are not equal under this case. Nevertheless, we can show that they are only a constant factor apart. In particular, we show that under binary XOS valuations, MMS ≤= APS ≤= 2 x MMS + 1. Further, we show that this is almost the tightest bound we can get using MMS, by giving an instance where APS ≥= 2 x MMS. The upper bound on APS, combined with [17], implies a 0.1222-approximation for APS under binary XOS valuations. And the lower bound implies the non-existence of better than 0.5-APS even when agents have identical valuations, which is in sharp contrast to the guaranteed existence of exact MMS allocation when agent valuations are identical.},
  archive   = {C_AAMAS},
  author    = {Kulkarni, Pooja and Kulkarni, Rucha and Mehta, Ruta},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1057–1065},
  title     = {Approximating APS under submodular and XOS valuations with binary marginals},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662961},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continuous monte carlo graph search. <em>AAMAS</em>,
1047–1056. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Online planning is crucial for high performance in many complex sequential decision-making tasks. Monte Carlo Tree Search (MCTS) employs a principled mechanism for trading off exploration for exploitation for efficient online planning, and it outperforms comparison methods in many discrete decision-making domains such as Go, Chess, and Shogi. Subsequently, extensions of MCTS to continuous domains have been developed. However, the inherent high branching factor and the resulting explosion of the search tree size are limiting the existing methods. To address this problem, we propose Continuous Monte Carlo Graph Search (CMCGS), an extension of MCTS to online planning in environments with continuous state and action spaces. CMCGS takes advantage of the insight that, during planning, sharing the same action policy between several states can yield high performance. To implement this idea, at each time step, CMCGS clusters similar states into a limited number of stochastic action bandit nodes, which produce a layered directed graph instead of an MCTS search tree. Experimental evaluation shows that CMCGS outperforms comparable planning methods in several complex continuous DeepMind Control Suite benchmarks and 2D navigation and exploration tasks with limited sample budgets. Furthermore, CMCGS can be scaled up through parallelization, and it outperforms the Cross-Entropy Method (CEM) in continuous control with learned dynamics models.},
  archive   = {C_AAMAS},
  author    = {Kujanp\&quot;{a}\&quot;{a}, Kalle and Babadi, Amin and Zhao, Yi and Kannala, Juho and Ilin, Alexander and Pajarinen, Joni},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1047–1056},
  title     = {Continuous monte carlo graph search},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662960},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Minimizing state exploration while searching graphs with
unknown obstacles. <em>AAMAS</em>, 1038–1046. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the challenge of finding a shortest path in a graph with unknown obstacles where the exploration cost to detect whether a state is free or blocked is very high (e.g., due to sensor activation for obstacle detection). The main objective is to solve the problem while minimizing the number of explorations. To achieve this, we propose MXA*, a novel heuristic search algorithm based on A*. The key innovation in MXA* lies in modifying the heuristic calculation to avoid obstacles that have already been revealed. Furthermore, this paper makes a noteworthy contribution by introducing the concept of a dynamic heuristic. In contrast to the conventional static heuristic, a dynamic heuristic leverages information that emerges during the search process and adapts its estimations accordingly. By employing a dynamic heuristic, we suggest enhancements to MXA* based on real-time information obtained from both the open and closed lists. We demonstrate empirically that MXA* finds the shortest path while significantly reducing the number of explored states compared to traditional A*. The code is available at https://github.com/bernuly1/MXA-Star.},
  archive   = {C_AAMAS},
  author    = {Koyfman, Daniel and Shperberg, Shahaf S. and Atzmon, Dor and Felner, Ariel},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1038–1046},
  title     = {Minimizing state exploration while searching graphs with unknown obstacles},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662959},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fine-grained liquid democracy for cumulative ballots.
<em>AAMAS</em>, 1029–1037. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate efficient ways for the incorporation of liquid democracy into election settings in which voters submit cumulative ballots, i.e., when each voter is assigned a virtual coin that she can then distribute as she wishes among the available election options. In particular, aiming at improving the quality of decision making, we are interested in fine-grained liquid democracy, meaning that voters are able to designate a partial coin to a set of election options and delegate the decision on how to further split this partial coin among those election options to another voter of her choice.The fact that we wish such delegations to be transitive-combined with our aim at fully respecting such delegations---means that inconsistencies and cycles can occur, thus we set to find computationally-efficient ways of resolving voters&#39; delegations. To this end we develop a theory based on fixed-point theorems and mathematical programming techniques and we show that for various variants of definitions regarding how to resolve such transitive delegations, there is always a feasible resolution; and we identify under which conditions such solutions are efficiently computable. For example, we provide a parameterized algorithm whose running time depends on a distance from triviality of a given instance.},
  archive   = {C_AAMAS},
  author    = {K\&quot;{o}ppe, Matthias and Kouteck\&#39;{y}, Martin and Sornat, Krzysztof and Talmon, Nimrod},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1029–1037},
  title     = {Fine-grained liquid democracy for cumulative ballots},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662958},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Veto core consistent preference aggregation. <em>AAMAS</em>,
1020–1028. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The proportional veto principle is the notion that a coalition of x\% of the voters should be able to block roughly x\% of the outcomes. This is in opposition to the majority principle, which holds that 51\% of the voters should have all the decision power and the remaining 49\% zero; or the utilitarian principle, which focuses selecting an outcome that is best on average, even if that outcome is inadmissible for certain individuals or groups. Originating in public choice, the proportional veto principle found rich application in the theory of social multi-criteria evaluation and bilateral choice, but to date the family of voting rules which are consistent with this principle have not been subject to the same axiomatic scrutiny as majoritarian or positional approaches to voting. In this paper we address this gap by analysing two broad families of such rules, and six concrete examples, with respect to the properties of monotonicity, participation, and independence of unanimous losers.},
  archive   = {C_AAMAS},
  author    = {Kondratev, Aleksei Y. and Ianovski, Egor},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1020–1028},
  title     = {Veto core consistent preference aggregation},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662957},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Catfished! Impacts of strategic misrepresentation in online
dating. <em>AAMAS</em>, 1011–1019. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Online dating is a multibillion-dollar global industry, and is increasingly becoming the go-to method for finding partners. Intricate dynamics mark its operation, influenced by varying user preferences, strategies, and traits, as well as by the underlying matchmaking algorithm. This complexity renders it a pertinent subject for multiagent systems research. Despite its relevance, an established simulation framework for online dating is lacking. This paper introduces a multiagent simulation framework for this domain. The framework is extensible and capable of modeling agents with diverse attributes and preferences, either reported or latent. It also supports varied strategies, outcomes, and types of matchmaking logic. Using this framework, we simulate an online dating platform based on real-world demographics to examine the effects of strategic misrepresentation, a notable concern in online dating. Surprisingly, the negative effect of strategic misrepresentation on users is marginal. Moreover, it disproportionately benefits female or honest agents more, enhances the overall welfare of the user population, and benefits attractive users - whether deceitful or not - over less attractive ones.},
  archive   = {C_AAMAS},
  author    = {Kilic, Oz and Tsang, Alan},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1011–1019},
  title     = {Catfished! impacts of strategic misrepresentation in online dating},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662956},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scaling opponent shaping to high dimensional games.
<em>AAMAS</em>, 1001–1010. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In multi-agent settings with mixed incentives, methods developed for zero-sum games have been shown to lead to detrimental outcomes. To address this issue, opponent shaping (OS) methods explicitly learn to influence the learning dynamics of co-players and empirically lead to improved individual and collective outcomes. However, OS methods have only been evaluated in low-dimensional environments due to the challenges associated with estimating higher-order derivatives or scaling model-free meta-learning. Alternative methods that scale to more complex settings either converge to undesirable solutions or rely on unrealistic assumptions about the environment or co-players. In this paper, we successfully scale an OS-based approach to general-sum games with temporally-extended actions and long-time horizons for the first time. After analysing the representations of the meta-state and history used by previous algorithms, we propose a simplified version called SHAPER. We show empirically that SHAPER leads to improved individual and collective outcomes in a range of challenging settings from literature. We further formalize a technique previously implicit in the literature and analyse its contribution to opponent shaping. We show empirically that this technique is helpful for the functioning of prior methods in certain environments. Lastly, we show that previous environments, such as the CoinGame, are inadequate for analysing temporally-extended general-sum interactions.},
  archive   = {C_AAMAS},
  author    = {Khan, Akbir and Willi, Timon and Kwan, Newton and Tacchetti, Andrea and Lu, Chris and Grefenstette, Edward and Rockt\&quot;{a}schel, Tim and Foerster, Jakob},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1001–1010},
  title     = {Scaling opponent shaping to high dimensional games},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662955},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient method for finding optimal strategies in chopstick
auctions with uniform objects values. <em>AAMAS</em>, 992–1000. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose an algorithm for computing Nash equilibria (NE) in a class of conflicts with multiple battlefields with uniform battlefields values and a non-linear aggregation function. By expanding the symmetrization idea of Hart[9], proposed for the Colonel Blotto game, to the wider class of symmetric conflicts with multiple battlefields, we reduce the number of strategies of the players by an exponential factor. We propose a clash matrix algorithm which allows for computing the payoffs in the symmetrized model in polynomial time. Combining symmetrization and clash matrix algorithm with the double oracle algorithm we obtain an algorithm for computing NE in the models in question that achieves a significant speed-up as compared to the standard, LP-based, approach. We also introduce a heuristic to further speed up the process. Overall, our approach offers an efficient and novel method for computing NE in a specific class of conflicts, with potential practical applications in various fields.},
  archive   = {C_AAMAS},
  author    = {Kazmierowski, Stanislaw and Dziubinski, Marcin},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {992–1000},
  title     = {Efficient method for finding optimal strategies in chopstick auctions with uniform objects values},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662954},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). To lead or to be led: A generalized condorcet jury theorem
under dependence. <em>AAMAS</em>, 983–991. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Aggregating pieces of information or beliefs held by (abstract) agents is central to a variety of belief merging applications. When the merging process aims at retrieving an underlying ground truth, the Condorcet Jury Theorem (CJT) allows identifying voting rules that almost surely track the true piece of information for large groups of agents, given that specific conditions are met. As essential assumptions, the CJT relies on all agents being equally competent as well as independent from one another. In the search for a generalization of the CJT applicable to real-world scenarios, both aforementioned assumptions were weakened separately. In this work, we provide a generalization of the CJT that allows, at the same time, for heterogeneous competence levels across agents as well as a degree of dependence modeled through an opinion leader exerting influence on the electorate. Additionally, we derive a concrete bound on the number of agents necessary to successfully track the underlying ground truth, and examine the bound&#39;s tightness by means of statistical simulations.},
  archive   = {C_AAMAS},
  author    = {Karge, Jonas and Burkhardt, Juliette-Michelle and Rudolph, Sebastian and Rusovac, Dominik},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {983–991},
  title     = {To lead or to be led: A generalized condorcet jury theorem under dependence},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662953},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Safe model-based multi-agent mean-field reinforcement
learning. <em>AAMAS</em>, 973–982. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many applications, e.g., in shared mobility, require coordinating a large number of agents. Mean-field reinforcement learning addresses the resulting scalability challenge by optimizing the policy of a representative agent interacting with the infinite population of identical agents instead of considering individual pairwise interactions. In this paper, we address an important generalization where there exist global constraints on the distribution of agents (e.g., requiring capacity constraints or minimum coverage requirements to be met). We propose Safe-M3-UCRL, the first model-based mean-field reinforcement learning algorithm that attains safe policies even in the case of unknown transitions. As a key ingredient, it uses epistemic uncertainty in the transition model within a log-barrier approach to ensure pessimistic constraints satisfaction with high probability. Beyond the synthetic swarm motion benchmark, we showcase Safe-M3-UCRL on the vehicle repositioning problem faced by many shared mobility operators and evaluate its performance through simulations built on vehicle trajectory data from a service provider in Shenzhen. Our algorithm effectively meets the demand in critical areas while ensuring service accessibility in regions with low demand.},
  archive   = {C_AAMAS},
  author    = {Jusup, Matej and P\&#39;{a}sztor, Barna and Janik, Tadeusz and Zhang, Kenan and Corman, Francesco and Krause, Andreas and Bogunovic, Ilija},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {973–982},
  title     = {Safe model-based multi-agent mean-field reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662952},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decentralized federated policy gradient with byzantine
fault-tolerance and provably fast convergence. <em>AAMAS</em>, 964–972.
(<a href="https://dl.acm.org/doi/10.5555/3635637.3662951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In Federated Reinforcement Learning (FRL), agents aim to collaboratively learn a common task, while each agent is acting in its local environment without exchanging raw trajectories. Existing approaches for FRL either (a) do not provide any fault-tolerance guarantees (against misbehaving agents), or (b) rely on a trusted central agent (a single point of failure) for aggregating updates. We provide the first decentralized Byzantine fault-tolerant FRL method. Towards this end, we first propose a new centralized Byzantine fault-tolerant policy gradient (PG) algorithm that improves over existing methods by relying only on assumptions standard for non-fault-tolerant PG. Then, as our main contribution, we show how a combination of robust aggregation and Byzantine-resilient agreement methods can be leveraged in order to eliminate the need for a trusted central entity. Since our results represent the first sample complexity analysis for Byzantine fault-tolerant decentralized federated non-convex optimization, our technical contributions may be of independent interest. Finally, we corroborate our theoretical results experimentally for common RL environments, demonstrating the speed-up of decentralized federations w.r.t. the number of participating agents and resilience against various Byzantine attacks.},
  archive   = {C_AAMAS},
  author    = {Jordan, Philip and Gr\&quot;{o}tschla, Florian and Fan, Flint Xiaofeng and Wattenhofer, Roger},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {964–972},
  title     = {Decentralized federated policy gradient with byzantine fault-tolerance and provably fast convergence},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662951},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Recourse under model multiplicity via argumentative
ensembling. <em>AAMAS</em>, 954–963. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Model Multiplicity (MM) arises when multiple, equally performing machine learning models can be trained to solve the same prediction task. Recent studies show that models obtained under MM may produce inconsistent predictions for the same input. When this occurs, it becomes challenging to provide counterfactual explanations (CEs), a common means for offering recourse recommendations to individuals negatively affected by models&#39; predictions. In this paper, we formalise this problem, which we name recourse-aware ensembling, and identify several desirable properties which methods for solving it should satisfy. We show that existing ensembling methods, naturally extended in different ways to provide CEs, fail to satisfy these properties. We then introduce argumentative ensembling, deploying computational argumentation to guarantee robustness of CEs to MM, while also accommodating customisable user preferences. We show theoretically and experimentally that argumentative ensembling satisfies properties that the existing methods lack, and that the trade-offs are minimal wrt accuracy.},
  archive   = {C_AAMAS},
  author    = {Jiang, Junqi and Leofante, Francesco and Rago, Antonio and Toni, Francesca},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {954–963},
  title     = {Recourse under model multiplicity via argumentative ensembling},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662950},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Disentangling policy from offline task representation
learning via adversarial data augmentation. <em>AAMAS</em>, 944–953. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Offline meta-reinforcement learning (OMRL) proficiently allows an agent to tackle novel tasks while solely relying on a static dataset. For precise and efficient task identification, existing OMRL research suggests learning separate task representations that be incorporated with policy input, thus forming a context-based meta-policy. A major approach to train task representations is to adopt contrastive learning using multi-task offline data. The dataset typically encompasses interactions from various policies (i.e., the behavior policies), thus providing a plethora of contextual information regarding different tasks. Nonetheless, amassing data from a substantial number of policies is not only impractical but also often unattainable in realistic settings. Instead, we resort to a more constrained yet practical scenario, where multi-task data collection occurs with a limited number of policies. We observed that learned task representations from previous OMRL methods tend to correlate spuriously with the behavior policy instead of reflecting the essential characteristics of the task, resulting in unfavorable out-of-distribution generalization. To alleviate this issue, we introduce a novel algorithm to disentangle the impact of behavior policy from task representation learning through a process called adversarial data augmentation. Specifically, the objective of adversarial data augmentation is not merely to generate data analogous to offline data distribution; instead, it aims to create adversarial examples designed to confound learned task representations and lead to incorrect task identification. Our experiments show that learning from such adversarial samples significantly enhances the robustness and effectiveness of the task identification process and realizes satisfactory out-of-distribution generalization. The results in MuJoCo locomotion tasks demonstrate that our approach surpasses other OMRL baselines across various meta-learning task sets.},
  archive   = {C_AAMAS},
  author    = {Jia, Chengxing and Zhang, Fuxiang and Li, Yi-Chen and Gao, Chen-Xiao and Liu, Xu-Hui and Yuan, Lei and Zhang, Zongzhang and Yu, Yang},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {944–953},
  title     = {Disentangling policy from offline task representation learning via adversarial data augmentation},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662949},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discovering consistent subelections. <em>AAMAS</em>,
935–943. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We show how hidden interesting subelections can be discovered in ordinal elections. An interesting subelection consists of a reasonably large set of voters and a reasonably large set of candidates such that the former have a consistent opinion about the latter. Consistency may take various forms but we focus on three: Identity (all selected voters rank all selected candidates the same way), antagonism (half of the selected voters rank candidates in some order and the other half in the reverse order), and clones (all selected voters rank all selected candidates contiguously in the original election). We first study the computation of such hidden subelections. Second, we analyze synthetic and real-life data, and find that identifying hidden consistent subelections allows us to uncover some relevant concepts.},
  archive   = {C_AAMAS},
  author    = {Janeczko, \L{}ukasz and Lang, J\&#39;{e}r\^{o}me and Lisowski, Grzegorz and Szufa, Stanislaw},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {935–943},
  title     = {Discovering consistent subelections},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662948},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Playing quantitative games against an authority: On the
module checking problem. <em>AAMAS</em>, 926–934. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Module checking is a decision problem to formalize the verification of (possibly multi-agent) systems that must adapt their behavior to the input they receive from the environment, also viewed as an authority. So far, module checking has been only considered in the Boolean setting, which does not capture the different levels of quality inherent to complex systems (e.g., systems dealing with quantitative utilities or sensor inputs). In this paper, we address this issue by proposing quantitative module checking. We study the problem in the quantitative and multi-agent setting, which enables the verification of different levels of satisfaction in relation to a specification. We consider specifications given in Quantitative Alternating-time Temporal logics and investigate their complexity and expressivity.},
  archive   = {C_AAMAS},
  author    = {Jamroga, Wojciech and Mittelmann, Munyque and Murano, Aniello and Perelli, Giuseppe},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {926–934},
  title     = {Playing quantitative games against an authority: On the module checking problem},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662947},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unraveling the tapestry of deception and personality: A deep
dive into multi-issue human-agent negotiation dynamics. <em>AAMAS</em>,
916–925. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Exploring the intricacies of human behavior in negotiations is pivotal in developing advanced human-agent interaction systems. This study delves into the complex interplay between deception, personality traits, and self-reported truthfulness in the context of human-agent negotiations, leveraging the IAGO platform [34] to facilitate multi-issue bargaining tasks. Our exploration, which also ventures into the realm of agent avatar gender and personality trait display, is centered around understanding how individual personality traits influence deceptive behaviors and perceptions in negotiations. Our findings establish a significant alignment between participants&#39; self-reported truthfulness and their actual behaviors, underscoring the reliability of self-reports. Moreover, intricate relationships were uncovered between the Big Five personality dimensions-Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism [3, 9]-and human user&#39;s self-reported truthfulness, as well as beliefs about the necessity of deception in negotiations. To illustrate, individuals with higher levels of Openness were more likely to report being truthful but also believed more strongly in the necessity of deception for successful negotiations. These nuanced insights into personality-driven behaviors and perceptions are instrumental in fostering the development of adaptive and sophisticated negotiation agents, enhancing the comprehension of dynamics in human-agent interactions. Our findings present refined perspectives on the congruence and potential divergences between perceived necessity and the actual enactment of deceptive behaviors, laying a robust foundation for future investigations in agent personalization and human-agent interactions within negotiation contexts.},
  archive   = {C_AAMAS},
  author    = {Jahan, Nusrath and Mell, Johnathan},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {916–925},
  title     = {Unraveling the tapestry of deception and personality: A deep dive into multi-issue human-agent negotiation dynamics},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662946},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards a principle-based framework for repair selection in
inconsistent knowledge bases. <em>AAMAS</em>, 907–915. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper investigates a general principle-based framework for retrieving preferred repairs from inconsistent knowledge bases under a broad family of strategies. To begin with, we define a set of principles that ensure rational behaviours of repair selection strategies. Then, we classify the strategies into two basic categories: (i) comparing repairs without requiring formula information; and (ii) comparing repairs based on formula information. Based on this classification, we present several novel repair selection strategies and show that our framework encompasses various existing popular strategies. Through a systematical analysis of these selection strategies using the proposed principles, we conclude that our principles allow for effective discrimination among the strategies. Finally, preliminary experimental results are presented to show the feasibility of our approach.},
  archive   = {C_AAMAS},
  author    = {Jabbour, Said and Ma, Yue and Raddaoui, Badran},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {907–915},
  title     = {Towards a principle-based framework for repair selection in inconsistent knowledge bases},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662945},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Is limited information enough? An approximate multi-agent
coverage control in non-convex discrete environments. <em>AAMAS</em>,
898–906. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Conventional distributed approaches to coverage control may suffer from lack of convergence and poor performance, due to the fact that agents have limited information, especially in non-convex discrete environments. To address this issue, we extend the approach of [12] which demonstrates how a limited degree of inter-agent communication can be exploited to overcome such pitfalls in one-dimensional discrete environments. The focus of this paper is on extending such results to general dimensional settings. We show that the extension is convergent and keeps the approximation ratio of 2, meaning that any stable solution is guaranteed to have a performance within 50\% of the optimal one. The experimental results exhibit that our algorithm outperforms several state-of-the-art algorithms, and also that the runtime is scalable.},
  archive   = {C_AAMAS},
  author    = {Iwase, Tatsuya and Beynier, Aur\&#39;{e}lie and Bredeche, Nicolas and Maudet, Nicolas and Marden, Jason R.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {898–906},
  title     = {Is limited information enough? an approximate multi-agent coverage control in non-convex discrete environments},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662944},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A cloud-based microservices solution for multi-agent traffic
control systems. <em>AAMAS</em>, 889–897. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a cloud-based microservices architecture designed for multi-agent-based Traffic Control Systems (TCS). This architecture facilitates decentralized control, real-time responsiveness, and dynamic scalability. To validate our approach, we have transitioned DALI, a multi-agent collaborative TCS, from its current server-based architecture to the cloud, implementing this new architecture. Experimental results demonstrate significant improvements in latency reduction and system adaptability when compared to the traditional server-based model.},
  archive   = {C_AAMAS},
  author    = {Ihejimba, Chikadibia and Wenkstern, Rym Z.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {889–897},
  title     = {A cloud-based microservices solution for multi-agent traffic control systems},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662943},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BDI agents in natural language environments. <em>AAMAS</em>,
880–888. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Developing autonomous agents to deal with real-world problems is challenging, especially when developers are not necessarily specialists in artificial intelligence. This poses two key challenges regarding the interface of the programming with the developer, and the efficiency of the resulting agents. In this paper we tackle both challenges in an efficient agent architecture that leverages recent developments in natural language processing, and the intuitive folk psychology abstraction of the beliefs, desires, intentions (BDI) architecture. The resulting architecture uses existing reinforcement learning techniques to bootstrap the agent&#39;s reasoning capabilities while allowing a developer to instruct the agent more directly using natural language as its programming interface. We empirically show the efficiency gains of natural language plans over a pure machine learning approach in the ScienceWorld environment.},
  archive   = {C_AAMAS},
  author    = {Ichida, Alexandre Yukio and Meneguzzi, Felipe and Cardoso, Rafael C.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {880–888},
  title     = {BDI agents in natural language environments},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662942},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rational verification with quantitative probabilistic goals.
<em>AAMAS</em>, 871–879. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the rational verification problem for multi-agent systems in a setting where agents have quantitative probabilistic goals. We use concurrent stochastic games to model multi-agent systems and assume players desire to maximise the probability of satisfying their goals, specified using Linear Temporal Logic (LTL). The main decision problem in this setting is whether a given LTL formula is almost surely satisfied on some pure Nash equilibrium of a given game. We prove that this problem is undecidable in the general case, and then characterise the complexity of this problem under various restrictions on strategies. We also study the problem of deciding whether a given strategy profile is a Nash equilibrium in a given game and show that, unlike the previous verification problem, this question is decidable for several common strategy models.},
  archive   = {C_AAMAS},
  author    = {Hyland, David and Gutierrez, Julian and Shankaranarayanan, Krishna and Wooldridge, Michael},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {871–879},
  title     = {Rational verification with quantitative probabilistic goals},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662941},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the stability of learning in network games with many
players. <em>AAMAS</em>, 861–870. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent learning algorithms have been shown to display complex, unstable behaviours in a wide array of games. In fact, previous works indicate that convergent behaviours are less likely to occur as the total number of agents increases. This seemingly prohibits convergence to stable strategies, such as Nash Equilibria, in games with many players.To make progress towards addressing this challenge we study the Q-Learning Dynamics, a classical model for exploration and exploitation in multi-agent learning. In particular, we study the behaviour of Q-Learning on games where interactions between agents are constrained by a network. We determine a number of sufficient conditions, depending on the game and network structure, which guarantee that agent strategies converge to a unique stable strategy, called the Quantal Response Equilibrium (QRE). Crucially, these sufficient conditions are independent of the total number of agents, allowing for provable convergence in arbitrarily large games. This ensures convergence in games with many agents, so long as the underlying interaction network is chosen suitably.Next, we compare the learned QRE to the underlying NE of the game, by showing that any QRE is an ∈-approximate Nash Equilibrium. We first provide tight bounds on ∈ and show how these bounds lead naturally to a centralised scheme for choosing exploration rates, which enables independent learners to learn stable approximate Nash Equilibrium strategies. We validate the method through experiments and demonstrate its effectiveness even in the presence of numerous agents and actions. Through these results, we show that independent learning dynamics may converge to approximate Nash Equilibria, even in the presence of many agents.},
  archive   = {C_AAMAS},
  author    = {Hussain, Aamal and Leonte, Dan and Belardinelli, Francesco and Piliouras, Georgios},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {861–870},
  title     = {On the stability of learning in network games with many players},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662940},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Keeping the harmony between neighbors: Local fairness in
graph fair division. <em>AAMAS</em>, 852–860. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of allocating indivisible resources under the connectivity constraints of a graph G. This model, initially introduced by Bouveret et al. (published in IJCAI, 2017), effectively encompasses a diverse array of scenarios characterized by spatial or temporal limitations, including the division of land plots and the allocation of time plots. In this paper, we introduce a novel fairness concept that integrates local comparisons within the social network formed by a connected allocation of the item graph. Our particular focus is to achieve pairwise-maximin fair share (PMMS) among the &quot;neighbors&quot; within this network. For any underlying graph structure, we show that a connected allocation that maximizes Nash welfare guarantees a (1/2)-PMMS fairness. Moreover, for two agents, we establish that a (3/4)-PMMS allocation can be efficiently computed. Additionally, we demonstrate that for three agents and the items aligned on a path, a PMMS allocation is always attainable and can be computed in polynomial time. Lastly, when agents have identical additive utilities, we present a pseudo-polynomial-time algorithm for a (3/4)-PMMS allocation, irrespective of the underlying graph G. Furthermore, we provide a polynomial-time algorithm for obtaining a PMMS allocation when G is a tree.},
  archive   = {C_AAMAS},
  author    = {Hummel, Halvard and Igarashi, Ayumi},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {852–860},
  title     = {Keeping the harmony between neighbors: Local fairness in graph fair division},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662939},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Applying opponent modeling for automatic bidding in online
repeated auctions. <em>AAMAS</em>, 843–851. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Online auction scenarios, such as bidding searches on advertising platforms, often require bidders to participate repeatedly in auctions for identical or similar items. Most previous studies have only considered the process by which the seller learns the prior-dependent optimal mechanism in a repeated auction. However, in this paper, we define a multiagent reinforcement learning environment in which strategic bidders and the seller learn their strategies simultaneously and design an automatic bidding algorithm that updates the strategy of bidders through online interactions. We propose Bid Net to replace the linear shading function as a representation of the strategic bidders&#39; strategy, which effectively improves the utility of strategy learned by bidders. We apply and revise the opponent modeling methods to design the PG (pseudo-gradient) algorithm, which allows bidders to learn optimal bidding strategies with predictions of the other agents&#39; strategy transition. We prove that when a bidder uses the PG algorithm, it can learn the best response to static opponents. When all bidders adopt the PG algorithm, the system will converge to the equilibrium of the game induced by the auction. In experiments with diverse environmental settings and varying opponent strategies, the PG algorithm maximizes the utility of bidders. We hope that this article will inspire research on automatic bidding strategies for strategic bidders.},
  archive   = {C_AAMAS},
  author    = {Hu, Yudong and Han, Congying and Guo, Tiande and Xiao, Hao},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {843–851},
  title     = {Applying opponent modeling for automatic bidding in online repeated auctions},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662938},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Measuring policy distance for multi-agent reinforcement
learning. <em>AAMAS</em>, 834–842. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Diversity plays a crucial role in improving the performance of multi-agent reinforcement learning (MARL). Currently, many diversity-based methods have been developed to overcome the drawbacks of excessive parameter sharing in traditional MARL. However, there remains a lack of a general metric to quantify policy differences among agents. Such a metric would not only facilitate the evaluation of the diversity evolution in multi-agent systems, but also provide guidance for the design of diversity-based MARL algorithms. In this paper, we propose the multi-agent policy distance (MAPD), a general tool for measuring policy differences in MARL. By learning the conditional representations of agents&#39; decisions, MAPD can computes the policy distance between any pair of agents. Furthermore, we extend MAPD to a customizable version, which can quantify differences among agent policies on specified aspects. Based on the online deployment of MAPD, we design a multi-agent dynamic parameter sharing (MADPS) algorithm as an example of the MAPD&#39;s applications. Extensive experiments demonstrate that our method is effective in measuring differences in agent policies and specific behavioral tendencies. Moreover, in comparison to other methods of parameter sharing, MADPS exhibits superior performance.},
  archive   = {C_AAMAS},
  author    = {Hu, Tianyi and Pu, Zhiqiang and Ai, Xiaolin and Qiu, Tenghai and Yi, Jianqiang},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {834–842},
  title     = {Measuring policy distance for multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662937},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tight approximations for graphical house allocation.
<em>AAMAS</em>, 825–833. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Graphical House Allocation problem asks: how can n houses (each with a fixed non-negative value) be assigned to the n vertices of an undirected graph G, so as to minimize the &quot;aggregate local envy&#39;&#39;, i.e., the sum of absolute differences along the edges of G? This problem generalizes the classical Minimum Linear Arrangement problem, as well as the well-known House Allocation Problem from Economics, the latter of which has notable practical applications in organ exchanges. Recent work has studied the computational aspects of Graphical House Allocation and observed that the problem is NP-hard and inapproximable even on particularly simple classes of graphs, such as vertex disjoint unions of paths. However, the dependence of any approximations on the structural properties of the underlying graph had not been studied.In this work, we give a complete characterization of the approximability of Graphical House Allocation. We present algorithms to approximate the optimal envy on general graphs, trees, planar graphs, bounded-degree graphs, bounded-degree planar graphs, and bounded-degree trees. For each of these graph classes, we then prove matching lower bounds, showing that in each case, no significant improvement can be attained unless P = NP. We also present general approximation ratios as a function of structural parameters of the underlying graph, such as treewidth; these match the aforementioned tight upper bounds in general, and are significantly better approximations for many natural subclasses of graphs. Finally, we present constant factor approximation schemes for the special classes of complete binary trees and random graphs.Some of the technical highlights of our work are the use of expansion properties of Ramanujan graphs in the context of a classical resource allocation problem, and approximating optimal cuts in binary trees by analyzing the behavior of consecutive runs in bitstrings.},
  archive   = {C_AAMAS},
  author    = {Hosseini, Hadi and McGregor, Andrew and Sengupta, Rik and Vaish, Rohit and Viswanathan, Vignesh},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {825–833},
  title     = {Tight approximations for graphical house allocation},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662936},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Facility location games with scaling effects.
<em>AAMAS</em>, 816–824. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We take the classic facility location problem and consider a variation, in which each agent&#39;s individual cost function is equal to their distance from the facility multiplied by a scaling factor which is determined by the facility placement. In addition to the general class of continuous scaling functions, we also provide results for piecewise linear scaling functions which can effectively approximate or model the scaling of many real world scenarios. We focus on the objectives of total and maximum cost, describing the computation of the optimal solution. We then move to the approximate mechanism design setting, observing that the agents&#39; preferences may no longer be single-peaked. Consequently, we characterize the conditions on scaling functions which ensure that agents have single-peaked preferences. Under these conditions, we find results on the total and maximum cost approximation ratios achievable by strategyproof and anonymous mechanisms.},
  archive   = {C_AAMAS},
  author    = {He, Yu and Lam, Alexander and Li, Minming},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {816–824},
  title     = {Facility location games with scaling effects},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662935},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Solving two-player games with QBF solvers in general game
playing. <em>AAMAS</em>, 807–815. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Game solving is a relatively less explored area in general game playing. This paper introduces a translation from the Game Description Language GDL to Quantified Boolean Formulas (QBF) that lets us leverage QBF solvers to compute winning strategies in two-player games described in GDL. We implement this approach and measure the computation time needed by state-of-the-art QBF solvers on a range of two-player zero-sum turn-taking games. We introduce a variety of optimizations to the translation and evaluate them experimentally. Our empirical analysis establishes that our proposed approach is suitable for solving small games and can potentially help general game players evaluate endgame positions.},
  archive   = {C_AAMAS},
  author    = {He, Yifan and Saffidine, Abdallah and Thielscher, Michael},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {807–815},
  title     = {Solving two-player games with QBF solvers in general game playing},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662934},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Forecasting and mitigating disruptions in public bus transit
services. <em>AAMAS</em>, 798–806. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Public transportation systems often suffer from unexpected fluctuations in demand and disruptions, such as mechanical failures and medical emergencies. These fluctuations and disruptions lead to delays and overcrowding, which are detrimental to the passengers&#39; experience and to the overall performance of the transit service. To proactively mitigate such events, many transit agencies station substitute (reserve) vehicles throughout their service areas, which they can dispatch to augment or replace vehicles on routes that suffer overcrowding or disruption. However, determining the optimal locations where substitute vehicles should be stationed is a challenging problem due to the inherent randomness of disruptions and due to the combinatorial nature of selecting locations across a city. In collaboration with the transit agency of Nashville, TN, we address this problem by introducing data-driven statistical and machine-learning models for forecasting disruptions and an effective randomized local-search algorithm for selecting locations where substitute vehicles are to be stationed. Our research demonstrates promising results in proactive disruption management, offering a practical and easily implementable solution for transit agencies to enhance the reliability of their services. Our results resonate beyond mere operational efficiency-by advancing proactive strategies, our approach fosters more resilient and accessible public transportation, contributing to equitable urban mobility and ultimately benefiting the communities that rely on public transportation the most.},
  archive   = {C_AAMAS},
  author    = {Han, Chaeeun and Talusan, Jose Paolo and Freudberg, Dan and Mukhopadhyay, Ayan and Dubey, Abhishek and Laszka, Aron},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {798–806},
  title     = {Forecasting and mitigating disruptions in public bus transit services},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662933},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sample and communication efficient fully decentralized MARL
policy evaluation via a new approach: Local TD update. <em>AAMAS</em>,
789–797. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In actor-critic framework for fully decentralized multi-agent reinforcement learning (MARL), one of the key components is the MARL policy evaluation (PE) problem, where a set of N agents work cooperatively to evaluate the value function of the global states for a given policy through communicating with their neighbors. In MARL-PE, a critical challenge is how to lower the sample and communication complexities, which are defined as the number of training samples and communication rounds needed to converge to some ε-stationary point. To lower communication complexity in MARL-PE, a &#39;&#39;natural&#39;&#39; idea is to perform multiple local TD-update steps between each consecutive rounds of communication to reduce the communication frequency. However, the validity of the local TD-update approach remains unclear due to the potential &#39;&#39;agent-drift&#39;&#39; phenomenon resulting from heterogeneous rewards across agents in general. This leads to an interesting open question: Can the local TD-update approach entail low sample and communication complexities? In this paper, we make the first attempt to answer this fundamental question. We focus on the setting of MARL-PE with average reward, which is motivated by many multi-agent network optimization problems. Our theoretical and experimental results confirm that allowing multiple local TD-update steps is indeed an effective approach in lowering the sample and communication complexities of MARL-PE compared to consensus-based MARL-PE algorithms. Specifically, the local TD-update steps between two consecutive communication rounds can be as large as O (1/ε1/2 log(1/ε)) in order to converge to an ε-stationary point of MARL-PE. Moreover, we show theoretically that in order to reach the optimal sample complexity, the communication complexity of local TD-update approach is O (1/ε1/2 log(1/ε)).},
  archive   = {C_AAMAS},
  author    = {Hairi and Zhang, Zifan and Liu, Jia},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {789–797},
  title     = {Sample and communication efficient fully decentralized MARL policy evaluation via a new approach: Local TD update},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662932},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weighted proportional allocations of indivisible goods and
chores: Insights via matchings. <em>AAMAS</em>, 780–788. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study fair allocation of indivisible goods and chores for agents with ordinal preferences and arbitrary entitlements. In the case of both goods and chores, we show that there always exist allocations that are weighted necessarily proportional up to one item WSD-PROP1, that is, allocations that are WPROP1 under all additive valuations consistent with agents&#39; ordinal preferences. We give a polynomial-time algorithm to find such allocations by reducing it to a problem of finding perfect matchings in a bipartite graph. We give a complete characterization of these allocations as extreme points of a perfect matching polytope. Using this polytope, we can optimize any linear objective function over all WSD-PROP1 allocations, for example, to find a min-cost WSD-PROP1 allocation of goods or most efficient WSD-PROP1 allocation of chores. Additionally, we show the existence and computation of sequencible (SEQ) WSD-PROP1 allocation using rank-maximal perfect matchings and show the incompatibility of Pareto optimality under all valuations with the WSD-PROP1 notion.We also consider the notion of Best-of-Both-Worlds (BoBW) fairness. Using our characterization, we give a polynomial-time algorithm to compute Ex-ante envy-free (WSD-EF) and Ex-post WSD-PROP1 allocations for both goods and chores.},
  archive   = {C_AAMAS},
  author    = {H.V., Vishwa Prakash and Nimbhorkar, Prajakta},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {780–788},
  title     = {Weighted proportional allocations of indivisible goods and chores: Insights via matchings},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662931},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Causal explanations for sequential decision-making in
multi-agent systems. <em>AAMAS</em>, 771–779. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present CEMA: Causal Explanations in Multi-A gent systems; a framework for creating causal natural language explanations of an agent&#39;s decisions in dynamic sequential multi-agent systems to build more trustworthy autonomous agents. Unlike prior work that assumes a fixed causal structure, CEMA only requires a probabilistic model for forward-simulating the state of the system. Using such a model, CEMA simulates counterfactual worlds that identify the salient causes behind the agent&#39;s decisions. We evaluate CEMA on the task of motion planning for autonomous driving and test it in diverse simulated scenarios. We show that CEMA correctly and robustly identifies the causes behind the agent&#39;s decisions, even when a large number of other agents is present, and show via a user study that CEMA&#39;s explanations have a positive effect on participants&#39; trust in autonomous vehicles and are rated as high as high-quality baseline explanations elicited from other participants. We release the collected explanations with annotations as the HEADD dataset.},
  archive   = {C_AAMAS},
  author    = {Gyevnar, Balint and Wang, Cheng and Lucas, Christopher G. and Cohen, Shay B. and Albrecht, Stefano V.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {771–779},
  title     = {Causal explanations for sequential decision-making in multi-agent systems},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662930},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). First 100 days of pandemic: An interplay of pharmaceutical,
behavioral and digital interventions - a study using agent based
modeling. <em>AAMAS</em>, 761–770. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pandemics, notably the recent COVID-19 outbreak, have impacted both public health and global economy. We need a profound understanding of disease progression and efficient response strategies to prepare for potential future outbreaks. In this paper, we emphasize the potential of Agent-Based Models (ABM) in capturing complex infection dynamics and understanding the impact of interventions. We simulate realistic pharmaceutical, behavioral, and digital interventions and suggest a holistic combination of these interventions for pandemic response. We study the trends of emergent behavior on a large-scale population based on real-world socio-demographic and geo-census data from Kings County in Washington. Our analysis reveals the pivotal role of the initial 100 days in dictating a pandemic&#39;s course, emphasizing the importance of quick decision-making and efficient policy development. Further, we highlight that investing in behavioral and digital interventions can reduce the burden on pharmaceutical interventions by reducing the total number of hospitalizations, and by delaying the pandemic&#39;s peak. We also infer that allocating the same amount of dollars towards extensive testing with contact tracing and self-quarantine offers greater cost efficiency compared to spending the entire budget on vaccinations. Our code: https://github.com/mitmedialab/DeepABM-Pandemic/.},
  archive   = {C_AAMAS},
  author    = {Gupta, Gauri and Kapila, Ritvik and Chopra, Ayush and Raskar, Ramesh},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {761–770},
  title     = {First 100 days of pandemic: An interplay of pharmaceutical, behavioral and digital interventions - a study using agent based modeling},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662929},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cooperation and coordination in heterogeneous populations
with interaction diversity. <em>AAMAS</em>, 752–760. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cooperation, a prosocial behavior enhancing collective rewards in multi-agent games, intricately intertwines with coordination. This study explores how interaction diversity and zero-sum gifting influence cooperation and coordination in heterogeneous populations, where agents engage in threshold public goods games with multiple equilibria. Our model accommodates two sources of inequality: variations in agents&#39; capabilities to provide public goods and differences in the rewards they receive upon successful public good provision. In the absence of gifting, we demonstrate the inevitability of intermediate interaction intensity in fostering global cooperation, elucidating conditions for co-dominance, coexistence, and the polarized state of cooperation. While gifting introduces reciprocity opportunities, our findings highlight the importance of maintaining moderate levels of gifting, as excessive gifting can paradoxically undermine global cooperation. This research contributes valuable insights into the emergence of cooperation and coordination dynamics.},
  archive   = {C_AAMAS},
  author    = {Guo, Hao and Wang, Zhen and Xing, Junliang and Tao, Pin and Shi, Yuanchun},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {752–760},
  title     = {Cooperation and coordination in heterogeneous populations with interaction diversity},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662928},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cost-aware offline safe meta reinforcement learning with
robust in-distribution online task adaptation. <em>AAMAS</em>, 743–751.
(<a href="https://dl.acm.org/doi/10.5555/3635637.3662927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite the gained prominence made by reinforcement learning (RL) in various domains, ensuring safety in real-world applications remains a significant challenge. Offline safe RL, which learns safe policies from pre-collected data, has emerged to address these concerns. However, existing approaches assume a single constraint mode and lack adaptability to diverse safety constraints. In real-world scenarios, we often find ourselves working with datasets gathered from various tasks, with the aim of developing a generalized policy capable of handling unknown tasks during testing. To deal with such offline safe meta RL problem, we introduce a novel framework called COSTA, which is designed to facilitate the learning of a safe generalized policy that can adapt and be transferred to unknown tasks during testing. COSTA addresses two key challenges in offline safe meta RL: First, it develops a cost-aware task inference module using contrastive learning to distinguish tasks based on safety constraints, mitigating the MDP ambiguity problem. Second, COSTA introduces a novel metric, Safe In-Distribution Score (SIDS), to assess the in-distribution degree of trajectories, out of the consideration of both reward maximization and cost constraint satisfaction. Trajectories collected with a safe exploration policy are filtered using SIDS for robust online task adaptation. Experimental results in a tailored benchmark suite within the Mujoco environments demonstrate that COSTA consistently balances safety and reward maximization, outperforming multiple baselines.},
  archive   = {C_AAMAS},
  author    = {Guan, Cong and Xue, Ruiqi and Zhang, Ziqian and Li, Lihe and Li, Yi-Chen and Yuan, Lei and Yu, Yang},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {743–751},
  title     = {Cost-aware offline safe meta reinforcement learning with robust in-distribution online task adaptation},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662927},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MaDi: Learning to mask distractions for generalization in
visual deep reinforcement learning. <em>AAMAS</em>, 733–742. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The visual world provides an abundance of information, but many input pixels received by agents often contain distracting stimuli. Autonomous agents need the ability to distinguish useful information from task-irrelevant perceptions, enabling them to generalize to unseen environments with new distractions. Existing works approach this problem using data augmentation or large auxiliary networks with additional loss functions. We introduce MaDi, a novel algorithm that learns to mask distractions by the reward signal only. In MaDi, the conventional actor-critic structure of deep reinforcement learning agents is complemented by a small third sibling, the Masker. This lightweight neural network generates a mask to determine what the actor and critic will receive, such that they can focus on learning the task. The masks are created dynamically, depending on the current input. We run experiments on the DeepMind Control Generalization Benchmark, the Distracting Control Suite, and a real UR5 Robotic Arm. Our algorithm improves the agent&#39;s focus with useful masks, while its efficient Masker network only adds 0.2\% more parameters to the original structure, in contrast to previous work. MaDi consistently achieves generalization results better than or competitive to state-of-the-art methods.},
  archive   = {C_AAMAS},
  author    = {Grooten, Bram and Tomilin, Tristan and Vasan, Gautham and Taylor, Matthew E. and Mahmood, A. Rupam and Fang, Meng and Pechenizkiy, Mykola and Mocanu, Decebal Constantin},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {733–742},
  title     = {MaDi: Learning to mask distractions for generalization in visual deep reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662926},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforcement learning with ensemble model predictive safety
certification. <em>AAMAS</em>, 724–732. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning algorithms need exploration to learn. However, unsupervised exploration prevents the deployment of such algorithms on safety-critical tasks and limits real-world deployment. In this paper, we propose a new algorithm called Ensemble Model Predictive Safety Certification that combines model-based deep reinforcement learning with tube-based model predictive control to correct the actions taken by a learning agent, keeping safety constraint violations at a minimum through planning. Our approach aims to reduce the amount of prior knowledge about the actual system by requiring only offline data generated by a safe controller. Our results show that we can achieve significantly fewer constraint violations than comparable reinforcement learning methods.},
  archive   = {C_AAMAS},
  author    = {Gronauer, Sven and Haider, Tom and Schmoeller da Roza, Felippe and Diepold, Klaus},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {724–732},
  title     = {Reinforcement learning with ensemble model predictive safety certification},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662925},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Symbolic computation of sequential equilibria.
<em>AAMAS</em>, 715–723. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The sequential equilibrium is a standard solution concept for extensive-form games with imperfect information that includes an explicit representation of the players&#39; beliefs. An assessment consisting of a strategy and a belief is a sequential equilibrium if it satisfies the properties of sequential rationality and consistency.Our main result is that both properties together can be written as a single finite system of polynomial equations and inequalities. The solutions to this system are exactly the sequential equilibria of the game. We construct this system explicitly and describe an implementation that solves it using cylindrical algebraic decomposition. To write consistency as a finite system of equations, we need to compute the extreme directions of a set of polyhedral cones. We propose a modified version of the double description method, optimized for this specific purpose. To the best of our knowledge, our implementation is the first to symbolically solve general finite imperfect information games for sequential equilibria.},
  archive   = {C_AAMAS},
  author    = {Graf, Moritz and Engesser, Thorsten and Nebel, Bernhard},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {715–723},
  title     = {Symbolic computation of sequential equilibria},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662924},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nash stability in hedonic skill games. <em>AAMAS</em>,
706–714. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This article deals with hedonic skill games, the strategic counterpart of coalitional skill games which model collaboration among entities through the abstract notions of tasks and the skills required to complete them. We show that deciding whether an instance of the game admits a Nash stable outcome is NP-complete in the weighted tasks setting. We then characterize the instances admitting a Nash stable outcome in the weighted tasks setting. This characterization relies on the fact that every agent holds (resp., every task requires) either a single skill or more than one skill. For these instances, the complexity of computing a Nash stable outcome is determined, together with the possibility that a natural dynamics converges to a Nash stable outcome from any initial configuration. Our study is completed with a thorough analysis of the price of anarchy of instances always admitting a Nash stable outcome.},
  archive   = {C_AAMAS},
  author    = {Gourv\`{e}s, Laurent and Monaco, Gianpiero},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {706–714},
  title     = {Nash stability in hedonic skill games},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662923},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Capacity modification in the stable matching problem.
<em>AAMAS</em>, 697–705. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of capacity modification in the many-to-one stable matching of workers and firms. Our goal is to systematically study how the set of stable matchings changes when some seats are added to or removed from the firms. We make three main contributions: First, we examine whether firms and workers can improve or worsen upon changing the capacities under worker-proposing and firm-proposing deferred acceptance algorithms. Second, we study the computational problem of adding or removing seats to either match a fixed worker-firm pair in some stable matching or make a fixed matching stable with respect to the modified problem. We develop polynomial-time algorithms for these problems when only the overall change in the firms&#39; capacities is restricted, and show NP-hardness when there are additional constraints for individual firms. Lastly, we compare capacity modification with the classical model of preference manipulation by firms and identify scenarios under which one mode of manipulation outperforms the other. We find that a threshold on a given firm&#39;s capacity, which we call its peak, crucially determines the effectiveness of different manipulation actions.},
  archive   = {C_AAMAS},
  author    = {Gokhale, Salil and Singla, Samarth and Narang, Shivika and Vaish, Rohit},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {697–705},
  title     = {Capacity modification in the stable matching problem},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662922},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NovelGym: A flexible ecosystem for hybrid planning and
learning agents designed for open worlds. <em>AAMAS</em>, 688–696. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As AI agents leave the lab and venture into the real world as autonomous vehicles, delivery robots, and cooking robots, it is increasingly necessary to design and comprehensively evaluate algorithms that tackle the &quot;open-world&#39;&#39;. To this end, we introduce NovelGym, a flexible and adaptable ecosystem designed to simulate gridworld environments, serving as a robust platform for benchmarking reinforcement learning (RL) and hybrid planning and learning agents in open-world contexts. The modular architecture of NovelGym facilitates rapid creation and modification of task environments, including multi-agent scenarios, with multiple environment transformations, thus providing a dynamic testbed for researchers to develop open-world AI agents.},
  archive   = {C_AAMAS},
  author    = {Goel, Shivam and Wei, Yichen and Lymperopoulos, Panagiotis and Chur\&#39;{a}, Kl\&#39;{a}ra and Scheutz, Matthias and Sinapov, Jivko},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {688–696},
  title     = {NovelGym: A flexible ecosystem for hybrid planning and learning agents designed for open worlds},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662921},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modelling the rise and fall of two-sided markets.
<em>AAMAS</em>, 679–687. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Two-sided markets disrupted our economies, reshaping markets as diverse as tourism (airbnb), mobility (Uber) and food deliveries (UberEats). New market leaders arose leveraging on platform-based business model, questioning well-established paradigms. The underlying processes behind their growth are non-trivial, inherently microscopic, and leverage on complex human interactions. Platforms need to reach critical mass of both supply and demand to trigger the so-called cross-sided network effects.To this end, platforms adopt a variety of strategies to first create the market, then expand it and finally successfully compete with others. Such a complex social system with many non-linear interactions and learning processes calls for a dedicated modelling approach. State-of-the-art methods well estimate the macroscopic equilibrium conditions, but struggle to reproduce the complex growth patterns and individual human behaviour behind. To bridge this gap, we propose the microscopic S-shaped learning model where agents build their perception on the new service with time, affected by both endogenous (service quality) and exogenous (marketing and word-of-mouth) factors cumulated from experiences. We illustrate it with the case of two-sided mobility platform (Uber), where the platform applies a series of marketing actions leading to rise and then fall on the market where 200 drivers serve 2000 travellers on the complex urban network of Amsterdam.Our model is the first to reproduce not only behaviourally sound, but also empirically observed growth trajectories, it remains sensitive to a variety of marketing strategies, allows reproducing the competition between platforms and is designed to be integrated with machine learning algorithms to identify the optimal market entry strategy.},
  archive   = {C_AAMAS},
  author    = {Ghasemi, Farnoud and Kucharski, Rafal},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {679–687},
  title     = {Modelling the rise and fall of two-sided markets},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662920},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Approximating the core via iterative coalition sampling.
<em>AAMAS</em>, 669–678. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The core is a central solution concept in cooperative game theory, defined as the set of feasible allocations or payments such that no subset of agents has incentive to break away and form their own subgroup or coalition. However, it has long been known that the core (and approximations, such as the least-core) are hard to compute. This limits our ability to analyze cooperative games in general, and to fully embrace cooperative game theory contributions in domains such as explainable AI (XAI), where the core can complement the Shapley values to identify influential features or instances supporting predictions by black-box models. We propose novel iterative algorithms for computing variants of the core, which avoid the computational bottleneck of many other approaches; namely solving large linear programs. As such, they scale better to very large problems as we demonstrate across different classes of cooperative games, including weighted voting games, induced subgraph games, and marginal contribution networks. We also explore our algorithms in the context of XAI, providing further evidence of the power of the core for such applications.},
  archive   = {C_AAMAS},
  author    = {Gemp, Ian and Lanctot, Marc and Marris, Luke and Mao, Yiran and Du\&#39;{e}\~{n}ez-Guzm\&#39;{a}n, Edgar and Perrin, Sarah and Gyorgy, Andras and Elie, Romuald and Piliouras, Georgios and Kaisers, Michael and Hennes, Daniel and Bullard, Kalesha and Larson, Kate and Bachrach, Yoram},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {669–678},
  title     = {Approximating the core via iterative coalition sampling},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662919},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Deep reinforcement learning with coalition action selection
for online combinatorial resource allocation with arbitrary action
space. <em>AAMAS</em>, 660–668. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current DRL algorithms typically assume a fixed number of possible actions and sequentially select one action at a time, making them inefficient for resource allocation problems with arbitrarily large action spaces. Sequential action selection requires updating the state for every action selected, which increases the depth of the decision, the state space, the uncertainty, and the number of executions. This affects the convergence of the algorithm and slows the execution speed. Additionally, current DRL algorithms are not efficient for online resource allocation problems with an arbitrary number of task arrivals per time step because they assume a fixed number of actions. To address these challenges, we propose a novel coalition action selection approach that enables the DRL algorithm to simultaneously select a coalition of an arbitrary number of actions from a set with an arbitrary number of possible actions. By making simultaneous decisions at each time step, coalition action selection avoids the computational cost and large state space caused by the sequential decision that updates the state multiple times. We evaluate the performance and complexity of coalition action selection and sequential action selection approaches using an online combinatorial resource allocation problem. The results demonstrate that the coalition action selection approach retains close performance to the offline optimal for various online traffic demand arrival rates of the online combinatorial resource allocation problem, while the performance of the sequential action selection approach decreases as the size of the problem increases. The experiments also demonstrate that coalition action selection has much lower computational complexity than sequential action selection.},
  archive   = {C_AAMAS},
  author    = {Gebrekidan, Tesfay Zemuy and Stein, Sebastian and Norman, Timothy J.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {660–668},
  title     = {Deep reinforcement learning with coalition action selection for online combinatorial resource allocation with arbitrary action space},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662918},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incentives for early arrival in cooperative games.
<em>AAMAS</em>, 651–659. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study cooperative games where players join sequentially, and the value generated by those who have joined at any point must be irrevocably divided among these players. We introduce two desiderata for the value division mechanism: that the players should have incentives to join as early as possible, and that the division should be considered fair. For the latter, we require that each player&#39;s expected share in the mechanism should equal her Shapley value if the players&#39; arrival order is uniformly at random.When the value generation function is submodular, allocating the marginal value to the player satisfies these properties. This is no longer true for more general functions. Our main technical contribution is a complete characterization of 0-1 value games for which desired mechanisms exist. We show that a natural mechanism, Rewarding First Critical Player (RFC), is complete, in that a 0-1 value function admits a mechanism with the properties above if and only if RFC satisfies them; we analytically characterize all such value functions. Moreover, we give an algorithm that decomposes, in an online fashion, any value function into 0-1 value functions, on each of which RFC can be run. In this way, we design an extension of RFC for general monotone games, and the properties are proved to be maintained.},
  archive   = {C_AAMAS},
  author    = {Ge, Yaoxin and Zhang, Yao and Zhao, Dengji and Tang, Zhihao Gavin and Fu, Hu and Lu, Pinyan},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {651–659},
  title     = {Incentives for early arrival in cooperative games},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662917},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Surge routing: Event-informed multiagent reinforcement
learning for autonomous rideshare. <em>AAMAS</em>, 641–650. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large events such as conferences, concerts and sports games, often cause surges in demand for ride services that are not captured in average demand patterns, posing unique challenges for routing algorithms. We propose a learning framework for an autonomous fleet of taxis that leverages event data from the internet to predict demand surges and generate cooperative routing policies. We achieve this through a combination of two major components: (i) a demand prediction framework that uses textual event information in the form of events&#39; descriptions and reviews to predict event-driven demand surges over street intersections, and (ii) a scalable multiagent reinforcement learning framework that leverages demand predictions and uses one-agent-at-a-time rollout, combined with limited sampling certainty equivalence, to learn intersection-level routing policies. For our experimental results we consider real NYC ride share data for the year 2022 and information for more than 2000 events across 300 unique venues in Manhattan. We test our approach with a fleet of 100 taxis on a map with 2235 street intersections. Our experimental results demonstrate that our method learns routing policies that reduce wait time overhead per serviced request by 25\% to 75\%, while picking up 1\% to 4\% more requests than other model-based RL frameworks and classical methods from operations research.},
  archive   = {C_AAMAS},
  author    = {Garces, Daniel and Gil, Stephanie},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {641–650},
  title     = {Surge routing: Event-informed multiagent reinforcement learning for autonomous rideshare},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662916},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RACCER: Towards reachable and certain counterfactual
explanations for reinforcement learning. <em>AAMAS</em>, 632–640. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While reinforcement learning (RL) algorithms have been successfully applied to numerous tasks, their reliance on neural networks makes their behavior difficult to understand and trust. Counterfactual explanations are human-friendly explanations that offer users actionable advice on how to alter the model inputs to achieve the desired output from a black-box system. However, current approaches to generating counterfactuals in RL ignore the stochastic and sequential nature of RL tasks and can produce counterfactuals that are difficult to obtain or do not deliver the desired outcome. In this work, we propose RACCER, the first RL-specific approach to generating counterfactual explanations for the behavior of RL agents. We first propose and implement a set of RL-specific counterfactual properties that ensure easily reachable counterfactuals with highly probable desired outcomes. We use a heuristic tree search of the agent&#39;s execution trajectories to find the most suitable counterfactuals based on the defined properties. We evaluate RACCER in two tasks as well as conduct a user study to show that RL-specific counterfactuals help users better understand agents&#39; behavior compared to the current state-of-the-art approaches.},
  archive   = {C_AAMAS},
  author    = {Gajcin, Jasmina and Dusparic, Ivana},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {632–640},
  title     = {RACCER: Towards reachable and certain counterfactual explanations for reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662915},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analysing the sample complexity of opponent shaping.
<em>AAMAS</em>, 623–631. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning in general-sum games often yields collectively sub-optimal results. Addressing this, opponent shaping (OS) methods actively guide the learning processes of other agents, empirically leading to improved individual and group performances. Early OS methods use higher-order derivatives to shape the learning of co-players, making them unsuitable to shape multiple learning steps. Follow-up work Model-free Opponent Shaping (M-FOS) addresses the short-comings of earlier OS methods by reframing the OS problem into a meta-game. In contrast to early OS methods, there is little theoretical understanding of the M-FOS framework. Providing theoretical guarantees for M-FOS is hard because A) there is little literature on theoretical sample complexity bounds for meta-reinforcement learning; B) M-FOS operates in continuous state and action spaces hence theoretical analysis is challenging. In this work, we present R-FOS, a tabular version of M-FOS that is more suitable for theoretical analysis. R-FOS discretises the continuous meta-game MDP into a tabular MDP. Within this discretised MDP, we adapt the Rmax algorithm, most prominently used to derive PAC-bounds for MDPs, as the meta-learner in the R-FOS algorithm. We derive a sample complexity bound that is exponential in the cardinality of the inner state and action space and the number of agents. Our bound guarantees that, with high probability, the optimal policy learned by an R-FOS agent can get close to the optimal policy, apart from a constant factor. Finally, we investigate how R-FOS&#39;s sample complexity scales in the size of state-action space. Our theoretical results on scaling are supported empirically in the Matching Pennies environment.},
  archive   = {C_AAMAS},
  author    = {Fung, Kitty and Zhang, Qizhen and Lu, Chris and Wan, Jia and Willi, Timon and Foerster, Jakob},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {623–631},
  title     = {Analysing the sample complexity of opponent shaping},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662914},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From market saturation to social reinforcement:
Understanding the impact of non-linearity in information diffusion
models. <em>AAMAS</em>, 614–622. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Diffusion of information in networks is at the core of many problems in AI. Common examples include the spread of ideas and rumors as well as marketing campaigns. Typically, information diffuses at a non-linear rate, for example, if markets become saturated or if users of social networks reinforce each other&#39;s opinions. Despite these characteristics, this area has seen little research, compared to the vast amount of results for linear models, which exhibit less complex dynamics. Especially, when considering the possibility of re-infection, no fully rigorous guarantees exist so far.We address this shortcoming by studying a very general non-linear diffusion model that captures saturation as well as reinforcement. More precisely, we consider a variant of the SIS model in which vertices get infected at a rate that scales polynomially in the number of their infected neighbors, weighted by an infection coefficient λ. We give the first fully rigorous results for thresholds of λ at which the expected survival time becomes super-polynomial. For cliques we show that when the infection rate scales sub-linearly, the threshold only shifts by a poly-logarithmic factor, compared to the standard SIS model. In contrast, super-linear scaling changes the process considerably and shifts the threshold by a polynomial term. For stars, sub-linear and super-linear scaling behave similar and both shift the threshold by a polynomial factor. Our bounds are almost tight, as they are only apart by at most a poly-logarithmic factor from the lower thresholds, at which the expected survival time is logarithmic.},
  archive   = {C_AAMAS},
  author    = {Friedrich, Tobias and G\&quot;{o}bel, Andreas and Klodt, Nicolas and Krejca, Martin S. and Pappik, Marcus},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {614–622},
  title     = {From market saturation to social reinforcement: Understanding the impact of non-linearity in information diffusion models},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662913},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BrainSLAM: SLAM on neural population activity data.
<em>AAMAS</em>, 607–613. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simultaneous localisation and mapping (SLAM) algorithms are commonly used in robotic systems for learning maps of novel environments. Brains also appear to learn maps, but the mechanisms are not known and it is unclear how to infer these maps from neural activity data. We present BrainSLAM; a method for performing SLAM using only population activity (local field potential, LFP) data simultaneously recorded from three brain regions in rats: hippocampus, prefrontal cortex, and parietal cortex. This system uses a convolutional neural network (CNN) to decode velocity and familiarity information from wavelet scalograms of neural local field potential data recorded from rats as they navigate a 2D maze. The CNN&#39;s output drives a RatSLAM-inspired architecture, powering an attractor network which performs path integration plus a separate system which performs &#39;loop closure&#39; (detecting previously visited locations and correcting map aliasing errors). Together, these three components can construct faithful representations of the environment while simultaneously tracking the animal&#39;s location. This is the first demonstration of inference of a spatial map from brain recordings. Our findings expand SLAM to a new modality, enabling a new method of mapping environments and facilitating a better understanding of the role of cognitive maps in navigation and decision making.},
  archive   = {C_AAMAS},
  author    = {Freud, Kipp and Lepora, Nathan and Jones, Matt W. and O&#39;Donnell, Cian},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {607–613},
  title     = {BrainSLAM: SLAM on neural population activity data},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662912},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning complex teamwork tasks using a given sub-task
decomposition. <em>AAMAS</em>, 598–606. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Training a team to complete a complex task via multi-agent reinforcement learning can be difficult due to challenges such as policy search in a large joint policy space, and non-stationarity caused by mutually adapting agents. To facilitate efficient learning of complex multi-agent tasks, we propose an approach which uses an expert-provided decomposition of a task into simpler multi-agent sub-tasks. In each sub-task, a subset of the entire team is trained to acquire sub-task-specific policies. The sub-teams are then merged and transferred to the target task, where their policies are collectively fine-tuned to solve the more complex target task. We show empirically that such approaches can greatly reduce the number of timesteps required to solve a complex target task relative to training from-scratch. However, we also identify and investigate two problems with naive implementations of approaches based on sub-task decomposition, and propose a simple and scalable method to address these problems which augments existing actor-critic algorithms. We demonstrate the empirical benefits of our proposed method, enabling sub-task decomposition approaches to be deployed in diverse multi-agent tasks.},
  archive   = {C_AAMAS},
  author    = {Fosong, Elliot and Rahman, Arrasy and Carlucho, Ignacio and Albrecht, Stefano V.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {598–606},
  title     = {Learning complex teamwork tasks using a given sub-task decomposition},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662911},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Potential-based reward shaping for intrinsic motivation.
<em>AAMAS</em>, 589–597. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently there has been a proliferation of intrinsic motivation (IM) reward-shaping methods to learn in complex and sparse-reward environments. These methods can often inadvertently change the set of optimal policies in an environment, leading to suboptimal behavior. Previous work on mitigating the risks of reward shaping, particularly through potential-based reward shaping (PBRS), has not been applicable to many IM methods, as they are often complex, trainable functions themselves, and therefore dependent on a wider set of variables than the traditional reward functions that PBRS was developed for. We present an extension to PBRS that we prove preserves the set of optimal policies under a more general set of functions than has been previously proven. We also present Potential-Based Intrinsic Motivation (PBIM), a method for converting IM rewards into a potential-based form that is useable without altering the set of optimal policies. Testing in the MiniGrid DoorKey and Cliff Walking environments, we demonstrate that PBIM successfully prevents the agent from converging to a suboptimal policy and can speed up training.},
  archive   = {C_AAMAS},
  author    = {Forbes, Grant C. and Gupta, Nitish and Villalobos-Arias, Leonardo and Potts, Colin M. and Jhala, Arnav and Roberts, David L.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {589–597},
  title     = {Potential-based reward shaping for intrinsic motivation},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662910},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Preventing deadlocks for multi-agent pickup and delivery in
dynamic environments. <em>AAMAS</em>, 580–588. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Multi-Agent Pickup and Delivery (MAPD) problem, in which a team of agents has to plan paths to accomplish incoming pickup and delivery tasks without collisions, has recently attracted significant attention both from academia and industry. In this paper, we consider a MAPD setting in which the environment is dynamic, namely it is populated by other moving agents, beyond those belonging to the team. For instance, in a warehouse, moving agents could be humans or cleaning robots. We assume that the team agents cannot communicate with the moving agents and cannot interfere with their tasks and paths, which are a priori unknown and cannot be modified. As a consequence, team agents have to reactively try to solve potential collisions when they appear. However, it can happen that some conflicts are not solvable without affecting the moving agents, resulting in deadlocks. Since deadlocks can become rather frequent, especially in crowded environments, in this paper we propose an approach that, by imposing minor constraints on the environment and the movements of the agents, solves potential collisions and prevents the formation of deadlocks by design. Experimental results show that our approach prevents deadlocks, even in very crowded environments, with negligible impact on the performance of task completion.},
  archive   = {C_AAMAS},
  author    = {Flammini, Benedetta and Azzalini, Davide and Amigoni, Francesco},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {580–588},
  title     = {Preventing deadlocks for multi-agent pickup and delivery in dynamic environments},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662909},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Probabilistic multi-agent only-believing. <em>AAMAS</em>,
571–579. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Levesque introduced the notion of only-knowing to precisely capture the beliefs of a knowledge base. While numerous studies of only-knowing have emerged, such as the representation of probabilistic beliefs or reasoning about beliefs in an uncertain dynamical system, most remain confined to single-agent contexts. This limitation predominantly stems from an absence of a logical framework, which faithfully extends Levesque&#39;s intuition of only-knowing to multi-agent, probabilistic scenarios.In this paper, we introduce a first-order logical account with probabilistic beliefs and only-believing of many agents. We demonstrate that the categorical fragment of our account forms a KD45n modal system, and the notion of belief has behavior following the laws of probability. We also show how an agent&#39;s beliefs and non-beliefs about the environment or other agents&#39; beliefs are precisely captured through the modalities of only-believing, which paves the way to generalize tools for interfacing with symbolic, probabilistic knowledge bases. By way of example, we demonstrate how non-monotonic conclusions including default reasoning can be handled by our account.},
  archive   = {C_AAMAS},
  author    = {Feng, Qihui and Lakemeyer, Gerhard},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {571–579},
  title     = {Probabilistic multi-agent only-believing},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662908},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalized strategy synthesis of infinite-state impartial
combinatorial games via exact binary classification. <em>AAMAS</em>,
562–570. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In game theory, a fundamental class of games is impartial combinatorial games (ICGs). One of the challenging and long-standing problems of ICGs is to compute generalized winning strategies for possibly infinite number of legal states. Recently, Wu et al. proposed an automated method to synthesize generalized winning strategies of infinite-state ICGs. Their method has two major drawbacks: (1) it fails to generate winning formula with large size; and (2) it cannot usually construct the winning strategy even the winning formula is obtained. To tackle the above two drawbacks, in this paper, we propose the problem of exact binary classification and design a partial MaxSAT-based method to this problem. Then, we reduce the synthesis problem of generalized winning strategies of infinite-state ICGs to exact binary classification. The experimental results show that our method is more scalable and effective than Wu et al.&#39;s approach.},
  archive   = {C_AAMAS},
  author    = {Fang, Liangda and Yang, Meihong and Cheng, Dingliang and Hao, Yunlai and Guan, Quanlong and Xiong, Liping},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {562–570},
  title     = {Generalized strategy synthesis of infinite-state impartial combinatorial games via exact binary classification},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662907},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Facility location games with fractional preferences and
limited resources. <em>AAMAS</em>, 553–561. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we study the heterogeneous facility location game with fractional preferences under resource constraints. In this model, a group of agents are positioned along the interval [0,1], where each agent has position information and fractional preferences indicated as support weights for facilities. Our main focus is to design mechanisms that choose and locate one facility out of two facilities while motivating agents to truthfully report their information, aiming to approximately maximize the social utility, defined as the sum of utilities of all agents. Based on the types of private information held by agents, we consider three different settings. For the known-preferences setting, we provide a deterministic group strategy-proof mechanism with 2-approximation and a randomized group strategy-proof mechanism with 4 over 3-approximation. We also provide lower bounds of 2 on the approximation ratio for any deterministic strategy-proof mechanism and 1.043 for any randomized strategy-proof mechanism. For the known-positions setting and the general setting, we present a deterministic group strategy-proof mechanism with 6-approximation and a randomized strategy-proof mechanism with 4-approximation, respectively. Furthermore, we give lower bounds of 1.554 for any deterministic strategy-proof mechanism and 1.2 for any randomized strategy-proof mechanism in the known-positions setting. Finally, we extend the model to the scenario of choosing k facilities out of m facilities. For the known-preferences setting, we provide a 2-approximate deterministic group strategy-proof mechanism, which is also the best deterministic strategy-proof mechanism. For the known-positions setting, when k ≥ 2, we give a lower bound of 2-1 over k for any deterministic strategy-proof mechanism.},
  archive   = {C_AAMAS},
  author    = {Fang, Jiazhu and Liu, Wenjing},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {553–561},
  title     = {Facility location games with fractional preferences and limited resources},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662906},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-level, collaborative task planning grammar and
execution for heterogeneous agents. <em>AAMAS</em>, 544–552. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a new multi-agent task grammar to encode collaborative tasks for a team of heterogeneous agents that can have overlapping capabilities. The grammar allows users to specify the relationship between agents and parts of the task without providing explicit assignments or constraints on the number of agents required. We develop a method to automatically find a team of agents and synthesize correct-by-construction control with synchronization policies to satisfy the task. We demonstrate the scalability of our approach through simulation and compare our method to existing task grammars that encode multi-agent tasks.},
  archive   = {C_AAMAS},
  author    = {Fang, Amy and Kress-Gazit, Hadas},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {544–552},
  title     = {High-level, collaborative task planning grammar and execution for heterogeneous agents},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662905},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning and calibrating heterogeneous bounded rational
market behaviour with multi-agent reinforcement learning.
<em>AAMAS</em>, 534–543. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Agent-based models (ABMs) have shown promise for modelling various real world phenomena incompatible with traditional equilibrium analysis. However, a critical concern is the manual definition of behavioural rules in ABMs. Recent developments in multi-agent reinforcement learning (MARL) offer a way to address this issue from an optimisation perspective, where agents strive to maximise their utility, eliminating the need for manual rule specification. This learning-focused approach aligns with established economic and financial models through the use of rational utility-maximising agents. However, this representation departs from the fundamental motivation for ABMs: that realistic dynamics emerging from bounded rationality and agent heterogeneity can be modelled. To resolve this apparent disparity between the two approaches, we propose a novel technique for representing heterogeneous processing-constrained agents within a MARL framework. The proposed approach treats agents as constrained optimisers with varying degrees of strategic skills, permitting departure from strict utility maximisation. Behaviour is learnt through repeated simulations with policy gradients to adjust action likelihoods. To allow efficient computation, we use parameterised shared policy learning with distributions of agent skill levels. Shared policy learning avoids the need for agents to learn individual policies yet still enables a spectrum of bounded rational behaviours. We validate our model&#39;s effectiveness using real-world data on a range of canonical n-agent settings, demonstrating significantly improved predictive capability.},
  archive   = {C_AAMAS},
  author    = {Evans, Benjamin Patrick and Ganesh, Sumitra},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {534–543},
  title     = {Learning and calibrating heterogeneous bounded rational market behaviour with multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662904},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Holonic learning: A flexible agent-based distributed machine
learning framework. <em>AAMAS</em>, 525–533. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ever-increasing ubiquity of data and computational resources in the last decade have propelled a notable transition in the machine learning paradigm towards more distributed approaches. Such a transition seeks to not only tackle the scalability and resource distribution challenges but also to address pressing privacy and security concerns. To contribute to the ongoing discourse, this paper introduces Holonic Learning (HoL), a collaborative and privacy-focused learning framework designed for training deep learning models. By leveraging holonic concepts, the HoL framework establishes a structured self-similar hierarchy in the learning process, enabling more nuanced control over collaborations through the individual model aggregation approach of each holon, along with their intra-holon commitment and communication patterns. HoL, in its general form, provides extensive design and flexibility potentials. For empirical analysis and to demonstrate its effectiveness, this paper implements HoloAvg, a special variant of HoL that employs weighted averaging for model aggregation across all holons. The convergence of the proposed method is validated through experiments on both identically and independently distributed (IID) and Non-IID settings of the standard MNIST dataset. Furthermore, the performance behaviors of HoL are investigated under various holarchical designs and data distribution scenarios. The presented results affirm HoL&#39;s prowess in delivering competitive performance particularly, in the context of the Non-IID data distribution.},
  archive   = {C_AAMAS},
  author    = {Esmaeili, Ahmad and Ghorrati, Zahra and Matson, Eric T.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {525–533},
  title     = {Holonic learning: A flexible agent-based distributed machine learning framework},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662903},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforcement learning in the wild with maximum
likelihood-based model transfer. <em>AAMAS</em>, 516–524. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we study the problem of transferring the available Markov Decision Process (MDP) models to learn and plan efficiently in an unknown but similar MDP. We refer to it as Model Transfer Reinforcement Learning (MTRL) problem. First, we formulate MTRL for discrete MDPs and Linear Quadratic Regulators (LQRs) with continuous state actions. Then, we propose a generic two-stage algorithm, MLEMTRL, to address the MTRL problem in discrete and continuous settings. In the first stage, MLEMTRL uses a constrained Maximum Likelihood Estimation (MLE) -based approach to estimate the target MDP model using a set of known MDP models. In the second stage, using the estimated target MDP model, MLEMTRL deploys a model-based planning algorithm appropriate for the MDP class. Theoretically, we prove worst-case regret bounds for MLEMTRL both in realisable and non-realisable settings. We empirically demonstrate that MLEMTRL allows faster learning in new MDPs than learning from scratch and achieves near-optimal performance depending on the similarity of the available MDPs and the target MDP.},
  archive   = {C_AAMAS},
  author    = {Eriksson, Hannes and Tram, Tommy and Basu, Debabrota and Alibeigi, Mina and Dimitrakakis, Christos},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {516–524},
  title     = {Reinforcement learning in the wild with maximum likelihood-based model transfer},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662902},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-agent reinforcement learning for assessing false-data
injection attacks on transportation networks. <em>AAMAS</em>, 508–515.
(<a href="https://dl.acm.org/doi/10.5555/3635637.3662901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The increasing reliance of drivers on navigation applications has made transportation networks more susceptible to data-manipulation attacks by malicious actors. Adversaries may exploit vulnerabilities in the data collection or processing of navigation services to inject false information, and to thus interfere with the drivers&#39; route selection. Such attacks can significantly increase traffic congestions, resulting in substantial waste of time and resources, and may even disrupt essential services that rely on road networks. To assess the threat posed by such attacks, we introduce a computational framework to find worst-case data-injection attacks against transportation networks. First, we devise an adversarial model with a threat actor who can manipulate drivers by increasing the travel times that they perceive on certain roads. Then, we employ hierarchical multi-agent reinforcement learning to find an approximate optimal adversarial strategy for data manipulation. We demonstrate the applicability of our approach through simulating attacks on the Sioux Falls, ND network topology.},
  archive   = {C_AAMAS},
  author    = {Eghtesad, Taha and Li, Sirui and Vorobeychik, Yevgeniy and Laszka, Aron},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {508–515},
  title     = {Multi-agent reinforcement learning for assessing false-data injection attacks on transportation networks},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662901},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Computational aspects of distortion. <em>AAMAS</em>,
499–507. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The distortion framework in social choice theory allows quantifying the efficiency of (randomized) selection of an alternative based on the preferences of a set of agents. We make two fundamental contributions to this framework.First, we develop a linear-programming-based algorithm for computing the optimal randomized decision on a given instance, which is simpler and faster than the state-of-the-art solutions. For practitioners who may prefer to deploy a classical decision-making rule over the aforementioned optimal rule, we develop an algorithm based on non-convex quadratic programming for computing the exact distortion of any (and the best) randomized positional scoring rule. For a small number of alternatives, we find that the exact distortion bounds are significantly better than the asymptotic bounds established in prior literature and lead to different recommendations on which rules to use.These results rely on a novel characterization of the instances yielding the worst distortion, which may be of independent interest.},
  archive   = {C_AAMAS},
  author    = {Ebadian, Soroush and Filos-Ratsikas, Aris and Latifian, Mohamad and Shah, Nisarg},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {499–507},
  title     = {Computational aspects of distortion},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662900},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Population synthesis as scenario generation for
simulation-based planning under uncertainty. <em>AAMAS</em>, 490–498.
(<a href="https://dl.acm.org/doi/10.5555/3635637.3662899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Agent-based models have the potential to become instrumental tools in real-world decision-making, equipping policy-makers with the ability to experiment with high-fidelity representations of complex systems. Such models often rely crucially on the generation of synthetic populations with which the model is simulated, and their behaviour can depend strongly on the population&#39;s composition. Existing approaches to synthesising populations attempt to model distributions over agent-level attributes on the basis of data collected from a real-world population. Unfortunately, these approaches are of limited utility when data is incomplete or altogether absent - such as during novel, unprecedented circumstances - so that considerable uncertainty regarding the characteristics of the population being modelled remains, even after accounting for any such data. What is therefore needed in these cases are tools to simulate and plan for the possible future behaviours of the complex system that can be generated by populations that are consistent with this remaining uncertainty. To this end, we frame the problem of synthesising populations in agent-based models as a problem of scenario generation. The framework that we present is designed to generate synthetic populations that are on the one hand consistent with any persisting uncertainty, while on the other hand matching closely a target, user-specified scenario that the decision-maker would like to explore and plan for. We propose and compare two generic approaches to generating synthetic populations that produce target scenarios, and demonstrate through simulation studies that these approaches are able to automatically generate synthetic populations whose behaviours match the target scenario, thereby facilitating simulation-based planning under uncertainty.},
  archive   = {C_AAMAS},
  author    = {Dyer, Joel and Quera-Bofarull, Arnau and Bishop, Nicholas and Farmer, J. Doyne and Calinescu, Anisoara and Wooldridge, Michael},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {490–498},
  title     = {Population synthesis as scenario generation for simulation-based planning under uncertainty},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662899},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic epistemic logic of resource bounded information
mining agents. <em>AAMAS</em>, 481–489. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Logics for resource-bounded agents have been getting more and more attention in recent years since they provide us with more realistic tools for modelling and reasoning about multi-agent systems. While many existing approaches are based on the idea of agents as imperfect reasoners, who must spend their resources to perform logical inference, this is not the only way to introduce resource constraints into logical settings. In this paper we study agents as perfect reasoners, who may purchase a new piece of information from a trustworthy source. For this purpose we propose dynamic epistemic logic for semi-public queries for resource-bounded agents. In this logic (groups of) agents can perform a query (ask a question) about whether some formula is true and receive a correct answer. These queries are called semi-public, because the very fact of the query is public, while the answer is private. We also assume that every query has a cost and every agent has a budget constraint. Finally, our framework allows us to reason about group queries, in which agents may share resources to obtain a new piece of information together. We demonstrate that our logic is complete, decidable and has an efficient model checking procedure.},
  archive   = {C_AAMAS},
  author    = {Dolgorukov, Vitaliy and Galimullin, Rustam and Gladyshev, Maksim},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {481–489},
  title     = {Dynamic epistemic logic of resource bounded information mining agents},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662898},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). It is among us: Identifying adversaries in ad-hoc domains
using q-valued bayesian estimations. <em>AAMAS</em>, 472–480. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ad-hoc teamwork models are crucial for solving distributed tasks in environments with unknown teammates. In order to improve performance, agents may collaborate in the same environment, trusting each other and exchanging information. However, what happens if there is an impostor among the team? In this paper, we present BAE, a novel and efficient framework for online planning and estimation within ad-hoc teamwork domains where there is an adversarial agent disguised as a teammate. Our approach considers the identification of the impostor through a process we term &quot;Q-valued Bayesian Estimation&#39;&#39;. BAE can identify the adversary at the same time the agent performs ad-hoc estimation in order to improve coordination. Our results show that BAE has superior accuracy and faster reasoning capabilities in comparison to the state-of-the-art.},
  archive   = {C_AAMAS},
  author    = {Do Carmo Alves, Matheus Aparecido and Varma, Amokh and Elkhatib, Yehia and Soriano Marcolino, Leandro},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {472–480},
  title     = {It is among us: Identifying adversaries in ad-hoc domains using Q-valued bayesian estimations},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662897},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gerrymandering planar graphs. <em>AAMAS</em>, 463–471. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the computational complexity of the map redistricting problem (gerrymandering). Mathematically, the electoral district designer (gerrymanderer) attempts to partition a weighted graph into k connected components (districts) such that its candidate (party) wins as many districts as possible. Prior work has principally concerned the special cases where the graph is a path or a tree. Our focus concerns the realistic case where the graph is planar. We prove that the gerrymandering problem is solvable in polynomial time in λ-outerplanar graphs, when the number of candidates and λ are constants and the vertex weights (voting weights) are polynomially bounded. In contrast, the problem is NP-complete in general planar graphs even with just two candidates. This motivates the study of approximation algorithms for gerrymandering planar graphs. However, when the number of candidates is large, we prove it is hard to distinguish between instances where the gerrymanderer cannot win a single district and instances where the gerrymanderer can win at least one district. This immediately implies that the redistricting problem is inapproximable in polynomial time in planar graphs, unless P=NP. This conclusion appears terminal for the design of good approximation algorithms - but it is not. The inapproximability bound can be circumvented as it only applies when the maximum number of districts the gerrymanderer can win is extremely small, say one. Indeed, for a fixed number of candidates, our main result is that there is a constant factor approximation algorithm for redistricting unweighted planar graphs, provided the optimal value is a large enough constant.},
  archive   = {C_AAMAS},
  author    = {Dippel, Jack and Dupr\&#39;{e} la Tour, Max and Niu, April and Roy, Sanjukta and Vetta, Adrian},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {463–471},
  title     = {Gerrymandering planar graphs},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662896},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continual optimistic initialization for value-based
reinforcement learning. <em>AAMAS</em>, 453–462. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Comprehensive state-action exploration is essential for reinforcement learning (RL) algorithms. It enables them to find optimal solutions and avoid premature convergence. In value-based RL, optimistic initialization of the value function ensures sufficient exploration for finding the optimal solution. Optimistic values lead to curiosity-driven exploration enabling visitation of under-explored regions. However, optimistic initialization has limitations in stochastic and non-stationary environments due to its inability to explore &#39;&#39;infinitely-often&#39;&#39;. To address this limitation, we propose a novel exploration strategy for value-based RL, denoted COIN, based on recurring optimistic initialization. By injecting a continual exploration bonus, we overcome the shortcoming of optimistic initialization (sensitivity to environment noise). We provide a rigorous theoretical comparison of COIN versus existing popular exploration strategies and prove it provides a unique set of attributes (coverage, infinite-often, no visitation tracking, and curiosity). We demonstrate the superiority of COIN over popular existing strategies on a designed toy domain as well as present results on common benchmark tasks. We observe that COIN outperforms existing exploration strategies in four out of six benchmark tasks while performing on par with the best baseline on the other two tasks.},
  archive   = {C_AAMAS},
  author    = {Dey, Sheelabhadra and Ault, James and Sharon, Guni},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {453–462},
  title     = {Continual optimistic initialization for value-based reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662895},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Informativeness of reward functions in reinforcement
learning. <em>AAMAS</em>, 444–452. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reward functions are central in specifying the task we want a reinforcement learning agent to perform. Given a task and desired optimal behavior, we study the problem of designing informative reward functions so that the designed rewards speed up the agent&#39;s convergence. In particular, we consider expert-driven reward design settings where an expert or teacher seeks to provide informative and interpretable rewards to a learning agent. Existing works have considered several different reward design formulations; however, the key challenge is formulating a reward informativeness criterion that adapts w.r.t. the agent&#39;s current policy and can be optimized under specified structural constraints to obtain interpretable rewards. In this paper, we propose a novel reward informativeness criterion, a quantitative measure that captures how the agent&#39;s current policy will improve if it receives rewards from a specific reward function. We theoretically showcase the utility of the proposed informativeness criterion for adaptively designing rewards for an agent. Experimental results on two navigation tasks demonstrate the effectiveness of our adaptive reward informativeness criterion.},
  archive   = {C_AAMAS},
  author    = {Devidze, Rati and Kamalaruban, Parameswaran and Singla, Adish},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {444–452},
  title     = {Informativeness of reward functions in reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662894},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward a quality model for hybrid intelligence teams.
<em>AAMAS</em>, 434–443. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hybrid Intelligence (HI) is an emerging paradigm in which artificial intelligence (AI) augments human intelligence. The current literature lacks systematic models that guide the design and evaluation of HI systems. Further, discussions around HI primarily focus on technology, neglecting the holistic human-AI ensemble. In this paper, we take the initial steps toward the development of a quality model for characterizing and evaluating HI systems from a human-AI teams perspective. We conducted a study investigating the adequacy of properties commonly associated with effective human teams to describe HI. Our study, featuring the insights of 50 HI researchers, shows that various human team properties, including boundedness, interdependence, competency, purposefulness, initiative, normativity, and effectiveness, are important for HI systems. Our study also reveals limitations in applying certain human team properties, such as coaching, rewards, and recognition, to HI systems due to the inherent human-AI asymmetry.},
  archive   = {C_AAMAS},
  author    = {Dell&#39;Anna, Davide and Murukannaiah, Pradeep K. and Dudzik, Bernd and Grossi, Davide and Jonker, Catholijn M. and Oertel, Catharine and Yolum, P\i{}nar},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {434–443},
  title     = {Toward a quality model for hybrid intelligence teams},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662893},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The parameterized complexity of welfare guarantees in
schelling segregation. <em>AAMAS</em>, 425–433. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Schelling&#39;s model considers k types of agents each of whom needs to select a vertex on an undirected graph, where every agent prefers neighboring agents of the same type. We are motivated by a recent line of work that studies solutions that are optimal with respect to notions related to the welfare of the agents. We explore the parameterized complexity of computing such solutions. We focus on the well-studied notions of social welfare (WO) and Pareto optimality (PO), alongside the recently proposed notions of group-welfare optimality (GWO) and utility-vector optimality (UVO), both of which lie between WO and ¶O. Firstly, we focus on the fundamental case where k=2 and there are r red agents and b blue agents. We show that all solution-notions we consider are intractable even when b=1 and that they do not admit an FPT algorithm when parameterized by r and b, unless FPT = W[1]. In addition, we show that WO and GWO remain intractable even on cubic graphs. We complement these negative results with an FPT algorithm parameterized by r, b and the maximum degree of the graph. For the general case with k types of agents, we prove that for any of the notions we consider the problem remains hard when parameterized by k for a large family of graphs that includes trees. We accompany these negative results with an XP algorithm parameterized by k and the treewidth of the graph.},
  archive   = {C_AAMAS},
  author    = {Deligkas, Argyrios and Eiben, Eduard and Goldsmith, Tiger-Lily},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {425–433},
  title     = {The parameterized complexity of welfare guarantees in schelling segregation},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662892},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Designing redistribution mechanisms for reducing transaction
fees in blockchains. <em>AAMAS</em>, 416–424. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Blockchains deploy Transaction Fee Mechanisms (TFMs) to determine which user transactions to include in blocks and determine their payments (i.e., transaction fees). Increasing demand and scarce block resources have led to high user transaction fees. As these blockchains are a public resource, it may be preferable to reduce these transaction fees. To this end, we introduce Transaction Fee Redistribution Mechanisms (TFRMs) - redistributing VCG payments collected from such TFMs as rebates to minimize transaction fees. Classic redistribution mechanisms (RMs) achieve this while ensuring Allocative Efficiency (AE) and User Incentive Compatibility (UIC). Our first result shows the non-triviality of applying RM in TFMs. More concretely, we prove that it is impossible to reduce transaction fees when (i) transactions that are not confirmed do not receive rebates and (ii) the miner can strategically manipulate the mechanism. Driven by this, we propose Robust TFRM (R-TFRM): a mechanism that compromises on an honest miner&#39;s individual rationality to guarantee strictly positive rebates to the users. We then introduce Robust and Rational TFRM (R2f-TFRM) that uses trusted on-chain randomness that additionally guarantees miner&#39;s individual rationality (in expectation) and strictly positive rebates. Our results show that TFRMs provide a promising new direction for reducing transaction fees in public blockchains.},
  archive   = {C_AAMAS},
  author    = {Damle, Sankarshan and Padala, Manisha and Gujar, Sujit},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {416–424},
  title     = {Designing redistribution mechanisms for reducing transaction fees in blockchains},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662891},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A simple 1.5-approximation algorithm for a wide range of
maximum size stable matching problems. <em>AAMAS</em>, 409–415. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We give a simple approximation algorithm for a common generalization of many previously studied extensions of the maximum size stable matching problem with ties. These generalizations include the existence of critical vertices in the graph, amongst whom we must match as much as possible, free edges, that cannot be blocking edges and Δ-stabilities, which mean that for an edge to block, the improvement should be large enough on one or both sides. We also introduce other notions to generalize these even further, which allows our framework to capture many existing and future applications. We show that the edge duplicating technique allows us to treat these different types of generalizations simultaneously, while also making the algorithm, the proofs and the analysis much simpler and shorter than in previous approaches. In particular, we answer an open question by Askalidis et al.[1] about the existence of a 3 over 2-approximation algorithm for the SMTI problem with free edges. This demonstrates that this technique can grasp the underlying essence of these problems quite well and have the potential to be able to solve many future applications.},
  archive   = {C_AAMAS},
  author    = {Cs\&#39;{a}ji, Gergely},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {409–415},
  title     = {A simple 1.5-approximation algorithm for a wide range of maximum size stable matching problems},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662890},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-robot allocation of assistance from a shared uncertain
operator. <em>AAMAS</em>, 400–408. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Shared autonomy systems allow robots to either operate autonomously or request assistance from a human operator. In such settings, the human operator may exhibit sub-optimal behaviours, influenced by latent variables such as attention level or task proficiency. In this paper, we consider shared autonomy systems composed of multiple robots and one human. In this setting, we aim to synthesise a controller that selects, at each decision step, the actions to be taken by each robot and which (if any) robot the human operator should assist. To efficiently allocate the human operator to a robot at any given time, we propose a controller that reasons about the uncertainty over the latent variables impacting the human operator&#39;s performance. To ensure scalability, we use an online bidding system, where each robot plans while considering its belief over the human&#39;s performance, and bids according to the direct benefit of human assistance and how much information will be gained by the system about the human. We experiment on two domains, where we outperform approaches for allocation of human assistance that do not consider the human&#39;s latent variables, and show that the performance of the overall system increases when robots consider the information gained by requesting human assistance when bidding.},
  archive   = {C_AAMAS},
  author    = {Costen, Clarissa and Gautier, Anna and Hawes, Nick and Lacerda, Bruno},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {400–408},
  title     = {Multi-robot allocation of assistance from a shared uncertain operator},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662889},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flame: A framework for learning in agent-based ModEls.
<em>AAMAS</em>, 391–399. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Agent-based models (ABMs) are discrete simulators comprising agents that act and interact in a computational world. Despite wide applicability, infrastructure for ABMs has been fragmented and lacks a standard framework to integrate benefits of recent computing advances, especially in machine learning and automatic differentiation (autograd). To alleviate this gap we introduce flame: a framework to define, simulate and optimize differentiable agent-based models. First, flame introduces a domain-specific language that describes ABMs with stochastic dynamics across several domains and can be implemented using abstractions of autograd. Second, flame models can execute simulations on GPU, process millions of interactions per second and seamlessly scale from few hundred agents to million-size populations. Third, flame provides custom utilities to implement fully differentiable ABMs which can benefit from gradient-based learning and integrate with deep neural networks (DNNs), in several ways. Specifically, ABMs can now use supervised and reinforcement learning to calibrate simulation parameters, optimize agent actions and learn expressive interaction rules. Finally, flame is easily accessible with a simple Python API. We validate flame through multiple case studies that study tissue morphogenesis over bio-electric networks, infectious disease epidemiology over physical networks and opinion dynamics over social networks. We hope flame can ignite further innovation at the intersection of AI and ABMs. Our code is available at github.com/AgentTorch/AgentTorch},
  archive   = {C_AAMAS},
  author    = {Chopra, Ayush and Subramanian, Jayakumar and Krishnamurthy, Balaji and Raskar, Ramesh},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {391–399},
  title     = {Flame: A framework for learning in agent-based ModEls},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662888},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Private agent-based modeling. <em>AAMAS</em>, 381–390. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The practical utility of agent-based models in decision-making relies on their capacity to accurately replicate populations while seamlessly integrating real-world data streams. Yet, the incorporation of such data poses significant challenges due to privacy concerns. To address this issue, we introduce a paradigm for private agent-based modeling wherein the simulation, calibration, and analysis of agent-based models can be achieved without centralizing the agents&#39; attributes or interactions. The key insight is to leverage techniques from secure multi-party computation to design protocols for decentralized computation in agent-based models. This ensures the confidentiality of the simulated agents without compromising on simulation accuracy. We showcase our protocols on a case study with an epidemiological simulation comprising over 150,000 agents. We believe this is a critical step towards deploying agent-based models to real-world applications.},
  archive   = {C_AAMAS},
  author    = {Chopra, Ayush and Quera-Bofarull, Arnau and Giray-Kuru, Nurullah and Wooldridge, Michael and Raskar, Ramesh},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {381–390},
  title     = {Private agent-based modeling},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662887},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fairness and efficiency trade-off in two-sided matching.
<em>AAMAS</em>, 372–380. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The theory of two-sided matching has been extensively developed and applied to many real-life application domains. As the theory has been applied to increasingly diverse types of environments, researchers and practitioners have encountered various forms of distributional constraints. As a mechanism can handle a more general class of constraints, we can assign students more flexibly to colleges to increase students&#39; welfare. However, it turns out that there exists a trade-off between students&#39; welfare (efficiency) and fairness (which means no student has justified envy). Furthermore, this trade-off becomes sharper as the class of constraints becomes more general. The first contribution of this paper is to clarify the boundary on whether a strategyproof and fair mechanism can satisfy certain efficiency properties for each class of constraints. Our second contribution is to establish a weaker fairness requirement called envy-freeness up to k peers (EF-k), which is inspired by a similar concept used in the fair division of indivisible items. EF-k guarantees that each student has justified envy towards at most k students. By varying k, EF-k can represent different levels of fairness. We investigate theoretical properties associated with EF-k. Furthermore, we develop two contrasting strategyproof mechanisms that work for general hereditary constraints, i.e., one mechanism can guarantee a strong efficiency requirement, while the other can guarantee EF-k for any fixed k. We evaluate the performance of these mechanisms through computer simulation.},
  archive   = {C_AAMAS},
  author    = {Cho, Sung-Ho and Kimura, Kei and Liu, Kiki and Liu, Kwei-guu and Liu, Zhengjie and Sun, Zhaohong and Yahiro, Kentaro and Yokoo, Makoto},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {372–380},
  title     = {Fairness and efficiency trade-off in two-sided matching},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662886},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning a social network by influencing opinions.
<em>AAMAS</em>, 363–371. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study a campaigner who wants to learn the structure of a social network by observing the underlying diffusion process and intervening on it. Using synchronous majoritarian updates on binary opinions as the underlying dynamics, we offer upper bounds on the campaigner&#39;s budget for learning any network with certainty, considering both observation and intervention resources, and further improving them for the case of clique networks. Additionally, we investigate the learning progress of the campaigner when her budget falls below these upper bounds. For such cases, we design a greedy campaigning strategy aimed at optimising the campaigner&#39;s information gain at each opinion diffusion step.},
  archive   = {C_AAMAS},
  author    = {Chistikov, Dmitry and Estrada, Luisa and Paterson, Mike and Turrini, Paolo},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {363–371},
  title     = {Learning a social network by influencing opinions},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662885},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast and slow goal recognition. <em>AAMAS</em>, 354–362. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Goal recognition is a crucial aspect of understanding the intentions and objectives of agents by observing some of their actions. The most prominent approaches to goal recognition can be divided into two main categories: (1) trustworthy systems, which exploit automated reasoning for computing plans compatible with the observed actions, and (2) swifter systems, which try to quickly infer goals, often overlooking complex cognitive processes, and have no formal guarantees of their results. This paper introduces a novel approach inspired by the dual process theory, which integrates these two techniques. A dual-process model is proposed, leveraging fast, experience-based recognition for immediate goal identification, and slow, deliberate analysis for deeper understanding. Machine learning techniques and classical planning techniques are employed to obtain this dual-process system. Experimental evaluations demonstrate the effectiveness of the approach, reducing the amount of resources required to compute a solution (e.g., time to find a goal), while at the same time enhancing accuracy and robustness, especially in more complex scenarios.},
  archive   = {C_AAMAS},
  author    = {Chiari, Mattia and Gerevini, Alfonso Emilio and Loreggia, Andrea and Putelli, Luca and Serina, Ivan},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {354–362},
  title     = {Fast and slow goal recognition},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662884},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ODEs learn to walk: ODE-net based data-driven modeling for
crowd dynamics. <em>AAMAS</em>, 345–353. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Predicting the behaviors of pedestrian crowds is of critical importance for a variety of real-world problems. Data driven modeling, which aims to learn the mathematical models from observed data, is a promising tool to construct models that can make accurate predictions of such systems. In this work, we present a data-driven modeling approach based on the ODE-Net framework, for constructing continuous-time models of crowd dynamics. We discuss some challenging issues in applying the ODE-Net method to such problems, which are primarily associated with the dimensionality of the underlying crowd system, and we propose to address these issues by incorporating the social-force concept in the ODE-Net framework. Finally application examples are provided to demonstrate the performance of the proposed method.},
  archive   = {C_AAMAS},
  author    = {Cheng, Chen and Li, Jinglai},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {345–353},
  title     = {ODEs learn to walk: ODE-net based data-driven modeling for crowd dynamics},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662883},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Boosting continuous control with consistency policy.
<em>AAMAS</em>, 335–344. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Due to its training stability and strong expression, the diffusion model has attracted considerable attention in offline reinforcement learning. However, several challenges have also come with it: 1) The demand for a large number of diffusion steps makes the diffusion-model-based methods time inefficient and limits their applications in real-time control; 2) How to achieve policy improvement with accurate guidance for diffusion model-based policy is still an open problem. Inspired by the consistency model, we propose a novel time-efficiency method named Consistency Policy with Q-Learning (CPQL), which derives action from noise by a single step. By establishing a mapping from the reverse diffusion trajectories to the desired policy, we simultaneously address the issues of time efficiency and inaccurate guidance when updating diffusion model-based policy with the learned Q-function. We demonstrate that CPQL can achieve policy improvement with accurate guidance for offline reinforcement learning, and can be seamlessly extended for online RL tasks. Experimental results indicate that CPQL achieves new state-of-the-art performance on 11 offline and 21 online tasks, significantly improving inference speed by nearly 45 times compared to Diffusion-QL. Code is available at https://github.com/cccedric/cpql.},
  archive   = {C_AAMAS},
  author    = {Chen, Yuhui and Li, Haoran and Zhao, Dongbin},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {335–344},
  title     = {Boosting continuous control with consistency policy},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662882},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive primal-dual method for safe reinforcement learning.
<em>AAMAS</em>, 326–334. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Primal-dual methods have a natural application in Safe Reinforcement Learning (SRL), posed as a constrained policy optimization problem. In practice however, applying primal-dual methods to SRL is challenging, due to the inter-dependency of the learning rate (LR) and Lagrangian multipliers (dual variables) each time an embedded unconstrained RL problem is solved. In this paper, we propose, analyze and evaluate adaptive primal-dual (APD) methods for SRL, where two adaptive LRs are adjusted to the Lagrangian multipliers so as to optimize the policy in each iteration. We theoretically establish the convergence, optimality and feasibility of the APD algorithm. Finally, we conduct numerical evaluation of the practical APD algorithm with four well-known environments in Bullet-Safey-Gym employing two state-of-the-art SRL algorithms: PPO-Lagrangian and DDPG-Lagrangian. All experiments show that the practical APD algorithm outperforms (or achieves comparable performance) and attains more stable training than the constant LR cases. Additionally, we substantiate the robustness of selecting the two adaptive LRs by empirical evidence.},
  archive   = {C_AAMAS},
  author    = {Chen, Weiqin and Onyejizu, James and Vu, Long and Hoang, Lan and Subramanian, Dharmashankar and Kar, Koushik and Mishra, Sandipan and Paternain, Santiago},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {326–334},
  title     = {Adaptive primal-dual method for safe reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662881},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Foresight distribution adjustment for off-policy
reinforcement learning. <em>AAMAS</em>, 317–325. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Off-policy reinforcement learning algorithms maintain a replay buffer to utilize samples obtained from earlier policies. The sampling strategy that prioritizes certain data in a buffer to train the value function or the policy, has been shown to significantly influence the sample efficiency and the final performance of the algorithm. However, which distribution for the experience prioritization is the best choice has not been explored thoroughly. In this paper, we proved that the post-update policy distribution (i.e. the visitation distribution of the policy after the current iteration of update) is the best Q training distribution to benefit the policy improvement. Nevertheless, accessing this &quot;future&quot; distribution is not straightforward. In this work, we find that the current experiences can be modulated by the critic information to simulate the post-update policy distribution. Technically, we derive the gradient of the visitation distribution with respect to the policy parameter and obtain an explicit expression to approximate the post-update policy distribution. The derived method is named as Foresight Distribution Adjustment (FoDA), and seamlessly integrates with conventional off-policy actor-critic algorithms. Our experiments validate FoDA&#39;s ability to closely approximate the post-update policy distribution, and demonstrate its utility in enhancing performance across continuous control task benchmarks.},
  archive   = {C_AAMAS},
  author    = {Chen, Ruifeng and Liu, Xu-Hui and Liu, Tian-Shuo and Jiang, Shengyi and Xu, Feng and Yu, Yang},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {317–325},
  title     = {Foresight distribution adjustment for off-policy reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662880},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep anomaly detection via active anomaly search.
<em>AAMAS</em>, 308–316. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Anomaly detection (AD) holds substantial practical value, and considering the limited labeled data, the semi-supervised anomaly detection technique has garnered increasing attention. We find that previous methods suffer from insufficient exploitation of labeled data and under-exploration of unlabeled data. To tackle the above problem, we aim to search for possible anomalies in unlabeled data and use the searched anomalies to enhance performance. We innovatively model this search process as a Markov decision process and utilize a reinforcement learning algorithm to solve it. Our method, Deep Anomaly Detection and Search (DADS), integrates the exploration of unlabeled data and the exploitation of labeled data into one framework. Experimentally, we compare DADS with several state-of-the-art methods in widely used benchmarks, and the results show that DADS can efficiently search anomalies from unlabeled data and learn from them, thus achieving good performance. Code: https://github.com/LAMDA-RL/DADS},
  archive   = {C_AAMAS},
  author    = {Chen, Chao and Wang, Dawei and Mao, Feng and Xu, Jiacheng and Zhang, Zongzhang and Yu, Yang},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {308–316},
  title     = {Deep anomaly detection via active anomaly search},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662879},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Think global, act local - agent-based inline recovery for
airline operations. <em>AAMAS</em>, 299–307. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Flight delays can significantly affect airline operations. Airlines use inline recovery actions (e.g., speeding up aircraft cleaning) to mitigate the effect of flight delays. Inline (or tactical) recovery for disruptions mostly relies on human expertise that is locally optimal (e.g., at an airport-level). Because an airline is a complex and stochastic network of dependencies, a locally optimal action may be globally sub-optimal. Considering global effects for inline recovery is computationally challenging with conventional algorithmic approaches for optimization. We complement existing approaches with Stochastic Minplus with State (SMS), a novel agent-based approach for inline recovery. SMS generalizes message passing algorithms for a state-dependent stochastic airline network with resource constraints. We evaluate our approach on a real-world airline network with around 4,000 flights per day for two regimes with 1) normal delays, and 2) interrupted operations. As baselines, we use approaches based on greedy local optimality, integer programming (IP), and constraint programming (CP). Our evaluation shows that: 1) a globally informed SMS improves over greedy locally optimal approach by 24.7\% in quality, 2) SMS achieves solutions that are better by 14.1\% (10.6\%) in quality and 7x (9x) in computation time over IP (CP) with timeout, and 3) SMS achieves solutions within 5\% of the optimal solution for simpler problem instances.},
  archive   = {C_AAMAS},
  author    = {Chati, Yashovardhan S. and Suriyanarayanan, Ramasubramanian and Vasan, Arunchandar},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {299–307},
  title     = {Think global, act local - agent-based inline recovery for airline operations},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662878},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cooperative electric vehicles planning. <em>AAMAS</em>,
290–298. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces the Cooperative Electric Vehicles Planning Problem (CEVPP), which consists in finding a path for each vehicle of a fleet of electric vehicles, such that the global plan execution time (including travel time, charging time and waiting time) is minimal (e.g., by limiting the number of vehicles who need to charge simultaneously at the same charging station, which leads to waiting time). We show that the strategy which consists in planning each possible permutation of EVs and keeping the one providing the best solution is not only time intractable, but also not optimal. We propose different centralized planning algorithms to solve CEVPP instances: (1) a baseline non-cooperative CEVPP planner, (2) an optimal cooperative planner that finds a solution inside a carefully designed state space, and (3) multiple variants of an approximate cooperative planner based on the Cooperative-A* algorithm. We compare the solutions&#39; quality and computation times obtained by these CEVPP planners. Our empirical results show that our best approximate cooperative EV planner found solutions with a reasonably small computational overhead compared to the baseline algorithm. The solutions found by our cooperative planners had significantly lower plan execution time globally, including travel time, waiting time and charging time, than the solution found by our baseline non-cooperative planner. On average, our empirical results show that our cooperative algorithms decreased the global (including each EVs) waiting time by more than 90\%, while having a negligible impact on the charging and driving time.},
  archive   = {C_AAMAS},
  author    = {Champagne Gareau, Ja\&quot;{e}l and Lavoie, Marc-Andr\&#39;{e} and Gosset, Guillaume and Beaudry, \&#39;{E}ric},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {290–298},
  title     = {Cooperative electric vehicles planning},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662877},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Aligning credit for multi-agent cooperation via model-based
counterfactual imagination. <em>AAMAS</em>, 281–289. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent years have witnessed considerable progress in model-based reinforcement learning research. Inspired by the significant improvement in sample efficiency, researchers have explored its application in multi-agent scenarios to mitigate the huge demands in training data of multi-agent reinforcement learning (MARL) approaches. However, existing methods retain the training framework designed for single-agent settings, resulting in inadequate promotion of multi-agent cooperation. In this work, we propose a novel model-based MARL method called Multi-Agent Counterfactual Dreamer (MACD). MACD introduces a centralized imagination with decentralized execution (CIDE) framework to generate higher-quality pseudo data for policy learning, thus further improving the algorithm&#39;s sample efficiency. Moreover, we address the credit assignment and non-stationary challenges by performing an additional counterfactual trajectory based on the learned world model. We provide a theoretical proof that this counterfactual policy update rule maximizes the multi-agent learning objective. Empirical studies validate the superiority of our method in terms of sample efficiency, training stability, and final cooperation performance when compared with several state-of-the-art model-free and model-based MARL algorithms. Ablation studies and visualization demonstration further underscore the significance of both the CIDE framework and the counterfactual module in our approach.},
  archive   = {C_AAMAS},
  author    = {Chai, Jiajun and Fu, Yuqian and Zhao, Dongbin and Zhu, Yuanheng},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {281–289},
  title     = {Aligning credit for multi-agent cooperation via model-based counterfactual imagination},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662876},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Obstruction alternating-time temporal logic: A strategic
logic to reason about dynamic models. <em>AAMAS</em>, 271–280. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-Agent Systems (MAS) operating within dynamic models have been extensively studied in various domains, including cybersecurity and planning. In this paper, we introduce a dedicated logic for analyzing a specific category of MAS that involve strategic objectives within dynamic models. Within these MAS, there exists an agent known as the &quot;Demon&quot;, which possesses the capability to modify the MAS model itself, while other agents operate as traditional MAS entities. We demonstrate that the model-checking problem for our logic is solvable in polynomial time. Furthermore, we showcase how this logic can be effectively employed to articulate significant properties within the realm of cybersecurity.},
  archive   = {C_AAMAS},
  author    = {Catta, Davide and Leneutre, Jean and Malvone, Vadim and Murano, Aniello},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {271–280},
  title     = {Obstruction alternating-time temporal logic: A strategic logic to reason about dynamic models},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662875},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Finding effective ad allocations: How to exploit user
history. <em>AAMAS</em>, 262–270. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A primary source of revenue for web platforms is digital advertising. Platforms typically maximize the effectiveness of advertising campaigns by exploiting user features (i.e., targeted advertising). However, performance can be further improved by leveraging user navigation history. In particular, the advent of new augmented reality platforms encourages users to spend a considerable amount of time in the same virtual environment, opening up the challenge of determining which ads to display and at which time of their experience. In this paper, we initiate the study of history-dependent advertising by providing a user model and optimized ad allocation algorithms. Our model assumes that users move through a series of scenes where they are exposed to ads. The performance of an ad may be influenced by various factors, such as the features of the scene in which it is displayed, the externalities of previously observed ads and the possibility that a user has already purchased the promoted product. We analyze the computational complexity of finding an optimal ad allocation for several model flavors and provide practical approximation algorithms with tight theoretical guarantees. We also discuss under which conditions our approximation algorithms are monotone according to Myerson&#39;s definition, thus leading to truthful auction mechanisms.},
  archive   = {C_AAMAS},
  author    = {Castiglioni, Matteo and Latino, Alberto and Marchesi, Alberto and Romano, Giulia and Gatti, Nicola and Palayamkottai, Chokha},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {262–270},
  title     = {Finding effective ad allocations: How to exploit user history},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662874},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A distributed approach for fault detection in swarms of
robots. <em>AAMAS</em>, 253–261. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Swarm Robotic Systems (SRSs) are multi-robot systems usually composed of relatively simple robots. Local decisions and communication between robots allow for the emergence of complex behaviors of the entire SRS. The distributed nature of the SRSs enables their use in many real-world applications. However, despite the common belief that these systems are inherently robust and fault tolerant, it has been shown that even a few faulty robots could considerably hinder the performance of the entire SRS. In this paper, we propose a distributed fault detection approach that exploits machine learning classifiers to allow each robot of a SRS to detect faults in other robots and/or in itself. The proposed fault detection approach is data-driven, requiring a reduced amount of explicit domain knowledge, and is based on data that can be easily collected by common swarm robotics platforms. We test the proposed fault detection approach in simulation and analyze the results using non-parametric statistical tests. Our extensive experimental campaign shows that our approach has good performance and is robust regardless of the ratio of faulty robots in the SRS.},
  archive   = {C_AAMAS},
  author    = {Carminati, Alessandro and Azzalini, Davide and Vantini, Simone and Amigoni, Francesco},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {253–261},
  title     = {A distributed approach for fault detection in swarms of robots},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662873},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the complexity of pareto-optimal and envy-free lotteries.
<em>AAMAS</em>, 244–252. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the classic problem of dividing a collection of indivisible resources in a fair and efficient manner among a set of agents having varied preferences. Pareto optimality is a standard notion of economic efficiency, which states that it should be impossible to find an allocation that improves some agent&#39;s utility without reducing any other&#39;s. On the other hand, a fundamental notion of fairness in resource allocation settings is that of envy-freeness, which renders an allocation to be fair if every agent (weakly) prefers her own bundle over that of any other agent&#39;s bundle. Unfortunately, an envy-free allocation may not exist if we wish to divide a collection of indivisible items. Introducing randomness is a typical way of circumventing the non-existence of solutions, and therefore, em allocation lotteries, i.e., distributions over allocations have been explored while relaxing the notion of fairness to ex-ante envy freeness.We consider a general fair division setting with n agents and a family of admissible n-partitions of an underlying set of items. Every agent is endowed with partition-based utilities, which specify her cardinal utility for each bundle of items in every admissible partition. In such fair division instances, Cole and Tao~(2021) have proved that an ex-ante envy-free and Pareto-optimal allocation lottery is always guaranteed to exist. We strengthen their result while examining the computational complexity of the above total problem and establish its membership in the complexity class ¶PAD. Furthermore, for instances with a constant number of agents, we develop a polynomial-time algorithm to find an ex-ante envy-free and Pareto-optimal allocation lottery. On the negative side, we prove that maximizing social welfare over ex-ante envy-free and Pareto-optimal allocation lotteries is NP-hard.},
  archive   = {C_AAMAS},
  author    = {Caragiannis, Ioannis and Hansen, Kristoffer Arnsfelt and Rathi, Nidhi},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {244–252},
  title     = {On the complexity of pareto-optimal and envy-free lotteries},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662872},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HELP! Providing proactive support in the presence of
knowledge asymmetry. <em>AAMAS</em>, 234–243. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While the development of proactive personal assistants has been a popular topic within AI research, most research in this direction tends to focus on a small subset of possible interaction settings. An important setting that is often overlooked is one where the users may have an incomplete or incorrect understanding of the task. This could lead to the user following incorrect plans with potentially disastrous consequences. Supporting such settings requires agents that are able to detect when the user&#39;s actions might be leading them to a possibly undesirable state and, if they are, intervene so the user can correct their course of actions. For the former problem, we introduce a novel planning compilation that transforms the task of estimating the likelihood of task failures into a probabilistic goal recognition problem. This allows us to leverage the existing goal recognition techniques to detect the likelihood of failure. For the intervention problem, we use model search algorithms to detect the set of minimal model updates that could help users identify valid plans. These identified model updates become the basis for agent intervention. We further extend the proposed approach by developing methods for pre-emptive interventions, to prevent the users from performing actions that might result in eventual plan failure. We show how we can identify such intervention points by using an efficient approximation of the true intervention problems, which are best represented as a Partially Observable Markov Decision-Process (POMDP). To substantiate our claims and demonstrate the applicability of our methodology, we have conducted exhaustive evaluations across a diverse range of planning benchmarks. These tests have consistently shown the robustness and adaptability of our approach, further solidifying its potential utility in real-world applications.},
  archive   = {C_AAMAS},
  author    = {Caglar, Turgay and Sreedharan, Sarath},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {234–243},
  title     = {HELP! providing proactive support in the presence of knowledge asymmetry},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662871},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust popular matchings. <em>AAMAS</em>, 225–233. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study popularity for matchings under preferences. This solution concept captures matchings that do not lose against any other matching in a majority vote by the agents. A popular matching is said to be robust if it is popular among multiple instances. We present a polynomial-time algorithm for deciding whether there exists a robust popular matching if instances only differ with respect to the preferences of a single agent while obtaining NP-completeness if two instances differ only by a downward shift of one alternative by four agents. Moreover, we find a complexity dichotomy based on preference completeness for the case where instances differ by making some options unavailable.},
  archive   = {C_AAMAS},
  author    = {Bullinger, Martin and Gangam, Rohith Reddy and Shahkar, Parnian},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {225–233},
  title     = {Robust popular matchings},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662870},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An online learning theory of brokerage. <em>AAMAS</em>,
216–224. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate brokerage between traders from an online learning perspective. At any round t, two traders arrive with their private valuations, and the broker proposes a trading price. Unlike other bilateral trade problems already studied in the online learning literature, we focus on the case where there are no designated buyer and seller roles: each trader will attempt to either buy or sell depending on the current price of the good.We assume the agents&#39; valuations are drawn i.i.d. from a fixed but unknown distribution. If the distribution admits a density bounded by some constant M, then, for any time horizon T: If the agents&#39; valuations are revealed after each interaction, we provide an algorithm achieving regret M log T and show this rate is optimal, up to constant factors.If only their willingness to sell or buy at the proposed price is revealed after each interaction, we provide an algorithm achieving regret √M T and show this rate is optimal, up to constant factors. Finally, if we drop the bounded density assumption, we show that the optimal rate degrades to √T in the first case, and the problem becomes unlearnable in the second.},
  archive   = {C_AAMAS},
  author    = {Boli\&#39;{c}, Natasa and Cesari, Tommaso and Colomboni, Roberto},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {216–224},
  title     = {An online learning theory of brokerage},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662869},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On green sustainability of resource selection games with
equitable cost-sharing. <em>AAMAS</em>, 207–215. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As increasing concern for environmental sustainability urges to bring attention to green-aware multi-agent systems, we put forward a game-theoretic model in which agents compete for the usage of power-consuming resources and are charged a cost proportional to their fair share of the power consumption. Using the widely adopted cube-root rule for CMOS-based devices, our model becomes a congestion game in which two distinct parts coexist, namely, congestion games with polynomial latency functions and fair cost-sharing games. The interplay between these two components is governed by two resource-specific constants regulating the static and dynamic power consumption of each resource. Our findings show that, despite these games being highly inefficient in the general case (a super-constant price of stability), performance at equilibrium significantly improves (a constant price of anarchy) when the ratio between the static and dynamic power consumption of each resource remains bounded by a constant. This suggests that, in uncoordinated green-aware multi-agent systems, technology plays a fundamental role in shaping the efficiency of stable solutions.},
  archive   = {C_AAMAS},
  author    = {Bil\`{o}, Vittorio and Flammini, Michele and Monaco, Gianpiero and Moscardelli, Luca and Vinci, Cosimo},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {207–215},
  title     = {On green sustainability of resource selection games with equitable cost-sharing},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662868},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal referral auction design. <em>AAMAS</em>, 198–206.
(<a href="https://dl.acm.org/doi/10.5555/3635637.3662867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The auction of a single indivisible item is one of the most celebrated problems in mechanism design with transfers. Despite its simplicity, it provides arguably the cleanest and most insightful results in the literature. When the information that the auction is running is available to every participant, Myerson[19] provided a seminal result to characterize the incentive-compatible auctions along with revenue optimality. However, such a result does not hold in an auction on a network, where the information of the auction is spread via the agents, and they need incentives to forward the information. In recent times, a few auctions (e.g., [12, 16]) were designed that appropriately incentivized the intermediate nodes on the network to promulgate the information to potentially more valuable bidders. In this paper, we provide a Myerson-like characterization of incentive-compatible auctions on a network and show that the currently known auctions fall within this class of randomized auctions. We then consider a special class called the referral auctions that are inspired by the multi-level marketing mechanisms, [1, 5, 6] and obtain the structure of a revenue optimal referral auction for i.i.d. bidders.},
  archive   = {C_AAMAS},
  author    = {Bhattacharyya, Rangeet and Dave, Parvik and Dey, Palash and Nath, Swaprava},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {198–206},
  title     = {Optimal referral auction design},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662867},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hyper strategy logic. <em>AAMAS</em>, 189–197. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Strategy logic (SL) is a powerful temporal logic that enables strategic reasoning in multi-agent systems. SL supports explicit (first-order) quantification over strategies and provides a logical framework to express many important properties such as Nash equilibria, dominant strategies, etc. While in SL the same strategy can be used in multiple strategy profiles, each such profile is evaluated w.r.t. a path-property, i.e., a property that considers the single path resulting from a particular strategic interaction. In this paper, we present Hyper Strategy Logic (HyperSL), a strategy logic where the outcome of multiple strategy profiles can be compared w.r.t. a hyperproperty, i.e., a property that relates multiple paths. We show that HyperSL can capture important properties that cannot be expressed in SL, including non-interference, quantitative Nash equilibria, optimal adversarial planning, and reasoning under imperfect information. On the algorithmic side, we identify an expressive fragment of HyperSL with decidable model checking and present a model-checking algorithm. We contribute a prototype implementation of our algorithm and report on encouraging experimental results.},
  archive   = {C_AAMAS},
  author    = {Beutner, Raven and Finkbeiner, Bernd},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {189–197},
  title     = {Hyper strategy logic},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662866},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Monitoring second-order hyperproperties. <em>AAMAS</em>,
180–188. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hyperproperties express the relationship between multiple executions of a system. This is needed in many AI-related fields, such as knowledge representation and planning, to capture system properties related to knowledge, information flow, and privacy. In this paper, we study the monitoring of complex hyperproperties at runtime. Previous work in this area has either focused on the simpler problem of monitoring trace properties (which are sets of traces, while hyperproperties are sets of sets of traces) or on monitoring first-order hyperproperties, which are expressible in temporal logics with first-order quantification over traces, such as HyperLTL. We present the first monitoring algorithm for the much more expressive class of second-order hyperproperties. Second-order hyperproperties include system properties like common knowledge, which cannot be expressed in first-order logics like HyperLTL.We introduce Hyper2LTLundefined, a temporal logic over finite traces that allows for second-order quantification over sets of traces. We study the monitoring problem in two fundamental execution models: (1) the parallel model, where a fixed number of traces is monitored in parallel, and (2) the sequential model, where an unbounded number of traces is observed sequentially, one trace after the other. For the parallel model, we show that the monitoring of the second-order hyperproperties of Hyper2LTLundefined can be reduced to monitoring first-order hyperproperties. For the sequential model, we present a monitoring algorithm that handles second-order quantification efficiently, exploiting optimizations based on the monotonicity of subformulas, graph-based storing of executions, and fixpoint hashing. We present experimental results from a range of benchmarks, including examples from common knowledge and planning.},
  archive   = {C_AAMAS},
  author    = {Beutner, Raven and Finkbeiner, Bernd and Frenkel, Hadar and Metzger, Niklas},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {180–188},
  title     = {Monitoring second-order hyperproperties},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662865},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Combining voting and abstract argumentation to understand
online discussions. <em>AAMAS</em>, 170–179. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Online discussion platforms are a vital part of the public discourse in a deliberative democracy. However, how to interpret the outcomes of the discussions on these platforms is often unclear. In this paper, we propose a novel and explainable method for selecting a set of most representative, consistent points of view by combining methods from computational social choice and abstract argumentation. Specifically, we model online discussions as abstract argumentation frameworks combined with information regarding which arguments voters approve of. Based on ideas from approval-based multiwinner voting, we introduce several voting rules for selecting a set of preferred extensions that represents voters&#39; points of view. We compare the proposed methods across several dimensions, theoretically and in numerical simulations, and give clear suggestions on which methods to use depending on the specific situation.},
  archive   = {C_AAMAS},
  author    = {Bernreiter, Michael and Maly, Jan and Nardi, Oliviero and Woltran, Stefan},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {170–179},
  title     = {Combining voting and abstract argumentation to understand online discussions},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662864},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Verification of stochastic multi-agent systems with
forgetful strategies. <em>AAMAS</em>, 160–169. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Intelligent autonomous agents need to reason about different kinds of uncertainty in a Multi-Agent System (MAS): first, due to the occurrence of randomization and, second, their inability to completely observe the state of the system. In this paper, we investigate the verification of system specifications in probabilistic variants of the logics ATL and ATL* under imperfect information (II). The resulting setting combines these two sources of uncertainty and captures the situation in which agents have qualitative uncertainty about the local state as well as quantitative uncertainty about the occurrence of future events. Since the model-checking problem is undecidable when considered in the context of strategies with perfect recall, we focus on memoryless (positional) strategies. As the main result, we show that, in stochastic MAS under II, model-checking Probabilistic ATL is in EXPTIME when agents play probabilistic strategies. Filling the gap in recent work, we also show that model-checking Probabilistic ATL* is PSPACE-complete when the proponent coalition is restricted to deterministic strategies.},
  archive   = {C_AAMAS},
  author    = {Belardinelli, Francesco and Jamroga, Wojtek and Mittelmann, Munyque and Murano, Aniello},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {160–169},
  title     = {Verification of stochastic multi-agent systems with forgetful strategies},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662863},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parameterized guarantees for almost envy-free allocations.
<em>AAMAS</em>, 151–159. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study fair allocation of indivisible goods among agents with additive valuations. We obtain novel approximation guarantees for two of the strongest fairness notions in discrete fair division, namely envy-free up to the removal of any positively-valued good (EFx) and pairwise maximin shares (PMMS). Our approximation guarantees are in terms of an instance-dependent parameter γ ∈ (0,1] that upper bounds, for each indivisible good in the given instance, the multiplicative range of nonzero values for the good across the agents.First, we consider allocations wherein, between any pair of agents and up to the removal of any positively-valued good, the envy is multiplicatively bounded. Specifically, the current work develops a polynomial-time algorithm that computes a ( 2 over γ √ 5+4 γ-1 &amp;amp;41;-approximately EFx allocation for any given fair division instance with range parameter γ ∈ (0,1]. For instances with γ ≥ 0.511, the obtained approximation guarantee for EFx surpasses the previously best-known approximation bound of (φ-1) ≈ 0.618, here φ denotes the golden ratio.Furthermore, we study multiplicative approximations for PMMS. For fair division instances with range parameter γ ∈ (0,1], the current paper develops a polynomial-time algorithm for finding allocations wherein the PMMS requirement is satisfied, between every pair of agents, within a multiplicative factor of 5 over 6 γ. En route to this result, we obtain novel existential and computational guarantees for 5 over 6-approximately PMMS allocations under restricted additive valuations.},
  archive   = {C_AAMAS},
  author    = {Barman, Siddharth and Kar, Debajyoti and Pathak, Shraddha},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {151–159},
  title     = {Parameterized guarantees for almost envy-free allocations},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662862},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A model-based solution to the offline multi-agent
reinforcement learning coordination problem. <em>AAMAS</em>, 141–150.
(<a href="https://dl.acm.org/doi/10.5555/3635637.3662861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Training multiple agents to coordinate is an essential problem with applications in robotics, game theory, economics, and social sciences. However, most existing Multi-Agent Reinforcement Learning (MARL) methods are online and thus impractical for real-world applications in which collecting new interactions is costly or dangerous. While these algorithms should leverage offline data when available, doing so gives rise to what we call the offline coordination problem. Specifically, we identify and formalize the strategy agreement (SA) and the strategy fine-tuning (SFT) coordination challenges, two issues at which current offline MARL algorithms fail. Concretely, we reveal that the prevalent model-free methods are severely deficient and cannot handle coordination-intensive offline multi-agent tasks in either toy or MuJoCo domains. To address this setback, we emphasize the importance of inter-agent interactions and propose the very first model-based offline MARL method. Our resulting algorithm, Model-based Offline Multi-Agent Proximal Policy Optimization (MOMA-PPO) generates synthetic interaction data and enables agents to converge on a strategy while fine-tuning their policies accordingly. This simple model-based solution solves the coordination-intensive offline tasks, significantly outperforming the prevalent model-free methods even under severe partial observability and with learned world models.},
  archive   = {C_AAMAS},
  author    = {Barde, Paul and Foerster, Jakob and Nowrouzezahrai, Derek and Zhang, Amy},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {141–150},
  title     = {A model-based solution to the offline multi-agent reinforcement learning coordination problem},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662861},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trust in shapley: A cooperative quest for global trust in
P2P network. <em>AAMAS</em>, 132–140. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modeling the trust of peers in peer-to-peer networks is pivotal in maintaining the security and functionality of the network. This trust is commonly perceived as a peer&#39;s reliability based on past interactions and is generally classified as local and global trust values. In a traditional client-server network, the responsibility of maintaining the integrity of the network falls on the central authority responsible for enforcing the security protocols and safeguarding the network against adversarial activities. In contrast, peer-to-peer networks may lack a central authority due to their decentralized nature, needing innovative mechanisms to maintain network trust. Incorporating a trust mechanism that considers peer interactions within peer groups becomes convenient in the absence of central authority. This paper introduces a novel approach to global trust computation. We propose a transferable utility coalitional game that pools local trust values between peers. The coalitions of peers aggregate the local trust values by considering internal and external trust. Internal trust is defined as the sum of the local trust values of the peers in the coalition, and external trust is constituted by the minimal trustworthiness of peers in the coalition to the peers outside. The resulting trust game is superadditive, monotone, and has a non-empty core. The global trust values of individual peers are the Shapley values in the trust game. Our numerical experiments in three different settings show that the resulting global trust captures the peer behavior faithfully, and we compared our method to Eigentrust.},
  archive   = {C_AAMAS},
  author    = {Bandhana, Arti and Kroupa, Tom\&#39;{a} and Garc\&#39;{\i}a, Sebasti\&#39;{a}n},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {132–140},
  title     = {Trust in shapley: A cooperative quest for global trust in P2P network},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662860},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Strategic reasoning under capacity-constrained agents.
<em>AAMAS</em>, 123–131. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Personality traits, experience level, or physical characteristics can affect the capacity (or profile) of an agent. For instance, a basketball player may be right-handed or left-handed, and these two versions cannot do the same actions. Dribbling past the player may require, first, understanding its handedness and, accordingly, execute a trick. Generally, capacities apply to systems where multiple entities can play the same role in the system, such as different client versions in protocol analysis, different robots in heterogeneous fleets, different personality traits in social structure modeling, or different attacker profiles in cybersecurity. With the capacity of other agents being unknown at the system&#39;s initialization, the hardness of imperfect information arises. Our contributions are: (i) introducing Capacity Alternating-time Temporal Logic (CapATL) to reason about concurrent game structure where agents are bounded to capacities, (ii) a model-checking algorithm for CapATL, and (iii) a case study of adaptive honeypot design for cyber deception.},
  archive   = {C_AAMAS},
  author    = {Ballot, Gabriel and Malvone, Vadim and Leneutre, Jean and Laarouchi, Youssef},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {123–131},
  title     = {Strategic reasoning under capacity-constrained agents},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662859},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Minimax exploiter: A data efficient approach for competitive
self-play. <em>AAMAS</em>, 114–122. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in Competitive Self-Play (CSP) have achieved, or even surpassed, human level performance in complex game environments such as Dota 2 and StarCraft II using Distributed Multi-Agent Reinforcement Learning (MARL). One core component of these methods relies on creating a pool of learning agents - consisting of the Main Agent, past versions of this agent, and Exploiter Agents - where Exploiter Agents learn counter-strategies to the Main Agents. A key drawback of these approaches is the large computational cost and physical time that is required to train the system, making them impractical to deploy in highly iterative real-life settings such as video game productions. In this paper, we propose the Minimax Exploiter, a game theoretic approach to exploiting Main Agents that leverages knowledge of its opponents, leading to significant increases in data efficiency. We validate our approach in a diversity of settings, including simple turn based games, the arcade learning environment, and For Honor, a modern video game. The Minimax Exploiter consistently outperforms strong baselines, demonstrating improved stability and data efficiency, leading to a robust CSP-MARL method that is both flexible and easy to deploy.},
  archive   = {C_AAMAS},
  author    = {Bairamian, Daniel and Marcotte, Philippe and Romoff, Joshua and Robert, Gabriel and Nowrouzezahrai, Derek},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {114–122},
  title     = {Minimax exploiter: A data efficient approach for competitive self-play},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662858},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Impact of tie-breaking on the manipulability of elections.
<em>AAMAS</em>, 105–113. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we quantify the impact of manipulation using the price of anarchy measurement and study the impact of the lexicographic and the random candidate tie-breaking rules. We show that neither dominates the other in terms of mitigating the impact of manipulation. Specifically, we show that the random candidate tie-breaking rule lowers the impact of manipulation in plurality elections whereas the lexicographic tie-breaking rule lowers the impact of manipulation in elections determined by majority judgment.},
  archive   = {C_AAMAS},
  author    = {Bailey, James P. and Tovey, Craig A.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {105–113},
  title     = {Impact of tie-breaking on the manipulability of elections},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662857},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stability of weighted majority voting under estimated
weights. <em>AAMAS</em>, 96–104. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Majority Voting (WMV) is a well-known decision making rule. The weights of sources are determined by the probabilities that sources provide accurate information (trustworthiness). However, in reality, the trustworthiness is usually not a known quantity to the decision maker - they have to rely on an estimate called trust. An algorithm that computes trust is called unbiased when it has the property that it does not systematically overestimate or underestimate the trustworthiness. To formally analyze the uncertainty to the decision process brought by such unbiased trust values, we introduce and analyze two important properties of WMV: Stability of Correctness and Stability of Optimality. Stability of Correctness measures the difference between the decision accuracy that the decision maker believes he can achieve and the accuracy he actually achieves. We prove Stability of Correctness absolutely holds for WMV - the difference is 0. Stability of Optimality measures the difference between the actual accuracy of decisions made using trust values, and those made using trustworthiness values. We find a relatively tight upper bound on the Stability of Optimality, meaning that, although using (unbiased) trust values is suboptimal compared to using the true trustworthiness values, the difference is small. Meanwhile, a counter-intuitive observation is that while distributions of trustworthiness influence the Stability of Optimality, the number of sources barely influences it. We also provide an overview of how sensitive decision accuracy is to the changes in trust and trustworthiness.},
  archive   = {C_AAMAS},
  author    = {Bai, Shaojie and Wang, Dongxia and Muller, Tim and Cheng, Peng and Chen, Jiming},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {96–104},
  title     = {Stability of weighted majority voting under estimated weights},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662856},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Extended ranking mechanisms for the m-capacitated facility
location problem in bayesian mechanism design. <em>AAMAS</em>, 87–95.
(<a href="https://dl.acm.org/doi/10.5555/3635637.3662855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we initiate the study of the m-Capacitated Facility Location Problem (m-CFLP) within a Bayesian Mechanism Design framework. We consider the case in which every agent&#39;s private information is their position on a line and assume that every agent&#39;s position is independently drawn from a common and known distribution μ. In this context, we propose the Extended Ranking Mechanisms (ERMs), a truthful generalization of the recently introduced Ranking Mechanisms, that allows to handle problems where the total facility capacity exceeds the number of agents. Our primary results pertain to the study of the efficiency guarantees of the ERMs. In particular, we demonstrate that the limit of the ratio between the expected Social Cost of an ERM and the expected optimal Social Cost is finite. En route to these results, we reveal that the optimal Social Cost and the Social Cost of any ERMs can be expressed as the objective value of a suitable norm minimization problem in the Wasserstein space. We then tackle the problem of determining an optimal ERM tailored to a m-CFLP and a distribution μ. Specifically, we aim to identify an ERM whose limit Bayesian approximation ratio is the lowest compared to all other ERMs. We detail how to retrieve an optimal ERM in two frameworks: (i) when the total facility capacity matches the number of agents and (ii) when μ is the uniform distribution and we have two facilities to place. Lastly, we conduct extensive numerical experiments to compare the performance of the ERMs against other truthful mechanisms and to evaluate the convergence speed of the Bayesian approximation ratio. In summary, all our findings highlight that a well-tuned ERM consistently outperforms all other known mechanisms, making it a valid choice for solving the m -CFLP within a Bayesian framework.},
  archive   = {C_AAMAS},
  author    = {Auricchio, Gennaro and Zhang, Jie and Zhang, Mengxiao},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {87–95},
  title     = {Extended ranking mechanisms for the m-capacitated facility location problem in bayesian mechanism design},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662855},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Willy wonka mechanisms. <em>AAMAS</em>, 78–86. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bounded rationality in mechanism design aims to ensure incentive-compatibility for agents who are cognitively limited. These agents lack the contingent reasoning skills that traditional mechanism design assumes, and depending on how these cognitive limitations are modelled this alters the class of incentive-compatible mechanisms. In this work we design mechanisms without any &#39;&#39;obvious&#39;&#39; manipulations for several auction settings that aim to either maximise revenue or minimise the compensation paid to the agents. A mechanism without obvious manipulations is said to be not obviously manipulable (NOM), and assumes agents act truthfully as long as the maximum and minimum utilities from doing so are no worse than the maximum and minimum utilities from lying, with the extremes taken over all possible actions of the other agents. We exploit the definition of NOM by introducing the concept of golden tickets and wooden spoons, which designate bid profiles ensuring the mechanism&#39;s incentive-compatibility for each agent. We then characterise these &#39;&#39;Willy Wonka&#39;&#39; mechanisms, and by carefully choosing the golden tickets and wooden spoons we use this to design revenue-maximising auctions and frugal procurement auctions.},
  archive   = {C_AAMAS},
  author    = {Archbold, Thomas and de Keijzer, Bart and Ventre, Carmine},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {78–86},
  title     = {Willy wonka mechanisms},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662854},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Collective robustness of heterogeneous decision-makers
against stubborn individuals. <em>AAMAS</em>, 68–77. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Can heterogeneity be a cost-effective solution for swarm robotics? Motivated by what we see in animal groups, especially eusocial insect colonies, that exploit behavioural heterogeneity as the cornerstone of their success, we investigate whether or not swarms of robots with different behaviours can be more cost-effective than homogeneous swarms. We focus on the process of collective decision-making where robots must achieve a consensus on the best alternative between two options with different qualities, the best-of-2 problem. We consider four behaviours from the literature where robots use rules of voter-like models to exchange and update their opinions. We study the swarm&#39;s ability to be robust to the presence of zealots, i.e., stubborn robots that do not change their opinions. Our analysis is based on mean-field models that describe the change of the sub-populations holding different opinions. We show that heterogeneous swarms can be more efficient when we include in the analysis the cost of social interactions between robots. Normally, more interactive behaviours (e.g., pooling many neighbours&#39; opinions at each timestep rather than one per timestep) are quicker in making a decision and more robust to zealots. Heterogeneous swarms combine high performance with lower costs, as not the entire group must be highly interactive to maximise collective performance. Our results are useful when seeking a balance between making accurate collective decisions and minimising the cost of social interactions, the objective of artificial and natural swarms.},
  archive   = {C_AAMAS},
  author    = {Antonic, Nemanja and Zakir, Raina and Dorigo, Marco and Reina, Andreagiovanni},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {68–77},
  title     = {Collective robustness of heterogeneous decision-makers against stubborn individuals},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662853},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Offline risk-sensitive RL with partial observability to
enhance performance in human-robot teaming. <em>AAMAS</em>, 58–67. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The integration of physiological computing into mixed-initiative human-robot interaction systems offers valuable advantages in autonomous task allocation by incorporating real-time features as human state observations into the decision-making system. This approach may alleviate the cognitive load on human operators by intelligently allocating mission tasks between agents. Nevertheless, accommodating a diverse pool of human participants with varying physiological and behavioral measurements presents a substantial challenge. To address this, resorting to a probabilistic framework becomes necessary, given the inherent uncertainty and partial observability on the human&#39;s state. Recent research suggests to learn a Partially Observable Markov Decision Process (POMDP) model from a data set of previously collected experiences that can be solved using Offline Reinforcement Learning (ORL) methods. In the present work, we not only highlight the potential of partially observable representations and physiological measurements to improve human operator state estimation and performance, but also enhance the overall mission effectiveness of a human-robot team. Importantly, as the fixed data set may not contain enough information to fully represent complex stochastic processes, we propose a method to incorporate model uncertainty, thus enabling risk-sensitive sequential decision-making. Experiments were conducted with a group of twenty-six human participants within a simulated robot teleoperation environment, yielding empirical evidence of the method&#39;s efficacy. The obtained adaptive task allocation policy led to statistically significant higher scores than the one that was used to collect the data set, allowing for generalization across diverse participants also taking into account risk-sensitive metrics.},
  archive   = {C_AAMAS},
  author    = {Angelotti, Giorgio and Chanel, Caroline P. C. and Moreira Pinto, Adam Henrique and Lounis, Christophe and Chauffaut, Corentin and Drougard, Nicolas},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {58–67},
  title     = {Offline risk-sensitive RL with partial observability to enhance performance in human-robot teaming},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662852},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the potential and limitations of proxy voting: Delegation
with incomplete votes. <em>AAMAS</em>, 49–57. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study elections where voters are faced with the challenge of expressing preferences over an extreme number of issues under consideration. This is largely motivated by emerging blockchain governance systems, which include voters with different weights and a massive number of community generated proposals. In such scenarios, it is natural to expect that voters will have incomplete preferences, as they may only be able to evaluate or be confident about a very small proportion of the alternatives. As a result, the election outcome may be significantly affected, leading to suboptimal decisions. Our central inquiry revolves around whether delegation of ballots to proxies possessing greater expertise or a more comprehensive understanding of the voters&#39; preferences can lead to outcomes with higher legitimacy and enhanced voters&#39; satisfaction in elections where voters submit incomplete preferences. To explore this, we introduce a model where potential proxies advertise their ballots over multiple issues, and each voter either delegates to a seemingly attractive proxy or casts a ballot directly. We identify necessary and sufficient conditions that could lead to a socially better outcome by leveraging the participation of proxies. We accompany our theoretical findings with experiments on instances derived from real datasets. Our results enhance the understanding of the power of delegation towards improving election outcomes.},
  archive   = {C_AAMAS},
  author    = {Amanatidis, Georgios and Filos-Ratsikas, Aris and Lazos, Philip and Markakis, Evangelos and Papasotiropoulos, Georgios},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {49–57},
  title     = {On the potential and limitations of proxy voting: Delegation with incomplete votes},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662851},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Beliefs, shocks, and the emergence of roles in asset
markets: An agent-based modeling approach. <em>AAMAS</em>, 40–48. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although predictive AI models have grown to dominate computational finance, they are often limited in their applications when it comes to studying interventions and explaining behavioral outcomes. Financial economics, on the other hand, has a rich history of analytical approaches to asset-pricing theory, often requiring sweeping assumptions. In this paper, we construct an agent-based model of asset markets that is able to dispense with onerous restrictions on agent behaviors and beliefs, while having analytical validity and providing insights into the functioning of asset markets. In particular, we evaluate our models with respect to several traditional financial economic theories like Tobin&#39;s separation theorem and the capital asset pricing model (CAPM). We devise a network representing trades to show the emergence of different roles played by the agents. We study interventions, such as shocks, and explain the outcomes using our model. Finally, we investigate the effects of noise trading and show that noisy agents converge to different equilibrium points due to their differences in beliefs. Put together, this paper presents an agent-based model that can be used to study the effects of heterogeneous beliefs and risks of the agents and shocks to assets at a systemic level, thereby connecting localized agent and asset characteristics to global or collective outcomes.},
  archive   = {C_AAMAS},
  author    = {Albers, Evan and Irfan, Mohammad T and Botsch, Matthew J.},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {40–48},
  title     = {Beliefs, shocks, and the emergence of roles in asset markets: An agent-based modeling approach},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662850},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Provably learning nash policies in constrained markov
potential games. <em>AAMAS</em>, 31–39. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent reinforcement learning addresses sequential decision-making problems with multiple agents, where each agent optimizes its own objective. In many real-world scenarios, agents not only aim to maximize their goals but also need to ensure safe behavior. For example, in traffic routing, each vehicle (acting as an agent) seeks to reach its destination swiftly (an objective) while avoiding collisions (a safety constraint). Constrained Markov Games (CMGs) offer a natural framework for addressing safe MARL problems, but they are typically computationally challenging. In this work, we introduce and study Constrained Markov Potential Games (CMPGs), a significant subclass of CMGs. Initially, we demonstrate that Nash policies for CMPGs can be computed through constrained optimization. Then, we showed that Lagrangian-primal dual methods (one tempting approach to solve this optimization problem) cannot be used in this setting. In fact, unlike in single-agent scenarios, CMPGs do not satisfy strong duality, rendering such approaches inapplicable and potentially unsafe. To tackle the CMPG problem, we propose a novel algorithm Coordinate-Ascent for CMPGs with Exploration (CA-CMPG-E), which provably converges to a Nash policy in tabular, finite-horizon CMPGs. The idea behind the algorithm is to solve for each agent a Constrained Markov Decision Process and update the joint policy in the direction of the steepest improvement. Furthermore, we provide the first sample complexity bounds for learning Nash policies in unknown CMPGs guaranteeing safe exploration.},
  archive   = {C_AAMAS},
  author    = {Alatur, Pragnya and Ramponi, Giorgia and He, Niao and Krause, Andreas},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {31–39},
  title     = {Provably learning nash policies in constrained markov potential games},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662849},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Can poverty be reduced by acting on discrimination? An
agent-based model for policy making. <em>AAMAS</em>, 22–30. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the last decades, there has been a deceleration in the rates of poverty reduction, suggesting that traditional redistributive approaches to poverty mitigation could be losing effectiveness, and alternative insights to advance the number one UN Sustainable Development Goal are required. The criminalization of poor people has been denounced by several NGOs, and an increasing number of voices suggest that discrimination against the poor (a phenomenon known as aporophobia) could be an impediment to mitigating poverty. In this paper, we present the novel Aporophobia Agent-Based Model (AABM) to provide evidence of the correlation between aporophobia and poverty computationally. We present our use case built with real-world demographic data and poverty-mitigation public policies (either enforced or under parliamentary discussion) for the city of Barcelona. We classify policies as discriminatory or non-discriminatory against the poor, with the support of specialized NGOs, and we observe the results in the AABM in terms of the impact on wealth inequality. The simulation provides evidence of the relationship between aporophobia and the increase of wealth inequality levels, paving the way for a new generation of poverty reduction policies that act on discrimination and tackle poverty as a societal problem (not only a problem of the poor).},
  archive   = {C_AAMAS},
  author    = {Aguilera, Alba and Montes, Nieves and Curto, Georgina and Sierra, Carles and Osman, Nardine},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {22–30},
  title     = {Can poverty be reduced by acting on discrimination? an agent-based model for policy making},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662848},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Value-based resource matching with fairness criteria:
Application to agricultural water trading. <em>AAMAS</em>, 13–21. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Optimal allocation of agricultural water in the event of droughts is an important global problem. In addressing this problem, many aspects, including the welfare of farmers, the economy, and the environment, must be considered. Under this backdrop, our work focuses on several resource-matching problems accounting for agents with multi-crop portfolios, geographic constraints, and fairness. First, we address a matching problem where the goal is to maximize a welfare function in two-sided markets where buyers&#39; requirements and sellers&#39; supplies are represented by value functions that assign prices (or costs) to specified volumes of water. For the setting where the value functions satisfy certain monotonicity properties, we present an efficient algorithm that maximizes a social welfare function. When there are minimum water requirement constraints, we present a randomized algorithm which ensures that the constraints are satisfied in expectation. For a single seller--multiple buyers setting with fairness constraints, we design an efficient algorithm that maximizes the minimum level of satisfaction of any buyer. We also present computational complexity results that highlight the limits on the generalizability of our results. We evaluate the algorithms developed in our work with experiments on both real-world and synthetic data sets with respect to drought severity, value functions, and seniority of agents.},
  archive   = {C_AAMAS},
  author    = {Adiga, Abhijin and Trabelsi, Yohai and Ferdousi, Tanvir and Marathe, Madhav and Ravi, S. S. and Swarup, Samarth and Vullikanti, Anil Kumar and Wilson, Mandy L. and Kraus, Sarit and Basu, Reetwika and Savalkar, Supriya and Yourek, Matthew and Brady, Michael and Rajagopalan, Kirti and Yoder, Jonathan},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {13–21},
  title     = {Value-based resource matching with fairness criteria: Application to agricultural water trading},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662847},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Team performance and user satisfaction in mixed human-agent
teams. <em>AAMAS</em>, 4–12. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human-agent teams, consisting of at least one human and one agent teaming together to achieve a common objective, are increasingly prevalent and effective in both social and industrial spheres. Associated changes in human preferences and expectations from autonomous teammates will continue to shape and alter collaboration opportunities and dynamics within human-agent teams. New environments are emerging, including Ad Hoc teams where teammates collaborate without pre-coordination or prior knowledge of other teammates capabilities. Team members in ad hoc human-agent teams have to collaborate to find tasks allocations to effectively leverage teammate capabilities to improve team performance and human satisfaction. In this paper, we investigate ad hoc team dynamics under different team compositions, including those comprised of only humans or of human and agent team members, as well as teams consisting of more than two members. Experiments are run with MTurk workers and several hypotheses are evaluated on the effects of teammate type and team size on team performance and human satisfaction using a collaborative Human-Agent Taskboard (CHATboard) platform where teams repeatedly collaborate to complete assigned tasks.},
  archive   = {C_AAMAS},
  author    = {Abuhaimed, Sami and Sen, Sandip},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {4–12},
  title     = {Team performance and user satisfaction in mixed human-agent teams},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662846},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 30 years of engineering multi-agent systems: What and why?
<em>AAMAS</em>, 3. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Research on Engineering Multi-Agent Systems (EMAS) aims to provide software engineers with what they need to be able to effectively develop multi-agent systems. The field is broad, covering foundational concepts, notations, processes, techniques, tools, and languages. Although it is difficult to pin down a precise year, it can be argued that the community is now 30 years old [1]. This talk will review where the field is, and outline some challenges and future directions. In addition to answering the question &quot;What is EMAS?&quot; the talk aims to raise the profile of the field and the community, answering the question &quot;Why should I care?&quot;.},
  archive   = {C_AAMAS},
  author    = {Winikoff, Michael},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {3},
  title     = {30 years of engineering multi-agent systems: What and why?},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662844},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Agents and humans: Trajectories and perspectives.
<em>AAMAS</em>, 2. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The AAMAS conference was established in 2002 as the merger of three highly successful conferences: AA (the International Conference on Autonomous Agents), ICMAS (the International Conference on Multiagent Systems) and ATAL (the International Workshop on Agent Theories, Architectures, and Languages). In this talk I draw on my own experiences and that of others in investigating human-agent collectives. I reflect on aspects of the trajectory of research topics in the AAMAS community over the past 20+ years, and on selected challenges for human-centred AI.},
  archive   = {C_AAMAS},
  author    = {Sonenberg, Liz},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2},
  title     = {Agents and humans: Trajectories and perspectives},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662843},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trustworthy reinforcement learning: Opportunities and
challenges. <em>AAMAS</em>, 1. (<a
href="https://dl.acm.org/doi/10.5555/3635637.3662842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement Learning (RL) has long outgrown the traditional representations that guaranteed policy convergence but severely limited its application to complex domains. Modern Deep RL enables far richer and complex behaviour, yet at the cost of transparency and explainability. While these latter issues have recently received much attention in Machine Learning, they are underexplored in RL. In this talk, I will discuss them from multiple angles, survey state-of-the-art approaches, including recent developments in policy distillation and formal guarantees, and touch upon the related question of fairness.},
  archive   = {C_AAMAS},
  author    = {Now\&#39;{e}, Ann},
  booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1},
  title     = {Trustworthy reinforcement learning: Opportunities and challenges},
  url       = {https://dl.acm.org/doi/10.5555/3635637.3662842},
  year      = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
