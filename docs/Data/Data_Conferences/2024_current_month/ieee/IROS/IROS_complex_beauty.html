<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IROS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="iros---1581">IROS - 1581</h2>
<ul>
<li><details>
<summary>
(2024). Deep geometric potential functions for tracking on
manifolds. <em>IROS</em>, i–viii. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce a novel approach for designing invariant control laws through potential functions for fully actuated dynamical systems evolving on manifolds by leveraging the power of neural networks. The geometry and non-linearity inherent to manifold-based dynamical systems pose challenges for traditional control law design, necessitating techniques with the interplay of differential geometry and dynamical systems for ensuring stability. Apart from stability, performance and optimality are other challenging areas to address for dynamical systems evolving on manifolds. On top of these, the concept of invariance helps us improve learning transferability skills from one scene to another scene. We propose invariant potential functions on manifolds defined by neural networks that can be used to generate elastic forces for asymptotic tracking of trajectories. The weights of the potential function can be tuned to shape the potential functions according to the performance requirements through minimizing a loss function.},
  archive   = {C_IROS},
  author    = {Nikhil Potu Surya Prakash and Joohwan Seo and Koushil Sreenath and Jongeun Choi and Roberto Horowitz},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801512},
  month     = {10},
  pages     = {i-viii},
  title     = {Deep geometric potential functions for tracking on manifolds},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UW-SDF: Exploiting hybrid geometric priors for neural SDF
reconstruction from underwater multi-view monocular images.
<em>IROS</em>, 14248–14255. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Due to the unique characteristics of underwater environments, accurate 3D reconstruction of underwater objects poses a challenging problem in tasks such as underwater exploration and mapping. Traditional methods that rely on multiple sensor data for 3D reconstruction are time-consuming and face challenges in data acquisition in underwater scenarios. We propose UW-SDF, a framework for reconstructing target objects from multi-view underwater images based on neural SDF. We introduce hybrid geometric priors to optimize the reconstruction process, markedly enhancing the quality and efficiency of neural SDF reconstruction. Additionally, to address the challenge of segmentation consistency in multi-view images, we propose a novel few-shot multi-view target segmentation strategy using the general-purpose segmentation model (SAM), enabling rapid automatic segmentation of unseen objects. Through extensive qualitative and quantitative experiments on diverse datasets, we demonstrate that our proposed method outperforms the traditional underwater 3D reconstruction method and other neural rendering approaches in the field of underwater 3D reconstruction.},
  archive   = {C_IROS},
  author    = {Zeyu Chen and Jingyi Tang and Gu Wang and Shengquan Li and Xinghui Li and Xiangyang Ji and Xiu Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802499},
  month     = {10},
  pages     = {14248-14255},
  title     = {UW-SDF: Exploiting hybrid geometric priors for neural SDF reconstruction from underwater multi-view monocular images},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). DragTraffic: Interactive and controllable traffic scene
generation for autonomous driving. <em>IROS</em>, 14241–14247. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Evaluating and training autonomous driving systems require diverse and scalable corner cases. However, most existing scene generation methods lack controllability, accuracy, and versatility, resulting in unsatisfactory generation results. Inspired by DragGAN in image generation, we propose DragTraffic, a generalized, interactive, and controllable traffic scene generation framework based on conditional diffusion. DragTraffic enables non-experts to generate a variety of realistic driving scenarios for different types of traffic agents through an adaptive mixture expert architecture. We employ a regression model to provide a general initial solution and a refinement process based on the conditional diffusion model to ensure diversity. User-customized context is introduced through cross-attention to ensure high controllability. Experiments on a real-world driving dataset show that DragTraffic outperforms existing methods in terms of authenticity, diversity, and freedom. Demo videos and code are available at https://chantsss.github.io/Dragtraffic/.},
  archive   = {C_IROS},
  author    = {Sheng Wang and Ge Sun and Fulong Ma and Tianshuai Hu and Qiang Qin and Yongkang Song and Lei Zhu and Junwei Liang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801623},
  month     = {10},
  pages     = {14241-14247},
  title     = {DragTraffic: Interactive and controllable traffic scene generation for autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Shape-prior free space-time neural radiance field for 4D
semantic reconstruction of dynamic scene from sparse-view RGB videos.
<em>IROS</em>, 14233–14240. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many applications in Augmented/Virtual Reality or robotics require precise geometry modeling of individual elements in a dynamic scene under a sparse-view camera setup, without any prior information about their semantic labels or shapes. In our research, we introduce a 3D shape prior-free Neural Radiance Field-based technique for detailed geometry reconstruction under human-object interactions, offering an explicit surface reconstruction with semantic labels for reconstructed geometry. Our approach harnesses the capabilities of an Invertible Neural Network to learn a deformation function that effectively connects local (current-frame input) and canonical spaces for each of the components under motion. The deformation process is guided by temporal constraints from multi-frame, facilitating the precise reconstruction of the complex interactions between humans and objects. Our experimental evaluations highlight the effectiveness of our framework, demonstrating its ability to accurately represent both the comprehensive object-compositional scene and individual components over state-of-the-art methods, under complex interactions between the scene entities. This research is deemed to mark a significant stride in semantic 3D geometry modeling within dynamic interactive environments, relying solely on sparse multi-view RGB data.},
  archive   = {C_IROS},
  author    = {Sandika Biswas and Biplab Banerjee and Hamid Rezatofighi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802221},
  month     = {10},
  pages     = {14233-14240},
  title     = {Shape-prior free space-time neural radiance field for 4D semantic reconstruction of dynamic scene from sparse-view RGB videos},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A framework for reproducible benchmarking and performance
diagnosis of SLAM systems. <em>IROS</em>, 14225–14232. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose SLAMFuse, an open-source SLAM benchmarking framework that provides consistent cross-platform environments for evaluating multi-modal SLAM algorithms, along with tools for data fuzzing, failure detection, and diagnosis across different datasets. Our framework introduces a fuzzing mechanism to test the resilience of SLAM algorithms against dataset perturbations. This enables the assessment of pose estimation accuracy under varying conditions and identifies critical perturbation thresholds. SLAMFuse improves diagnostics with failure detection and analysis tools, examining algorithm behaviour against dataset characteristics. SLAMFuse uses Docker to ensure reproducible testing conditions across diverse datasets and systems by streamlining dependency management. Emphasizing the importance of reproducibility and introducing advanced tools for algorithm evaluation and performance diagnosis, our work sets a new precedent for reliable benchmarking of SLAM systems. We provide ready-to-use docker compatible versions of the algorithms and datasets used in the experiments, together with guidelines for integrating and benchmarking new algorithms. Code is available at https://github.com/nikolaradulov/slamfuse},
  archive   = {C_IROS},
  author    = {Nikola Radulov and Yuhao Zhang and Mihai Bujanca and Ruiqi Ye and Mikel Luján},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801690},
  month     = {10},
  pages     = {14225-14232},
  title     = {A framework for reproducible benchmarking and performance diagnosis of SLAM systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SLIP embodied robust quadruped robot control. <em>IROS</em>,
14219–14224. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent research on quadruped robots has been achieving high-performance motion control based on optimization and reinforcement learning (RL). However, there is still ongoing research aimed at demonstrating that implementing high-performance motion based on simple and dominant dynamic principles is possible. In this paper, we proposed a novel control approach that projects Spring-Loaded Inverted Pendulum (SLIP) dynamics to articulated legs, utilizing admittance control based force observer within a rotating workspace (RWFOB). Unlike other legged robots that depend on sensor-based estimation of external forces, the proposed method presents an alternative approach that reduces the reliance on sensors. Additionally, we introduce a comprehensive control framework for quadruped robot motion control, establishing the connection between trunk and SLIP-realized leg movements using Jacobian. Through comparative analysis with Virtual Model Control (VMC) in simulations, we illustrate the effectiveness of the proposed framework as a robust and reliable trunk feedback controller.},
  archive   = {C_IROS},
  author    = {Jinsong Hong and Changmin Yeo and Sangjin Bae and Jeongwoo Hong and Sehoon Oh},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802545},
  month     = {10},
  pages     = {14219-14224},
  title     = {SLIP embodied robust quadruped robot control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Coordinated multi-arm 3D printing using reeb decomposition.
<em>IROS</em>, 14212–14218. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic additive manufacturing has the potential to replace traditional production techniques with more flexible, capable and efficient methods. However, this potential has not yet been realized due to the intrinsic physical limitations of extruders. Thus improved speed and efficiency lies in coordinated fabrication by multiple extruders. In this paper, we propose a framework for utilizing multiple extruders to collaboratively fabricate objects in a shared workspace. In contrast to related work, we make use of a Reeb decomposition method of the input model, which dramatically reduces the search space over feasible toolpaths but still results in highly effective allocation of model components to each extruder. We demonstrate superior performance of our approach in simulation as well as in hardware with a two-robot arm extruder system. When compared to a single extruder approach over a benchmark of 14 models, our method achieves a mean improvement of 72% in extruder utilization. When compared to two additional toolpath planning methods for multiple extruders, only our method is able to successfully complete a valid toolpath on all models. Prior methods fail on more than half of our benchmark due to reaching deadlocked states during toolpath planning. On the models for which all methods succeed, our approach achieves mean utilization improvements of 132% over zone-blocking and 12% over contour greedy. We demonstrate that our approach is also suitable for models with non-planar slicings, which yield further improvements in extruder utilization. For more results and information see: https://sites.google.com/view/multi-arm-reeb.},
  archive   = {C_IROS},
  author    = {Jayant Khatkar and Fouad Sukkar and Lee Clemon and Ramgopal Mettu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801992},
  month     = {10},
  pages     = {14212-14218},
  title     = {Coordinated multi-arm 3D printing using reeb decomposition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Zero123-6D: Zero-shot novel view synthesis for RGB
category-level 6D pose estimation. <em>IROS</em>, 14204–14211. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Estimating the pose of objects through vision is essential to make robotic platforms interact with the environment. Yet, it presents many challenges, often related to the lack of flexibility and generalizability of state-of-the-art solutions. Diffusion models are a cutting-edge neural architecture transforming 2D and 3D computer vision, outlining remarkable performances in zero-shot novel-view synthesis. Such a use case is particularly intriguing for reconstructing 3D objects. However, localizing objects in unstructured environments is rather unexplored. To this end, this work presents Zero123-6D, the first work to demonstrate the utility of Diffusion Model-based novel-view-synthesizers in enhancing RGB 6D pose estimation at category-level, by integrating them with feature extraction techniques. Novel View Synthesis allows to obtain a coarse pose that is refined through an online optimization method introduced in this work to deal with intra-category geometric differences. In such a way, the outlined method shows reduction in data requirements, removal of the necessity of depth information in zero-shot category-level 6D pose estimation task, and increased performance, quantitatively demonstrated through experiments on the CO3D dataset.},
  archive   = {C_IROS},
  author    = {Francesco Di Felice and Alberto Remus and Stefano Gasperini and Benjamin Busam and Lionel Ott and Federico Tombari and Roland Siegwart and Carlo Alberto Avizzano},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802157},
  month     = {10},
  pages     = {14204-14211},
  title     = {Zero123-6D: Zero-shot novel view synthesis for RGB category-level 6D pose estimation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient multimodal semantic segmentation via dual-prompt
learning. <em>IROS</em>, 14196–14203. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multimodal (e.g., RGB-Depth/RGB-Thermal) fusion has shown great potential for improving semantic segmentation in complex scenes (e.g., indoor/low-light conditions). Existing approaches often fully fine-tune a dual-branch encoder-decoder framework with a complicated feature fusion strategy for achieving multimodal semantic segmentation, which is training-costly due to the massive parameter updates in feature extraction and fusion. To address this issue, we propose a surprisingly simple yet effective dual-prompt learning network (dubbed DPLNet) for training-efficient multimodal (e.g., RGBD/T) semantic segmentation. The core of DPLNet is to directly adapt a frozen pre-trained RGB model to multimodal semantic segmentation, reducing parameter updates. For this purpose, we present two prompt learning modules, comprising multimodal prompt generator (MPG) and multimodal feature adapter (MFA). MPG works to fuse the features from different modalities in a compact manner and is inserted from shallow to deep stages to generate the multi-level multimodal prompts that are injected into the frozen backbone, while MFA adapts prompted multimodal features in the frozen backbone for better multimodal semantic segmentation. Since both the MPG and MFA are lightweight, only a few trainable parameters (3.88M, 4.4% of the pre-trained backbone parameters) are introduced for multimodal feature fusion and learning. Using a simple decoder (3.27M parameters), DPLNet achieves new state-of-the-art performance or is on a par with other complex approaches on four RGB-D/T semantic segmentation datasets while satisfying parameter efficiency. Moreover, we show DPLNet is general and applicable to other multimodal segmentation tasks. Without special design, DPLNet outperforms many complicated models. The source code can be found at https://github.com/ShaohuaDong2021/DPLNet.},
  archive   = {C_IROS},
  author    = {Shaohua Dong and Yunhe Feng and Qing Yang and Yan Huang and Dongfang Liu and Heng Fan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801872},
  month     = {10},
  pages     = {14196-14203},
  title     = {Efficient multimodal semantic segmentation via dual-prompt learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DMFuser: Distilled multi-task learning for end-to-end
transformer-based sensor fusion in autonomous driving. <em>IROS</em>,
14188–14195. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In end-to-end autonomous driving, current sensor fusion and navigational control techniques used by imitation learning algorithms are insufficient in challenging scenarios involving multiple dynamic agents and result in poor driving capabilities. To tackle this issue, we introduce DMFuser, a transformer-based algorithm that employs knowledge distillation between multi-task student and single-task teachers and combines attention and convolutions to fuse multiple RGB-D camera representations to produce vehicular navigational commands (throttle, steering and brake). Our model incorporates two modules. The first module, perception, encodes data from RGB-D cameras for tasks like semantic segmentation, semantic depth cloud (SDC) mapping, and traffic light state recognition. To enhance feature extraction and fusion from both RGB and depth sources, we harness local and global capabilities of convolution and transformer modules. We employ an attention-CNN fusion structure to effectively learn and fuse RGB and SDC map features. Subsequently, the control module decodes these features along with supplementary data, containing environment’s static and dynamic information, to predict waypoints and vehicular control actions. We evaluate the model and conduct a comparative analysis, in various scenarios, weather conditions, and traffic situations, spanning from normal to adversarial in the CARLA simulator. We achieve better or comparable results in term of driving score (DS) and other metrics with respect to our baselines. Also, our ablation studies demonstrate the effectiveness of our contributions to improve the driving skills. Our code is available at the following github page: https://github.com/pagand/e2etransfuser},
  archive   = {C_IROS},
  author    = {Pedram Agand and Mohammad Mahdavian and Manolis Savva and Mo Chen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802740},
  month     = {10},
  pages     = {14188-14195},
  title     = {DMFuser: Distilled multi-task learning for end-to-end transformer-based sensor fusion in autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PoCo: Point context cluster for RGBD indoor place
recognition. <em>IROS</em>, 14180–14187. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel end-to-end algorithm (PoCo) for the indoor RGB-D place recognition task, aimed at identifying the most likely match for a given query frame within a reference database. The task presents inherent challenges attributed to the constrained field of view and limited range of perception sensors. We propose a new network architecture, which generalizes the recent Context of Clusters (CoCs) to extract global descriptors directly from the noisy point clouds through end-to-end learning. Moreover, we develop the architecture by integrating both color and geometric modalities into the point features to enhance the global descriptor representation. We conducted evaluations on public datasets ScanNet-PR and ARKit with 807 and 5047 scenarios, respectively. PoCo achieves SOTA performance: on ScanNet-PR, we achieve R@1 of 64.63%, a 5.7% improvement from the best-published result CGis (61.12%); on Arkit, we achieve R@1 of 45.12%, a 13.3% improvement from the best-published result CGis (39.82%). In addition, PoCo shows higher efficiency than CGis in inference time (1.75X-faster), and we demonstrate the effectiveness of PoCo in recognizing places within a real-world laboratory environment. Video: https://youtu.be/D8dObAeMiCw;},
  archive   = {C_IROS},
  author    = {Jing Liang and Zhuo Deng and Zheming Zhou and Omid Ghasemalizadeh and Dinesh Manocha and Min Sun and Cheng-Hao Kuo and Arnie Sen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802135},
  month     = {10},
  pages     = {14180-14187},
  title     = {PoCo: Point context cluster for RGBD indoor place recognition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Competitive multi-team behavior in dynamic flight scenarios.
<em>IROS</em>, 14172–14179. (<a
href="https://doi.org/10.1109/IROS58592.2024.10803062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Efficiently learning strategic multi-agent behavior remains a challenge for robotic systems deployed in real-world scenarios, especially when considering underactuated or dynamically unstable systems. Such systems demand an integrated approach that informs long-term strategic planning with constraints imposed by reactive control, and vice versa, to effectively accomplish task objectives in competitive scenarios. In this paper, we introduce a hierarchical control model to address this: a high-level controller synthesizes strategic guidance from aggregated team experiences, while a low-level controller formulates corresponding task-specific continuous controls. We apply this concept to coordination of competitive multi-team behavior in dynamic flight scenarios with F-16 aircraft. This work introduces a hierarchical reinforcement learning approach for multi-agent coordination, leveraging decoupled distributional value representations at the high-level together with goal-conditioned policy learning at the low-level, providing a control structure that integrates long-horizon strategic planning with short-horizon dynamic control. We further provide a parallel simulator for efficient learning with multi-agent F-16 dynamics.},
  archive   = {C_IROS},
  author    = {Tim Seyde and Mathias Lechner and Joshua Rountree and Daniela Rus},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10803062},
  month     = {10},
  pages     = {14172-14179},
  title     = {Competitive multi-team behavior in dynamic flight scenarios},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Equivariant ensembles and regularization for reinforcement
learning in map-based path planning. <em>IROS</em>, 14164–14171. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In reinforcement learning (RL), exploiting environmental symmetries can significantly enhance efficiency, robustness, and performance. However, ensuring that the deep RL policy and value networks are respectively equivariant and invariant to exploit these symmetries is a substantial challenge. Related works try to design networks that are equivariant and invariant by construction, limiting them to a very restricted library of components, which in turn hampers the expressiveness of the networks. This paper proposes a method to construct equivariant policies and invariant value functions without specialized neural network components, which we term equivariant ensembles. We further add a regularization term for adding inductive bias during training. In a map-based path planning case study, we show how equivariant ensembles and regularization benefit sample efficiency and performance.},
  archive   = {C_IROS},
  author    = {Mirco Theile and Hongpeng Cao and Marco Caccamo and Alberto L. Sangiovanni-Vincentelli},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801688},
  month     = {10},
  pages     = {14164-14171},
  title     = {Equivariant ensembles and regularization for reinforcement learning in map-based path planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MFC-EQ: Mean-field control with envelope q-learning for
moving decentralized agents in formation. <em>IROS</em>, 14156–14163.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study a decentralized version of Moving Agents in Formation (MAiF), a variant of Multi-Agent Path Finding aiming to plan collision-free paths for multiple agents with the dual objectives of reaching their goals quickly while maintaining a desired formation. The agents must balance these objectives under conditions of partial observation and limited communication. The formation maintenance depends on the joint state of all agents, whose dimensionality increases exponentially with the number of agents, rendering the learning process intractable. Additionally, learning a single policy that can accommodate different linear preferences for these two objectives presents a significant challenge. In this paper, we propose Mean-Field Control with Envelop Q-learning (MFC-EQ), a scalable and adaptable learning framework for this bi-objective multi-agent problem. We approximate the dynamics of all agents using mean-field theory while learning a universal preference-agnostic policy through envelop Q-learning. Our empirical evaluation of MFC-EQ across numerous instances shows that it outperforms state-of-the-art centralized MAiF baselines. Furthermore, MFC-EQ effectively handles more complex scenarios where the desired formation changes dynamically—a challenge that existing MAiF planners cannot address.},
  archive   = {C_IROS},
  author    = {Qiushi Lin and Hang Ma},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802293},
  month     = {10},
  pages     = {14156-14163},
  title     = {MFC-EQ: Mean-field control with envelope Q-learning for moving decentralized agents in formation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient global trajectory planning for multi-robot system
with affinely deformable formation. <em>IROS</em>, 14148–14155. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Global trajectory planning is crucial for long-range formation navigation tasks of multi-robot systems in efficiency improvement and energy saving, whose main challenges are the joint space constraints of the whole team and the long-range deployment. To overcome the above difficulties, we reformulate the original problem into an affine formation planning problem in parameter space. Further, we propose a front-end &amp; back-end framework for global trajectory planning of Multi-Robot Systems (MRS) with affinely deformable formation. For the front-end, an RL-steering affine formation RRT* method is designed to search a global formation-level trajectory in affine parameter space, combining the efficient BVP-solving capability of RL and the global guidance and generalizing ability of RRT*. For the back-end, we propose a formationlevel affine parameter trajectory optimization method to refine the front-end trajectory, and further transform it into peragent trajectories for execution. Extensive benchmarks and ablation experiments in simulation show the effectiveness of our framework for the global trajectory generation of a multiUAV system with affinely deformable formation. The appendix can be seen here3.},
  archive   = {C_IROS},
  author    = {Hao Sha and Yuxiang Cui and Wangtao Lu and Dongkun Zhang and Chaoqun Wang and Jun Wu and Rong Xiong and Yue Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802518},
  month     = {10},
  pages     = {14148-14155},
  title     = {Efficient global trajectory planning for multi-robot system with affinely deformable formation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contact stability control of stepping over partial footholds
using plantar tactile feedback. <em>IROS</em>, 14141–14147. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents a novel method to keep stable contact and balance while stepping over partial footholds for biped humanoid robots with flat feet. We exploit plantar tactile feedback to detect the geometry of the terrain and reconstruct online the new supporting polygon after landing every step. Plantar tactile feedback detects early contacts to stop the swing foot motion. Then we compute the convex hull of the cluster of contact points detected by distributed normal force sensors over the foot soles. The centroid of the supporting polygon is then used for retargeting the reference ZMP and DCM positions. Finally, the supporting polygon is used to define constraints for ZMP balance feedback control. These methods were implemented in two biped humanoid robots running different walking controllers.},
  archive   = {C_IROS},
  author    = {J. Rogelio Guadarrama-Olvera and Shuuji Kajita and Fumio Kanehiro and Gordon Cheng},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802686},
  month     = {10},
  pages     = {14141-14147},
  title     = {Contact stability control of stepping over partial footholds using plantar tactile feedback},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reconfigurable robot identification from motion data.
<em>IROS</em>, 14133–14140. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Integrating Large Language Models (LLMs) and Vision-Language Models (VLMs) with robotic systems enables robots to process and understand complex natural language instructions and visual information. However, a fundamental challenge remains: for robots to fully capitalize on these advancements, they must have a deep understanding of their physical embodiment. The gap between AI models’ cognitive capabilities and the understanding of physical embodiment leads to the following question: Can a robot autonomously understand and adapt to its physical form and functionalities through interaction with its environment? This question underscores the transition towards developing self-modeling robots without reliance on external sensory or pre-programmed knowledge about their structure. Here, we propose a meta-self-modeling that can deduce robot morphology through proprioception—the robot’s internal sense of its body’s position and movement. Our study introduces a 12-DoF reconfigurable legged robot, accompanied by a diverse dataset of 200k unique configurations, to systematically investigate the relationship between robotic motion and robot morphology. Utilizing a deep neural network model comprising a robot signature encoder and a configuration decoder, we demonstrate the capability of our system to accurately predict robot configurations from proprioceptive signals. This research contributes to the field of robotic self-modeling, aiming to enhance robot’s understanding of their physical embodiment and adaptability in real-world scenarios.},
  archive   = {C_IROS},
  author    = {Yuhang Hu and Yunzhe Wang and Ruibo Liu and Zhou Shen and Hod Lipson},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801809},
  month     = {10},
  pages     = {14133-14140},
  title     = {Reconfigurable robot identification from motion data},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SCOML: Trajectory planning based on self-correcting
meta-reinforcement learning in hybrid terrain for mobile robot.
<em>IROS</em>, 14125–14132. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trajectory planning is important for ground robots to achieve safe and efficient autonomous navigation in unstructured off-road environments. Most existing methods treat each terrain as a single type. However, in the real world, a ground usually consists of hybrid terrains. In this work, we propose a novel trajectory planning network that handles hybrid terrain. To further enhance safety, we have designed a self-correcting structure based on historical planning data. This structure can correct the trajectory when an inappropriate one is planned. To train the network, we introduce a two-stage training scheme based on Offline Meta-Reinforcement Learning, which can train the network with pre-collected non-optimal datasets and reduce the occurrence of hazardous planning. The proposed approach has been evaluated on both simulated datasets and a real robot platform. Compared to state-of-the-art baseline methods, the proposed approach reduces hazardous planning by 59.3% in hybrid terrains.},
  archive   = {C_IROS},
  author    = {Andong Yang and Wei Li and Yu Hu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801626},
  month     = {10},
  pages     = {14125-14132},
  title     = {SCOML: Trajectory planning based on self-correcting meta-reinforcement learning in hybrid terrain for mobile robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient balance detection for modular robots.
<em>IROS</em>, 14119–14124. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we explore the field of self-reconfigurable modular robots, representing a significant advance in robotic technology. These robots have many capabilities, offering high adaptability and flexibility for a variety of applications. However, computing the stability is challenging as it is computationally intensive, it needs to be distributed and fast, as close as possible of real-time. In this article, we introduce a distributed algorithm designed to overcome these challenges while taking mechanical constraints into account. At the heart of this algorithm is the notion of the &quot;support polygon&quot;, which enables the stability of a modular robot to be assessed in real time. The algorithm is based on a fully distributed tree partitioning approach, facilitating efficient communication and collaboration between modules. The algorithm also uses a polygon merging approach to reduce the number of messages when creating the polygon support, thus significantly reducing response time. In fact, the response time of the method used is very small compared to other research. We also present simulation results on a simulator, VisibleSim, as well as experimental validation on real robotic modules, which underlines the practical viability of the approach. Overall, this work lays a solid base for further advances aiming to guarantee the stability of modular robots.},
  archive   = {C_IROS},
  author    = {Ikrame Yazidi and Benoît Piranda and Morvan Ouisse and Julien Bourgeois},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802149},
  month     = {10},
  pages     = {14119-14124},
  title     = {Efficient balance detection for modular robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BEVPose: Unveiling scene semantics through pose-guided
multi-modal BEV alignment. <em>IROS</em>, 14111–14118. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the field of autonomous driving and mobile robotics, there has been a significant shift in the methods used to create Bird’s Eye View (BEV) representations. This shift is characterised by using transformers and learning to fuse measurements from disparate vision sensors, mainly lidar and cameras, into a 2D planar ground-based representation. However, these learning-based methods for creating such maps often rely heavily on extensive annotated data, presenting notable challenges, particularly in diverse or non-urban environments where large-scale datasets are scarce. In this work, we present BEVPose, a framework that integrates BEV representations from camera and lidar data, using sensor pose as a guiding supervisory signal. This method notably reduces the dependence on costly annotated data. By leveraging pose information, we align and fuse multi-modal sensory inputs, facilitating the learning of latent BEV embeddings that capture both geometric and semantic aspects of the environment. Our pretraining approach demonstrates promising performance in BEV map segmentation tasks, outperforming fully-supervised state-of the-art methods, while necessitating only a minimal amount of annotated data. This development not only confronts the challenge of data efficiency in BEV representation learning but also broadens the potential for such techniques in a variety of domains, including off-road and indoor environments.},
  archive   = {C_IROS},
  author    = {Mehdi Hosseinzadeh and Ian Reid},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801390},
  month     = {10},
  pages     = {14111-14118},
  title     = {BEVPose: Unveiling scene semantics through pose-guided multi-modal BEV alignment},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TeFF: Tracking-enhanced forgetting-free few-shot 3D LiDAR
semantic segmentation. <em>IROS</em>, 14103–14110. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In autonomous driving, 3D LiDAR plays a crucial role in understanding the vehicle’s surroundings. However, the newly emerged, unannotated objects presents few-shot learning problem for semantic segmentation. This paper addresses the limitations of current few-shot semantic segmentation by exploiting the temporal continuity of LiDAR data. Employing a tracking model to generate pseudo-ground-truths from a sequence of LiDAR frames, our method significantly augments the dataset, enhancing the model’s ability to learn on novel classes. However, this approach introduces a data imbalance biased to novel data that presents a new challenge of catastrophic forgetting. To mitigate this, we incorporate LoRA, a technique that reduces the number of trainable parameters, thereby preserving the model’s performance on base classes while improving its adaptability to novel classes. This work represents a significant step forward in few-shot 3D LiDAR semantic segmentation for autonomous driving. Our code is available at https://github.com/BowmanChow/Track-no-forgetting.},
  archive   = {C_IROS},
  author    = {Junbao Zhou and Jilin Mei and Pengze Wu and Liang Chen and Fangzhou Zhao and Xijun Zhao and Yu Hu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801543},
  month     = {10},
  pages     = {14103-14110},
  title     = {TeFF: Tracking-enhanced forgetting-free few-shot 3D LiDAR semantic segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning from spatio-temporal correlation for
semi-supervised LiDAR semantic segmentation. <em>IROS</em>, 14095–14102.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the challenges of the semi-supervised LiDAR segmentation (SSLS) problem, particularly in low-budget scenarios. The two main issues in low-budget SSLS are the poor-quality pseudo-labels for unlabeled data, and the performance drops due to the significant imbalance between ground-truth and pseudo-labels. This imbalance leads to a vicious training cycle. To overcome these challenges, we leverage the spatio-temporal prior by recognizing the substantial overlap between temporally adjacent LiDAR scans. We propose a proximity-based label estimation, which generates highly accurate pseudo-labels for unlabeled data by utilizing semantic consistency with adjacent labeled data. Additionally, we enhance this method by progressively expanding the pseudo-labels from the nearest unlabeled scans, which helps significantly reduce errors linked to dynamic classes. Additionally, we employ a dual-branch structure to mitigate performance degradation caused by data imbalance. Experimental results demonstrate remarkable performance in low-budget settings (i.e., ≤ 5%) and meaningful improvements in normal budget settings (i.e., 5 – 50%). Finally, our method has achieved new state-of-the-art results on SemanticKITTI and nuScenes in semi-supervised LiDAR segmentation. With only 5% labeled data, it offers competitive results against fully-supervised counterparts. Moreover, it surpasses the performance of the previous state-of-the-art at 100% labeled data (75.2%) using only 20% of labeled data (76.0%) on nuScenes. The code is available on https://github.com/halbielee/PLE.},
  archive   = {C_IROS},
  author    = {Seungho Lee and Hwijeong Lee and Hyunjung Shim},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801453},
  month     = {10},
  pages     = {14095-14102},
  title     = {Learning from spatio-temporal correlation for semi-supervised LiDAR semantic segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HeLiMOS: A dataset for moving object segmentation in 3D
point clouds from heterogeneous LiDAR sensors. <em>IROS</em>,
14087–14094. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Moving object segmentation (MOS) using a 3D light detection and ranging (LiDAR) sensor is crucial for scene understanding and identification of moving objects. Despite the availability of various types of 3D LiDAR sensors in the market, MOS research still predominantly focuses on 3D point clouds from mechanically spinning omnidirectional LiDAR sensors. Thus, we are, for example, lacking a dataset with MOS labels for point clouds from solid-state LiDAR sensors which have irregular scanning patterns. In this paper, we present a labeled dataset, called HeLiMOS, that enables to test MOS approaches on four heterogeneous LiDAR sensors, including two solid-state LiDAR sensors. Furthermore, we introduce a novel automatic labeling method to substantially reduce the labeling effort required from human annotators. To this end, our framework exploits an instance-aware static map building approach and tracking-based false label filtering. Finally, we provide experimental results regarding the performance of commonly used state-of-the-art MOS approaches on HeLiMOS that suggest a new direction for a sensor-agnostic MOS, which generally works regardless of the type of LiDAR sensors used to capture 3D point clouds. Our dataset is available at https://sites.google.com/view/helimos.},
  archive   = {C_IROS},
  author    = {Hyungtae Lim and Seoyeon Jang and Benedikt Mersch and Jens Behley and Hyun Myung and Cyrill Stachniss},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801938},
  month     = {10},
  pages     = {14087-14094},
  title     = {HeLiMOS: A dataset for moving object segmentation in 3D point clouds from heterogeneous LiDAR sensors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Infrastructure-less UWB-based active relative localization.
<em>IROS</em>, 14079–14086. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In multi-robot systems, relative localization between platforms plays a crucial role in many tasks, such as leader following, target tracking, or cooperative maneuvering. State of the Art (SotA) approaches either rely on infrastructure-based or on infrastructure-less setups. The former typically achieve high localization accuracy but require fixed external structures. The latter provide more flexibility, however, most of the works use cameras or lidars that require Line-of-Sight (LoS) to operate. Ultra Wide Band (UWB) devices are emerging as a viable alternative to build infrastructure-less solutions that do not require LoS. These approaches directly deploy the UWB sensors on the robots. However, they require that at least one of the platforms is static, limiting the advantages of an infrastructure-less setup. In this work, we remove this constraint and introduce an active method for infrastructureless relative localization. Our approach allows the robot to adapt its position to minimize the relative localization error of the other platform. To this aim, we first design a specialized anchor placement for the active localization task. Then, we propose a novel UWB Relative Localization Loss that adapts the Geometric Dilution Of Precision metric to the infrastructureless scenario. Lastly, we leverage this loss function to train an active Deep Reinforcement Learning-based controller for UWB relative localization. An extensive simulation campaign and real-world experiments validate our method, showing up to a 60% reduction of the localization error compared to current SotA approaches.},
  archive   = {C_IROS},
  author    = {Valerio Brunacci and Alberto Dionigi and Alessio De Angelis and Gabriele Costante},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801618},
  month     = {10},
  pages     = {14079-14086},
  title     = {Infrastructure-less UWB-based active relative localization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A slices perspective for incremental nonparametric inference
in high dimensional state spaces. <em>IROS</em>, 14071–14078. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce an innovative method for incremental nonparametric probabilistic inference in high-dimensional state spaces. Our approach leverages slices from highdimensional surfaces to efficiently approximate posterior distributions of any shape. Unlike many existing graph-based methods, our slices perspective eliminates the need for additional intermediate reconstructions, maintaining a more accurate representation of posterior distributions. Additionally, we propose a novel heuristic to balance between accuracy and efficiency, enabling real-time operation in nonparametric scenarios. In empirical evaluations on synthetic and real-world datasets, our slices approach consistently outperforms other state-of-the-art methods. It demonstrates superior accuracy and achieves a significant reduction in computational complexity, often by an order of magnitude.},
  archive   = {C_IROS},
  author    = {Moshe Shienman and Ohad Levy-Or and Michael Kaess and Vadim Indelman},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801340},
  month     = {10},
  pages     = {14071-14078},
  title     = {A slices perspective for incremental nonparametric inference in high dimensional state spaces},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast and communication-efficient multi-UAV exploration via
voronoi partition on dynamic topological graph. <em>IROS</em>,
14063–14070. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Efficient data transmission and reasonable task allocation are important to improve multi-robot exploration efficiency. However, most communication data types typically contain redundant information and thus require massive communication volume. Moreover, exploration-oriented task allocation is far from trivial and becomes even more challenging for resource-limited unmanned aerial vehicles (UAVs). In this paper, we propose a fast and communication-efficient multi-UAV exploration method for exploring large environments. We first design a multi-robot dynamic topological graph (MR-DTG) consisting of nodes representing the explored and exploring regions and edges connecting nodes. Supported by MR-DTG, our method achieves efficient communication by only transferring the necessary information required by exploration planning. To further improve the exploration efficiency, a hierarchical multi-UAV exploration method is devised using MR-DTG. Specifically, the graph Voronoi partition is used to allocate MR-DTG’s nodes to the closest UAVs, considering the actual motion cost, thus achieving reasonable task allocation. To our knowledge, this is the first work to address multi-UAV exploration using graph Voronoi partition. The proposed method is compared with a state-of-the-art method in simulations. The results show that the proposed method is able to reduce the exploration time and communication volume by up to 38.3% and 95.5%, respectively. Finally, the effectiveness of our method is validated in the real-world experiment with 6 UAVs. We will release the source code to benefit the community.},
  archive   = {C_IROS},
  author    = {Qianli Dong and Haobo Xi and Shiyong Zhang and Qingchen Bi and Tianyi Li and Ziyu Wang and Xuebo Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801613},
  month     = {10},
  pages     = {14063-14070},
  title     = {Fast and communication-efficient multi-UAV exploration via voronoi partition on dynamic topological graph},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Priority-based deadlock recovery for distributed swarm
obstacle avoidance in cluttered environments. <em>IROS</em>,
14056–14062. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel hierarchical priority mechanism for deadlock recovery of distributed swarm via on-demand collision avoidance in cluttered dynamic environments. The proposed priority mechanism dynamically assigns certain priority and an optimized detour point for each agent based on its spatial context to avoid deadlocks which are predicted by properly designed deadlock conditions; as a byproduct, this priority mechanism allows us to effectively resolve livelocks as well. The resulting optimization problem is then solved by polar reformulation and alternating minimization methods. Simulation results demonstrate that, in both static and dynamic environments, our method (termed PriDRAM) outperforms the baseline Alternating Minimization Swarm (AMSwarm) method which does not explicitly account for deadlock recovery, with a 10.5% improvement in average smoothness and a 4.8% reduction in flight time. Moreover, for narrow passages, our method shows a superior performance against the Distributed Linear Safe Corridor (DLSC) method, with a more reasonable passing order and an achievement of up to 40% reduction in flight path length. Finally, we verify the efficacy of our proposed method with a Crazyflie 2.1 quadrotor swarm.},
  archive   = {C_IROS},
  author    = {Jiacheng He and Fangguo Zhao and Shaohao Zhu and Shuo Li and Jinming Xu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802116},
  month     = {10},
  pages     = {14056-14062},
  title     = {Priority-based deadlock recovery for distributed swarm obstacle avoidance in cluttered environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Thermally-resilient soft gripper for on-orbit operations.
<em>IROS</em>, 14050–14055. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Research in soft manipulators has significantly enhanced object grasping capabilities, thanks to their adaptability to various shapes and sizes. Applying this technology to on-orbit servicing, especially during the capture and containment stages of active space debris removal missions, might offer a secure, adaptable, and cost-effective solution compared to the trend of increasing the degrees of freedom and complexity of the manipulator (e.g. ClearSpace, Astroscale). This work aims to conduct an experimental proof of concept, for which challenges such as radiation, vacuum, and microgravity are significant, but the predominant issue is ensuring effective operation in the extreme temperature swings, where flexible materials may exhibit cryogenic crystallization or drastic shifts in their elasticity. This work addresses this challenge through an initial stage of analytical modeling of the thermal dynamics inside the manipulator in orbit; which is then used for the development of a first multi-layered experimental prototype (leveraging the properties of TPU, silicone, PTFE and aerogel) tested with liquid nitrogen and heat guns. The tendon-actuated servo-driven gripper is tested in the laboratory by varying the shape and size of objects during the grasping. The results, based on servomotor force metrics to assess the flexible manipulator’s adaptability and object capture efficiency across temperature changes, affirm the concept’s viability. Forces increase up to 220% in cryogenic conditions and decrease by no more than 50% at high temperatures.},
  archive   = {C_IROS},
  author    = {F. Ruiz and B.C. Arrue and A. Ollero},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801537},
  month     = {10},
  pages     = {14050-14055},
  title     = {Thermally-resilient soft gripper for on-orbit operations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mobility performance characterization of transformable nano
rover for lunar exploration. <em>IROS</em>, 14042–14049. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In January 2024, a Japanese lunar lander successfully touched down on the moon as part of the Smart Lander for Investigating Moon (SLIM) mission. Accompanying the SLIM on its journey were two small rovers named lunar excursion vehicles (LEV-1 and 2). LEV-2, a transformable nano rover, was particularly designed to capture images of the SLIM on the lunar surface. During the mission, LEV-2 successfully maneuvered and captured images of the lander. Understanding the mobility characteristics of LEV-2 was crucial for the success of this task. This study analyzed the mobility characteristics of LEV-2 through numerical simulation and traveling tests. In the numerical simulation, a dynamics model of LEV-2 was first developed. Subsequently, the motion behaviors of LEV2 were analyzed by utilizing the discrete element method. Traveling tests were then conducted using the LEV-2 engineering model on various terrains covered with a lunar regolith simulant. The results of both the numerical simulations and experiments revealed consistent trends across different moving modes of LEV-2. Furthermore, the simulation results indicated no significant deterioration in the mobility performance of LEV-2 on the moon when compared to that on the Earth. Based on the characteristics predicted from the simulation and the quantitative data obtained from the experiments, it was concluded that LEV-2 is supposed to travel at least 15◦ slopes on the moon.},
  archive   = {C_IROS},
  author    = {Masataku Sutoh and Daichi Hirano and Mariko Inazawa and Yuta Kawai and Hirotaka Sawada},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802179},
  month     = {10},
  pages     = {14042-14049},
  title     = {Mobility performance characterization of transformable nano rover for lunar exploration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DRIFT: Deep reinforcement learning for intelligent floating
platforms trajectories. <em>IROS</em>, 14034–14041. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This investigation introduces a novel deep reinforcement learning-based suite to control floating platforms in both simulated and real-world environments. Floating platforms serve as versatile test-beds to emulate microgravity environments on Earth, useful to test autonomous navigation systems for space applications. Our approach addresses the system and environmental uncertainties in controlling such platforms by training policies capable of precise maneuvers amid dynamic and unpredictable conditions. Leveraging Deep Reinforcement Learning (DRL) techniques, our suite achieves robustness, adaptability, and good transferability from simulation to reality. Our deep reinforcement learning framework provides advantages such as fast training times, large-scale testing capabilities, rich visualization options, and ROS bindings for integration with real-world robotic systems. Being open access, our suite serves as a comprehensive platform for practitioners who want to replicate similar research in their own simulated environments and labs.},
  archive   = {C_IROS},
  author    = {Matteo El-Hariry and Antoine Richard and Vivek Muralidharan and Matthieu Geist and Miguel Olivares-Mendez},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801927},
  month     = {10},
  pages     = {14034-14041},
  title     = {DRIFT: Deep reinforcement learning for intelligent floating platforms trajectories},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rocket landing control with random annealing jump start
reinforcement learning. <em>IROS</em>, 14026–14033. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Rocket recycling is a crucial pursuit in aerospace technology, aimed at reducing costs and environmental impact in space exploration. The primary focus centers on rocket landing control, involving the guidance of a nonlinear under-actuated rocket with limited fuel in real-time. This challenging task prompts the application of reinforcement learning (RL), yet goal-oriented nature of the problem poses difficulties for standard RL algorithms due to the absence of intermediate reward signals. This paper, for the first time, significantly elevates the success rate of rocket landing control from 8% with a baseline controller to 97% on a high-fidelity rocket model using RL. Our approach, called Random Annealing Jump Start (RAJS), is tailored for real-world goal-oriented problems by leveraging prior feedback controllers as guide policy to facilitate environmental exploration and policy learning in RL. In each episode, the guide policy navigates the environment for the guide horizon, followed by the exploration policy taking charge to complete remaining steps. This jump-start strategy prunes exploration space, rendering the problem more tractable to RL algorithms. The guide horizon is sampled from a uniform distribution, with its upper bound annealing to zero based on performance metrics, mitigating distribution shift and mismatch issues in existing methods. Additional enhancements, including cascading jump start, refined reward and terminal condition, and action smoothness regulation, further improve policy performance and practical applicability. The proposed method is validated through extensive evaluation and Hardware-in-the-Loop testing, affirming the effectiveness, real-time feasibility, and smoothness of the proposed controller.},
  archive   = {C_IROS},
  author    = {Yuxuan Jiang and Yujie Yang and Zhiqian Lan and Guojian Zhan and Shengbo Eben Li and Qi Sun and Jian Ma and Tianwen Yu and Changwu Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801306},
  month     = {10},
  pages     = {14026-14033},
  title     = {Rocket landing control with random annealing jump start reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tactile comfort: Lowering heart rate through interactions
with a pocket robot. <em>IROS</em>, 14020–14025. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Children diagnosed with anxiety disorders are taught a range of strategies to navigate situations of heightened anxiety. Techniques such as deep breathing and repetition of mantras are commonly employed, as they are known to be calming and reduce elevated heart rates. Although these strategies are often effective, their successful application relies on prior training of the children for successful use when faced with challenging situations. This paper investigates a pocket-sized companion robot designed to offer a relaxation technique requiring no prior training, with a focus on immediate impact on the user’s heart rate. The robot utilizes a tactile game to divert the user’s attention, thereby promoting relaxation. We conducted two studies with children who were not diagnosed with anxiety: a 14-day pilot study with two children (age 8) and a main study with 18 children (ages 7-8). Both studies employed a within-subjects design and focused on measuring heart rate during tactile interaction with the robot and during non-use. Interacting with the robot was found to significantly lower the study participants’ heart rate (p&lt;0.01) compared to the nonuse condition, indicating a consistent calming effect across all participants. These results suggest that tactile companion robots have the potential to enhance the therapeutic value of relaxation techniques.},
  archive   = {C_IROS},
  author    = {Morten Roed Frederiksen and Kasper Stoy and Maja Matarić},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802076},
  month     = {10},
  pages     = {14020-14025},
  title     = {Tactile comfort: Lowering heart rate through interactions with a pocket robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards unconstrained collision injury protection data sets:
Initial surrogate experiments for the human hand. <em>IROS</em>,
14012–14019. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safety for physical human-robot interaction (pHRI) is a major concern for all application domains. While current standardization for industrial robot applications provide safety constraints that address the onset of pain in blunt impacts, these impact thresholds are difficult to use on edged or pointed impactors. The most severe injuries occur in constrained contact scenarios, where crushing is possible. Nevertheless, situations potentially resulting in constrained contact only occur in certain areas of a workspace and design or organisational approaches can be used to avoid them. What remains are risks to the human physical integrity caused by unconstrained accidental contacts, which are difficult to avoid while maintaining robot motion efficiency. Nevertheless, the probability and severity of injuries occurring with edged or pointed impacting objects in unconstrained collisions is hardly researched. In this paper, we propose an experimental setup and procedure using two pendulums modeling human hands and arms and robots to understand the injury potential of unconstrained collisions of human hands with edged objects. Pig feet are used as ex vivo surrogate samples - as these closely resemble the physiological characteristics of human hands - to create an initial injury database on the severity of injuries caused by unconstrained edged or pointed impacts. For the effective mass range of typical lightweight robots, the data obtained show low probabilities of injuries such as skin cuts or bone/tendon injuries in unconstrained collisions when the velocity is reduced to &lt; 0.5 m/s. Additionally, distinct differences between injury probability of the finger substitutes and the back of the hand substitutes are observed. The proposed experimental setups and procedures should be complemented by sufficient human modeling, e.g. the effective masses of human body parts, and will eventually lead to a complete understanding of the biomechanical injury potential in pHRI.},
  archive   = {C_IROS},
  author    = {Robin Jeanne Kirschner and Jinyu Yang and Edonis Elshani and Carina M. Micheler and Tobias Leibbrand and Dirk Müller and Claudio Glowalla and Nader Rajaei and Rainer Burgkart and Sami Haddadin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801892},
  month     = {10},
  pages     = {14012-14019},
  title     = {Towards unconstrained collision injury protection data sets: Initial surrogate experiments for the human hand},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A unified interaction control framework for safe robotic
ultrasound scanning with human-intention-aware compliance.
<em>IROS</em>, 14004–14011. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ultrasound scanning robot operates in environments where frequent human-robot interactions occur. Most existing control methods for ultrasound scanning address only one specific interaction situation or implement hard switches between controllers for different situations, which compromises both safety and efficiency. In this paper, we propose a unified interaction control framework for ultrasound scanning robots capable of handling all common interactions, distinguishing both human-intended and unintended types, and adapting with appropriate compliance. Specifically, the robot suspends or modulates its ongoing main task if the interaction is intended, e.g., when the doctor grasps the robot to lead the end effector actively. Furthermore, it can identify unintended interactions and avoid potential collision in the null space beforehand. Even if that collision has happened, it can become compliant with the collision in the null space and try to reduce its impact on the main task (where the scan is ongoing) kinematically and dynamically. The multiple situations are integrated into a unified controller with a smooth transition to deal with the interactions by exhibiting human-intention-aware compliance. Experimental results validate the framework’s ability to cope with all common interactions including intended intervention and unintended collision in a collaborative carotid artery ultrasound scanning task.},
  archive   = {C_IROS},
  author    = {Xiangjie Yan and Shaqi Luo and Yongpeng Jiang and Mingrui Yu and Chen Chen and Senqiang Zhu and Gao Huang and Shiji Song and Xiang Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801755},
  month     = {10},
  pages     = {14004-14011},
  title     = {A unified interaction control framework for safe robotic ultrasound scanning with human-intention-aware compliance},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Four-axis adaptive fingers hand for object insertion: FAAF
hand. <em>IROS</em>, 13996–14003. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots operating in the real world face significant but unavoidable issues in object localization that must be dealt with. A typical approach to address this is the addition of compliance mechanisms to hardware to absorb and compensate for some of these errors. However, for fine-grained manipulation tasks, the location and choice of appropriate compliance mechanisms are critical for success. For objects to be inserted in a target site on a flat surface, the object must first be successfully aligned with the opening of the slot, as well as correctly oriented along its central axis, before it can be inserted. We developed the Four-Axis Adaptive Finger Hand (FAAF hand) that is equipped with fingers that can passively adapt in four axes (x, y, z, yaw) enabling it to perform insertion tasks including lid fitting in the presence of significant localization errors. Furthermore, this adaptivity allows the use of simple control methods without requiring contact sensors or other devices. Our results confirm the ability of the FAAF hand on challenging insertion tasks of square and triangle-shaped pegs (or prisms) and placing of container lids in the presence of position errors in all directions and rotational error along the object’s central axis, using a simple control scheme.},
  archive   = {C_IROS},
  author    = {Naoki Fukaya and Koki Yamane and Shimpei Masuda and Avinash Ummadisingu and Shin-ichi Maeda and Kuniyuki Takahashi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802047},
  month     = {10},
  pages     = {13996-14003},
  title     = {Four-axis adaptive fingers hand for object insertion: FAAF hand},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continual learning for autonomous robots: A prototype-based
approach. <em>IROS</em>, 13988–13995. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans and animals learn throughout their lives from limited amounts of sensed data, both with and without supervision. Autonomous, intelligent robots of the future are often expected to do the same. The existing continual learning (CL) methods are usually not directly applicable to robotic settings: they typically require buffering and a balanced replay of training data. A few-shot online continual learning (FS-OCL) setting has been proposed to address more realistic scenarios where robots must learn from a non-repeated sparse data stream. To enable truly autonomous life-long learning, an additional challenge of detecting novelties and learning new items without supervision needs to be addressed. We address this challenge with our new prototype-based approach called Continually Learning Prototypes (CLP). In addition to being capable of FS-OCL learning, CLP also detects novel objects and learns from them without supervision. To mitigate forgetting, CLP utilizes a novel metaplasticity mechanism that adapts the learning rate individually per prototype. CLP is rehearsal-free, hence does not require a memory buffer, and is compatible with neuromorphic hardware, characterized by ultra-low power consumption, real-time processing abilities, and on-chip learning. Indeed, we have open-sourced both the PyTorch implementation of CLP and a simpler version in the neuromorphic software framework Lava, targetting Intel’s neuromorphic chip Loihi 2. We evaluate CLP on a robotic vision dataset, OpenLORIS. In a low-instance FS-OCL scenario, CLP shows state-of-the-art results. In the open world, CLP detects novelties with superior precision and recall and learns features of the detected novel classes without supervision, achieving a strong baseline of 99% base class and 65%/76% (5-shot/10-shot) novel class accuracy.},
  archive   = {C_IROS},
  author    = {Elvin Hajizada and Balachandran Swaminathan and Yulia Sandamirskaya},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802683},
  month     = {10},
  pages     = {13988-13995},
  title     = {Continual learning for autonomous robots: A prototype-based approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uncertainty-aware deep imitation learning and deployment for
autonomous navigation through crowded intersections. <em>IROS</em>,
13980–13987. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Navigation through crowded intersections is a challenge for autonomous vehicles, where uncertainty arises from interaction with other road users, encountering new scenes and weathers, etc. Recent end-to-end autonomous control deep models learned from human drivers have shown promising driving performance, whereas they are not as transparent and safe as traditional rule-based systems. When facing situations that they are unfamiliar with or uncertain about, the deep models’ predictions could be unsafe and untrustworthy. Without the ability to identify these situations and issue warnings beforehand, cascading errors of deep models may result in catastrophes. Therefore, this work combines the strengths of both data-driven and traditional rule-based approaches to achieve better driving quality and safety. We propose a heterogeneity uncertainty quantification method based on imitation learning, where both data and model uncertainties of the lateral and longitudinal control tasks are quantified. We also propose a policy deployment strategy where a safety indicator is developed upon estimated uncertainty to bridge the data-driven performance layer and the rule-based fallback layer. We learned from human driving demonstrations and conducted extensive closed-loop tests. Results demonstrate the effectiveness and importance of the proposed uncertainty quantification method and policy deployment strategy.},
  archive   = {C_IROS},
  author    = {Zeyu Zhu and Shuai Wang and Huijing Zhao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801536},
  month     = {10},
  pages     = {13980-13987},
  title     = {Uncertainty-aware deep imitation learning and deployment for autonomous navigation through crowded intersections},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward universal and scalable road graph partitioning for
efficient multi-robot path planning. <em>IROS</em>, 13974–13979. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To date, multi-robot path planning has primarily been addressed by centralized solvers, typically aiming to maintain optimality. However, given its NP-hard nature, directly applying existing solvers in large and complex scenarios proves inefficient. A promising alternative lies in adopting a divide- and-conquer strategy to break down the problem into manageable sub-problems. In this work, we propose a systematic, universal, and scalable graph partitioning method, aiming to automatically divide any real-world environment into multiple regions. Building upon this, we convert the path planning on the entire graph into distributed sub-region path planning and devise corresponding inter-regional strategies. Our work can be easily implementable in practical systems and effectively enhances the scalability of existing solvers. Experimentally, our approach contributes to a tenfold improvement in computational efficiency while only sacrificing about 10% of optimality.},
  archive   = {C_IROS},
  author    = {Xingyao Han and Bo Cao and Zhe Liu and Shunbo Zhou and Heng Zhang and Hesheng Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802503},
  month     = {10},
  pages     = {13974-13979},
  title     = {Toward universal and scalable road graph partitioning for efficient multi-robot path planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fingertip tactile sensor for detecting rope slip.
<em>IROS</em>, 13967–13973. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When a robot manipulates a flexible object, a delicate grasp is required. Slip is an important indicator that can enable a robotic manipulator to delicately grasp an object. Therefore, we created a 3×3 tactile sensor matrix covered with a elastomer grip, intended to be used on a robotic fingertip, to detect slip. We obtained tactile sensor values corresponding to the slip and nonslip states in advance and created a training dataset with several patterns. Random Forest Classifiers were trained on the datasets, and the results were compared. All nine sensor elements in the sensor matrix could detect slips with practical accuracy in the real-time robot experiments. In time periods wherein no slip occurred, no nonslip states were erroneously determined as slip states under any of the tested conditions. The slip detector created in this study was demonstrated to be applicable to types of rope that were not used to train the detector.},
  archive   = {C_IROS},
  author    = {Takayuki Koga and Junya Sato and Takuya Daigo and Kohei Kimura and Shunsuke Kudoh},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801624},
  month     = {10},
  pages     = {13967-13973},
  title     = {Fingertip tactile sensor for detecting rope slip},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning incipient slip with GelSight sensors: Attention
classification with video vision transformers. <em>IROS</em>,
13960–13966. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An important aspect of robotic grasping is the ability to detect incipient slip based on real-time information through tactile sensors. In this paper, we propose to use Video Vision Transformers to detect the onset of slip in grasping scenarios. The dynamic nature of slip makes Video Vision Transformers well-suited for capturing temporal correlations with relatively small datasets. The training data is acquired through two GelSight tactile sensors attached to the generic finger grippers of a Panda Franka Emika robot arm that grasps, lifts and shakes 30 everyday objects in order to induce slip. We further conducted an ablation study by considering 5, 4, 3, and 2 frames prior to slip onset, revealing consistent prediction accuracy. Our approach demonstrates the capability to predict slips well in advance, even up to the 5th frame before the onset. This underscores the predictive capability of our approach, indicating its effectiveness in slip detection well before of its occurrence. This advance prediction capability may be a valuable tool for undertaking preemptive corrective actions, such as implementing a more secure gripper closure. We evaluate the efficiency of our approach to predict onset of slip on 10 previously-unseen objects and achieve a zero-shot mean prediction accuracy of 99%.},
  archive   = {C_IROS},
  author    = {Amit Parag and Edward H. Adelson and Ekrem Misimi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801324},
  month     = {10},
  pages     = {13960-13966},
  title     = {Learning incipient slip with GelSight sensors: Attention classification with video vision transformers},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multidirectional slip detection and avoidance using dynamic
3D tactile meshes from visuotactile sensors. <em>IROS</em>, 13953–13959.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visuotactile sensors have gained attention during the last years in robotics because they are able to reconstruct with high precision the 3D contact shape (or mesh) between the robotic fingers and the object. A new slip detection and avoidance algorithm is proposed based on the dynamic variation of the height of the contact mesh. Firstly, the contact mesh is reconstructed in real time by applying a neural network that estimates normal vectors from color variations along all the pixels of the images recorded by the camera inside the tactile sensor. The contact mesh corresponding to this height map is used for detecting slip with higher success rates in comparison with previous approaches based on machine learning methods directly applied to contact images or the analysis of markers integrated into the sensor’s surface. The proposed algorithm is validated experimentally in multiple directions not only for different types of objects (volumetric/planar/linear, deformable/rigid) but also with different resolutions of the contact mesh.},
  archive   = {C_IROS},
  author    = {Peng Song and Juan Antonio Corrales Ramón and Youcef Mezouar},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802378},
  month     = {10},
  pages     = {13953-13959},
  title     = {Multidirectional slip detection and avoidance using dynamic 3D tactile meshes from visuotactile sensors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large-scale deployment of vision-based tactile sensors on
multi-fingered grippers. <em>IROS</em>, 13946–13952. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision-based Tactile Sensors (VBTSs) show significant promise in that they can leverage image measurements to provide high-spatial-resolution human-like performance. However, current VBTS designs, typically confined to the fingertips of robotic grippers, prove somewhat inadequate, as many grasping and manipulation tasks require multiple contact points with the object. With an end goal of enabling large-scale, multi-surface tactile sensing via VBTSs, our research (i) develops a synchronized image acquisition system with minimal latency, (ii) proposes a modularized VBTS design for easy integration into finger phalanges, and (iii) devises a zero-shot calibration approach to improve data efficiency in the simultaneous calibration of multiple VBTSs. In validating the system within a miniature 3-fingered robotic gripper equipped with 7 VBTSs we demonstrate improved tactile perception performance by covering the contact surfaces of both gripper fingers and palm. Additionally, we show that our VBTS design can be seamlessly integrated into various end-effector morphologies significantly reducing the data requirements for calibration.},
  archive   = {C_IROS},
  author    = {Meng Wang and Wanlin Li and Hao Liang and Boren Li and Kaspar Althoefer and Yao Su and Hangxin Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801566},
  month     = {10},
  pages     = {13946-13952},
  title     = {Large-scale deployment of vision-based tactile sensors on multi-fingered grippers},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning-based hierarchical control: Emulating the central
nervous system for bio-inspired legged robot locomotion. <em>IROS</em>,
13938–13945. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Animals possess a remarkable ability to navigate challenging terrains, achieved through the interplay of various pathways between the brain, central pattern generators (CPGs) in the spinal cord, and musculoskeletal system. Traditional bioinspired control frameworks often rely on a singular control policy that models both higher (supraspinal) and spinal cord functions. In this work, we build upon our previous research by introducing two distinct neural networks: one tasked with modulating the frequency and amplitude of CPGs to generate the basic locomotor rhythm (referred to as the spinal policy), and the other responsible for receiving environmental perception data and directly modulating the rhythmic output from the spinal policy to execute precise movements on challenging terrains (referred to as the descending modulation policy). This division of labor more closely mimics the hierarchical locomotor control systems observed in legged animals, thereby enhancing the robot’s ability to navigate various uneven surfaces, including steps, high obstacles, and terrains with gaps. Additionally, we investigate the impact of sensorimotor delays within our framework, validating several biological assumptions about animal locomotion systems. Specifically, we demonstrate that spinal circuits play a crucial role in generating the basic locomotor rhythm, while descending pathways are essential for enabling appropriate gait modifications to accommodate uneven terrain. Notably, our findings also reveal that the multi-layered control inherent in animals exhibits remarkable robustness against sensorimotor delays. These findings advance our understanding of the fundamental principles governing the interplay between spinal and supraspinal mechanisms in biological locomotion. Moreover, they inform the design of bioinspired locomotion controllers that emulate these biological structures, facilitating natural movement in complex and realistic environments.},
  archive   = {C_IROS},
  author    = {Ge Sun and Milad Shafiee and Peizhuo Li and Guillaume Bellegarda and Auke Ijspeert and Guillaume Sartoretti},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801412},
  month     = {10},
  pages     = {13938-13945},
  title     = {Learning-based hierarchical control: Emulating the central nervous system for bio-inspired legged robot locomotion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Patterned structure muscle: Arbitrary shaped wire-driven
artificial muscle utilizing anisotropic flexible structure for
musculoskeletal robots. <em>IROS</em>, 13930–13937. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Muscles of the human body are composed of tiny actuators made up of myosin and actin filaments. They can exert force in various shapes such as curved or flat, under contact forces and deformations from the environment. On the other hand, muscles in musculoskeletal robots so far have faced challenges in generating force in such shapes and environments. To address this issue, we propose Patterned Structure Muscle (PSM), artificial muscles for musculoskeletal robots. PSM utilizes patterned structures with anisotropic characteristics, wire-driven mechanisms, and is made of flexible material Thermoplastic Polyurethane (TPU) using FDM 3D printing. This method enables the creation of various shapes of muscles, such as simple 1 degree-of-freedom (DOF) muscles, Multi-DOF wide area muscles, joint-covering muscles, and branched muscles. We created an upper arm structure using these muscles to demonstrate wide range of motion, lifting heavy objects, and movements through environmental contact. These experiments show that the proposed PSM is capable of operating in various shapes and environments, and is suitable for the muscles of musculoskeletal robots.},
  archive   = {C_IROS},
  author    = {Shunnosuke Yoshimura and Akihiro Miki and Kazuhiro Miyama and Yuta Sahara and Kento Kawaharazuka and Kei Okada and Masayuki Inaba},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801899},
  month     = {10},
  pages     = {13930-13937},
  title     = {Patterned structure muscle: Arbitrary shaped wire-driven artificial muscle utilizing anisotropic flexible structure for musculoskeletal robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). An efficient position reconfiguration approach for
maximizing lifetime of fixed-wing swarm drones. <em>IROS</em>,
13922–13929. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the development and application of swarm drones, some researchers have tried to replicating the migration patterns of geese in drones swarm formation to extend their lifetime. However, the problem of performing appropriate position reconfiguration based on the battery energy still remains an unsolved issue. This paper proposes an efficient position reconfiguration approach that reduces the energy consumption imbalance of the swarm and prolongs the lifetime. The approach includes: (1) a two-step MIP (mixed-integer programming)-based optimization method. (2) a two-step heuristic algorithm that can run in pseudo-polynomial time and without the need for an optimization solver. The approach provides a complete position reconfiguration solution that determines (i) the number of position reconfiguration; (ii) which drones need to exchange positions in every position reconfiguration; (iii) the length of time to maintain each position before next reconfiguration. Finally, the approach is compared with other three methods in experiments which demonstrate the effectiveness of it.},
  archive   = {C_IROS},
  author    = {Han Liu and Tian Liu and Mingyue Cui and Yunxiao Shan and Shuai Zhao and Kai Huang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802362},
  month     = {10},
  pages     = {13922-13929},
  title     = {An efficient position reconfiguration approach for maximizing lifetime of fixed-wing swarm drones},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Wing twist and folding work in synergy to propel flapping
wing animals and robots. <em>IROS</em>, 13915–13921. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We designed and built a three degrees-of-freedom (DOF) flapping wing robot, Flapperoo, to study the aerodynamic benefits of wing folding and twisting. Forces and moments of this physical model are measured in wind tunnel experiments over a Strouhal number range of St = 0.2–0.4 - typical for animal flight. We perform particle image velocimetry (PIV) measurements to visualize the air jet produced by wing clapping under the ventral side of the body when wing folding is at the extreme. The results show that this jet can be directed by controlling the wing twist at the moment of clapping, which leads to greatly enhanced cycle-averaged thrust, especially at high St or low flight speeds. Additional benefits of more thrust and less negative lift are gained during upstroke using wing twist. Remarkably, less total actuating force, or less total power, is required during upstroke with wing twist. These findings emphasize the benefits of critical wing articulation for the future flapping wing/fin robots and for an accurate test platform to study natural flapping wing flight or underwater vehicles.},
  archive   = {C_IROS},
  author    = {Xiaozhou Fan and Alexander Gehrke and Kenneth Breuer},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801820},
  month     = {10},
  pages     = {13915-13921},
  title     = {Wing twist and folding work in synergy to propel flapping wing animals and robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PINSAT: Parallelized interleaving of graph search and
trajectory optimization for kinodynamic motion planning. <em>IROS</em>,
13907–13914. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trajectory optimization is a widely used technique in robot motion planning for letting the dynamics of the system shape and synthesize complex behaviors. Several previous works have shown its benefits in high-dimensional continuous state spaces and under differential constraints. However, long time horizons and planning around obstacles in non-convex spaces pose challenges in guaranteeing convergence or finding optimal solutions. As a result, discrete graph search planners and sampling-based planers are preferred when facing obstacle-cluttered environments. A recently developed algorithm called INSAT effectively combines graph search in the low-dimensional subspace and trajectory optimization in the full-dimensional space for global kinodynamic planning over long horizons. Although INSAT successfully reasoned about and solved complex planning problems, the numerous expensive calls to an optimizer resulted in large planning times, thereby limiting its practical use. Inspired by the recent work on edge-based parallel graph search, we present PINSAT, which introduces systematic parallelization in INSAT to achieve lower planning times and higher success rates, while maintaining significantly lower costs over relevant baselines. We demonstrate PINSAT by evaluating it on 6 DoF kinodynamic manipulation planning with obstacles. We demonstrate PINSAT by evaluating it on two kinodynamic manipulation planning scenarios: (i) a single ball blocking task among obstacles using a 6 DoF ABB arm, and (ii) a multi-ball blocking task where the balls are separated by short time intervals using a 7 DoF KUKA LBR iiwa arm with obstacles.},
  archive   = {C_IROS},
  author    = {Ramkumar Natarajan and Shohin Mukherjee and Howie Choset and Maxim Likhachev},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801413},
  month     = {10},
  pages     = {13907-13914},
  title     = {PINSAT: Parallelized interleaving of graph search and trajectory optimization for kinodynamic motion planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Motion planning for object manipulation by edge-rolling.
<em>IROS</em>, 13899–13906. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A common way to manipulate heavy objects is to maintain at least one point of the object in contact with the environment during the manipulation. When the object has a cylindrical shape or, in general, a curved edge, not only sliding and pivoting motions but also rolling the object along the edge can effectively satisfy this condition. Edge-rolling offers several advantages in terms of efficiency and maneuverability. This paper aims to develop a novel approach for approximating the prehensile edge-rolling motion on any path by a sequence of constant screw displacements, leveraging the principles of screw theory. Based on this approach, we proposed an algorithmic method for task-space-based path generation of object manipulation between two given configurations using a sequence of rolling and pivoting motions. The method is based on an optimization algorithm that takes into account the joint limitations of the robot. To validate our approach, we conducted experiments to manipulate a cylinder along linear and curved paths using the Franka Emika Panda manipulator.Video— https://youtu.be/MX1-MAR9ubc},
  archive   = {C_IROS},
  author    = {Maede Boroji and Vahid Danesh and Imin Kao and Amin Fakhari},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802581},
  month     = {10},
  pages     = {13899-13906},
  title     = {Motion planning for object manipulation by edge-rolling},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). One-shot transfer of long-horizon extrinsic manipulation
through contact retargeting. <em>IROS</em>, 13891–13898. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Extrinsic manipulation, the use of environment contacts to achieve manipulation objectives, enables strategies that are otherwise impossible with a parallel jaw gripper. However, orchestrating a long-horizon sequence of contact interactions between the robot, object, and environment is notoriously challenging due to the scene diversity, large action space, and difficult contact dynamics. We observe that most extrinsic manipulation are combinations of short-horizon primitives, each of which depend strongly on initializing from a desirable contact configuration to succeed. Therefore, we propose to generalize one extrinsic manipulation trajectory to diverse objects and environments by retargeting contact requirements. We prepare a single library of robust short-horizon, goal-conditioned primitive policies, and design a framework to compose state constraints stemming from contacts specifications of each primitive. Given a test scene and a single demo prescribing the primitive sequence, our method enforces the state constraints on the test scene and find intermediate goal states using inverse kinematics. The goals are then tracked by the primitive policies. Using a 7+1 DoF robotic arm-gripper system, we achieved an overall success rate of 80.5% on hardware over 4 long-horizon extrinsic manipulation tasks, each with up to 4 primitives. Our experiments cover 10 objects and 6 environment configurations. We further show empirically that our method admits a wide range of demonstrations, and that contact retargeting is indeed the key to successfully combining primitives for long-horizon extrinsic manipulation. Code and additional details are available at stanford-tml.github. io/extrinsic-manipulation.},
  archive   = {C_IROS},
  author    = {Albert Wu and Ruocheng Wang and Sirui Chen and Clemens Eppner and C. Karen Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801356},
  month     = {10},
  pages     = {13891-13898},
  title     = {One-shot transfer of long-horizon extrinsic manipulation through contact retargeting},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Unified control framework for real-time interception and
obstacle avoidance of fast-moving objects with diffusion variational
autoencoder. <em>IROS</em>, 13883–13890. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-time interception of fast-moving objects by robotic arms in dynamic environments poses a formidable challenge due to the need for rapid reaction times, often within milliseconds, amidst dynamic obstacles. This paper introduces a unified control framework to address the above challenge by simultaneously intercepting dynamic objects and avoiding moving obstacles. Central to our approach is using diffusion-based variational autoencoder for motion planning to perform both object interception and obstacle avoidance. We begin by encoding the high-dimensional temporal information from streaming events into a two-dimensional latent manifold, enabling the discrimination between safe and colliding trajectories, culminating in the construction of an offline densely connected trajectory graph. Subsequently, we employ an extended Kalman filter to achieve precise real-time tracking of the moving object. Leveraging a graph-traversing strategy on the established offline dense graph, we generate encoded robotic motor control commands. Finally, we decode these commands to enable real-time motion of robotic motors, ensuring effective obstacle avoidance and high interception accuracy of fast-moving objects. Experimental validation on both computer simulations and autonomous 7-DoF robotic arms demonstrates the efficacy of our proposed framework. Results indicate the capability of the robotic manipulator to navigate around multiple obstacles of varying sizes and shapes while successfully intercepting fast-moving objects thrown from different angles by hand. Complete video demonstrations of our experiments can be found in https://sites.google.com/view/multirobotskill/home.},
  archive   = {C_IROS},
  author    = {Apan Dastider and Hao Fang and Mingjie Lin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802535},
  month     = {10},
  pages     = {13883-13890},
  title     = {Unified control framework for real-time interception and obstacle avoidance of fast-moving objects with diffusion variational autoencoder},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards kbps-level vehicle teleoperation via
persistent-transient environment modelling. <em>IROS</em>, 13875–13882.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traditional teleoperation technologies based on video streaming are facing several challenges in practical applications, including limited bandwidth, constrained spatial awareness, and sensitivity to illumination. Existing studies have not adequately addressed these issues. This paper presents a novel non-video based teleoperation framework for autonomous vehicles operating in bandwidth-limited environments. To reduce the amount of data being transmitted, a persistent-transient environment model is proposed for telepresence. Initially, a digital twin of the environment is preconstructed, containing only persistent environmental information. Subsequently, transient information captured by onboard sensors, such as vehicle state and dynamic objects, necessitate real-time transmission. Based on this model, a 3D virtual scene is rendered in front of the teleoperator, offering any desired virtual viewpoint to enhance spatial awareness. This telepresence model only requires real-time transmission of minimal data, i.e., vehicle state and detected objects, and remains unaffected by illumination conditions, enabling teleoperation even in applications with Kbps-level bandwidth constraints. Experimental results showcase the substantial potential of the proposed framework in bandwidth-limited settings.},
  archive   = {C_IROS},
  author    = {Chunyang Zhao and Zeyu Zhou and Haoran Liu and Dogan Kircali and Guoyi Chi and Hongming Shen and Yuanzhe Wang and Danwei Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802639},
  month     = {10},
  pages     = {13875-13882},
  title     = {Towards kbps-level vehicle teleoperation via persistent-transient environment modelling},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Demonstrating trustworthiness in open-loop model mediated
teleoperation for collecting lunar regolith simulant*. <em>IROS</em>,
13869–13874. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Teleoperated robotics will be an essential tool to support upcoming lunar exploration and in-situ resource utilisation activities. However, the communication delays between Earth and the Moon makes operating these robots extremely challenging. Model-Mediated Teleoperation (MMT) is a method of controlling these remote systems in perceived real-time, via a simulation, but is dependent on the accuracy of its model. In this work, a computationally efficient model of lunar regolith was implemented in an open-loop MMT system. The behaviour of the virtual model was compared with its physical equivalent during manipulation tasks. The model predicted the outcome of a regolith simulant scooping task with sufficient accuracy to be considered effective and trustworthy 100% and 92.5% of the time, respectively. Pouring actions were less accurate, but trustworthiness and effectiveness can still be ensured by restricting the orientation of the end effector whilst carrying simulant material. Simulated haptic interactions were representative of the real-world during simple, linear tasks (pressing and dragging), but not during more complex motions. This simulation could be adapted to account for reduced gravity, to form a delay-robust lunar MMT system, or to build operators’ trust in the system by familiarising themselves in a low-risk virtual world.},
  archive   = {C_IROS},
  author    = {Joe Loucaw and Aliz Zemeny and Antonia Tzemanaki and Romain Charles},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801837},
  month     = {10},
  pages     = {13869-13874},
  title     = {Demonstrating trustworthiness in open-loop model mediated teleoperation for collecting lunar regolith simulant*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Radiance fields for robotic teleoperation. <em>IROS</em>,
13861–13868. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Radiance field methods such as Neural Radiance Fields (NeRFs) or 3D Gaussian Splatting (3DGS), have revo-lutionized graphics and novel view synthesis. Their ability to synthesize new viewpoints with photo-realistic quality, as well as capture complex volumetric and specular scenes, makes them an ideal visualization for robotic teleoperation setups. Direct camera teleoperation provides high-fidelity operation at the cost of maneuverability, while reconstruction-based approaches offer controllable scenes with lower fidelity. With this in mind, we propose replacing the traditional reconstruction-visualization components of the robotic teleoperation pipeline with online Radiance Fields, offering highly maneuverable scenes with photorealistic quality. As such, there are three main contributions to state of the art: (1) online training of Radiance Fields using live data from multiple cameras, (2) support for a variety of radiance methods including NeRF and 3DGS, (3) visualization suite for these methods including a virtual reality scene. To enable seamless integration with existing setups, these components were tested with multiple robots in multiple configurations and were displayed using traditional tools as well as the VR headset. The results across methods and robots were compared quantitatively to a baseline of mesh reconstruction, and a user study was conducted to compare the different visualization methods. The code and additional samples are available at https://leggedrobotics.github.io/rffr.github.io/.},
  archive   = {C_IROS},
  author    = {Maximum Wilder-Smith and Vaishakh Patil and Marco Hutter},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801345},
  month     = {10},
  pages     = {13861-13868},
  title     = {Radiance fields for robotic teleoperation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feelit: Combining compliant shape displays with vision-based
tactile sensors for real-time teletaction. <em>IROS</em>, 13853–13860.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Teletaction, the transmission of tactile feedback or touch, is a crucial aspect in the field of teleoperation. High-quality teletaction feedback allows users to remotely manipulate objects and increase the quality of the humanmachine interface between the operator and the robot, making complex manipulation tasks possible. Advances in the field of teletaction for teleoperation however, have yet to make full use of the high-resolution 3D data provided by modern vision-based tactile sensors. Existing solutions for teletaction lack in one or more areas of form or function, such as fidelity or hardware footprint. In this paper, we showcase our design for a low-cost teletaction device that can utilize real-time high-resolution tactile information from vision-based tactile sensors, through both physical 3D surface reconstruction and shear displacement. We present our device, the Feelit, which uses a combination of a pin-based shape display and compliant mechanisms to accomplish this task. The pin-based shape display utilizes an array of 24 servomotors with miniature Bowden cables, giving the device a resolution of 6x4 pins in a 15x10 mm display footprint. Each pin can actuate up to 3 mm in 200 ms, while providing 80 N of force and 1.5 um of depth resolution. Shear displacement and rotation is achieved using a compliant mechanism design, allowing a minimum of 1 mm displacement laterally and 10 degrees of rotation. This real-time 3D tactile reconstruction is achieved with the use of a vision-based tactile sensor, the GelSight [1], along with an algorithm that samples the depth data and marker tracking to generate actuator commands. Through a series of experiments including shape recognition and relative weight identification, we show that our device has the potential to expand teletaction capabilities in the teleoperation space.},
  archive   = {C_IROS},
  author    = {Oscar Yu and Yu She},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802027},
  month     = {10},
  pages     = {13853-13860},
  title     = {Feelit: Combining compliant shape displays with vision-based tactile sensors for real-time teletaction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Malicious path manipulations via exploitation of
representation vulnerabilities of vision-language navigation systems.
<em>IROS</em>, 13845–13852. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Building on the unprecedented capabilities of large language models for command understanding and zero-shot recognition of multi-modal vision-language transformers, visual language navigation (VLN) has emerged as an effective way to address multiple fundamental challenges toward a natural language interface to robot navigation. However, such vision-language models are inherently vulnerable due to the lack of semantic meaning of the underlying embedding space. Using a recently developed gradient-based optimization procedure, we demonstrate that images can be modified imperceptibly to match the representation of totally different images and unrelated texts for a vision-language model. Building on this, we develop algorithms that can adversarially modify a minimal number of images so that the robot will follow a route of choice for commands that require a number of landmarks. We demonstrate that experimentally using a recently proposed VLN system; for a given navigation command, a robot can be made to follow drastically different routes. We also develop an efficient algorithm to detect such malicious modifications reliably based on the fact that the adversarially modified images have much higher sensitivity to added Gaussian noise than the original images.},
  archive   = {C_IROS},
  author    = {Chashi Mahiul Islam and Shaeke Salman and Montasir Shams and Xiuwen Liu and Piyush Kumar},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802618},
  month     = {10},
  pages     = {13845-13852},
  title     = {Malicious path manipulations via exploitation of representation vulnerabilities of vision-language navigation systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CoNVOI: Context-aware navigation using vision language
models in outdoor and indoor environments. <em>IROS</em>, 13837–13844.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present CoNVOI, a novel method for autonomous robot navigation in real-world indoor and outdoor environments using Vision Language Models (VLMs). We employ VLMs in two ways: first, we leverage their zero-shot image classification capability to identify the context or scenario (e.g., indoor corridor, outdoor terrain, crosswalk, etc) of the robot’s surroundings, and formulate context-based navigation behaviors as simple text prompts (e.g. &quot;stay on the pavement&quot;). Second, we utilize their state-of-the-art semantic understanding and logical reasoning capabilities to compute a suitable trajectory given the identified context. To this end, we propose a novel multi-modal visual marking approach to annotate the obstacle-free regions in the RGB image used as input to the VLM with numbers, by correlating it with a local occupancy map of the environment. The marked numbers ground image locations in the real-world, direct the VLM’s attention solely to navigable locations, and elucidate the spatial relationships between them and terrains depicted in the image to the VLM. Next, we query the VLM to select numbers on the marked image that satisfy the context-based behavior text prompt, and construct a reference path using the selected numbers. Finally, we propose a method to extrapolate the reference trajectory when the robot’s environmental context has not changed to prevent unnecessary VLM queries. We use the reference trajectory to guide a motion planner, and demonstrate that it leads to human-like behaviors (e.g. not cutting through a group of people, using crosswalks, etc.) in various real-world indoor and outdoor scenarios. We perform several ablations and navigation comparisons and demonstrate that CoNVOI’s trajectories are most similar to human teleoperated ground truth in terms of Fréchet distance (9.7-58.2% closer), lowest path errors (up to 88.13% lower), and up to 86.09% lower % of unacceptable paths.},
  archive   = {C_IROS},
  author    = {Adarsh Jagan Sathyamoorthy and Kasun Weerakoon and Mohamed Elnoor and Anuj Zore and Brian Ichter and Fei Xia and Jie Tan and Wenhao Yu and Dinesh Manocha},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802716},
  month     = {10},
  pages     = {13837-13844},
  title     = {CoNVOI: Context-aware navigation using vision language models in outdoor and indoor environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ActiveRIR: Active audio-visual exploration for acoustic
environment modeling. <em>IROS</em>, 13830–13836. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An environment acoustic model represents how sound is transformed by the physical characteristics of an indoor environment, for any given source/receiver location. Traditional methods for constructing acoustic models involve expensive and time-consuming collection of large quantities of acoustic data at dense spatial locations in the space, or rely on privileged knowledge of scene geometry to intelligently select acoustic data sampling locations. We propose active acoustic sampling, a new task for efficiently building an environment acoustic model of an unmapped environment in which a mobile agent equipped with visual and acoustic sensors jointly constructs the environment acoustic model and the occupancy map on-the-fly. We introduce ActiveRIR, a reinforcement learning (RL) policy that leverages information from audio-visual sensor streams to guide agent navigation and determine optimal acoustic data sampling positions, yielding a high quality acoustic model of the environment from a minimal set of acoustic samples. We train our policy with a novel RL reward based on information gain in the environment acoustic model. Evaluating on diverse unseen indoor environments from a state-of-the-art acoustic simulation platform, ActiveRIR outperforms an array of methods—both traditional navigation agents based on spatial novelty and visual exploration as well as existing state-of-the-art methods.},
  archive   = {C_IROS},
  author    = {Arjun Somayazulu and Sagnik Majumder and Changan Chen and Kristen Grauman},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801510},
  month     = {10},
  pages     = {13830-13836},
  title     = {ActiveRIR: Active audio-visual exploration for acoustic environment modeling},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing exploratory capability of visual navigation using
uncertainty of implicit scene representation. <em>IROS</em>,
13824–13829. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the context of visual navigation in unknown scenes, both “exploration” and “exploitation” are equally crucial. Robots must first establish environmental cognition through exploration and then utilize the cognitive information to accomplish target searches. However, most existing methods for image-goal navigation prioritize target search over the generation of exploratory behavior. To address this, we propose the Navigation with Uncertainty-driven Exploration (NUE) pipeline, which uses an implicit and compact scene representation, NeRF, as a cognitive structure. We estimate the uncertainty of NeRF and augment the exploratory ability by the uncertainty to in turn facilitate the construction of implicit representation. Simultaneously, we extract memory information from NeRF to enhance the robot’s reasoning ability for determining the location of the target. Ultimately, we seamlessly combine the two generated abilities to produce navigational actions. Our pipeline is end-to-end, with the environmental cognitive structure being constructed online. Extensive experimental results on image-goal navigation demonstrate the capability of our pipeline to enhance exploratory behaviors, while also enabling a natural transition from the exploration to exploitation phase. This enables our model to outperform existing memory-based cognitive navigation structures in terms of navigation performance. Project page: https://github.com/IRMVLab/NUE-NeRF-nav},
  archive   = {C_IROS},
  author    = {Yichen Wang and Qiming Liu and Zhe Liu and Hesheng Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801778},
  month     = {10},
  pages     = {13824-13829},
  title     = {Enhancing exploratory capability of visual navigation using uncertainty of implicit scene representation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MADE: Malicious agent detection for robust multi-agent
collaborative perception. <em>IROS</em>, 13817–13823. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, multi-agent collaborative (MAC) perception has been proposed and outperformed the traditional single-agent perception in many applications, such as autonomous driving. However, MAC perception is more vulnerable to adversarial attacks than single-agent perception due to the information exchange. The attacker can easily degrade the performance of a victim agent by sending harmful information from a malicious agent nearby. In this paper, we propose Malicious Agent Detection (MADE), a reactive defense specific to MAC perception that can be deployed by an agent to accurately detect and then remove any potential malicious agent in its local collaboration network. In particular, MADE inspects each agent in the network independently using a semi-supervised anomaly detector based on a double-hypothesis test with the Benjamini-Hochberg procedure for false positive control. For the two hypothesis tests, we propose a match loss statistic and a collaborative reconstruction loss statistic, respectively, both based on the consistency between the agent to be inspected and the ego agent deployed with our detector. We comprehensively evaluate MADE on a benchmark 3D dataset, V2X-sim, and a real-road dataset, DAIR-V2X, comparing it to baseline defenses. Notably, with the protection of MADE, the drops in the average precision compared with the best-case ‘Oracle’ defender are merely 1.27% and 0.28%, respectively.},
  archive   = {C_IROS},
  author    = {Yangheng Zhao and Zhen Xiang and Sheng Yin and Xianghe Pang and Yanfeng Wang and Siheng Chen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801337},
  month     = {10},
  pages     = {13817-13823},
  title     = {MADE: Malicious agent detection for robust multi-agent collaborative perception},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ODD-diLLMma: Driving automation system ODD compliance
checking using LLMs. <em>IROS</em>, 13809–13816. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although Driving Automation Systems (DASs) are rapidly becoming more advanced and ubiquitous, they are still confined to specific Operational Design Domains (ODDs) over which the system must be trained and validated. Yet, each DAS has a bespoke and often informally defined ODD, which makes it intractable to manually judge whether a dataset satisfies a DAS’s ODD. This results in inadequate data leaking into the training and testing processes, weakening them, and causes large amounts of collected data to go unused given the inability to check their ODD compliance. This presents a dilemma: How do we cost-effectively determine if existing sensor data complies with a DAS’s ODD? To address this challenge, we start by reviewing the ODD specifications of 10 commercial DASs to understand current practices in ODD documentation. Next, we present ODD-diLLMma, an automated method that leverages Large Language Models (LLMs) to analyze existing datasets with respect to the natural language specifications of ODDs. Our evaluation of ODD-diLLMma examines its utility in analyzing inputs from 3 real-world datasets. Our empirical findings show that ODD-diLLMma significantly enhances the efficiency of detecting ODD compliance, showing improvements of up to 147% over a human baseline. Further, our analysis highlights the strengths and limitations of employing LLMs to support ODD-diLLMma, underscoring their potential to effectively address the challenges of ODD compliance detection.},
  archive   = {C_IROS},
  author    = {Carl Hildebrandt and Trey Woodlief and Sebastian Elbaum},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801369},
  month     = {10},
  pages     = {13809-13816},
  title     = {ODD-diLLMma: Driving automation system ODD compliance checking using LLMs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CoBL-diffusion: Diffusion-based conditional robot planning
in dynamic environments using control barrier and lyapunov functions.
<em>IROS</em>, 13801–13808. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Equipping autonomous robots with the ability to navigate safely and efficiently around humans is a crucial step toward achieving trusted robot autonomy. However, generating robot plans while ensuring safety in dynamic multi-agent environments remains a key challenge. Building upon recent work on leveraging deep generative models for robot planning in static environments, this paper proposes CoBL-Diffusion, a novel diffusion-based safe robot planner for dynamic environments. CoBL-Diffusion uses Control Barrier and Lyapunov functions to guide the denoising process of a diffusion model, iteratively refining the robot control sequence to satisfy the safety and stability constraints. We demonstrate the effectiveness of CoBL-Diffusion using two settings: a synthetic single-agent environment and a real-world pedestrian dataset. Our results show that CoBL-Diffusion generates smooth trajectories that enable the robot to reach goal locations while maintaining a low collision rate with dynamic obstacles.},
  archive   = {C_IROS},
  author    = {Kazuki Mizuta and Karen Leung},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802549},
  month     = {10},
  pages     = {13801-13808},
  title     = {CoBL-diffusion: Diffusion-based conditional robot planning in dynamic environments using control barrier and lyapunov functions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploiting hybrid policy in reinforcement learning for
interpretable temporal logic manipulation. <em>IROS</em>, 13795–13800.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement Learning (RL) based methods have been increasingly explored for robot learning. However, RL based methods often suffer from low sampling efficiency in the exploration phase, especially for long-horizon manipulation tasks, and generally neglect the semantic information from the task level, resulted in a delayed convergence or even tasks failure. To tackle these challenges, we propose a Temporal-Logic-guided Hybrid policy framework (HyTL) which leverages three-level decision layers to improve the agent’s performance. Specifically, the task specifications are encoded via linear temporal logic (LTL) to improve performance and offer interpretability. And a waypoints planning module is designed with the feedback from the LTL-encoded task level as a high-level policy to improve the exploration efficiency. The middle-level policy selects which behavior primitives to execute, and the low-level policy specifies the corresponding parameters to interact with the environment. We evaluate HyTL on four challenging manipulation tasks, which demonstrate its effectiveness and interpretability. Our project is available at: https://sites.google.com/view/hytl-0257/.},
  archive   = {C_IROS},
  author    = {Hao Zhang and Hao Wang and Xiucai Huang and Wenrui Chen and Zhen Kan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802202},
  month     = {10},
  pages     = {13795-13800},
  title     = {Exploiting hybrid policy in reinforcement learning for interpretable temporal logic manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active semantic mapping and pose graph spectral analysis for
robot exploration. <em>IROS</em>, 13787–13794. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Exploration in unknown and unstructured environments is a pivotal requirement for robotic applications. A robot’s exploration behavior can be inherently affected by the performance of its Simultaneous Localization and Mapping (SLAM) subsystem, although SLAM and exploration are generally studied separately. In this paper, we formulate exploration as an active mapping problem and extend it with semantic information. We introduce a novel active metric-semantic SLAM approach, leveraging recent research advances in information theory and spectral graph theory: we combine semantic mutual information and the connectivity metrics of the underlying pose graph of the SLAM subsystem. We use the resulting utility function to evaluate different trajectories to select the most favorable strategy during exploration. Exploration and SLAM metrics are analyzed in experiments. Running our algorithm on the Habitat dataset, we show that, while maintaining efficiency close to the state-of-the-art exploration methods, our approach effectively increases the performance of metric-semantic SLAM with a 21% reduction in average map error and a 9% improvement in average semantic classification accuracy.},
  archive   = {C_IROS},
  author    = {Rongge Zhang and Haechan Mark Bong and Giovanni Beltrame},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802821},
  month     = {10},
  pages     = {13787-13794},
  title     = {Active semantic mapping and pose graph spectral analysis for robot exploration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). OV-MAP: Open-vocabulary zero-shot 3D instance segmentation
map for robots. <em>IROS</em>, 13780–13786. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce OV-MAP, a novel approach to open-world 3D mapping for mobile robots by integrating open-features into 3D maps to enhance object recognition capabilities. A significant challenge arises when overlapping features from adjacent voxels reduce instance-level precision, as features spill over voxel boundaries, blending neighboring regions together. Our method overcomes this by employing a class-agnostic segmentation model to project 2D masks into 3D space, combined with a supplemented depth image created by merging raw and synthetic depth from point clouds. This approach, along with a 3D mask voting mechanism, enables accurate zero-shot 3D instance segmentation without relying on 3D supervised segmentation models. We assess the effectiveness of our method through comprehensive experiments on public datasets such as ScanNet200 and Replica, demonstrating superior zero-shot performance, robustness, and adaptability across diverse environments. Additionally, we conducted real-world experiments to demonstrate our method’s adaptability and robustness when applied to diverse real-world environments.},
  archive   = {C_IROS},
  author    = {Juno Kim and Yesol Park and Hye-Jung Yoon and Byoung-Tak Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801841},
  month     = {10},
  pages     = {13780-13786},
  title     = {OV-MAP: Open-vocabulary zero-shot 3D instance segmentation map for robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ASI-seg: Audio-driven surgical instrument segmentation with
surgeon intention understanding. <em>IROS</em>, 13773–13779. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Surgical instrument segmentation is crucial in surgical scene understanding, thereby facilitating surgical safety. Existing algorithms directly detected all instruments of predefined categories in the input image, lacking the capability to segment specific instruments according to the surgeon’s intention. During different stages of surgery, surgeons exhibit varying preferences and focus toward different surgical instruments. Therefore, an instrument segmentation algorithm that adheres to the surgeon’s intention can minimize distractions from irrelevant instruments and assist surgeons to a great extent. The recent Segment Anything Model (SAM) reveals the capability to segment objects following prompts, but the manual annotations for prompts are impractical during the surgery. To address these limitations in operating rooms, we propose an audio-driven surgical instrument segmentation framework, named ASI-Seg, to accurately segment the required surgical instruments by parsing the audio commands of surgeons. Specifically, we propose an intention-oriented multimodal fusion to interpret the segmentation intention from audio commands and retrieve relevant instrument details to facilitate segmentation. Moreover, to guide our ASI-Seg segment of the required surgical instruments, we devise a contrastive learning prompt encoder to effectively distinguish the required instruments from the irrelevant ones. Therefore, our ASI-Seg promotes the workflow in the operating rooms, thereby providing targeted support and reducing the cognitive load on surgeons. Extensive experiments are performed to validate the ASI-Seg framework, which reveals remarkable advantages over classical state-of-the-art and medical SAMs in both semantic segmentation and intention-oriented segmentation. The source code is available at https://github.com/Zonmgin-Zhang/ASI-Seg.},
  archive   = {C_IROS},
  author    = {Zhen Chen and Zongming Zhang and Wenwu Guo and Xingjian Luo and Long Bai and Jinlin Wu and Hongliang Ren and Hongbin Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801703},
  month     = {10},
  pages     = {13773-13779},
  title     = {ASI-seg: Audio-driven surgical instrument segmentation with surgeon intention understanding},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GRID: Scene-graph-based instruction-driven robotic task
planning. <em>IROS</em>, 13765–13772. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent works have shown that Large Language Models (LLMs) can facilitate the grounding of instructions for robotic task planning. Despite this progress, most existing works have primarily focused on utilizing raw images to aid LLMs in understanding environmental information. However, this approach not only limits the scope of observation but also typically necessitates extensive multimodal data collection and large-scale models. In this paper, we propose a novel approach called Graph-based Robotic Instruction Decomposer (GRID), which leverages scene graphs instead of images to perceive global scene information and iteratively plan subtasks for a given instruction. Our method encodes object attributes and relationships in graphs through an LLM and Graph Attention Networks, integrating instruction features to predict subtasks consisting of pre-defined robot actions and target objects in the scene graph. This strategy enables robots to acquire semantic knowledge widely observed in the environment from the scene graph. To train and evaluate GRID, we establish a dataset construction pipeline to generate synthetic datasets for graph-based robotic task planning. Experiments have shown that our method outperforms GPT-4 by over 25.4% in subtask accuracy and 43.6% in task accuracy. Moreover, our method achieves a real-time speed of 0.11s per inference. Experiments conducted on datasets of unseen scenes and scenes with varying numbers of objects demonstrate that the task accuracy of GRID declined by at most 3.8%, showcasing its robust cross-scene generalization ability. We validate our method in both physical simulation and the real world. More details can be found on the project page https://jackyzengl.github.io/GRID.github.io/.},
  archive   = {C_IROS},
  author    = {Zhe Ni and Xiaoxin Deng and Cong Tai and Xinyue Zhu and Qinghongbing Xie and Weihang Huang and Xiang Wu and Long Zeng},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801291},
  month     = {10},
  pages     = {13765-13772},
  title     = {GRID: Scene-graph-based instruction-driven robotic task planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the benefits of GPU sample-based stochastic predictive
controllers for legged locomotion. <em>IROS</em>, 13757–13764. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Quadrupedal robots excel in mobility, navigating complex terrains with agility. However, their complex control systems present challenges that are still far from being fully addressed. In this paper, we introduce the use of Sample-Based Stochastic control strategies for quadrupedal robots, as an alternative to traditional optimal control laws. We show that Sample-Based Stochastic methods, supported by GPU acceleration, can be effectively applied to real quadruped robots. In particular, in this work, we focus on achieving gait frequency adaptation, a notable challenge in quadrupedal locomotion for gradient-based methods. To validate the effectiveness of Sample-Based Stochastic controllers we test two distinct approaches for quadrupedal robots and compare them against a conventional gradientbased Model Predictive Control system. Our findings, validated both in simulation and on a real 21Kg Aliengo quadruped, demonstrate that our method is on par with a traditional Model Predictive Control strategy when the robot is subject to zero or moderate disturbance, while it surpasses gradient-based methods in handling sustained external disturbances, thanks to the straightforward gait adaptation strategy that is possible to achieve within their formulation.},
  archive   = {C_IROS},
  author    = {Giulio Turrisi and Valerio Modugno and Lorenzo Amatucci and Dimitrios Kanoulas and Claudio Semini},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801698},
  month     = {10},
  pages     = {13757-13764},
  title     = {On the benefits of GPU sample-based stochastic predictive controllers for legged locomotion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FootstepNet: An efficient actor-critic method for fast
on-line bipedal footstep planning and forecasting. <em>IROS</em>,
13749–13756. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Designing a humanoid locomotion controller is challenging and classically split up in sub-problems. Footstep planning is one of those, where the sequence of footsteps is defined. Even in simpler environments, finding a minimal sequence, or even a feasible sequence, yields a complex optimization problem. In the literature, this problem is usually addressed by search-based algorithms (e.g. variants of A*). However, such approaches are either computationally expensive or rely on hand-crafted tuning of several parameters. In this work, at first, we propose an efficient footstep planning method to navigate in local environments with obstacles, based on state-of-the art Deep Reinforcement Learning (DRL) techniques, with very low computational requirements for on-line inference. Our approach is heuristic-free and relies on a continuous set of actions to generate feasible footsteps. In contrast, other methods necessitate the selection of a relevant discrete set of actions. Second, we propose a forecasting method, allowing to quickly estimate the number of footsteps required to reach different candidates of local targets. This approach relies on inherent computations made by the actor-critic DRL architecture. We demonstrate the validity of our approach with simulation results, and by a deployment on a kid-size humanoid robot during the RoboCup 2023 competition.},
  archive   = {C_IROS},
  author    = {Clément Gaspard and Grégoire Passault and Mélodie Daniel and Olivier Ly},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802320},
  month     = {10},
  pages     = {13749-13756},
  title     = {FootstepNet: An efficient actor-critic method for fast on-line bipedal footstep planning and forecasting},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time model predictive control with zonotope-based
neural networks for bipedal social navigation. <em>IROS</em>,
13741–13748. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study addresses the challenge of bipedal navigation in a dynamic human-crowded environment, a research area that remains largely underexplored in the field of legged navigation. We propose two cascaded zonotope-based neural networks: a Pedestrian Prediction Network (PPN) for pedestrians’ future trajectory prediction and an Ego-agent Social Network (ESN) for ego-agent social path planning. Representing future paths as zonotopes allows for efficient reachability-based planning and collision checking. The ESN is then integrated with a Model Predictive Controller (ESN-MPC) for footstep planning for our bipedal robot Digit designed by Agility Robotics. ESN-MPC solves for a collision-free optimal trajectory by optimizing through the gradients of ESN. ESN-MPC optimal trajectory is sent to the low-level controller for full-order simulation of Digit. The overall proposed framework is validated with extensive simulations on randomly generated initial settings with varying human crowd densities.},
  archive   = {C_IROS},
  author    = {Abdulaziz Shamsah and Krishanu Agarwal and Shreyas Kousik and Ye Zhao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801435},
  month     = {10},
  pages     = {13741-13748},
  title     = {Real-time model predictive control with zonotope-based neural networks for bipedal social navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Motion planning for automata-based objectives using
efficient gradient-based methods. <em>IROS</em>, 13734–13740. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, there has been increasing interest in using formal methods-based techniques to safely achieve temporal tasks, such as timed sequence of goals, or patrolling objectives. Such tasks are often expressed in real-time logics such as Signal Temporal Logic (STL), whereby, the logical specification is encoded into an optimization problem. Such approaches usually involve optimizing over the quantitative semantics, or robustness degree, of the logic over bounded horizons: the semantics can be encoded as mixed-integer linear constraints or into smooth approximations of the robustness degree. A major limitation of this approach is that it faces scalability challenges with respect to temporal complexity: for example, encoding long-term tasks requires storing the entire history of the system. In this paper, we present a quantitative generalization of such tasks in the form of symbolic automata objectives. Specifically, we show that symbolic automata can be expressed as matrix operators that lend themselves to automatic differentiation, allowing for the use of off-the-shelf gradient-based optimizers. We show how this helps solve the need to store arbitrarily long system trajectories, while efficiently leveraging the task structure encoded in the automaton.},
  archive   = {C_IROS},
  author    = {Anand Balakrishnan and Merve Atasever and Jyotirmoy V. Deshmukh},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802177},
  month     = {10},
  pages     = {13734-13740},
  title     = {Motion planning for automata-based objectives using efficient gradient-based methods},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Road boundary estimation using sparse automotive radar
inputs. <em>IROS</em>, 13726–13733. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Low-cost millimeter wavelength automotive radar can work effectively under low visibility or low reflection conditions caused by lighting, weather, pollution, or object surface properties when a camera or a lidar may fail. It can serve as a fallback solution to improve safety in autonomous driving. However, after filtering, radar signals tend to be sparse and noisy which poses new challenges in scene understanding. This paper presents a new approach to detecting road boundaries based on sparse radar signals. We model the roadway using a homogeneous model and derive its conditional predictive model under known radar motion. Using this predictive model and modeling radar points using a Dirichlet Process Mixture Model, we employ Mean Field Variational Inference (MFVI) to derive an unconditional road boundary model distribution. To generate initial candidate solutions for the MFVI, we develop a custom Random Sample and Consensus (RANSAC) variant to propose unseen model instances as candidate road boundaries. For each radar point cloud we alternate the MFVI and RANSAC proposal steps until convergence to generate the best estimate of all candidate models. We select the candidate model with the minimum lateral distance to the radar on each side as the estimates of the left and right boundaries. We have implemented the proposed algorithm and it has shown satisfactory results. More specifically, the mean lane boundary estimation error is not more than 11.0 cm.},
  archive   = {C_IROS},
  author    = {Aaron Kingery and Dezhen Song},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802490},
  month     = {10},
  pages     = {13726-13733},
  title     = {Road boundary estimation using sparse automotive radar inputs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lightweight language-driven grasp detection using
conditional consistency model. <em>IROS</em>, 13719–13725. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Language-driven grasp detection is a fundamental yet challenging task in robotics with various industrial applications. This work presents a new approach for language-driven grasp detection that leverages lightweight diffusion models to achieve fast inference time. By integrating diffusion processes with grasping prompts in natural language, our method can effectively encode visual and textual information, enabling more accurate and versatile grasp positioning that aligns well with the text query. To overcome the long inference time problem in diffusion models, we leverage the image and text features as the condition in the consistency model to reduce the number of denoising timesteps during inference. The intensive experimental results show that our method outperforms other recent grasp detection methods and lightweight diffusion models by a clear margin. We further validate our method in real-world robotic experiments to demonstrate its fast inference time capability.},
  archive   = {C_IROS},
  author    = {Nghia Nguyen and Minh Nhat Vu and Baoru Huang and An Vuong and Ngan Le and Thieu Vo and Anh Nguyen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802007},
  month     = {10},
  pages     = {13719-13725},
  title     = {Lightweight language-driven grasp detection using conditional consistency model},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Similarity distance-based label assignment for tiny object
detection. <em>IROS</em>, 13711–13718. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tiny object detection is becoming one of the most challenging tasks in computer vision because of the limited object size and lack of information. The label assignment strategy is a key factor affecting the accuracy of object detection. Although there are some effective label assignment strategies for tiny objects, most of them focus on reducing the sensitivity to the bounding boxes to increase the number of positive samples and have some fixed hyperparameters need to set. However, more positive samples may not necessarily lead to better detection results, in fact, excessive positive samples may lead to more false positives. In this paper, we introduce a simple but effective strategy named the Similarity Distance (SimD) to evaluate the similarity between bounding boxes. This proposed strategy not only considers both location and shape similarity but also learns hyperparameters adaptively, ensuring that it can adapt to different datasets and various object sizes in a dataset. Our approach can be simply applied in common anchor-based detectors in place of the IoU for label assignment and Non Maximum Suppression (NMS). Extensive experiments on four mainstream tiny object detection datasets demonstrate superior performance of our method, especially, 1.8 AP points and 4.1 AP points of very tiny higher than the state-of-the-art competitors on AI-TOD. Code is available at: https://github.com/cszzshi/simd.},
  archive   = {C_IROS},
  author    = {Shuohao Shi and Qiang Fang and Xin Xu and Tong Zhao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801448},
  month     = {10},
  pages     = {13711-13718},
  title     = {Similarity distance-based label assignment for tiny object detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IC-FPS: Instance-centroid faster point sampling framework
for 3D point-based object detection. <em>IROS</em>, 13703–13710. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D object detection is one of the most important tasks in autonomous driving and robotics. Our research focuses on tackling low efficiency issue of point-based methods, and we propose a novel Instance-Centroid Faster Point Sampling (IC-FPS) framework. We design a Neighboring Feature Diffusion Module (NFDM) to extract local features for the purpose of efficiently distinguishing the foreground from the background. Considering Farthest Point Sampling (FPS) strategy for downsampling is computationally intensive, we propose the Centroid-Instance Sampling Strategy (CISS). CISS samples center point in large-scale point cloud by rapidly sampling the centroid and instance points of the foreground block. The proposed IC-FPS framework can be inserted into every point-based model and effectively replace the first Set Abstraction (SA) layer. Extensive experiments on several public benchmarks demonstrate the superior performance of our proposed IC-FPS. On the Waymo dataset, IC-FPS significantly improves performance of the benchmark model and increases inference speed by 3.8 times. And real-time detection of point-based methods is realized for the first time, which is meaningful for industrial applications.},
  archive   = {C_IROS},
  author    = {Haotian Hu and Fanyi Wang and Yaonong Wang and Laifeng Hu and Zhiwang Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802628},
  month     = {10},
  pages     = {13703-13710},
  title     = {IC-FPS: Instance-centroid faster point sampling framework for 3D point-based object detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transformer-based multi-agent reinforcement learning for
generalization of heterogeneous multi-robot cooperation. <em>IROS</em>,
13695–13702. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in multi-agent reinforcement learning (MARL) have significantly enhanced cooperation capabilities within multi-robot teams. However, the application to heterogeneous teams poses the critical challenge of combinatorial generalization—adapting learned policies to teams with new compositions of varying sizes and robots capabilities. This challenge is paramount for dynamic real-world scenarios where teams must swiftly adapt to changing environmental and task conditions. To address this, we introduce a novel transformer-based MARL method for heterogeneous multirobot cooperation. Our approach leverages graph neural networks and self-attention mechanisms to effectively capture the intricate dynamics among heterogeneous robots, facilitating policy adaptation to team size variations. Moreover, by treating robot team decisions as sequential inputs, a capability-oriented decoder is introduced to generate actions in an auto-regressive manner, enabling decentralized decision-making that tailored each robot’s varying capabilities and heterogeneity type. Furthermore, we evaluate our method across two heterogeneous cooperation scenarios in both simulated and real-world environments, featuring variations in team number and robot capabilities. Comparative results reveal our method’s superior generalization performance compared to existing MARL methodologies, marking its potential for real-world multi-robot applications.},
  archive   = {C_IROS},
  author    = {Yuxin Cai and Xiangkun He and Hongliang Guo and Wei-Yun Yau and Chen Lv},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802580},
  month     = {10},
  pages     = {13695-13702},
  title     = {Transformer-based multi-agent reinforcement learning for generalization of heterogeneous multi-robot cooperation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An attention-aware deep reinforcement learning framework for
UAV-UGV collaborative route planning. <em>IROS</em>, 13687–13694. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unmanned aerial vehicles (UAVs) possess the capability to survey vast areas, yet their operational range is limited by their battery capacity. Deploying mobile recharging stations via unmanned ground vehicles (UGVs) can significantly enhance the endurance and effectiveness of UAVs. However, optimizing the routes for both UAVs and UGVs, referred to as the UAV-UGV cooperative routing problem, requires a sophisticated planning framework to determine the vehicles’ routes and their recharging points. To address this, in this paper, we utilize a deep reinforcement learning (DRL) based framework equipped with multi-head attention layers. The framework is designed to sequentially select actions to construct routes for the UAV and UGV and to establish their rendezvous points for recharging. We evaluate our framework across various problem instance sizes and distributions, comparing it against recent heuristic-based methods and an existing learning-based method as baselines. Our proposed algorithm surpasses these baselines in terms of solution quality and runtime efficiency in the test scenarios, thus proving its effectiveness. Additionally, we investigate the application of our DRL policy in online mission planning to accommodate dynamic changes within the mission scenario.},
  archive   = {C_IROS},
  author    = {Md Safwan Mondal and Subramanian Ramasamy and James D. Humann and James M. Dotterweich and Jean-Paul F. Reddinger and Marshal A. Childers and Pranav Bhounsule},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801704},
  month     = {10},
  pages     = {13687-13694},
  title     = {An attention-aware deep reinforcement learning framework for UAV-UGV collaborative route planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating dynamic environment difficulty for obstacle
avoidance benchmarking. <em>IROS</em>, 13679–13686. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dynamic obstacle avoidance is a popular research topic for autonomous systems, such as micro aerial vehicles and service robots. Accurately evaluating the performance of dynamic obstacle avoidance methods necessitates the establishment of a metric to quantify the environment’s difficulty, a crucial aspect that remains unexplored. In this paper, we propose four metrics to measure the difficulty of dynamic environments. These metrics aim to comprehensively capture the influence of obstacles’ number, size, velocity, and other factors on the difficulty. We compare the proposed metrics with existing static environment difficulty metrics and validate them through over 1.5 million trials in a customized simulator. This simulator excludes the effects of perception and control errors and supports different motion and gaze planners for obstacle avoidance. The results indicate that the survivability metric outperforms and establishes a monotonic relationship between the success rate, with a Spearman’s Rank Correlation Coefficient (SRCC) of over 0.9. Specifically, for every planner, lower survivability leads to a higher success rate. This metric not only facilitates fair and comprehensive benchmarking but also provides insights for refining collision avoidance methods, thereby furthering the evolution of autonomous systems in dynamic environments.},
  archive   = {C_IROS},
  author    = {Moji Shi and Gang Chen and Álvaro Serra Gómez and Siyuan Wu and Javier Alonso-Mora},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802413},
  month     = {10},
  pages     = {13679-13686},
  title     = {Evaluating dynamic environment difficulty for obstacle avoidance benchmarking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EnduRL: Enhancing safety, stability, and efficiency of mixed
traffic under real-world perturbations via reinforcement learning.
<em>IROS</em>, 13671–13678. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human-driven vehicles (HVs) amplify naturally occurring perturbations in traffic, leading to congestion – a major contributor to increased fuel consumption, higher collision risks, and reduced road capacity utilization. While previous research demonstrates that Robot Vehicles (RVs) can be leveraged to mitigate these issues, most such studies rely on simulations with simplistic models of human car-following behaviors. In this work, we analyze real-world driving trajectories and extract a wide range of acceleration profiles. We then incorporates these profiles into simulations for training RVs to mitigate congestion. We evaluate the safety, efficiency, and stability of mixed traffic via comprehensive experiments conducted in two mixed traffic environments (Ring and Bottleneck) at various traffic densities, configurations, and RV penetration rates. The results show that under real-world perturbations, prior RV controllers experience performance degradation on all three objectives (sometimes even lower than 100% HVs). To address this, we introduce a reinforcement learning based RV that employs a congestion stage classifier to optimize the safety, efficiency, and stability of mixed traffic. Our RVs demonstrate significant improvements: safety by up to 66%, efficiency by up to 54%, and stability by up to 97%.},
  archive   = {C_IROS},
  author    = {Bibek Poudel and Weizi Li and Kevin Heaslip},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802689},
  month     = {10},
  pages     = {13671-13678},
  title     = {EnduRL: Enhancing safety, stability, and efficiency of mixed traffic under real-world perturbations via reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatic 3D road surface reconstruction via cross-section
modeling and interpolation*. <em>IROS</em>, 13664–13670. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate 3D road surfaces are important for the development of detailed and realistic scenarios to validate autonomous driving algorithms. In these scenarios, simulations can be conducted, for instance, to evaluate the response of a safety system under dangerous conditions. In this paper, we propose an approach designed to automatically generate 3D road surfaces from data collected by a vehicle equipped with various sensors, including a LiDAR. These road surfaces are meant to be both accurate and realistic for driving simulations. The proposed approach, after deriving the clothoidal representation of the surface borders, pursues the idea of extracting and interpolating a set of smooth 3D cross-section profiles. The resulting surface provides a 3D representation in analytical form, allowing detailed rendering at the desired resolution. We experimentally evaluate the proposed approach in a real-world scenario to assess its performance in terms of accuracy, scalability, and computing time.},
  archive   = {C_IROS},
  author    = {Matteo Bellusci and Matteo Matteucci},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801670},
  month     = {10},
  pages     = {13664-13670},
  title     = {Automatic 3D road surface reconstruction via cross-section modeling and interpolation*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards enhanced fairness and sample efficiency in traffic
signal control. <em>IROS</em>, 13656–13663. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traffic signal control (TSC) has seen substantial advancements through the application of reinforcement learning (RL) algorithms, which have shown remarkable potential in enhancing traffic flow efficiency. These RL-based approaches often surpass traditional rule-based methods, particularly in dynamic traffic environments. However, current RL solutions for TSC predominantly rely on model-free methods, necessitating extensive environmental interactions during training. This requirement can be prohibitively expensive or unfeasible in real-world implementations. Furthermore, existing methods have frequently neglected the issue of fairness in multi-intersection control, resulting in unbalanced congestion across different intersections. To address these challenges, we present FM2Light, a fairness-aware model-based multi-agent RL framework for TSC. Our approach leverages an ensemble of global world models for generating synthetic samples to enhance sample efficiency, thereby mitigating the data-intensive nature of the training process. Additionally, FM2Light incorporates a refined reward structure to promote fairness and improve coordination across multiple intersections. Extensive evaluations conducted in diverse real-world scenarios demonstrate that FM2Light achieves performance comparable to or exceeding that of model-free RL (MFRL) methods, while significantly reducing sample requirements and ensuring more equitable control among multiple agents.},
  archive   = {C_IROS},
  author    = {Xingshuai Huang and Di Wu and Michael Jenkin and Benoit Boulet},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801781},
  month     = {10},
  pages     = {13656-13663},
  title     = {Towards enhanced fairness and sample efficiency in traffic signal control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Multi-agent traffic prediction via denoised endpoint
distribution. <em>IROS</em>, 13648–13655. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The exploration of high-speed movement by robots or road traffic agents is crucial for autonomous driving and navigation. Trajectory prediction at high speeds requires considering historical features and interactions with surrounding entities, a complexity not as pronounced in lower-speed environments. Prior methods have assessed the spatiotemporal dynamics of agents but often neglected intrinsic intent and uncertainty, thereby limiting their effectiveness. We present the Denoised Endpoint Distribution model for trajectory prediction, which distinctively models agents’ spatio-temporal features alongside their intrinsic intentions and un-certainties. By employing Diffusion and Transformer models to focus on agent endpoints rather than entire trajectories, our approach significantly reduces model complexity and enhances performance through endpoint information. Our experiments on open datasets, coupled with comparison and ablation studies, demonstrate our model’s efficacy and the importance of its components. This approach advances trajectory prediction in high-speed scenarios and lays groundwork for future developments.},
  archive   = {C_IROS},
  author    = {Yao Liu and Ruoyu Wang and Yuanjiang Cao and Quan Z. Sheng and Lina Yao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802142},
  month     = {10},
  pages     = {13648-13655},
  title     = {Multi-agent traffic prediction via denoised endpoint distribution},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Environment-adaptive gait planning for obstacle avoidance in
lower-limb robotic exoskeletons. <em>IROS</em>, 13640–13647. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Powered lower limb exoskeletons (LLEs) have emerged as wearable robots designed to augment users’ locomotion capabilities, offering mechanical support and additional power for both healthy and impaired subjects. However, current assistive exoskeletons are limited by predefined motion trajectories, hindering adaptability to unstructured environments encountered in daily life. To address this limitation, this paper proposes an environment-adaptive gait planning (EAGP) solution. The approach integrates scene understanding, pose estimation, and adaptive gait planning modules. A novel Collision-Free Foot Trajectory Generator (CFFTG) algorithm facilitates obstacle avoidance by computing collision-free foot trajectories, enhancing safety and adaptability. Through inverse kinematics, the planned trajectories are converted into angular joint trajectories for execution by low-level control. This comprehensive framework aims to enhance the adaptability and safety of LLEs, paving the way for broader real-world applications beyond clinical and research settings.},
  archive   = {C_IROS},
  author    = {Edoardo Trombin and Stefano Tortora and Emanuele Menegatti and Luca Tonin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802769},
  month     = {10},
  pages     = {13640-13647},
  title     = {Environment-adaptive gait planning for obstacle avoidance in lower-limb robotic exoskeletons},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effect of tactile and deep sensory feedback synchronized
with the manipulation of myoelectric hand on body recognition.
<em>IROS</em>, 13634–13639. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Currently, myoelectric prosthetic hands are not recognized as part of the body due to the lack of sensory feedback (FB). To address this issue, it is necessary to investigate the factors that influence body recognition. Most existing research focuses on stationary prosthetic hands, such as in the rubber hand illusion, and discusses two concepts: sense of ownership (SO) and sense of agency (SA). SO refers to the feeling that a body part belongs to one&#39;s own body, while SA refers to the feeling that one is in control of the movements of one&#39;s own body parts. In this study, we developed a wearable and operable prototype myoelectric prosthetic hand equipped with tactile and deep sensory feedback, rather than a stationary prosthetic hand. Furthermore, we investigated the effect of tactile and deep sensory feedback on body recognition through psychophysical experiments. The results indicated that tactile feedback improved body recognition and deep sensory feedback improved SO; however, no effect was observed on SA.},
  archive   = {C_IROS},
  author    = {Rintaro Hamaoka and Ryu Kato},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801379},
  month     = {10},
  pages     = {13634-13639},
  title     = {Effect of tactile and deep sensory feedback synchronized with the manipulation of myoelectric hand on body recognition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A closed-loop control for lower limb exoskeleton considering
overall deformations: A simple and direct application method.
<em>IROS</em>, 13626–13633. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, considering overall deformations of the exoskeleton, we couple deformations relationship network (DRN) with fractional order viscoelastic (FOV) controller, proposing a novel DRN-FOV closed-loop control method, endowing exoskeleton with stable dynamic walking ability. Simply by utilizing only the data from the 6-axis force/torque sensors, the DRN can directly capture the mapping relationship between the foot reaction force/torque of the exoskeleton and its overall deformations. We introduce the FOV to eliminate disturbances and stabilize during walking tasks. The closed-loop control method directly compensates for the overall deformations of the exoskeleton and enables the wearer to walk stably wearing the exoskeleton. To assess the effectiveness of the proposed control method, walking tasks were effectively carried out on subjects with varying body parameters using the developed exoskeleton. The experimental results show that the DRN-FOV closed-loop control method accurately estimates and compensates for deformations, resulting in an improved dynamic walking ability of the exoskeleton with wearers.},
  archive   = {C_IROS},
  author    = {Feng Li and Ming Yang and Ziqiang Chen and Mengbo Luan and Dingkui Tian and Xinyu Wu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801977},
  month     = {10},
  pages     = {13626-13633},
  title     = {A closed-loop control for lower limb exoskeleton considering overall deformations: A simple and direct application method},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A velocity dependent delayed output feedback control
(v-DOFC) for gait assistance with an ergonomically designed
bi-directional cable-driven hip assist device. <em>IROS</em>,
13620–13625. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hip assistance with cable-driven devices has been proven to help decrease the metabolic cost of gait. However, most existing devices use heavy actuating modules or provide assistance in only one direction, limiting the effectiveness. Cable-driven devices are also difficult to accurately estimate the hip position using only motor encoders, therefore utilizing various auxiliary sensors. This paper introduces a 1.5 kg cable-driven soft wearable hip assist device that can provide assistance in both flexion and extension, using a velocity-dependent delayed output feedback controller (v-DOFC). The device is designed with the consideration of ergonomics and pressure distribution of wearable parts, to increase the anchoring performance and comfort. The controller uses time-delayed feedback proportional to the velocity output state, allowing control without requiring accurate position estimation. Additionally, directional weighting is used to provide different assistance forces for extension and flexion to match different optimal assistance values. Experimental results show that the device can reduce metabolic cost by 13.8 % compared to walking without the device. The soft wearable hip assist device can be applied to help the elderly with weaker muscles to walk longer distances.},
  archive   = {C_IROS},
  author    = {Dong Hyun Kim and Junghoon Park and Gyowook Shin and Chiyul Yoon and Yongtae Giovanni Kim and Sang-Hun Kim and Seungyong Hyung and Sungchul Kang and Minhyung Lee},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802311},
  month     = {10},
  pages     = {13620-13625},
  title     = {A velocity dependent delayed output feedback control (v-DOFC) for gait assistance with an ergonomically designed bi-directional cable-driven hip assist device},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Task planning for long-horizon cooking tasks based on large
language models. <em>IROS</em>, 13613–13619. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the field of robot manipulation, learnable task planners are gaining attention, especially for long-horizon tasks such as cooking. However, existing methods that predominantly rely on symbolic representations suffer from limitations in generalization capabilities, particularly in handling unseen objects. Given that objects may vary in real-world environments, this limitation may constrain their practical applicability. To address this issue, we propose a novel task-planning framework that leverages a pretrained large language model (LLM) for environmental interpretation. Our proposed framework extracts semantic features directly from textual data, enabling the planner to accommodate unfamiliar objects. We further incorporate a transformer-based encoder-decoder framework to understand environmental attributes derived from the language model and generate sequential predictions in line with object-oriented subgoals. To validate the effectiveness of our model, we utilize a dataset focused on cooking recipes. Going a step further, we propose a method that automatically generates object-oriented data from natural language description using recurrent LLM, enhancing the framework to manage previously unseen targets as well. Our framework shows an average success rate of 95% when validated with test sets that involve unseen objects. By providing the automatically generated dataset to the framework, we achieve a significant 27% increase in success rate on unknown target recipes. We also provide evidence of the real-world viability of our planner by successfully deploying it on a robot platform.},
  archive   = {C_IROS},
  author    = {Jungkyoo Shin and Jieun Han and SeungJun Kim and Yoonseon Oh and Eunwoo Kim},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801687},
  month     = {10},
  pages     = {13613-13619},
  title     = {Task planning for long-horizon cooking tasks based on large language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LGMCTS: Language-guided monte-carlo tree search for
executable semantic object rearrangement. <em>IROS</em>, 13607–13612.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present LGMCTS, a framework that uniquely combines language guidance with geometrically informed sampling distributions to effectively rearrange objects according to geometric patterns dictated by natural language descriptions. LGMCTS uses Monte Carlo Tree Search (MCTS) to create feasible action plans that ensure executable semantic object rearrangement. We present a comprehensive comparison with leading approaches that use language to generate goal rearrangements independently of actionable planning, including Structformer, StructDiffusion, and Code as policies. We also present a new benchmark, the Executable Language Guided Rearrangement (ELGR) Bench, containing tasks involving intricate geometry. With the ELGR bench, we show limitations of task and motion planning (TAMP) solutions that are purely based on Large Language Models (LLM) such as Code as Policies and Progprompt on such tasks. Our findings advocate for using LLMs to generate intermediary representations rather than direct action planning in geometrically complex rearrangement scenarios, aligning with perspectives from recent literature. Our code and supplementary materials are accessible at https://lgmcts.github.io/.},
  archive   = {C_IROS},
  author    = {Haonan Chang and Kai Gao and Kowndinya Boyalakuntla and Alex Lee and Baichuan Huang and Jingjin Yu and Abdeslam Boularias},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802562},
  month     = {10},
  pages     = {13607-13612},
  title     = {LGMCTS: Language-guided monte-carlo tree search for executable semantic object rearrangement},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On learning scene-aware generative state abstractions for
task-level mobile manipulation planning. <em>IROS</em>, 13599–13606. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Task and motion planning (TAMP) is a promising approach for efficient long-horizon manipulation planning, which is a prerequisite for being able to deploy manipulation systems in human-centered environments at scale. TAMP systems often rely on so-called predicates to abstractly describe the world. Today, predicates and their groundings are often hand-engineered. Furthermore, robot action parameterizations required to fulfill desired predicates are typically discovered by sampling naively or using oracles (again hand-engineered). We aim to automate predicate discovery and grounding with a system that learns to classify the state of predicates in a set of scenes while concurrently learning to generate scene configurations that fulfill the desired predicates. Our results show that high classification accuracies and generation success rates can be achieved with architectures based on multi-layer perceptrons (MLPs) and graph neural networks (GNNs) that are trained on bounding box as well as point cloud-based features in a Generative Adversarial Network (GAN)-inspired fashion, decisively outperforming both decision tree and uniform sampler baselines. The integration of our framework into a TAMP system demonstrates its positive impact on solving mobile manipulation tasks. A reference implementation of our method and data are available at https://github.com/ethzasl/predicate_learning.},
  archive   = {C_IROS},
  author    = {Julian Förster and Jen Jen Chung and Lionel Ott and Roland Siegwart},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802285},
  month     = {10},
  pages     = {13599-13606},
  title     = {On learning scene-aware generative state abstractions for task-level mobile manipulation planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ComTraQ-MPC: Meta-trained DQN-MPC integration for trajectory
tracking with limited active localization updates. <em>IROS</em>,
13592–13598. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Optimal decision-making for trajectory tracking in partially observable, stochastic environments where the number of active localization updates—the process by which the agent obtains its true state information from the sensors—are limited, presents a significant challenge. Traditional methods often struggle to balance resource conservation, accurate state estimation and precise tracking, resulting in suboptimal performance. This problem is particularly pronounced in environments with large action spaces, where the need for frequent, accurate state data is paramount, yet the capacity for active localization updates is restricted by external limitations. This paper introduces ComTraQ-MPC, a novel framework that combines Deep Q-Networks (DQN) and Model Predictive Control (MPC) to optimize trajectory tracking with constrained active localization updates. The meta-trained DQN ensures adaptive active localization scheduling, while the MPC leverages available state information to improve tracking. The central contribution of this work is their reciprocal interaction: DQN’s update decisions inform MPC’s control strategy, and MPC’s outcomes refine DQN’s learning, creating a cohesive, adaptive system. Empirical evaluations in simulated and real-world settings demonstrate that ComTraQ-MPC significantly enhances operational efficiency and accuracy, providing a generalizable and approximately optimal solution for trajectory tracking in complex partially observable environments. [Code]1 [Video]2},
  archive   = {C_IROS},
  author    = {Gokul Puthumanaillam and Manav Vora and Melkior Ornik},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801659},
  month     = {10},
  pages     = {13592-13598},
  title     = {ComTraQ-MPC: Meta-trained DQN-MPC integration for trajectory tracking with limited active localization updates},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Passive underwater robot hand utilizing water resistance.
<em>IROS</em>, 13584–13591. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Numerous robot grippers have been developed to reduce energy consumption by utilizing contact forces with fixed objects. In underwater environments, most objects are not fixed, particularly in the mid and surface layers, posing a challenge for obtaining contact forces. To address this issue, this study proposes a multi-finger gripper that utilizes water resistance for opening and closing actions underwater. As the gripper ascends in water, it closes its fingers, each equipped with a locking mechanism at the tip. This mechanism allows the fingers to maintain a closed shape when converged towards the center and locked. Unlocking occurs when the gripper descends underwater, as the direction of water resistance changes. This design enables locking and unlocking without actuators, offering a solution for grasping underwater objects. The gripper’s underwater movement has speed limits. Below the lower limit, fingers may not achieve sufficient bending, while exceeding the upper limit can cause vibrations and affect the locking function. Therefore, maintaining an appropriate speed is crucial. Finally, gripping experiments were conducted to confirm the gripper’s ability to grasp objects underwater.},
  archive   = {C_IROS},
  author    = {Issei Nate and Shinichi Hirai},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802745},
  month     = {10},
  pages     = {13584-13591},
  title     = {Passive underwater robot hand utilizing water resistance},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Development force control of a series elastic actuator to
excavator for mechanization of manual work *. <em>IROS</em>,
13577–13583. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automation can address labor shortage and enhance worker safety in construction. However, workers continue to perform majority of the work at construction sites that can be automated. Construction machinery require force control for automation, which can absorb external shocks and provide appropriate forces along with environmental forces. This study proposes a force-controlled excavator that fulfills these requirements by replacing the hydraulic system with a series elastic actuator (SEA). Few studies have applied SEA to large high-output construction machinery. We designed the structure of the SEA to deliver high output power in a compact form that can be mounted on an excavator. A 2.5-T class excavator equipped with this SEA is designed, which achieved a force resolution of 15–35 N at the tip. The effectiveness of this excavator in automating a major portion of the manual work is demonstrated.},
  archive   = {C_IROS},
  author    = {T. Hiramatsu and M. Saiki and N. Hara and M. Yamada and M. Momii and Y. Uebayashi and H. Sugiura},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802090},
  month     = {10},
  pages     = {13577-13583},
  title     = {Development force control of a series elastic actuator to excavator for mechanization of manual work *},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learned slip-detection-severity framework using tactile
deformation field feedback for robotic manipulation. <em>IROS</em>,
13569–13576. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safely handling objects and avoiding slippage are fundamental challenges in robotic manipulation, yet traditional techniques often oversimplify the issue by treating slippage as a binary occurrence. Our research presents a framework that both identifies slip incidents and measures their severity. We introduce a set of features based on detailed vector field analysis of tactile deformation data captured by the GelSight Mini sensor. Two distinct machine learning models use these features: one focuses on slip detection, and the other evaluates the slip’s severity, which is the slipping velocity of the object against the sensor surface. Our slip detection model achieves an average accuracy of 92%, and the slip severity estimation model exhibits a mean absolute error (MAE) of 0.6 cm/s for unseen objects. To demonstrate the synergistic approach of this framework, we employ both the models in a tactile feedback-guided vertical sliding task. Leveraging the high accuracy of slip detection, we utilize it as the foundational and corrective model and integrate the slip severity estimation into the feedback control loop to address slips without overcompensating. Videos and demonstrations are available at: https://sites.google.com/uw.edu/lsds},
  archive   = {C_IROS},
  author    = {Neel Jawale and Navneet Kaur and Amy Santoso and Xiaohai Hu and Xu Chen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802687},
  month     = {10},
  pages     = {13569-13576},
  title     = {Learned slip-detection-severity framework using tactile deformation field feedback for robotic manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep domain adaptation regression for force calibration of
optical tactile sensors. <em>IROS</em>, 13561–13568. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Optical tactile sensors provide robots with rich force information for robot grasping in unstructured environments. The fast and accurate calibration of three-dimensional contact forces holds significance for new sensors and existing tactile sensors which may have incurred damage or aging. However, the conventional neural-network-based force calibration method necessitates a large volume of force-labeled tactile images to minimize force prediction errors, with the need for accurate Force/Torque measurement tools as well as a time-consuming data collection process. To address this challenge, we propose a novel deep domain-adaptation force calibration method, designed to transfer the force prediction ability from a calibrated optical tactile sensor to uncalibrated ones with various combinations of domain gaps, including marker presence, illumination condition, and elastomer modulus. Experimental results show the effectiveness of the proposed unsupervised force calibration method, with lowest force prediction errors of 0.102N (3.4% in full force range) for normal force, and 0.095N (6.3%) and 0.062N (4.1%) for shear forces along the x-axis and y-axis, respectively. This study presents a promising, general force calibration methodology for optical tactile sensors.},
  archive   = {C_IROS},
  author    = {Zhuo Chen and Ni Ou and Jiaqi Jiang and Shan Luo},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801904},
  month     = {10},
  pages     = {13561-13568},
  title     = {Deep domain adaptation regression for force calibration of optical tactile sensors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heading control for obstacle avoidance using dynamic posture
manipulation during tumbling locomotion. <em>IROS</em>, 13555–13560. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Passive tumbling structures are energy efficient, but often sacrifice control authority due to their under actuated nature. Unlike many passive tumbling robots, Northeastern University’s COBRA is a snake robot with eleven articulated joints that transforms into a wheel-like structure with a high degree of posture control during tumbling, and using this posture manipulation, COBRA can control its forward velocity and heading angle while tumbling. This paper presents a mathematical framework that describes the dynamics of posture manipulation during tumbling and identifies two types of control actions that allow it to control its movement. This is validated in hardware testing to demonstrate obstacle avoidance during passive tumbling using only posture manipulation.},
  archive   = {C_IROS},
  author    = {Adarsh Salagame and Kruthika Gangaraju and Eric Sihite and Gunar Schirner and Alireza Ramezani},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801515},
  month     = {10},
  pages     = {13555-13560},
  title     = {Heading control for obstacle avoidance using dynamic posture manipulation during tumbling locomotion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online optimization of central pattern generators for
quadruped locomotion. <em>IROS</em>, 13547–13554. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Typical legged locomotion controllers are designed or trained offline. This is in contrast to many animals, which are able to locomote at birth, and rapidly improve their locomotion skills with few real-world interactions. Such motor control is possible through oscillatory neural networks located in the spinal cord of vertebrates, known as Central Pattern Generators (CPGs). Models of the CPG have been widely used to generate locomotion skills in robotics, but can require extensive hand-tuning or offline optimization of inter-connected parameters with genetic algorithms. In this paper, we present a framework for the online optimization of the CPG parameters through Bayesian Optimization. We show that our framework can rapidly optimize and adapt to varying velocity commands and changes in the terrain, for example to varying coefficients of friction, terrain slope angles, and added mass payloads placed on the robot. We study the effects of sensory feedback on the CPG, and find that both force feedback in the phase equations, as well as posture control (Virtual Model Control) are both beneficial for robot stability and energy efficiency. In hardware experiments on the Unitree Go1, we show rapid optimization (in under 3 minutes) and adaptation of energy-efficient gaits to varying target velocities in a variety of scenarios: varying coefficients of friction, added payloads up to 15 kg, and variable slopes up to 10 degrees.},
  archive   = {C_IROS},
  author    = {Zewei Zhang and Guillaume Bellegarda and Milad Shafiee and Auke Ijspeert},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802762},
  month     = {10},
  pages     = {13547-13554},
  title     = {Online optimization of central pattern generators for quadruped locomotion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Importance of translational velocity for bird-scale flapping
wing vehicles incapable of hovering. <em>IROS</em>, 13540–13546. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There exist multiple types of flyers in the world that either achieve lift through leveraging aerodynamics by cleverly choosing the wing airfoils of an airplane, or by flapping their wings. As the flapping wing vehicles (FWVs) decrease in scale, lift is predominantly generated by flapping. However, bigger birds or bird-scale flapping wing vehicle (BFWV) may not be able to hover but leverage the forward velocity to augment the lift generation. In this paper, we analyze the aerodynamic lift augmentation through the translational velocity for a 12g, BFWV with tail (flapping around 14 Hz). We prove that the vehicle is unable to hover, but there exist a significant augmentation through the translational motion. A cycle-averaged aerodynamic model including the contribution from the translational velocity is proposed, and experimentally validated. Finally, the analysis of the sufficient conditions on the velocity direction and orientation are studied to maintain the zero angular moment forward flight.},
  archive   = {C_IROS},
  author    = {Shijun Zhou and Aidan Orr and Nak-Seung P. Hyun},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801912},
  month     = {10},
  pages     = {13540-13546},
  title     = {Importance of translational velocity for bird-scale flapping wing vehicles incapable of hovering},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian deep predictive coding for snake-like robotic
control in unknown terrains. <em>IROS</em>, 13533–13539. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Effectively modeling the spatio-temporal interactions both internally and externally is a challenge in controlling multi-linked snake robots. This paper presents an effective method based on deep predictive coding: SnakeFormer, to address the aforementioned issue. The main contributions include: 1) Deriving a variational free energy function with two innovative regularization terms through Bayesian probabilistic analysis, offering a novel perspective to simulate the interactions between agent and the environment; 2) Introducing an interaction-attention model within a Transformer structure for predicting dynamics, and collaboratively addressing path planning and obstacle avoidance tasks. 3) By incorporating serpenoid embedding and optimizing self-attention computations, the gait stability and motion efficiency are improved. Preliminary experiments and comparative analysis with baseline models fully validate the effectiveness and generalizability of the method.},
  archive   = {C_IROS},
  author    = {William Ziming Qu and Jessica Ziyu Qu and Li Li and Jie Yang and Yuanyuan Jia},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801298},
  month     = {10},
  pages     = {13533-13539},
  title     = {Bayesian deep predictive coding for snake-like robotic control in unknown terrains},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Autonomous robotic assembly: From part singulation to
precise assembly. <em>IROS</em>, 13525–13532. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Imagine a robot that can assemble a functional product from the individual parts presented in any configuration to the robot. Designing such a robotic system is a complex problem which presents several open challenges. To bypass these challenges, the current generation of assembly systems is built with a lot of system integration effort to provide the structure and precision necessary for assembly. These systems are mostly responsible for part singulation, part kitting, and part detection, which is accomplished by intelligent system design. In this paper, we present autonomous assembly of a gear box with minimum requirements on structure. The assembly parts are randomly placed in a two-dimensional work environment for the robot. The proposed system makes use of several different manipulation skills such as sliding for grasping, in-hand manipulation, and insertion to assemble the gear box. All these tasks are run in a closed-loop fashion using vision, tactile, and Force-Torque (F/T) sensors. We perform extensive hardware experiments to show the robustness of the proposed methods as well as the overall system. See supplementary video at https://www.youtube.com/watch?v=cZ9M1DQ23OI.},
  archive   = {C_IROS},
  author    = {Kei Ota and Devesh K. Jha and Siddarth Jain and Bill Yerazunis and Radu Corcodel and Yash Shukla and Antonia Bronars and Diego Romeres},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802423},
  month     = {10},
  pages     = {13525-13532},
  title     = {Autonomous robotic assembly: From part singulation to precise assembly},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). In-hand following of deformable linear objects using
dexterous fingers with tactile sensing. <em>IROS</em>, 13518–13524. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most research on deformable linear object (DLO) manipulation assumes rigid grasping. However, beyond rigid grasping and re-grasping, in-hand following is also an essential skill that humans use to dexterously manipulate DLOs, which requires continuously changing the grasp point by in-hand sliding while holding the DLO to prevent it from falling. Achieving such a skill is very challenging for robots without using specially designed but not versatile end-effectors. Previous works have attempted using generic parallel grippers, but their robustness is unsatisfactory owing to the conflict between following and holding, which is hard to balance with a one-degree-of-freedom gripper. In this work, inspired by how humans use fingers to follow DLOs, we explore the usage of a generic dexterous hand with tactile sensing to imitate human skills and achieve robust in-hand DLO following. To enable the hardware system to function in the real world, we develop a framework that includes Cartesian-space arm-hand control, tactile-based in-hand 3-D DLO pose estimation, and task-specific motion design. Experimental results demonstrate the significant superiority of our method over using parallel grippers, as well as its great robustness, generalizability, and efficiency.},
  archive   = {C_IROS},
  author    = {Mingrui Yu and Boyuan Liang and Xiang Zhang and Xinghao Zhu and Lingfeng Sun and Changhao Wang and Shiji Song and Xiang Li and Masayoshi Tomizuka},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802081},
  month     = {10},
  pages     = {13518-13524},
  title     = {In-hand following of deformable linear objects using dexterous fingers with tactile sensing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning generalizable manipulation policy with
adapter-based parameter fine-tuning. <em>IROS</em>, 13510–13517. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study investigates the use of adapters in reinforcement learning for robotic skill generalization across multiple robots and tasks. Traditional methods are typically reliant on robot-specific retraining and face challenges such as efficiency and adaptability, particularly when scaling to robots with varying kinematics. We propose an alternative approach where a disembodied (virtual) hand manipulator learns a task (i.e., an abstract skill) and then transfers it to various robots with different kinematic constraints without retraining the entire model (i.e., the concrete, physical implementation of the skill). Whilst adapters are commonly used in other domains with strong supervision available, we show how weaker feedback from robotic control can be used to optimize task execution by preserving the abstract skill dynamics whilst adapting to new robotic domains. We demonstrate the effectiveness of our method with experiments conducted in the SAPIEN ManiSkill environment, showing improvements in generalization and task success rates. All code, data, and additional videos are at this GitHub link: https://kl-research.github.io/genrob.},
  archive   = {C_IROS},
  author    = {Kai Lu and Kim Tien Ly and William Hebberd and Kaichen Zhou and Ioannis Havoutis and Andrew Markham},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801544},
  month     = {10},
  pages     = {13510-13517},
  title     = {Learning generalizable manipulation policy with adapter-based parameter fine-tuning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Object augmentation algorithm: Computing virtual object
motion and object induced interaction wrench from optical markers.
<em>IROS</em>, 13502–13509. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study addresses the critical need for diverse and comprehensive data focused on human arm joint torques while performing activities of daily living (ADL). Previous studies have often overlooked the influence of objects on joint torques during ADL, resulting in limited datasets for analysis. To address this gap, we propose an Object Augmentation Algorithm (OAA) capable of augmenting existing marker-based databases with virtual object motions and object-induced joint torque estimations. The OAA consists of five phases: (1) computing hand coordinate systems from optical markers, (2) characterising object movements with virtual markers, (3) calculating object motions through inverse kinematics (IK), (4) determining the wrench necessary for prescribed object motion using inverse dynamics (ID), and (5) computing joint torques resulting from object manipulation. The algorithm’s accuracy is validated through trajectory tracking and torque analysis on a 5+4 degree of freedom (DoF) robotic hand-arm system, manipulating three unique objects. The results show that the OAA can accurately and precisely estimate 6 DoF object motion and object-induced joint torques. Correlations between computed and measured quantities were &gt; 0.99 for object trajectories and &gt; 0.93 for joint torques. The OAA was further shown to be robust to variations in the number and placement of input markers, which are expected between databases. Differences between repeated experiments were minor but significant (p &lt; 0.05). The algorithm expands the scope of available data and facilitates more comprehensive analyses of human-object interaction dynamics.},
  archive   = {C_IROS},
  author    = {Christopher Herneth and Junnan Li and Muhammad Hilman Fatoni and Amartya Ganguly and Sami Haddadin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802350},
  month     = {10},
  pages     = {13502-13509},
  title     = {Object augmentation algorithm: Computing virtual object motion and object induced interaction wrench from optical markers},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A digital twin-driven immersive teleoperation framework for
robot-assisted microsurgery. <em>IROS</em>, 13495–13501. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel digital twin (DT)-driven framework for immersive teleoperation in the domain of robot-assisted microsurgery (RAMS). The proposed method leverages the power of DT with mixed reality (MR) technology to create an interactive, immersive teleoperation environment for surgeons to conduct RAMS with higher precision, improved safety, and higher efficiency. More specifically, the MR device can provide operators with the 3D visualization of a digital microsurgical robot mimicking the motions of the physical one as well as the 2D real-time microscopic images during microsurgical operation. We evaluated the proposed framework through user studies based on a Trajectory Following task and conducted comparisons between scenarios with and without using the proposed framework for RAMS. The NASA-TLX questionnaire, along with additional evaluation metrics such as total trajectory, time cost, mean velocity, and a predefined collision metric, were used to analyze the user studies. Results indicated that the proposed DT-driven immersive teleoperation framework could enhance the precision, safety, and efficiency of teleoperation, and provide a satisfactory user experience to operators during microsurgical operation.},
  archive   = {C_IROS},
  author    = {Peiyang Jiang and Dandan Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801954},
  month     = {10},
  pages     = {13495-13501},
  title     = {A digital twin-driven immersive teleoperation framework for robot-assisted microsurgery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GestRight: Understanding the feasibility of gesture-driven
tele-operation in human-robot teams. <em>IROS</em>, 13487–13494. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose GestRight, a real-time system for gesture-based tele-operation of a mobile robot. For field use (e.g., smart factory settings, search and rescue missions, etc.), relying on tablet-based controls or joysticks are limiting which has led to the recent interest in hands-free operation of these assistive robots. In this work, we design three gesture-based schemes, namely, fist, touch, and wheel, represent three levels of precision–intuitiveness tradeoffs for low-level navigational control of mobile robots. GestRight includes a head-mounted device that captures hand joint data for accurate gesture recognition which is then translated to motion commands at an edge server. Through a user study involving seventeen participants, we present quantitative insights in comparison to traditional modes of control. Specifically, we evaluate GestRight in terms of the ease of navigational control, task time, and amount of errors/corrective actions required, run extensive statistical analyses, and provide a series of design recommendations for gesture-driven teleoperation systems. Our results show that gesture based schemes perform as well as traditional modes of control in contrast to participants’ self-reports on how successful they felt in controlling the robots.},
  archive   = {C_IROS},
  author    = {Kevin Rippy and Aryya Gangopadhyay and Kasthuri Jayarajah},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802649},
  month     = {10},
  pages     = {13487-13494},
  title     = {GestRight: Understanding the feasibility of gesture-driven tele-operation in human-robot teams},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interactive multi-stiffness mixed reality interface:
Controlling and visualizing robot and environment stiffness.
<em>IROS</em>, 13479–13486. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Teleoperation is a crucial technology enabling human operators to control robots remotely to perform tasks in hazardous and difficult-to-access environments. Tasks in such environments often involve complex physical interactions with tools and objects of various softness. To this end, teleimpedance enables the operators to adjust the robot impedance in real-time to simplify such interactions. While the existing teleimpedance approaches provide several interfaces to command the robot impedance, there are no interfaces to visualize both the commanded impedance and that of the objects to be interacted with. This paper presents a novel interface to provide visual feedback on the impedance of remote robots and objects. To do so, we use virtual stiffness ellipsoids and different modes that display the individual impedance of the robot and objects as well as combined post-contact impedance. The key advantage of visual feedback on the impedance compared to force feedback is that the operator can see the interaction characteristics before the contact occurs. This enables the operator to act proactively before contact rather than just reactively after the contact. This paper also proposes a new intuitive way to command the robot impedance using mixed reality, interacting with these ellipsoids and modifying them as needed. To demonstrate the key functionalities of the developed interface, we performed proof-of-concept experiments on teleoperated tasks.},
  archive   = {C_IROS},
  author    = {Alejandro Díaz Rosales and Jose Rodriguez-Nogueira and Eloise Matheson and David A. Abbink and Luka Peternel},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801866},
  month     = {10},
  pages     = {13479-13486},
  title     = {Interactive multi-stiffness mixed reality interface: Controlling and visualizing robot and environment stiffness},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reducing performance variability and overcoming limited
spatial ability: Targeted training for remote robot teleoperation.
<em>IROS</em>, 13473–13478. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a targeted training approach for remote teleoperation aimed at achieving consistent proficiency levels across users with varying capabilities. Our approach begins by assessing users’ abilities to perform robot motion control, workspace adaptation, and gripper control. It then provides tailored training based on identified skill gaps to enhance the learning effectiveness and user experience. To demonstrate our approach, we conducted a user study, with one group undergoing conventional, free-form training and the other engaging in targeted training in accordance with their skill gaps; after the training phase, participants teleoperated a robotic arm in a simulated medication preparation task for performance evaluation. Our results show that the targeted training approach effectively reduces performance variability and mitigates the influence of spatial ability on both training and task completion time. We discuss the implications of our results for practical teleoperation training and future research.},
  archive   = {C_IROS},
  author    = {Tsung-Chi Lin and Juo-Tung Chen and Chien-Ming Huang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801973},
  month     = {10},
  pages     = {13473-13478},
  title     = {Reducing performance variability and overcoming limited spatial ability: Targeted training for remote robot teleoperation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Imagine2Servo: Intelligent visual servoing with
diffusion-driven goal generation for robotic tasks. <em>IROS</em>,
13466–13472. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual servoing, the method of controlling robot motion through feedback from visual sensors, has seen significant advancements with the integration of optical flow-based methods. However, its application remains limited by inherent challenges, such as the necessity for a target image at test time, the requirement of substantial overlap between initial and target images, and the reliance on feedback from a single camera. This paper introduces Imagine2Servo †, an innovative approach leveraging diffusion-based image editing techniques to enhance visual servoing algorithms by generating intermediate goal images. This methodology allows for the extension of visual servoing applications beyond traditional constraints, enabling tasks like long-range navigation and manipulation without predefined goal images. We propose a pipeline that synthesizes subgoal images grounded in the task at hand, facilitating servoing in scenarios with minimal initial and target image overlap and integrating multi-camera feedback for comprehensive task execution. Our contributions demonstrate a novel application of image generation to robotic control, significantly broadening the capabilities of visual servoing systems. Real-world experiments validate the effectiveness and versatility of the Imagine2Servo framework in accomplishing a variety of tasks, marking a notable advancement in the field of visual servoing.},
  archive   = {C_IROS},
  author    = {Pranjali Pathre and Gunjan Gupta and M. Nomaan Qureshi and Mandyam Brunda and Samarth Brahmbhatt and K. Madhava Krishna},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802688},
  month     = {10},
  pages     = {13466-13472},
  title     = {Imagine2Servo: Intelligent visual servoing with diffusion-driven goal generation for robotic tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CMR-agent: Learning a cross-modal agent for iterative
image-to-point cloud registration. <em>IROS</em>, 13458–13465. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image-to-point cloud registration aims to determine the relative camera pose of an RGB image with respect to a point cloud. It plays an important role in camera localization within pre-built LiDAR maps. Despite the modality gaps, most learning-based methods establish 2D-3D point correspondences in feature space without any feedback mechanism for iterative optimization, resulting in poor accuracy and interpretability. In this paper, we propose to reformulate the registration procedure as an iterative Markov decision process, allowing for incremental adjustments to the camera pose based on each intermediate state. To achieve this, we employ reinforcement learning to develop a cross-modal registration agent (CMRAgent), and use imitation learning to initialize its registration policy for stability and quick-start of the training. According to the cross-modal observations, we propose a 2D-3D hybrid state representation that fully exploits the fine-grained features of RGB images while reducing the useless neutral states caused by the spatial truncation of camera frustum. Additionally, the overall framework is well-designed to efficiently reuse oneshot cross-modal embeddings, avoiding repetitive and timeconsuming feature extraction. Extensive experiments on the KITTI-Odometry and NuScenes datasets demonstrate that CMR-Agent achieves competitive accuracy and efficiency in registration. Once the one-shot embeddings are completed, each iteration only takes a few milliseconds. [code3].},
  archive   = {C_IROS},
  author    = {Gongxin Yao and Yixin Xuan and Xinyang Li and Yu Pan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802594},
  month     = {10},
  pages     = {13458-13465},
  title     = {CMR-agent: Learning a cross-modal agent for iterative image-to-point cloud registration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). In-flight initialization of global visual-inertial
estimators using geospatial data. <em>IROS</em>, 13450–13457. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we propose a solution that leverages geospatial data to initialize the monocular visual-inertial navigation system. For Visual-Inertial Navigation Systems (VINS) operating on UAVs, the ability to perform initialization and relocalization in mid-air is essential. However, degenerate motion can cause VINS to lose scale, making traditional initialization algorithms less reliable. To address this issue, we fuse geographic information in the initialization process, and utilize a learning-based feature matching algorithm to associate the information with inertial states. The proposed approach demonstrates adaptability to the degenerate motions of UAVs and significantly surpasses the estimation accuracy of conventional VINS initialization algorithms. Compared to methods that assist initialization by using a laser-range-finder (LRF), the proposed method solely relies on low-cost satellite imagery and elevation information. We evaluate the proposed approach on a large-scale UAV dataset, and compare with existing methods. The results demonstrate the superior effectiveness of the proposed method.},
  archive   = {C_IROS},
  author    = {Chunyu Li and Mengfan He and Xu Lyu and Ziyang Meng},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802622},
  month     = {10},
  pages     = {13450-13457},
  title     = {In-flight initialization of global visual-inertial estimators using geospatial data},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AutoNeRF: Training implicit scene representations with
autonomous agents. <em>IROS</em>, 13442–13449. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Implicit representations such as Neural Radiance Fields (NeRF) allow to map color, density and semantics in a 3D scene through a continuous neural function. However, these models typically require manual and careful human data collection for training. This paper addresses the problem of active exploration for autonomous NeRF construction. We study how an agent can learn to efficiently explore an unknown 3D environment so that the data collected during autonomous exploration enables the learning of a high-quality neural implicit map representation. The quality of the learned representation is evaluated on four robotics-related downstream tasks: classical viewpoint rendering, map reconstruction, planning, and pose refinement. We compare the impact of different exploration strategies including frontier-based and learning-based approaches (end-to-end and modular) with different reward functions tailored to this problem. Empirical results show that NeRFs can be trained on actively collected data using just a single episode of experience in an unseen environment and that AutoNeRF, a modular exploration policy trained with reinforcement learning, enables obtaining a higher-quality NeRF for the considered downstream robotic tasks. Finally, we show that with AutoNeRF an agent can be deployed to a previously unknown scene and then automatically improve its navigation performance by adapting to the scene through a cycle of exploration, reconstruction, and policy finetuning.},
  archive   = {C_IROS},
  author    = {Pierre Marza and Laetitia Matignon and Olivier Simonin and Dhruv Batra and Christian Wolf and Devendra S. Chaplot},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802101},
  month     = {10},
  pages     = {13442-13449},
  title     = {AutoNeRF: Training implicit scene representations with autonomous agents},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Recovering missed detections in an elevator button
segmentation task. <em>IROS</em>, 13355–13362. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One obstacle that mobile service robots face is operating elevators. Reading elevator control panel buttons involves both an instance segmentation of buttons and labels and associating buttons with their respective metal labels in the elevator. Segmentation algorithms, however, can miss detections. This paper presents a segmentation model specifically designed to solve the problem of missed detections. This can be used to recover detections that the initial model misses. This work presents: 1) a new elevator button dataset containing both 108 images sampled from the internet and 292 images imaged from 24 buildings from the University of Texas at Austin campus and the surrounding neighborhood, along with their segmentation boundaries and associated labels; 2) a vision pipeline based on Mask-RCNN for solving the initial image segmentation and labeling task; and 3) a novel method for identifying missed detections, using a Mask-RCNN network trained on expected button locations. Results show that the missed detections model, specifically developed to recover buttons and labels that were missed by the initial pass, is accurate on up to 99.33% of its predicted missed features on a synthetic missed-detection dataset and 97.14% of its predictions for features missed on a non-synthetic dataset. In the case of the average accuracy of successful button and label detections of a specifically-trained &quot;weak&quot; initial detector at a standard IoU threshold of 0.5, the missed detection model improves the detector’s success rate from 80.38% on the button recognition task with the initial segmentation model only to an average accuracy of 90.7% with the missed detections model enabled. The overall accuracy of the best-performing pipeline implementing the missed detections model is 91.73% and 98.27% on our Internet subset and Campus subset of our dataset, respectively.},
  archive   = {C_IROS},
  author    = {Nicholas Verzic and Abhinav Chadaga and Justin Hart},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801499},
  month     = {10},
  pages     = {13355-13362},
  title     = {Recovering missed detections in an elevator button segmentation task},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid stereo dense depth estimation for robotic tasks in
industrial automation. <em>IROS</em>, 13349–13354. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a simple yet effective approach for dense depth reconstruction that operates directly on raw disparity data, eliminating the need for additional disparity refinement stages. By leveraging disparity maps generated from conventional stereo methods, we train a U-Net-based model to directly map disparity to depth, bypassing complex feature engineering. Our method capitalizes on the robustness of traditional stereo matching techniques to varying scenes, focusing exclusively on dense depth reconstruction. This approach not only simplifies the training process but also significantly reduces the requirement for large-scale training datasets. Extensive evaluations demonstrate that our method surpasses classical stereo matching frameworks and state-of-the-art classical post-refinement techniques, achieving superior accuracy. Additionally, our approach offers competitive inference times, comparable to classical as well as end-to-end deep learning methods, making it highly suitable for real-time robotic applications.},
  archive   = {C_IROS},
  author    = {Suhani Singh and Michael Suppa and Raúl Suárez and Jan Rosell},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802129},
  month     = {10},
  pages     = {13349-13354},
  title     = {Hybrid stereo dense depth estimation for robotic tasks in industrial automation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploiting priors from 3D diffusion models for RGB-based
one-shot view planning. <em>IROS</em>, 13341–13348. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Object reconstruction is relevant for many autonomous robotic tasks that require interaction with the environment. A key challenge in such scenarios is planning view configurations to collect informative measurements for reconstructing an initially unknown object. One-shot view planning enables efficient data collection by predicting view configurations and planning the globally shortest path connecting all views at once. However, prior knowledge about the object is required to conduct one-shot view planning. In this work, we propose a novel one-shot view planning approach that utilizes the powerful 3D generation capabilities of diffusion models as priors. By incorporating such geometric priors into our pipeline, we achieve effective one-shot view planning starting with only a single RGB image of the object to be reconstructed. Our planning experiments in simulation and real-world setups indicate that our approach balances well between object reconstruction quality and movement cost.},
  archive   = {C_IROS},
  author    = {Sicong Pan and Liren Jin and Xuying Huang and Cyrill Stachniss and Marija Popović and Maren Bennewitz},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802551},
  month     = {10},
  pages     = {13341-13348},
  title     = {Exploiting priors from 3D diffusion models for RGB-based one-shot view planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SSCBench: A large-scale 3D semantic scene completion
benchmark for autonomous driving. <em>IROS</em>, 13333–13340. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Monocular scene understanding is a foundational component of autonomous systems. Within the spectrum of monocular perception topics, one crucial and useful task for holistic 3D scene understanding is semantic scene completion (SSC), which jointly completes semantic information and geometric details from RGB input. However, progress in SSC, particularly in large-scale street views, is hindered by the scarcity of high-quality datasets. To address this issue, we introduce SSCBench, a comprehensive benchmark that integrates scenes from widely used automotive datasets (e.g., KITTI-360, nuScenes, and Waymo). SSCBench follows an established setup and format in the community, facilitating the easy exploration of SSC methods in various street views. We benchmark models using monocular, trinocular, and point cloud input to assess the performance gap resulting from sensor coverage and modality. Moreover, we have unified semantic labels across diverse datasets to simplify cross-domain generalization testing. We commit to including more datasets and SSC models to drive further advancements in this field. Our data and code are available at https://github.com/ai4ce/SSCBench.},
  archive   = {C_IROS},
  author    = {Yiming Li and Sihang Li and Xinhao Liu and Moonjun Gong and Kenan Li and Nuo Chen and Zijun Wang and Zhiheng Li and Tao Jiang and Fisher Yu and Yue Wang and Hang Zhao and Zhiding Yu and Chen Feng},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802143},
  month     = {10},
  pages     = {13333-13340},
  title     = {SSCBench: A large-scale 3D semantic scene completion benchmark for autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Language-embedded gaussian splats (LEGS): Incrementally
building room-scale representations with a mobile robot. <em>IROS</em>,
13326–13332. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Building semantic 3D maps is valuable for searching for objects of interest in offices, warehouses, stores, and homes. We present a mapping system that incrementally builds a Language-Embedded Gaussian Splat (LEGS): a detailed 3D scene representation that encodes both appearance and semantics in a unified representation. LEGS is trained online as a robot traverses its environment to enable localization of open-vocabulary object queries. We evaluate LEGS on 4 room-scale scenes where we query for objects in the scene to assess how LEGS can capture semantic meaning. We compare LEGS to LERF [1] and find that while both systems have comparable object query success rates, LEGS trains over 3.5x faster than LERF. Results suggest that a multi-camera setup and incremental bundle adjustment can boost visual reconstruction quality in constrained robot trajectories, and suggest LEGS can localize open-vocabulary and long-tail object queries with up to 66% accuracy. See project website at: berkeleyautomation.github.io/LEGS},
  archive   = {C_IROS},
  author    = {Justin Yu and Kush Hari and Kishore Srinivas and Karim El-Refai and Adam Rashid and Chung Min Kim and Justin Kerr and Richard Cheng and Muhammad Zubair Irshad and Ashwin Balakrishna and Thomas Kollar and Ken Goldberg},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802196},
  month     = {10},
  pages     = {13326-13332},
  title     = {Language-embedded gaussian splats (LEGS): Incrementally building room-scale representations with a mobile robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Commonsense scene graph-based target localization for object
search. <em>IROS</em>, 13318–13325. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Object search is a fundamental skill for household robots, yet the core problem lies in the robot’s ability to locate the target object accurately. The dynamic nature of household environments, characterized by the arbitrary placement of daily objects by users, makes it challenging to perform target localization. To efficiently locate the target object, the robot needs to be equipped with knowledge at both the object and room level. However, existing approaches rely solely on one type of knowledge, leading to unsatisfactory object localization performance and, consequently, inefficient object search processes. To address this problem, we propose a commonsense scene graph-based target localization, CSG-TL, to enhance target object search in the household environment. Given the pre-built map with stationary items, the robot models the room-level spatial knowledge with object-level commonsense knowledge generated by a large language model (LLM) to a commonsense scene graph (CSG), supporting both types of knowledge for CSG-TL. To demonstrate the superiority of CSG-TL on target localization, extensive experiments are performed on the real-world ScanNet dataset and the AI2THOR simulator. Moreover, we have extended CSG-TL to an object search framework, CSG-OS, validated in both simulated and real-world environments. Code and videos are available at https://sites.google.com/view/csg-os.},
  archive   = {C_IROS},
  author    = {Wenqi Ge and Chao Tang and Hong Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801656},
  month     = {10},
  pages     = {13318-13325},
  title     = {Commonsense scene graph-based target localization for object search},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). QueSTMaps: Queryable semantic topological maps for 3D scene
understanding. <em>IROS</em>, 13311–13317. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic tasks such as planning and navigation require a hierarchical semantic understanding of a scene, which could include multiple floors and rooms. Current methods primarily focus on object segmentation for 3D scene understanding. However, such methods struggle to segment out topological regions like &quot;kitchen&quot; in the scene. In this work, we introduce a two-step pipeline to solve this problem. First, we extract a topological map, i.e., floorplan of the indoor scene using a novel multi-channel occupancy representation. Then, we generate CLIP-aligned features and semantic labels for every room instance based on the objects it contains using a self-attention transformer. Our language-topology alignment supports natural language querying, e.g., a &quot;place to cook&quot; locates the &quot;kitchen&quot;. We outperform the current state-of-the-art on room segmentation by ~20% and room classification by ~12%. Our detailed qualitative analysis and ablation studies provide insights into the problem of joint structural and semantic 3D scene understanding. Project Page: quest-maps.github.io},
  archive   = {C_IROS},
  author    = {Yash Mehan and Kumaraditya Gupta and Rohit Jayanti and Anirudh Govil and Sourav Garg and Madhava Krishna},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801814},
  month     = {10},
  pages     = {13311-13317},
  title     = {QueSTMaps: Queryable semantic topological maps for 3D scene understanding},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CaT: Constraints as terminations for legged locomotion
reinforcement learning. <em>IROS</em>, 13303–13310. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep Reinforcement Learning (RL) has demonstrated impressive results in solving complex robotic tasks such as quadruped locomotion. Yet, current solvers fail to produce efficient policies respecting hard constraints. In this work, we advocate for integrating constraints into robot learning and present Constraints as Terminations (CaT), a novel constrained RL algorithm. Departing from classical constrained RL formulations, we reformulate constraints through stochastic terminations during policy learning: any violation of a constraint triggers a probability of terminating potential future rewards the RL agent could attain. We propose an algorithmic approach to this formulation, by minimally modifying widely used off-the-shelf RL algorithms in robot learning (such as Proximal Policy Optimization). Our approach leads to excellent constraint adherence without introducing undue complexity and computational overhead, thus mitigating barriers to broader adoption. Through empirical evaluation on the real quadruped robot Solo crossing challenging obstacles, we demonstrate that CaT provides a compelling solution for incorporating constraints into RL frameworks. Videos and code are available at constraints-as-terminations.github.io.},
  archive   = {C_IROS},
  author    = {Elliot Chane-Sane and Pierre-Alexandre Leziart and Thomas Flayols and Olivier Stasse and Philippe Souères and Nicolas Mansard},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802334},
  month     = {10},
  pages     = {13303-13310},
  title     = {CaT: Constraints as terminations for legged locomotion reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Experience-learning inspired two-step reward method for
efficient legged locomotion learning towards natural and robust gaits.
<em>IROS</em>, 13297–13302. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Legged robots excel in navigating complex terrains, yet learning natural and robust motions in such environments remains challenging. Inspired by animals’ experience-based stepwise learning process, we propose a two-stage framework for legged robots to progressively learn naturally robust movements using a two-step reward method. Initially robots learn the fundamental gaits on flat terrains with gait-rewards and generating valuable motion data. Subsequently, leveraging learned motion experience, they adopt adversarial imitation learning to tackle challenging terrains with refined movements. Our method addresses the challenge of acquiring effective imitation data and facilitates the learning process under various gait parameters with ease. The effectiveness of this approach has been validated on both quadruped and hexapod robots, demonstrating naturally robust gaits in real-world applications.},
  archive   = {C_IROS},
  author    = {Yinghui Li and Jinze Wu and Xin Liu and Weizhong Guo and Yufei Xue},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802685},
  month     = {10},
  pages     = {13297-13302},
  title     = {Experience-learning inspired two-step reward method for efficient legged locomotion learning towards natural and robust gaits},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Co-RaL: Complementary radar-leg odometry with 4-DoF
optimization and rolling contact. <em>IROS</em>, 13289–13296. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robust and accurate localization in challenging environments is becoming crucial for SLAM. In this paper, we propose a unique sensor configuration for precise and robust odometry by integrating chip radar and a legged robot. Specifically, we introduce a tightly coupled radar-leg odometry algorithm for complementary drift correction. Adopting the 4-DoF optimization and decoupled RANSAC to mmWave chip radar significantly enhances radar odometry beyond the existing method, especially z-directional even when using a single radar. For the leg odometry, we employ rolling contact modeling-aided forward kinematics, accommodating scenarios with the potential possibility of contact drift and radar failure. We evaluate our method by comparing it with other chip radar odometry algorithms using real-world datasets with diverse environments while the datasets will be released for the robotics community. https://github.com/SangwooJung98/Co-RaL-Dataset},
  archive   = {C_IROS},
  author    = {Sangwoo Jung and Wooseong Yang and Ayoung Kim},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801960},
  month     = {10},
  pages     = {13289-13296},
  title     = {Co-RaL: Complementary radar-leg odometry with 4-DoF optimization and rolling contact},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accurate power consumption estimation method makes walking
robots energy efficient and quiet. <em>IROS</em>, 13282–13288. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Power consumption is a frequently over-looked aspect in robotics, especially in the context of legged robots. Nevertheless, improving the efficiency of walking robots is crucial to overcome the current limitations in runtime. This work proposes a novel method for precisely estimating actuator power consumption based on LSTM neural networks. The performance of this approach is benchmarked against currently employed models and validated on real hardware using certified instruments. The proposed method is integrated into the Isaac Gym framework and utilized to train a power-efficient policy. Instead of optimizing for handcrafted cost functions, such as the often used torque-square minimization, our approach for the first time trains RL policies that minimize the effective energy consumption. Hardware results demonstrate a reduction of approximately 25% in the robot’s total power consumption, with a notable 50% decrease observed for the knee actuator. Additionally, the newly developed policy generates significantly smoother and quieter motions.},
  archive   = {C_IROS},
  author    = {Giorgio Valsecchi and Andrea Vicari and Fabian Tischhauser and Manolo Garabini and Marco Hutter},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802798},
  month     = {10},
  pages     = {13282-13288},
  title     = {Accurate power consumption estimation method makes walking robots energy efficient and quiet},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BEV2PR: BEV-enhanced visual place recognition with
structural cues. <em>IROS</em>, 13274–13281. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a new image-based visual place recognition (VPR) framework by exploiting the structural cues in bird’s-eye view (BEV) from a single monocular camera. The motivation arises from two key observations about place recognition methods based on both appearance and structure: 1) For the methods relying on LiDAR sensors, the integration of LiDAR in robotic systems has led to increased expenses, while the alignment of data between different sensors is also a major challenge. 2) Other image-/camera-based methods, involving integrating RGB images and their derived variants (e.g., pseudo depth images, pseudo 3D point clouds), exhibit several limitations, such as the failure to effectively exploit the explicit spatial relationships between different objects. To tackle the above issues, we design a new BEV-enhanced VPR framework, namely BEV2PR, generating a composite descriptor with both visual cues and spatial awareness based on a single camera. The key points lie in: 1) We use BEV features as an explicit source of structural knowledge in constructing global features. 2) The lower layers of the pretrained backbone from BEV generation are shared for visual and structural streams in VPR, facilitating the learning of fine-grained local features in the visual stream. 3) The complementary visual and structural features can jointly enhance VPR performance. Our BEV2PR framework enables consistent performance improvements over several popular aggregation modules for RGB global features. The experiments on our collected VPR-NuScenes dataset demonstrate an absolute gain of 2.47% on Recall@1 for the strong Conv-AP baseline to achieve the best performance in our setting, and notably, a 18.06% gain on the hard set. The code and dataset will be available at https://github.com/FudongGe/BEV2PR.},
  archive   = {C_IROS},
  author    = {Fudong Ge and Yiwei Zhang and Shuhan Shen and Weiming Hu and Yue Wang and Jin Gao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802401},
  month     = {10},
  pages     = {13274-13281},
  title     = {BEV2PR: BEV-enhanced visual place recognition with structural cues},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Depth completion using galerkin attention. <em>IROS</em>,
13269–13273. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current depth completion methods usually employ a pair of calibrated RGB and depth sensors to reconstruct a dense depth map. Although RGB (dense) and depth (sparse) measurements are collected from the same underlying scene, they reflect different physical characteristics and thus it remains rather intricate how the devised RGB guidance scheme can effectively leads to a faithful depth recovery. Different from existing 3D geometry representations, such as point cloud, voxels or meshes, we propose to define 3D scenes as vector-valued functions, f : Ω ∋ (u, v) ↦ (r, g, b, d) ∈ ℝ4 , mapping from the image plane Ω to RGBD vectors. This scene function representation brings two benefits: 1) allowing for the adaptation of the Galerkin method to explore the nodal basis of the scene function space, and 2) transforming the irregularly scattered (X, Y, Z) points in the Euclidean space into the depth function defined over the regular grid in the image plane. We further leverage these two benefits within a deep neural network, characterized by an efficient Galerkin attention-based RGBD function embedding to effectively explore the interaction of color and depth information, and by the utilization of equivariant convolution operation on the RGBD feature map as efficient basic blocks. Experiments show that the proposed method achieves significant performance improvement over state-of-the-arts. Code at https://github.com/ZXS-Labs/DCGA.},
  archive   = {C_IROS},
  author    = {Yinuo Xu and Xuesong Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802287},
  month     = {10},
  pages     = {13269-13273},
  title     = {Depth completion using galerkin attention},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A point-based approach to efficient LiDAR multi-task
perception. <em>IROS</em>, 13261–13268. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-task perception networks hold great potential as they can improve performance and computational efficiency compared to their single-task counterparts, facilitating online deployment. However, current multi-task architectures in point cloud perception combine multiple task-specific point cloud representations, each requiring a separate feature encoder, making the network significantly large and slow. In this work, we propose PAttFormer, an efficient multi-task learning architecture for joint semantic segmentation and object detection in point clouds, only relying on a point-based representation. The network builds on transformer-based feature encoders using neighborhood attention and grid-pooling, complemented with a query-based detection decoder using a novel 3D deformable-attention detection head topology. Unlike other LiDAR-based multi-task architectures, our proposed PAttFormer does not require separate feature encoders for multiple task-specific point cloud representations, resulting in a network that is 3× smaller and 1.4× faster while achieving competitive performance on the nuScenes and KITTI benchmarks for autonomous driving perception. We perform extensive evaluations that show substantial improvement from multi-task learning, achieving +1.7% in mIoU for LiDAR semantic segmentation and +1.7% in mAP for 3D object detection on the nuScenes benchmark compared to the single-task models.},
  archive   = {C_IROS},
  author    = {Christopher Lang and Alexander Braun and Lars Schillingmann and Abhinav Valada},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802240},
  month     = {10},
  pages     = {13261-13268},
  title     = {A point-based approach to efficient LiDAR multi-task perception},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PCT: Perspective cue training framework for multi-camera BEV
segmentation. <em>IROS</em>, 13253–13260. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generating annotations for bird’s-eye-view (BEV) segmentation presents significant challenges due to the scenes’ complexity and the high manual annotation cost. In this work, we address these challenges by leveraging the abundance of unlabeled data available. We propose the Perspective Cue Training (PCT) framework, a novel training framework that utilizes pseudo-labels generated from unlabeled perspective images using publicly available semantic segmentation models trained on large street-view datasets. PCT applies a perspective view task head to the image encoder shared with the BEV segmentation head, effectively utilizing the unlabeled data to be trained with the generated pseudo-labels. Since image encoders are present in nearly all camera-based BEV segmentation architectures, PCT is flexible and applicable to various existing BEV architectures. In this paper, we applied PCT for semi-supervised learning (SSL) and unsupervised domain adaptation (UDA). Additionally, we introduce strong input perturbation through Camera Dropout (CamDrop) and feature perturbation via BEV Feature Dropout (BFD), which are crucial for enhancing SSL capabilities using our teacher-student framework. Our comprehensive approach is simple and flexible but yields significant improvements over various baselines for SSL and UDA, achieving competitive performances even against the current state-of-the-art.},
  archive   = {C_IROS},
  author    = {Haruya Ishikawa and Takumi Iida and Yoshinori Konishi and Yoshimitsu Aoki},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801350},
  month     = {10},
  pages     = {13253-13260},
  title     = {PCT: Perspective cue training framework for multi-camera BEV segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IR2: Implicit rendezvous for robotic exploration teams under
sparse intermittent connectivity. <em>IROS</em>, 13245–13252. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Information sharing is critical in time-sensitive and realistic multi-robot exploration, especially for smaller robotic teams in large-scale environments where connectivity may be sparse and intermittent. Existing methods often overlook such communication constraints by assuming unrealistic global connectivity. Other works account for communication constraints (by maintaining close proximity or line of sight during information exchange), but are often inefficient. For instance, preplanned rendezvous approaches typically involve unnecessary detours resulting from poorly timed rendezvous, while pursuit-based approaches often result in short-sighted decisions due to their greedy nature. We present IR2, a deep reinforcement learning approach to information sharing for multi-robot exploration. Leveraging attention-based neural networks trained via reinforcement and curriculum learning, IR2 allows robots to effectively reason about the longer-term trade-offs between disconnecting for solo exploration and reconnecting for information sharing. In addition, we propose a hierarchical graph formulation to maintain a sparse yet informative graph, enabling our approach to scale to large-scale environments. We present simulation results in three large-scale Gazebo environments, which show that our approach yields 6.6−34.1% shorter exploration paths and significantly improved mapped area consistency among robots when compared to state-of-the-art baselines. Our simulation training and testing code is available at https://github.com/marmotlab/IR2.},
  archive   = {C_IROS},
  author    = {Derek Ming Siang Tan and Yixiao Ma and Jingsong Liang and Yi Cheng Chng and Yuhong Cao and Guillaume Sartoretti},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801761},
  month     = {10},
  pages     = {13245-13252},
  title     = {IR2: Implicit rendezvous for robotic exploration teams under sparse intermittent connectivity},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A heterogeneous system of systems framework for proactive
path planning of a UAV-assisted UGV in uncertain environments.
<em>IROS</em>, 13237–13244. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A common challenge for mobile robots is traversing uncertain environments containing obstacles, rough terrain, or hazards. Without full knowledge of the environment, an unmanned ground vehicle (UGV) navigating towards a goal could easily drive down a path that is blocked (requiring the robot to retrace sections of its path) or run into a hazard causing a catastrophic failure. To address this issue we propose a system of systems (SoS) abstraction to group a distributed set of robots into a single system. Specifically, we propose augmenting the sensing capabilities of a UGV using an unmanned aerial vehicle (UAV). With different dynamic and sensing capabilities, the UAV scouts ahead and proactively updates the plan for the UGV using information discovered about the environment. To predict reachable states of the UGV, the UAV employs a sampling-based method in which a set of virtual particles representing simulated instances of the UGV are used to approximate the distribution of possible trajectories. The UAV assesses if the current UGV path plan is inefficient or unsafe, and if so, provides an alternative path to the UGV. For robustness, a model predictive path integral (MPPI) optimization method is used to modify the waypoints when delivered to the UGV. The strategy is validated in simulation and experimentally.Note—Videos of the simulations and experiments are provided in the supplementary material and can be accessed at: https://www.bezzorobotics.com/ps-iros24.},
  archive   = {C_IROS},
  author    = {Patrick Sherman and Nicola Bezzo},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801791},
  month     = {10},
  pages     = {13237-13244},
  title     = {A heterogeneous system of systems framework for proactive path planning of a UAV-assisted UGV in uncertain environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust online epistemic replanning of multi-robot missions.
<em>IROS</em>, 13229–13236. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As Multi-Robot Systems (MRS) become more affordable and computing capabilities grow, they provide significant advantages for complex applications such as environmental monitoring, underwater inspections, or space exploration. However, accounting for potential communication loss or the unavailability of communication infrastructures in these application domains remains an open problem. Much of the applicable MRS research assumes that the system can sustain communication through proximity regulations and formation control or by devising a framework for separating and adhering to a predetermined plan for extended periods of disconnection. The latter technique enables an MRS to be more efficient, but breakdowns and environmental uncertainties can have a domino effect throughout the system, particularly when the mission goal is intricate or time-sensitive. To deal with this problem, our proposed framework has two main phases: i) a centralized planner to allocate mission tasks by rewarding intermittent rendezvous between robots to mitigate the effects of the unforeseen events during mission execution, and ii) a decentralized replanning scheme leveraging epistemic planning to formalize belief propagation and a Monte Carlo tree search for policy optimization given distributed rational belief updates. The proposed framework outperforms a baseline heuristic and is validated using simulations and experiments with aerial vehicles.},
  archive   = {C_IROS},
  author    = {Lauren Bramblett and Branko Miloradović and Patrick Sherman and Alessandro V. Papadopoulos and Nicola Bezzo},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802214},
  month     = {10},
  pages     = {13229-13236},
  title     = {Robust online epistemic replanning of multi-robot missions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-robot communication-aware cooperative belief space
planning with inconsistent beliefs: An action-consistent approach.
<em>IROS</em>, 13221–13228. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-robot belief space planning (MR-BSP) is essential for reliable and safe autonomy. While planning, each robot maintains a belief over the state of the environment and reasons how the belief would evolve in the future for different candidate actions. Yet, existing MR-BSP works have a common assumption that the beliefs of different robots are consistent at planning time. Such an assumption is often highly unrealistic, as it requires prohibitively extensive and frequent communication capabilities. In practice, each robot may have a different belief about the state of the environment. Crucially, when the beliefs of different robots are inconsistent, state-of-the-art MR-BSP approaches could result in a lack of coordination between the robots, and in general, could yield dangerous, unsafe and suboptimal decisions. In this paper, we tackle this crucial gap. We develop a novel decentralized algorithm that is guaranteed to find a consistent joint action. For a given robot, our algorithm reasons for action preferences about 1) its local information, 2) what it perceives about the reasoning of the other robot, and 3) what it perceives about the reasoning of itself perceived by the other robot. This algorithm finds a consistent joint action whenever these steps yield the same best joint action obtained by reasoning about action preferences; otherwise, it self-triggers communication between the robots. Experimental results show efficacy of our algorithm in comparison with two baseline algorithms.},
  archive   = {C_IROS},
  author    = {Tanmoy Kundu and Moshe Rafaeli and Vadim Indelman},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801372},
  month     = {10},
  pages     = {13221-13228},
  title     = {Multi-robot communication-aware cooperative belief space planning with inconsistent beliefs: An action-consistent approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). M3-GMN: A multi-environment, multi-LiDAR, multi-task dataset
for grid map based navigation. <em>IROS</em>, 13213–13220. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a multi-environment, multi-LiDAR, multi-task dataset to promote the grid map-based navigation capability for autonomous vehicles. The dataset comprises structured and unstructured environmental data captured by different types of LiDAR and contains various challenging scenarios, including moving objects, negative obstacles, steep slopes, cliffs, overhangs, etc. Further, we have devised an innovative method for generating ground truth, facilitating the creation of dense, accurate, and stable grid maps with a minimal requirement for human annotation efforts. A new baseline method and two existing approaches are evaluated on this dataset. Results indicate that existing approaches perform much worse than the proposed baseline. The dataset will be made publicly available at https://github.com/guanglei96/M3-GMN.},
  archive   = {C_IROS},
  author    = {Guanglei Xie and Hao Fu and Hanzhang Xue and Bokai Liu and Xin Xu and Xiaohui Li and Zhenping Sun},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802553},
  month     = {10},
  pages     = {13213-13220},
  title     = {M3-GMN: A multi-environment, multi-LiDAR, multi-task dataset for grid map based navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ParkingE2E: Camera-based end-to-end parking network, from
images to planning. <em>IROS</em>, 13206–13212. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous parking is a crucial task in the intelligent driving field. Traditional parking algorithms are usually implemented using rule-based schemes. However, these methods are less effective in complex parking scenarios due to the intricate design of the algorithms. In contrast, neural-network-based methods tend to be more intuitive and versatile than the rule-based methods. By collecting a large number of expert parking trajectory data and emulating human strategy via learning-based methods, the parking task can be effectively addressed. In this paper, we employ imitation learning to perform end-to-end planning from RGB images to path planning by imitating human driving trajectories. The proposed end-to-end approach utilizes a target query encoder to fuse images and target features, and a transformer-based decoder to autoregressively predict future waypoints. We conduct extensive experiments in real-world scenarios, and the results demonstrate that the proposed method achieved an average parking success rate of 87.8% across four different real-world garages. Real-vehicle experiments further validate the feasibility and effectiveness of the method proposed in this paper. The code can be found at: https://github.com/qintonguav/ParkingE2E.},
  archive   = {C_IROS},
  author    = {Changze Li and Ziheng Ji and Zhe Chen and Tong Qin and Ming Yang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801763},
  month     = {10},
  pages     = {13206-13212},
  title     = {ParkingE2E: Camera-based end-to-end parking network, from images to planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MapLocNet: Coarse-to-fine feature registration for visual
re-localization in navigation maps. <em>IROS</em>, 13198–13205. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robust localization is the cornerstone of autonomous driving, especially in challenging urban environments where GPS signals suffer from multipath errors. Traditional localization approaches rely on high-definition (HD) maps, which consist of precisely annotated landmarks. However, building HD map is expensive and challenging to scale up. Given these limitations, leveraging navigation maps has emerged as a promising low-cost alternative for localization. Current approaches based on navigation maps can achieve highly accurate localization, but their complex matching strategies lead to unacceptable inference latency that fails to meet the real-time demands. To address these limitations, we introduce MapLocNet, a novel transformer-based neural re-localization method. Inspired by image registration, our approach performs a coarse-to-fine neural feature registration between navigation map features and visual bird’s-eye view features. MapLocNet substantially outperforms the current state-of-the-art methods on both nuScenes and Argoverse datasets, demonstrating significant improvements in localization accuracy and inference speed across both single-view and surround-view input settings. We highlight that our research presents an HD-map-free localization method for autonomous driving, offering a costeffective, reliable, and scalable solution for challenging urban environments.},
  archive   = {C_IROS},
  author    = {Hang Wu and Zhenghao Zhang and Siyuan Lin and Xiangru Mu and Qiang Zhao and Ming Yang and Tong Qin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802757},
  month     = {10},
  pages     = {13198-13205},
  title     = {MapLocNet: Coarse-to-fine feature registration for visual re-localization in navigation maps},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). METAVerse: Meta-learning traversability cost map for
off-road navigation. <em>IROS</em>, 13190–13197. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous navigation in off-road conditions requires an accurate estimation of terrain traversability. However, traversability estimation in unstructured environments is subject to high uncertainty due to the variability of numerous factors that influence vehicle-terrain interaction. Consequently, it is challenging to obtain a generalizable model that can accurately predict traversability in a variety of environments. This paper presents METAVerse, a meta-learning framework for learning a global model that accurately and reliably predicts terrain traversability across diverse environments. We train the traversability prediction network to generate a dense and continuous-valued cost map from a sparse LiDAR point cloud, leveraging vehicle-terrain interaction feedback in a self-supervised manner. Meta-learning is utilized to train a global model with driving data collected from multiple environments, effectively minimizing estimation uncertainty. During deployment, online adaptation is performed to rapidly adapt the network to the local environment by exploiting recent interaction experiences. To conduct a comprehensive evaluation, we collect driving data from various terrains and demonstrate that our method can obtain a global model that minimizes uncertainty. Moreover, by integrating our model with a model predictive controller, we demonstrate that the reduced uncertainty results in safe and stable navigation in unstructured and unknown terrains.},
  archive   = {C_IROS},
  author    = {Junwon Seo and Taekyung Kim and Seongyong Ahn and Kiho Kwak},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802444},
  month     = {10},
  pages     = {13190-13197},
  title     = {METAVerse: Meta-learning traversability cost map for off-road navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intraocular reflection modeling and avoidance planning in
image-guided ophthalmic surgeries. <em>IROS</em>, 13183–13189. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Intuitive enhancement of surgical precision in robotic retinal surgery highly depends on the stable acquisition of intraocular imaging data. Such acquisition requires segmenting intraocular components, especially instrument-tip positions, to achieve state estimation and subsequent navigation and motion control. However, intraocular light reflections and glares significantly impact instrument segmentation, state estimation, and subsequent visual servoing in retinal surgery. At the same time, light reflections are among the sources of information for intraoperative navigation. In this work, we propose a method for modeling and optimizing light reflections using microscopy as the standard surgical imaging modality. Beyond optimization, our approach seamlessly integrates the optimized reflection with path planning, strategically circumventing reflection areas and ensuring uninterrupted visibility of instrument tips throughout the surgical procedure. Experiments demonstrate the methodology’s efficacy in avoiding glare affections during eye surgeries.},
  archive   = {C_IROS},
  author    = {Junjie Yang and Zhihao Zhao and Yinzheng Zhao and Daniel Zapp and Mathias Maier and Kai Huang and Nassir Navab and M. Ali Nasseri},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801530},
  month     = {10},
  pages     = {13183-13189},
  title     = {Intraocular reflection modeling and avoidance planning in image-guided ophthalmic surgeries},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating the joint angles of a magnetic surgical tool
using monocular 3D keypoint detection and particle filtering.
<em>IROS</em>, 13176–13182. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Magnetic surgical tools benefit greatly from real-time pose estimation, as this is essential for controlling them safely and effectively. Current pose estimation methods for surgical tools either focus on rigid tools, or are developed specifically for the da Vinci surgical system. In this work, we use computer vision from a monocular endoscopic camera to estimate the pose of an articulated magnetic surgical tool. In particular, we present a deep 3D keypoint estimation framework and a particle filter to achieve this. The former method can be used for any articulated surgical tool, while the latter method is specific to magnetic tools. We show that the deep 3D keypoint estimation framework estimates the surgical tool’s joint angles with an average error of 4.0 degrees and a speed of 29 Hz. In addition, we demonstrate the robustness of the magnetic particle filter and the deep pose estimation method for real-time tool pose estimation.},
  archive   = {C_IROS},
  author    = {Erik Fredin and Eric Diller},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802411},
  month     = {10},
  pages     = {13176-13182},
  title     = {Estimating the joint angles of a magnetic surgical tool using monocular 3D keypoint detection and particle filtering},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A CT-guided control framework of a robotic flexible
endoscope for the diagnosis of the maxillary sinusitis. <em>IROS</em>,
13168–13175. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Flexible endoscopes are commonly adopted in narrow and confined anatomical cavities due to their higher reachability and dexterity. However, prolonged and unintuitive manipulation of these endoscopes leads to an increased workload on surgeons and risks of collision. To address these challenges, this paper proposes a CT-guided control framework for the diagnosis of maxillary sinusitis by using a robotic flexible endoscope. In the CT-guided control framework, a feasible path to the target position in the maxillary sinus cavity for the robotic flexible endoscope is designed. Besides, an optimal control scheme is proposed to autonomously control the robotic flexible endoscope to follow the feasible path. This greatly improves the efficiency and reduces the workload for surgeons. Several experiments were conducted based on a widely utilized sinus phantom, and the results showed that the robotic flexible endoscope can accurately and autonomously follow the feasible path and reach the target position in the maxillary sinus cavity. The results also verified the feasibility of the CT-guided control framework, which contributes an effective approach to early diagnosis of sinusitis in the future.},
  archive   = {C_IROS},
  author    = {Puchen Zhu and Huayu Zhang and Xin Ma and Xiaoyin Zheng and Xuchen Wang and Kwok Wai Samuel Au},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802259},
  month     = {10},
  pages     = {13168-13175},
  title     = {A CT-guided control framework of a robotic flexible endoscope for the diagnosis of the maxillary sinusitis},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DeepBHMR: Learning bidirectional hybrid mixture models for
generalized rigid point set registration. <em>IROS</em>, 13160–13167.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce a novel normal-assisted learning-based rigid registration approach, i.e., Deep Bi-directional Hybrid Mixture Registration (DeepBHMR). Our approach utilises helpful normal vectors explicitly in both correspondence and transformation stages and formulates the optimization objective of registration in a bi-directional way that considers noise in both point sets. DeepBHMR consists of three modules: (1) the correspondence network that estimates the correspondence probability relating points within one generalized point set (i.e., positional and normal vectors) with components of Hybrid Mixture Models (HMMs) representing the other generalized point set; (2) the posterior module that computes HMMs parameters; (3) the transformation module that computes the rotation matrix and the translation vector given the estimated generalized-point to hybrid-distribution correspondences and HMMs parameters. DeepBHMR has been validated on 291 human femur and 260 hip models, and extensive experimental results demonstrate that DeepBHMR outperforms the state-of-the-art registration methods (p-value &lt; 0.01). In the circumstance of femur bones, the mean rotation and translation error values are around 1° (i.e., 1.01°) and less than 1 mm (i.e., 0.36mm), respectively. Furthermore, even under the large transformation (i.e., in the range of [0,180]° and [0, 100] mm), the mean RMSE values being 3.05 mm is still satisfactory. Additionally, the results demonstrate the DeepBHMR’s favorable generalizability from femur shapes to hip shapes. We have carefully validated the significant benefits of incorporating normal vectors and the bidirectional mechanism. DeepBHMR can successfully handle the challenging scenario of large transformation and partial registration. The codes are available at https://github.com/zzyrobot/DeepBHMR.git.},
  archive   = {C_IROS},
  author    = {Zhe Min and Zhengyan Zhang and Ang Zhang and Rui Song and Yibin Li and Max Q.-H. Meng},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802351},
  month     = {10},
  pages     = {13160-13167},
  title     = {DeepBHMR: Learning bidirectional hybrid mixture models for generalized rigid point set registration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CoDe: A cooperative and decentralized collision avoidance
algorithm for small-scale UAV swarms considering energy efficiency.
<em>IROS</em>, 13152–13159. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a cooperative and decentralized collision avoidance algorithm (CoDe) for small-scale UAV swarms consisting of up to three UAVs. CoDe improves energy efficiency of UAVs by achieving effective cooperation among UAVs. Moreover, CoDe is specifically tailored for UAV’s operations by addressing the challenges faced by existing schemes, such as ineffectiveness in selecting actions from continuous action spaces and high computational complexity. CoDe is based on Multi-Agent Reinforcement Learning (MARL), and finds cooperative policies by incorporating a novel credit assignment scheme. The novel credit assignment scheme estimates the contribution of an individual by subtracting a baseline from the joint action value for the swarm. The credit assignment scheme in CoDe outperforms other benchmarks as the baseline takes into account not only the importance of a UAV’s action but also the interrelation between UAVs. Furthermore, extensive experiments are conducted against existing MARL-based and conventional heuristic-based algorithms to demonstrate the advantages of the proposed algorithm.},
  archive   = {C_IROS},
  author    = {Shuangyao Huang and Haibo Zhang and Zhiyi Huang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802312},
  month     = {10},
  pages     = {13152-13159},
  title     = {CoDe: A cooperative and decentralized collision avoidance algorithm for small-scale UAV swarms considering energy efficiency},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model predictive path integral control for agile unmanned
aerial vehicles. <em>IROS</em>, 13144–13151. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a control architecture for real-time and onboard control of Unmanned Aerial Vehicles (UAVs) in environments with obstacles using the Model Predictive Path Integral (MPPI) methodology. MPPI allows the use of the full nonlinear model of UAV dynamics and a more general cost function at the cost of a high computational demand. To run the controller in real-time, the sampling-based optimization is performed in parallel on a graphics processing unit onboard the UAV. We propose an approach to the simulation of the nonlinear system which respects low-level constraints, while also able to dynamically handle obstacle avoidance, and prove that our methods are able to run in real-time without the need for external computers. The MPPI controller is compared to MPC and SE(3) controllers on the reference tracking task, showing a comparable performance. We demonstrate the viability of the proposed method in multiple simulation and real-world experiments, tracking a reference at up to 44 km h−1 and acceleration close to 20 m s−2, while still being able to avoid obstacles. To the best of our knowledge, this is the first method to demonstrate an MPPI-based approach in real flight.},
  archive   = {C_IROS},
  author    = {Michal Minařík and Robert Pěnička and Vojtěch Vonásek and Martin Saska},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802756},
  month     = {10},
  pages     = {13144-13151},
  title     = {Model predictive path integral control for agile unmanned aerial vehicles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TOPPQuad: Dynamically-feasible time-optimal path
parametrization for quadrotors. <em>IROS</em>, 13136–13143. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Planning time-optimal trajectories for quadrotors in cluttered environments is a challenging, non-convex problem. This paper addresses minimizing the traversal time of a given collision-free geometric path without violating actuation bounds of the vehicle. Previous approaches have either relied on convex relaxations that do not guarantee dynamic feasibility or have generated overly conservative time parametrizations. We propose TOPPQuad, a time-optimal path parameterization algorithm for quadrotors which explicitly incorporates quadrotor rigid body dynamics and constraints, such as bounds on inputs (including motor thrusts) and state of the vehicle (including the pose, linear and angular velocity and acceleration). We demonstrate the ability of the planner to generate faster trajectories that respect hardware constraints of the robot compared to planners with relaxed notions of dynamic feasibility in both simulation and hardware. We also demonstrate how TOPPQuad can be used to plan trajectories for quadrotors that utilize bidirectional motors. Overall, the proposed approach paves a way towards maximizing the efficacy of autonomous micro aerial vehicles while ensuring their safety.},
  archive   = {C_IROS},
  author    = {Katherine Mao and Igor Spasojevic and M. Ani Hsieh and Vijay Kumar},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801611},
  month     = {10},
  pages     = {13136-13143},
  title     = {TOPPQuad: Dynamically-feasible time-optimal path parametrization for quadrotors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identifying optimal launch sites of high-altitude
latex-balloons using bayesian optimisation for the task of
station-keeping. <em>IROS</em>, 13128–13135. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Station-keeping tasks for high-altitude balloons show promise in areas such as ecological surveys, atmospheric analysis, and communication relays. However, identifying the optimal time and position to launch a latex high-altitude balloon is still a challenging and multifaceted problem. For example, tasks such as forest fire tracking place geometric constraints on the launch location of the balloon. Furthermore, identifying the most optimal location also heavily depends on atmospheric conditions. We first illustrate how reinforcement learning-based controllers, frequently used for station-keeping tasks, can exploit the environment. This exploitation can degrade performance on unseen weather patterns and affect station-keeping performance when identifying an optimal launch configuration. Valuing all states equally in the region, the agent exploits the region’s geometry by flying near the edge, leading to risky behaviours. We propose a modification which compensates for this exploitation and finds this leads to, on average, higher steps within the target region on unseen data. Then, we illustrate how Bayesian Optimisation (BO) can identify the optimal launch location to perform station-keeping tasks, maximising the return from a given rollout. We show BO can find this launch location in fewer steps compared to other optimisation methods. Results indicate that, surprisingly, the most optimal location to launch from is not commonly within the target region. Please find further information about our project at https://sites.google.com/view/bo-lauch-balloon/.},
  archive   = {C_IROS},
  author    = {Jack Saunders and Sajad Saeedi and Adam Hartshorne and Binbin Xu and Özgür Şimşek and Alan Hunter and Wenbin Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802275},
  month     = {10},
  pages     = {13128-13135},
  title     = {Identifying optimal launch sites of high-altitude latex-balloons using bayesian optimisation for the task of station-keeping},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fine manipulation using a tactile skin: Learning in
simulation and sim-to-real transfer. <em>IROS</em>, 13120–13127. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We want to enable fine manipulation with a multi-fingered robotic hand by using modern deep reinforcement learning methods. Key for fine manipulation is a spatially resolved tactile sensor. Here, we present a novel model of a tactile skin that can be used together with rigid-body (hence fast) physics simulators. The model considers the softness of the real fingertips such that a contact can spread across multiple taxels of the sensor depending on the contact geometry. We calibrate the model parameters to allow for an accurate simulation of the real-world sensor. For this, we present a self-contained calibration method without external tools or sensors. To demonstrate the validity of our approach, we learn two challenging fine manipulation tasks: Rolling a marble and a bolt between two fingers. We show in simulation experiments that tactile feedback is crucial for precise manipulation and reaching sub-taxel resolution of &lt;1 mm (despite a taxel spacing of 4 mm). Moreover, we demonstrate that all policies successfully transfer from the simulation to the real robotic hand.Website: aidx-lab.org/skin/iros24},
  archive   = {C_IROS},
  author    = {Ulf Kasolowsky and Berthold Bäuml},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801397},
  month     = {10},
  pages     = {13120-13127},
  title     = {Fine manipulation using a tactile skin: Learning in simulation and sim-to-real transfer},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning a shape-conditioned agent for purely tactile
in-hand manipulation of various objects. <em>IROS</em>, 13112–13119. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reorienting diverse objects with a multi-fingered hand is a challenging task. Current methods in robotic in-hand manipulation are either object-specific or require permanent supervision of the object state from visual sensors. This is far from human capabilities and from what is needed in real-world applications. In this work, we address this gap by training shape-conditioned agents to reorient diverse objects in hand, relying purely on tactile feedback (via torque and position measurements of the fingers’ joints). To achieve this, we propose a learning framework that exploits shape information in a reinforcement learning policy and a learned state estimator. We find that representing 3D shapes by vectors from a fixed set of basis points to the shape’s surface, transformed by its predicted 3D pose, is especially helpful for learning dexterous in-hand manipulation. In simulation and real-world experiments, we show the reorientation of many objects with high success rates, on par with state-of-the-art results obtained with specialized single-object agents. Moreover, we show generalization to novel objects, achieving success rates of ~90% even for non-convex shapes.Website: https://aidx-lab.org/manipulation/iros24},
  archive   = {C_IROS},
  author    = {Johannes Pitz and Lennart Röstel and Leon Sievers and Darius Burschka and Berthold Bäuml},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802864},
  month     = {10},
  pages     = {13112-13119},
  title     = {Learning a shape-conditioned agent for purely tactile in-hand manipulation of various objects},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating a movable palm in caging inspired grasping using
a reinforcement learning-based approach. <em>IROS</em>, 13104–13111. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we study the effectiveness of using a rigid movable palm for grasping varied objects, on a caging inspired gripper with three flexible fingers. This rigid palm extends to actively exert downwards force on objects, in contrast with existing methods, which combine movable palms with negative pressure to exert lifting forces on objects. We compare grasping with and without the palm, whilst also changing finger stiffness and fingertip angle, to analyse the effect on grasp success rate and stability over 24 design permutations. Reinforcement learning was used to train a unique grasping controller in every design case, aiming to achieve optimal grasping as the basis for comparison. Validation in both simulation and the real world was completed for every permutation. We demonstrated that the using palm improved success rates on average by 11% in simulation, 13% in the real world, and achieved a best real world success rate of 96% on 18 YCB benchmark food objects. Grasp stability against disturbances in three axes improved by 15% on average when using the palm. Our investigation determined fingertip angle had a large effect, whereas finger stiffness was less important.},
  archive   = {C_IROS},
  author    = {Luke Beddow and Helge Wurdemann and Dimitrios Kanoulas},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801702},
  month     = {10},
  pages     = {13104-13111},
  title     = {Evaluating a movable palm in caging inspired grasping using a reinforcement learning-based approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AO-grasp: Articulated object grasp generation.
<em>IROS</em>, 13096–13103. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce AO-Grasp, a grasp proposal method that generates 6 DoF grasps that enable robots to interact with articulated objects, such as opening and closing cabinets and appliances. AO-Grasp consists of two main contributions: the AO-Grasp Model and the AO-Grasp Dataset. Given a segmented partial point cloud of a single articulated object, the AO-Grasp Model predicts the best grasp points on the object with an Actionable Grasp Point Predictor. Then, it finds corresponding grasp orientations for each of these points, resulting in stable and actionable grasp proposals. We train the AO-Grasp Model on our new AO-Grasp Dataset, which contains 78K actionable parallel-jaw grasps on synthetic articulated objects. In simulation, AO-Grasp achieves a 45.0% grasp success rate, whereas the highest performing baseline achieves a 35.0% success rate. Additionally, we evaluate AO-Grasp on 120 real-world scenes of objects with varied geometries, articulation axes, and joint states, where AO-Grasp produces successful grasps on 67.5% of scenes, while the baseline only produces successful grasps on 33.3% of scenes. To the best of our knowledge, AO-Grasp is the first method for generating 6 DoF grasps on articulated objects directly from partial point clouds without requiring part detection or hand-designed grasp heuristics. The AO-Grasp Dataset and a pre-trained AO-Grasp model are available at our project website: https://stanford-iprl-lab.github.io/ao-grasp/.},
  archive   = {C_IROS},
  author    = {Carlota Parés Morlans and Claire Chen and Yijia Weng and Michelle Yi and Yuying Huang and Nick Heppert and Linqi Zhou and Leonidas Guibas and Jeannette Bohg},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802558},
  month     = {10},
  pages     = {13096-13103},
  title     = {AO-grasp: Articulated object grasp generation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Design and control of a three-dimensional electromagnetic
drive system for micro-robots. <em>IROS</em>, 13090–13095. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Three-dimensional electromagnetic field drive technology, as a cutting-edge remote wireless control method, is extensively utilized in the biomedical diagnosis and treatment of micro-robots. This paper presents the design of a three-dimensional electromagnetic drive system for micro-robots, leveraging a gradient magnetic field to achieve comprehensive automatic control in three axes. Firstly, we refine the iron core’s end structure to produce an uniform gradient magnetic field throughout the three-dimensional space. Following that, the parameters at the end of the iron core are fine-tuned to meet the specifications for magnetic field gradient, magnetic flux density, and effective workspace. Then a three-dimensional electromagnetic drive system with strong magnetic field gradient is established, achieving a remarkable maximum gradient of 1.70 T/m at the center of the workspace. Compared with other systems, the gradient is significantly enhanced. Subsequently, we carry out a three-dimensional drive experiment for a micro-robot, confirming the system’s driving efficacy. To enable precise path following for micro-robots within a three-dimensional space, we have formulated a control strategy rooted in micro-robot dynamics. The controller stability is guaranteed through the Lyapunov theory. Ultimately, a three-dimensional path following experiment is executed on the developed electromagnetic drive system. The experiment confirms the capability of our designed system which can achieve the three-dimensional closed-loop motion for the micro-robot.},
  archive   = {C_IROS},
  author    = {Yunrui Zhang and Yueyue Liu and Qigao Fan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801811},
  month     = {10},
  pages     = {13090-13095},
  title     = {Design and control of a three-dimensional electromagnetic drive system for micro-robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Absolute pose estimation for a millimeter-scale vision
system. <em>IROS</em>, 13082–13089. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision is an important component of robotic perception systems due to the rich information provided by high resolution image sensors, but computer vision algorithms can be computationally expensive and ill-suited to resource-constrained robotic systems. Here, we present a mm-scale vision system capable of performing absolute pose estimation at 16.5 FPS. This novel vision system uses a commercial-off-the-shelf sensor and microcontroller unit, as well as planar light-based landmarks in the environment to simplify feature detection. We exploit the structure of the planar pose problem to reduce algorithmic complexity and improve latency and energy consumption through software-, processor-, and hardware-in-the-loop testing. The end-to-end system consumes 49 mA of current and computes absolute pose estimates within 15 mm over a number of reference trajectories.},
  archive   = {C_IROS},
  author    = {Derin Ozturk and Zilin Wang and E. Farrell Helbling},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801942},
  month     = {10},
  pages     = {13082-13089},
  title     = {Absolute pose estimation for a millimeter-scale vision system},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time particle cluster manipulation with holographic
acoustic end-effector under microscope. <em>IROS</em>, 13076–13081. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Non-contact particle cluster manipulation holds significant promise in the realms of advanced manufacturing, chemistry, and pharmacy. However, achieving precise and dynamic control over the spatial kinematics of particle clusters remains a significant challenge, necessitating real-time and accurately programmable robotic end-effector. To this end, we develop an innovative non-contact, precise particle cluster manipulation system with ultrasonic phased array transducer (PAT) under microscope. This system combines a physics-based deep learning algorithm for real-time calculation of phase-only holograms (POHs), supporting PAT to dynamically form acoustic fields, namely holographic acoustic end-effector (HAE). Leveraging the dynamically and accurately generated HAEs by our system, kinematics control of particle clusters including aggregation, rotation, and translation is yielded. The extensive experiments well demonstrated the effectiveness of proposed system for particle cluster manipulation.},
  archive   = {C_IROS},
  author    = {Siyuan An and Chengxi Zhong and Mingyue Wang and Shudong Wang and Haojian Lu and Jiaqi Li and Youfu Li and Song Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801630},
  month     = {10},
  pages     = {13076-13081},
  title     = {Real-time particle cluster manipulation with holographic acoustic end-effector under microscope},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning the inverse kinematics of magnetic continuum robot
for teleoperated navigation. <em>IROS</em>, 13070–13075. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Magnetic continuum robots are subject to external magnetic fields and deformed remotely, simplifying the robot’s transmission mechanism and providing it with significant potential for miniaturization and operational flexibility. However, modeling magnetic field distribution generated by permanent magnets is complex and requires time-consuming pre-calibrations. Moreover, it is highly susceptible to environments with ferromagnetic materials, posing significant challenges for the control of magnetic continuum robots. In response, we propose an approach that does not overly focus on the magnetic field distribution but instead directly learns the inverse kinematics of magnetic continuum robots end-to-end. Binding the robot’s configuration to the pose of external magnets, precise control of continuum robots is facilitated. Additionally, we leverage teleoperation techniques to broaden the applicability of this method. By mounting magnets on a robotic arm and directly utilizing the target pose of the external magnet predicted by a multi-layer perceptron (MLP), we achieve the operation and navigation of magnetic continuum robots in complex environments. Experiments demonstrate that the mean control accuracy along the robot using our learning-based inverse kinematics is about half of the robot’s diameter.},
  archive   = {C_IROS},
  author    = {Pingyu Xiang and Ke Qiu and Danying Sun and Jingyu Zhang and Qin Fang and Xiangyu Mi and Shudong Wang and Mengxiao Chen and Yue Wang and Rong Xiong and Haojian Lu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801526},
  month     = {10},
  pages     = {13070-13075},
  title     = {Learning the inverse kinematics of magnetic continuum robot for teleoperated navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ActNeRF: Uncertainty-aware active learning of NeRF-based
object models for robot manipulators using visual and re-orientation
actions. <em>IROS</em>, 13062–13069. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Manipulating unseen objects is challenging without a 3D representation, as objects generally have occluded surfaces. This requires physical interaction with objects to build their internal representations. This paper presents an approach that enables a robot to rapidly learn the complete 3D model of a given object for manipulation in unfamiliar orientations. We use an ensemble of partially constructed NeRF models to quantify model uncertainty to determine the next action (a visual or re-orientation action) by optimizing informativeness and feasibility. Further, our approach determines when and how to grasp and re-orient an object given its partial NeRF model and re-estimates the object pose to rectify misalignments introduced during the interaction. Experiments with a simulated Franka Emika Robot Manipulator operating in a tabletop environment with benchmark objects demonstrate an improvement of (i) 14% in visual reconstruction quality (PSNR), (ii) 20% in the geometric/depth reconstruction of the object surface (F-score) and (iii) 71% in the task success rate of manipulating objects a-priori unseen orientations/stable configurations in the scene; over current methods. The project page can be found at https://actnerf.github.io/},
  archive   = {C_IROS},
  author    = {Saptarshi Dasgupta and Akshat Gupta and Shreshth Tuli and Rohan Paul},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801767},
  month     = {10},
  pages     = {13062-13069},
  title     = {ActNeRF: Uncertainty-aware active learning of NeRF-based object models for robot manipulators using visual and re-orientation actions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Map-aware human pose prediction for robot follow-ahead.
<em>IROS</em>, 13031–13038. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the robot follow-ahead task, a mobile robot is tasked to maintain its relative position in front of a moving human actor while keeping the actor in sight. To accomplish this task, it is important that the robot understand the full 3D pose of the human (since the head orientation can be different than the torso) and predict future human poses so as to plan accordingly. This prediction task is especially tricky in a complex environment with junctions and multiple corridors. In this work, we address the problem of forecasting the full 3D trajectory of a human in such environments. Our main insight is to show that one can first predict the 2D trajectory and then estimate the full 3D trajectory by conditioning the estimator on the predicted 2D trajectory. With this approach, we achieve results comparable or better than the state-of-the-art methods three times faster. As part of our contribution, we present a new dataset where, in contrast to existing datasets, the human motion is in a much larger area than a single room. We also present a complete robot system that integrates our human pose forecasting network on the mobile robot to enable real-time robot follow-ahead and present results from real-world experiments in multiple buildings on campus. Our project page, including supplementary material and videos, can be found at: https://qingyuan-jiang.github.io/iros2024_poseForecasting/},
  archive   = {C_IROS},
  author    = {Qingyuan Jiang and Burak Susam and Jun-Jee Chao and Volkan Isler},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802110},
  month     = {10},
  pages     = {13031-13038},
  title     = {Map-aware human pose prediction for robot follow-ahead},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hyp2Nav: Hyperbolic planning and curiosity for crowd
navigation. <em>IROS</em>, 13023–13030. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous robots are increasingly becoming a strong fixture in social environments. Effective crowd navigation requires not only safe yet fast planning, but should also enable interpretability and computational efficiency for working in real-time on embedded devices. In this work, we advocate for hyperbolic learning to enable crowd navigation and we introduce Hyp2Nav. Different from conventional reinforcement learning-based crowd navigation methods, Hyp2Nav leverages the intrinsic properties of hyperbolic geometry to better encode the hierarchical nature of decision-making processes in navigation tasks. We propose a hyperbolic policy model and a hyperbolic curiosity module that results in effective social navigation, best success rates, and returns across multiple simulation settings, using up to 6 times fewer parameters than competitor state-of-the-art models. With our approach, it becomes even possible to obtain policies that work in 2-dimensional embedding spaces, opening up new possibilities for low-resource crowd navigation and model interpretability. Insightfully, the internal hyperbolic representation of Hyp2Nav correlates with how much attention the robot pays to the surrounding crowds, e.g. due to multiple people occluding its pathway or to a few of them showing colliding plans, rather than to its own planned route. The code is available at https://github.com/GDam90/hyp2nav.},
  archive   = {C_IROS},
  author    = {Guido M. D’Amely di Melendugno and Alessandro Flaborea and Pascal Mettes and Fabio Galasso},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801513},
  month     = {10},
  pages     = {13023-13030},
  title     = {Hyp2Nav: Hyperbolic planning and curiosity for crowd navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GazeMotion: Gaze-guided human motion forecasting.
<em>IROS</em>, 13017–13022. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present GazeMotion – a novel method for human motion forecasting that combines information on past human poses with human eye gaze. Inspired by evidence from behavioural sciences showing that human eye and body movements are closely coordinated, GazeMotion first predicts future eye gaze from past gaze, then fuses predicted future gaze and past poses into a gaze-pose graph, and finally uses a residual graph convolutional network to forecast body motion. We extensively evaluate our method on the MoGaze, ADT, and GIMO benchmark datasets and show that it outperforms state-of-the-art methods by up to 7.4% improvement in mean per joint position error. Using head direction as a proxy to gaze, our method still achieves an average improvement of 5.5%. We finally report an online user study showing that our method also outperforms prior methods in terms of perceived realism. These results show the significant information content available in eye gaze for human motion forecasting as well as the effectiveness of our method in exploiting this information.},
  archive   = {C_IROS},
  author    = {Zhiming Hu and Syn Schmitt and Daniel Häufle and Andreas Bulling},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802548},
  month     = {10},
  pages     = {13017-13022},
  title     = {GazeMotion: Gaze-guided human motion forecasting},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SparseGTN: Human trajectory forecasting with sparsely
represented scene and incomplete trajectories. <em>IROS</em>,
13009–13016. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, great progress has been made in forecasting human motion in crowded scenes. However, current methods are far from practical applications due to the unbearable high computation costs, especially for encoding scene context. In addition, neglecting the partially detected trajectories makes the predicted outcome deviate from the real trajectory distribution. To handle the aforementioned concerns, we propose to represent the scene context and partially observed trajectories with sparse graphs. Customized for this special data structure, we design a hierarchical Graph Transformer Network model SparseGTN to predict multiple possible future trajectories of the target pedestrian by digesting the sparsely represented inputs. Our approach exhibits superiority over the state-of-the-art (SOTA) methods, utilizing a mere 3.42% of the number of floating point operations (FLOPs) and 0.53% of the number of model parameters. The code will be available online⋆.},
  archive   = {C_IROS},
  author    = {Jianbang Liu and Guangyang Li and Xinyu Mao and Fei Meng and Jie Mei and Max Q.-H. Meng},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801803},
  month     = {10},
  pages     = {13009-13016},
  title     = {SparseGTN: Human trajectory forecasting with sparsely represented scene and incomplete trajectories},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distilling knowledge for short-to-long term trajectory
prediction. <em>IROS</em>, 13001–13008. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Long-term trajectory forecasting is an important and challenging problem in the fields of computer vision, machine learning, and robotics. One fundamental difficulty stands in the evolution of the trajectory that becomes more and more uncertain and unpredictable as the time horizon grows, subsequently increasing the complexity of the problem. To overcome this issue, in this paper, we propose Di-Long, a new method that employs the distillation of a short-term trajectory model forecaster that guides a student network for long-term trajectory prediction during the training process. Given a total sequence length that comprehends the allowed observation for the student network and the complementary target sequence, we let the student and the teacher solve two different related tasks defined over the same full trajectory: the student observes a short sequence and predicts a long trajectory, whereas the teacher observes a longer sequence and predicts the remaining short target trajectory. The teacher’s task is less uncertain, and we use its accurate predictions to guide the student through our knowledge distillation framework, reducing long-term future uncertainty. Our experiments show that our proposed Di-Long method is effective for long-term forecasting and achieves state-of-the-art performance on the Intersection Drone Dataset (inD) and the Stanford Drone Dataset (SDD).},
  archive   = {C_IROS},
  author    = {Sourav Das and Guglielmo Camporese and Shaokang Cheng and Lamberto Ballan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801315},
  month     = {10},
  pages     = {13001-13008},
  title     = {Distilling knowledge for short-to-long term trajectory prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mind the error! Detection and localization of instruction
errors in vision-and-language navigation. <em>IROS</em>, 12993–13000.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision-and-Language Navigation in Continuous Environments (VLN-CE) is one of the most intuitive yet challenging embodied AI tasks. Agents are tasked to navigate towards a target goal by executing a set of low-level actions, following a series of natural language instructions. All VLN-CE methods in the literature assume that language instructions are exact. However, in practice, instructions given by humans can contain errors when describing a spatial environment due to inaccurate memory or confusion. Current VLN-CE benchmarks do not address this scenario, making the state-of-the-art methods in VLN-CE fragile in the presence of erroneous instructions from human users. For the first time, we propose a novel benchmark dataset that introduces various types of instruction errors considering potential human causes. This benchmark provides valuable insight into the robustness of VLN systems in continuous environments. We observe a noticeable performance drop (up to −25%) in Success Rate when evaluating the state-of-the-art VLN-CE methods on our benchmark. Moreover, we formally define the task of Instruction Error Detection and Localization, and establish an evaluation protocol on top of our benchmark dataset. We also propose an effective method, based on a cross-modal transformer architecture, that achieves the best performance in error detection and localization, compared to baselines. Surprisingly, our proposed method has revealed errors in the validation set of the two commonly used datasets for VLN-CE, i.e., R2R-CE and RxR-CE, demonstrating the utility of our technique in other tasks.},
  archive   = {C_IROS},
  author    = {Francesco Taioli and Stefano Rosa and Alberto Castellini and Lorenzo Natale and Alessio Del Bue and Alessandro Farinelli and Marco Cristani and Yiming Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801822},
  month     = {10},
  pages     = {12993-13000},
  title     = {Mind the error! detection and localization of instruction errors in vision-and-language navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RNR-nav: A real-world visual navigation system using
renderable neural radiance maps. <em>IROS</em>, 12987–12992. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel visual localization and navigation framework for real-world environments directly integrating observed visual information into the bird-eye-view map. While the renderable neural radiance map (RNR-Map) [1] shows considerable promise in simulated settings, its deployment in real-world scenarios poses undiscovered challenges. RNR-Map utilizes projections of multiple vectors into a single latent code, resulting in information loss under suboptimal conditions. To address such issues, our enhanced RNR-Map for real-world robots, RNR-Map++, incorporates strategies to mitigate information loss, such as a weighted map and positional encoding. For robust real-time localization, we integrate a particle filter into the correlation-based localization framework using RNR-Map++ without a rendering procedure. Consequently, we establish a real-world robot system for visual navigation utilizing RNR-Map++, which we call &quot;RNR-Nav.&quot; Experimental results demonstrate that the proposed methods significantly enhance rendering quality and localization robustness compared to previous approaches. In real-world navigation tasks, RNR-Nav achieves a success rate of 84.4%, marking a 68.8% enhancement over the methods of the original RNR-Map paper.},
  archive   = {C_IROS},
  author    = {Minsoo Kim and Obin Kwon and Howoong Jun and Songhwai Oh},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801722},
  month     = {10},
  pages     = {12987-12992},
  title     = {RNR-nav: A real-world visual navigation system using renderable neural radiance maps},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DD-VNB: A depth-based dual-loop framework for real-time
visually navigated bronchoscopy. <em>IROS</em>, 12979–12986. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-time 6 DOF localization of bronchoscopes is crucial for enhancing intervention quality. However, current vision-based technologies struggle to balance between generalization to unseen data and computational speed. In this study, we propose a Depth-based Dual-Loop framework for real-time Visually Navigated Bronchoscopy (DD-VNB) that can generalize across patient cases without the need of re-training. The DD-VNB framework integrates two key modules: depth estimation and dual-loop localization. To address the domain gap among patients, we propose a knowledge-embedded depth estimation network that maps endoscope frames to depth, ensuring generalization by eliminating patient-specific textures. The network embeds view synthesis knowledge into a cycle adversarial architecture for scale-constrained monocular depth estimation. For real-time performance, our localization module embeds a fast ego-motion estimation network into the loop of depth registration. The ego-motion inference network estimates the pose change of the bronchoscope in high frequency while depth registration against the pre-operative 3D model provides absolute pose periodically. Specifically, the relative pose changes are fed into the registration process as the initial guess to boost its accuracy and speed. Experiments on phantom and in-vivo data from patients demonstrate the effectiveness of our framework: 1) monocular depth estimation outperforms SOTA, 2) localization achieves an accuracy of Absolute Tracking Error (ATE) of 4.7 ± 3.17 mm in phantom and 6.49 ± 3.88 mm in patient data, 3) with a frame-rate approaching video capture speed, 4) without the necessity of case-wise network retraining. The framework’s superior speed and accuracy demonstrate its promising clinical potential for real-time bronchoscopic navigation.},
  archive   = {C_IROS},
  author    = {Qingyao Tian and Huai Liao and Xinyan Huang and Jian Chen and Zihui Zhang and Bingyu Yang and Sebastien Ourselin and Hongbin Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801553},
  month     = {10},
  pages     = {12979-12986},
  title     = {DD-VNB: A depth-based dual-loop framework for real-time visually navigated bronchoscopy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conditional generative denoiser for nighttime UAV tracking.
<em>IROS</em>, 12971–12978. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {State-of-the-art (SOTA) visual object tracking methods have significantly enhanced the autonomy of unmanned aerial vehicles (UAVs). However, in low-light conditions, the presence of irregular real noise from the environments severely degrades the performance of these SOTA methods. Moreover, existing SOTA denoising techniques often fail to meet the real-time processing requirements when deployed as plug-and-play denoisers for UAV tracking. To address this challenge, this work proposes a novel conditional generative denoiser (CG-Denoiser), which breaks free from the limitations of traditional deterministic paradigms and generates the noise conditioning on the input, subsequently removing it. To better align the input dimensions and accelerate inference, a novel nested residual Transformer conditionalizer is developed. Furthermore, an innovative multi-kernel conditional refiner is designed to pertinently refine the denoised output. Extensive experiments show that CGDenoiser promotes the tracking precision of the SOTA tracker by 18.18% on DarkTrack2021 whereas working 5.8 times faster than the second well-performed denoiser. Real-world tests with complex challenges also prove the effectiveness and practicality of CGDenoiser. Code, video demo and supplementary proof for CGDenoier are now available at: https://github.com/vision4robotics/CGDenoiser.},
  archive   = {C_IROS},
  author    = {Yucheng Wang and Changhong Fu and Kunhan Lu and Liangliang Yao and Haobo Zuo},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802714},
  month     = {10},
  pages     = {12971-12978},
  title     = {Conditional generative denoiser for nighttime UAV tracking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sparse points to dense clouds: Enhancing 3D detection with
limited LiDAR data. <em>IROS</em>, 12963–12970. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D detection is a critical task that enables machines to identify and locate objects in three-dimensional space. It has a broad range of applications in several fields, including autonomous driving, robotics and augmented reality. Monocular 3D detection is attractive as it requires only a single camera, however, it lacks the accuracy and robustness required for real world applications. High resolution LiDAR on the other hand, can be expensive and lead to interference problems in heavy traffic given their active transmissions. We propose a balanced approach that combines the advantages of monocular and point cloud-based 3D detection. Our method requires only a small number of 3D points, that can be obtained from a low-cost, low-resolution sensor. Specifically, we use only 512 points, which is just 1% of a full LiDAR frame in the KITTI dataset. Our method reconstructs a complete 3D point cloud from this limited 3D information combined with a single image. The reconstructed 3D point cloud and corresponding image can be used by any multi-modal off-the-shelf detector for 3D object detection. By using the proposed network architecture with an off-the-shelf multi-modal 3D detector, the accuracy of 3D detection improves by 20% compared to the state-of-theart monocular detection methods and 6% to 9% compare to the baseline multi-modal methods on KITTI and JackRabbot datasets.},
  archive   = {C_IROS},
  author    = {Aakash Kumar and Chen Chen and Ajmal Mian and Neils Lobo and Mubarak Shah},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801818},
  month     = {10},
  pages     = {12963-12970},
  title     = {Sparse points to dense clouds: Enhancing 3D detection with limited LiDAR data},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NRDF - neural region descriptor fields as implicit ROI
representation for robotic 3D surface processing. <em>IROS</em>,
12955–12962. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To automate 3D surface processing across diverse category-level objects it is imperative to represent process-related region of interest (P-ROI), which is not obtained with conventional keypoint or semantic part correspondences. To resolve this issue, we propose Neural Region Descriptor Fields (NRDF) for achieving unsupervised dense 3D surface region correspondence such that arbitrary ROI is retrieved for a new instance of a known category of object. We utilize the NRDF representation as a medium to facilitate one-shot P-ROI level process knowledge transfer. Recent developments in implicit 3D object representations have focused on keypoint or part correspondences, which have resulted in applications like robotic grasping and manipulation. However, explicit one-shot P-ROI correspondence, and its application for 3D surface process knowledge transfer, is treated for the first time in this work, to the best of our knowledge. The evaluation results show that the proposed approach outperforms the dense correspondence baselines in implicit shape representation and the capacity to retrieve matching arbitrary ROIs. In addition, we validate the practicality of our proposed system in a real-world robotic surface processing application. Our code is available at https://github.com/Profactor/Neural-Region-Descriptor-Fields.},
  archive   = {C_IROS},
  author    = {Anish Pratheepkumar and Markus Ikeda and Michael Hofmann and Fabian Widmoser and Andreas Pichler and Markus Vincze},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802862},
  month     = {10},
  pages     = {12955-12962},
  title     = {NRDF - neural region descriptor fields as implicit ROI representation for robotic 3D surface processing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PanopticRecon: Leverage open-vocabulary instance
segmentation for zero-shot panoptic reconstruction. <em>IROS</em>,
12947–12954. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Panoptic reconstruction is a challenging task in 3D scene understanding. However, most existing methods heavily rely on pre-trained semantic segmentation models and known 3D object bounding boxes for 3D panoptic segmentation, which is not available for in-the-wild scenes. In this paper, we propose a novel zero-shot panoptic reconstruction method from RGB-D images of scenes. For zero-shot segmentation, we leverage open-vocabulary instance segmentation, but it has to face partial labeling and instance association challenges. We tackle both challenges by propagating partial labels with the aid of dense generalized features and building a 3D instance graph for associating 2D instance IDs. Specifically, we exploit partial labels to learn a classifier for generalized semantic features to provide complete labels for scenes with dense distilled features. Moreover, we formulate instance association as a 3D instance graph segmentation problem, allowing us to fully utilize the scene geometry prior and all 2D instance masks to infer global unique pseudo 3D instance ID. Our method outperforms state-of-the-art methods on the indoor dataset ScanNet V2 and the outdoor dataset KITTI-360, demonstrating the effectiveness of our graph segmentation method and reconstruction network.},
  archive   = {C_IROS},
  author    = {Xuan Yu and Yili Liu and Chenrui Han and Sitong Mao and Shunbo Zhou and Rong Xiong and Yiyi Liao and Yue Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801961},
  month     = {10},
  pages     = {12947-12954},
  title     = {PanopticRecon: Leverage open-vocabulary instance segmentation for zero-shot panoptic reconstruction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-modal NeRF self-supervision for LiDAR semantic
segmentation. <em>IROS</em>, 12939–12946. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {LiDAR Semantic Segmentation is a fundamental task in autonomous driving perception consisting of associating each LiDAR point to a semantic label. Fully-supervised models have widely tackled this task, but they require labels for each scan, which either limits their domain or requires impractical amounts of expensive annotations.Camera images, which are generally recorded alongside LiDAR pointclouds, can be processed by the widely available 2D foundation models, which are generic and dataset-agnostic. However, distilling knowledge from 2D data to improve LiDAR perception raises domain adaptation challenges. For example, the classical perspective projection suffers from the parallax effect produced by the position shift between both sensors at their respective capture times.We propose a Semi-Supervised Learning setup to leverage unlabeled LiDAR pointclouds alongside distilled knowledge from the camera images. To self-supervise our model on the unlabeled scans, we add an auxiliary NeRF head and cast rays from the camera viewpoint over the unlabeled voxel features. The NeRF head predicts densities and semantic logits at each sampled ray location which are used for rendering pixel semantics. Concurrently, we query the Segment-Anything (SAM) foundation model with the camera image to generate a set of unlabeled generic masks. We fuse the masks with the rendered pixel semantics from LiDAR to produce pseudo-labels that supervise the pixel predictions. During inference, we drop the NeRF head and run our model with only LiDAR.We show the effectiveness of our approach in three public LiDAR Semantic Segmentation benchmarks: nuScenes, SemanticKITTI and ScribbleKITTI.},
  archive   = {C_IROS},
  author    = {Xavier Timoneda and Markus Herb and Fabian Duerr and Daniel Goehring and Fisher Yu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802453},
  month     = {10},
  pages     = {12939-12946},
  title     = {Multi-modal NeRF self-supervision for LiDAR semantic segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Answerability fields: Answerable location estimation via
diffusion models. <em>IROS</em>, 12932–12938. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose Answerability Fields (AnsFields), a novel approach for predicting the answerability of questions at different locations within indoor environments. AnsFields is represented as a map, where each grid’s score reflects how well a question can be answered using the panoramic image at that location. Using a 3D question-answering dataset, we construct comprehensive AnsFields covering diverse scenes from ScanNet. Additionally, we employ a diffusion model to infer AnsFields from a scene’s top-down view image and the question. We then conduct 3D question-answering using these predicted AnsFields and achieve a 24% improvement in accuracy over the standard 3D-QA method. Our results demonstrate the importance of object locations for answering questions in the environment, highlighting the potential of AnsFields for applications in robotics, augmented reality, and human-robot interaction.},
  archive   = {C_IROS},
  author    = {Daichi Azuma and Taiki Miyanishi and Shuhei Kurita and Koya Sakamoto and Motoaki Kawanabe},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802662},
  month     = {10},
  pages     = {12932-12938},
  title     = {Answerability fields: Answerable location estimation via diffusion models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Volumetric semantically consistent 3D panoptic mapping.
<em>IROS</em>, 12924–12931. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce an online 2D-to-3D semantic instance mapping algorithm aimed at generating comprehensive, accurate, and efficient semantic 3D maps suitable for autonomous agents in unstructured environments. The proposed approach is based on a Voxel-TSDF representation used in recent algorithms. It introduces novel ways of integrating semantic prediction confidence during mapping, producing semantic and instance-consistent 3D regions. Further improvements are achieved by graph optimization-based semantic labeling and instance refinement. The proposed method achieves accuracy superior to the state of the art on public large-scale datasets, improving on a number of widely used metrics. We also highlight a downfall in the evaluation of recent studies: using the ground truth trajectory as input instead of a SLAM-estimated one substantially affects the accuracy, creating a large gap between the reported results and the actual performance on real-world data. The code is available: https://github.com/y9miao/ConsistentPanopticSLAM.},
  archive   = {C_IROS},
  author    = {Yang Miao and Iro Armeni and Marc Pollefeys and Daniel Barath},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802241},
  month     = {10},
  pages     = {12924-12931},
  title     = {Volumetric semantically consistent 3D panoptic mapping},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling and gait analysis of passive rimless wheel with
compliant feet. <em>IROS</em>, 12918–12923. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The movement of the legs involves the interaction between the feet and the ground. Consequently, most animals possess a wide variety of foot morphologies and multifunctional capabilities. The selection and switching of these foot functions are passive and environment-dependent, ensuring environmental compliance. Despite this, current research on compliant feet lacks mathematical models that simultaneously encompass locomotion and foot compliance. Therefore, conducting in-depth studies on locomotion properties under current conditions is challenging. In this study, we present novel passive compliant feet applicable to the passive walking of a rimless wheel. We first introduce a dynamic model, achieve passive walking through numerical simulations, and subsequently analyze the gait patterns for compliance and multi-period gait. This study bridges a gap in understanding the interaction between motion and compliance in foot design, providing insights into the dynamics of compliant motion.},
  archive   = {C_IROS},
  author    = {Yanqiu Zheng and Cong Yan and Yuetong He and Fumihiko Asano and Isao T. Tokuda},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802468},
  month     = {10},
  pages     = {12918-12923},
  title     = {Modeling and gait analysis of passive rimless wheel with compliant feet},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DexDribbler: Learning dexterous soccer manipulation via
dynamic supervision. <em>IROS</em>, 12910–12917. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning dexterous locomotion policy for legged robots is becoming increasingly popular due to its ability to handle diverse terrains and resemble intelligent behaviors. However, joint manipulation of moving objects and locomotion with legs, such as playing soccer, receive scant attention in the learning community, although it is natural for humans and smart animals. A key challenge to solve this multitask problem is to infer the objectives of locomotion from the states and targets of the manipulated objects. The implicit relation between the object states and robot locomotion can be hard to capture directly from the training experience. We propose adding a feedback control block to compute the necessary body- level movement accurately and using the outputs as dynamic joint-level locomotion supervision explicitly. We further utilize an improved ball dynamic model, an extended context-aided estimator, and a comprehensive ball observer to facilitate transferring policy learned in simulation to the real world. We observe that our learning scheme can not only make the policy network converge faster but also enable soccer robots to perform sophisticated maneuvers like sharp cuts and turns on flat surfaces, a capability that was lacking in previous methods. Video and code are available at github.com/SysCV/soccer-player.},
  archive   = {C_IROS},
  author    = {Yutong Hu and Kehan Wen and Fisher Yu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802022},
  month     = {10},
  pages     = {12910-12917},
  title     = {DexDribbler: Learning dexterous soccer manipulation via dynamic supervision},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LEEPS: Learning end-to-end legged perceptive parkour skills
on challenging terrains. <em>IROS</em>, 12904–12909. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Empowering legged robots with agile maneuvers is a great challenge. While existing works have proposed diverse control-based and learning-based methods, it remains an open problem to endow robots with animal-like perception and athleticism. Towards this goal, we develop an End-to-End Legged Perceptive Parkour Skill Learning (LEEPS) framework to train quadruped robots to master parkour skills in complex environments. In particular, LEEPS incorporates a vision-based perception module equipped with multi-layered scans, supplying robots with comprehensive, precise, and adaptable information about their surroundings. Leveraging such visual data, a position-based task formulation liberates the robot from velocity constraints and directs it toward the target using innovative reward mechanisms. The resulting controller empowers an affordable quadruped robot to successfully traverse previously challenging and unprecedented obstacles. We evaluate LEEPS on various challenging tasks, which demonstrate its effectiveness, robustness, and generalizability. Supplementary and videos are available at: https://sites.google.com/view/leeps},
  archive   = {C_IROS},
  author    = {Tangyu Qian and Hao Zhang and Zhangli Zhou and Hao Wang and Mingyu Cai and Zhen Kan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801925},
  month     = {10},
  pages     = {12904-12909},
  title     = {LEEPS: Learning end-to-end legged perceptive parkour skills on challenging terrains},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MonoForce: Self-supervised learning of physics-informed
model for predicting robot-terrain interaction. <em>IROS</em>,
12896–12903. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While autonomous navigation of mobile robots on rigid terrain is a well-explored problem, navigating on deformable terrain such as tall grass or bushes remains a challenge. To address it, we introduce an explainable, physics-aware and end-to-end differentiable model which predicts the outcome of robot-terrain interaction from camera images, both on rigid and non-rigid terrain. The proposed MonoForce model consists of a black-box module which predicts robot-terrain interaction forces from onboard cameras, followed by a white-box module, which transforms these forces and a control signals into predicted trajectories, using only the laws of classical mechanics. The differentiable white-box module allows backpropagating the predicted trajectory errors into the black-box module, serving as a self-supervised loss that measures consistency between the predicted forces and ground-truth trajectories of the robot. Experimental evaluation on a public dataset and our data has shown that while the prediction capabilities are comparable to state-of-the-art algorithms on rigid terrain, MonoForce shows superior accuracy on nonrigid terrain such as tall grass or bushes. To facilitate the reproducibility of our results, we release both the code and datasets.},
  archive   = {C_IROS},
  author    = {Ruslan Agishev and Karel Zimmermann and Vladimír Kubelka and Martin Pecka and Tomáš Svoboda},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801353},
  month     = {10},
  pages     = {12896-12903},
  title     = {MonoForce: Self-supervised learning of physics-informed model for predicting robot-terrain interaction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Text3DAug - prompted instance augmentation for LiDAR
perception. <em>IROS</em>, 12888–12895. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {LiDAR data of urban scenarios poses unique challenges, such as heterogeneous characteristics and inherent class imbalance. Therefore, large-scale datasets are necessary to apply deep learning methods. Instance augmentation has emerged as an efficient method to increase dataset diversity. However, current methods require the time-consuming curation of 3D models or costly manual data annotation. To overcome these limitations, we propose Text3DAug, a novel approach leveraging generative models for instance augmentation. Text3DAug does not depend on labeled data and is the first of its kind to generate instances and annotations from text. This allows for a fully automated pipeline, eliminating the need for manual effort in practical applications. Additionally, Text3DAug is sensor agnostic and can be applied regardless of the LiDAR sensor used. Comprehensive experimental analysis on LiDAR segmentation, detection and novel class discovery demonstrates that Text3DAug is effective in supplementing existing methods or as a standalone method, performing on par or better than established methods, however while overcoming their specific drawbacks. The code is publicly available.1},
  archive   = {C_IROS},
  author    = {Laurenz Reichardt and Luca Uhr and Oliver Wasenmüller},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802726},
  month     = {10},
  pages     = {12888-12895},
  title     = {Text3DAug - prompted instance augmentation for LiDAR perception},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PathFinder: Attention-driven dynamic non-line-of-sight
tracking with a mobile robot. <em>IROS</em>, 12880–12887. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The study of non-line-of-sight (NLOS) imaging is growing due to its many potential applications, including rescue operations and pedestrian detection by self-driving cars. However, implementing NLOS imaging on a moving camera remains an open area of research. Existing NLOS imaging methods rely on time-resolved detectors and laser configurations that require precise optical alignment, making it difficult to deploy them in dynamic environments. This work proposes a data-driven approach to NLOS imaging, PathFinder, that can be used with a standard RGB camera mounted on a small, power-constrained mobile robot, such as an aerial drone. Our experimental pipeline is designed to accurately estimate the 2D trajectory of a person who moves in a Manhattan-world environment while remaining hidden from the camera’s field-of-view. We introduce a novel approach to process a sequence of dynamic successive frames in a line-of-sight (LOS) video using an attention-based neural network that performs inference in real-time. The method also includes a preprocessing selection metric that analyzes images from a moving camera which contain multiple vertical planar surfaces, such as walls and building facades, and extracts planes that return maximum NLOS information. We validate the approach on in-the-wild scenes using a drone for video capture, thus demonstrating low-cost NLOS imaging in dynamic capture environments. The real-world dataset that we collected and used to train the network can be found at https://srchandr.github.io/DynamicNLOS/.},
  archive   = {C_IROS},
  author    = {Shenbagaraj Kannapiran and Sreenithy Chandran and Suren Jayasuriya and Spring Berman},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801990},
  month     = {10},
  pages     = {12880-12887},
  title     = {PathFinder: Attention-driven dynamic non-line-of-sight tracking with a mobile robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NeuralFloors++: Consistent street-level scene generation
from BEV semantic maps. <em>IROS</em>, 12872–12879. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning autonomous driving capabilities requires diverse and realistic training data. This has led to exploring generative techniques as an alternative to real-world data collection. In this paper we propose a method for synthesising photo-realistic urban driving scenes, along with semantic, instance and depth ground-truth. Our model relies on Bird’s Eye View (BEV) representations due to their compositionality and scene content control capabilities, reducing the need for traditional simulators. We employ a two-stage process: first, a 3D scene representation is extracted from BEV semantic, instance and style maps using a neural field. After rendering the semantic, instance, depth and style maps from a ground-view perspective, a second stage based on a diffusion model is used to generate the photo-realistic scene. We extend our prior work - NeuralFloors, to include multiple-view outputs, style manipulation for finer control at the object level through instance-wise style maps and cross-frame consistency via auto-regressive training. The proposed system is evaluated extensively on the KITTI-360 dataset, showing improved realism and semantic alignment for generated images.},
  archive   = {C_IROS},
  author    = {Valentina Muşat and Daniele De Martini and Matthew Gadd and Paul Newman},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802002},
  month     = {10},
  pages     = {12872-12879},
  title     = {NeuralFloors++: Consistent street-level scene generation from BEV semantic maps},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DarkGS: Learning neural illumination and 3D gaussians
relighting for robotic exploration in the dark. <em>IROS</em>,
12864–12871. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans have the remarkable ability to construct consistent mental models of an environment, even under limited or varying levels of illumination. We wish to endow robots with this same capability. In this paper, we tackle the challenge of constructing a photorealistic scene representation under poorly illuminated conditions and with a moving light source. We approach the task of modeling illumination as a learning problem, and utilize the developed illumination model to aid in scene reconstruction. We introduce an innovative framework that uses a data-driven approach, Neural Light Simulators (NeLiS), to model and calibrate the camera-light system. Furthermore, we present DarkGS, a method that applies NeLiS to create a relightable 3D Gaussian scene model capable of real-time, photorealistic rendering from novel viewpoints. We show the applicability and robustness of our proposed simulator and system in a variety of real-world environments. Code released at https://tyz1030.github.io/proj/darkgs.html},
  archive   = {C_IROS},
  author    = {Tianyi Zhang and Kaining Huang and Weiming Zhi and Matthew Johnson-Roberson},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802684},
  month     = {10},
  pages     = {12864-12871},
  title     = {DarkGS: Learning neural illumination and 3D gaussians relighting for robotic exploration in the dark},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Probabilistic homotopy optimization for dynamic motion
planning. <em>IROS</em>, 12856–12863. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a homotopic approach to solving challenging, optimization-based motion planning problems. The approach uses Homotopy Optimization, which, unlike standard continuation methods for solving homotopy problems, solves a sequence of constrained optimization problems rather than a sequence of nonlinear systems of equations. The insight behind our proposed algorithm is formulating the discovery of this sequence of optimization problems as a search problem in a multidimensional homotopy parameter space. Our proposed algorithm, the Probabilistic Homotopy Optimization algorithm, switches between solve and sample phases, using solutions to easy problems as initial guesses to more challenging problems. We analyze how our algorithm performs in the presence of common challenges to homotopy methods, such as bifurcation, folding, and disconnectedness of the homotopy solution manifold. Finally, we demonstrate its utility via a case study on two dynamic motion planning problems. the cart-pole and the MIT Humanoid.},
  archive   = {C_IROS},
  author    = {Shayan Pardis and Matthew Chignoli and Sangbae Kim},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802528},
  month     = {10},
  pages     = {12856-12863},
  title     = {Probabilistic homotopy optimization for dynamic motion planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pose graph optimization over planar unit dual quaternions:
Improved accuracy with provably convergent riemannian optimization.
<em>IROS</em>, 12848–12855. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It is common in pose graph optimization (PGO) algorithms to assume that noise in the translations and rotations of relative pose measurements is uncorrelated. However, existing work shows that in practice these measurements can be highly correlated, which leads to degradation in the accuracy of PGO solutions that rely on this assumption. Therefore, in this paper we develop a novel algorithm derived from a realistic, correlated model of relative pose uncertainty, and we quantify the resulting improvement in the accuracy of the solutions we obtain relative to state-of-the-art PGO algorithms. Our approach utilizes Riemannian optimization on the planar unit dual quaternion (PUDQ) manifold, and we prove that it converges to first-order stationary points of a Lie-theoretic maximum likelihood objective. Then we show experimentally that, compared to state-of-the-art PGO algorithms, this algorithm produces estimation errors that are lower by 10% to 25% across several orders of magnitude of correlated noise levels and graph sizes.},
  archive   = {C_IROS},
  author    = {William D. Warke and J. Humberto Ramos and Prashant Ganesh and Kevin M. Brink and Matthew T. Hale},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802328},
  month     = {10},
  pages     = {12848-12855},
  title     = {Pose graph optimization over planar unit dual quaternions: Improved accuracy with provably convergent riemannian optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Perfecting periodic trajectory tracking: Model predictive
control with a periodic observer (π-MPC). <em>IROS</em>, 12840–12847.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In Model Predictive Control (MPC), discrepancies between the actual system and the predictive model can lead to substantial tracking errors and significantly degrade performance and reliability. While such discrepancies can be alleviated with more complex models, this often complicates controller design and implementation. By leveraging the fact that many trajectories of interest are periodic, we show that perfect tracking is possible when incorporating a simple observer that estimates and compensates for periodic disturbances. We present the design of the observer and the accompanying tracking MPC scheme, proving that their combination achieves zero tracking error asymptotically, regardless of the complexity of the unmodelled dynamics. We validate the effectiveness of our method, demonstrating asymptotically perfect tracking on a high-dimensional soft robot with nearly 10,000 states and a fivefold reduction in tracking errors compared to a baseline MPC on small-scale autonomous race car experiments.},
  archive   = {C_IROS},
  author    = {Luis Pabon and Johannes Köhler and John Irvin Alora and Patrick Benito Eberhard and Andrea Carron and Melanie N. Zeilinger and Marco Pavone},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801564},
  month     = {10},
  pages     = {12840-12847},
  title     = {Perfecting periodic trajectory tracking: Model predictive control with a periodic observer (Π-MPC)},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Centroidal state estimation based on the koopman embedding
for dynamic legged locomotion. <em>IROS</em>, 12832–12839. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce a novel approach to centroidal state estimation, which plays a crucial role in predictive model-based control strategies for dynamic legged locomotion. Our approach uses the Koopman operator theory to transform the robot’s complex nonlinear dynamics into a linear system, by employing dynamic mode decomposition and deep learning for model construction. We evaluate both models on their linearization accuracy and capability to capture both fast and slow dynamic system responses. We then select the most suitable model for estimation purposes, and integrate it within a moving horizon estimator. This estimator is formulated as a convex quadratic program to facilitate robust, real-time centroidal state estimation. Through extensive simulation experiments on a quadruped robot executing various dynamic gaits, our data-driven framework outperforms conventional Extended Kalman Filtering technique based on nonlinear dynamics. Our estimator addresses challenges posed by force/torque measurement noise in highly dynamic motions and accurately recovers the centroidal states, demonstrating the adaptability and effectiveness of the Koopman-based linear representation for complex locomotive behaviors. Importantly, our model based on dynamic mode decomposition, trained with two locomotion patterns (trot and jump), successfully estimates the centroidal states for a different motion (bound) without retraining.},
  archive   = {C_IROS},
  author    = {Shahram Khorshidi and Murad Dawood and Maren Bennewitz},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801750},
  month     = {10},
  pages     = {12832-12839},
  title     = {Centroidal state estimation based on the koopman embedding for dynamic legged locomotion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluation and deployment of LiDAR-based place recognition
in dense forests. <em>IROS</em>, 12824–12831. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many LiDAR place recognition systems have been developed and tested specifically for urban driving scenarios. Their performance in natural environments such as forests and woodlands have been studied less closely. In this paper, we analyzed the capabilities of four different LiDAR place recognition systems, both handcrafted and learning-based methods, using LiDAR data collected with a handheld device and legged robot within dense forest environments. In particular, we focused on evaluating localization where there is significant translational and orientation difference between corresponding LiDAR scan pairs. This is particularly important for forest survey systems where the sensor or robot does not follow a defined road or path. Extending our analysis we then incorporated the best performing approach, Logg3dNet, into a full 6-DoF pose estimation system—introducing several verification layers for precise registration. We demonstrated the performance of our methods in three operational modes: online SLAM, offline multi-mission SLAM map merging, and relocalization into a prior map. We evaluated these modes using data captured in forests from three different countries, achieving 80 % of correct loop closures candidates with baseline distances up to 5 m, and 60 % up to 10 m. Video at: https://youtu.be/86l-oxjwmjY},
  archive   = {C_IROS},
  author    = {Haedam Oh and Nived Chebrolu and Matias Mattamala and Leonard Freißmuth and Maurice Fallon},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801297},
  month     = {10},
  pages     = {12824-12831},
  title     = {Evaluation and deployment of LiDAR-based place recognition in dense forests},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Refractive COLMAP: Refractive structure-from-motion
revisited. <em>IROS</em>, 12816–12823. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a complete refractive Structure-from-Motion (RSfM) framework for underwater 3D reconstruction using refractive camera setups (for both, flat-and dome-port underwater housings). Despite notable achievements in refractive multi-view geometry over the past decade, a robust, complete and publicly available solution for such tasks is not available at present, and often practical applications have to resort to approximating refraction effects by the intrinsic (distortion) parameters of a pinhole camera model. To fill this gap, we have integrated refraction considerations throughout the entire SfM process within the state-of-the-art, open-source SfM framework COLMAP. Numerical simulations and reconstruction results on synthetically generated but photo-realistic images with ground truth validate that enabling refraction does not compromise accuracy or robustness as compared to in-air reconstructions. Finally, we demonstrate the capability of our approach for large-scale refractive scenarios using a dataset consisting of nearly 6000 images. The implementation is released as open-source at: https://cau-git.rz.uni-kiel.de/inf-ag-koeser/colmap_underwater.},
  archive   = {C_IROS},
  author    = {Mengkun She and Felix Seegräber and David Nakath and Kevin Köser},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802043},
  month     = {10},
  pages     = {12816-12823},
  title     = {Refractive COLMAP: Refractive structure-from-motion revisited},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging GNSS and onboard visual data from consumer
vehicles for robust road network estimation. <em>IROS</em>, 12808–12815.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Maps are essential for diverse applications, such as vehicle navigation and autonomous robotics. Both require spatial models for effective route planning and localization. This paper addresses the challenge of road graph construction for autonomous vehicles. Despite recent advances, creating a road graph remains labor-intensive and has yet to achieve full automation. The goal of this paper is to generate such graphs automatically and accurately. Modern cars are equipped with onboard sensors used for today&#39;s advanced driver assistance systems like lane keeping. We propose using global navigation satellite system (GNSS) traces and basic image data acquired from these standard sensors in consumer vehicles to estimate road-level maps with minimal effort. We exploit the spatial information in the data by framing the problem as a road centerline semantic segmentation task using a convolutional neural network. We also utilize the data’s time series nature to refine the neural network’s output by using map matching. We implemented and evaluated our method using a fleet of real consumer vehicles, only using the deployed onboard sensors. Our evaluation demonstrates that our approach not only matches existing methods on simpler road configurations but also significantly outperforms them on more complex road geometries and topologies. This work received the 2023 Woven by Toyota Invention Award.},
  archive   = {C_IROS},
  author    = {Balázs Opra and Betty Le Dem and Jeffrey M. Walls and Dimitar Lukarski and Cyrill Stachniss},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802800},
  month     = {10},
  pages     = {12808-12815},
  title     = {Leveraging GNSS and onboard visual data from consumer vehicles for robust road network estimation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian floor field: Transferring people flow predictions
across environments. <em>IROS</em>, 12801–12807. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mapping people dynamics is a crucial skill for robots, because it enables them to coexist in human-inhabited environments. However, learning a model of people dynamics is a time consuming process which requires observation of large amount of people moving in an environment. Moreover, approaches for mapping dynamics are unable to transfer the learned models across environments: each model is only able to describe the dynamics of the environment it has been built in. However, the impact of architectural geometry on people’s movement can be used to anticipate their patterns of dynamics, and recent work has looked into learning maps of dynamics from occupancy. So far however, approaches based on trajectories and those based on geometry have not been combined. In this work we propose a novel Bayesian approach to learn people dynamics able to combine knowledge about the environment geometry with observations from human trajectories. An occupancy-based deep prior is used to build an initial transition model without requiring any observations of pedestrian; the model is then updated when observations become available using Bayesian inference. We demonstrate the ability of our model to increase data efficiency and to generalize across real large-scale environments, which is unprecedented for maps of dynamics.},
  archive   = {C_IROS},
  author    = {Francesco Verdoja and Tomasz Piotr Kucner and Ville Kyrki},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802300},
  month     = {10},
  pages     = {12801-12807},
  title     = {Bayesian floor field: Transferring people flow predictions across environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online adaptive impedance control with gravity compensation
for an interactive lower-limb exoskeleton. <em>IROS</em>, 12793–12800.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While lower-limb exoskeletons have been increasingly used for gait assistance and rehabilitation, most of them continue to function as assistive devices in the exoskeleton-user relationship as a leader and follower. This limits the user’s ability to interactively contribute to gait control. Therefore, this study proposes an interactive user-exoskeleton control strategy to translate the exoskeletons into interactive compliant companion devices with the exoskeleton-user relationship as the collaborator. This strategy is implemented through online adaptive impedance control with gravity compensation (OAIC-GC). It relies solely on internal pose feedback (joint position) rather than external sensors such as electromyography, torque, or force, as utilized in other assist-as-needed (AAN) control methods. The OAIC-GC can automatically capture the mechanical impedance dynamics of the user’s lower limbs during walking and thus facilitate adaptive, versatile, and personalized gait assistance. It is evaluated using a real lower-limb exoskeleton system with six degrees of freedom (DOFs) across different users engaging in various activities. These activities include symmetrical and asymmetrical walking on a split-belt treadmill at different speeds, as well as walking up stairs. The results indicate a significant improvement in the exoskeleton’s performance in terms of adaptability and movement smoothness under all activities when compared to traditional control. The proposed control reduces joint assistance torque across all exoskeleton joints, enhancing user interaction and comfort. This enables users to actively control their gait patterns, enabling the exoskeleton to operate in an interactive assist-as-needed (IAAN) mode.},
  archive   = {C_IROS},
  author    = {Run Janna and Kanut Tarapongnivat and Natchaya Sricom and Chaicharn Akkawutvanich and Xiaofeng Xiong and Poramate Manoonpong},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802289},
  month     = {10},
  pages     = {12793-12800},
  title     = {Online adaptive impedance control with gravity compensation for an interactive lower-limb exoskeleton},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards design and development of a soft pressure sensing
sleeve for performing safe colonoscopic procedures. <em>IROS</em>,
12787–12792. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, with the goal of enhancing the safety of current colonoscopic procedures and providing the pressure and location of the contact between the colonoscope and the colon’s surface, we propose design and development of a unique Soft Pressure Sensing Sleeve (SPSS). SPSS can seamlessly be integrated with the existing colonoscopic devices and would not change the existing diagnosis workflow. The pressure sensing of SPSS is performed based on the resistance change of a liquid metal (i.e., Gallium) embedded into several micro-channels located within SPSS’s deformable sleeve when it interacts with the colon surface. To demonstrate functionality of the SPSS, without loss of generality, in this paper, we designed and fabricated a SPSS with 4 sensing regions. We also proposed and experimentally evaluated an empirical calibration function for this sensor. Results demonstrate high accuracy (RMSE=2.45 and mean absolute error &lt;3%) of the proposed calibration function compared with the evaluation experiments.},
  archive   = {C_IROS},
  author    = {Mohammad Rafiee Javazm and Sonika Kiehler and Ozdemir Can Kara and Farshid Alambeigi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802828},
  month     = {10},
  pages     = {12787-12792},
  title     = {Towards design and development of a soft pressure sensing sleeve for performing safe colonoscopic procedures},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SuPerPM: A surgical perception framework based on deep point
matching learned from physical constrained simulation data.
<em>IROS</em>, 12780–12786. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A major source of endoscopic tissue tracking errors during deformations stems from wrong data association between observed sensor measurements with previously tracked scene. To mitigate this issue, we present a surgical perception framework, SuPerPM, that leverages learning-based non-rigid point cloud matching for data association, thus accommodating larger deformations than previous approaches which relied on Iterative Closest Point (ICP) for point associations. The learning models typically require training data with ground truth point cloud correspondences, which is challenging or even impractical to collect in surgical environments. Thus, for tuning the learning model, we gather endoscopic data of soft tissue being manipulated by a surgical robot and then establish correspondences between point clouds at different time points to serve as ground truth. This was achieved by employing a position-based dynamics (PBD) simulation to ensure that the correspondences adhered to physical constraints. The proposed framework is demonstrated on several challenging surgical datasets that are characterized by large deformations, achieving superior performance over advanced surgical scene tracking algorithms. 1},
  archive   = {C_IROS},
  author    = {Shan Lin and Albert J. Miao and Ali Alabiad and Fei Liu and Kaiyuan Wang and Jingpei Lu and Florian Richter and Michael C. Yip},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802079},
  month     = {10},
  pages     = {12780-12786},
  title     = {SuPerPM: A surgical perception framework based on deep point matching learned from physical constrained simulation data},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Single protoplasts pickup system combining brightfield and
confocal images. <em>IROS</em>, 12774–12779. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a system that picks up protoplasts produced by removing the surrounding cell wall of root cells while preserving their positional information. The fundamental concept of this system involves scanning the root tip over time using a confocal microscopy to measure the positional information of each cell. Then, the protoplast pickup is conducted after switching to a brightfield microscopy to ensure the certainty of pickup. The system measures the position of single protoplasts, adjusts the position of the pipette using a 3-axis micromanipulator, and picks up the target protoplast using a microfluidic pump driven by a piezoelectric actuator. To automate this pickup process, we achieved calibration of the system. The fully automatic 3D calibration of the pipette tip was achieved, allowing 3D micromanipulation under the microscope with an accuracy of 3.1 μm in the XY-plane. Furthermore, by implementing multiple functions such as automatic detection of protoplasts, the process of protoplast pickup has been achieved.},
  archive   = {C_IROS},
  author    = {Daito Ando and Bilal Turan and Satoshi Amaya and Yuko Ukai and Yoshikatsu Sato and Fumihito Arai},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801320},
  month     = {10},
  pages     = {12774-12779},
  title     = {Single protoplasts pickup system combining brightfield and confocal images},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning long-horizon predictions for quadrotor dynamics.
<em>IROS</em>, 12758–12765. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate modeling of system dynamics is crucial for achieving high-performance planning and control of robotic systems. Although existing data-driven approaches represent a promising approach for modeling dynamics, their accuracy is limited to a short prediction horizon, overlooking the impact of compounding prediction errors over longer prediction horizons. Strategies to mitigate these cumulative errors remain underexplored. To bridge this gap, in this paper, we study the key design choices for efficiently learning long-horizon prediction dynamics for quadrotors. Specifically, we analyze the impact of multiple architectures, historical data, and multi-step loss formulation. We show that sequential modeling techniques showcase their advantage in minimizing compounding errors compared to other types of solutions. Furthermore, we propose a novel decoupled dynamics learning approach, which further simplifies the learning process while also enhancing the approach modularity. Extensive experiments and ablation studies on real-world quadrotor data demonstrate the versatility and precision of the proposed approach. Our outcomes offer several insights and methodologies for enhancing long-term predictive accuracy of learned quadrotor dynamics for planning and control.},
  archive   = {C_IROS},
  author    = {Pratyaksh Prabhav Rao and Alessandro Saviolo and Tommaso Castiglione Ferrari and Giuseppe Loianno},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801793},
  month     = {10},
  pages     = {12758-12765},
  title     = {Learning long-horizon predictions for quadrotor dynamics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kinodynamic motion planning for a team of multirotors
transporting a cable-suspended payload in cluttered environments.
<em>IROS</em>, 12750–12757. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a motion planner for cable-driven payload transportation using multiple unmanned aerial vehicles (UAVs) in an environment cluttered with obstacles. Our planner is kinodynamic, i.e., it considers the full dynamics model of the transporting system including actuation constraints. Due to the high dimensionality of the planning problem, we use a hierarchical approach where we first solve for the geometric motion using a sampling-based method with a novel sampler, followed by constrained trajectory optimization that considers the full dynamics of the system. Both planning stages consider inter-robot and robot/obstacle collisions. We demonstrate in a software-in-the-loop simulation and real flight experiments that there is a significant benefit in kinodynamic motion planning for such payload transport systems with respect to payload tracking error and energy consumption compared to the standard methods of planning for the payload alone. Notably, we observe a significantly higher success rate in scenarios where the team formation changes are needed to move through tight spaces.},
  archive   = {C_IROS},
  author    = {Khaled Wahba and Joaquim Ortiz-Haro and Marc Toussaint and Wolfgang Hönig},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802794},
  month     = {10},
  pages     = {12750-12757},
  title     = {Kinodynamic motion planning for a team of multirotors transporting a cable-suspended payload in cluttered environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Det-recon-reg: An intelligent framework towards automated
large-scale infrastructure inspection. <em>IROS</em>, 12742–12749. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual inspection plays a predominant role in inspecting infrastructure surface. However, the generalization of existing visual inspection systems to large-scale real-world scenes remains challenging. In this paper, we introduce Det-Recon-Reg, an intelligent framework separating the complex inspection procedure into three stages: Detect, Reconstruct, and Register. (1) For defect detection (Detect), we present the first high-resolution defect dataset tailored for large-scale defect detection. Based on the dataset, we evaluate the most effective real-time object detection algorithms and push the boundary by proposing CUBIT-Net for real-world defect inspection. (2) For infrastructure reconstruction (Reconstruct), we propose a learning-based multi-view stereo (MVS) network to adapt to large-scale scenes, taking as input the multi-view images and outputting the point cloud reconstruction, where its performance has been validated on the standard MVS datasets, including BlendedMVS, DTU, and Tanks and Temples datasets. (3) For defect localization (Register), we propose an effective registration method based on the geographic information system that registers the detected defects onto the reconstructed infrastructure model to establish a global reference for maintenance measures. The real-world experiments further verify the effectiveness and efficiency of our proposed framework. More details about our proposed dataset, code, and appendix are available on our project page: https://cuhk-usr-group.github.io/large-scale-inspect-framework/.},
  archive   = {C_IROS},
  author    = {Guidong Yang and Jihan Zhang and Benyun Zhao and Chuanxiang Gao and Yijun Huang and Junjie Wen and Qingxiang Li and Jerry Tang and Xi Chen and Ben M. Chen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802463},
  month     = {10},
  pages     = {12742-12749},
  title     = {Det-recon-reg: An intelligent framework towards automated large-scale infrastructure inspection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accelerating model predictive control for legged robots
through distributed optimization. <em>IROS</em>, 12734–12741. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel approach to enhance Model Predictive Control (MPC) for legged robots through Distributed Optimization. Our method focuses on decomposing the robot dynamics into smaller, parallelizable subsystems, and utilizing the Alternating Direction Method of Multipliers (ADMM) to ensure consensus among them. Each subsystem is managed by its own Optimal Control Problem, with ADMM facilitating consistency between their optimizations. This approach not only decreases the computational time but also allows for effective scaling with more complex robot configurations, facilitating the integration of additional subsystems such as articulated arms on a quadruped robot. We demonstrate, through numerical evaluations, the convergence of our approach on two systems with increasing complexity. In addition, we showcase that our approach converges towards the same solution when compared to a state-of-the-art centralized whole-body MPC implementation. Moreover, we quantitatively compare the computational efficiency of our method to the centralized approach, revealing up to a 75% reduction in computational time. Overall, our approach offers a promising avenue for accelerating MPC solutions for legged robots, paving the way for more effective utilization of the computational performance of modern hardware. Accompanying video at https://youtu.be/Yar4W-Vlh2A. The related code can be found at https://github.com/iit-DLSLab/DWMPC},
  archive   = {C_IROS},
  author    = {Lorenzo Amatucci and Giulio Turrisi and Angelo Bratta and Victor Barasuol and Claudio Semini},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801676},
  month     = {10},
  pages     = {12734-12741},
  title     = {Accelerating model predictive control for legged robots through distributed optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive stochastic nonlinear model predictive control with
look-ahead deep reinforcement learning for autonomous vehicle motion
control. <em>IROS</em>, 12726–12733. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Propagating uncertainties through nonlinear system dynamics in the context of Stochastic Nonlinear Model Predictive Control (SNMPC) is challenging, especially for high-dimensional systems requiring real-time control and operating under time-variant uncertainties such as autonomous vehicles. In this work, we propose an Adaptive SNMPC (aSNMPC) driven by Deep Reinforcement Learning (DRL) to optimize uncertainty handling, constraints robustification, feasibility, and closed-loop performance. To this end, our SNMPC uses Polynomial Chaos Expansion (PCE) for efficient uncertainty propagation, limits its propagation time through an Uncertainty Propagation Horizon (UPH), and transforms nonlinear chance constraints into robustified deterministic ones. We conceive a DRL agent to proactively anticipate upcoming control tasks and to dynamically reduce conservatism by determining the most suitable constraints robustification factor κ, and to enhance feasibility by choosing optimal UPH length Tu. We analyze the trained DRL agent’s decision-making process and highlight its ability to learn context-dependent optimal parameters. We showcase the enhanced robustness and feasibility of our DRL-driven aSNMPC through the real-time motion control task of an autonomous passenger vehicle when confronted with significant time-variant disturbances while achieving a minimum solution frequency of 110Hz. The code used in this research is publicly accessible as open-source software: https://github.com/bzarr/TUM-CONTROL},
  archive   = {C_IROS},
  author    = {Baha Zarrouki and Chenyang Wang and Johannes Betz},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801876},
  month     = {10},
  pages     = {12726-12733},
  title     = {Adaptive stochastic nonlinear model predictive control with look-ahead deep reinforcement learning for autonomous vehicle motion control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feasibility-guided safety-aware model predictive control for
jump markov linear systems. <em>IROS</em>, 12718–12725. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a controller framework that synthesizes control policies for Jump Markov Linear Systems subject to stochastic mode switches and imperfect mode estimation. Our approach builds on safe and robust methods for Model Predictive Control (MPC), but in contrast to existing approaches that either optimize without regard to feasibility or utilize soft constraints that increase computational requirements, we employ a safe and robust control approach informed by the feasibility of the optimization problem. We formulate and encode finite horizon safety for multiple model systems in our MPC design using Control Barrier Functions (CBFs). When subject to inaccurate hybrid state estimation, our feasibility-guided MPC generates a control policy that is maximally robust to uncertainty in the system’s modes. We evaluate our approach on an orbital rendezvous problem and a six degree-of-freedom hexacopter under several scenarios and benchmarks to demonstrate the utility of the framework. Results indicate that the proposed technique of maximizing the robustness horizon, and the use of CBFs for safety awareness, improve the overall safety and performance of MPC for Jump Markov Linear Systems.},
  archive   = {C_IROS},
  author    = {Zakariya Laouar and Qi Heng Ho and Rayan Mazouz and Tyler Becker and Zachary N. Sunberg},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801951},
  month     = {10},
  pages     = {12718-12725},
  title     = {Feasibility-guided safety-aware model predictive control for jump markov linear systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Grow-to-shape control of variable length continuum robots
via adaptive visual servoing. <em>IROS</em>, 12710–12717. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose an adaptive eye-to-hand vision-based control methodology, which enables a closed-loop grow-to-shape capability for variable length continuum manipulators in 2D. Our method utilizes shape features of the continuum robot, i.e. module curvature and length, which are obtained from the image. Our adaptive control algorithm servos the robot to converge and track the desired values of these features in the image space without the need of a robot model. As a result the robot starts from a minimum length configuration and grows into a given desired shape, always staying on the course of the desired shape. We believe that this approach unlocks capabilities for variable length continuum robots by leveraging their actuation redundancy and avoiding obstacles while carrying out object manipulation or inspection tasks in cluttered and constrained environments. We perform experiments in simulations and on a real robot to assess the performance of our visual servoing algorithm. Our experimental results demonstrate the controllers ability to accurately converge the current features to their references, for a variety of desired shapes in the image, while ensuring a smooth tracking response. We also present some proof of concept results demonstrating the effectiveness of this technique for controlling the robot in constrained environments. Markedly, this is the first successful demonstration for automatic grow-to-shape control using visual feedback for variable length continuum manipulators.},
  archive   = {C_IROS},
  author    = {Abhinav Gandhi and Shou-Shan Chiang and Cagdas D. Onal and Berk Calli},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802313},
  month     = {10},
  pages     = {12710-12717},
  title     = {Grow-to-shape control of variable length continuum robots via adaptive visual servoing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Motion primitives planning for center-articulated vehicles.
<em>IROS</em>, 12702–12709. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous navigation across unstructured terrains, including forests and construction areas, faces unique challenges due to intricate obstacles and the element of the unknown. Lacking pre-existing maps, these scenarios necessitate a motion planning approach that combines agility with efficiency. Critically, it must also incorporate the robot’s kinematic constraints to navigate more effectively through complex environments. This work introduces a novel planning method for center-articulated vehicles (CAV), leveraging motion primitives within a receding horizon planning framework using onboard sensing. The approach commences with the offline creation of motion primitives, generated through forward simulations that reflect the distinct kinematic model of center-articulated vehicles. These primitives undergo evaluation through a heuristic-based scoring function, facilitating the selection of the most suitable path for real-time navigation. To account for disturbances, we develop a pose-stabilizing controller, tailored to the kinematic specifications of center-articulated vehicles. During experiments, our method demonstrates a 67% improvement in SPL (Success Rate weighted by Path Length) performance over existing strategies. Furthermore, its efficacy was validated through real-world experiments conducted with a tree harvester vehicle - SAHA.},
  archive   = {C_IROS},
  author    = {Jiangpeng Hu and Fan Yang and Fang Nan and Marco Hutter},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801937},
  month     = {10},
  pages     = {12702-12709},
  title     = {Motion primitives planning for center-articulated vehicles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforcement learning control for autonomous hydraulic
material handling machines with underactuated tools. <em>IROS</em>,
12694–12701. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The precise and safe control of heavy material handling machines presents numerous challenges due to the hard-to-model hydraulically actuated joints and the need for collision-free trajectory planning with a free-swinging end-effector tool. In this work, we propose an RL-based controller that commands the cabin joint and the arm simultaneously. It is trained in a simulation combining data-driven modeling techniques with first-principles modeling. On the one hand, we employ a neural network model to capture the highly nonlinear dynamics of the upper carriage turn hydraulic motor, incorporating explicit pressure prediction to handle delays better. On the other hand, we model the arm as velocity-controllable and the free-swinging end-effector tool as a damped pendulum using first principles. This combined model enhances our simulation environment, enabling the training of RL controllers that can be directly transferred to the real machine. Designed to reach steady-state Cartesian targets, the RL controller learns to leverage the hydraulic dynamics to improve accuracy, maintain high speeds, and minimize end-effector tool oscillations. Our controller, tested on a mid-size prototype material handler, is more accurate than an inexperienced operator and causes fewer tool oscillations. It demonstrates competitive performance even compared to an experienced professional driver.},
  archive   = {C_IROS},
  author    = {Filippo A. Spinelli and Pascal Egli and Julian Nubert and Fang Nan and Thilo Bleumer and Patrick Goegler and Stephan Brockes and Ferdinand Hofmann and Marco Hutter},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802199},
  month     = {10},
  pages     = {12694-12701},
  title     = {Reinforcement learning control for autonomous hydraulic material handling machines with underactuated tools},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic walking on highly underactuated point foot
humanoids: Closing the loop between HZD and HLIP. <em>IROS</em>,
12686–12693. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Realizing bipedal locomotion on humanoid robots with point feet is especially challenging due to their highly underactuated nature, high degrees of freedom, and hybrid dynamics resulting from impacts. With the goal of addressing this challenging problem, this paper develops a control framework for realizing dynamic locomotion and implements it on a novel point foot humanoid: ADAM. To this end, we close the loop between Hybrid Zero Dynamics (HZD) and Hybrid linear inverted pendulum (HLIP) based step length regulation. To leverage the full-order hybrid dynamics of the robot, walking gaits are first generated offline by utilizing HZD. These trajectories are stabilized online through the use of a HLIP based regulator. Finally, the planned trajectories are mapped into the full-order system using a task space controller incorporating inverse kinematics. The proposed method is verified through numerical simulations and hardware experiments on the humanoid robot ADAM marking the first humanoid point foot walking. Moreover, we experimentally demonstrate the robustness of the realized walking via the ability to track a desired reference speed, robustness to pushes, and locomotion on uneven terrain.},
  archive   = {C_IROS},
  author    = {Adrian B. Ghansah and Jeeseop Kim and Kejun Li and Aaron D. Ames},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802542},
  month     = {10},
  pages     = {12686-12693},
  title     = {Dynamic walking on highly underactuated point foot humanoids: Closing the loop between HZD and HLIP},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On performing non-prehensile rolling manipulations:
Stabilizing synchronous motions of butterfly robots⋆. <em>IROS</em>,
12679–12685. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The paper explores the challenging task of performing a non-prehensile manipulation of several balls synchronously rolling on the curved hands of Butterfly robots. Each Butterfly robot represents a standard benchmark hardware setup, comprising a DC motor rotating a butterfly-shaped frame in a vertical plane, with a ball moving freely upon it, equipped with integrated computer vision, communication, programmable control, and computation interfaces. The combined dynamics of the considered system, consisting of N ≥ 2 such robots, is inherently underactuated, characterized by N active and N passive degrees of freedom, as well as N independent unilateral constraints that model the interactions between the frames and the balls, assuming no slipping. We focus on designing a model-based centralized feedback controller to achieve synchronized rotations of the balls. We assume the accuracy of our mathematical model and the feasibility of implementing a discretized version of the proposed continuous-time controller with a sufficiently small sampling time, that, in particular, is necessary for numerical differentiation. Relying on orbital stability of nominal periodic solution of the closed-loop system, we will experimentally check robustness to various inevitable challenges such as noises, disturbances, uncertainties, and communication delays. Hence, our concentration lies in designing an orbitally stabilizing controller for the underactuated models. The primary contribution is proposing one set of transverse coordinates, enabling transverse-linearization-based controller design, accompanied by pertinent closed-loop system analysis tools, thereby enhancing the efficacy of solving the manipulation task. Analytical and model-based arguments are validated through successful simulations and experiments conducted on two Butterfly robots, thereby emphasizing the validity and practicality of the proposed approach.},
  archive   = {C_IROS},
  author    = {Maksim O. Surov and Stepan S. Pchelkin and Anton S. Shiriaev and Sergei V. Gusev and Leonid B. Freidovich},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801522},
  month     = {10},
  pages     = {12679-12685},
  title     = {On performing non-prehensile rolling manipulations: Stabilizing synchronous motions of butterfly robots⋆},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-agent behavior retrieval: Retrieval-augmented policy
training for cooperative push manipulation by mobile robots.
<em>IROS</em>, 12671–12678. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Due to the complex interactions between agents, learning multi-agent control policy often requires a prohibitive amount of data. This paper aims to enable multi-agent systems to effectively utilize past memories to adapt to novel collaborative tasks in a data-efficient fashion. We propose the Multi-Agent Coordination Skill Database, a repository for storing a collection of coordinated behaviors associated with key vectors distinctive to them. Our Transformer-based skill encoder effectively captures spatio-temporal interactions that contribute to coordination and provides a unique skill representation for each coordinated behavior. By leveraging only a small number of demonstrations of the target task, the database enables us to train the policy using a dataset augmented with the retrieved demonstrations. Experimental evaluations demonstrate that our method achieves a significantly higher success rate in push manipulation tasks compared with baseline methods like few-shot imitation learning. Furthermore, we validate the effectiveness of our retrieve-and-learn framework in a real environment using a team of wheeled robots.},
  archive   = {C_IROS},
  author    = {So Kuroki and Mai Nishimura and Tadashi Kozuno},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801334},
  month     = {10},
  pages     = {12671-12678},
  title     = {Multi-agent behavior retrieval: Retrieval-augmented policy training for cooperative push manipulation by mobile robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning variable compliance control from a few
demonstrations for bimanual robot with haptic feedback teleoperation
system. <em>IROS</em>, 12663–12670. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automating dexterous, contact-rich manipulation tasks using rigid robots is a significant challenge in robotics. Rigid robots, defined by their actuation through position commands, face issues of excessive contact forces due to their inability to adapt to contact with the environment, potentially causing damage. While compliance control schemes have been introduced to mitigate these issues by controlling forces via external sensors, they are hampered by the need for fine-tuning task-specific controller parameters. Learning from Demonstrations (LfD) offers an intuitive alternative, allowing robots to learn manipulations through observed actions. In this work, we introduce a novel system to enhance the teaching of dexterous, contact-rich manipulations to rigid robots. Our system is twofold: firstly, it incorporates a teleoperation interface utilizing Virtual Reality (VR) controllers, designed to provide an intuitive and cost-effective method for task demonstration with haptic feedback. Secondly, we present Comp-ACT (Compliance Control via Action Chunking with Transformers), a method that leverages the demonstrations to learn variable compliance control from a few demonstrations. Our methods have been validated across various complex contact-rich manipulation tasks using single-arm and bimanual robot setups in simulated and real-world environments, demonstrating the effectiveness of our system in teaching robots dexterous manipulations with enhanced adaptability and safety. Code available at https://github.com/omron-sinicx/CompACT.},
  archive   = {C_IROS},
  author    = {Tatsuya Kamijo and Cristian C. Beltran-Hernandez and Masashi Hamaya},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801731},
  month     = {10},
  pages     = {12663-12670},
  title     = {Learning variable compliance control from a few demonstrations for bimanual robot with haptic feedback teleoperation system},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic manipulation of deformable objects using imitation
learning with adaptation to hardware constraints. <em>IROS</em>,
12655–12662. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Imitation Learning (IL) is a promising paradigm for learning dynamic manipulation of deformable objects since it does not depend on difficult-to-create accurate simulations of such objects. However, the translation of motions demonstrated by a human to a robot is a challenge for IL, due to differences in the embodiments and the robot’s physical limits. These limits are especially relevant in dynamic manipulation where high velocities and accelerations are typical. To address this problem, we propose a framework that first maps a dynamic demonstration into a motion that respects the robot’s constraints using a constrained Dynamic Movement Primitive. Second, the resulting object state is further optimized by quasi-static refinement motions to optimize task performance metrics. This allows both efficiently altering the object state by dynamic motions and stable small-scale refinements. We evaluate the framework in the challenging task of bag opening, designing the system BILBO: Bimanual dynamic manipulation using Imitation Learning for Bag Opening. Our results show that BILBO can successfully open a wide range of crumpled bags, using a demonstration with a single bag. See supplementary material at https://sites.google.com/view/bilbo-bag.},
  archive   = {C_IROS},
  author    = {Eric Hannus and Tran Nguyen Le and David Blanco-Mulero and Ville Kyrki},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802478},
  month     = {10},
  pages     = {12655-12662},
  title     = {Dynamic manipulation of deformable objects using imitation learning with adaptation to hardware constraints},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical action chunking transformer: Learning temporal
multimodality from demonstrations with fast imitation behavior.
<em>IROS</em>, 12648–12654. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Behavioral cloning from human demonstrations has succeeded in programming a robot to generate fine-grained motion, but it is still challenging to learn multimodal trajectories such as with various speeds. This restricts the use of a robot dataset collected by multiusers because the different proficiency of robot operators makes the dataset have diverse distributions of speed. To tackle this issue, we develop Hierarchical Action Chunking Transformer with Vector-quantization (HACT-Vq) to efficiently learn temporal multimodality in addition to fine-grained motion. The proposed hierarchical model consists of a high-level policy to make planning for a latent subgoal and style, and a low-level policy to predict an action chunk conditioned with the latent subgoal and style. The latent subgoal and style are trained as discrete representations so that high-level policy can efficiently learn multimodal distributions of demonstrations and retrieve the mode of fast behavior. In experiments, we set up bimanual robots in both simulation and real-world environments, and collected demonstrations with various speeds. The proposed model with the quantized subgoal and style showed the highest success rates with fast imitation behavior. Our code is available at https://github.com/SamsungLabs/hierarchical-act.},
  archive   = {C_IROS},
  author    = {J. Hyeon Park and Wonhyuk Choi and Sunpyo Hong and Hoseong Seo and Joonmo Ahn and Changsu Ha and Heungwoo Han and Junghyun Kwon},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802845},
  month     = {10},
  pages     = {12648-12654},
  title     = {Hierarchical action chunking transformer: Learning temporal multimodality from demonstrations with fast imitation behavior},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MLPER: Multi-level prompts for adaptively enhancing
vision-language emotion recognition. <em>IROS</em>, 12640–12647. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the field of robotics, vision-based Emotion Recognition (ER) has achieved significant progress, but it still faces the challenge of poor generalization ability under unconstrained conditions (e.g., occlusions and pose variations). In this work, we propose MLPER model, which introduces Vision-Language Model for Emotion Recognition to learn discriminative representations adaptively. Specifically, different from typically leveraging a hand-crafted prompt (e.g., &quot;a photo of a [class] person&quot;), we first establish Multi-Level Prompts from three aspects: facial expression, human posture and situational condition using large language models, like ChatGPT. Correspondingly, we extract the visual tokens from three levels: the face, body, and context. Further, to achieve fine-grained alignment at each level, we adopt textual tokens from the positive and the hard negative to query visual tokens, predicting whether a pair of image and text is matched. Experimental results demonstrate that our MLPER model outperforms the state-of-the-art methods on several ER benchmarks, especially under the conditions of occlusions and pose variations.},
  archive   = {C_IROS},
  author    = {Yu Gao and Weihong Ren and Xinglong Xu and Yan Wang and Zhiyong Wang and Honghai Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801375},
  month     = {10},
  pages     = {12640-12647},
  title     = {MLPER: Multi-level prompts for adaptively enhancing vision-language emotion recognition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning to recover from plan execution errors during robot
manipulation: A neuro-symbolic approach. <em>IROS</em>, 12632–12639. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automatically detecting and recovering from failures is an important but challenging problem for autonomous robots. Most of the recent work on learning to plan from demonstrations lacks the ability to detect and recover from errors in the absence of an explicit state representation and/or a (sub-) goal check function. We propose an approach (blending learning with symbolic search) for automated error discovery and recovery, without needing annotated data of failures. Central to our approach is a neuro-symbolic state representation, in the form of dense scene graph, structured based on the objects present within the environment. This enables efficient learning of the transition function and a discriminator that not only identifies failures but also localizes them facilitating fast re-planning via computation of heuristic distance function. We also present an anytime version of our algorithm, where instead of recovering to the last correct state, we search for a sub-goal in the original plan minimizing the total distance to the goal given a re-planning budget. Experiments on a physics simulator with a variety of simulated failures show the effectiveness of our approach compared to existing baselines, both in terms of efficiency as well as accuracy of our recovery mechanism.},
  archive   = {C_IROS},
  author    = {Namasivayam K and Arnav Tuli and Vishal Bindal and Himanshu Singh and Parag Singla and Rohan Paul},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801831},
  month     = {10},
  pages     = {12632-12639},
  title     = {Learning to recover from plan execution errors during robot manipulation: A neuro-symbolic approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Environment transformer and policy optimization for
model-based offline reinforcement learning. <em>IROS</em>, 12625–12631.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Interacting with the actual environment to acquire data is often costly and time-consuming in robotic tasks. Model-based offline reinforcement learning (RL) provides a feasible solution. On the one hand, it eliminates the requirements of interaction with the actual environment. On the other hand, it learns the transition dynamics and reward function from the offline datasets and generates simulated rollouts to accelerate training. Previous model-based offline RL methods adopt probabilistic ensemble neural networks (NN) to model aleatoric uncertainty and epistemic uncertainty. However, this results in a great increase in training time and computing resource requirements. Furthermore, these methods are easily disturbed by the accumulative errors of the environment dynamics models when simulating long-term rollouts. To solve the above problems, we propose an uncertainty-aware sequence modeling architecture called Environment Transformer. It models the probability distribution of the environment dynamics and reward function to capture aleatoric uncertainty and treats epistemic uncertainty as a learnable noise parameter. Benefiting from the accurate modeling of the transition dynamics and reward function, Environment Transformer can be combined with arbitrary planning, dynamics programming, or policy optimization algorithms for offline RL. In this case, we perform Conservative Q-Learning (CQL) to learn a conservative Q-function. Through simulation experiments, we demonstrate that our method achieves or exceeds state-of-the-art performance in widely studied offline RL benchmarks. Moreover, we show that Environment Transformer&#39;s simulated rollout quality, sample efficiency, and long-term rollout simulation capability are superior to those of previous model-based offline RL methods.},
  archive   = {C_IROS},
  author    = {Pengqin Wang and Meixin Zhu and Shaojie Shen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802094},
  month     = {10},
  pages     = {12625-12631},
  title     = {Environment transformer and policy optimization for model-based offline reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Confidence-aware decision-making and control for tool
selection*. <em>IROS</em>, 12617–12624. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-reflecting about our performance (e.g., how confident we are) before doing a task is essential for decision making, such as selecting the most suitable tool or choosing the best route to drive. While this form of awareness—thinking about our performance or metacognitive performance—is well-known in humans, robots still lack this cognitive ability. This reflective monitoring can enhance their embodied decision power, robustness and safety. Here, we take a step in this direction by introducing a mathematical framework that allows robots to use their control self-confidence to make better-informed decisions. We derive a mathematical closed-form expression for control confidence for dynamic systems (i.e., the posterior inverse covariance of the control action). This control confidence seamlessly integrates within an objective function for decision making, that balances the: i) performance for task completion, ii) control effort, and iii) self-confidence. To evaluate our theoretical account, we framed the decision-making within the tool selection problem, where the agent has to select the best robot arm for a particular control task. The statistical analysis of the numerical simulations with randomized 2DOF arms shows that using control confidence during tool selection improves both real task performance, and the reliability of the tool for performance under unmodelled perturbations (e.g., external forces). Furthermore, our results indicate that control confidence is an early indicator of performance and thus, it can be used as a heuristic for making decisions when computation power is restricted or decision-making is intractable. Overall, we show the advantages of using confidence-aware decision-making and control scheme for dynamic systems.},
  archive   = {C_IROS},
  author    = {Ajith Anil Meera and Pablo Lanillos},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801635},
  month     = {10},
  pages     = {12617-12624},
  title     = {Confidence-aware decision-making and control for tool selection*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online refractive camera model calibration in visual
inertial odometry. <em>IROS</em>, 12609–12616. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a general refractive camera model and online co-estimation of odometry and the refractive index of an unknown media. This enables operation in diverse and varying refractive fluids, given only the camera calibration in air. The refractive index is estimated online as a state variable of a monocular visual-inertial odometry framework in an iterative formulation using the proposed camera model. The method was verified on data collected using an underwater robot traversing inside a pool. The evaluations demonstrate convergence to the ideal refractive index for water despite significant perturbations in the initialization. Simultaneously, the approach enables on-par visual-inertial odometry performance in refractive media without prior knowledge of the refractive index or requirement of medium-specific camera calibration.},
  archive   = {C_IROS},
  author    = {Mohit Singh and Kostas Alexis},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802302},
  month     = {10},
  pages     = {12609-12616},
  title     = {Online refractive camera model calibration in visual inertial odometry},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NVINS: Robust visual inertial navigation fused with
NeRF-augmented camera pose regressor and uncertainty quantification.
<em>IROS</em>, 12601–12608. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, Neural Radiance Fields (NeRF) have emerged as a powerful tool for 3D reconstruction and novel view synthesis. However, the computational cost of NeRF rendering and degradation in quality due to the presence of artifacts pose significant challenges for its application in real-time and robust robotic tasks, especially on embedded systems. This paper introduces a novel framework that integrates NeRF-derived localization information with Visual-Inertial Odometry (VIO) to provide a robust solution for real-time robotic navigation. By training an absolute pose regression network with augmented image data rendered from a NeRF and quantifying its uncertainty, our approach effectively counters positional drift and enhances system reliability. We also establish a mathematically sound foundation for combining visual inertial navigation with camera localization neural networks, considering uncertainty under a Bayesian framework. Experimental validation in a photorealistic simulation environment demonstrates significant improvements in accuracy compared to a conventional VIO approach.},
  archive   = {C_IROS},
  author    = {Juyeop Han and Lukas Lao Beyer and Guilherme V. Cavalheiro and Sertac Karaman},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801527},
  month     = {10},
  pages     = {12601-12608},
  title     = {NVINS: Robust visual inertial navigation fused with NeRF-augmented camera pose regressor and uncertainty quantification},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A point-line features fusion method for fast and robust
monocular visual-inertial initialization. <em>IROS</em>, 12594–12600.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fast and robust initialization is essential for highly accurate monocular visual-inertial odometer (VIO), but at present majority of initialization methods rely only on point features, unstable in low texture and blurring situations. Therefore, we propose a novel point-line features fusion method for monocular visual-inertial initialization, as line features are more stable and provide richer geometric information than point features: 1) a closed-form line features initialization method is presented, and combined with point features to obtain a more integrated and robust linear system; 2) a monocular depth network is adopted to provide learned affine-invariant depth map, requiring only one prior depth map for the first frame, which can improve performance under low-parallax scenarios; 3) we can easily use RANSAC to reject outliers in solving linear system based on our formulation. Moreover, line feature re-projection residual is added to visual-inertial bundle adjustment (VI-BA) to obtain more accurate initial parameters. The proposed method is more accurate and robust than state-of-the-art methods due to the line features, especially under extreme low-parallax scenarios, and extensive experiments on popular datasets have confirmed, 0.5s initialization window on EuRoC MAV, 0.3s initialization window on TUM-VI, while the standard method normally waits for a window of 2s.},
  archive   = {C_IROS},
  author    = {Guoqiang Xie and Jie Chen and Tianhang Tang and Zeyu Chen and Ling Lei and Yiguang Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801745},
  month     = {10},
  pages     = {12594-12600},
  title     = {A point-line features fusion method for fast and robust monocular visual-inertial initialization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EverySync: An open hardware time synchronization sensor
suite for common sensors in SLAM. <em>IROS</em>, 12587–12593. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-sensor fusion systems have been widely applied in various fields, including mobile robot, simultaneous localization and mapping (SLAM), and autonomous driving. For a tightly coupled multi-sensor fusion system, strict time synchronization between sensors will improve the accuracy of the system. However, there is currently a lack of open-source and general-purpose hardware synchronization systems for Cameras, IMUs, LiDARs, GNSS/RTK in the academic community. Therefore, we propose EverySync, an open hardware time synchronization system to address this gap. The synchronization accuracy of the system was evaluated through multiple experiments, achieving an accuracy of less than 1 ms. And, real-world experiments proved that hardware time synchronization improves the accuracy of the SLAM system. This open-source system is available on GitHub.},
  archive   = {C_IROS},
  author    = {Xuankang Wu and Haoxiang Sun and Rongguang Wu and Zheng Fang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801968},
  month     = {10},
  pages     = {12587-12593},
  title     = {EverySync: An open hardware time synchronization sensor suite for common sensors in SLAM},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Event-intensity stereo with cross-modal fusion and contrast.
<em>IROS</em>, 12581–12586. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For binocular stereo, traditional cameras excel in capturing fine details and texture information but are limited in terms of dynamic range and their ability to handle rapid motion. On the contrary, event cameras provide pixel-level intensity changes with low latency and a wide dynamic range, albeit at the cost of less detail in their output. It is natural to leverage the strengths of both modalities. We solve this problem by introducing a cross-modal fusion module that learns a visual representation from both sensor inputs. Additionally, we extract and compare dense event-intensity stereo pair features by contrasting “pairs of event-intensity pairs from different views and different modalities and different timestamps”. This provides the flexibility in masking hard negatives and enables networks to effectively combine event-intensity signals within a contrastive learning framework, leading to an improved matching accuracy and facilitating more accurate estimation of disparity. Experimental results validate the effectiveness of our model and the improvement of disparity estimation accuracy.},
  archive   = {C_IROS},
  author    = {Yuanbo Wang and Shanglai Qu and Tianyu Meng and Yan Cui and Haiyin Piao and XiaoPeng Wei and Xin Yang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802786},
  month     = {10},
  pages     = {12581-12586},
  title     = {Event-intensity stereo with cross-modal fusion and contrast},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accurate and efficient loop closure detection with deep
binary image descriptor and augmented point cloud registration.
<em>IROS</em>, 12573–12580. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Loop Closure Detection (LCD) is an essential component of Simultaneous Localization and Mapping (SLAM), helping to correct drift errors, facilitate map merging, or both by identifying previously observed scenes. Despite its importance, traditional LCD algorithms based on single sensor such as camera or LiDAR exhibit degraded performance in challenging scenarios due to their inherent limitations. To address this issue, we propose a novel LCD method based on camera-LiDAR fusion, exploiting the rich textural information from cameras and the accurate geometric data from LiDAR to ensure robustness and speed in challenging environments. Specifically, we first employ deep hashing learning to encode deep image features into binary image descriptors for extremely fast loop candidate (LC) retrieval. Then, LiDAR points are augmented with image color for accurate geometric verification. Finally, we incorporate a spatial-temporal consistency check that mandates an LC to have consistently matched neighbors to be accepted as true. Our method is extensively verified and compared with the state-of-the-art methods on various datasets encompassing both indoor and outdoor environments. Experimental results demonstrate that our method obtains the best performance, increasing the maximum recall rate at 100% precision by a significant margin of 20% while operating in real-time at an average speed of 30 fps.},
  archive   = {C_IROS},
  author    = {Jialiang Wang and Zhi Gao and Zhipeng Lin and Zhiyu Zhou and Xiaonan Wang and Jianhua Cheng and Hao Zhang and Xinyi Liu and Ben M. Chen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802610},
  month     = {10},
  pages     = {12573-12580},
  title     = {Accurate and efficient loop closure detection with deep binary image descriptor and augmented point cloud registration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time truly-coupled lidar-inertial motion correction and
spatiotemporal dynamic object detection. <em>IROS</em>, 12565–12572. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Over the past decade, lidars have become a cornerstone of robotics state estimation and perception thanks to their ability to provide accurate geometric information about their surroundings in the form of 3D scans. Unfortunately, most of nowadays lidars do not take snapshots of the environment but sweep the environment over a period of time (typically around 100 ms). Such a rolling-shutter-like mechanism introduces motion distortion into the collected lidar scan, thus hindering downstream perception applications. In this paper, we present a novel method for motion distortion correction of lidar data by tightly coupling lidar with Inertial Measurement Unit (IMU) data. The motivation of this work is a map-free dynamic object detection based on lidar. The proposed lidar data undistortion method relies on continuous preintegrated of IMU measurements that allow parameterising the sensors’ continuous 6-DoF trajectory using solely eleven discrete state variables (biases, initial velocity, and gravity direction). The undistortion consists of feature-based distance minimisation of point-to-line and point-to-plane residuals in a non-linear least-square formulation. Given undistorted geometric data over a short temporal window, the proposed pipeline computes the spatiotemporal normal vector of each of the lidar points. The temporal component of the normals is a proxy for the corresponding point’s velocity, therefore allowing for learning-free dynamic object classification without the need for registration in a global reference frame. We demonstrate the soundness of the proposed method and its different components using public datasets and compare them with state-of-the-art lidar-inertial state estimation and dynamic object detection algorithms.},
  archive   = {C_IROS},
  author    = {Cedric Le Gentil and Raphael Falque and Teresa Vidal-Calleja},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802233},
  month     = {10},
  pages     = {12565-12572},
  title     = {Real-time truly-coupled lidar-inertial motion correction and spatiotemporal dynamic object detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Error-state kalman filter based visual-inertial odometry
using orientation measurement on unit quaternion group. <em>IROS</em>,
12559–12564. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The inaccessibility of data from standard sensor suites on closed-platform unmanned aerial vehicles (UAVs) has been a hindrance to developing a compatible visual-inertial odometry (VIO). Despite the advance of recent VIO research, these works often emphasize fusing detailed sensor models with available sensor data at relatively high frequencies. To address this issue, in this paper, we derive an innovation signal for an orientation measurement model on the unit quaternion group ${\mathbb{S}^3}$ based on the error-state Kalman filter (ESKF) framework. Leveraging the error-state formulation, the innovation signal directly exploits the geometric error representation on ${\mathbb{S}^3}$ instead of treating unit quaternions as ℝ4 vectors. Flight experiments on a small commercial UAV (Fig. 1) have been carried out to compare the performance of the proposed ESKF with quaternion measurements on ${\mathbb{S}^3}$ (ESKF-Q) against the original ESKF framework. Experimental results demonstrate that while both representations of unit quaternion measurements in ESKF framework improve orientation estimates with unperturbed orientation measurement model, only the proposed ESKF-Q exhibits convergent state estimates in the presence of uncertainties in the orientation measurement model.},
  archive   = {C_IROS},
  author    = {Chao-Wei Chang and Feng-Li Lian},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801565},
  month     = {10},
  pages     = {12559-12564},
  title     = {Error-state kalman filter based visual-inertial odometry using orientation measurement on unit quaternion group},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DCSANet: Dual cross-channel and spatial attention make RGB-t
object detection better. <em>IROS</em>, 12552–12558. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multimodal image pairs can make object detection more reliable in challenging environments, so RGB-T object detection has gained extensive attention over the past decade. To alleviate the complementarity of the visible and thermal modality, we propose a novel lightweight Feature Enhancement-fusion Module (FEM), which is composed of the Channel Enhancement-fusion Unit (CEU) and Spatial Enhancement-fusion Unit (SEU) by extending the attention mechanism to operate on two modalities. CEU is used to exploit the complementarity and alleviate the data imbalance by combining internal and global channel attention. Additionally, SEU is utilized to guide the model to pay more attention to the regions of interest. By incorporating FEM, enhanced and fused features are obtained, leading to improved performance. The effectiveness and generalizability of FEM are validated by two public datasets and our proposed DCSANet achieves competitive performance while maintaining high speed (+%7.0 on LLVIP and +1.2% on FLIR in mAP). Moreover, we conducted ablation experiments to verify the effectiveness of the proposed operators.},
  archive   = {C_IROS},
  author    = {Xiaoxiong Lan and Shenghao Liu and Zhiyong Zhang and Changzhen Qiu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802230},
  month     = {10},
  pages     = {12552-12558},
  title     = {DCSANet: Dual cross-channel and spatial attention make RGB-T object detection better},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Monocular event-inertial odometry with adaptive decay-based
time surface and polarity-aware tracking. <em>IROS</em>, 12544–12551.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Event cameras have garnered considerable attention due to their advantages over traditional cameras in low power consumption, high dynamic range, and no motion blur. This paper proposes a monocular event-inertial odometry incorporating an adaptive decay kernel-based time surface with polarity-aware tracking. We utilize an adaptive decay-based Time Surface to extract texture information from asynchronous events, which adapts to the dynamic characteristics of the event stream and enhances the representation of environmental textures. However, polarity-weighted time surfaces suffer from event polarity shifts during changes in motion direction. To mitigate its adverse effects on feature tracking, we optimize the feature tracking by incorporating an additional polarityinverted time surface to enhance the robustness. Comparative analysis with visual-inertial and event-inertial odometry methods shows that our approach outperforms state-of-the-art techniques, with competitive results across various datasets.},
  archive   = {C_IROS},
  author    = {Kai Tang and Xiaolei Lang and Yukai Ma and Yuehao Huang and Laijian Li and Yong Liu and Jiajun Lv},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802605},
  month     = {10},
  pages     = {12544-12551},
  title     = {Monocular event-inertial odometry with adaptive decay-based time surface and polarity-aware tracking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VIRUS-NeRF - vision, InfraRed and UltraSonic based neural
radiance fields. <em>IROS</em>, 12536–12543. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous mobile robots are an increasingly integral part of modern factory and warehouse operations. Obstacle detection, avoidance and path planning are critical safety-relevant tasks, which are often solved using expensive LiDAR sensors and depth cameras. We propose to use cost-effective low-resolution ranging sensors, such as ultrasonic and infrared time-of-flight sensors by developing VIRUS-NeRF - Vision, InfraRed, and UltraSonic based Neural Radiance Fields.Building upon Instant Neural Graphics Primitives with a Multiresolution Hash Encoding (Instant-NGP), VIRUS-NeRF incorporates depth measurements from ultrasonic and infrared sensors and utilizes them to update the occupancy grid used for ray marching. Experimental evaluation in 2D demonstrates that VIRUS-NeRF achieves comparable mapping performance to LiDAR point clouds regarding coverage. Notably, in small environments, its accuracy aligns with that of LiDAR measurements, while in larger ones, it is bounded by the utilized ultrasonic sensors. An in-depth ablation study reveals that adding ultrasonic and infrared sensors is highly effective when dealing with sparse data and low view variation. Further, the proposed occupancy grid of VIRUS-NeRF improves the mapping capabilities and increases the training speed by 46% compared to Instant-NGP. Overall, VIRUS-NeRF presents a promising approach for cost-effective local mapping in mobile robotics, with potential applications in safety and navigation tasks. The code can be found at https://github.com/ethz-asl/virus_nerf.},
  archive   = {C_IROS},
  author    = {Nicolaj Schmid and Cornelius Von Einem and Cesar Cadena and Roland Siegwart and Lorenz Hruby and Florian Tschopp},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802852},
  month     = {10},
  pages     = {12536-12543},
  title     = {VIRUS-NeRF - vision, InfraRed and UltraSonic based neural radiance fields},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive visual-aided 4D radar odometry through
transformer-based feature fusion. <em>IROS</em>, 12529–12535. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multimodal sensor fusion has been successfully utilized in many odometry and localization methods as it increases both estimate accuracy and robustness in application scenarios. To address the challenge of odometry under varying-weather conditions, we propose a novel visual 4D radar fusion based odometry in an unsupervised deep learning approach. In our method, we adopt transformer-based cascaded decoders to facilitate efficient feature extraction of images and radar point clouds. Considering that radars are weather-agnostic and information-rich cameras are susceptible to adverse weathers, we deliberately introduce an adaptive attention-based feature fusion mechanism, in which the attention shifts dynamically to adapt to changing weather conditions based on the amount of information content in image features. Through extensive comparative experiments, our method surpasses different state-of-the-art single-modal odometry estimation methods. Our code and trained model will be released publicly.},
  archive   = {C_IROS},
  author    = {Yuanfan Zhang and Renxiang Xiao and Ziyang Hong and Liang Hu and Jie Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802367},
  month     = {10},
  pages     = {12529-12535},
  title     = {Adaptive visual-aided 4D radar odometry through transformer-based feature fusion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bridging language, vision and action: Multimodal VAEs in
robotic manipulation tasks. <em>IROS</em>, 12522–12528. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we focus on unsupervised vision-language-action mapping in the area of robotic manipulation. Recently, multiple approaches employing pre-trained large language and vision models have been proposed for this task. However, they are computationally demanding and require careful fine-tuning of the produced output. A more lightweight alter-native would be the implementation of multimodal Variational Autoencoders (VAEs) which can extract the latent features of the data and integrate them into a joint representation, as has been demonstrated mostly on image-image or image-text data for the state-of-the-art models. Here, we explore whether and how multimodal VAEs can be employed in unsupervised robotic manipulation tasks in a simulated environment. Based on the results obtained, we propose a model-invariant training alternative that improves the models’ performance in a simulator by up to 55 %. Moreover, we systematically evaluate the challenges raised by individual tasks, such as object or robot position variability, number of distractors, or task length. Our work thus also sheds light on the potential benefits and limitations of using the current multimodal VAEs for unsupervised learning of robotic motion trajectories based on vision and language.},
  archive   = {C_IROS},
  author    = {Gabriela Sejnova and Michal Vavrecka and Karla Stepanova},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802160},
  month     = {10},
  pages     = {12522-12528},
  title     = {Bridging language, vision and action: Multimodal VAEs in robotic manipulation tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accurately tracking relative positions of moving trackers
based on UWB ranging and inertial sensing without anchors.
<em>IROS</em>, 12515–12521. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a tracking system for relative positioning that can operate on entirely moving tracking nodes without the need for stationary anchors. Each node embeds a 9-DOF magnetic and inertial measurement unit and a single-antenna ultra-wideband radio. We introduce a multi-stage filtering pipeline through which our system estimates the relative layout of all tracking nodes within the group. The key novelty of our method is the integration of a custom Extended Kalman filter (EKF) with a refinement step via multidimensional scaling (MDS). Our method integrates the MDS output back into the EKF, thereby creating a dynamic feedback loop for more robust estimates. We complement our method with UWB ranging protocol that we designed to allow tracking nodes to opportunistically join and leave the group.In our evaluation with constantly moving nodes, our system estimated relative positions with an error of 10.2 cm (in 2D) and 21.7 cm (in 3D), including obstacles that occluded the line of sight between tracking nodes. Our approach requires no external infrastructure, making it particularly suitable for operation in environments where stationary setups are impractical.},
  archive   = {C_IROS},
  author    = {Rayan Armani and Christian Holz},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801834},
  month     = {10},
  pages     = {12515-12521},
  title     = {Accurately tracking relative positions of moving trackers based on UWB ranging and inertial sensing without anchors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Audio-visual traffic light state detection for urban robots.
<em>IROS</em>, 12509–12514. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a multimodal traffic light state detection using vision and sound, from the viewpoint of a quadruped robot navigating in urban settings. This is a challenging problem because of the visual occlusions and noise from robot locomotion. Our method combines features from raw audio with the ratios of red and green pixels within bounding boxes, identified by established vision-based detectors. The fusion method aggregates features across multiple frames in a given timeframe, increasing robustness and adaptability. Results show that our approach effectively addresses the challenge of visual occlusion and surpasses the performance of single-modality solutions when the robot is in motion. This study serves as a proof of concept, highlighting the significant, yet often overlooked, potential of multi-modal perception in robotics.},
  archive   = {C_IROS},
  author    = {Sagar Gupta and Akansel Cosgun},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802855},
  month     = {10},
  pages     = {12509-12514},
  title     = {Audio-visual traffic light state detection for urban robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Implicit neural fusion of RGB and far-infrared 3D imagery
for invisible scenes. <em>IROS</em>, 12501–12508. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Optical sensors, such as the Far Infrared (FIR) sensor, have demonstrated advantages over traditional imaging. For example, 3D reconstruction in the FIR field captures the heat distribution of a scene that is invisible to RGB, aiding various applications like gas leak detection. However, less texture information and challenges in acquiring FIR frames hinder the reconstruction process. Given that implicit neural representations (INRs) can integrate geometric information across different sensors, we propose Implicit Neural Fusion (INF) of RGB and FIR for 3D reconstruction of invisible scenes in the FIR field. Our method first obtains a neural density field of objects from RGB frames. Then, with the trained object density field, a separate neural density field of gases is optimized using limited view inputs of FIR frames. Our method not only demonstrates outstanding reconstruction quality in the FIR field through extensive experiments but also can isolate the geometric information of the invisible, offering a new dimension of scene understanding.},
  archive   = {C_IROS},
  author    = {Xiangjie Li and Shuxiang Xie and Ken Sakurada and Ryusuke Sagawa and Takeshi Oishi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802441},
  month     = {10},
  pages     = {12501-12508},
  title     = {Implicit neural fusion of RGB and far-infrared 3D imagery for invisible scenes},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AnytimeFusion: Parameter-free RGB camera-radar sensor fusion
algorithm in complex maritime situations. <em>IROS</em>, 12493–12500.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Determining the position of obstacles is crucial for unmanned vehicles, and, to achieve this, cameras and radar sensors are widely utilized. However, establishing correlation between two or more sensors proves challenging in the dynamically changing maritime environment. To solve these issues, we propose the AnytimeFusion algorithm. The key innovation of AnytimeFusion lies in the utilization of a parameter-free method that does not require accurate sensor alignment and calibration. The algorithm consists of the following four stages. First, calibration targets are selected in the maritime environment based on segmentation images. Second, radar and camera data are pre-fused to model the correlation of azimuth information. After completing the auto-calibration stages, Inverse Perspective Mapping (IPM) is employed to integrate the coordinate systems of the two sensors. To determine the parameters for this integration, optimization based on the Particle Swarm Optimization (PSO) method is employed. Finally, an Error Polygon for the positions of the camera and radar is generated, and sensor fusion is carried out based on this information. We validated our method through experiments conducted on real ships in complex maritime environments, achieving an average accuracy of 95.7%.},
  archive   = {C_IROS},
  author    = {Yeongha Shin and Hanguen Kim and Jinwhan Kim},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801864},
  month     = {10},
  pages     = {12493-12500},
  title     = {AnytimeFusion: Parameter-free RGB camera-radar sensor fusion algorithm in complex maritime situations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). KLILO: Kalman filter based LiDAR-inertial-leg odometry for
legged robots. <em>IROS</em>, 12487–12492. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a Kalman filter based LiDAR-Inertial-Leg Odometry (KLILO) system for legged robots to navigate in challenging environments. In particular, we employ the iterated error-state extended Kalman filter framework on manifolds to fuse measurements from the inertial measurement unit (IMU), LiDAR, joint encoders, and contact force sensors in a tightly coupled manner. To assess the performance of KLILO, we build a dataset that encompasses intricate environments with challenging conditions such as dynamic objects and deformable terrains. The results demonstrate that our algorithm can provide efficient and reliable localization in all tests. It exhibits an average improvement of around 40% in positioning accuracy compared to the baselines. Furthermore, we validate KLILO in a challenging navigation task on a real robot, where the LiDAR encounters ineffective measurements.},
  archive   = {C_IROS},
  author    = {Shaohang Xu and Wentao Zhang and Lijun Zhu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802030},
  month     = {10},
  pages     = {12487-12492},
  title     = {KLILO: Kalman filter based LiDAR-inertial-leg odometry for legged robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ASY-VRNet: Waterway panoptic driving perception model based
on asymmetric fair fusion of vision and 4D mmWave radar. <em>IROS</em>,
12479–12486. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Panoptic Driving Perception (PDP) is critical for the autonomous navigation of Unmanned Surface Vehicles (USVs). A PDP model typically integrates multiple tasks, necessitating the simultaneous and robust execution of various perception tasks to facilitate downstream path planning. The fusion of visual and radar sensors is currently acknowledged as a robust and cost-effective approach. However, most existing research has primarily focused on fusing visual and radar features dedicated to object detection or utilizing a shared feature space for multiple tasks, neglecting the individual representation differences between various tasks. To address this gap, we propose a pair of Asymmetric Fair Fusion (AFF) modules with favorable explainability designed to efficiently interact with independent features from both visual and radar modalities, tailored to the specific requirements of object detection and semantic segmentation tasks. The AFF modules treat image and radar maps as irregular point sets and transform these features into a crossed-shared feature space for multitasking, ensuring equitable treatment of vision and radar point cloud features. Leveraging AFF modules, we propose a novel and efficient PDP model, ASY-VRNet, which processes image and radar features based on irregular super-pixel point sets. Additionally, we propose an effective multi-task learning method specifically designed for PDP models. Compared to other lightweight models, ASY-VRNet achieves state-of-the-art performance in object detection, semantic segmentation, and drivable-area segmentation on the WaterScenes benchmark. Our project is publicly available at https://github.com/GuanRunwei/ASY-VRNet.},
  archive   = {C_IROS},
  author    = {Runwei Guan and Shanliang Yao and Ka Lok Man and Xiaohui Zhu and Yong Yue and Jeremy Smith and Eng Gee Lim and Yutao Yue},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802447},
  month     = {10},
  pages     = {12479-12486},
  title     = {ASY-VRNet: Waterway panoptic driving perception model based on asymmetric fair fusion of vision and 4D mmWave radar},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A case study on visual-audio-tactile cross-modal retrieval.
<em>IROS</em>, 12472–12478. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cross-Modal Retrieval (CMR), which retrieves relevant items from one modality (e.g., audio) given a query in another modality (e.g., visual), has undergone significant advancements in recent years. This capability is crucial for robots to integrate and interpret information across diverse sensory inputs. However, the retrieval space in existing robotic CMR approaches often consists of only one modality, which limits the performance of the robot. In this paper, we propose a novel CMR model that incorporates three different modalities, i.e., visual, audio, and tactile, for enhanced multi-modal object retrieval, referred to as VAT-CMR. In this model, multi-modal representations are first fused to provide a holistic view of object features. Then, to mitigate the semantic gaps between representations of different modalities, a dominant modality is selected during the classification training phase to improve the distinctiveness of the representations and enhance the retrieval performance. To evaluate our proposed approach, we conducted a case study and the results demonstrate that our VAT-CMR model surpasses competing approaches. Further, our proposed dominant modality selection significantly enhances cross-retrieval accuracy.},
  archive   = {C_IROS},
  author    = {Jagoda Wojcik and Jiaqi Jiang and Jiacheng Wu and Shan Luo},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802105},
  month     = {10},
  pages     = {12472-12478},
  title     = {A case study on visual-audio-tactile cross-modal retrieval},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Collision detection between smooth convex bodies via
riemannian optimization framework. <em>IROS</em>, 12464–12471. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collision detection is a fundamental problem across various fields such as robotics, physical simulation, and computer graphics. While numerous studies have provided efficient solutions, based on the well-known Gilbert, Johnson, and Keerthi (GJK) algorithm and Expanding Polytope Algorithm (EPA), existing methods utilizing GJK-EPA often struggle with smooth strictly convex shapes like ellipsoids. This paper proposes a novel approach to the collision detection problem converting it to a problem compatible with an unconstrained Riemannian optimization problem. Moreover, we presents a specific method of solving the problem based on twice differentiable support functions and the Riemannian trust region (RTR) method. The method exhibits fast and robust convergence rate, leveraging the well-established theory of Riemannian optimization. The evaluation studies comparing our method to GJK-EPA method are done with pre-defined primitive shapes. Additionally, a test result with several more complex shapes is demonstrated exhibiting the method’s effectiveness and applicability.},
  archive   = {C_IROS},
  author    = {Seoki An and Somang Lee and Jeongmin Lee and Sunkyung Park and Dongjun Lee},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802286},
  month     = {10},
  pages     = {12464-12471},
  title     = {Collision detection between smooth convex bodies via riemannian optimization framework},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). RoboCop: A robust zero-day cyber-physical attack detection
framework for robots. <em>IROS</em>, 12457–12463. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Zero-day vulnerabilities pose a significant challenge to robot cyber-physical systems (CPS). Attackers can exploit software vulnerabilities in widely-used robotics software, such as the Robot Operating System (ROS), to manipulate robot behavior, compromising both safety and operational effectiveness. The hidden nature of these vulnerabilities requires strong defense mechanisms to guarantee the safety and dependability of robotic systems. In this paper, we introduce RoboCop, a cyber-physical attack detection framework designed to protect robots from zero-day threats. RoboCop leverages static software features in the pre-execution analysis along with runtime state monitoring to identify attack patterns and deviations that signal attacks, thus ensuring the robot’s operational integrity. We evaluated RoboCop on the F1-tenth autonomous car platform. It achieves a 93% detection accuracy against a variety of zero-day attacks targeting sensors, actuators, and controller logic. Importantly, in on-robot deployments, it identifies attacks in less than 7 seconds with a 12% computational overhead.},
  archive   = {C_IROS},
  author    = {Upinder Kaur and Z. Berkay Celik and Richard M. Voyles},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802522},
  month     = {10},
  pages     = {12457-12463},
  title     = {RoboCop: A robust zero-day cyber-physical attack detection framework for robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). RoboGuardZ: A scalable zero-shot framework for detecting
zero-day malware in robots. <em>IROS</em>, 12450–12456. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ubiquitous deployment of robots across diverse domains, from industrial automation to personal care, underscores their critical role in modern society. However, this growing dependence has also revealed security vulnerabilities. An attack vector involves the deployment of malicious software (malware) on robots, which can cause harm to robots themselves, users, and even the surrounding environment. Machine learning approaches, particularly supervised ones, have shown promise in malware detection by building intricate models to identify known malicious code patterns. However, these methods are inherently limited in detecting unseen or zero-day malware variants as they require regularly updated massive datasets that might be unavailable to robots. To address this challenge, we introduce RoboGuardZ, a novel malware detection framework based on zero-shot learning for robots. This approach allows RoboGuardZ to identify unseen malware by establishing relationships between known malicious code and benign behaviors, allowing detection even before the code executes on the robot. To ensure practical deployment in resource-constrained robotic hardware, we employ a unique parallel structured pruning and quantization strategy that compresses the RoboGuardZ detection model by 37.4% while maintaining its accuracy. This strategy reduces the size of the model and computational demands, making it suitable for real-world robotic systems. We evaluated RoboGuardZ on a recent dataset containing real-world binary executables from multi-sensor autonomous car controllers. The framework was deployed on two popular robot embedded hardware platforms. Our results demonstrate an average detection accuracy of 94.25% and a low false negative rate of 5.8% with a minimal latency of 20 ms, which demonstrates its effectiveness and practicality.},
  archive   = {C_IROS},
  author    = {Upinder Kaur and Z. Berkay Celik and Richard M. Voyles},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801765},
  month     = {10},
  pages     = {12450-12456},
  title     = {RoboGuardZ: A scalable zero-shot framework for detecting zero-day malware in robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid continuum-eversion robot: Precise navigation and
decontamination in nuclear environments using vine robot. <em>IROS</em>,
12443–12449. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft growing vine robots show great potential for navigation and decontamination tasks in the nuclear industry. This paper introduces a novel hybrid continuum-eversion robot designed to address certain challenges in relation to navigating and operating within pipe networks and enclosed remote vessels. The hybrid robot combines the flexibility of a soft eversion robot with the precision of a continuum robot at its tip, allowing for controlled steering and movement in hard to access and/or complex environments. The design enables the delivery of sensors, liquids, and aerosols to remote areas, supporting remote decontamination activities.This paper outlines the design and construction of the robot and the methods by which it achieves selective steering. We also include a comprehensive review of current related work in eversion robotics, as well as other steering devices and actuators currently under research, which underpin this novel active steering approach. This is followed by an experimental evaluation that demonstrates the robot’s real-world capabilities in delivering liquids and aerosols to remote locations. The experiments reveal successful outcomes, with over 95% success in precision spraying tests. The paper concludes by discussing future work alongside limitations in the current design, ultimately showcasing its potential as a solution for remote decontamination operations in the nuclear industry.},
  archive   = {C_IROS},
  author    = {Mohammed Al-Dubooni and Cuebong Wong and Kaspar Althoefer},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801985},
  month     = {10},
  pages     = {12443-12449},
  title     = {Hybrid continuum-eversion robot: Precise navigation and decontamination in nuclear environments using vine robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Recover: A neuro-symbolic framework for failure detection
and recovery. <em>IROS</em>, 12435–12442. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recognizing failures during task execution and implementing recovery procedures is challenging in robotics. Traditional approaches rely on the availability of extensive data or a tight set of constraints, while more recent approaches leverage large language models (LLMs) to verify task steps and replan accordingly. However, these methods often operate offline, necessitating scene resets and incurring in high costs. This paper introduces Recover, a neuro-symbolic framework for online failure identification and recovery. By integrating ontologies, logical rules, and LLM-based planners, Recover exploits symbolic information to enhance the ability of LLMs to generate recovery plans and also to decrease the associated costs. In order to demonstrate the capabilities of our method in a simulated kitchen environment, we introduce OntoThor, an ontology describing the AI2Thor simulator setting. Empirical evaluation shows that OntoThor’s logical rules accurately detect all failures in the analyzed tasks, and that Recover considerably outperforms, for both failure detection and recovery, a baseline method reliant solely on LLMs. Supplementary material, including the OntoThor ontology, is available at: https://recover-ontothor.github.io.},
  archive   = {C_IROS},
  author    = {Cristina Cornelio and Mohammed Diab},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801853},
  month     = {10},
  pages     = {12435-12442},
  title     = {Recover: A neuro-symbolic framework for failure detection and recovery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CBFkit: A control barrier function toolbox for robotics
applications. <em>IROS</em>, 12428–12434. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces CBFkit, a Python/ROS toolbox for safe robotics planning and control under uncertainty. The toolbox provides a general framework for designing control barrier functions for mobility systems within both deterministic and stochastic environments. It can be connected to the ROS open-source robotics middleware, allowing for the setup of multi-robot applications, encoding of environments and maps, and integrations with predictive motion planning algorithms. Additionally, it offers multiple CBF variations and algorithms for robot control. The CBFKit is demonstrated on the Toyota Human Support Robot (HSR) in both simulation and in physical experiments.},
  archive   = {C_IROS},
  author    = {Mitchell Black and Georgios Fainekos and Bardh Hoxha and Hideki Okamoto and Danil Prokhorov},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801431},
  month     = {10},
  pages     = {12428-12434},
  title     = {CBFkit: A control barrier function toolbox for robotics applications},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Safe multi-agent reinforcement learning for bimanual
dexterous manipulation. <em>IROS</em>, 12420–12427. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bimanual dexterous manipulation in robotics, essential for a wide range of applications, addresses the critical challenge of balancing intricate operational capabilities with assured safety and reliability. While Safe Reinforcement Learning is integral to the robustness of robotic systems, the area of safe multi-agent reinforcement learning (MARL), cooperative control of multiple robots has been scarcely studied. In this study, we explore MARL for safe cooperative control with multiple robot hands. Each robot must follow individual and collective safety guidelines to ensure safe team actions. However, the non-stationarity inherent in current algorithms hinders the precise updating of strategies to satisfy these safety constraints effectively. In this paper, we propose Multi-Agent Constrained Proximal Advantage Optimization (MAC-PAO), which considers the sequence of agent updates and integrates non-stationarity into sequential update schemes. This algorithm ensures consistent improvement in both rewards and adherence to safety constraints in each iteration. We tested MACPAO on various tasks with safety constraints and demonstrated that it outperforms other MARL algorithms in balancing reward enhancement and safety compliance. Supplementary materials and code are available at the provided link https://github.com/YONEX4090/MultiSafeHand.git.},
  archive   = {C_IROS},
  author    = {Weishu Zhan and Peter Chin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801490},
  month     = {10},
  pages     = {12420-12427},
  title     = {Safe multi-agent reinforcement learning for bimanual dexterous manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MIXED-SENSE: A mixed reality sensor emulation framework for
test and evaluation of UAVs against false data injection attacks.
<em>IROS</em>, 12414–12419. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a high-fidelity Mixed Reality sensor emulation framework for testing and evaluating the resilience of Unmanned Aerial Vehicles (UAVs) against false data injection (FDI) attacks. The proposed approach can be utilized to assess the impact of FDI attacks, benchmark attack detector performance, and validate the effectiveness of mitigation/reconfiguration strategies in single-UAV and UAV swarm operations. Our Mixed Reality framework leverages high-fidelity simulations of Gazebo and a Motion Capture system to emulate proprioceptive (e.g., GNSS) and exteroceptive (e.g., camera) sensor measurements in real-time. We propose an empirical approach to faithfully recreate signal characteristics such as latency and noise in these measurements. Finally, we illustrate the efficacy of our proposed framework through a Mixed Reality experiment consisting of an emulated GNSS attack on an actual UAV, which (i) demonstrates the impact of false data injection attacks on GNSS measurements and (ii) validates a mitigation strategy utilizing a distributed camera network developed in our previous work. Our open-source implementation is available at https://github.com/CogniPilot/mixed_sense},
  archive   = {C_IROS},
  author    = {Kartik A. Pant and Li-Yu Lin and Jaehyeok Kim and Worawis Sribunma and James M. Goppert and Inseok Hwang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802327},
  month     = {10},
  pages     = {12414-12419},
  title     = {MIXED-SENSE: A mixed reality sensor emulation framework for test and evaluation of UAVs against false data injection attacks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Differential-algebraic equation control barrier function for
flexible link manipulator. <em>IROS</em>, 12408–12413. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a control barrier function (CBF) for systems described by differential-algebraic equations and applies the method to guarantee the safety of a two-link flexible-link manipulator. The two main contributions of the paper are: a) an extension of CBFs to systems governed by differential-algebraic equations; b) a framework for simulation of flexible-link robots in a floating frame of reference formulation (FFRF) finite element method (FEM). Numerical simulations demonstrate the minimally invasive safety control of a flexible two-link manipulator with position constraints through CBF quadratic programming without converting the differential-algebraic equations to a control-affine system.},
  archive   = {C_IROS},
  author    = {Younghwa Park and Christoffer Sloth},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801482},
  month     = {10},
  pages     = {12408-12413},
  title     = {Differential-algebraic equation control barrier function for flexible link manipulator},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Safe offline-to-online multi-agent decision transformer: A
safety conscious sequence modeling approach. <em>IROS</em>, 12400–12407.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce the Safe Offline-to-Online Multi-Agent Decision Transformer (SO2-MADT), an innovative framework that revolutionizes safety considerations in Multi-agent Reinforcement Learning (MARL) through a novel sequence modeling approach. Leveraging the dynamic capabilities inherent in Decision Transformers, our methodology seamlessly incorporates safety protocols as a cornerstone element, ensuring secure operations throughout both the offline pre-training phase and the adaptive online fine-tuning phase. At the core of our framework lie two pivotal innovations: the Safety-To-Go (STG) token, embedding safety at a macro level, and the Agent Prioritization Module (APM), facilitating explicit credit assignment at a micro level. Through extensive testing against the challenging environments of the StarCraft Multi-Agent Challenge (SMAC) and Multi-agent MuJoCo, our SO2-MADT not only excels in offline pre-training but also demonstrates superior performance during online fine-tuning, without any degradation in performance. The implications of our work provide a pathway for deployment in critical real-world applications where safety is paramount and non-negotiable. The code is available at https://github.com/shahaamirbader/SO2-MADT.},
  archive   = {C_IROS},
  author    = {Aamir Bader Shah and Yu Wen and Jiefu Chen and Xuqing Wu and Xin Fu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801292},
  month     = {10},
  pages     = {12400-12407},
  title     = {Safe offline-to-online multi-agent decision transformer: A safety conscious sequence modeling approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interruptive language control of bipedal locomotion.
<em>IROS</em>, 12394–12399. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of natural language-based control of dynamic bipedal locomotion from the perspective of operational robustness and hardware safety. Existing work on natural language-based robot control has focused on episodic command execution for stable robot platforms, such as fixed-based manipulators in table-top scenarios. These scenarios feature non-overlapping phases of instruction and execution, with execution mishaps usually posing no threat to the robot safety. This allows for non-trivial failure rates to be acceptable. In contrast, our work involves indistinguishable instruction and execution stages for a dynamically unstable robot where execution failures can harm the robot. For example, interrupting a bipedal robot with a new instruction in certain states may cause it to fall. Our first contribution is to design and train a natural language-based controller for the bipedal robot Cassie that can take in new language commands at any time. Our second contribution is to introduce a protocol for evaluating the robustness to interruptions of such controllers and evaluating the learned controller in simulation under different interruption distributions. Our third contribution is to learn a detector for interruptions that are likely to lead to failure and to integrate that detector into a failure mitigation strategy. Overall, our results show that interruptions can lead to non-trivial failure rates for the original controller and that the proposed mitigation strategy can help to significantly reduce that rate.},
  archive   = {C_IROS},
  author    = {Ashish Malik and Stefan Lee and Alan Fern},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801602},
  month     = {10},
  pages     = {12394-12399},
  title     = {Interruptive language control of bipedal locomotion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive splitting of reusable temporal monitors for rare
traffic violations. <em>IROS</em>, 12386–12393. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous Vehicles (AVs) are often tested in simulation to estimate the probability they will violate safety specifications. Two common issues arise when using existing techniques to produce this estimation: If violations occur rarely, simple Monte-Carlo sampling techniques can fail to produce efficient estimates; if simulation horizons are too long, importance sampling techniques (which learn proposal distributions from past simulations) can fail to converge. This paper addresses both issues by interleaving rare-event sampling techniques with online specification monitoring algorithms. We use adaptive multilevel splitting to decompose simulations into partial trajectories, then calculate the distance of those partial trajectories to failure by leveraging robustness metrics from Signal Temporal Logic (STL). By caching those partial robustness metric values, we can efficiently re-use computations across multiple sampling stages. Our experiments on an interstate lane-change scenario show our method is viable for testing simulated AV-pipelines, efficiently estimating failure probabilities for STL specifications based on real traffic rules. We produce better estimates than Monte-Carlo and importance sampling in fewer simulations.},
  archive   = {C_IROS},
  author    = {Craig Innes and Subramanian Ramamoorthy},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802747},
  month     = {10},
  pages     = {12386-12393},
  title     = {Adaptive splitting of reusable temporal monitors for rare traffic violations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Safe reinforcement learning via hierarchical adaptive
chance-constraint safeguards. <em>IROS</em>, 12378–12385. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ensuring safety in Reinforcement Learning (RL), typically framed as a Constrained Markov Decision Process (CMDP), is crucial for real-world exploration applications. Current approaches in handling CMDP struggle to balance optimality and feasibility, as direct optimization methods can-not ensure state-wise in-training safety, and projection-based methods correct actions inefficiently through lengthy iterations. To address these challenges, we propose Adaptive Chance-constrained Safeguards (ACS), an adaptive, model-free safe RL algorithm using the safety recovery rate as a surrogate chance constraint to iteratively ensure safety during exploration and after achieving convergence. Theoretical analysis indicates that the relaxed probabilistic constraint sufficiently guarantees forward invariance to the safe set. And extensive experiments conducted on both simulated and real-world safety-critical tasks demonstrate its effectiveness in enforcing safety (nearly zero-violation) while preserving optimality (+23.8%), robustness, and fast response in stochastic real-world settings.},
  archive   = {C_IROS},
  author    = {Zhaorun Chen and Zhuokai Zhao and Tairan He and Binhao Chen and Xuhao Zhao and Liang Gong and Chengliang Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801338},
  month     = {10},
  pages     = {12378-12385},
  title     = {Safe reinforcement learning via hierarchical adaptive chance-constraint safeguards},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online efficient safety-critical control for mobile robots
in unknown dynamic multi-obstacle environments. <em>IROS</em>,
12370–12377. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a LiDAR-based goal-seeking and exploration framework, addressing the efficiency of online obstacle avoidance in unstructured environments populated with static and moving obstacles. This framework addresses two significant challenges associated with traditional dynamic control barrier functions (D-CBFs): their online construction and the diminished real-time performance caused by utilizing multiple D-CBFs. To tackle the first challenge, the framework’s perception component begins with clustering point clouds via the DBSCAN algorithm, followed by encapsulating these clusters with the minimum bounding ellipses (MBEs) algorithm to create elliptical representations. By comparing the current state of MBEs with those stored from previous moments, the differentiation between static and dynamic obstacles is realized, and the Kalman filter is utilized to predict the movements of the latter. Such analysis facilitates the D-CBF’s online construction for each MBE. To tackle the second challenge, we introduce buffer zones, generating Type-II D-CBFs online for each identified obstacle. Utilizing these buffer zones as activation areas substantially reduces the number of D-CBFs that need to be activated. Upon entering these buffer zones, the system prioritizes safety, autonomously navigating safe paths, and hence referred to as the exploration mode. Exiting these buffer zones triggers the system’s transition to goal-seeking mode. We demonstrate that the system’s states under this framework achieve safety and asymptotic stabilization. Experimental results in simulated and real-world environments have validated our framework’s capability, allowing a LiDAR-equipped mobile robot to efficiently and safely reach the desired location within dynamic environments containing multiple obstacles. Video and code are available: https://zyzhang4.wixsite.com/iros2024.},
  archive   = {C_IROS},
  author    = {Yu Zhang and Guangyao Tian and Long Wen and Xiangtong Yao and Liding Zhang and Zhenshan Bing and Wei He and Alois Knoll},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802727},
  month     = {10},
  pages     = {12370-12377},
  title     = {Online efficient safety-critical control for mobile robots in unknown dynamic multi-obstacle environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fixing symbolic plans with reinforcement learning in
object-based action spaces. <em>IROS</em>, 12363–12369. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning techniques are widely used when robots have to learn new tasks but they typically operate on action spaces defined by the joints of the robot. We present a contrasting approach where actions spaces are the trajectories of objects in the environment, requiring robots to discover events such as object changes and behaviors that must occur to accomplish the task. We show that this allows robots to learn faster, to learn semantic representations that can be communicated to humans, and to learn in a manner that does not depend on the robot itself, enabling low-cost policy transfer between different types of robots. Our demonstrations can be replicated using provided source code 1.},
  archive   = {C_IROS},
  author    = {Christopher Thierauf and Matthias Scheutz},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801362},
  month     = {10},
  pages     = {12363-12369},
  title     = {Fixing symbolic plans with reinforcement learning in object-based action spaces},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TacLink-integrated robot arm toward safe human-robot
interaction. <em>IROS</em>, 12356–12362. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent developments in vision-based tactile sensing offer a simple means to enable robots to perceive touch interactions. However, existing sensors are primarily designed for small-scale applications like robotic hands, lacking research on their integration for large-sized robot bodies that can be leveraged for safe human-robot interactions. This paper explores the utilization of the previously-developed vision-based tactile sensing link (called TacLink) with soft skin as a safety control mechanism, which can serve as an alternative to conventional rigid robot links and impact observers. We characterize the behavior of a robot integrated with the soft TacLink in response to collisions, particularly employing a reactive control strategy. The controller is primarily driven by tactile force information acquired from the soft TacLink sensor through a data-driven sim2real learning method. Compared with a standard rigid link, the results obtained from collision experiments also confirm the advantages of our &quot;soft&quot; solution in impact resilience and in facilitating controls that are difficult to achieve with a stiff robot body. This study can act as a benchmark for assessing the efficiency of soft tactile-sensitive skins in reactive collision responses and open new safety standards for soft skin-based collaborative robots in human-robot interaction scenarios.},
  archive   = {C_IROS},
  author    = {Quan Khanh Luu and Alessandro Albini and Perla Maiolino and Van Anh Ho},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802077},
  month     = {10},
  pages     = {12356-12362},
  title     = {TacLink-integrated robot arm toward safe human-robot interaction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual timing for sound source depth estimation in the wild.
<em>IROS</em>, 12348–12355. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Depth estimation enables a wide variety of 3D applications, such as robotics and autonomous driving. Despite significant work on various depth sensors, it is challenging to develop an all-in-one method to meet multiple basic criteria. In this paper, we propose a novel audio-visual learning scheme by integrating semantic features with physical spatial cues to boost monocular depth with only one microphone. Inspired by the flash-to-bang theory, we develop FBDepth, the first passive audio-visual depth estimation framework. It is based on the difference between the time-of-flight (ToF) of the light and the sound. We formulate sound source depth estimation as an audio-visual event localization task for collision events. To approach decimeter-level depth accuracy, we design a coarse-to-fine pipeline to push the temporary localization accuracy from event-level to millisecond-level by aligning audio-visual correspondence and manipulating optical flow. FBDepth feeds the estimated visual timestamp together with the audio clip and objects visual features to regress the source depth. We use a mobile phone to collect 3.6K+ video clips with 24 different objects at up to 65m. FBDepth shows superior performance especially at a long range compared to monocular and stereo methods.},
  archive   = {C_IROS},
  author    = {Wei Sun and Lili Qiu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802318},
  month     = {10},
  pages     = {12348-12355},
  title     = {Visual timing for sound source depth estimation in the wild},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AutoInst: Automatic instance-based segmentation of LiDAR 3D
scans. <em>IROS</em>, 12340–12347. (<a
href="https://doi.org/10.1109/IROS58592.2024.10803059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, progress in acquisition equipment such as LiDAR sensors has enabled sensing increasingly spacious outdoor 3D environments. Making sense of such 3D acquisitions requires fine-grained scene understanding, such as constructing instance-based 3D scene segmentations. Commonly, a neural network is trained for this task; however, this requires access to a large, densely annotated dataset, which is widely known to be challenging to obtain. To address this issue, in this work we propose to predict instance segmentations for 3D scenes in an unsupervised way, without relying on ground-truth annotations. To this end, we construct a learning framework consisting of two components: (1) a pseudo-annotation scheme for generating initial unsupervised pseudo-labels; and (2) a self-training algorithm for instance segmentation to fit robust, accurate instances from initial noisy proposals. To enable generating 3D instance mask proposals, we construct a weighted proxy-graph by connecting 3D points with edges integrating multi-modal image-and point-based self-supervised features, and perform graphcuts to isolate individual pseudo-instances. We then build on a state-of-the-art point-based architecture and train a 3D instance segmentation model, resulting in significant refinement of initial proposals. To scale to arbitrary complexity 3D scenes, we design our algorithm to operate on local 3D point chunks and construct a merging step to generate scene-level instance segmentations. Experiments on the challenging SemanticKITTI benchmark demonstrate the potential of our approach, where it attains 13.3% higher Average Precision and 9.1% higher F1 score compared to the best-performing baseline. The code is publicly available at https://github.com/artonson/autoinst.},
  archive   = {C_IROS},
  author    = {Cedric Perauer and Laurenz Adrian Heidrich and Haifan Zhang and Matthias Nießner and Anastasiia Kornilova and Alexey Artemov},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10803059},
  month     = {10},
  pages     = {12340-12347},
  title     = {AutoInst: Automatic instance-based segmentation of LiDAR 3D scans},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). RCAL: A lightweight road cognition and automated labeling
system for autonomous driving scenarios. <em>IROS</em>, 12332–12339. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vectorized reconstruction and topological cognition of road structures are crucial for autonomous vehicles to handle complex scenes. Traditional frameworks rely heavily on high-definition (HD) maps, which place significant demands on storage, computation, and manual labor. To overcome these limitations, we introduce a lightweight Road Cognition and Automated Labeling (RCAL) system. It leverages lightweight road data captured from mass-produced vehicles to vectorize road elements and cognize their topology. RCAL compiles multi-trip data on cloud servers for enhanced accuracy and coverage, addressing the limitations of single-trip data. In the field of element extraction, we proposed a pivotal point priority sampling strategy that can balance the contradiction between road scale and processing efficiency. Additionally, traffic flow is utilized to enhance the accuracy of road topology cognition. With its impressive automation, reliability, and efficiency, RCAL stands as an advanced solution in the field. Our evaluations on the intersection dataset from the real world confirm that RCAL not only achieves comparable precision to traditional HD map labeling systems but also substantially reducing resource costs.},
  archive   = {C_IROS},
  author    = {Jiancheng Chen and Chao Yu and Huayou Wang and Kun Liu and Yifei Zhan and Xianpeng Lang and Changliang Xue},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802247},
  month     = {10},
  pages     = {12332-12339},
  title     = {RCAL: A lightweight road cognition and automated labeling system for autonomous driving scenarios},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TivNe-SLAM: Dynamic mapping and tracking via time-varying
neural radiance fields. <em>IROS</em>, 12324–12331. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Previous attempts to integrate Neural Radiance Fields (NeRF) into the Simultaneous Localization and Mapping (SLAM) framework either rely on the assumption of static scenes or require the ground truth camera poses, which impedes their application in real-world scenarios. This paper proposes a time-varying representation to track and reconstruct the dynamic scenes. Firstly, two processes, a tracking process and a mapping process, are maintained simultaneously in our framework. In the tracking process, all input images are uniformly sampled and then progressively trained in a self-supervised paradigm. In the mapping process, we leverage motion masks to distinguish dynamic objects from the static background, and sample more pixels from dynamic areas. Secondly, the parameter optimization for both processes is comprised of two stages: the first stage associates time with 3D positions to convert the deformation field to the canonical field. The second stage associates time with the embeddings of the canonical field to obtain colors and a Signed Distance Function (SDF). Lastly, we propose a novel keyframe selection strategy based on the overlapping rate. Our approach is evaluated on two synthetic datasets and one real-world dataset, and the experiments validate that our method achieves competitive results in both tracking and mapping when compared to existing state-of-the-art NeRF-based dynamic SLAM systems.},
  archive   = {C_IROS},
  author    = {Chengyao Duan and Zhiliu Yang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802146},
  month     = {10},
  pages     = {12324-12331},
  title     = {TivNe-SLAM: Dynamic mapping and tracking via time-varying neural radiance fields},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). I-ASM: Iterative acoustic scene mapping for enhanced robot
auditory perception in complex indoor environments. <em>IROS</em>,
12318–12323. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the challenge of acoustic scene mapping (ASM) in complex indoor environments with multiple sound sources. Unlike existing methods that rely on prior data association or SLAM frameworks, we propose a novel particle filter-based iterative framework, termed I-ASM, for ASM using a mobile robot equipped with a microphone array and LiDAR. I-ASM harnesses an innovative &quot;implicit association&quot; to align sound sources with Direction of Arrival (DoA) observations without requiring explicit pairing, thereby streamlining the mapping process. Given inputs including an occupancy map, DoA estimates from various robot positions, and corresponding robot pose data, I-ASM performs multi-source mapping through an iterative cycle of &quot;Filtering-Clustering-Implicit Associating&quot;. The proposed framework has been tested in real-world scenarios with up to 10 concurrent sound sources, demonstrating its robustness against missing and false DoA estimates while achieving high-quality ASM results. To benefit the community, we open-source all the codes and data at https://github.com/AISLAB-sustech/Acoustic-Scene-Mapping},
  archive   = {C_IROS},
  author    = {Linya Fu and Yuanzheng He and Jiang Wang and Xu Qiao and He Kong},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802630},
  month     = {10},
  pages     = {12318-12323},
  title     = {I-ASM: Iterative acoustic scene mapping for enhanced robot auditory perception in complex indoor environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MOE: A dense LiDAR MOving event dataset, detection benchmark
and LeaderBoard. <em>IROS</em>, 12310–12317. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Detecting moving events produced by moving objects is a crucial task in the realms of autonomous driving and mobile robots. Moving objects have the potential to create ghost artifacts in mapped environments and pose risks to autonomous navigation. LiDAR serves as a vital sensor for autonomous systems due to its ability to provide dense and precise range measurements. However, existing LiDAR datasets often lack sufficient discussion on the motion labeling of moving objects, containing only a limited representation of moving entities within a single scene. Furthermore, the methodologies for Moving Event Detection (MED) on LiDAR sensors have not been comprehensively explored or evaluated. To address these gaps, this study focuses on constructing a diverse LiDAR moving event dataset encompassing multiple scenes with a high density of moving objects. A thorough review of current MED techniques is conducted, followed by the establishment of a performance benchmark based on evaluating these methods using our dataset. Additionally, part sequences of the dataset are utilized to host an online MED competition, aimed at fostering collaboration within the research community and advancing related studies.},
  archive   = {C_IROS},
  author    = {Zhiming Chen and Haozhe Fang and Jiapeng Chen and Michael Yu Wang and Hongyu Yu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802513},
  month     = {10},
  pages     = {12310-12317},
  title     = {MOE: A dense LiDAR MOving event dataset, detection benchmark and LeaderBoard},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active loop closure for OSM-guided robotic mapping in
large-scale urban environments. <em>IROS</em>, 12302–12309. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The autonomous mapping of large-scale urban scenes presents significant challenges for autonomous robots. To mitigate the challenges, global planning, such as utilizing prior GPS trajectories from OpenStreetMap (OSM), is often used to guide the autonomous navigation of robots for mapping. However, due to factors like complex terrain, unexpected body movement, and sensor noise, the uncertainty of the robot’s pose estimates inevitably increases over time, ultimately leading to the failure of robotic mapping. To address this issue, we propose a novel active loop closure procedure, enabling the robot to actively re-plan the previously planned GPS trajectory. The method can guide the robot to re-visit the previous places where the loop-closure detection can be performed to trigger the back-end optimization, effectively reducing errors and uncertainties in pose estimation. The proposed active loop closure mechanism is implemented and embedded into a real-time OSM-guided robot mapping framework. Empirical results on several large-scale outdoor scenarios demonstrate its effectiveness and promising performance.},
  archive   = {C_IROS},
  author    = {Wei Gao and Zezhou Sun and Mingle Zhao and Cheng-Zhong Xu and Hui Kong},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802364},
  month     = {10},
  pages     = {12302-12309},
  title     = {Active loop closure for OSM-guided robotic mapping in large-scale urban environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large-scale indoor mapping with failure detection and
recovery in SLAM. <em>IROS</em>, 12294–12301. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the failure detection and recovery problem in visual-inertial based Simultaneous Localization and Mapping (SLAM) systems for large-scale indoor environments. Camera and Inertial Measurement Unit (IMU) are popular choices for SLAM in many robotics tasks (e.g., navigation) due to their complementary sensing capabilities and low cost. However, vision has inherent challenges even in well-lit scenes, including motion blur, lack of features, or even accidental camera blockage. These failures can cause drifts to accumulate over time and can severely impact the scalability of existing solutions to large areas. To address these issues, we propose an automatic map generation service with (i) a failure detection method based on visual feature tracking quality using a health tracker which identifies and discards faulty measurements and (ii) a continuous session merging approach in SLAM. Taken together, this allows us to handle erroneous data without any manual intervention, and allows us to scale to extremely large spaces. The proposed system has been validated on benchmark datasets. Also, experimental results on multiple custom large-scale grocery stores, each between 1700 m2 to 3700 m2, and duration 60 to 80 minutes, are presented. Our approach shows the lowest error in all large-scale SLAM cases when compared with state-of-the-art visual-inertial SLAM packages, which often produce highly erroneous trajectories or lose track. Additionally, we provide dense 3D reconstruction with the presence of a depth camera by simply registering the point cloud from RGB-D image with respect to the SLAM generated trajectory – and the quality of the reconstruction illustrates the efficacy of our proposed method.},
  archive   = {C_IROS},
  author    = {Sharmin Rahman and Robert DiPietro and Dharanish Kedarisetti and Vinod Kulathumani},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802593},
  month     = {10},
  pages     = {12294-12301},
  title     = {Large-scale indoor mapping with failure detection and recovery in SLAM},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MM-gaussian: 3D gaussian-based multi-modal fusion for
localization and reconstruction in unbounded scenes. <em>IROS</em>,
12287–12293. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Localization and mapping are critical tasks for various applications such as autonomous vehicles and robotics. The challenges posed by outdoor environments present particular complexities due to their unbounded characteristics. In this work, we present MM-Gaussian, a LiDAR-camera multimodal fusion system for localization and mapping in unbounded scenes. Our approach is inspired by the recently developed 3D Gaussians, which demonstrate remarkable capabilities in achieving high rendering quality and fast rendering speed. Specifically, our system fully utilizes the geometric structure information provided by solid-state LiDAR to address the problem of inaccurate depth encountered when relying solely on visual solutions in unbounded, outdoor scenarios. Additionally, we utilize 3D Gaussian point clouds, with the assistance of pixel-level gradient descent, to fully exploit the color information in photos, thereby achieving realistic rendering effects. To further bolster the robustness of our system, we designed a relocalization module, which assists in returning to the correct trajectory in the event of a localization failure. Experiments conducted in multiple scenarios demonstrate the effectiveness of our method.},
  archive   = {C_IROS},
  author    = {Chenyang Wu and Yifan Duan and Xinran Zhang and Yu Sheng and Jianmin Ji and Yanyong Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801605},
  month     = {10},
  pages     = {12287-12293},
  title     = {MM-gaussian: 3D gaussian-based multi-modal fusion for localization and reconstruction in unbounded scenes},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DeepMIF: Deep monotonic implicit fields for large-scale
LiDAR 3D mapping. <em>IROS</em>, 12279–12286. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, significant progress has been achieved in sensing real large-scale outdoor 3D environments, particularly by using modern acquisition equipment such as LiDAR sensors. Unfortunately, they are fundamentally limited in their ability to produce dense, complete 3D scenes. To address this issue, recent learning-based methods integrate neural implicit representations and optimizable feature grids to approximate surfaces of 3D scenes. However, naively fitting samples along raw LiDAR rays leads to noisy 3D mapping results due to the nature of sparse, conflicting LiDAR measurements. Instead, in this work we depart from fitting LiDAR data exactly, instead letting the network optimize a non-metric monotonic implicit field defined in 3D space. To fit our field, we design a learning system integrating a monotonicity loss that enables optimizing neural monotonic fields and leverages recent progress in large-scale 3D mapping. Our algorithm achieves high-quality dense 3D mapping performance as cap¬tured by multiple quantitative and perceptual measures and visual results obtained for Mai City, Newer College, and KITTI benchmarks. The code of our approach is publicly available at https://github.com/artonson/deepmif.},
  archive   = {C_IROS},
  author    = {Kutay Yılmaz and Matthias Nießner and Anastasiia Kornilova and Alexey Artemov},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801519},
  month     = {10},
  pages     = {12279-12286},
  title     = {DeepMIF: Deep monotonic implicit fields for large-scale LiDAR 3D mapping},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Long-term map-maintenance in changing environments using
ray-bundle-impact-factor estimation. <em>IROS</em>, 12271–12278. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ensuring an accurate and robust localization is one of the most significant problems in the field of mobile robotics. In this context, map-based localization methods utilizing 3D LiDARS for environmental perception are widely used. Even if there exist multiple promising techniques in this field, the majority of approaches can only guarantee an accurate and robust operation if there is no deviation between the map and the real surroundings. Consequently, state of the art localization methods frequently suffer from unreliable results or even complete failure in the case of changing environments. In this paper, we propose an efficient technique for a precise and robust maintenance of localization maps through real-time incorporation of 3D LiDAR scans. Our map update procedure is based on a novel way of estimating the interference between laserbeams and map contents, denoted as Ray-Bundle-Impact-Factor (RBIF). Our technique additionally solves the widespread problem of disruptive hole creation caused by discretization effects. Experiments on real-world as well as synthetic data demonstrate the precision and stability of our method under various challenging conditions and evaluate our approach in comparison to multiple SOTA map maintenance algorithms.},
  archive   = {C_IROS},
  author    = {Matthias Breitfuss and Marcus Geimer and Christoph Gruber},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802029},
  month     = {10},
  pages     = {12271-12278},
  title     = {Long-term map-maintenance in changing environments using ray-bundle-impact-factor estimation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LCP-fusion: A neural implicit SLAM with enhanced local
constraints and computable prior. <em>IROS</em>, 12263–12270. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently the dense Simultaneous Localization and Mapping (SLAM) based on neural implicit representation has shown impressive progress in hole filling and high-fidelity mapping. Nevertheless, existing methods either heavily rely on known scene bounds or suffer inconsistent reconstruction due to drift in potential loop-closure regions, or both, which can be attributed to the inflexible representation and lack of local constraints. In this paper, we present LCP-Fusion, a neural implicit SLAM system with enhanced local constraints and computable prior, which takes the sparse voxel octree structure containing feature grids and SDF priors as hybrid scene representation, enabling the scalability and robustness during mapping and tracking. To enhance the local constraints, we propose a novel sliding window selection strategy based on visual overlap to address the loop-closure, and a practical warping loss to constrain relative poses. Moreover, we estimate SDF priors as coarse initialization for implicit features, which brings additional explicit constraints and robustness, especially when a light but efficient adaptive early ending is adopted. Experiments demonstrate that our method achieve better localization accuracy and reconstruction consistency than existing RGB-D implicit SLAM, especially in challenging real scenes (ScanNet) as well as self-captured scenes with unknown scene bounds. The code is available at https://github.com/laliwang/LCP-Fusion.},
  archive   = {C_IROS},
  author    = {Jiahui Wang and Yinan Deng and Yi Yang and Yufeng Yue},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802626},
  month     = {10},
  pages     = {12263-12270},
  title     = {LCP-fusion: A neural implicit SLAM with enhanced local constraints and computable prior},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep sensor fusion with constraint safety bounds for high
precision localization. <em>IROS</em>, 12256–12262. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In mobile robotics, particularly in autonomous driving, localization is one of the key challenges for navigation and planning. For safe operation in the open world where vulnerable participants are present, precise and guaranteed safe localization is required. While current classical fusion approaches are safe due to provably bounded closed-form formulation, their situation-adaptivity is limited. In contrast, data-driven approaches are situation-adaptive based on the underlying training data but unbounded and unsafe. In our work, we propose a novel data-driven but provably bounded sensor fusion and apply it to mobile robotic localization. In extensive experiments using an autonomous driving test vehicle, we show that our fusion method outperforms other safe fusion approaches.},
  archive   = {C_IROS},
  author    = {Sebastian Schmidt and Ludwig Stumpp and Diego Valverde and Stephan Günnemann},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802242},
  month     = {10},
  pages     = {12256-12262},
  title     = {Deep sensor fusion with constraint safety bounds for high precision localization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EgoVM: Achieving precise ego-localization using lightweight
vectorized maps. <em>IROS</em>, 12248–12255. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate and reliable ego-localization is critical for autonomous driving. In this paper, we present EgoVM, an end-to-end localization network that achieves comparable localization accuracy to prior state-of-the-art methods, but uses lightweight vectorized maps instead of heavy point-based maps. To begin with, we extract BEV features from online multi-view images and LiDAR point cloud. Then, we employ a set of learnable semantic embeddings to encode the semantic types of map elements and supervise them with semantic segmentation, to make their feature representation consistent with BEV features. After that, we feed map queries, composed of learnable semantic embeddings and coordinates of map elements, into a transformer decoder to perform cross-modality matching with BEV features. Finally, we adopt a robust histogram-based pose solver to estimate the optimal pose by searching exhaustively over candidate poses. We comprehensively validate the effectiveness of our method using both the nuScenes dataset and a newly collected dataset. The experimental results show that our method achieves centimeter-level localization accuracy, and outperforms existing methods using vectorized maps by a large margin. Furthermore, our model has been extensively tested in a large fleet of autonomous vehicles under various challenging urban scenes.},
  archive   = {C_IROS},
  author    = {Yuzhe He and Shuang Liang and Xiaofei Rui and Chengying Cai and Guowei Wan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801387},
  month     = {10},
  pages     = {12248-12255},
  title     = {EgoVM: Achieving precise ego-localization using lightweight vectorized maps},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Backpropagation-based analytical derivatives of EKF
covariance for active sensing. <em>IROS</em>, 12240–12247. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To enhance accuracy of robot state estimation, active sensing (or perception-aware) methods seek trajectories that maximize the information gathered by the sensors. To this aim, one possibility is to seek trajectories that minimize the estimation error covariance matrix output by an extended Kalman filter (EKF), w.r.t. its control inputs over a given horizon. However, this is computationally demanding. In this article, we derive novel backpropagation analytical formulas for the derivatives of the covariance matrices of an EKF w.r.t. all its inputs. We then leverage the obtained analytical gradients as an enabling technology to derive perception-aware optimal motion plans. Simulations validate the approach, showcasing improvements in execution time, notably over PyTorch’s automatic differentiation. Experimental results on a real vehicle also support the method.},
  archive   = {C_IROS},
  author    = {Jonas Benhamou and Silvère Bonnabel and Camille Chapdelaine},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801586},
  month     = {10},
  pages     = {12240-12247},
  title     = {Backpropagation-based analytical derivatives of EKF covariance for active sensing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). PS-loc: Robust LiDAR localization with prior structural
reference. <em>IROS</em>, 12234–12239. (<a
href="https://doi.org/10.1109/IROS58592.2024.10803104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Prior structural reference like floor plan is readily accessible in indoor scene, which exhibits the potential of improving localization quality without the requirements of a previously-built high-precision map. This paper introduces a novel optimal transport-based framework for prior structural reference-based localization, aiming to improve the robustness for the robot localization. Leveraging the spacial relations of structures, a matching method based on optimal transport theory is proposed and it improves the robustness of matching results in dynamic scene and rapid rotation conditions. Additionally, this paper handles metric inaccuracies in the known structural reference by implementing an prior guided plane adjustment-based updating strategy. This strategy combines prior and observational information to jointly optimize the structural information within a sliding window. The performance of the framework is validated through real-world experiments, demonstrating superior accuracy and robustness to disturbances from dynamic occlusion and rapid rotation compared to common state-of-the-art SLAM and localization methods.},
  archive   = {C_IROS},
  author    = {Rui Li and Wentao Zhao and Tianchen Deng and Yanbo Wang and Jingchuan Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10803104},
  month     = {10},
  pages     = {12234-12239},
  title     = {PS-loc: Robust LiDAR localization with prior structural reference},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A tetherless soft robotic wearable haptic human machine
interface for robot teleoperation. <em>IROS</em>, 12226–12233. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work describes the development, demonstration, and performance evaluation study of a wearable human machine interface for robotic teleoperation. We present a novel tetherless human machine interface in the form of a backpack, wearable 3D arm motion capture sensors, finger flexion sensors, and pneumatic haptic feedback muscles. The system is integrated in a complete teleoperation framework, enabling users to be immersed in a remote environment through virtual reality headgear, facilitating intuitive manipulation of an industrial articulated arm. The human machine interface samples the kinematic configuration of the user’s arm, hand, and fingers using multiple inertial measurement units and capacitive sensors respectively, and streams it to the teleoperation software stack. The gripping forces experienced at the robot’s end-effector are acquired using a custom three-dimensional Hall-effect magnetic sensor. The system simultaneously renders the kinesthetic and tactile feedback on the user’s fingers through custom designed pneumatically actuated soft robotic haptic muscles. The efficacy of the human machine interface and the teleoperation system was tested and evaluated by conducting user studies, which showed 31.4% faster teleoperation compared to a keypad controller, and 60% less gripping force utilized with haptics enabled. The findings of the study guided the design and prototype development of a printed electronics based stretchable sleeve and glove motion capture unit to improve the portability, ergonomics, and user experience of the human machine interface.},
  archive   = {C_IROS},
  author    = {Shilpa Thakur and Nathalia Diaz Armas and Joseph Adegite and Ritwik Pandey and Joey Mead and Pratap M. Rao and Cagdas D. Onal},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802410},
  month     = {10},
  pages     = {12226-12233},
  title     = {A tetherless soft robotic wearable haptic human machine interface for robot teleoperation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An optimization-based scheme for real-time transfer of human
arm motion to robot arm. <em>IROS</em>, 12220–12225. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Performing human-like motion is crucial for service humanoid robots. Real-time motion retargeting allows clear observation of the robot’s pose and provides instant feedback during human demonstrator actions. This paper presents an optimization-based real-time anthropomorphic motion retargeting framework for transferring human arm motion to a robot arm. The framework is generic, applicable to both spherical-rotational-spherical (SRS) and non-SRS robot arms. We introduce the normalized normal vector of the arm plane as an anthropomorphic criterion within our framework. The method is validated on a service humanoid robot, with both static and dynamic evaluations. The statistical analysis show that our method maintains strong anthropomorphic features while ensuring accurate wrist pose tracking.},
  archive   = {C_IROS},
  author    = {Zhelin Yang and Seongjin Bien and Simone Nertinger and Abdeldjallil Naceri and Sami Haddadin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802348},
  month     = {10},
  pages     = {12220-12225},
  title     = {An optimization-based scheme for real-time transfer of human arm motion to robot arm},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning-based delay compensation framework for
teleoperated wheeled rovers on soft terrains. <em>IROS</em>,
12212–12219. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The difficulties posed by terrain-induced slippage for wheeled rovers traversing soft terrains are critical to ensuring safe and precise mobility. While bilateral teleoperation systems offer a promising solution to this issue, the inherent network-induced delays hinder the fidelity of the closed-loop integration, potentially compromising teleoperator system controls, and resulting in poor command-tracking performance. This work introduces a new model-free predictor framework based on deep learning designed to improve prediction performance and effectively compensate for large network delays in teleoperated wheeled rovers. Our approach employs the Recurrent Neural Network (RNN) to achieve a significant improvement in modeling complexity and prediction accuracy. Particularly, our framework consists of two distinct predictors, each tailored to the forward and backward coupling variables of the teleoperated wheeled rover. Human-in-the-loop experiments were conducted to validate the effectiveness of the developed framework in compensating for the delays encountered by teleoperated wheeled rovers coupled with terrain-induced slippage. The results confirm the improved prediction accuracy of the framework. This improvement is evidenced by improved performance and transparency metrics, which lead to better command-tracking performance. A supplementary video is available at https://youtu.be/-06UGumQ0tA.},
  archive   = {C_IROS},
  author    = {Ahmad Abubakar and Yahya Zweiri and Mubarak Yakubu and Ruqayya Alhammadi and Mohammed Mohiuddin and Abdel Gafoor Haddad and Jorge Dias and Lakmal Seneviratne},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802432},
  month     = {10},
  pages     = {12212-12219},
  title     = {Deep learning-based delay compensation framework for teleoperated wheeled rovers on soft terrains},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring cognitive load dynamics in human-machine
interaction for teleoperation: A user-centric perspective on remote
operation system design. <em>IROS</em>, 12204–12211. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Teleoperated robots, especially in hazardous environments, integrate human cognition with machine efficiency, but can increase cognitive load, causing stress and reducing task performance and safety. This study examines the impact of the information available to the operator on cognitive load, physiological responses (e.g., GSR, blinking, facial temperature), and performance during teleoperation in three conditions: C1 - in presence, C2 - remote with Visual feedback, and C3 - remote with telepresence robot. The findings from our user study involving 20 participants show that information availability significantly impacts perceived cognitive load, as evidenced by the differences observed between conditions in our analysis. Furthermore, the results indicated that blinking rates varied significantly among the conditions. The results also underline that individuals with higher error scores on the spatial orientation test (SOT), reflecting lower spatial ability, are more likely to experience failure in conditions 2 and 3. The results show that information availability significantly affects cognitive load and teleoperation performance, especially depth perception of the robot’s actions. Additionally, the thermal and GSR data findings indicate an increase in stress and anxiety levels when operators perform conditions 2 and 3, thus corroborating an increase in the user’s cognitive load.},
  archive   = {C_IROS},
  author    = {Juan Jose Garcia Cardenas and Xiaoxuan Hei and Adriana Tapus},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802226},
  month     = {10},
  pages     = {12204-12211},
  title     = {Exploring cognitive load dynamics in human-machine interaction for teleoperation: A user-centric perspective on remote operation system design},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). User-customizable shared control for robot teleoperation via
virtual reality. <em>IROS</em>, 12196–12203. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Shared control can ease and enhance a human operator’s ability to teleoperate robots, particularly for intricate tasks demanding fine control over multiple degrees of freedom. However, the arbitration process dictating how much autonomous assistance to administer in shared control can confuse novice operators and impede their understanding of the robot’s behavior. To overcome these adverse side-effects, we propose a novel formulation of shared control that enables operators to tailor the arbitration to their unique capabilities and preferences. Unlike prior approaches to &quot;customizable&quot; shared control where users could indirectly modify the latent parameters of the arbitration function by issuing a feedback command, we instead make these parameters observable and directly editable via a virtual reality (VR) interface. We present our user-customizable shared control method for a teleoperation task in SE(3), known as the buzz wire game. A user study is conducted with participants teleoperating a robotic arm in VR to complete the game. The experiment spanned two weeks per subject to investigate longitudinal trends. Our findings reveal that users allowed to interactively tune the arbitration parameters across trials generalize well to adaptations in the task, exhibiting improvements in precision and fluency over direct teleoperation and conventional shared control.},
  archive   = {C_IROS},
  author    = {Rui Luo and Mark Zolotas and Drake Moore and Taşkın Padır},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802544},
  month     = {10},
  pages     = {12196-12203},
  title     = {User-customizable shared control for robot teleoperation via virtual reality},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluation of predictive display for teleoperated driving
using CARLA simulator. <em>IROS</em>, 12190–12195. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Before the world-wide deployment of autonomous vehicles, it is essential to implement intermediate solutions with partial autonomy. One such solution is the use of vehicle teleoperation, the act of controlling a vehicle from a distance. In real time applications of teleoperation, it is often pertinent to use augmented reality components within the teleoperator view, which are referred to as a predictive display. In this work, we evaluate our predictive display method, which is a guiding path based on the free space in the environment. The path is generated based on our Dual Transformer Network (DTNet), which uses both object detection and lane semantic segmentation to define the free space in the environment. While the model has previously performed well on image data, it is necessary to observe its accuracy in the presence of time delay and packet loss, to assess its performance in a real-time setting. Thus, in this work, we use CARLA simulator to compare the detected free space on the teleoperator side to the true free space on the vehicle side across different values of time delay and packet loss. Under optimal network conditions, our model yielded a remarkable 87.9% DSC score and 81.3% IoU score. Defining our minimum performance threshold as 80% DSC and 70% IoU, we conclude that our model can effectively mitigate the challenges of time delay below 100ms and packet loss below 1%, both of which represent substantial tolerances.},
  archive   = {C_IROS},
  author    = {Fatima Kashwani and Bilal Hassan and Peng-Yong Kong and Majid Khonji and Jorge Dias},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801533},
  month     = {10},
  pages     = {12190-12195},
  title     = {Evaluation of predictive display for teleoperated driving using CARLA simulator},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 6D variable virtual fixtures for telemanipulated insertion
tasks. <em>IROS</em>, 12183–12189. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Telemanipulation enables humans to perform tasks in dangerous environments without exposing them to any risk. The COVID-19 pandemic sadly showed, that these environments can also include the treatment and interaction with infected patients. Since human-robot interactions demand for low interaction forces yet high precision, telemanipulation often results in a high mental workload for the operator. To overcome this, we present a virtual guidance approach to perform telemanipulated insertion tasks. A nasopharyngeal swap sampling procedure is taken as use case. We extend our previously presented approach by adding an additional position fixture, introducing distance-dependent variable stiffness values and guaranteeing stability using energy tanks. Based on RGB-D data, the operator is guided towards a desirable insertion line while approaching the nostril. The distance-dependent stiffness values increase the smoothness of the fixture. Since variable stiffness values can result in unstable behavior, energy tanks for the fixtures are introduced. Experiments show the improvements compared to our previous approach. Further, a comparison between guided and unguided samplings performed by an expert user gives a first impression of the improvements resulting from the fixture.},
  archive   = {C_IROS},
  author    = {Stephan Andreas Schwarz and Ulrike Thomas},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801971},
  month     = {10},
  pages     = {12183-12189},
  title     = {6D variable virtual fixtures for telemanipulated insertion tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Immersive human-in-the-loop control: Real-time 3D surface
meshing and physics simulation. <em>IROS</em>, 12176–12182. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces the TactiMesh Teleoperator Interface (TTI), a novel predictive visual and haptic system designed explicitly for human-in-the-loop robot control using a head-mounted display (HMD). By employing simultaneous localization and mapping (SLAM) in tandem with a space carving method (CARV), TTI creates a real-time 3D surface mesh of remote environments from an RGB camera mounted on a Barrett WAM arm. The generated mesh is integrated into a physics simulator, featuring a digital twin of the WAM robot arm to create a virtual environment. In this virtual environment, TTI provides haptic feedback directly in response to the operator’s movements, eliminating the problem with delayed response from the haptic follower robot. Furthermore, texturing the 3D mesh with keyframes from SLAM allows the operator to control the viewpoint of their Head Mounted Display (HMD) independently of the arm-mounted robot camera, giving a better visual immersion and improving manipulation speed. Incorporating predictive visual and haptic feedback significantly improves tele-operation in applications such as search and rescue, inspection, and remote maintenance.},
  archive   = {C_IROS},
  author    = {Sait Akturk and Justin Valentine and Junaid Ahmad and Martin Jagersand},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802748},
  month     = {10},
  pages     = {12176-12182},
  title     = {Immersive human-in-the-loop control: Real-time 3D surface meshing and physics simulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Development of a bilateral control teleoperation system for
bipedal humanoid robot utilizing foot sole haptics feedback *.
<em>IROS</em>, 12170–12175. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Teleoperating bipedal humanoid robots presents unique challenges, including decreased stability and reduced operator presence. This paper addresses these challenges by proposing a method that leverages the operator’s inherent sense of stability by feedback from a sole haptics display to operate a bipedal humanoid robot. We developed a bilateral control system that integrates a device replicating sole haptics feedback and provides the operator with feedback on changes in the robot’s center of gravity. We conducted operating experiments in the forward-backward direction to evaluate its effectiveness and investigate the effectiveness of sole haptics on robot operation. The results demonstrate that operating with both vision and sole haptics feedback significantly reduces the robot’s fall rate by over 56% when disturbances are applied, compared to using only vision feedback. Moreover, operators reported a 21% higher sense of presence with both vision and sole haptics feedback compared to using only vision feedback.},
  archive   = {C_IROS},
  author    = {Yang Shen and Masanobu Kanazawa and Kazuki Mori and Ryu Isono and Yuri Nakazawa and Atsuo Takanishi and Takuya Otani},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801625},
  month     = {10},
  pages     = {12170-12175},
  title     = {Development of a bilateral control teleoperation system for bipedal humanoid robot utilizing foot sole haptics feedback *},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time dexterous telemanipulation with an
end-effect-oriented learning-based approach. <em>IROS</em>, 12164–12169.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dexterous telemanipulation is crucial in advancing human-robot systems, especially in tasks requiring precise and safe manipulation. However, it faces significant challenges due to the physical differences between human and robotic hands, the dynamic interaction with objects, and the indirect control and perception of the remote environment. Current approaches predominantly focus on mapping the human hand onto robotic counterparts to replicate motions, which exhibits a critical oversight: it often neglects the physical interaction with objects and relegates the interaction burden to the human to adapt and make laborious adjustments in response to the indirect and counter-intuitive observation of the remote environment. This work develops an End-Effects-Oriented Learning-based Dexterous Telemanipulation (EFOLD) framework to address telemanipulation tasks. EFOLD models telemanipulation as a Markov Game, introducing multiple end-effect features to interpret the human operator’s commands during interaction with objects. These features are used by a Deep Reinforcement Learning policy to control the robot and reproduce such end effects. EFOLD was evaluated with real human subjects and two end-effect extraction methods for controlling a virtual Shadow Robot Hand in telemanipulation tasks. EFOLD achieved real-time control capability with low command following latency (delay&lt;0.11s) and highly accurate tracking (MSE&lt;0.084 rad).},
  archive   = {C_IROS},
  author    = {Haoyang Wang and He Bai and Xiaoli Zhang and Yunsik Jung and Michel Bowman and Lingfeng Tao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801694},
  month     = {10},
  pages     = {12164-12169},
  title     = {Real-time dexterous telemanipulation with an end-effect-oriented learning-based approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GELLO: A general, low-cost, and intuitive teleoperation
framework for robot manipulators. <em>IROS</em>, 12156–12163. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans can teleoperate robots to accomplish complex manipulation tasks. Imitation learning has emerged as a powerful framework that leverages human teleoperated demonstrations to teach robots new skills. However, the performance of the learned policies is bottlenecked by the quality, scale, and variety of the demonstration data. In this paper, we aim to lower the barrier to collecting large and high-quality human demonstration data by proposing a GEneraL framework for building LOw-cost and intuitive teleoperation systems for robotic manipulation (GELLO). Given a target robot arm, we build a GELLO controller device that has the same kinematic structure as the target arm, leveraging 3D-printed parts and economical off-the-shelf motors. GELLO is easy to build and intuitive to use. Through an extensive user study, we show that GELLO enables more reliable and efficient demonstration collection compared to other cost efficient teleoperation devices commonly used in the imitation learning literature such as virtual reality controllers and 3D spacemouses. We further demonstrate the capabilities of GELLO for performing complex bi-manual and contact-rich manipulation tasks. To make GELLO accessible to everyone, we have designed and built GELLO systems for 3 commonly used robotic arms: Franka, UR5, and xArm. All software and hardware are open-sourced and can be found on our website: https://wuphilipp.github.io/gello/.},
  archive   = {C_IROS},
  author    = {Philipp Wu and Yide Shentu and Zhongke Yi and Xingyu Lin and Pieter Abbeel},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801581},
  month     = {10},
  pages     = {12156-12163},
  title     = {GELLO: A general, low-cost, and intuitive teleoperation framework for robot manipulators},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Local linearity is all you need (in data-driven
teleoperation). <em>IROS</em>, 12148–12155. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One of the critical aspects of assistive robotics is to provide a control system of a high-dimensional robot from a low-dimensional user input (i.e. a 2D joystick). Data-driven teleoperation seeks to provide an intuitive user interface called an action map to map the low dimensional input to robot velocities from human demonstrations. Action maps are machine learning models trained on robotic demonstration data to map user input directly to desired movements as opposed to aspects of robot pose (&quot;move to cup or pour content&quot; vs. &quot;move along x- or y-axis&quot;). Many works have investigated nonlinear action maps with multi-layer perceptrons, but recent work suggests that local-linear neural approximations provide better control of the system. However, local linear models assume actions exist on a linear subspace and may not capture nuanced motions in training data. In this work, we hypothesize that local-linear neural networks are effective because they make the action map odd w.r.t. the user input, enhancing the intuitiveness of the controller. Based on this assumption, we propose two nonlinear means of encoding odd behavior that do not constrain the action map to a local linear function. However, our analysis reveals that these models effectively behave like local linear models for relevant mappings between user joysticks and robot movements. We support this claim in simulation, and show on a realworld use case that there is no statistical benefit of using non-linear maps, according to the users experience. These negative results suggest that further investigation into model architectures beyond local linear models may offer diminishing returns for improving user experience in data-driven teleoperation systems.},
  archive   = {C_IROS},
  author    = {Michael Przystupa and Gauthier Gidel and Matthew E. Taylor and Martin Jagersand and Justus Piater and Samuele Tosatto},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802052},
  month     = {10},
  pages     = {12148-12155},
  title     = {Local linearity is all you need (in data-driven teleoperation)},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SMART-LLM: Smart multi-agent robot task planning using large
language models. <em>IROS</em>, 12140–12147. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we introduce SMART-LLM, an innovative framework designed for embodied multi-robot task planning. SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models (LLMs), harnesses the power of LLMs to convert high-level task instructions provided as input into a multi-robot task plan. It accomplishes this by executing a series of stages, including task decomposition, coalition formation, and task allocation, all guided by programmatic LLM prompts within the few-shot prompting paradigm. We create a benchmark dataset designed for validating the multi-robot task planning problem, encompassing four distinct categories of high-level instructions that vary in task complexity. Our evaluation experiments span both simulation and real-world scenarios, demonstrating that the proposed model can achieve promising results for generating multi-robot task plans. The experimental videos, code, and datasets from the work can be found at https://sites.google.com/view/smart-llm/.},
  archive   = {C_IROS},
  author    = {Shyam Sundar Kannan and Vishnunandan L. N. Venkatesh and Byung-Cheol Min},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802322},
  month     = {10},
  pages     = {12140-12147},
  title     = {SMART-LLM: Smart multi-agent robot task planning using large language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sequential discrete action selection via blocking conditions
and resolutions. <em>IROS</em>, 12132–12139. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we introduce a strategy that frames the sequential action selection problem for robots in terms of resolving blocking conditions, i.e., situations that impede progress on an action en route to a goal. This strategy allows a robot to make one-at-a-time decisions that take in pertinent contextual information and swiftly adapt and react to current situations. We present a first instantiation of this strategy that combines a state-transition graph and a zero-shot Large Language Model (LLM). The state-transition graph tracks which previously attempted actions are currently blocked and which candidate actions may resolve existing blocking conditions. This information from the state-transition graph is used to automatically generate a prompt for the LLM, which then uses the given context and set of possible actions to select a single action to try next. This selection process is iterative, with each chosen and executed action further refining the state-transition graph, continuing until the agent either fulfills the goal or encounters a termination condition. We demonstrate the effectiveness of our approach by comparing it to various LLM and traditional task-planning methods in a testbed of simulation experiments. We discuss the implications of our work based on our results.},
  archive   = {C_IROS},
  author    = {Liam Merz Hoffmeister and Brian Scassellati and Daniel Rakita},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802019},
  month     = {10},
  pages     = {12132-12139},
  title     = {Sequential discrete action selection via blocking conditions and resolutions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DoReMi: Grounding language model by detecting and recovering
from plan-execution misalignment. <em>IROS</em>, 12124–12131. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large language models (LLMs) encode a vast amount of semantic knowledge and possess remarkable understanding and reasoning capabilities. Previous work has explored how to ground LLMs in robotic tasks to generate feasible and executable textual plans. However, low-level execution in the physical world may deviate from the high-level textual plan due to environmental perturbations or imperfect controller design. In this paper, we propose DoReMi, a novel language model grounding framework that enables immediate Detection and Recovery from Misalignments between plan and execution. Specifically, we leverage LLMs to play a dual role, aiding not only in high-level planning but also generating constraints that can indicate misalignment during execution. Then vision language models (VLMs) are utilized to detect constraint violations continuously. Our pipeline can monitor the low-level execution and enable timely recovery if certain plan-execution misalignment occurs. Experiments on various complex tasks including robot arms and humanoid robots demonstrate that our method can lead to higher task success rates and shorter task completion times.},
  archive   = {C_IROS},
  author    = {Yanjiang Guo and Yen-Jen Wang and Lihan Zha and Jianyu Chen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802284},
  month     = {10},
  pages     = {12124-12131},
  title     = {DoReMi: Grounding language model by detecting and recovering from plan-execution misalignment},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NLNS-MASPF for solving multi-agent scheduling and
path-finding. <em>IROS</em>, 12116–12123. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we propose a novel method, NLNS-MASPF, to solve the Multi-Agent Scheduling and Pathfinding (MASPF) problem. The problem exhibits a bi-level structure, consisting of High-level Scheduling and Low-level Pathfinding. Our method applies a graph neural network in the high-level scheduling process and utilizes a MAPF solver with a schedule segmenting technique in the low-level pathfinding process. Through these approaches, NLNS-MASPF has experimentally demonstrated superior performance compared to the previous state-of-the-art MASPF algorithm, LNS-PBS, in solving the MASPF problem.},
  archive   = {C_IROS},
  author    = {Heemang Park and Kyuree Ahn and Jinkyoo Park},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802133},
  month     = {10},
  pages     = {12116-12123},
  title     = {NLNS-MASPF for solving multi-agent scheduling and path-finding},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reactive temporal logic-based planning and control for
interactive robotic tasks. <em>IROS</em>, 12108–12115. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots interacting with humans must be safe, reactive and adapt online to unforeseen environmental and task changes. Achieving these requirements concurrently is a challenge as interactive planners lack formal safety guarantees, while safe motion planners lack flexibility to adapt. To tackle this, we propose a modular control architecture that generates both safe and reactive motion plans for human-robot interaction by integrating temporal logic-based discrete task level plans with continuous Dynamical System (DS)-based motion plans. We formulate a reactive temporal logic formula that enables users to define task specifications through structured language, and propose a planning algorithm at the task level that generates a sequence of desired robot behaviors while being adaptive to environmental changes. At the motion level, we incorporate control Lyapunov functions and control barrier functions to compute stable and safe continuous motion plans for two types of robot behaviors: (i) complex, possibly periodic motions given by autonomous DS and (ii) time-critical tasks specified by Signal Temporal Logic (STL). Our methodology is demonstrated on the Franka robot arm performing wiping tasks on a whiteboard and a mannequin that is compliant to human interactions and adaptive to environmental changes.},
  archive   = {C_IROS},
  author    = {Farhad Nawaz and Shaoting Peng and Lars Lindemann and Nadia Figueroa and Nikolai Matni},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802335},
  month     = {10},
  pages     = {12108-12115},
  title     = {Reactive temporal logic-based planning and control for interactive robotic tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient target singulation with multi-fingered gripper
using propositional logic. <em>IROS</em>, 12101–12107. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When multiple tablewares are closely packed on a table, rearranging obstacles to make space is necessary to grasp the target, often called target singulation. Due to the nature of handling fragile tablewares (i.e. plates, bowls), we make a few assumptions for the target singulation problem. First, tableware is grasped with a multi-fingered gripper; second, rearrangement is based on prehensile motions like pick-and-place. Under these assumptions, we aim to generate a relocation plan that guarantees global optimality. Furthermore, if any relocation plan cannot singulate the target, we aim to determine it quickly. Therefore, we propose a search method that utilizes the relationship between the object and its nearby obstacles expressed in propositional logic. We define the problem as determining logical entailment (i.e., whether the target can be singulated) and expand the search tree from the target while generating an optimal relocation plan. We demonstrate the performance of our algorithm by increasing the number of objects and validate the plan in a simulation environment.},
  archive   = {C_IROS},
  author    = {Hyojeong Kim and JeongYong Jo and Myo-Taeg Lim and ChangHwan Kim},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802127},
  month     = {10},
  pages     = {12101-12107},
  title     = {Efficient target singulation with multi-fingered gripper using propositional logic},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). StratXplore: Strategic novelty-seeking and
instruction-aligned exploration for vision and language navigation.
<em>IROS</em>, 12093–12100. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Embodied navigation requires robots to understand and interact with the environment based on given tasks. Vision-Language Navigation (VLN) is an embodied navigation task, where a robot navigates within a previously seen and unseen environment, based on linguistic instruction and visual inputs. VLN agents need access to both local and global action spaces; former for immediate decision making and the latter for recovering from navigational mistakes. Prior VLN agents rely only on instruction-viewpoint alignment for local and global decision making and back-track to a previously visited viewpoint, if the instruction and its current viewpoint mismatches. These methods are prone to mistakes, due to the complexity of the instruction and partial observability of the environment. We posit that, back-tracking is sub-optimal and agent that is aware of its mistakes can recover efficiently. For optimal recovery, exploration should be extended to unexplored viewpoints (or frontiers). The optimal frontier is a recently observed but unexplored viewpoint that aligns with the instruction and is novel. We introduce a memory-based and mistake-aware path planning strategy for VLN agents, called StratXplore, that presents global and local action planning to select the optimal frontier for path correction. The proposed method collects all past actions and viewpoint features during navigation and then selects the optimal frontier suitable for recovery. Experimental results show this simple yet effective strategy improves the success rate on two VLN datasets with different task complexities.},
  archive   = {C_IROS},
  author    = {Muraleekrishna Gopinathan and Jumana Abu-Khalaf and David Suter and Martin Masek},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802128},
  month     = {10},
  pages     = {12093-12100},
  title     = {StratXplore: Strategic novelty-seeking and instruction-aligned exploration for vision and language navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LLM3: Large language model-based task and motion planning
with motion failure reasoning. <em>IROS</em>, 12086–12092. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Conventional Task and Motion Planning (TAMP) approaches rely on manually designed interfaces connecting symbolic task planning with continuous motion generation. These domain-specific and labor-intensive modules are limited in addressing emerging tasks in real-world settings. Here, we present LLM3, a novel Large Language Model (LLM)-based TAMP framework featuring a domain-independent interface. Specifically, we leverage the powerful reasoning and planning capabilities of pre-trained LLMs to propose symbolic action sequences and select continuous action parameters for motion planning. Crucially, LLM3 incorporates motion planning feedback through prompting, allowing the LLM to iteratively refine its proposals by reasoning about motion failure. Consequently, LLM3 interfaces between task planning and motion planning, alleviating the intricate design process of handling domain-specific messages between them. Through a series of simulations in a box-packing domain, we quantitatively demonstrate the effectiveness of LLM3 in solving TAMP problems and the efficiency in selecting action parameters. Ablation studies underscore the significant contribution of motion failure reasoning to the success of LLM3. Furthermore, we conduct qualitative experiments on a physical manipulator, demonstrating the practical applicability of our approach in real-world settings.},
  archive   = {C_IROS},
  author    = {Shu Wang and Muzhi Han and Ziyuan Jiao and Zeyu Zhang and Ying Nian Wu and Song-Chun Zhu and Hangxin Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801328},
  month     = {10},
  pages     = {12086-12092},
  title     = {LLM3: Large language model-based task and motion planning with motion failure reasoning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-stage monte carlo tree search for non-monotone object
rearrangement planning in narrow confined environments. <em>IROS</em>,
12078–12085. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Non-monotone object rearrangement planning in confined spaces such as cabinets and shelves is a widely occurring but challenging problem in robotics. Both the robot motion and the available regions for object relocation are highly constrained because of the limited space. This work proposes a Multi-Stage Monte Carlo Tree Search (MS-MCTS) method to solve non-monotone object rearrangement planning problems in confined spaces. Our approach decouples the complex problem into simpler subproblems using an object stage topology. A subgoal-focused tree expansion algorithm that jointly considers the high-level planning and the low-level robot motion is designed to reduce the search space and better guide the search process. By fitting the task into the MCTS paradigm, our method generates short object rearrangement sequences by balancing exploration and exploitation. The experiments demonstrate that our method outperforms the existing methods in terms of the planning time, the number of steps, the object moving distance and the gripper moving distance. Moreover, we deploy our MS-MCTS to a real-world robot system and verify its performance in different scenarios.},
  archive   = {C_IROS},
  author    = {Hanwen Ren and Ahmed H. Qureshi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801607},
  month     = {10},
  pages     = {12078-12085},
  title     = {Multi-stage monte carlo tree search for non-monotone object rearrangement planning in narrow confined environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A framework for neurosymbolic goal-conditioned continual
learning in open world environments. <em>IROS</em>, 12070–12077. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In dynamic open-world environments, agents continually face new challenges due to sudden and unpredictable novelties, hindering Task and Motion Planning (TAMP) in autonomous systems. We introduce a novel TAMP architecture that integrates symbolic planning with reinforcement learning to enable autonomous adaptation in such environments, operating without human guidance. Our approach employs symbolic goal representation within a goal-oriented learning framework, coupled with planner-guided goal identification, effectively managing abrupt changes where traditional reinforcement learning, re-planning, and hybrid methods fall short. Through sequential novelty injections in our experiments, we assess our method’s adaptability to continual learning scenarios. Extensive simulations conducted in a robotics domain corroborate the superiority of our approach, demonstrating faster convergence to higher performance compared to traditional methods. The success of our framework in navigating diverse novelty scenarios within a continuous domain underscores its potential for critical real-world applications.},
  archive   = {C_IROS},
  author    = {Pierrick Lorang and Shivam Goel and Yash Shukla and Patrik Zips and Matthias Scheutz},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801627},
  month     = {10},
  pages     = {12070-12077},
  title     = {A framework for neurosymbolic goal-conditioned continual learning in open world environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-robot multi-goal mission planning in terrains of
varying energy consumption. <em>IROS</em>, 12064–12069. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper considers planning missions for a fleet of robots with limited energy. Each robot has size, heading, and velocity and its motion is described by non-linear differential equations. The dynamics of movements, existing obstacles, multiple robots, and waypoints are additional challenges, as the combined task and motion planning procedure prevents collisions. On their long-term missions, robots have to visit several waypoints in a cost-minimizing manner to satisfy the overall mission task. The robots consume energy and have to be recharged. The framework guides expanding a motion tree via a state projection to a discrete problem, whose solutions serve as search heuristics. Our experiments highlight that despite all these challenges, even sizable problem tasks can be solved even for complex environments.},
  archive   = {C_IROS},
  author    = {Jáchym Herynek and Stefan Edelkamp},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802578},
  month     = {10},
  pages     = {12064-12069},
  title     = {Multi-robot multi-goal mission planning in terrains of varying energy consumption},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HPHS: Hierarchical planning based on hybrid frontier
sampling for unknown environments exploration. <em>IROS</em>,
12056–12063. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Rapid sampling from the environment to acquire available frontier points and timely incorporating them into subsequent planning to reduce fragmented regions are critical to improve the efficiency of autonomous exploration. We propose HPHS, a fast and effective method for the autonomous exploration of unknown environments. In this work, we efficiently sample frontier points directly from the LiDAR data and the local map around the robot, while exploiting a hierarchical planning strategy to provide the robot with a global perspective. The hierarchical planning framework divides the updated environment into multiple subregions and arranges the order of access to them by considering the overall revenue of the global path. The combination of the hybrid frontier sampling method and hierarchical planning strategy reduces the complexity of the planning problem and mitigates the issue of region remnants during the exploration process. Detailed simulation and real-world experiments demonstrate the effectiveness and efficiency of our approach in various aspects. The source code will be released to benefit the further research1.},
  archive   = {C_IROS},
  author    = {Shijun Long and Ying Li and Chenming Wu and Bin Xu and Wei Fan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802539},
  month     = {10},
  pages     = {12056-12063},
  title     = {HPHS: Hierarchical planning based on hybrid frontier sampling for unknown environments exploration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Extended tree search for robot task and motion planning.
<em>IROS</em>, 12048–12055. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Integrated Task and Motion Planning (TAMP) offers opportunities for achieving generalized autonomy in robots but also poses challenges. It involves searching in both symbolic task space and high-dimensional motion space, while also addressing geometrically infeasible actions within its hierarchical process. We introduce a novel TAMP decision-making framework, utilizing an extended decision tree for both symbolic task planning and high-dimensional motion variable binding. Employing top-k planning, we generate a skeleton space with diverse candidate plans, seamlessly integrating it with motion variable spaces into an extended decision space. Subsequently, Monte-Carlo Tree Search (MCTS) is utilized to maintain a balance between exploration and exploitation at decision nodes, ultimately yielding optimal solutions. Our approach combines symbolic top-k planning with concrete motion variable binding, leveraging MCTS for proven optimality, resulting in a powerful algorithm for handling combinatorial complexity in long-horizon manipulation tasks. Empirical evaluations demonstrate the algorithm’s effectiveness in diverse, challenging robot tasks, in comparison with the baseline methods.},
  archive   = {C_IROS},
  author    = {Tianyu Ren and Georgia Chalvatzaki and Jan Peters},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802552},
  month     = {10},
  pages     = {12048-12055},
  title     = {Extended tree search for robot task and motion planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EMPOWER: Embodied multi-role open-vocabulary planning with
online grounding and execution. <em>IROS</em>, 12040–12047. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Task planning for robots in real-life settings presents significant challenges. These challenges stem from three primary issues: the difficulty in identifying grounded sequences of steps to achieve a goal; the lack of a standardized mapping between high-level actions and low-level commands; and the challenge of maintaining low computational overhead given the limited resources of robotic hardware. We introduce EMPOWER, a framework designed for open-vocabulary online grounding and planning for embodied agents aimed at addressing these issues. By leveraging efficient pre-trained foundation models and a multi-role mechanism, EMPOWER demonstrates notable improvements in grounded planning and execution. Quantitative results highlight the effectiveness of our approach, achieving an average success rate of 0.73 across six different real-life scenarios using a TIAGo robot.},
  archive   = {C_IROS},
  author    = {Francesco Argenziano and Michele Brienza and Vincenzo Suriani and Daniele Nardi and Domenico D. Bloisi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802251},
  month     = {10},
  pages     = {12040-12047},
  title     = {EMPOWER: Embodied multi-role open-vocabulary planning with online grounding and execution},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Elliptical k-nearest neighbors - path optimization via
coulomb’s law and invalid vertices in c-space obstacles. <em>IROS</em>,
12032–12039. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Path planning has long been an important and active research area in robotics. To address challenges in high-dimensional motion planning, this study introduces the Force Direction Informed Trees (FDIT*), a sampling-based planner designed to enhance speed and cost-effectiveness in pathfinding. FDIT* builds upon the state-of-the-art informed sampling planner, the Effort Informed Trees (EIT*), by capitalizing on often-overlooked information in invalid vertices. It incorporates principles of physical force, particularly Coulomb’s law. This approach proposes the elliptical k-nearest neighbors search method, enabling fast convergence navigation and avoiding high solution cost or infeasible paths by exploring more problem-specific search-worthy areas. It demonstrates benefits in search efficiency and cost reduction, particularly in confined, high-dimensional environments. It can be viewed as an extension of nearest neighbors search techniques. Fusing invalid vertex data with physical dynamics facilitates force-direction-based search regions, resulting in an improved convergence rate to the optimum. FDIT* outperforms existing single-query, sampling-based planners on the tested problems in ℝ4 to ℝ16 and has been demonstrated on a real-world mobile manipulation task.},
  archive   = {C_IROS},
  author    = {Liding Zhang and Zhenshan Bing and Yu Zhang and Kuanqi Cai and Lingyun Chen and Fan Wu and Sami Haddadin and Alois Knoll},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802280},
  month     = {10},
  pages     = {12032-12039},
  title     = {Elliptical K-nearest neighbors - path optimization via coulomb’s law and invalid vertices in C-space obstacles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Search-based strategy for spatio-temporal environmental
property restoration. <em>IROS</em>, 12024–12031. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the spatio-temporal areas restoration problem: a robot, with limited battery life, deployed in a known environment, needs to persistently plan a schedule to visit areas of interest and charge its battery as needed. The goal is to restore the areas’ properties that temporally decay—such as air quality—so that the time the measured property values are below a certain threshold is minimized. This problem is different from typical problems solved in the area of monitoring a spatio-temporal environment. A related problem is the orienteering problem, where a robot visits nodes to maximize the profit collected at each visited node within a time budget frame. That problem is NP-hard. The typical formulation considers static profit, while we consider a time-varying one. Given look-ahead time window or schedule length, we formulate the problem as an optimization search problem with a temporal objective, and devise a heuristic function that enables finding solutions in polynomial time. The heuristic evaluates the discounted opportunity costs of a visit—a concept borrowed from economics. We then develop a greedy algorithm that takes the immediate feasible visit that minimizes this heuristic. This strategy addresses a primary limitation of a recent approach in applications where being able to revisit highly urgent areas within the time window of the schedule is critical. We provide a theoretical analysis on lower and upper bounds for the problem. Extensive experimental results with a robotic simulator show that our method is able to keep the areas in the environment above the threshold better than other methods and closer to the optimal. This work can enable high-impact applications, such as environmental preservation.},
  archive   = {C_IROS},
  author    = {Amel Nestor Docena and Alberto Quattrini Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801411},
  month     = {10},
  pages     = {12024-12031},
  title     = {Search-based strategy for spatio-temporal environmental property restoration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Task-based design and policy co-optimization for
tendon-driven underactuated kinematic chains. <em>IROS</em>,
12016–12023. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Underactuated manipulators reduce the number of bulky motors, thereby enabling compact and mechanically robust designs. However, fewer actuators than joints means that the manipulator can only access a specific manifold within the joint space, which is particular to a given hardware configuration and can be low-dimensional and/or discontinuous. Determining an appropriate set of hardware parameters for this class of mechanisms, therefore, is difficult - even for traditional task-based co-optimization methods. In this paper, our goal is to implement a task-based design and policy co-optimization method for underactuated, tendon-driven manipulators. We first formulate a general model for an underactuated, tendon-driven transmission. We then use this model to co-optimize a three-link, two-actuator kinematic chain using reinforcement learning. We demonstrate that our optimized tendon transmission and control policy can be transferred reliably to physical hardware with real-world reaching experiments.},
  archive   = {C_IROS},
  author    = {Sharfin Islam and Zhanpeng He and Matei Ciocarlie},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802329},
  month     = {10},
  pages     = {12016-12023},
  title     = {Task-based design and policy co-optimization for tendon-driven underactuated kinematic chains},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). SoftMAC: Differentiable soft body simulation with
forecast-based contact model and two-way coupling with articulated rigid
bodies and clothes. <em>IROS</em>, 12008–12015. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Differentiable physics simulation provides an avenue to tackle previously intractable challenges through gradient-based optimization, thereby greatly improving the efficiency of solving robotics-related problems. To apply differentiable simulation in diverse robotic manipulation scenarios, a key challenge is to integrate various materials in a unified framework. We present SoftMAC, a differentiable simulation framework that couples soft bodies with articulated rigid bodies and clothes. SoftMAC simulates soft bodies with the continuum-mechanics-based Material Point Method (MPM). We provide a novel forecast-based contact model for MPM, which effectively reduces penetration without introducing other artifacts like unnatural rebound. To couple MPM particles with deformable and non-volumetric clothes meshes, we also propose a penetration tracing algorithm that reconstructs the signed distance field in local area. Diverging from previous works, SoftMAC simulates the complete dynamics of each modality and incorporates them into a cohesive system with an explicit and differentiable coupling mechanism. The feature empowers SoftMAC to handle a broader spectrum of interactions, such as soft bodies serving as manipulators and engaging with underactuated systems. We conducted comprehensive experiments to validate the effectiveness and accuracy of the proposed differentiable pipeline in downstream robotic manipulation applications. Supplementary materials are available on our project website at https://damianliumin.github.io/SoftMAC.},
  archive   = {C_IROS},
  author    = {Min Liu and Gang Yang and Siyuan Luo and Lin Shao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801308},
  month     = {10},
  pages     = {12008-12015},
  title     = {SoftMAC: Differentiable soft body simulation with forecast-based contact model and two-way coupling with articulated rigid bodies and clothes},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive feedforward super-twisting sliding mode control of
parallel kinematic manipulators with real-time experiments.
<em>IROS</em>, 12000–12007. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel adaptive feedforward super-twisting sliding mode control algorithm to resolve the tracking control problem of parallel manipulators. The proposed control scheme includes three main terms, (i) the standard super-twisting algorithm, (ii) an adaptive feedforward dynamic model, and (iii) a feedback term to ensure stability. The proposed controller provides robustness towards uncertainties and disturbances, less sensitive to measurement noise, and allows dynamic parameters adaptation of the manipulator while executing a certain task. Real-time experiments are conducted on a 3-DOF non-redundant Delta parallel robot, including two main scenarios, (i) nominal case, and (ii) robustness towards operating acceleration changes. The relevance of the proposed controller is verified experimentally in both scenarios and compared with two other controllers from the literature, including the standard and the feedforward super-twisting sliding mode control algorithms.},
  archive   = {C_IROS},
  author    = {Hussein Saied and Ahmed Chemori and Mohamed Bouri and Maher El Rafei and Clovis Francis},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802641},
  month     = {10},
  pages     = {12000-12007},
  title     = {Adaptive feedforward super-twisting sliding mode control of parallel kinematic manipulators with real-time experiments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fast online omnidirectional quadrupedal jumping framework
via virtual-model control and minimum jerk trajectory generation.
<em>IROS</em>, 11993–11999. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Exploring the limits of quadruped robot agility, particularly in the context of rapid and real-time planning and execution of omnidirectional jump trajectories, presents significant challenges due to the complex dynamics involved, especially when considering significant impulse contacts. This paper introduces a new framework to enable fast, omnidirectional jumping capabilities for quadruped robots. Utilizing minimum jerk technology, the proposed framework efficiently generates jump trajectories that exploit its analytical solutions, ensuring numerical stability and dynamic compatibility with minimal computational resources. The virtual model control is employed to formulate a Quadratic Programming (QP) optimization problem to accurately track the Center of Mass (CoM) trajectories during the jump phase. The whole-body control strategies facilitate precise and compliant landing motion. Moreover, the different jumping phase is triggered by time-schedule. The framework’s efficacy is demonstrated through its implementation on an enhanced version of the open-source Mini Cheetah robot. Omnidirectional jumps-including forward, backward, and other directional-were successfully executed, showcasing the robot’s capability to perform rapid and consecutive jumps with an average trajectory generation and tracking solution time of merely 50 microseconds.},
  archive   = {C_IROS},
  author    = {Linzhu Yue and Lingwei Zhang and Zhitao Song and Hongbo Zhang and Jinhu Dong and Xuanqi Zeng and Yun-Hui Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802277},
  month     = {10},
  pages     = {11993-11999},
  title     = {A fast online omnidirectional quadrupedal jumping framework via virtual-model control and minimum jerk trajectory generation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Disturbance-aware model predictive control of underactuated
robotics systems. <em>IROS</em>, 11985–11992. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While robust model predictive control (MPC) has been studied extensively in recent decades, addressing unmatched disturbances in underactuated robotic systems is still challenging. In this paper, we propose a method to enhance the robustness of the MPC through the online estimation of disturbances using a nonlinear disturbance observer (NDOB). We call this method disturbance-aware MPC (DA-MPC), because the proposed method explicitly utilizes the estimated disturbance in the future prediction. We provide a performance analysis of the NDOB, establishing the boundedness between the predicted and real states. The main advantages of the DA-MPC include its applicability to real-time control and its compatibility with off-the-shelf optimal control problem (OCP) solvers. We demonstrate the application of the proposed method using an underactuated quadrotor system. The simulation validation shows the effectiveness of the proposed method compared to ℒ1-adaptive MPC, which is one of the state-of-the-art robust MPC methods.},
  archive   = {C_IROS},
  author    = {Jiwon Kim and Min Jun Kim},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801851},
  month     = {10},
  pages     = {11985-11992},
  title     = {Disturbance-aware model predictive control of underactuated robotics systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bi-level trajectory optimization on uneven terrains with
differentiable wheel-terrain interaction model. <em>IROS</em>,
11977–11984. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Navigation of wheeled vehicles on uneven terrain necessitates going beyond the 2D approaches for trajectory planning. Specifically, it is essential to incorporate the full 6dof variation of vehicle pose and its associated stability cost in the planning process. To this end, most recent works aim to learn a neural network model to predict vehicle evolution. However, such approaches are data-intensive and fraught with generalization issues.In this paper, we present a purely model-based approach that just requires the digital elevation information of the terrain. Specifically, we express the wheel-terrain interaction and 6dof pose prediction as a non-linear least squares (NLS) problem. As a result, trajectory planning can be viewed as a bi-level optimization. The inner optimization layer predicts the pose on the terrain along a given trajectory, while the outer layer deforms the trajectory itself to reduce the stability and kinematic costs of the pose.We improve the state-of-the-art in the following respects. First, we show that our NLS-based pose prediction closely matches the output of a high-fidelity physics engine. This result, coupled with the fact that we can query gradients of the NLS solver, makes our pose predictor a differentiable wheel-terrain interaction model. We further leverage this differentiability to efficiently solve the proposed bi-level trajectory optimization problem. Finally, we perform extensive experiments and comparisons with a baseline to showcase the effectiveness of our approach in obtaining smooth, stable trajectories.},
  archive   = {C_IROS},
  author    = {Amith Manoharan and Aditya Sharma and Himani Belsare and Kaustab Pal and K. Madhava Krishna and Arun Kumar Singh},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802848},
  month     = {10},
  pages     = {11977-11984},
  title     = {Bi-level trajectory optimization on uneven terrains with differentiable wheel-terrain interaction model},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive trajectory database learning for nonlinear control
with hybrid gradient optimization. <em>IROS</em>, 11969–11976. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel experience-based technique, called EHGO, for sample-efficient adaptive control of nonlinear systems in the presence of dynamical modeling errors. The starting point for EHGO is a database seeded with many trajectories optimized under a reference estimate of real system dynamics. When executed on the real system, these trajectories will be suboptimal due to errors in the reference dynamics. The approach then leverages a hybrid gradient optimization technique, GRILC, which observes executed trajectories and computes gradients from the reference model to refine the control policy without requiring an explicit model of the real system. In past work, GRILC was applied in a restrictive setting in which a robot executes multiple rollouts from identical start states. In this paper, we show how to leverage a database to enable GRILC to operate across a wide envelope of possible start states in different iterations. The database is used to balance between start state proximity and recentness-of-experience via a learned distance metric to generate good initial guesses. Experiments on three dynamical systems (pendulum, car, drone) show that the proposed approach adapts quickly to online experience even when the reference model has significant errors. In these examples EHGO generates near-optimal solutions within hundreds of epochs of real execution, which can be orders of magnitude more sample efficient than reinforcement learning techniques.},
  archive   = {C_IROS},
  author    = {Kuan-Yu Tseng and Mengchao Zhang and Kris Hauser and Geir E. Dullerud},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801717},
  month     = {10},
  pages     = {11969-11976},
  title     = {Adaptive trajectory database learning for nonlinear control with hybrid gradient optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Collaboration strategies for two heterogeneous pursuers in a
pursuit-evasion game using deep reinforcement learning. <em>IROS</em>,
11962–11968. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate a pursuit-evasion game taking place in an unbounded three-dimensional space, where a flexible pursuer with hybrid dynamics collaborates with a fast pursuer and aims to capture a flexible evader within a finite time. The key feature of this problem lies in the hybrid dynamics of the flexible pursuer, which can change its dynamics once during the game and switch to a fast pursuer with increased speed but lower maneuverability. To address this challenge, we devise a hybrid strategy based on the soft actor-critic framework, tailored specifically for the flexible pursuer, which encompasses both maneuvering and switch tactics. We introduce a switch factor to the input of the actor network and incorporate switch actions to further expand the action space. These additions enable the flexible pursuer to execute maneuvering actions and determine a moment to switch to a fast pursuer. The reward function is designed to account for related angle, altitude, speed, and sparse reward. Through extensive ablation experiments conducted in a simulated environment, we demonstrate the efficacy of our algorithm in facilitating the learning of hybrid strategies for the flexible pursuer, resulting in significantly improved capture rates compared to alternative methods.},
  archive   = {C_IROS},
  author    = {Zhanping Zhong and Zhuoning Dong and Xiaoming Duan and Jianping He},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802839},
  month     = {10},
  pages     = {11962-11968},
  title     = {Collaboration strategies for two heterogeneous pursuers in a pursuit-evasion game using deep reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ensuring joint constraints of torque-controlled robot
manipulators under bounded jerk. <em>IROS</em>, 11954–11961. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes an optimization-based control framework for the torque-controlled robot, which can satisfy the joint position, velocity, and acceleration constraints under a bounded jerk. The optimization filter is incorporated as a module to modify the nominal controller output to ensure joint constraints. To formulate the optimization problem as a QP, the torque optimization problem is converted to the jerk optimization problem using the augmented state, and the constraints are reformulated to be affine in the jerk. Here, the viable constraints are derived using the time-optimal braking policy to guarantee the feasibility of the QP. The proposed method was validated in simulation and with a 6-DOF robot manipulator.},
  archive   = {C_IROS},
  author    = {Dongwoo Ko and Jonghyeok Kim and Wan Kyun Chung},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802521},
  month     = {10},
  pages     = {11954-11961},
  title     = {Ensuring joint constraints of torque-controlled robot manipulators under bounded jerk},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model predictive control for frenet-cartesian trajectory
tracking of a tricycle kinematic automated guided vehicle.
<em>IROS</em>, 11948–11953. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work proposes an optimal control scheme for a trajectory-tracking Automated Guided Vehicle considering motion and collision constraints in a warehouse environment. We outline how the simpler obstacle avoidance constraints in the Cartesian Coordinate Frame (CCF) can be retained, while projecting the tricycle kinematics to the Frenet Coordinate Frame (FCF) for track progress. The Nonlinear Model Predictive Control (NMPC) scheme is subsequently implemented using acados and its real-time feasibility is demonstrated in simulation and aboard a test vehicle at a warehouse.},
  archive   = {C_IROS},
  author    = {Akash Subash and Daniel Kloeser and Jonathan Frey and Rudolf Reiter and Moritz Diehl and Karsten Bohlmann},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802822},
  month     = {10},
  pages     = {11948-11953},
  title     = {Model predictive control for frenet-cartesian trajectory tracking of a tricycle kinematic automated guided vehicle},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Momentum-aware trajectory optimisation using full-centroidal
dynamics and implicit inverse kinematics. <em>IROS</em>, 11940–11947.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The current state-of-the-art gradient-based optimisation frameworks are able to produce impressive dynamic manoeuvres such as linear and rotational jumps. However, these methods, which optimise over the full rigid-body dynamics of the robot, often require precise foothold locations apriori, while real-time performance is not guaranteed without elaborate regularisation and tuning of the cost function. In contrast, we investigate the advantages of a task-space optimisation framework, with special focus on acrobatic motions. Our proposed formulation exploits the system&#39;s high-order nonlinearities, such as the nonholonomy of the angular momentum, in order to produce feasible, high-acceleration manoeuvres. By leveraging the full-centroidal dynamics of the quadruped ANYmal C and directly optimising its footholds and contact forces, the framework is capable of producing efficient motion plans with low computational overhead. Finally, we deploy our proposed framework on the ANYmal C platform, and demonstrate its true capabilities through real-world experiments, with the successful execution of high-acceleration motions, such as linear and rotational jumps. Extensive analysis of these shows that the robot&#39;s dynamics can be exploited to surpass its hardware limitations of having a high mass and low-torque limits.},
  archive   = {C_IROS},
  author    = {Aristotelis Papatheodorou and Wolfgang Merkt and Alexander L. Mitchell and Ioannis Havoutis},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801374},
  month     = {10},
  pages     = {11940-11947},
  title     = {Momentum-aware trajectory optimisation using full-centroidal dynamics and implicit inverse kinematics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robustifying model-based locomotion by zero-order stochastic
nonlinear model predictive control with guard saltation matrix.
<em>IROS</em>, 11932–11939. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a stochastic/robust nonlinear model predictive control (NMPC) to enhance the robustness of model-based legged locomotion against contact uncertainties. We integrate the contact uncertainties into the covariance propagation of stochastic/robust NMPC framework by lever-aging the guard saltation matrix and an extended Kalman filter-like covariance update. We achieve fast stochastic/robust NMPC computation by utilizing the zero-order algorithm with additional improvements in computational efficiency concerning the feedback gains. We conducted numerical experiments and demonstrate that the proposed method can accurately forecast future state covariance and generate trajectories that satisfies constraints even in the presence of the contact uncertainties. Hardware experiments on the perceptive locomotion of a wheeled-legged robot were also carried out, validating the feasibility of the proposed method in a real-world system with limited on-board computation.},
  archive   = {C_IROS},
  author    = {Sotaro Katayama and Noriaki Takasugi and Mitsuhisa Kaneko and Norio Nagatsuka and Masaya Kinoshita},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801470},
  month     = {10},
  pages     = {11932-11939},
  title     = {Robustifying model-based locomotion by zero-order stochastic nonlinear model predictive control with guard saltation matrix},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust two-view geometry estimation with implicit
differentiation. <em>IROS</em>, 11924–11931. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel two-view geometry estimation framework which is based on a differentiable robust loss function fitting. We propose to treat the robust fundamental matrix estimation as an implicit layer, which allows us to avoid backpropagation through time and significantly improves the numerical stability. To take full advantage of the information from the feature matching stage we incorporate learnable weights that depend on the matching confidences. In this way our solution brings together feature extraction, matching and two-view geometry estimation in a unified end-to-end trainable pipeline. We evaluate our approach on the camera pose estimation task in both outdoor and indoor scenarios. The experiments on several datasets show that the proposed method outperforms both classic and learning-based state-of- the-art methods by a large margin. The project webpage is available at: https://github.com/VladPyatov/ihls},
  archive   = {C_IROS},
  author    = {Vladislav Pyatov and Iaroslav Koshelev and Stamatios Lefkimmiatis},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801617},
  month     = {10},
  pages     = {11924-11931},
  title     = {Robust two-view geometry estimation with implicit differentiation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CLIPSwarm: Generating drone shows from text prompts with
vision-language models. <em>IROS</em>, 11917–11923. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces CLIPSwarm, a new algorithm designed to automate the modeling of swarm drone formations based on natural language. The algorithm begins by enriching a provided word, to compose a text prompt that serves as input to an iterative approach to find the formation that best matches the provided word. The algorithm iteratively refines formations of robots to align with the textual description, employing different steps for &quot;exploration&quot; and &quot;exploitation&quot;. Our framework is currently evaluated on simple formation targets, limited to contour shapes. A formation is visually represented through alpha-shape contours and the most representative color is automatically found for the input word. To measure the similarity between the description and the visual representation of the formation, we use CLIP [1], encoding text and images into vectors and assessing their similarity. Sub-sequently, the algorithm rearranges the formation to visually represent the word more effectively, within the given constraints of available drones. Control actions are then assigned to the drones, ensuring robotic behavior and collision-free movement. Experimental results demonstrate the system’s efficacy in accurately modeling robot formations from natural language descriptions. The algorithm’s versatility is showcased through the execution of drone shows in photorealistic simulation with varying shapes. We refer the reader to the supplementary video for a visual reference of the results.},
  archive   = {C_IROS},
  author    = {Pablo Pueyo and Eduardo Montijano and Ana C. Murillo and Mac Schwager},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801327},
  month     = {10},
  pages     = {11917-11923},
  title     = {CLIPSwarm: Generating drone shows from text prompts with vision-language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward control of wheeled humanoid robots with unknown
payloads: Equilibrium point estimation via real-to-sim adaptation.
<em>IROS</em>, 11909–11916. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Model-based controllers using a linearized model around the system’s equilibrium point is a common approach in the control of a wheeled humanoid due to their less computational load and ease of stability analysis. However, controlling a wheeled humanoid robot while it lifts an unknown object presents significant challenges, primarily due to the lack of knowledge in object dynamics. This paper presents a framework designed for predicting the new equilibrium point explicitly to control a wheeled-legged robot with unknown dynamics. We estimated the total mass and center of mass of the system from its response to initially unknown dynamics, then calculated the new equilibrium point accordingly. To avoid using additional sensors (e.g., force torque sensor) and reduce the effort of obtaining expensive real data, a data-driven approach is utilized with a novel real-to-sim adaptation. A more accurate nonlinear dynamics model, offering a closer representation of real-world physics, is injected into a rigid-body simulation for real-to-sim adaptation. The nonlinear dynamics model parameters were optimized using Particle Swarm Optimization. The efficacy of this framework was validated on a physical wheeled inverted pendulum, a simplified model of a wheeled-legged robot. The experimental results indicate that employing a more precise analytical model with optimized parameters significantly reduces the gap between simulation and reality, thus improving the efficiency of a model-based controller in controlling a wheeled robot with unknown dynamics.},
  archive   = {C_IROS},
  author    = {Donghoon Baek and Youngwoo Sim and Amartya Purushottam and Saurabh Gupta and Joao Ramos},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802514},
  month     = {10},
  pages     = {11909-11916},
  title     = {Toward control of wheeled humanoid robots with unknown payloads: Equilibrium point estimation via real-to-sim adaptation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stein movement primitives for adaptive multi-modal
trajectory generation. <em>IROS</em>, 11901–11908. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Probabilistic Movement Primitives (ProMPs) and their variants are powerful methods for enabling robots to learn complex tasks from human demonstrations, where motion trajectories are represented as stochastic processes with Gaussian assumptions. However, despite their computational efficiency, these methods have limited expressiveness in capturing the diversity found in human demonstrations, which are typically characterized by the multi-modality of motions. For example, when picking up an object partially obscured by an obstacle, some individuals may opt to go to the right, while others may choose the left side of the object. In this paper, we introduce Stein Movement Primitives (SMPs), a novel approach to probabilistic movement primitives. We formulate motion primitive adaptation as a non-parametric probabilistic inference using Stein Variational Gradient Descent (SVGD), thus avoiding any explicit posterior distribution assumptions and enabling the direct representation of the multi-modality in human demonstrations. We illustrate how our method can adapt robot motion to different scenarios while maintaining high similarity to the original demonstrations, even when the demonstrations are multi-modal. Experimentally, we demonstrate our approach to several domain adaptation problems using the LASA dataset and with a real robotic arm.},
  archive   = {C_IROS},
  author    = {Zeya Yin and Tin Lai and Subhan Khan and Jayadeep Jacob and Yonghui Li and Fabio Ramos},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801403},
  month     = {10},
  pages     = {11901-11908},
  title     = {Stein movement primitives for adaptive multi-modal trajectory generation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Offline meta-reinforcement learning with evolving gradient
agreement. <em>IROS</em>, 11893–11900. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Meta-Reinforcement Learning (Meta-RL) is a machine learning paradigm aimed at learning reinforcement learning policies that can quickly adapt to unseen tasks with few-shot data. Nevertheless, applying Meta-RL to real-world applications faces challenges due to the cost of data acquisition. To address this problem, offline Meta-RL has emerged as a promising solution, focusing on learning policies from pre-collected data that can effectively and rapidly adapt to unseen tasks. In this paper, we propose a new offline Meta-RL method called Meta-Actor-Critic with Evolving Gradient Agreement (MACEGA). MACEGA utilizes an evolutionary approach to estimate meta-gradients conductive to generalization across unseen tasks. During meta-training, gradient evolution is utilized to meta-update the value network and policies. Moreover, we use gradient agreement as an optimization objective for meta-learning, thereby enhancing the generalization ability of the meta-policy. We experimentally demonstrate the robustness of MACEGA in handling offline data quality. Furthermore, extensive experiments on various benchmarks provide empirical evidence that MACEGA outperforms previous state-of-the-art methods in generalizing to unseen tasks, thus demonstrating its potential for real-world applications.},
  archive   = {C_IROS},
  author    = {Jiaxing Chen and Weilin Yuan and Shaofei Chen and Furong Liu and Ao Ma and Zhenzhen Hu and Peng Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802167},
  month     = {10},
  pages     = {11893-11900},
  title     = {Offline meta-reinforcement learning with evolving gradient agreement},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feeling optimistic? Ambiguity attitudes for online decision
making. <em>IROS</em>, 11887–11892. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Due to the complexity of many decision making problems, tree search algorithms often have inadequate information to produce accurate transition models. This results in ambiguities (uncertainties for which there are multiple plausible models). Faced with ambiguities, robust methods have been used to produce safe solutions—often by maximizing the lower bound over the set of plausible transition models. However, they often overlook how much the representation of uncertainty can impact how a decision is made. This work introduces the Ambiguity Attitude Graph Search (AAGS), advocating for more comprehensive representations of ambiguities in decision making. Additionally, AAGS allows users to adjust their ambiguity attitude (or preference), promoting exploration and improving users’ ability to control how an agent should respond when faced with a set of plausible alternatives. Simulation in a dynamic sailing environment shows how environments with high entropy transition models can lead robust methods to fail. Results further demonstrate how adjusting ambiguity attitudes better fulfills objectives while mitigating this failure mode of robust approaches. Because this approach is a generalization of the robust framework, these results further demonstrate how algorithms focused on ambiguity have applicability beyond safety-critical systems.},
  archive   = {C_IROS},
  author    = {Jared J. Beard and R. Michael Butts and Yu Gu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802265},
  month     = {10},
  pages     = {11887-11892},
  title     = {Feeling optimistic? ambiguity attitudes for online decision making},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GeRM: A generalist robotic model with mixture-of-experts for
quadruped robot. <em>IROS</em>, 11879–11886. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-task robot learning holds significant importance in tackling diverse and complex scenarios. However, current approaches are hindered by performance issues and difficulties in collecting training datasets. In this paper, we propose GeRM (Generalist Robotic Model). We utilize offline reinforcement learning to optimize data utilization strategies to learn from both demonstrations and sub-optimal data, thus surpassing the limitations of human demonstrations. Thereafter, we employ a transformer-based VLA network to process multi-modal inputs and output actions. By introducing the Mixture-of-Experts structure, GeRM allows faster inference speed with higher whole model capacity, and thus resolves the issue of limited RL parameters, enhancing model performance in multi-task learning while controlling computational costs. Through a series of experiments, we demonstrate that GeRM outperforms other methods across all tasks, while also validating its efficiency in both training and inference processes. Additionally, we uncover its potential to acquire emergent skills. Additionally, we contribute the QUARD-Auto dataset, collected automatically to support our training approach and foster advancements in multi-task quadruped robot learning. This work presents a new paradigm for reducing the cost of collecting robot data and driving progress in the multi-task learning community.You can reach our project and video through the link: https://songwxuan.github.io/GeRM/.},
  archive   = {C_IROS},
  author    = {Wenxuan Song and Han Zhao and Pengxiang Ding and Can Cui and Shangke Lyu and Yaning Fan and Donglin Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801816},
  month     = {10},
  pages     = {11879-11886},
  title     = {GeRM: A generalist robotic model with mixture-of-experts for quadruped robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sensorimotor attention and language-based regressions in
shared latent variables for integrating robot motion learning and LLM.
<em>IROS</em>, 11872–11878. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, studies have been actively conducted on combining large language models (LLM) and robotics; however, most have not considered end-to-end feed-back in the robot-motion generation phase. The prediction of deep neural networks must contain errors, it is required to update the trained model to correspond to the real environment to generate robot motion adaptively. This study proposes an integration method that connects the robot-motion learning model and LLM using shared latent variables. When generating robot motion, the proposed method updates shared parameters based on prediction errors from both sensorimotor attention points and task language instructions given to the robot. This allows the model to search for latent parameters appropriate for the robot task efficiently. Through simulator experiments on multiple robot tasks, we demonstrated the effectiveness of our proposed method from two perspectives: position generalization and language instruction generalization abilities.},
  archive   = {C_IROS},
  author    = {Kanata Suzuki and Tetsuya Ogata},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802349},
  month     = {10},
  pages     = {11872-11878},
  title     = {Sensorimotor attention and language-based regressions in shared latent variables for integrating robot motion learning and LLM},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning agile locomotion on risky terrains. <em>IROS</em>,
11864–11871. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Quadruped robots have shown remarkable mobility on various terrains through reinforcement learning. Yet, in the presence of sparse footholds and risky terrains such as stepping stones and balance beams, which require precise foot placement to avoid falls, model-based approaches are often used. In this paper, we show that end-to-end reinforcement learning can also enable the robot to traverse risky terrains with dynamic motions. To this end, our approach involves training a generalist policy for agile locomotion on disorderly and sparse stepping stones before transferring its reusable knowledge to various more challenging terrains by finetuning specialist policies from it. Given that the robot needs to rapidly adapt its velocity on these terrains, we formulate the task as a navigation task instead of the commonly used velocity tracking which constrains the robot’s behavior and propose an exploration strategy to overcome sparse rewards and achieve high robustness. We validate our proposed method through simulation and real-world experiments on an ANYmal-D robot achieving peak forward velocity of ≥2.5 m/s on sparse stepping stones and narrow balance beams. Video: youtu.be/Z5X0J8OH6z4},
  archive   = {C_IROS},
  author    = {Chong Zhang and Nikita Rudin and David Hoeller and Marco Hutter},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801909},
  month     = {10},
  pages     = {11864-11871},
  title     = {Learning agile locomotion on risky terrains},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Guiding reinforcement learning with incomplete system
dynamics. <em>IROS</em>, 11857–11863. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Model-free reinforcement learning (RL) is inherently a reactive method, operating under the assumption that it starts with no prior knowledge of the system and entirely depends on trial-and-error for learning. This approach faces several challenges, such as poor sample efficiency, generalization, and the need for well-designed reward functions to guide learning effectively. On the other hand, controllers based on complete system dynamics do not require data. This paper addresses the intermediate situation where there is not enough model information for complete controller design, but there is enough to suggest that a model-free approach is not the best approach either. By carefully decoupling known and unknown information about the system dynamics, we obtain an embedded controller guided by our partial model and thus improve the learning efficiency of an RL-enhanced approach. A modular design allows us to deploy mainstream RL algorithms to refine the policy. Simulation results show that our method significantly improves sample efficiency compared with standard RL methods on continuous control tasks, and also offers enhanced performance over traditional control approaches. Experiments on a real ground vehicle also validate the performance of our method, including generalization and robustness.},
  archive   = {C_IROS},
  author    = {Shuyuan Wang and Jingliang Duan and Nathan P. Lawrence and Philip D. Loewen and Michael G. Forbes and R. Bhushan Gopaluni and Lixian Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802721},
  month     = {10},
  pages     = {11857-11863},
  title     = {Guiding reinforcement learning with incomplete system dynamics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data-driven force observer for human-robot interaction with
series elastic actuators using gaussian processes. <em>IROS</em>,
11849–11856. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ensuring safety and adapting to the user’s behavior are of paramount importance in physical human-robot interaction. Thus, incorporating elastic actuators in the robot’s mechanical design has become popular, since it offers intrinsic compliance and additionally provide a coarse estimate for the interaction force by measuring the deformation of the elastic components. While observer-based methods have been shown to improve these estimates, they rely on accurate models of the system, which are challenging to obtain in complex operating environments. In this work, we overcome this issue by learning the unknown dynamics components using Gaussian process (GP) regression. By employing the learned model in a Bayesian filtering framework, we improve the estimation accuracy and additionally obtain an observer that explicitly considers local model uncertainty in the confidence measure of the state estimate. Furthermore, we derive guaranteed estimation error bounds, thus, facilitating the use in safety-critical applications. We demonstrate the effectiveness of the proposed approach experimentally in a human-exoskeleton interaction scenario.},
  archive   = {C_IROS},
  author    = {Samuel Tesfazgi and Markus Keßler and Emilio Trigili and Armin Lederer and Sandra Hirche},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802608},
  month     = {10},
  pages     = {11849-11856},
  title     = {Data-driven force observer for human-robot interaction with series elastic actuators using gaussian processes},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic modeling of robotic fish considering background flow
using koopman operators. <em>IROS</em>, 11843–11848. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dynamic model is essential for robust and reliable robotic fish motion control. Despite considerable efforts in robotic fish dynamic modeling, background flow has not been well considered yet, leading to the deterioration of applying robotic fish to practice. In this paper, we propose a novel dynamic model, termed Flow-Aware Robotic fish Model (FARM), that with well consideration to background flow using Koopman operators without increasing computation complexity. Specifically, we first collect motion data of the robotic fish in different background flow fields, and then obtain a linear approximation (the dynamic model) of nonlinear dynamics through carefully selected lifted functions. The obtained model can predict the next state based on the current state, control input, and average flow velocity of the local flow field. We evaluate the effectiveness of obtained model by comparing the Root Mean Square Error (RMSE) of predicted motion trajectories with real trajectories in various flow field environments. The results indicate that FARM is highly promising for obtaining a reliable dynamic model and can achieve comparable prediction accuracy even in unseen flow field environments with rough flow maps.},
  archive   = {C_IROS},
  author    = {Xiaozhu Lin and Song Liu and Chengyuan Liu and Yang Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801736},
  month     = {10},
  pages     = {11843-11848},
  title     = {Dynamic modeling of robotic fish considering background flow using koopman operators},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Performing efficient and safe deformable package transport
operations using suction cups. <em>IROS</em>, 11835–11842. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Suction cups are popular for picking and transporting packages in warehouse applications. To maximize throughput, high transport speeds are desired. Many packages are deformable and may detach from the suction cups due to inertial loading if trajectories use excessive velocities. This paper introduces a novel methodology that analyzes package deformation through its curvature at the package-suction cup contact interface to generate a Factor-of-Safety (FOS) score for each waypoint in a given trajectory. By maintaining the FOS above a predetermined threshold, the trajectory planner is able to generate transport trajectories that are both safe and time-optimized. Experimental results show the method’s efficacy, demonstrating a 21.92% reduction in transport times compared to a conservative trajectory generation. Our FOS predictor identified trajectories that ensured safe package transport with 100% accuracy across all 627 real-world experiments.},
  archive   = {C_IROS},
  author    = {Rishabh Shukla and Zeren Yu and Samrudh Moode and Omey M. Manyar and Fan Wang and Siddharth Mayya and Satyandra K. Gupta},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802323},
  month     = {10},
  pages     = {11835-11842},
  title     = {Performing efficient and safe deformable package transport operations using suction cups},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Repairing neural networks for safety in robotic systems
using predictive models. <em>IROS</em>, 11827–11834. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a new method for safety-aware robot learning, focusing on repairing policies using predictive models. Our method combines behavioral cloning with neural network repair in a two-step supervised learning framework. It first learns a policy from expert demonstrations and then applies repair subject to predictive models to enforce safety constraints. The predictive models can encompass various aspects relevant to robot learning applications, such as proprioceptive states and collision likelihood. Our experimental results demonstrate that the learned policy successfully adheres to a predefined set of safety constraints on two applications: mobile robot navigation, and real-world lower-leg prostheses. Additionally, we have shown that our method effectively reduces repeated interaction with the robot, leading to substantial time savings during the learning process.},
  archive   = {C_IROS},
  author    = {Keyvan Majd and Geoffrey Clark and Georgios Fainekos and Heni Ben Amor},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802650},
  month     = {10},
  pages     = {11827-11834},
  title     = {Repairing neural networks for safety in robotic systems using predictive models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tube-GAN: A novel virtual tube generation method for
unmanned aerial swarms based on generative adversarial network.
<em>IROS</em>, 11819–11826. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Virtual tube is a two-dimensional or three-dimensional strip or tubular area similar to RSFC (Relative Safe Flight Corridor), which can provide smooth, feasible, and safe space for UAV swarm in environments with dense obstacles. In order to address the problem that current virtual tube planning methods are mainly based on complex heuristic algorithm with consuming time complexity, we modify the model architecture by introducing generative adversarial network (GAN), and propose a Tube-GAN model. Tube-GAN takes the key point prompt image and obstacle environment image as inputs, and outputs the image of the virtual tube, which transforms the optimization problem into an image generation problem, leveraging the performance of computational efficiency for the construction of virtual tube. The experimental results demonstrate that the proposed Tube-GAN model can quickly generate virtual tube in random environments (less than 25ms), providing a new direction for the construction of virtual tube in real-time.},
  archive   = {C_IROS},
  author    = {Shixun Zhai and Kaige Zhang and Bo Nan and Yanwen Sun and Qianyi Fu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801604},
  month     = {10},
  pages     = {11819-11826},
  title     = {Tube-GAN: A novel virtual tube generation method for unmanned aerial swarms based on generative adversarial network},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The power of input: Benchmarking zero-shot sim-to-real
transfer of reinforcement learning control policies for quadrotor
control. <em>IROS</em>, 11812–11818. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the last decade, data-driven approaches have become popular choices for quadrotor control, thanks to their ability to facilitate the adaptation to unknown or uncertain flight conditions. Among the different data-driven paradigms, Deep Reinforcement Learning (DRL) is currently one of the most explored. However, the design of DRL agents for Micro Aerial Vehicles (MAVs) remains an open challenge. While some works have studied the output configuration of these agents (i.e., what kind of control to compute), there is no general consensus on the type of input data these approaches should employ. Multiple works simply provide the DRL agent with full state information, without questioning if this might be redundant and unnecessarily complicate the learning process, or pose superfluous constraints on the availability of such information in real platforms. In this work, we provide an in-depth benchmark analysis of different configurations of the observation space. We optimize multiple DRL agents in simulated environments with different input choices and study their robustness and their sim-to-real transfer capabilities with zero-shot adaptation. We believe that the outcomes and discussions presented in this work supported by extensive experimental results could be an important milestone in guiding future research on the development of DRL agents for aerial robot tasks.},
  archive   = {C_IROS},
  author    = {Alberto Dionigi and Gabriele Costante and Giuseppe Loianno},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802831},
  month     = {10},
  pages     = {11812-11818},
  title     = {The power of input: Benchmarking zero-shot sim-to-real transfer of reinforcement learning control policies for quadrotor control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NFPDE: Normalizing flow-based parameter distribution
estimation for offline adaptive domain randomization. <em>IROS</em>,
11804–11811. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning with domain randomization (DR) has been proposed as a promising approach for learning robust policies to environmental changes. However, for DR to work well in real-world environments, it is necessary to design appropriate DR distributions for model parameters. This paper proposes Normalizing Flow-based Parameter Distribution Estimation (NFPDE), a new estimation method for DR distributions. NFPDE models the target distribution by a flow-based generative model using normalizing flow and estimates the target distribution based on an offline dataset collected a priori in the target environment. Through numerical experiments on the OpenAI gym environment, we show that NFPDE can estimate the target distribution more accurately and efficiently than the previous estimation methods. We also show that the estimated DR distributions can improve the robustness of trained policies.},
  archive   = {C_IROS},
  author    = {Rin Takano and Kei Takaya and Hiroyuki Oyama},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801378},
  month     = {10},
  pages     = {11804-11811},
  title     = {NFPDE: Normalizing flow-based parameter distribution estimation for offline adaptive domain randomization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised multiple proactive behavior learning of mobile
robots for smooth and safe navigation. <em>IROS</em>, 11796–11803. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While different control approaches have been developed for smooth and safe navigation, they are limited by the needs for model-based assumptions, true training target/reward function, and/or large sample data. To overcome these limitations, this study proposes a model-free neural control architecture with a generic plug-and-play online Multiple Proactive Behavior Learning (MPL) module. The MPL adapts robot neural control policy in an online unsupervised manner with small sample data by correlating its sensory inputs to a local planner command. As a result, it allows a mobile robot to autonomously and quickly learn and balance various proactive behaviors related to smooth motion and collision avoidance. It also compensates for the limited planning update rates and the planning model mismatch of an arbitrary local motion planner. Compared with existing control approaches without the MPL, our control architecture with the MPL leads to (1) a 10% improvement in the smoothness of robot motion and 30% fewer collisions in a narrow static environment, and (2) trading motion smoothness for up to 70% fewer collisions in an unknown dynamic environment. Taken together, this study also demonstrates how to apply model-free neural control with unsupervised learning to existing model-based control (e.g., local motion planner) for efficient proactive behavior learning and control of mobile robots.},
  archive   = {C_IROS},
  author    = {Arthicha Srisuchinnawong and Jonas Bæch and Marek Piotr Hyzy and Tsampikos Kounalakis and Evangelos Boukas and Poramate Manoonpong},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802071},
  month     = {10},
  pages     = {11796-11803},
  title     = {Unsupervised multiple proactive behavior learning of mobile robots for smooth and safe navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural kinodynamic planning: Learning for KinoDynamic tree
expansion. <em>IROS</em>, 11789–11795. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We integrate neural networks into kinodynamic motion planning and present the Learning for KinoDynamic Tree Expansion (L4KDE) method. Tree-based planning approaches, such as rapidly exploring random tree (RRT), are the dominant approach to finding globally optimal plans in continuous state-space motion planning. Central to these approaches is tree expansion, the procedure in which new nodes are added to an ever-expanding tree. We study the kinodynamic variants of tree-based planning, where we have known system dynamics and kinematic constraints. In the interest of quickly selecting nodes to connect newly sampled coordinates, existing methods typically cannot optimise the finding of nodes that have a low cost to transition to sampled coordinates. Instead, they use metrics like Euclidean distance between coordinates as a heuristic for selecting candidate nodes to connect to the search tree. We propose L4KDE to address this issue. L4KDE uses a neural network to predict transition costs between queried states, which can be efficiently computed in batch, providing much higher quality estimates of transition cost compared to commonly used heuristics while maintaining almost-surely asymptotic optimality guarantee. We empirically demonstrate the significant performance improvement provided by L4KDE on a variety of challenging system dynamics,with the ability to generalise across different instances of the same model class and in conjunction with a suite of modern tree-based motion planners.},
  archive   = {C_IROS},
  author    = {Tin Lai and Weiming Zhi and Tucker Hermans and Fabio Ramos},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801948},
  month     = {10},
  pages     = {11789-11795},
  title     = {Neural kinodynamic planning: Learning for KinoDynamic tree expansion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Archie snr: A robotic platform for autonomous apple fruitlet
thinning. <em>IROS</em>, 11781–11788. (<a
href="https://doi.org/10.1109/IROS58592.2024.10803063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Apple fruitlet thinning is critical in cultivating high-quality apples, requiring an expert workforce to manage the orchard. The thinning process requires precise mapping of fruitlet clusters across the tree branches to manage the desired load for each tree. This paper presents Archie Snr, which was developed to autonomously assess the current load of the tree and thin the excess apples as an expert thinner would. The platform has been extensively evaluated in a real-world commercial orchard. The results show the platform can generate an average load count accuracy of 82.1% with a recall of 93.3%. The system was then able to successfully thin 66.14% of the fruitlets from the canopy.},
  archive   = {C_IROS},
  author    = {Henry Williams and Ans H. Qureshi and David Smith and Trevor Gee and Ben McGuinness and Rahul Jangali and Kale Black and Scott Harvey and Catherine Downes and Hin Lim and Richard Oliver and Mike Duke and Bruce A. MacDonald},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10803063},
  month     = {10},
  pages     = {11781-11788},
  title     = {Archie snr: A robotic platform for autonomous apple fruitlet thinning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Roofus: Learning-based robotic moisture mapping on flat
rooftops with ground penetrating radar. <em>IROS</em>, 11773–11780. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robust moisture detection is crucial for building maintenance and cost reduction. Current methods are often limited by the type of roofing material or are cumbersome and expensive. Ground Penetrating Radar (GPR) has shown promise in recent works in moisture detection due to its effectiveness across a broader range of materials, its compactness and lightweight nature, and its ability to image the subsurface. We introduce Roofus, an integrated robotic moisture detection system for flat rooftops, designed to overcome traditional method limitations. It combines a remotely controlled robot with deep learning GPR data processing and automatic map generation. Real-world data is collected and manually annotated for supervised learning. We investigate a novel approach to interpreting GPR data via deep learning using Transformer-based classifiers. LiDAR inertial odometry is employed to integrate multiple individual GPR scans into a holistic moisture map over the rooftop. When evaluated against existing methods such as infrared thermal imaging, electrical capacitance surveys, and nuclear moisture gauges, our system shows promising viability for industry application.},
  archive   = {C_IROS},
  author    = {Kevin Lee and Wei-Heng Lin and Talha Javed and Sruti Madhusudhan and Bilal Sher and Chen Feng},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802148},
  month     = {10},
  pages     = {11773-11780},
  title     = {Roofus: Learning-based robotic moisture mapping on flat rooftops with ground penetrating radar},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online tree reconstruction and forest inventory on a mobile
robotic system. <em>IROS</em>, 11765–11772. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Terrestrial laser scanning (TLS) is the standard technique used to create accurate point clouds for digital forest inventories. However, the measurement process is demanding, requiring up to two days per hectare for data collection, significant data storage, as well as resource-heavy post-processing of 3D data. In this work, we present a real-time mapping and analysis system that enables online generation of forest inventories using mobile laser scanners that can be mounted e.g. on mobile robots. Given incrementally created and locally accurate submaps—data payloads—our approach extracts tree candidates using a custom, Voronoi-inspired clustering algorithm. Tree candidates are reconstructed using an algorithm based on the Hough transform, which enables robust modeling of the tree stem. Further, we explicitly incorporate the incremental nature of the data collection by consistently updating the database using a pose graph LiDAR SLAM system. This enables us to refine our estimates of the tree traits if an area is revisited later during a mission. We demonstrate competitive accuracy to TLS or manual measurements using laser scanners that we mounted on backpacks or mobile robots operating in conifer, broad-leaf and mixed forests. Our results achieve RMSE of 1.93 cm, a bias of 0.65 cm and a standard deviation of 1.81 cm (averaged across these sequences)—with no post-processing required after the mission is complete.},
  archive   = {C_IROS},
  author    = {Leonard Freißmuth and Matias Mattamala and Nived Chebrolu and Simon Schaefer and Stefan Leutenegger and Maurice Fallon},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802455},
  month     = {10},
  pages     = {11765-11772},
  title     = {Online tree reconstruction and forest inventory on a mobile robotic system},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Calibration-free vision-assisted container loading of RTG
cranes. <em>IROS</em>, 11758–11764. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision-assisted container loading of Rubber Tyred Gantry (RTG) cranes are facing two primary challenges. Firstly, the uncertainty inherent in Covolutional Neural Network (CNN) based detection hinders its direct application in the safety-critical operation of such heavy-duty machinery. Secondly, sensor calibration introduces additional complexities and errors into the system. However, existing studies have not adequately addressed these challenges. Motivated by this gap, this paper proposes an integrated approach for target detection and alignment control in container loading of RTG cranes. To ensure reliable target marker identification, a heuristic post-processing algorithm is developed as a complement to CNN-based foreground segmentation, thereby ensuring safety during the container handling process. On this basis, a pixel-based control scheme is designed to align the container with the target markers, which eliminates the need for offline or online sensor calibrations. The proposed approach has been successfully implemented on a real RTG crane manufactured by Shanghai Zhenhua Heavy Industries Co., Ltd. (ZPMC) and validated at the Port of Ningbo, China. Experimental results demonstrate the superiority of the proposed approach over current manual operations in port industries, highlighting its potential for crane automation.},
  archive   = {C_IROS},
  author    = {Jianbing Yang and Yuanzhe Wang and Hao Jiang and Bin Zhao and Yiming Li and Danwei Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802172},
  month     = {10},
  pages     = {11758-11764},
  title     = {Calibration-free vision-assisted container loading of RTG cranes},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intelligent fish detection system with similarity-aware
transformer. <em>IROS</em>, 11750–11757. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fish detection in water-land transfer has significantly contributed to the fishery. However, manual fish detection in crowd-collaboration performs inefficiently and expensively, involving insufficient accuracy. To further enhance the water-land transfer efficiency, improve detection accuracy, and reduce labor costs, this work designs a new type of lightweight and plug-and-play edge intelligent vision system to automatically conduct fast fish detection with high-speed camera. Moreover, a novel similarity-aware vision Transformer for fast fish detection (FishViT) is proposed to onboard identify every single fish in a dense and similar group. Specifically, a novel similarity-aware multi-level encoder is developed to enhance multi-scale features in parallel, thereby yielding discriminative representations for varying-size fish. Additionally, a new soft-threshold attention mechanism is introduced, which not only effectively eliminates background noise from images but also accurately recognizes both the edge details and overall features of different similar fish. 85 challenging video sequences with high framerate and high-resolution are collected to establish a benchmark from real fish water-land transfer scenarios. Exhaustive evaluation conducted with this challenging benchmark has proved the robustness and effectiveness of FishViT with over 80 FPS. Real work scenario tests validate the practicality of the proposed method. The code and demo video are available at https://github.com/vision4robotics/FishViT.},
  archive   = {C_IROS},
  author    = {Shengchen Li and Haobo Zuo and Changhong Fu and Zhiyong Wang and Zhiqiang Xu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802394},
  month     = {10},
  pages     = {11750-11757},
  title     = {Intelligent fish detection system with similarity-aware transformer},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CAIS: Culvert autonomous inspection robotic system.
<em>IROS</em>, 11744–11749. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Culverts, essential components of drainage systems, require regular inspection to ensure optimal functionality. However, culvert inspections pose numerous challenges, including accessibility, manpower, defect localization, and reliance on superficial assessments. To address these challenges, we propose a novel Culvert Autonomous Inspection Robotic System (CAIS) equipped with advanced sensing and evaluation capabilities. Our solution integrates an RGBD camera, deep learning, lighting systems, and non-destructive evaluation (NDE) techniques to enable accurate and comprehensive condition assessments. We present a pioneering Partially Observable Markov Decision Process (POMDP) framework to resolve uncertainty in autonomous inspections, especially in confined and unstructured environments like culverts or tunnels. The framework outputs detailed 3D maps highlighting visual defects and NDE condition assessments, demonstrating consistent and reliable performance in both indoor and outdoor scenarios. Additionally, we provide an open-source implementation of our framework on GitHub, contributing to the advancement of autonomous inspection technology and fostering collaboration within the research community. Source codes are available *.},
  archive   = {C_IROS},
  author    = {Chuong Phuoc Le and Pratik Walunj and An Duy Nguyen and Yongyi Zhou and Binh Nguyen and Thang Nguyen and Anton Netchaev and Hung Manh La},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802162},
  month     = {10},
  pages     = {11744-11749},
  title     = {CAIS: Culvert autonomous inspection robotic system},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Archie jnr: A robotic platform for autonomous cane pruning
of grapevines. <em>IROS</em>, 11736–11743. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cane pruning grapevines is a complex manual task requiring expert vine assessment to determine which canes to prune. This paper presents Archie Jnr, which was developed to autonomously assess the structure of the vine and prune the lower-quality canes as an expert pruner would. The platform has been extensively evaluated in a real-world commercial vineyard using a three-cane pruning method. The results show the effectiveness of the vision system for generating accurate assessments of a vine’s canes. The platform is also shown to be capable of successfully pruning 71.1% of the 311 total canes that required pruning across 25 vines.},
  archive   = {C_IROS},
  author    = {Henry Williams and David Smith and Jalil Shahabi and Trevor Gee and Ans Qureshi and Ben McGuinness and Scott Harvey and Catherine Downes and Rahul Jangali and Kale Black and Hin Lim and Mike Duke and Bruce A. MacDonald},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802290},
  month     = {10},
  pages     = {11736-11743},
  title     = {Archie jnr: A robotic platform for autonomous cane pruning of grapevines},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Development of a throwbot with shock absorption structure.
<em>IROS</em>, 11730–11735. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this study, a throwing robot equipped with an shock absorbing structure, utilizing paired-Cross Flexural Hinge (p-CFH) and an airbag, was fabricated and validated to assess the effectiveness of its impact absorption mechanism. This robot was developed in anticipation of situations where direct human intervention for life rescue would be challenging. Throwing robots can be broadly categorized into ball type, wheel type, and hybrid type. The hybrid type combines the advantages of both: the ease of throwing from ball type, due to its low air resistance coefficient, and the versatile mobility of the wheel type in diverse environments. However, hybrid type throwing robots are more vulnerable to external impacts due to the complexity of their internal structure, resulting in a lower maximum drop height compared to wheel type robots.To address these challenges, this research proposes a the Throwbot that combines the easy throwing capability of ball type with the obstacle overcoming ability of the wheel type, while also addressing the low free fall height drawback inherent in hybrid types. To achieve this, we developed a Throwbot with a ball to wheel transform structure, p-CFH mechanism, and airbag based impact absorption system. Additionally, materials were selected based on simulation results to refine the Throw-bot. The performance of the proposed robot was evaluated through various assessments, including free fall experiments and obstacle overcoming tests. Through this research, the proposed Throwbot effectively addresses the shortcomings of existing throwing robots, establishing a novel approach to throwing robot design.},
  archive   = {C_IROS},
  author    = {Jaeyeong Keum and Jaemin Kim and Changgi Lee and Seunghyun Lim and Insung Ju and Dongwon Yun},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801570},
  month     = {10},
  pages     = {11730-11735},
  title     = {Development of a throwbot with shock absorption structure},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Side-scan sonar based landmark detection for underwater
vehicles. <em>IROS</em>, 11723–11729. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose and analyze a pipeline to transform raw sonar data from underwater vehicles into actionable information for Simultaneous Localization and Mapping (SLAM) in real time. The pipeline encompasses three sequential steps, each building upon state-of-the-art algorithms from the existing literature: swath processing to preprocess the raw sonar data, with a primary focus on eliminating blind zones and noise reduction; transformation of these swaths into probabilistic maps of the surroundings of the sonar; and finally, detection and classification of underwater landmarks from the probabilistic maps.Our work contributes by modifying algorithms from the literature to ensure computational efficiency and integrating them so that they operate in sequence, thereby furnishing valuable information for navigation purposes.Through validation with field data, we then discuss which scenarios may prove difficult for the individual stages of the proposed pipeline. We provide indications on whether each step may encounter challenges and discuss how this occurrence may affect the overall quality of the final result. This empirical discussion is useful for discerning the practical applicability of the proposed pipeline in real-world settings.},
  archive   = {C_IROS},
  author    = {Simon Hoff and Vegard Haraldstad and Bjørnar Reitan Hogstad and Damiano Varagnolo},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801447},
  month     = {10},
  pages     = {11723-11729},
  title     = {Side-scan sonar based landmark detection for underwater vehicles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Low-cost urban localization with magnetometer and LoRa
technology. <em>IROS</em>, 11715–11722. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the goal of developing low-cost and innovative perception and localization techniques for autonomous vehicles, this work explores a system that solely relies on a LoRa receiver and a magnetometer for agent localization within urban environments. Using the received signal strength from LoRa beacons distributed across a test area of 16,000 square meters, a model of expected RSSI values per beacon is estimated using Gaussian Process (GP) regression. Motion is estimated using a probabilistic signal similarity classifier, and localization is obtained via a particle filter. Our experiments demonstrate that our proposed system is able to estimate our location to within three meters RMSE when the agent is within the convex hull of prior data. In real-world scenarios, characterized by signal interference and environmental complexities, our approach highlights the potential of leveraging affordable technology such as LoRa receivers and magnetometers for robust and accurate location estimation in complex urban environments. The integration of low-cost LoRa devices, Gaussian Process regression, particle filtering and our novel signal similarity motion estimator offers a promising avenue for achieving cost-effective localization solutions without compromising accuracy or reliability.},
  archive   = {C_IROS},
  author    = {Derek Benham and Ashton Palacios and Philip Lundrigan and Joshua G. Mangelson},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801786},
  month     = {10},
  pages     = {11715-11722},
  title     = {Low-cost urban localization with magnetometer and LoRa technology},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PARE: A plane-assisted autonomous robot exploration
framework in unknown and uneven terrain. <em>IROS</em>, 11707–11714. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Identifying traversable areas is a critical task for unmanned vehicles exploring safely through unstructured environments. In practice, the ambiguity in perceiving terrain traversability usually brings great challenges for autonomous exploration in unknown and uneven terrain, which often leads to conservative strategies or potential risk of vehicle damage, resulting in many unexplored areas in the environment. To that end, this paper proposes a plane-assisted autonomous robot exploration framework (PARE) to achieve maximum volume and safe autonomous exploration. The process is carried out by a three-step dual-layer framework: constructing a local tree using Plane-Assisted RRT* (PA-RRT*), calculating exploration gain based on terrain information, and maintaining a global search graph. Firstly, the planar feature metrics (flatness, sparsity, elevation variation, slope and slope variation) are introduced to determine the terrain traversability. Secondly, to completely explore the rugged environment, we propose a dual-layer exploration framework comprising local and global strategies. A local planner based on PA-RRT* is proposed to find the best path by evaluating the planar information and the volumetric gain within the local exploration tree. Meanwhile, a global planner constructed by graph is proposed to record unexplored nodes with high exploration gain from the local tree to ensure a high level of exploration volume. Extensive simulation and real-world experiments demonstrate that our method significantly outperforms existing frameworks, with an average improvement of more than 12% in exploration volume.},
  archive   = {C_IROS},
  author    = {Pu Xu and Zhaoqiang Bai and Haoming Liu and Zheng Fang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802459},
  month     = {10},
  pages     = {11707-11714},
  title     = {PARE: A plane-assisted autonomous robot exploration framework in unknown and uneven terrain},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time terrain assessment and bayesian-based path
planning for off-road navigation. <em>IROS</em>, 11700–11706. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the context of unstructured and unknown environment, the autonomous navigation still faces many challenges, such as assessing rough terrain and deciding how to safely navigate complex terrain. In this work, we propose a robust and practical off-road navigation framework that has been successfully deployed on a vibroseis truck for land exploration. First, in degraded wild scenes, a tightly coupled lidar-GNSS-inertial fusion odometry and mapping framework is adopted to construct a local point cloud map around the vehicle in real-time and provide precise localization. Then, based on amplitude-frequency characteristic analysis and point cloud PCA, a multi-layer terrain assessment map containing terrain roughness, obstacles and slope information is obtained. Finally, combining Gaussian distribution based adaptive sampler and Bayesian sequentially updated proposal distribution, a local graph is efficiently built to obtain multiple path solutions under constrained conditions. Both simulations and field experiments show that the proposed navigation framework can decide how to travel on a flat road even in harsh terrain conditions, naturally suppressing frequent attitude angle changes and preventing vehicle accidents.},
  archive   = {C_IROS},
  author    = {Tianwei Niu and Shuwei Yu and Liang Wang and Haoyu Yuan and Shoukun Wang and Junzheng Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802161},
  month     = {10},
  pages     = {11700-11706},
  title     = {Real-time terrain assessment and bayesian-based path planning for off-road navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On predicting terrain changes induced by mobile robot
traversal. <em>IROS</em>, 11694–11699. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile robots operating in convoys have a limited view of the terrain to be traversed if it is occluded by the preceding vehicle. Furthermore, the preceding vehicle might change the terrain geometry and eventually significantly alter its traversability by driving over the terrain. When the following vehicles do not consider such changes, they can use spurious terrain appearance and geometry to decide whether to follow in the tracks of the previous vehicle or to avoid them since the preceding vehicle’s tracks can make the terrain untraversable. We propose to predict the terrain changes induced by the robot traversal on the traversed terrain and thus support the decision-making of the following vehicles. The developed model projects the robot wheel footprint along the planned robot path and combines the projection with the terrain appearance and prior terrain elevation. The coupled model is used in a convolutional neural network that predicts the elevation after traversal. The footprint projection component is designed so that learned networks can be transferred to vehicles with different wheel footprints without relearning the model. The proposed model is verified using a dataset captured using a real, one-ton, six-wheel robot traversing rigid roads and vegetated fields.},
  archive   = {C_IROS},
  author    = {Miloš Prágr and Jan Bayer and Jan Faigl},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802070},
  month     = {10},
  pages     = {11694-11699},
  title     = {On predicting terrain changes induced by mobile robot traversal},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Proprioception is all you need: Terrain classification for
boreal forests. <em>IROS</em>, 11686–11693. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent works in field robotics highlighted the importance of resiliency against different types of terrains. Boreal forests, in particular, are home to many mobility-impeding terrains that should be considered for off-road autonomous navigation. Also, being one of the largest land biomes on Earth, boreal forests are an area where autonomous vehicles are expected to become increasingly common. In this paper, we address the issue of classifying boreal terrains by introducing BorealTC, a publicly available dataset for proprioceptive-based terrain classification (TC). Recorded with a Husky A200, our dataset contains 116 min of Inertial Measurement Unit (IMU), motor current, and wheel odometry data, focusing on typical boreal forest terrains, notably snow, ice, and silty loam. Combining our dataset with another dataset from the literature, we evaluate both a Convolutional Neural Network (CNN) and the novel state space model (SSM)-based Mamba architecture on a TC task. We show that while CNN outperforms Mamba on each separate dataset, Mamba achieves greater accuracy when trained on a combination of both. In addition, we demonstrate that Mamba’s learning capacity is greater than a CNN for increasing amounts of data. We show that the combination of two TC datasets yields a latent space that can be interpreted with the properties of the terrains. We also discuss the implications of merging datasets on classification. Our source code and dataset are publicly available online: https://github.com/norlab-ulaval/BorealTC.},
  archive   = {C_IROS},
  author    = {Damien LaRocque and William Guimont-Martin and David-Alexandre Duclos and Philippe Giguère and François Pomerleau},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801407},
  month     = {10},
  pages     = {11686-11693},
  title     = {Proprioception is all you need: Terrain classification for boreal forests},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kinetic-energy-optimal and safety-guaranteed trajectory
planning for bridge inspection robot manipulator. <em>IROS</em>,
11678–11685. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bridge inspections are essential for maintaining key infrastructure and preventing structural and functional failures. Nevertheless, traditional manual inspection techniques are plagued by laboriousness, high risk, and low efficiency. Although numerous automation inspection methods have been studied, inspection performance remains challenging. The main difficulties are redundant mechanisms, complex control, high energy consumption, and limited autonomy and safety. To address these problems, we are developing a small, lightweight, electrically-driven robotic manipulator for bridge inspection named the BIRM. Here, we propose a kinetic-energy-optimal and safety-guaranteed trajectory planning for BIRM. Compared with existing methods, it simultaneously addresses energy consumption and safety. The approach formulates a quadratic programming (QP) problem by considering the robot’s kinetic energy as the objective function, and the augmented Lagrange multiplier (ALM) is applied to find the solution of the QP. The proposed method completely satisfies the joint position, velocity, and acceleration limits at the speed level while considering collision avoidance. In this paper, the collision detection strategy can achieve low-complexity computation through several structural parameters of the bridge, thereby quickly adapting to environmental changes. Through simulation experiments, we validate the effectiveness and superiority of the proposed method. Through physical experiments, we demonstrate the sustainability and safety of bridge inspections in the field.},
  archive   = {C_IROS},
  author    = {Tianyu Zhang and Yong Chang and Hongguang Wang and Tianlong Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802354},
  month     = {10},
  pages     = {11678-11685},
  title     = {Kinetic-energy-optimal and safety-guaranteed trajectory planning for bridge inspection robot manipulator},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PhysORD: A neuro-symbolic approach for physics-infused
motion prediction in off-road driving. <em>IROS</em>, 11670–11677. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion prediction is critical for autonomous off-road driving, however, it presents significantly more challenges than on-road driving because of the complex interaction between the vehicle and the terrain. Traditional physics-based approaches encounter difficulties in accurately modeling dynamic systems and external disturbance. In contrast, data-driven neural networks require extensive datasets and struggle with explicitly capturing the fundamental physical laws, which can easily lead to poor generalization. By merging the advantages of both methods, neuro-symbolic approaches present a promising direction. These methods embed physical laws into neural models, potentially significantly improving generalization capabilities. However, no prior works were evaluated in real-world settings for off-road driving. To bridge this gap, we present PhysORD, a neural-symbolic approach integrating the conservation law, i.e., the Euler-Lagrange equation, into data-driven neural models for motion prediction in off-road driving. Our experiments showed that PhysORD can accurately predict vehicle motion and tolerate external disturbance by modeling uncertainties. It outperforms existing methods both in accuracy and efficiency and demonstrates data-efficient learning and generalization ability in long-term prediction.},
  archive   = {C_IROS},
  author    = {Zhipeng Zhao and Bowen Li and Yi Du and Taimeng Fu and Chen Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802099},
  month     = {10},
  pages     = {11670-11677},
  title     = {PhysORD: A neuro-symbolic approach for physics-infused motion prediction in off-road driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RATE: Real-time asynchronous feature tracking with event
cameras. <em>IROS</em>, 11662–11669. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision-based self-localization is a crucial technology for enabling autonomous robot navigation in GPS-deprived environments. However, standard frame cameras are subject to motion blur and suffer from a limited dynamic range. This research focuses on efficient feature tracking for self-localization by using event-based cameras. Such cameras do not provide regular snapshots of the environment but asynchronously collect events that correspond to a small delta of illumination in each pixel independently, thus addressing the issue of motion blur during fast motion and high dynamic range. Specifically, we propose a continuous real-time asynchronous event-based feature tracking pipeline, named RATE. This pipeline integrates (i) a corner detector node utilizing a time slice of the Surface of Active Events to initialize trackers continuously, along with (ii) a tracker node with a proposed &quot;tracking manager&quot;, consisting of a grid-based distributor to reduce redundant trackers and to remove feature tracks of poor quality. Evaluations using public datasets reveal that our method maintains a stable number of tracked features, and performs real-time tracking efficiently while maintaining or even improving tracking accuracy compared to state-of-the-art event-only tracking methods. Our ROS implementation is released as open-source: https://github.com/mikihiroikura/RATE},
  archive   = {C_IROS},
  author    = {Mikihiro Ikura and Cedric Le Gentil and Marcus G. Müller and Florian Schuler and Atsushi Yamashita and Wolfgang Stürzl},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802050},
  month     = {10},
  pages     = {11662-11669},
  title     = {RATE: Real-time asynchronous feature tracking with event cameras},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AirShot: Efficient few-shot detection for autonomous
exploration. <em>IROS</em>, 11654–11661. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Few-shot object detection has drawn increasing attention in the field of robotic exploration, where robots are required to find unseen objects with a few online provided examples. Despite recent efforts have been made to yield online processing capabilities, slow inference speeds of low-powered robots fail to meet the demands of real-time detection-making them impractical for autonomous exploration. Existing methods still face performance and efficiency challenges, mainly due to unreliable features and exhaustive class loops. In this work, we propose a new paradigm AirShot, and discover that, by fully exploiting the valuable correlation map, AirShot can result in a more robust and faster few-shot object detection system, which is more applicable to robotics community. The core module Top Prediction Filter (TPF) can operate on multi-scale correlation maps in both the training and inference stages. During training, TPF supervises the generation of a more representative correlation map, while during inference, it reduces looping iterations by selecting top-ranked classes, thus cutting down on computational costs with better performance. Surprisingly, this dual functionality exhibits general effectiveness and efficiency on various off-the-shelf models. Exhaustive experiments on COCO2017, VOC2014, and SubT datasets demonstrate that TPF can significantly boost the efficacy and efficiency of most off-the-shelf models, achieving up to 36.4% precision improvements along with 56.3% faster inference speed. Code and Data are at: https://github.com/ImNotPrepared/AirShot.},
  archive   = {C_IROS},
  author    = {Zihan Wang and Bowen Li and Chen Wang and Sebastian Scherer},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801738},
  month     = {10},
  pages     = {11654-11661},
  title     = {AirShot: Efficient few-shot detection for autonomous exploration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable network and adaptive refinement module for 6D pose
estimation of diverse industrial components*. <em>IROS</em>,
11646–11653. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The estimation of the 6D pose of industrial components is essential for smart manufacturing. Especially for complex units that require intensive manual operations, such as a concentrator photovoltaics solar panel, accurate spatial localization provides visual aids for industrial automation. In this paper, we propose an accurate and scalable framework to address the dimensional variability of industrial components and tackle practical implementation issues. First, we use the scalable architecture EfficientNet as the backbone coupled with an enhanced feature pyramid network to estimate the object’s pose. By introducing vertical and horizontal connections of shallow layers, the feature extraction of small objects is optimized for better detection accuracy. Second, leveraging the reliable 2D detection results and geometry information, an adaptive pose refinement module is designed to adjust the estimated 6D pose. The scaling of the backbone network and the computational complexity of refined modules are uniformly adjusted via a shared hyperparameter, resulting in a globally scalable framework. In terms of the pose estimation accuracy, the effectiveness of the refinement module and the real-time performance, validations are conducted both on the LINEMOD dataset and our customized datasets comprising of objects from the industrial photovoltaic system. Additionally, to further illustrate the effectiveness of the proposed method, a precision parallel robot is employed to validate the accuracy of real-time object pose tracking.},
  archive   = {C_IROS},
  author    = {Kun Qian and Mustafa Suphi Erden and Xianwen Kong},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801744},
  month     = {10},
  pages     = {11646-11653},
  title     = {Scalable network and adaptive refinement module for 6D pose estimation of diverse industrial components*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Monocular 3D reconstruction of cheetahs in the wild*.
<em>IROS</em>, 11639–11645. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a framework for monocular 3D reconstruction of cheetah movements, leveraging a combination of data-driven and physics-based modeling as well as trajectory optimization. Unlike traditional methods that rely solely on kinematics, our approach integrates dynamic motion principles, enhancing the plausibility and generalization of motion estimates. Validated on the cheetah running dataset, AcinoSet, we achieve mean per-joint position errors of 78.8 mm and 72.5 mm, showcasing significant advancements over the existing model used in AcinoSet. By addressing the challenge of absent ground truth data, this work not only advances animal motion capture techniques but also informs the development of bio-inspired robotic systems, offering a robust solution for accurately capturing complex animal locomotion in natural settings.},
  archive   = {C_IROS},
  author    = {Zico Da Silva and Zuhayr Parkar and Naoya Muramatsu and Fred Nicolls and Amir Patel},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802061},
  month     = {10},
  pages     = {11639-11645},
  title     = {Monocular 3D reconstruction of cheetahs in the wild*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing 3D single object tracking with efficient point
cloud segmentation. <em>IROS</em>, 11631–11638. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D single object tracking (SOT) based on point cloud has attracted much attention due to its important role in machine vision and autonomous driving. Recently, M2-Track proposes a two-stage tracking structure centered on motion, but they ignore the effect of segmentation errors in sparse point cloud scenarios, which hinder the ability of networks to accurately represent tracking targets. To solve the problems, we propose an efficient 3D single object tracker (Abbr. EST) that can effectively segment point cloud features. Firstly, the proposed fusion segmentation module makes up for the feature loss caused by the downsampling strategy and enhances the ability of the network to recognize foreground points. In addition, the global embedded module is used to further focus on the crucial features of the target. This module provides global information by using residual networks and adding background information. Numerous experiments conducted on KITTI and NuScenes benchmarks show that EST achieves superior point cloud tracking in both performance and efficiency.},
  archive   = {C_IROS},
  author    = {Yushi Yang and Baojie Fan and Yuyu Jiang and Wuyang Zhou and Dong Chen and Hongxin Xu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801597},
  month     = {10},
  pages     = {11631-11638},
  title     = {Enhancing 3D single object tracking with efficient point cloud segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Differentiable fluid physics parameter identification by
stirring and for stirring. <em>IROS</em>, 11624–11630. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fluid interactions are crucial in daily tasks, with properties like density and viscosity being key parameters. The property states can be used as control signals for robot operation. While density estimation is simple, assessing viscosity, especially for different fluid types, is complex. This study introduces a novel differentiable fitting framework, DiffStir, tailored to identify key physics parameters through stirring. Then, given the estimated physics parameters, we can generate commands to guide the robotic stirring. Comprehensive experiments were conducted to validate the efficacy of DiffStir, showcasing its precision in parameter estimation when benchmarked against reported values in the literature. More experiments and videos can be found in the supplementary materials and on the website: https://diffstir.robotflow.ai.},
  archive   = {C_IROS},
  author    = {Wenqiang Xu and Dongzhe Zheng and Yutong Li and Jieji Ren and Cewu Lu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801842},
  month     = {10},
  pages     = {11624-11630},
  title     = {Differentiable fluid physics parameter identification by stirring and for stirring},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Refining airway segmentation through breakage filling and
leakage reduction using point clouds. <em>IROS</em>, 11618–11623. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bronchoscopy reveals air passages and internal tissues for accurate diagnosis of various lung diseases. Robot-assisted bronchoscopy using an airway tree model can help path planning before surgery and navigation during surgery. In airway tree modeling, though volumetric deep learning methods have achieved good performance for airway segmentation, it remains a challenge due to the breakages and leakages. Some existing methods adopt post-processing using traditional methods like morphological and fuzzy connected algorithms. Also, some methods convert the volumetric data to point cloud format to refine segmentation. In this paper, we develop a new point cloud-based approach to refine volumetric segmentation. To address the breakage issue, we approach it as a regression problem of the branch extension direction and length. To tackle the leakage issue, we approach it as a segmentation task to eliminate leakages caused by breakage filling and from volumetric segmentation. Moreover, the direction information of branches is crucial for constructing the airway tree while point clouds do not naturally encode it. To introduce this information, we propose a directional feature aggregation, which first decomposes features of neighboring points based on their locations and aggregates decomposed features to aid the network in capturing the directional information effectively. Our proposed model has been evaluated on two public datasets, and the results show that our refinement can improve the volumetric segmentation.},
  archive   = {C_IROS},
  author    = {Yan Hu and Erik Meijering and Yang Song},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802488},
  month     = {10},
  pages     = {11618-11623},
  title     = {Refining airway segmentation through breakage filling and leakage reduction using point clouds},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Click to grasp: Zero-shot precise manipulation via visual
diffusion descriptors. <em>IROS</em>, 11610–11617. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Precise manipulation that is generalizable across scenes and objects remains a persistent challenge in robotics. Current approaches for this task heavily depend on having a significant number of training instances to handle objects with pronounced visual and/or geometric part ambiguities. Our work explores the grounding of fine-grained part descriptors for precise manipulation in a zero-shot setting by utilizing web-trained text-to-image diffusion-based generative models. We tackle the problem by framing it as a dense semantic part correspondence task. Our model returns a gripper pose for manipulating a specific part, using as reference a user-defined click from a source image of a visually different instance of the same object. We require no manual grasping demonstrations as we leverage the intrinsic object geometry and features. Practical experiments in a real-world tabletop scenario validate the efficacy of our approach, demonstrating its potential for advancing semantic-aware robotics manipulation.Web page: https://tsagkas.github.io/click2grasp},
  archive   = {C_IROS},
  author    = {Nikolaos Tsagkas and Jack Rome and Subramanian Ramamoorthy and Oisin Mac Aodha and Chris Xiaoxuan Lu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801488},
  month     = {10},
  pages     = {11610-11617},
  title     = {Click to grasp: Zero-shot precise manipulation via visual diffusion descriptors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robot traversability prediction: Towards third-person-view
extension of Walk2Map with photometric and physical constraints.
<em>IROS</em>, 11602–11609. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Walk2Map has emerged as a promising data-driven method to generate indoor traversability maps based solely on pedestrian trajectories, offering great potential for indoor robot navigation. In this study, we investigate a novel approach called Walk2Map++, which involves replacing Walk2Map’s first-person sensor (i.e., IMU) with a human observing third-person view from the robot’s onboard camera. However, human observation from a third-person camera is significantly ill-posed due to visual uncertainties resulting from occlusion, nonlinear perspective, depth ambiguity, and human-to-human interaction. To regularize the ill-posedness, we propose integrating two types of constraints: photometric (i.e., occlusion ordering) and physical (i.e., collision avoidance). We demonstrate that these constraints can be effectively inferred from the interaction between past and present observations, human trackers, and object reconstructions. We depict the seamless integration of asynchronous map optimization events, like loop closure, into the real-time traversability map, facilitating incremental and efficient map refinement. We validate the efficacy of our enhanced methodology through rigorous fusion and comparison with established techniques, demonstrating its capability to advance traversability prediction in complex indoor environments. The code and datasets associated with this study are available for further research and adoption in the field at https://github.com/jonathantyl97/HO3-SLAM.},
  archive   = {C_IROS},
  author    = {Jonathan Tay Yu Liang and Kanji Tanaka},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802356},
  month     = {10},
  pages     = {11602-11609},
  title     = {Robot traversability prediction: Towards third-person-view extension of Walk2Map with photometric and physical constraints},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FoveaCam++: Systems-level advances for long range
multi-object high-resolution tracking. <em>IROS</em>, 11594–11601. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {UAVs and other fast moving robots often need to keep track of distant objects. Conventional zoom cameras commit to a particular viewpoint, and carrying multiple zoom cameras for multi-object tracking is not feasible for power limited robotic systems. We present a dual camera setup that allows tracking of multiple targets at nearly 1km distance with high-resolution. Our setup includes a wide angle camera providing a conventional resolution view and a MEMS driven zoom camera that can query a specific region within the wide angle camera (WAC). We built and calibrated the two-camera system and implemented a real-time image fusion pipeline. We show multi-object tracking and stabilization in real world scenarios.},
  archive   = {C_IROS},
  author    = {Yuxuan Zhang and Sanjeev J. Koppal},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802188},
  month     = {10},
  pages     = {11594-11601},
  title     = {FoveaCam++: Systems-level advances for long range multi-object high-resolution tracking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OW3Det: Toward open-world 3D object detection for autonomous
driving. <em>IROS</em>, 11587–11593. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite their success in LIDAR object detection, modern detectors are vulnerable to uncommon instances and corner cases (e.g., a runaway tire) since they are closed-set and static. Networks under the closed-set setup only predict labels of seen classes, while static models suffer from catastrophic forgetting when gradually learning novel concepts. This motivates us to formulate the open-world 3D object detection task for autonomous driving, which aims to 1) tackle the closed-set issue by identifying unseen instances as unknown and 2) incrementally learn novel classes without forgetting previously obtained knowledge. To achieve the open-world objectives, we propose Open-World 3D Detector (OW3Det), the first framework for open-world 3D object detection. The OW3Det comprises a base detector, a self-supervised unknown identifier, and a knowledge-distillation-restricted incremental learner. Although knowledge distillation facilitates preserving memories, imposing penalties on areas containing unknown objects hinders the incremental learning process. We mitigate this hindrance by employing unknown-driven pivotal mask, which eliminates unnecessary restrictions on regions overlapping with novel instances. Abundant experiments and visualizations demonstrate that the proposed OW3Det attains state-of-the-art performance.},
  archive   = {C_IROS},
  author    = {Wenfei Hu and Weikai Lin and Hongyu Fang and Yi Wang and Dingsheng Luo},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802635},
  month     = {10},
  pages     = {11587-11593},
  title     = {OW3Det: Toward open-world 3D object detection for autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Direct TPS-based 3D non-rigid motion estimation on 3D
colored point cloud in eye-in-hand configuration. <em>IROS</em>,
11581–11586. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, a method for 3D non-rigid motion estimation of a surface using an RGB-D camera in eye-in-hand configuration is presented. The eye-in-hand configuration eliminates errors typically associated with camera-end-effector calibration, and is thus desirable for task on moving surfaces such as bioprinting. However, its implementation is challenging since camera and surface of interest are moving, making mesh-based approaches unsuitable. Thus, the proposed method operates directly on point clouds, benefiting from accurate and simplified data processing. A point cloud contains both intensity and depth data, with the former used to estimate in-plane deformation and the latter to compute full 3D deformation. Surface deformation is modeled via a Thin Plate Spline model. The method accuracy is assessed at 0.1 mm accuracy in simulated datasets, rendering it suitable for precision tasks, and its feasibility is validated experimentally on a moving platform that deforms at a rate of 0.8 Hz with a 4 mm in-plane amplitude and a 20 mm elevation amplitude.},
  archive   = {C_IROS},
  author    = {Lénaïc Cuau and João Cavalcanti Santos and Philippe Poignet and Nabil Zemiti},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802824},
  month     = {10},
  pages     = {11581-11586},
  title     = {Direct TPS-based 3D non-rigid motion estimation on 3D colored point cloud in eye-in-hand configuration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Every dataset counts: Scaling up monocular 3D object
detection with joint datasets training. <em>IROS</em>, 11574–11580. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Monocular 3D object detection is essential for autonomous driving. However, current monocular 3D detection algorithms rely on expensive 3D labels from LiDAR scans, making it difficult to use in new datasets and unfamiliar environments. This study explores training a monocular 3D object detection model using a mix of 3D and 2D datasets. The proposed framework includes a robust monocular 3D model that can adapt to different camera settings, a selective-training strategy to handle varying class annotations in datasets, and a pseudo 3D training method using 2D labels to improve detection ability in scenes with only 2D labels (as shown in Fig. 1). By utilizing this framework, we can train models on a combination of 3D and 2D datasets to improve generalization and performance on new datasets with only 2D labels. Extensive experiments on KITTI, nuScenes, ONCE, Cityscapes, and BDD100K datasets showcase the scalability of our proposed approach. Here is our project page: https://sites.google.com/view/fmaafmono3d.},
  archive   = {C_IROS},
  author    = {Fulong Ma and Xiaoyang Yan and Guoyang Zhao and Xiaojie Xu and Yuxuan Liu and Jun Ma and Ming Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802623},
  month     = {10},
  pages     = {11574-11580},
  title     = {Every dataset counts: Scaling up monocular 3D object detection with joint datasets training},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A low-cost, high-speed, and robust bin picking system for
factory automation enabled by a non-stop, multi-view, and active vision
scheme. <em>IROS</em>, 11566–11573. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bin picking systems in factory automation usually face robustness issues caused by sparse and noisy 3D data of metallic objects. Utilizing multiple views, especially with a one-shot 3D sensor and &quot;sensor on hand&quot; configuration is getting more popularity due to its effectiveness, flexibility, and low cost. While moving the 3D sensor to acquire multiple views for 3D fusion, joint optimization, or active vision suffers from low-speed issues. That is because sensing is taken as a decoupled module from motion tasks and is not intentionally designed for a bin picking system. To address the problems, we designed a bin picking system, which tightly couples a multi-view, active vision scheme with motion tasks in a &quot;sensor on hand&quot; configuration. It not only speeds up the system by parallelizing the high-speed sensing scheme to the robot place action but also decides the next sensing path to maintain the continuity of the whole picking process. Unlike others focusing only on sensing evaluation, we also evaluated our design by picking experiments on 5 different types of objects without human intervention. Our experiments show the whole sensing scheme can be finished within 1.682 seconds (maximum) on CPU and the average picking complete rate is over 97.75%. Due to the parallelization with robot motion, the sensing scheme accounts for only 0.635 seconds in takt time on average.},
  archive   = {C_IROS},
  author    = {Xingdou Fu and Lin Miao and Yasuhiro Ohnishi and Yuki Hasegawa and Masaki Suwa},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802485},
  month     = {10},
  pages     = {11566-11573},
  title     = {A low-cost, high-speed, and robust bin picking system for factory automation enabled by a non-stop, multi-view, and active vision scheme},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhanced model robustness to input corruptions by
per-corruption adaptation of normalization statistics. <em>IROS</em>,
11558–11565. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Developing a reliable vision system is a fundamental challenge for robotic technologies (e.g., indoor service robots and outdoor autonomous robots) which can ensure reliable navigation even in challenging environments such as adverse weather conditions (e.g., fog, rain), poor lighting conditions (e.g., over/under exposure), or sensor degradation (e.g., blurring, noise), and can guarantee high performance in safety-critical functions. Current solutions proposed to improve model robustness usually rely on generic data augmentation techniques or employ costly test-time adaptation methods. In addition, most approaches focus on addressing a single vision task (typically, image recognition) utilising synthetic data. In this paper, we introduce Per-corruption Adaptation of Normalization statistics (PAN) to enhance the model robustness of vision systems. Our approach entails three key components: (i) a corruption type identification module, (ii) dynamic adjustment of normalization layer statistics based on identified corruption type, and (iii) real-time update of these statistics according to input data. PAN can integrate seamlessly with any convolutional model for enhanced accuracy in several robot vision tasks. In our experiments, PAN obtains robust performance improvement on challenging real-world corrupted image datasets (e.g., OpenLoris, ExDark, ACDC), where most of the current solutions tend to fail. Moreover, PAN outperforms the baseline models by 20-30% on synthetic benchmarks in object recognition tasks.},
  archive   = {C_IROS},
  author    = {Elena Camuffo and Umberto Michieli and Simone Milani and Jijoong Moon and Mete Ozay},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802512},
  month     = {10},
  pages     = {11558-11565},
  title     = {Enhanced model robustness to input corruptions by per-corruption adaptation of normalization statistics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An ultrafast multi-object zooming system based on
low-latency stereo correspondence. <em>IROS</em>, 11552–11557. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we develop a multiple-object zooming system which can capture clear images of different objects at an ultrafast speed. The system consists of a panoramic HFR stereo camera and a galvanometer-based reflective pan-tilt-zoom (PTZ) camera. In order to alleviate the impact of brightness, noise, and viewing angle in the image, we use the high speed motion information of the object for stereo correspondence. According to the spatial positions of all objects obtained from HFR stereo correspondence, we can obtain the control voltage of the pan and tilt mirror of the galvanometer-based reflective PTZ camera through the mapping relationship. Then, PTZ camera captures clear images of multiple objects in a time-division multiplexed manner at an extremely fast speed. Experimental results show that we can distinguish multiple fast-moving people indoors in the HFR stereo camera and capture their high-definition facial images simultaneously.},
  archive   = {C_IROS},
  author    = {Qing Li and Shaopeng Hu and Kohei Shimasaki and Idaku Ishii},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802734},
  month     = {10},
  pages     = {11552-11557},
  title     = {An ultrafast multi-object zooming system based on low-latency stereo correspondence},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human orientation estimation under partial observation.
<em>IROS</em>, 11544–11551. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reliable Human Orientation Estimation (HOE) from a monocular image is critical for autonomous agents to understand human intention. Significant progress has been made in HOE under full observation. However, the existing methods easily make a wrong prediction under partial observation and give it an unexpectedly high confidence. To solve the above problems, this study first develops a method called Part-HOE that estimates orientation from the visible joints of a target person so that it is able to handle partial observation. Subsequently, we introduce a confidence-aware orientation estimation method, enabling more accurate orientation estimation and reasonable confidence estimation under partial observation. The effectiveness of our method is validated on both public and custom-built datasets, and it shows great accuracy and reliability improvement in partial observation scenarios. In particular, we show in real experiments that our method can benefit the robustness and consistency of the Robot Person Following (RPF) task.},
  archive   = {C_IROS},
  author    = {Jieting Zhao and Hanjing Ye and Yu Zhan and Hao Luan and Hong Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802390},
  month     = {10},
  pages     = {11544-11551},
  title     = {Human orientation estimation under partial observation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learned sensor fusion for robust human activity recognition
in challenging environments. <em>IROS</em>, 11537–11543. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human activity recognition is a vital area of robotics with significant real-world applications, from enhancing security and surveillance to improving healthcare and human-robot interaction. A critical challenge lies in bridging the gap between research models, which often assume ideal conditions, and the complexities of real-world environments. In practice, conditions can be far from perfect, including scenarios with poor lighting, adverse weather, or blurred views. In this paper, we present an innovative approach for robust activity recognition through learned sensor fusion, in which our recognition framework identifies a latent weighted combination of input modalities, enabling classifiers to capitalize on advantages provided by various sensors. In support of our work, we have released a dataset of human activities across multiple modalities with environmental degradation factors such as darkness, fog, and thermal blur. Our proposed approach identifies a weighted combination of modality representations derived from existing architectures. We show that our approach is able to achieve 24% higher classification performance than existing single-modality approaches. Our approach also attains comparable performance to modality fusion approaches in significantly reduced classification time. In real-world robotics applications, particularly those occurring in dangerous, degraded environments, this speed is critical.},
  archive   = {C_IROS},
  author    = {Max Conway and Brian Reily and Christopher Reardon},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802830},
  month     = {10},
  pages     = {11537-11543},
  title     = {Learned sensor fusion for robust human activity recognition in challenging environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Aligning learning with communication in shared autonomy.
<em>IROS</em>, 11530–11536. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Assistive robot arms can help humans by partially automating their desired tasks. Consider an adult with motor impairments controlling an assistive robot arm to eat dinner. The robot can reduce the number of human inputs — and how precise those inputs need to be — by recognizing what the human wants (e.g., a fork) and assisting for that task (e.g., moving towards the fork). Prior research has largely focused on learning the human’s task and providing meaningful assistance. But as the robot learns and assists, we also need to ensure that the human understands the robot’s intent (e.g., does the human know the robot is reaching for a fork?). In this paper, we study the effects of communicating learned assistance from the robot back to the human operator. We do not focus on the specific interfaces used for communication. Instead, we develop experimental and theoretical models of a) how communication changes the way humans interact with assistive robot arms, and b) how robots can harness these changes to better align with the human’s intent. We first conduct online and in-person user studies where participants operate robots that provide partial assistance, and we measure how the human’s inputs change with and without communication. With communication, we find that humans are more likely to intervene when the robot incorrectly predicts their intent, and more likely to release control when the robot correctly understands their task. We then use these findings to modify an established robot learning algorithm so that the robot can correctly interpret the human’s inputs when communication is present. Our results from a second in-person user study suggest that this combination of communication and learning outperforms assistive systems that isolate either learning or communication. See videos here: https://youtu.be/BET9yuVTVU4},
  archive   = {C_IROS},
  author    = {Joshua Hoegerman and Shahabedin Sagheb and Benjamin A. Christie and Dylan P. Losey},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801897},
  month     = {10},
  pages     = {11530-11536},
  title     = {Aligning learning with communication in shared autonomy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhanced robotic assistance for human activities through
human-object interaction segment prediction. <em>IROS</em>, 11523–11529.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic assistance is a current research topic with high application value and multiple challenges. Assistive robots are used in various scenarios, such as production lines, operating tables, and elderly care. While providing effective assistance, most of the assistance tasks that current robots can perform are limited to predefined tasks. This limitation arises from the insufficiency of the current robot perception system to forecast future human activities. To address this issue, we propose a novel 2-stage robotic assistant for human activities through future human-object interaction (HOI) segment prediction. Unlike previous work focusing on predefined or short-term tasks, our robotic assistant can make predictions for future assistance according to human habits. In the first stage, we propose a visual-based human-object interaction segment prediction method to predict human activities, which enables the robotic system to infer human intention. Moreover, we define the robotic executable tasks as an interactive tuple to keep the robotic assistance normatively consistent with human activity. Meanwhile, a graph convolutional network with geometric features that can predict human-object interaction segments is proposed to provide target manipulation and target object for the assistive robot. In the second stage, we present a mobile task completion process including visual navigation, object localization and grasping. The perception stage is evaluated on the MPHOI dataset and custom-collected SPHOI dataset. Finally, we evaluate our comprehensive framework through real-time experimentation.},
  archive   = {C_IROS},
  author    = {Yuankai Wu and Rayene Messaoud and Arne-Christoph Hildebrandt and Marco Baldini and Driton Salihu and Constantin Patsch and Eckehard Steinbach},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801332},
  month     = {10},
  pages     = {11523-11529},
  title     = {Enhanced robotic assistance for human activities through human-object interaction segment prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Can reasons help improve pedestrian intent estimation? A
cross-modal approach. <em>IROS</em>, 11515–11522. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the increased importance of autonomous navigation systems has come an increasing need to protect the safety of Vulnerable Road Users (VRUs) such as pedestrians. Predicting pedestrian intent is one such challenging task, where prior work predicts the binary cross/no-cross intention with a fusion of visual and motion features. However, there has been no effort so far to hedge such predictions with human-understandable reasons. We address this issue by introducing a novel problem setting of exploring the intuitive reasoning behind a pedestrian’s intent. In particular, we show that predicting the ‘WHY’ can be very useful in understanding the ‘WHAT’. To this end, we propose a novel, reason-enriched PIE++ dataset consisting of multi-label textual explanations/reasons for pedestrian intent. We also introduce a novel multi-task learning framework called MINDREAD, which leverages a cross-modal representation learning framework for predicting pedestrian intent as well as the reason behind the intent. Our comprehensive experiments show significant improvement of 5.6% and 7% in accuracy and F1-score for the task of intent prediction on the PIE++ dataset using MINDREAD. We also achieved a 4.4% improvement in accuracy on a commonly used JAAD dataset. Extensive evaluation using quantitative/qualitative metrics and user studies shows the effectiveness of our approach.},
  archive   = {C_IROS},
  author    = {Vaishnavi Khindkar and Vineeth Balasubramanian and Chetan Arora and Anbumani Subramanian and C.V. Jawahar},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802097},
  month     = {10},
  pages     = {11515-11522},
  title     = {Can reasons help improve pedestrian intent estimation? a cross-modal approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-view 2D to 3D lifting video-based optimization: A
robust approach for human pose estimation with occluded joint
prediction*. <em>IROS</em>, 11508–11514. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the context of robotics, accurate 3D human pose estimation is essential for enhancing human-robot collaboration and interaction. This manuscript introduces a multi-view 2D to 3D lifting optimization-based method designed for video-based 3D human pose estimation, incorporating temporal information. Our technique addresses key challenges, namely robustness to 2D joint detection error, occlusions, and varying camera perspectives. We evaluate the performance of the algorithm through extensive experiments on the MPI-INF-3DHP dataset. Our method demonstrates very good robustness up to 25 pixels of 2D joint error and shows resilience in scenarios involving several occluded joints. Comparative analyses against existing 2D to 3D lifting and multi-view methods showcase good performance of our approach.},
  archive   = {C_IROS},
  author    = {Daniela Rato and Miguel Oliveira and Vítor Santos and Angel Sappa and Bogdan Raducanu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802200},
  month     = {10},
  pages     = {11508-11514},
  title     = {Multi-view 2D to 3D lifting video-based optimization: A robust approach for human pose estimation with occluded joint prediction*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Predicting long-term human behaviors in discrete
representations via physics-guided diffusion. <em>IROS</em>,
11500–11507. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Long-term human trajectory prediction is a challenging yet critical task in robotics and autonomous systems. Prior work that studied how to predict accurate short-term human trajectories with only unimodal features often failed in long-term prediction. Reinforcement learning provides a good solution for learning human long-term behaviors but can suffer from challenges in data efficiency and optimization. In this work, we propose a long-term human trajectory forecasting framework that leverages a guided diffusion model to generate diverse long-term human behaviors in a high-level latent action space, obtained via a hierarchical action quantization scheme using a VQ-VAE to discretize continuous trajectories and the available context. The latent actions are predicted by our guided diffusion model, which uses physics-inspired guidance at test time to constrain generated multimodal action distributions. Specifically, we use reachability analysis during the reverse denoising process to guide the diffusion steps toward physically feasible latent actions. We evaluate our framework on two publicly available human trajectory forecasting datasets: SFU-Store-Nav and JRDB, and extensive experimental results show that our framework achieves superior performance in long-term human trajectory forecasting.},
  archive   = {C_IROS},
  author    = {Zhitian Zhang and Anjian Li and Angelica Lim and Mo Chen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802068},
  month     = {10},
  pages     = {11500-11507},
  title     = {Predicting long-term human behaviors in discrete representations via physics-guided diffusion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual-branch graph transformer network for 3D human mesh
reconstruction from video. <em>IROS</em>, 11493–11499. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human Mesh Reconstruction (HMR) from monocular video plays an important role in human-robot interaction and collaboration. However, existing video-based human mesh reconstruction methods face a trade-off between accurate reconstruction and smooth motion. These methods design networks based on either RNNs or attention mechanisms to extract local temporal correlations or global temporal dependencies, but the lack of complementary long-term information and local details limits their performance. To address this problem, we propose a Dual-branch Graph Transformer network for 3D human mesh Reconstruction from video, named DGTR. DGTR employs a dual-branch network including a Global Motion Attention (GMA) branch and a Local Details Refine (LDR) branch to par-allelly extract long-term dependencies and local crucial information, helping model global human motion and local human details (e.g., local motion, tiny movement). Specifically, GMA utilizes a global transformer to model long-term human motion. LDR combines modulated graph convolutional networks and the transformer framework to aggregate local information in adjacent frames and extract crucial information of human details. Experiments demonstrate that our DGTR outperforms state-of-the-art video-based methods in reconstruction accuracy and maintains competitive motion smoothness. Moreover, DGTR utilizes fewer parameters and FLOPs, which validate the effectiveness and efficiency of the proposed DGTR. Code is publicly available at https://github.com/TangTao-PKU/DGTR.},
  archive   = {C_IROS},
  author    = {Tao Tang and Hong Liu and Yingxuan You and Ti Wang and Wenhao Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801569},
  month     = {10},
  pages     = {11493-11499},
  title     = {Dual-branch graph transformer network for 3D human mesh reconstruction from video},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contacts from motion: Learning discrete features for
automatic contact detection and estimation from human movements *.
<em>IROS</em>, 11485–11492. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel method for detecting and estimating contact forces only from human motions using machine learning techniques. Knowing the location of the contacts with the environment and the magnitude of the exerted force is critical for dynamic human motion analysis. However, their annotation is usually made manually from captured motion data especially in case of multiple contacts even if the data includes force measurement. Moreover, most existing human motion datasets do not include contact force. To overcome these bottlenecks, we introduce a network that leverages vector-quantized variational autoencoder (VQ-VAE) and self-attention that learns a small set of discrete feature values representing various contact states. These feature values, called contact codes, allow human motions to be converted to contact states and resulting forces. By applying an optimization for contact estimation with a reduced set of manual annotations, the existence of contacts can be automatically determined, which is essential information for dynamic analysis. We validated the effectiveness and potential usefulness of the proposed method with a human walking gait dataset, by converting the human motions into contact sequences and forces and applying the estimated contacts to dynamic motion analysis.},
  archive   = {C_IROS},
  author    = {Hibiki Miyake and Ko Ayusawa and Ryusuke Sagawa and Eiichi Yoshida},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802414},
  month     = {10},
  pages     = {11485-11492},
  title     = {Contacts from motion: Learning discrete features for automatic contact detection and estimation from human movements *},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interactive reinforcement learning from natural language
feedback. <em>IROS</em>, 11478–11484. (<a
href="https://doi.org/10.1109/IROS58592.2024.10803055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large Language Models (LLMs) are increasingly influential in advancing robotics. This paper introduces ECLAIR (Evaluative Corrective Guidance Language as Reinforcement), a novel framework that leverages LLMs to interpret and incorporate diverse natural language feedback into robotic learning. ECLAIR unifies various forms of human advice into actionable insights within a Reinforcement Learning context, enabling more efficient robot instruction. Experiments with real-world users demonstrate that ECLAIR accelerates the robot’s learning process, aligning its policy closer to optimal from the outset and reducing the need for extensive human intervention. Additionally, ECLAIR effectively integrates multiple types of advice and adapts well to prompt modifications. It also supports multilingual instruction, broadening its applicability and fostering more inclusive human-robot interactions. Project website: https://sites.google.com/view/eclairiros},
  archive   = {C_IROS},
  author    = {Imene Tarakli and Samuele Vinanzi and Alessandro Di Nuovo},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10803055},
  month     = {10},
  pages     = {11478-11484},
  title     = {Interactive reinforcement learning from natural language feedback},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ROBOVERINE: A human-inspired neural robotic process model of
active visual search and scene grammar in naturalistic environments.
<em>IROS</em>, 11470–11477. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present ROBOVERINE, a neural dynamic robotic active vision process model of selective visual attention and scene grammar in naturalistic environments. The model addresses significant challenges for cognitive robotic models of visual attention: combined bottom-up salience and top-down feature guidance, combined overt and covert attention, coordinate transformations, two forms of inhibition of return, finding objects outside of the camera frame, integrated space-and object-based analysis, minimally supervised few-shot continuous online learning for recognition and guidance templates, and autonomous switching between exploration and visual search. Furthermore, it incorporates a neural process account of scene grammar — prior knowledge about the relation between objects in the scene — to reduce the search space and increase search efficiency. The model also showcases the strength of bridging two frameworks: Deep Neural Networks for feature extractions and Dynamic Field Theory for cognitive operations.},
  archive   = {C_IROS},
  author    = {Raul Grieben and Stephan Sehring and Jan Tekülve and John P. Spencer and Gregor Schöner},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801621},
  month     = {10},
  pages     = {11470-11477},
  title     = {ROBOVERINE: A human-inspired neural robotic process model of active visual search and scene grammar in naturalistic environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Roaming with robots: Utilizing artificial curiosity in
global path planning for autonomous mobile robots. <em>IROS</em>,
11462–11469. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous Mobile Robots are used with increasing frequency in inspection and maintenance tasks completing fixed goal sequences. The downtime robots experience between goals offers an opportunity to gather additional environment information instead of resting. Uncertainty in the amount of downtime available rules out the definition of a pre-determined schedule set by an external operator. Instead, the robot itself should decide dynamically, what information it should gather before its next task begins. This results in a multi-objective optimization problem trying to maximize information gain while utilizing as much of the available time as possible. We propose a genetic algorithm to solve the presented optimization problem and introduce two different models for artificial curiosity used inside the fitness function for gathering as much information as possible. For planning the genetic algorithm utilizes a multi-map approach using information and obstacle maps. We evaluated our models in a pre-defined and pre-mapped Gazebo environment with a given information map and evaluated their performance against an information-agnostic coverage algorithm. In this work, we show that utilizing artificial curiosity in path planning can result in major information gains by effectively using downtime.},
  archive   = {C_IROS},
  author    = {Niklas Spielbauer and Till Laube and David Oberacker and Arne Roennau and Rüdiger Dillmann},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802471},
  month     = {10},
  pages     = {11462-11469},
  title     = {Roaming with robots: Utilizing artificial curiosity in global path planning for autonomous mobile robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Voltage regulation in polymer electrolyte fuel cell systems
using gaussian process model predictive control. <em>IROS</em>,
11456–11461. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study presents a novel approach using Gaussian process model predictive control (MPC) to stabilize the output voltage of a polymer electrolyte fuel cell (PEFC) by regulating hydrogen and airflow rates. Two Gaussian process models capture PEFC dynamics, accounting for constraints like hydrogen pressure and input change rates to reduce predictive control errors. The performance of the physical model and Gaussian process MPC in handling constraints and system inputs is compared. Simulations show that the proposed Gaussian process MPC maintains the voltage at 48 V while adhering to safety constraints, even with workload disturbances from 110-120 A. Compared to traditional MPC with detailed system models, Gaussian process MPC has similar overshoot and slower response time but requires less system information and no underlying true system model.},
  archive   = {C_IROS},
  author    = {Xiufei Li and Miao Yang and Miao Zhang and Yuanxin Qi and Zhuowei Li and Senbin Yu and Yuantao Wang and Linpeng Shen and Xiang Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802243},
  month     = {10},
  pages     = {11456-11461},
  title     = {Voltage regulation in polymer electrolyte fuel cell systems using gaussian process model predictive control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Goal estimation-based adaptive shared control for
brain-machine interfaces remote robot navigation. <em>IROS</em>,
11448–11455. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this study, we propose a shared control method for teleoperated mobile robots using brain-machine interfaces (BMI). The control commands generated through BMI for robot operation face issues of low input frequency, discreteness, and uncertainty due to noise. To address these challenges, our method estimates the user’s intended goal from their commands and uses this goal to generate auxiliary commands through the autonomous system that are both at a higher input frequency and more continuous. Furthermore, by defining the confidence level of the estimation, we adaptively calculated the weights for combining user and autonomous commands, thus achieving shared control. We conducted navigation experiments in both simulated environments and participant experiments in real environments including user ratings, using a pseudo-BMI setup. As a result, the proposed method significantly reduced obstacle collisions in all experiments. It markedly shortened path lengths under almost all conditions in simulations and, in participant experiments, especially when user inputs become more discrete and noisy (p&lt;0.01). Furthermore, under such challenging conditions, it was demonstrated that users could operate more easily, with greater confidence, and at a comfortable pace through this system.},
  archive   = {C_IROS},
  author    = {Tomoka Muraoka and Tatsuya Aoki and Masayuki Hirata and Tadahiro Taniguchi and Takato Horii and Takayuki Nagai},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802197},
  month     = {10},
  pages     = {11448-11455},
  title     = {Goal estimation-based adaptive shared control for brain-machine interfaces remote robot navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Online hand movement recognition system with EEG-EMG fusion
using one-dimensional convolutional neural network. <em>IROS</em>,
11442–11447. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Upper limb amputees face significant challenges in their daily lives due to the loss of hand or arm functionality. Researchers have developed upper limb prostheses to restore normal hand movements for them. Most hand movement recognition systems of prostheses use electromyography (EMG) as the input signal source, but ignore the interrelationship with electroencephalography (EEG), which may contain valuable movement-related information as well. In order to enhance the accuracy of hand movement classification, we proposed a hand movement recognition system based on a one-dimensional convolutional neural network (1D-CNN) that combines EEG and EMG as the input signal sources to increase the quantity of accessible information. In this work, we collected the EEG and EMG of five subjects during the hand movements and used a 1D-CNN based model to classify the preprocessed signals. The average accuracy of using EEG-EMG fusion is 96.59±2.63%, significantly higher than 74.99±8.24% of using single EEG and 90.31±7.16% of using single EMG. Then, we applied the model trained by offline experiment for online recognition, and controlled the Pepper robot to complete the corresponding hand movements. The average accuracy of online recognition can reach 93.00±4.85% by using majority voting method. The results indicate that the method of EEG-EMG fusion can effectively enhance the performance of hand movement recognition system, which promote the development of upper limb prostheses and contribute to the rehabilitation of upper limb amputees.},
  archive   = {C_IROS},
  author    = {Haozheng Wang and Hao Jia and Zhe Sun and Feng Duan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801364},
  month     = {10},
  pages     = {11442-11447},
  title     = {Online hand movement recognition system with EEG-EMG fusion using one-dimensional convolutional neural network},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Development of permanent magnet elastomer-based tactile
sensor with adjustable compliance and sensitivity. <em>IROS</em>,
11434–11441. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tactile sensors are crucial in robotics as they enable robots to perceive and interact with their environment through touch, akin to the human sense of touch. Adjustable sensors that can adapt to various tasks by functional or structural modification have not been extensively explored. In terms of sensing adjustability of a sensor, two important aspects are the sensor’s sensitivity and compliance. This paper proposes a novel design for an adjustable compliance and sensitivity sensor composed of a silicone base, a permanent magnet elastomer (PME), and a printed circuit board (PCB) with magnetic transducers installed. Its adjustability is achieved by varying the pneumatic pressure. This paper presents the design, manufacturing process, and experimental characterization of such an adjustable compliance and sensitivity sensor. This paper thoroughly investigates how altering the pressure of the sensor influences its sensing properties. The results show that it can achieve adjustability in all three axes. For the current design, the sensitivity can be varied from 0.093 to 0.125 mT/N (34.41%), 0.089 to 0.13 mT/N (31.54%), and 0.169 to 0.45 mT/N (62.44%) in the X-, Y-, and Z-axis, respectively. The deformation it undertakes varies from 3.20 to 3.79 mm (18.44%), indicating the compliance change.},
  archive   = {C_IROS},
  author    = {Devesh Abhyankar and Yushi Wang and Yuhiro Iwamoto and Shigeki Sugano and Mitsuhiro Kamezaki},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801819},
  month     = {10},
  pages     = {11434-11441},
  title     = {Development of permanent magnet elastomer-based tactile sensor with adjustable compliance and sensitivity},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). A facile one-step injection novel composite sensor for
robot tactile assistance. <em>IROS</em>, 11428–11433. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tactile information is the research hotspot of wearable flexible sensors due to its importance and complexity. With the innovation of wearable technology and robotics in healthcare, researchers are increasingly integrating wearable flexible sensors on the front end of robots to reproduce the hand tactile manipulation of human tissues. Therefore, it is hoped to develop a thin-film sensor that can be deployed in a small area to assist robots in surgery and data collection of human tissues. Here we use a one-step injection method to fabricate a novel composite sensor based on liquid metal. By laminating multiple PDMS microfluidic layers, the two parameters of pressure and deformation are measured simultaneously in a decoupled manner. The sensor is small and thin, making it easy to integrate into fingers/robot fingers for assistance. The finger/robot finger exerts pressure on the sensor and the sensor deforms with the material to identify the hardness of the material being touched. Separate performance tests of the two sensors show that the strain and pressure functions are decoupled from each other, and their ratios can identify and classify the hardness of different touched materials (glass, PDMS and silicone). This novel composite sensor we proposed can assist robots in manipulating human tissues during medical surgeries. At the same time, its function in tactile information feedback also has broad applications in medical treatment, rehabilitation and services.},
  archive   = {C_IROS},
  author    = {Yuyin Zhang and Yue Wang and Na Liu and Songyi Zhong and Long Li and Xie Xie and Quan Zhang and Tao Yue and Toshio Fukuda},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802563},
  month     = {10},
  pages     = {11428-11433},
  title     = {A facile one-step injection novel composite sensor for robot tactile assistance},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bistable valve for electronics-free soft robots.
<em>IROS</em>, 11422–11427. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, there has been a notable shift towards electronics-free designs, which offer promising integration possibilities with soft robots, reducing reliance on traditional electronics. Despite numerous demonstrations showcasing logical control, contact sensors, and gait control, the conventional quake valve-based design is gradually struggling to meet the demands of electronics-free soft robots with increasingly complex functionalities. Integrating multiple tubes, channels, and valves has led to larger and bulkier overall systems. In this study, we introduce a simple yet powerful electronics-free pneumatic valve that excels in various aspects: it allows for flexible function configurations (operating individually, in pairs, or in larger groups), offers high-frequency synchronous reverse outputs, stores valve status, and ensures efficient maintenance. We believe that this work lays the groundwork for developing straightforward yet highly effective fully autonomous soft robots.},
  archive   = {C_IROS},
  author    = {Kan Longxin and Lam Jia Qing Joshua and Qin Zhihang and Li Keyi and Tang Zhiqiang and Cecilia Laschi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802258},
  month     = {10},
  pages     = {11422-11427},
  title     = {Bistable valve for electronics-free soft robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pneumatic bladder links with wide range of motion joints for
articulated inflatable robots. <em>IROS</em>, 11416–11421. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Exploration of various applications is the frontier of research on inflatable robots. We proposed an articulated robots consisting of multiple pneumatic bladder links connected by rolling contact joints called Hillberry joints. The bladder link is made of a double-layered structure of tarpaulin sheet and polyurethane sheet, which is both airtight and flexible in shape. The integration of the Hilberry joint into an inflatable robot is also a new approach. The rolling contact joint allows wide range of motion of ±150°, the largest among the conventional inflatable joints. Using the proposed mechanism for inflatable robots, we demonstrated moving a 500 g payload with a 3-DoF arm and lifting 3.4 kg and 5 kg payloads with 2-DoF and 1-DoF arms, respectively. We also experimented with a single 3-DoF inflatable leg attached to a dolly to show that the proposed structure worked for legged locomotion.},
  archive   = {C_IROS},
  author    = {Katsu Uchiyama and Ryuma Niiyama},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802836},
  month     = {10},
  pages     = {11416-11421},
  title     = {Pneumatic bladder links with wide range of motion joints for articulated inflatable robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A ’MAP’ to find high-performing soft robot designs:
Traversing complex design spaces using MAP-elites and topology
optimization. <em>IROS</em>, 11408–11415. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft robotics has emerged as the standard solution for grasping deformable objects, and has proven invaluable for mobile robotic exploration in extreme environments. However, despite this growth, there are no widely adopted computational design tools that produce quality, manufacturable designs. To advance beyond the diminishing returns of heuristic bio-inspiration, the field needs efficient tools to explore the complex, non-linear design spaces present in soft robotics, and find novel high-performing designs. In this work, we investigate a hierarchical design optimization methodology which combines the strengths of topology optimization and quality diversity optimization to generate diverse and high-performance soft robots by evolving the design domain. The method embeds variably sized void regions within the design domain and evolves their size and position, to facilitating a richer exploration of the design space and find a diverse set of high-performing soft robots. We demonstrate its efficacy on both benchmark topology optimization problems and soft robotic design problems, and show the method enhances grasp performance when applied to soft grippers. Our method provides a new framework to design parts in complex design domains, both soft and rigid.},
  archive   = {C_IROS},
  author    = {Yue Xie and Josh Pinskier and Lois Liow and David Howard and Fumiya Iida},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802427},
  month     = {10},
  pages     = {11408-11415},
  title     = {A ’MAP’ to find high-performing soft robot designs: Traversing complex design spaces using MAP-elites and topology optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Origami actuator with tunable limiting layer for
reconfigurable soft robotic grasping. <em>IROS</em>, 11402–11407. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a soft actuator inspired by origami and a tunable strain limiting layer, which is proposed for reconfigurable soft robotic grasping. Main structure of the actuator is based on Miura origami which generates extension under pressurized air while a limiting layer with tunable length enables the actuator with different motion patterns. By driving the limiting layer through a servo motor, the range of motion and trajectory of the actuator can be pre-programed and the gripper’s grasping range will be affected accordingly. This paper discusses the design, fabrication, analysis and experimental verification of the actuator. Then grasping performance of the gripper under objects of different shapes, sizes, and weights is experimentally evaluated. The reconfigurable soft gripper can be applied as an end-effector to accomplish adaptive grasping tasks with various targets.},
  archive   = {C_IROS},
  author    = {Yang Yang and Kejin Zhu and Yuan Xie and Shaoyang Yan and Juan Yi and Pei Jiang and Yunquan Li and Yazhan Zhang and Yingtian Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802452},
  month     = {10},
  pages     = {11402-11407},
  title     = {Origami actuator with tunable limiting layer for reconfigurable soft robotic grasping},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and control of an ultra-slender push-pull
multisection continuum manipulator for in-situ inspection of aeroengine.
<em>IROS</em>, 11394–11401. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Since the shape of industrial endoscopes is passively altered according to the contact around it, manual inspection approaches of aeroengines through the inspection ports have unreachable areas, and it’s difficult to traverse multistage blades and inspect them simultaneously, which requires engine disassembly or the cooperation of multiple operators, resulting in efficiency decline and increased costs. To this end, this paper proposes a novel continuum manipulator with push-pull multisection structure which provides a potential solution for the disadvantages mentioned above due to its higher flexibility, passability, and controllability in confined spaces. The ultra-slender design combined with a tendon-driven mechanism makes the manipulator acquire enough workspace and more flexible postures while maintaining a light weight. Considering the coupling between the tendons in multisection, a innovative kinematics decoupling control method is implemented, which can realize real-time control in the case of limited computational resources. A prototype is built to validate the capabilities of mechatronic design and the performance of the control algorithm. The experimental results demonstrate the advantages of our continuum manipulator in the in-situ inspection of aeroengines’ multistage blades, which has the potential to be a replacement solution for industrial endoscopes.},
  archive   = {C_IROS},
  author    = {Weiheng Zhong and Yuancan Huang and Da Hong and Nianfeng Shao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802783},
  month     = {10},
  pages     = {11394-11401},
  title     = {Design and control of an ultra-slender push-pull multisection continuum manipulator for in-situ inspection of aeroengine},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning dynamic tasks on a large-scale soft robot in a
handful of trials. <em>IROS</em>, 11388–11393. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft robots offer more flexibility, compliance, and adaptability than traditional rigid robots. They are also typically lighter and cheaper to manufacture. However, their use in real-world applications is limited due to modeling challenges and difficulties in integrating effective proprioceptive sensors. Large-scale soft robots (≈ two meters in length) have greater modeling complexity due to increased inertia and related effects of gravity. Common efforts to ease these modeling difficulties such as assuming simple kinematic and dynamics models also limit the general capabilities of soft robots and are not applicable in tasks requiring fast, dynamic motion like throwing and hammering. To overcome these challenges, we propose a data-efficient Bayesian optimization-based approach for learning control policies for dynamic tasks on a large-scale soft robot. Our approach optimizes the task objective function directly from commanded pressures, without requiring approximate kinematics or dynamics as an intermediate step. We demonstrate the effectiveness of our approach through both simulated and real-world experiments.},
  archive   = {C_IROS},
  author    = {Sicelukwanda Zwane and Daniel Cheney and Curtis C. Johnson and Yicheng Luo and Yasemin Bekiroglu and Marc D. Killpack and Marc Peter Deisenroth},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802122},
  month     = {10},
  pages     = {11388-11393},
  title     = {Learning dynamic tasks on a large-scale soft robot in a handful of trials},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predicting interaction shape of soft continuum robots using
deep visual models. <em>IROS</em>, 11381–11387. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft continuum robots, characterized by their inherent compliance and dexterity, are increasingly pivotal in applications requiring delicate interactions with the environment such as the medical field. Despite their advantages, challenges persist in accurately modeling and controlling their shape during interactions with surrounding objects. This is because of the difficulty in modeling the large degrees of freedom in soft-bodied objects that become more active during interactions. In this study, we present a deep visual model to predict the interaction shapes of a soft continuum robot in contact with surrounding objects. By formulating this task as a forward-statics problem, the model uses the initial state images containing the object configuration and future actuation values to predict interactive state images of the robot under this actuation condition. We developed and tested the model in both simulated and physical environments, explored the model’s predictive capabilities using monocular and binocular views, and tested the model’s generalization ability on different datasets. Our results show that deep learning methods are a promising tool for solving the complex problem of predicting the shape of a soft continuum robot interacting with the environment, requiring no prior knowledge about the system dynamics and explicit mapping of the environment. This study paves the way for future explorations in robot-environment interaction modeling and the development of more adaptable interaction shape control strategies.},
  archive   = {C_IROS},
  author    = {Yunqi Huang and Abdulaziz Y. Alkayas and Jialei Shi and Federico Renda and Helge Wurdemann and Thomas George Thuruthel},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801261},
  month     = {10},
  pages     = {11381-11387},
  title     = {Predicting interaction shape of soft continuum robots using deep visual models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human-robot interaction control for multi-mode exosuit with
reinforcement learning. <em>IROS</em>, 11374–11380. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft exoskeleton robots have promising potential in walking assistance with comfortable wearing experience. In this study, an exosuit equipped with a twisted string actuator (TSA) is developed to provide powerful driving force and diverse operating modes for hemiplegic patients in daily life. It is challenging to establish the human-robot coupling dynamic model due to the soft structure of the exosuit and tight coupling, precise control and effective assistance are difficult to guaranteed in current exosuits. Considering the impedance characteristics of human-robot interaction, an adaptive impedance control method based on reinforcement learning (RL) is proposed, where human motion intention is utilized to optimize impedance parameters and adjust the robot&#39;s operating mode. A nonlinear disturbance observer is proposed to compensate for the effects of model estimation errors, joint friction, and external disturbances. Experimental verification demonstrates the effectiveness and superiority of the robotic system.},
  archive   = {C_IROS},
  author    = {Kaizhen Huang and Jiajun Xu and Tianyi Zhang and Mengcheng Zhao and Aihong Ji and Guoli Song and Youfu Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802309},
  month     = {10},
  pages     = {11374-11380},
  title     = {Human-robot interaction control for multi-mode exosuit with reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A soft robotic system automatically learns precise agile
motions without model information. <em>IROS</em>, 11368–11373. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many application domains, e.g., in medicine and manufacturing, can greatly benefit from pneumatic Soft Robots (SRs). However, the accurate control of SRs has remained a significant challenge to date, mainly due to their nonlinear dynamics and viscoelastic material properties. Conventional control design methods often rely on either complex system modeling or time-intensive manual tuning, both of which require significant amounts of human expertise and thus limit their practicality. In recent works, the data-driven method, Automatic Neural ODE Control (ANODEC) has been successfully used to – fully automatically and utilizing only input-output data – design controllers for various nonlinear systems in silico, and without requiring prior model knowledge or extensive manual tuning. In this work, we successfully apply ANODEC to automatically learn to perform agile, non-repetitive reference tracking motion tasks in a real-world SR and within a finite time horizon. To the best of the authors’ knowledge, ANODEC achieves, for the first time, performant control of a SR with hysteresis effects from only 30 s of input-output data and without any prior model knowledge. We show that for multiple, qualitatively different and even out-of-training-distribution reference signals, a single feedback controller designed by ANODEC outperforms a manually tuned PID baseline consistently. Overall, this contribution not only further strengthens the validity of ANODEC, but it marks an important step towards more practical, easy-to-use SRs that can automatically learn to perform agile motions from minimal experimental interaction time.},
  archive   = {C_IROS},
  author    = {Simon Bachhuber and Alexander Pawluchin and Arka Pal and Ivo Boblan and Thomas Seel},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801724},
  month     = {10},
  pages     = {11368-11373},
  title     = {A soft robotic system automatically learns precise agile motions without model information},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fractional order modeling and control of hydrogel-based soft
pneumatic bending actuators. <em>IROS</em>, 11362–11367. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft pneumatic bending actuators (SPBAs) are commonly employed in soft robotics due to their unique characteristics, including safety, low weight, speed, and load capacity. However, the combination of pneumatics with soft materials causes SPBAs to exhibit nonlinearities and infinite degrees of freedom, complicating their dynamic modeling. In this work, we present how the dynamics of SPBAs can be adjusted to a fractional order model (FOM), showing an approach for their empirical identification. We also present a method for designing fractional order controllers (FOCs) for this type of actuators, based on the inversion of the empirical FOM. This modeling and control is applied to a modular SPBA made of a smart hydrogel, which endows the actuators with self-healing, self-adhesion, and self-sensing capabilities.},
  archive   = {C_IROS},
  author    = {Jesús De La Morena and David Redrejo and Francisco Ramos and Vicente Feliu and Andrés S. Vázquez},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802659},
  month     = {10},
  pages     = {11362-11367},
  title     = {Fractional order modeling and control of hydrogel-based soft pneumatic bending actuators},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Theoretical modeling and bio-inspired trajectory
optimization of a multiple-locomotion origami robot. <em>IROS</em>,
11355–11361. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent research on mobile robots has focused on increasing their adaptability to unpredictable and unstructured environments using soft materials and structures. However, the determination of key design parameters and control over these compliant robots are predominantly iterated through experiments, lacking a solid theoretical foundation. To improve their efficiency, this paper aims to provide mathematics modeling over two locomotion, crawling and swimming. Specifically, a dynamic model is first devised to reveal the influence of the contact surfaces’ frictional coefficients on displacements in different motion phases. Besides, a swimming kinematics model is provided using coordinate transformation, based on which, we further develop an algorithm that systematically plans human-like swimming gaits, with maximum thrust obtained. The proposed algorithm is highly generalizable and has the potential to be applied in other soft robots with similar multiple joints. Simulation experiments have been conducted to illustrate the effectiveness of the proposed modeling.},
  archive   = {C_IROS},
  author    = {Keqi Zhu and Haotian Guo and Wei Yu and Hassen Nigatu and Tong Li and Ruihong Dong and Huixu Dong},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801958},
  month     = {10},
  pages     = {11355-11361},
  title     = {Theoretical modeling and bio-inspired trajectory optimization of a multiple-locomotion origami robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design, modelling, and experimental validation of a soft
continuum wrist section developed for a prosthetic hand. <em>IROS</em>,
11347–11354. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft continuum sections are widely used in robotic mechanisms for achieving dexterous motions. However, most available designs of soft continuum sections cannot support payload during a motion. This paper presents a development of a novel soft robotic wrist section for a prosthetic hand named ‘PRISMA Hand II’. Our research focuses on various phases of development of a soft continuum wrist section, that can support a substantial payload and maintain postures of the hand. Mechanical design, fabrication, and modelling strategies adopted for developing the wrist section are described. The design of the wrist section is constructed by assembling springs, discs, and tendons. The numbers and dimensions of springs and discs are optimised using static structural analysis. Kinematic modelling and dynamic modelling of the wrist section are carried out using Geometric Variable Strain (GVS) approach based on Cosserat rod theory and a generalised coordinate method respectively. The geometric formulations involved in Cosserat rod theory guaranteed accurate and quick computations considering deformation parameters. Dynamic modelling approach also enhanced the performance of the wrist section reducing errors and computational time during real time implementations. This paper also discusses about a dynamic model based controller strategy for the wrist section and advantages of the proposed controller are proved using a comparative study with a kinematic model based PID controller. Experimental validations of motions of the fabricated wrist section employing the dynamic controller are also included in the paper.},
  archive   = {C_IROS},
  author    = {Shifa Sulaiman and Mehul Menon and Francesco Schetter and Fanny Ficuciello},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802467},
  month     = {10},
  pages     = {11347-11354},
  title     = {Design, modelling, and experimental validation of a soft continuum wrist section developed for a prosthetic hand},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). CFD-enabled approach for optimizing CPG control network for
underwater soft robotic fish. <em>IROS</em>, 11340–11346. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Central Pattern Generators (CPG) nonlinear oscillation network is being increasingly used in the control of multi-joint collaborative robots. The motion attitude of robots can be effectively adjusted by tuning parameters of the CPG neural network. However, the mapping from CPG parameters to motion attitude is relatively complicated. To improve the motion performance, an optimization method combining computational fluid dynamics (CFD) and CPG network is proposed. In this work, we design a three-joint biomimetic soft robot fish following the body structure of trevally and an improved CPG network based on the Hopf model is incorporated into the control system. Directly optimizing the swimming performance through experiments is time consuming and complex, a mode of first adjusting parameters on the simulation platform and then refining on the robot is usually adopted. Therefore, a CFD simulation platform using hydrodynamic solutions has been established to assist in analyzing the swimming effect. Finally, the experimental results show that the swimming simulation by the CFD is highly similar to the real test, and the swimming performance after the improved CPG network optimization has been significantly increased.},
  archive   = {C_IROS},
  author    = {Yunfei Wang and Weiyuan Sun and Wei Tang and Xianrui Zhang and Zhenping Yu and Shunxiang Cao and Juntian Qu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802666},
  month     = {10},
  pages     = {11340-11346},
  title     = {CFD-enabled approach for optimizing CPG control network for underwater soft robotic fish},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust-adaptive two-loop control for robots with mixed
rigid-elastic joints. <em>IROS</em>, 11332–11339. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In robotics, while rigid joints are common due to their accuracy and fast response ability, elastic joints are wellknown for their safety when interacting with the environment. To harmonise the advantages of these joint types, robots with mixed rigid-elastic joints can be considered. In this paper, a robust-adaptive two-loop control algorithm is proposed to control this type of hybrid robots when there are uncertainties in system parameters. In the outer loop, a robust control algorithm is proposed to deal with the uncertainties in the parameters of the joint dynamics, together with an adaptive controller for the rigid joints. In the inner loop, another robust control algorithm is proposed to handle the uncertainties in system parameters of the elastic joint’s motor contribution, and a similar adaptive control algorithm is presented to manipulate the elastic joints’ motors. The stability of the system is assured by Lyapunov’s stability theory. Finally, simulations are conducted to verify the proposed control algorithm.},
  archive   = {C_IROS},
  author    = {Minh Tuan Hua and Emil Mühlbradt Sveen and Siri Marte Schlanbusch and Filippo Sanfilippo},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802178},
  month     = {10},
  pages     = {11332-11339},
  title     = {Robust-adaptive two-loop control for robots with mixed rigid-elastic joints},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From CAD to URDF: Co-design of a jet-powered humanoid robot
including CAD geometry. <em>IROS</em>, 11325–11331. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Co-design optimization strategies usually rely on simplified robot models extracted from CAD. While these models are useful for optimizing geometrical and inertial parameters for robot control, they might overlook important details essential for prototyping the optimized mechanical design. For instance, they may not account for mechanical stresses exerted on the optimized geometries and the complexity of assembly-level design. In this paper, we introduce a co-design framework aimed at improving both the control performance and mechanical design of our robot. Specifically, we identify the robot links that significantly influence control performance. The geometric characteristics of these links are parameterized and optimized using a multi-objective evolutionary algorithm to achieve optimal control performance. Additionally, an automated Finite Element Method (FEM) analysis is integrated into the framework to filter solutions not satisfying the required structural safety margin. We validate the framework by applying it to enhance the mechanical design for flight performance of the jet-powered humanoid robot iRonCub.},
  archive   = {C_IROS},
  author    = {Punith Reddy Vanteddu and Gabriele Nava and Fabio Bergonti and Giuseppe L’Erario and Antonello Paolino and Daniele Pucci},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802023},
  month     = {10},
  pages     = {11325-11331},
  title     = {From CAD to URDF: Co-design of a jet-powered humanoid robot including CAD geometry},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Driving style alignment for LLM-powered driver agent.
<em>IROS</em>, 11318–11324. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, LLM-powered driver agents have demonstrated considerable potential in the field of autonomous driving, showcasing human-like reasoning and decision-making abilities. However, current research on aligning driver agent behaviors with human driving styles remains limited, partly due to the scarcity of high-quality natural language data from human driving behaviors. To address this research gap, we propose a multi-alignment framework designed to align driver agents with human driving styles through demonstrations and feedback. Notably, we construct a natural language dataset of human driver behaviors through naturalistic driving experiments and post-driving interviews, offering high-quality human demonstrations for LLM alignment. The framework’s effectiveness is validated through simulation experiments in the CARLA urban traffic simulator and further corroborated by human evaluations. Our research offers valuable insights into designing driving agents with diverse driving styles. The implementation of the framework 1 and details of the dataset 2 can be found at the link.},
  archive   = {C_IROS},
  author    = {Ruoxuan Yang and Xinyue Zhang and Anais Fernandez-Laaksonen and Xin Ding and Jiangtao Gong},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802629},
  month     = {10},
  pages     = {11318-11324},
  title     = {Driving style alignment for LLM-powered driver agent},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards designing a low-cost humanoid robot with flex
sensors-based movement*. <em>IROS</em>, 11310–11317. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humanoid robots have potential applications across diverse sectors, including education, healthcare, and customer service. This paper presents a project on designing and building a low-cost humanoid robot equipped with a flex sensor-based movement mechanism, highlighting its compatibility with Raspberry Pi and microcontrollers such as Arduino Uno and Nano. The project aims to investigate the robot&#39;s relevance and effectiveness within educational settings to showcase how a low-cost humanoid robot can potentially support the United Nations&#39; fourth Sustainable Development Goal (UN SDG4) by improving access to quality education through innovative robotics solutions. The robot was tested in a cycle two school (covering Grades 5 to 8 (ages 10 to 13)) in Dubai, United Arab Emirates. It was integrated into math, science, and design technology classes to assess its functionality and efficiency. Surveys conducted among students and teachers showed a high level of acceptance towards the robot, with over 85% of respondents expressing positive attitudes about its presence and interaction in the classroom. However, teachers and students provided feedback concerning the robot&#39;s shape, capabilities, and movement mechanism. Teachers also appreciated the robot&#39;s alignment with the UN SDG4, stating its capability to support students learning and engagement. The authors highlighted the robot&#39;s potential to assist students with sensory challenges, such as hearing and vision impairments, and learning difficulties like dyslexia while emphasizing their commitment to enhancing its accessibility features for a more inclusive learning environment.},
  archive   = {C_IROS},
  author    = {Muhammad H. Al Omoush and Sameer Kishore and Tracey Mehigan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801480},
  month     = {10},
  pages     = {11310-11317},
  title     = {Towards designing a low-cost humanoid robot with flex sensors-based movement*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fly by book: How to train a humanoid robot to fly an
airplane using large language models. <em>IROS</em>, 11302–11309. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A pilot needs to manipulate various gadgets in the cockpit based on vast knowledge of rules and procedures while verbally communicating with air traffic controllers. While precision manipulation in the cockpit during the flight is already a difficult task, a far more difficult thing is how to make a robot learn all the knowledge needed to fly an airplane in accordance with all the rules and regulations. As a pioneering effort, this paper introduces LLM-PIBOT, which leverages the latest advances in Large Language Models (LLMs) to empower a humanoid pilot robot (PIBOT) to take the full authority of an airplane by understanding and executing complex procedures outlined in Pilot’s Operating Handbooks (POHs). Unlike traditional rule-based methods, LLM-PIBOT system infers suitable flight procedures, employs an embedding process to accurately identify relevant procedures within documents, and structures the text-extracted flight tasks into tuples using our carefully crafted prompts. This approach enables PIBOT to adapt to the given POHs, generating and executing task plans in real-time in response to commands and situations. Experimental results show that LLM-PIBOT can comprehend and follow the complex procedures specified in the manuals and to fly the airplane on a full-scale simulator using the generated flight plans.},
  archive   = {C_IROS},
  author    = {Hyungjoo Kim and Sungjae Min and Gyuree Kang and Jihyeok Kim and David Hyunchul Shim},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802288},
  month     = {10},
  pages     = {11302-11309},
  title     = {Fly by book: How to train a humanoid robot to fly an airplane using large language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Magnetic tactile sensor with load tolerance and flexibility
using frame structures for estimating triaxial contact force
distribution of humanoid. <em>IROS</em>, 11294–11301. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For humanoid whole body contact motions, it is important to recognize the existence of whole body contacts and the contact forces. The challenges in recognizing the existence of whole body contacts and the contact forces in life-size humanoids are: 1) the measurement part with low mechanical strength must be tolerant of high load and 2) it is difficult to model thick elastic bodies with high impact tolerance and uneven sensor placements when applied to various shapes of the whole body. This paper proposes a method of constructing a load tolerant tactile sensor by separating the loaded part from the measuring part with magnetism and protecting the measuring part inside the frame of the robot. For modeling difficulties, this paper proposes learning the relationship between the change in the detected physical quantity due to deformation of the elastic body and the contact force distribution. This paper shows through experiments that the proposed tactile sensor based on a robot frame is load tolerant enough to support the weight of a life-sized humanoid, and that it can acquire contact force distribution and the robot is able to acclimate to external forces.},
  archive   = {C_IROS},
  author    = {Takuma Hiraoka and Ren Kunita and Kunio Kojima and Naoki Hiraoka and Masanori Konishi and Tasuku Makabe and Annan Tang and Kei Okada and Masayuki Inaba},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801784},
  month     = {10},
  pages     = {11294-11301},
  title     = {Magnetic tactile sensor with load tolerance and flexibility using frame structures for estimating triaxial contact force distribution of humanoid},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Feasible region construction by polygon merging for
continuous bipedal walking*. <em>IROS</em>, 11286–11293. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Feasible regions for continuous walking must provide necessary information for footstep planning, including surrounding landing areas and details about obstacles to be avoided during foot swing. However, the current frame lacks sufficient information to construct a feasible region needed at the current moment due to knee occlusion. To this end, this paper uses polygon merging to construct an information-complete feasible region. This polygon merging refers to merging polygons from the current frame and a specific previous frame. Since the polygon is more concise and efficient than point cloud for environmental representation, construction can be completed quickly without GPU acceleration. Experiments show that the proposed method successfully constructs informative feasible regions within the allowed time frame, enabling the robot to navigate stairs.},
  archive   = {C_IROS},
  author    = {Chao Li and Xuechao Chen and Hengbo Qi and Qingqing Li and Qingrui Zhao and Yongliang Shi and Zhangguo Yu and Lingxuan Zhao and Zhihong Jiang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802326},
  month     = {10},
  pages     = {11286-11293},
  title     = {Feasible region construction by polygon merging for continuous bipedal walking*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Physically consistent online inertial adaptation for
humanoid loco-manipulation. <em>IROS</em>, 11278–11285. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to accomplish manipulation and locomotion tasks in the presence of significant time-varying external loads is a remarkable skill of humans that has yet to be replicated convincingly by humanoid robots. Such an ability will be a key requirement in the environments we envision deploying our robots: dull, dirty, and dangerous. External loads constitute a large model bias, which is typically unaccounted for. In this work, we enable our humanoid robot to engage in loco-manipulation tasks in the presence of significant model bias due to external loads. We propose an online estimation and control framework involving the combination of a physically consistent extended Kalman filter for inertial parameter estimation coupled to a whole-body controller. We showcase our results both in simulation and in hardware, where weights are mounted on Nadia’s wrist links as a proxy for engaging in tasks where large external loads are applied to the robot.},
  archive   = {C_IROS},
  author    = {James Foster and Stephen McCrory and Christian DeBuys and Sylvain Bertrand and Robert Griffin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802012},
  month     = {10},
  pages     = {11278-11285},
  title     = {Physically consistent online inertial adaptation for humanoid loco-manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Whleaper: A 10-DOF flexible bipedal wheeled robot.
<em>IROS</em>, 11272–11277. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Wheel-legged robots combine the advantages of both wheeled robots and legged robots, offering versatile locomotion capabilities with excellent stability on challenging terrains and high efficiency on flat surfaces. However, existing wheel-legged robots typically have limited hip joint mobility compared to humans, while hip joint plays a crucial role in locomotion. In this paper, we introduce Whleaper, a novel 10-degree-of-freedom (DOF) bipedal wheeled robot, with 3 DOFs at the hip of each leg. Its humanoid joint design enables adaptable motion in complex scenarios, ensuring stability and flexibility. This paper introduces the details of Whleaper, with a focus on innovative mechanical design, control algorithms and system implementation. Firstly, stability stems from the increased DOFs at the hip, which expand the range of possible postures and improve the robot’s foot-ground contact. Secondly, the extra DOFs also augment its mobility. During walking or sliding, more complex movements can be adopted to execute obstacle avoidance tasks. Thirdly, we utilize two control algorithms to implement multimodal motion for walking and sliding. By controlling specific DOFs of the robot, we conducted a series of simulations and practical experiments, demonstrating that a high-DOF hip joint design can effectively enhance the stability and flexibility of wheel-legged robots. Whleaper shows its capability to perform actions such as squatting, obstacle avoidance sliding, and rapid turning in real-world scenarios.},
  archive   = {C_IROS},
  author    = {Yinglei Zhu and Sixiao He and Zhenghao Qi and Zhuoyuan Yong and Yihua Qin and Jianyu Chen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801355},
  month     = {10},
  pages     = {11272-11277},
  title     = {Whleaper: A 10-DOF flexible bipedal wheeled robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bipedal safe navigation over uncertain rough terrain:
Unifying terrain mapping and locomotion stability. <em>IROS</em>,
11264–11271. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of bipedal robot navigation in complex environments with uncertain and rough terrain. In particular, we consider a scenario in which the robot is expected to reach a desired goal location by traversing an environment with uncertain terrain elevation. Such terrain uncertainties induce not only untraversable regions but also robot motion perturbations. Thus, the problems of terrain mapping and locomotion stability are intertwined. We evaluate three different kernels for Gaussian process (GP) regression to learn the terrain elevation. We also learn the motion deviation resulting from both the terrain as well as the discrepancy between the reduced-order Prismatic Inverted Pendulum Model used for planning and the full-order locomotion dynamics. We propose a hierarchical locomotion-dynamics-aware sampling-based navigation planner. The global navigation planner plans a series of local waypoints to reach the desired goal locations while respecting locomotion stability constraints. Then, a local navigation planner is used to generate a sequence of dynamically feasible footsteps to reach local waypoints. We develop a novel trajectory evaluation metric to minimize motion deviation and maximize information gain of the terrain elevation map. We evaluate the efficacy of our planning framework on Digit bipedal robot simulation in MuJoCo.i},
  archive   = {C_IROS},
  author    = {Kasidit Muenprasitivej and Jesse Jiang and Abdulaziz Shamsah and Samuel Coogan and Ye Zhao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802816},
  month     = {10},
  pages     = {11264-11271},
  title     = {Bipedal safe navigation over uncertain rough terrain: Unifying terrain mapping and locomotion stability},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Revisiting reward design and evaluation for robust humanoid
standing and walking. <em>IROS</em>, 11256–11263. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A necessary capability for humanoid robots is the ability to stand and walk while rejecting natural disturbances. Recent progress has been made using sim-to-real reinforcement learning (RL) to train such locomotion controllers, with approaches differing mainly in their reward functions. However, prior works lack a clear method to systematically test new reward functions and compare controller performance through repeatable experiments. This limits our understanding of the trade-offs between approaches and hinders progress. To address this, we propose a low-cost, quantitative benchmarking method to evaluate and compare the real-world performance of standing and walking (SaW) controllers on metrics like command following, disturbance recovery, and energy efficiency. We also revisit reward function design and construct a minimally constraining reward function to train SaW controllers. We experimentally verify that our benchmarking framework can identify areas for improvement, which can be systematically addressed to enhance the policies. We also compare our new controller to state-of-the-art controllers on the Digit humanoid robot. The results provide clear quantitative trade-offs among the controllers and suggest directions for future improvements to the reward functions and expansion of the benchmarks.},
  archive   = {C_IROS},
  author    = {Bart van Marum and Aayam Shrestha and Helei Duan and Pranay Dugar and Jeremy Dao and Alan Fern},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802680},
  month     = {10},
  pages     = {11256-11263},
  title     = {Revisiting reward design and evaluation for robust humanoid standing and walking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrating model-based footstep planning with model-free
reinforcement learning for dynamic legged locomotion. <em>IROS</em>,
11248–11255. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we introduce a control framework that combines model-based footstep planning with Reinforcement Learning (RL), leveraging desired footstep patterns derived from the Linear Inverted Pendulum (LIP) dynamics. Utilizing the LIP model, our method forward predicts robot states and determines the desired foot placement given the velocity commands. We then train an RL policy to track the foot placements without following the full reference motions derived from the LIP model. This partial guidance from the physics model allows the RL policy to integrate the predictive capabilities of the physics-informed dynamics and the adaptability characteristics of the RL controller without overfitting the policy to the template model. Our approach is validated on the MIT Humanoid, demonstrating that our policy can achieve stable yet dynamic locomotion for walking and turning. We further validate the adaptability and generalizability of our policy by extending the locomotion task to unseen, uneven terrain. During the hardware deployment, we have achieved forward walking speeds of up to 1.5 m/s on a treadmill and have successfully performed dynamic locomotion maneuvers such as 90-degree and 180-degree turns.},
  archive   = {C_IROS},
  author    = {Ho Jae Lee and Seungwoo Hong and Sangbae Kim},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801468},
  month     = {10},
  pages     = {11248-11255},
  title     = {Integrating model-based footstep planning with model-free reinforcement learning for dynamic legged locomotion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint-level IS-MPC: A whole-body MPC with centroidal
feasibility for humanoid locomotion. <em>IROS</em>, 11240–11247. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose an effective whole-body MPC controller for locomotion of humanoid robots. Our method generates motions using the full kinematics, allowing it to account for joint limits and to exploit upper-body motions to reject disturbances. Each MPC iteration solves a single QP that considers the interplay between dynamic and kinematic features of the robot. Thanks to our special formulation, we are able to perform a feasibility analysis, which opens the door to future enhancements of functionality and performance, e.g., step adaptation in complex environments. We demonstrate its effectiveness through a campaign of dynamic simulations aimed at highlighting how the joint limits and the use of the angular momentum through upper-body motions are fundamental for maximizing performance, robustness, and ultimately make the robot able to execute more challenging gaits.},
  archive   = {C_IROS},
  author    = {Tommaso Belvedere and Nicola Scianca and Leonardo Lanari and Giuseppe Oriolo},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801628},
  month     = {10},
  pages     = {11240-11247},
  title     = {Joint-level IS-MPC: A whole-body MPC with centroidal feasibility for humanoid locomotion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward understanding key estimation in learning robust
humanoid locomotion. <em>IROS</em>, 11232–11239. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate state estimation plays a critical role in ensuring the robust control of humanoid robots, particularly in the context of learning-based control policies for legged robots. However, there is a notable gap in analytical research concerning estimations. Therefore, we endeavor to further understand how various types of estimations influence the decision-making processes of policies. In this paper, we provide quantitative insight into the effectiveness of learned state estimations, employing saliency analysis to identify key estimation variables and optimize their combination for humanoid locomotion tasks. Evaluations assessing tracking precision and robustness are conducted on comparative groups of policies with varying estimation combinations in both simulated and real-world environments. Results validated that the proposed policy is capable of crossing the sim-to-real gap and demonstrating superior performance relative to alternative policy configurations.},
  archive   = {C_IROS},
  author    = {Zhicheng Wang and Wandi Wei and Ruiqi Yu and Jun Wu and Qiuguo Zhu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801648},
  month     = {10},
  pages     = {11232-11239},
  title     = {Toward understanding key estimation in learning robust humanoid locomotion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Whole-body humanoid robot locomotion with human reference.
<em>IROS</em>, 11225–11231. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, humanoid robots have made significant advances in their ability to perform challenging tasks due to the deployment of Reinforcement Learning (RL), however, the inherent complexity of humanoid robots, including the difficulty of designing complicated reward functions and training entire sophisticated systems, still poses a notable challenge. To conquer these challenges, after many iterations and in-depth investigations, we have meticulously developed a full-size humanoid robot, &quot;Adam&quot;, whose innovative structural design greatly improves the efficiency and effectiveness of the imitation learning process. In addition, we have developed a novel imitation learning framework based on an adversarial motion prior, which applies not only to Adam but also to humanoid robots in general. Using the framework, Adam can exhibit unprecedented human-like characteristics in locomotion tasks. Our experimental results demonstrate that the proposed framework enables Adam to achieve human-comparable performance in complex locomotion tasks, marking the first time that human locomotion data has been used for imitation learning in a full-size humanoid robot. For more video demonstrations, please visit our YouTube channel: https://www.youtube.com/watch?v=7hK2ySYBa1I},
  archive   = {C_IROS},
  author    = {Qiang Zhang and Peter Cui and David Yan and Jingkai Sun and Yiqun Duan and Gang Han and Wen Zhao and Weining Zhang and Yijie Guo and Arthur Zhang and Renjing Xu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801451},
  month     = {10},
  pages     = {11225-11231},
  title     = {Whole-body humanoid robot locomotion with human reference},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Compliance optimization control for rigid-soft hybrid system
and its application in humanoid robot motion control. <em>IROS</em>,
11218–11224. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Flexibility and softness play a significant role in dynamic human motions. This includes the flexibility owing to ligaments in the human body and the softness of external structures such as a leaf-spring-type prosthesis. Thus, robotic systems need to utilize such flexibility to achieve dynamic and energy-efficient motion. In this study, we proposed a compliance optimization-based control framework for a rigid-soft hybrid robot system where the continuous deformation of a flexible structure is represented using the piece-wise constant strain (PCS) model. We divided the hybrid system into two states: single support and double support. We validated the proposed method in these states using forward dynamics simulations, assuming a hybrid link system that consists of a humanoid robot with a flexible prosthesis.},
  archive   = {C_IROS},
  author    = {Zewen He and Taiki Ishigaki and Ko Yamamoto},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801838},
  month     = {10},
  pages     = {11218-11224},
  title     = {Compliance optimization control for rigid-soft hybrid system and its application in humanoid robot motion control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Demonstrating a robust walking algorithm for underactuated
bipedal robots in non-flat, non-stationary environments. <em>IROS</em>,
11210–11217. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work explores an innovative algorithm designed to enhance the mobility of underactuated bipedal robots across challenging terrains, especially when navigating through spaces with constrained opportunities for foot support, like steps or stairs. By combining ankle torque with a refined angular momentum-based linear inverted pendulum model (ALIP), our method allows variability in the robot’s center of mass height. We employ a dual-strategy controller that merges virtual constraints for precise motion regulation across essential degrees of freedom with an ALIP-centric model predictive control (MPC) framework, aimed at enforcing gait stability. The effectiveness of our feedback design is demonstrated through its application on the Cassie bipedal robot, which features 20 degrees of freedom. Key to our implementation is the development of tailored nominal trajectories and an optimized MPC that reduces the execution time to under 500 microseconds—and, hence, is compatible with Cassie’s controller update frequency. This paper not only showcases the successful hardware deployment but also demonstrates a new capability, a bipedal robot using a moving walkway.},
  archive   = {C_IROS},
  author    = {Oluwami Dosunmu-Ogunbi and Aayushi Shrivastava and Jessy W Grizzle},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802406},
  month     = {10},
  pages     = {11210-11217},
  title     = {Demonstrating a robust walking algorithm for underactuated bipedal robots in non-flat, non-stationary environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GS-planner: A gaussian-splatting-based planning framework
for active high-fidelity reconstruction. <em>IROS</em>, 11202–11209. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Active reconstruction technique enables robots to autonomously collect scene data for full coverage, relieving users from tedious and time-consuming data capturing process. However, designed based on unsuitable scene representations, existing methods show unrealistic reconstruction results or the inability of online quality evaluation. Due to the recent advancements in explicit radiance field technology, online active high-fidelity reconstruction has become achievable. In this paper, we propose GS-Planner, a planning framework for active high-fidelity reconstruction using 3D Gaussian Splatting. With improvement on 3DGS to recognize unobserved regions, we evaluate the reconstruction quality and completeness of 3DGS map online to guide the robot. Then we design a sampling-based active reconstruction strategy to explore the unobserved areas and improve the reconstruction geometric and textural quality. To establish a complete robot active reconstruction system, we choose quadrotor as the robotic platform for its high agility. Then we devise a safety constraint with 3DGS to generate executable trajectories for quadrotor navigation in the 3DGS map. To validate the effectiveness of our method, we conduct extensive experiments and ablation studies in highly realistic simulation scenes.},
  archive   = {C_IROS},
  author    = {Rui Jin and Yuman Gao and Yingjian Wang and Yuze Wu and Haojian Lu and Chao Xu and Fei Gao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801715},
  month     = {10},
  pages     = {11202-11209},
  title     = {GS-planner: A gaussian-splatting-based planning framework for active high-fidelity reconstruction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Making the flow glow – robot perception under severe
lighting conditions using normalizing flow gradients. <em>IROS</em>,
11195–11201. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern robotic perception is highly dependent on neural networks. It is well known that neural network-based perception can be unreliable in real-world deployment, especially in difficult imaging conditions. Out-of-distribution detection is commonly proposed as a solution for ensuring reliability in real-world deployment. Previous work has shown that normalizing flow models can be used for out-of-distribution detection to improve reliability of robotic perception tasks. Specifically, camera parameters can be optimized with respect to the likelihood output from a normalizing flow, which allows a perception system to adapt to difficult vision scenarios. With this work we propose to use the absolute gradient values from a normalizing flow, which allows the perception system to optimize local regions rather than the whole image. By setting up a table top picking experiment with exceptionally difficult lighting conditions, we show that our method achieves a 60% higher success rate for an object detection task compared to previous methods.},
  archive   = {C_IROS},
  author    = {Simon Kristoffersson Lind and Rudolph Triebel and Volker Krüger},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801601},
  month     = {10},
  pages     = {11195-11201},
  title     = {Making the flow glow – robot perception under severe lighting conditions using normalizing flow gradients},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PCDepth: Pattern-based complementary learning for monocular
depth estimation by best of both worlds. <em>IROS</em>, 11187–11194. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Event cameras can record scene dynamics with high temporal resolution, providing rich scene details for monocular depth estimation (MDE) even at low-level illumination. Therefore, existing complementary learning approaches for MDE fuse intensity information from images and scene details from event data for better scene understanding. However, most methods directly fuse two modalities at pixel level, ignoring that the attractive complementarity mainly impacts high-level patterns that only occupy a few pixels. For example, event data is likely to complement contours of scene objects. In this paper, we discretize the scene into a set of high-level patterns to explore the complementarity and propose a Pattern-based Complementary learning architecture for monocular Depth estimation (PCDepth). Concretely, PCDepth comprises two primary components: a complementary visual representation learning module for discretizing the scene into high-level patterns and integrating complementary patterns across modalities and a refined depth estimator aimed at scene reconstruction and depth prediction while maintaining an efficiency-accuracy balance. Through pattern-based complementary learning, PCDepth fully exploits two modalities and achieves more accurate predictions than existing methods, especially in challenging nighttime scenarios. Extensive experiments on MVSEC and DSEC datasets verify the effectiveness and superiority of our PCDepth. Remarkably, compared with state-of-the-art, PCDepth achieves a 37.9% improvement in accuracy in MVSEC nighttime scenarios.},
  archive   = {C_IROS},
  author    = {Haotian Liu and Sanqing Qu and Fan Lu and Zongtao Bu and Florian Röhrbein and Alois Knoll and Guang Chen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802220},
  month     = {10},
  pages     = {11187-11194},
  title     = {PCDepth: Pattern-based complementary learning for monocular depth estimation by best of both worlds},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SDGE: Stereo guided depth estimation for 360°camera sets.
<em>IROS</em>, 11179–11186. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Depth estimation is a critical technology in autonomous driving, and multi-camera systems are often used to achieve a 360° perception. These 360° camera sets often have limited or low-quality overlap regions, making multi-view stereo methods infeasible for the entire image. Alternatively, monocular methods may not produce consistent cross-view predictions. To address these issues, we propose the Stereo Guided Depth Estimation (SGDE) method, which enhances depth estimation of the full image by explicitly utilizing multi-view stereo results on the overlap. We suggest building virtual pinhole cameras to resolve the distortion problem of fisheye cameras and unify the processing for the two types of 360° cameras. For handling the varying noise on camera poses caused by unstable movement, the approach employs a self-calibration method to obtain highly accurate relative poses of the adjacent cameras with minor overlap. These enable the use of robust stereo methods to obtain a high-quality depth prior in the overlap region. This prior serves not only as an additional input but also as pseudo-labels that enhance the accuracy of depth estimation methods and improve cross-view prediction consistency. The effectiveness of SGDE is evaluated on one fisheye camera dataset, Synthetic Urban, and two pinhole camera datasets, DDAD and nuScenes. Our experiments demonstrate that SGDE is effective for both supervised and self-supervised depth estimation, and highlight the potential of our method for advancing autonomous driving technology. Our project page is at https://github.com/JialeiXu/SGDE.},
  archive   = {C_IROS},
  author    = {Jialei Xu and Wei Yin and Dong Gong and Junjun Jiang and Xianming Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802599},
  month     = {10},
  pages     = {11179-11186},
  title     = {SDGE: Stereo guided depth estimation for 360°Camera sets},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Iterative reference learning for cartesian impedance control
of robot manipulators. <em>IROS</em>, 11171–11178. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, an iterative learning strategy was developed to improve trajectory tracking for an impedance-controlled robot manipulator. In this learning strategy, an update law was proposed to modify the Cartesian reference of an impedance controller. Also, the conditions that ensure its convergence considering the dynamics of the robot were derived. Finally, an experimental evaluation was performed using a Franka Emika Panda robot in two different robot tasks, and its results showed that robot task completion was achieved in a lower number of iterations, while maintaining a smooth physical interaction between the robot and its surroundings.},
  archive   = {C_IROS},
  author    = {Julian M. Salt Ducaju and Björn Olofsson and Rolf Johansson},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801796},
  month     = {10},
  pages     = {11171-11178},
  title     = {Iterative reference learning for cartesian impedance control of robot manipulators},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning deep dynamical systems using stable neural ODEs.
<em>IROS</em>, 11163–11170. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning complex trajectories from demonstrations in robotic tasks has been effectively addressed through the utilization of Dynamical Systems (DS). State-of-the-art DS learning methods ensure stability of the generated trajectories; however, they have three shortcomings: a) the DS is assumed to have a single attractor, which limits the diversity of tasks it can achieve, b) state derivative information is assumed to be available in the learning process and c) the state of the DS is assumed to be measurable at inference time. We propose a class of provably stable latent DS with possibly multiple attractors, that inherit the training methods of Neural Ordinary Differential Equations, thus, dropping the dependency on state derivative information. A diffeomorphic mapping for the output and a loss that captures time-invariant trajectory similarity are proposed. We validate the efficacy of our approach through experiments conducted on a public dataset of handwritten shapes and within a simulated object manipulation task.},
  archive   = {C_IROS},
  author    = {Andreas Sochopoulos and Michael Gienger and Sethu Vijayakumar},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801826},
  month     = {10},
  pages     = {11163-11170},
  title     = {Learning deep dynamical systems using stable neural ODEs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Working backwards: Learning to place by picking.
<em>IROS</em>, 11155–11162. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present placing via picking (PvP), a method to autonomously collect real-world demonstrations for a family of placing tasks in which objects must be manipulated to specific, contact-constrained locations. With PvP, we approach the collection of robotic object placement demonstrations by reversing the grasping process and exploiting the inherent symmetry of the pick and place problems. Specifically, we obtain placing demonstrations from a set of grasp sequences of objects initially located at their target placement locations. Our system can collect hundreds of demonstrations in contact-constrained environments without human intervention using two modules: compliant control for grasping and tactile regrasping. We train a policy directly from visual observations through behavioural cloning, using the autonomously-collected demonstrations. By doing so, the policy can generalize to object placement scenarios outside of the training environment without privileged information (e.g., placing a plate picked up from a table). We validate our approach in home robot scenarios that include dishwasher loading and table setting. Our approach yields robotic placing policies that outperform policies trained with kinesthetic teaching, both in terms of success rate and data efficiency, while requiring no human supervision.},
  archive   = {C_IROS},
  author    = {Oliver Limoyo and Abhisek Konar and Trevor Ablett and Jonathan Kelly and Francois R. Hogan and Gregory Dudek},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802173},
  month     = {10},
  pages     = {11155-11162},
  title     = {Working backwards: Learning to place by picking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Signal temporal logic-guided apprenticeship learning.
<em>IROS</em>, 11147–11154. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Apprenticeship learning crucially depends on effectively learning rewards, and hence control policies from user demonstrations. Of particular difficulty is the setting where the desired task consists of a number of sub-goals with temporal dependencies. The quality of inferred rewards and hence policies are typically limited by the quality of demonstrations, and poor inference of these can lead to undesirable outcomes. In this paper, we show how temporal logic specifications that describe high level task objectives, are encoded in a graph to define a temporal-based metric that reasons about behaviors of demonstrators and the learner agent to improve the quality of inferred rewards and policies. Through experiments on a diverse set of robot manipulator simulations, we show how our framework overcomes the drawbacks of prior literature by drastically improving the number of demonstrations required to learn a control policy.},
  archive   = {C_IROS},
  author    = {Aniruddh G. Puranic and Jyotirmoy V. Deshmukh and Stefanos Nikolaidis},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801924},
  month     = {10},
  pages     = {11147-11154},
  title     = {Signal temporal logic-guided apprenticeship learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PACC: A passive-arm approach for high-payload collaborative
carrying with quadruped robots using model predictive control.
<em>IROS</em>, 11139–11146. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce the concept of using passive arm structures with intrinsic impedance for robot-robot and human-robot collaborative carrying with quadruped robots. The concept is meant for a leader-follower task and takes a minimalist approach that focuses on exploiting the robots’ payload capabilities and reducing energy consumption, without compromising the robot locomotion capabilities. We introduce a preliminary arm mechanical design and describe how to use its joint displacements to guide the robot’s motion. To control the robot’s locomotion, we propose a decentralized Model Predictive Controller that incorporates an approximation of the arm dynamics and the estimation of the external forces from the collaborative carrying. We validate the overall system experimentally by performing both robot-robot and human-robot collaborative carrying on a stair-like obstacle and on rough terrain.},
  archive   = {C_IROS},
  author    = {Giulio Turrisi and Lucas Schulze and Vivian S. Medeiros and Claudio Semini and Victor Barasuol},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801456},
  month     = {10},
  pages     = {11139-11146},
  title     = {PACC: A passive-arm approach for high-payload collaborative carrying with quadruped robots using model predictive control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring constrained reinforcement learning algorithms for
quadrupedal locomotion. <em>IROS</em>, 11132–11138. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Shifting from traditional control strategies to Deep Reinforcement Learning (RL) for legged robots poses inherent challenges, especially when addressing real-world physical constraints during training. While high-fidelity simulations provide significant benefits, they often bypass these essential physical limitations. In this paper, we experiment with the Constrained Markov Decision Process (CMDP) framework instead of the conventional unconstrained RL for robotic applications. We evaluated five constrained policy optimization algorithms for quadrupedal locomotion using three different robot models. Our aim is to evaluate their applicability in real-world scenarios. Our robot experiments demonstrate the critical role of incorporating physical constraints, yielding successful sim-to-real transfers, and reducing operational errors on physical systems. The CMDP formulation streamlines the training process by separately handling constraints from rewards. Our findings underscore the potential of constrained RL for the effective development and deployment of learned controllers in robotics.},
  archive   = {C_IROS},
  author    = {Joonho Lee and Lukas Schroth and Victor Klemm and Marko Bjelonic and Alexander Reske and Marco Hutter},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801341},
  month     = {10},
  pages     = {11132-11138},
  title     = {Exploring constrained reinforcement learning algorithms for quadrupedal locomotion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Whole-body compliance control for quadruped manipulator
with actuation saturation of joint torque and ground friction.
<em>IROS</em>, 11124–11131. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In normal operations, when quadruped manipulators with impedance control experience external disturbances, they may become unstable and lose balance due to actuation saturation, affecting their stability, safety, and compliance with the environment. To address this issue, we propose a whole-body compliance controller to prevent unstable behaviors like slip, oscillation, and overshoot, which arise from actuation saturation. The controller includes an admittance scheme with a set-valued operator as the internal feedback, to constrain joint torques within actuators’ limits and ground reaction forces within friction cones to ensure stability against disturbances. Then, it formulates a hierarchical optimization problem using the Hierarchical Quadratic Programming (HQP) to impose the output of the admittance scheme while ensuring physical consistency to maintain compliance behaviors. Unlike traditional compliance control with one-dimensional torque limitations, our approach considers both joints torque limits of manipulator joints and friction cones of quadruped ground reaction as actuation saturation. This ensures overall compliance and stability for the quadruped manipulators, even under significant external forces, regardless of where they are exerted on the robot. We demonstrate through experiments involving variable stiffness environments and external forces during normal operations how effective our approach is in enhancing the safety of quadruped manipulators.},
  archive   = {C_IROS},
  author    = {Tianlin Zhang and Xuanbin Peng and Fenghao Lin and Xiaogang Xiong and Yunjiang Lou},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802754},
  month     = {10},
  pages     = {11124-11131},
  title     = {Whole-body compliance control for quadruped manipulator with actuation saturation of joint torque and ground friction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stable wheel gait generation for planar x-shaped walker with
telescopic legs based on asymmetric impact posture. <em>IROS</em>,
11118–11123. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a novel X-shaped walker with telescopic legs and investigates its control method with the aim of generating a stable wheel gait on a horizontal plane without including zero dynamics which is essentially unstable and difficult to stabilize. First, we outline a planar 6-DOF robot model with three control inputs, and describe the equations of motion and inelastic collision. Second, we design an output-following control system that smoothly controls the extension/contraction lengths of the legs and relative hip-joint angle to their target terminal values, and creates an asymmetric impact posture in the anteroposterior direction so that the robot can easily overcome the next potential barrier. The coefficients of the desired-time trajectory for each control output are updated with the position and velocity values immediately after each impact as the target initial values, so the generated leg motion and control inputs exhibit smooth time variation. The validity of the proposed gait generation method and the change trend of fundamental motion characteristics with respect to control parameters are investigated through numerical simulations.},
  archive   = {C_IROS},
  author    = {Fumihiko Asano and Mikito Komori and Taiki Sedoguchi and Yanqiu Zheng},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802771},
  month     = {10},
  pages     = {11118-11123},
  title     = {Stable wheel gait generation for planar X-shaped walker with telescopic legs based on asymmetric impact posture},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exposing the unseen: Exposure time emulation for offline
benchmarking of vision algorithms. <em>IROS</em>, 11110–11117. (<a
href="https://doi.org/10.1109/IROS58592.2024.10803057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual Odometry (VO) is one of the fundamental tasks in computer vision for robotics. However, its performance is deeply affected by High Dynamic Range (HDR) scenes, omnipresent outdoor. While new Automatic-Exposure (AE) approaches to mitigate this have appeared, their comparison in a reproducible manner is problematic. This stems from the fact that the behavior of AE depends on the environment, and it affects the image acquisition process. Consequently, AE has traditionally only been benchmarked in an online manner, making the experiments non-reproducible. To solve this, we propose a new methodology based on an emulator that can generate images at any exposure time. It leverages BorealHDR, a unique multi-exposure stereo dataset collected over 10 km, on 55 trajectories with challenging illumination conditions. Moreover, it includes lidar-inertial-based global maps with pose estimation for each image frame as well as Global Navigation Satellite System (GNSS) data, for comparison. We show that using these images acquired at different exposure times, we can emulate realistic images, keeping a Root-Mean-Square Error (RMSE) below 1.78 % compared to ground truth images. To demonstrate the practicality of our approach for offline benchmarking, we compared three state-of-the-art AE algorithms on key elements of Visual Simultaneous Localization And Mapping (VSLAM) pipeline, against four baselines. Consequently, reproducible evaluation of AE is now possible, speeding up the development of future approaches. Our code and dataset are available on-line at this link: https://github.com/norlab-ulaval/BorealHDR},
  archive   = {C_IROS},
  author    = {Olivier Gamache and Jean-Michel Fortin and Matěj Boxan and Maxime Vaidis and François Pomerleau and Philippe Giguère},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10803057},
  month     = {10},
  pages     = {11110-11117},
  title     = {Exposing the unseen: Exposure time emulation for offline benchmarking of vision algorithms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LiDAR-based 4D occupancy completion and forecasting.
<em>IROS</em>, 11102–11109. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Scene completion and forecasting are two popular perception problems in research for mobile agents like autonomous vehicles. Existing approaches treat the two problems in isolation, resulting in a separate perception of the two aspects. In this paper, we introduce a novel LiDAR perception task of Occupancy Completion and Forecasting (OCF) in the context of autonomous driving to unify these aspects into a cohesive framework. This task requires new algorithms to address three challenges altogether: (1) sparse-to-dense reconstruction, (2) partial-to-complete hallucination, and (3) 3D-to-4D prediction. To enable supervision and evaluation, we curate a large-scale dataset termed OCFBench from public autonomous driving datasets. We analyze the performance of closely related existing baselines and variants on our dataset. We envision that this research will inspire and call for further investigation in this evolving and crucial area of 4D perception. Our code for data curation and baseline implementation is available at https://github.com/ai4ce/Occ4cast.},
  archive   = {C_IROS},
  author    = {Xinhao Liu and Moonjun Gong and Qi Fang and Haoyu Xie and Yiming Li and Hang Zhao and Chen Feng},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801302},
  month     = {10},
  pages     = {11102-11109},
  title     = {LiDAR-based 4D occupancy completion and forecasting},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DaDiff: Domain-aware diffusion model for nighttime UAV
tracking. <em>IROS</em>, 11094–11101. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Domain adaptation is an inspiring solution to the misalignment issue of day/night image features for nighttime UAV tracking. However, the one-step adaptation paradigm is inadequate in addressing the prevalent difficulties posed by low-resolution (LR) objects when viewed from the UAVs at night, owing to the blurry edge contour and limited detail information. Moreover, these approaches struggle to perceive LR objects disturbed by nighttime noise. To address these challenges, this work proposes a novel progressive alignment paradigm, named domain-aware diffusion model (DaDiff), aligning nighttime LR object features to the daytime by virtue of progressive and stable generations. The proposed DaDiff includes an alignment encoder to enhance the detail information of nighttime LR objects, a tracking-oriented layer designed to achieve close collaboration with tracking tasks, and a successive distribution discriminator presented to distinguish different feature distributions at each diffusion timestep successively. Furthermore, an elaborate nighttime UAV tracking benchmark is constructed for LR objects, namely NUT-LR, consisting of 100 annotated sequences. Exhaustive experiments have demonstrated the robustness and feature alignment ability of the proposed DaDiff. The source code and video demo are available at https://github.com/vision4robotics/DaDiff.},
  archive   = {C_IROS},
  author    = {Haobo Zuo and Changhong Fu and Guangze Zheng and Liangliang Yao and Kunhan Lu and Jia Pan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802294},
  month     = {10},
  pages     = {11094-11101},
  title     = {DaDiff: Domain-aware diffusion model for nighttime UAV tracking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NeuralLabeling: A versatile toolset for labeling vision
datasets using neural radiance fields. <em>IROS</em>, 11086–11093. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present NeuralLabeling, a labeling approach and toolset for annotating 3D scenes using either bounding boxes or meshes and generating segmentation masks, affordance maps, 2D bounding boxes, 3D bounding boxes, 6DOF object poses, depth maps, and object meshes. NeuralLabeling uses Neural Radiance Fields (NeRF) as a renderer, allowing labeling to be performed using 3D spatial tools while incorporating geometric clues such as occlusions, relying only on images captured from multiple viewpoints as input. To demonstrate the applicability of NeuralLabeling to a practical problem in robotics, we added ground truth depth maps to 30000 frames of transparent object RGB and noisy depth maps of glasses placed in a dishwasher captured using an RGBD sensor, yielding the Dishwasher30k dataset. We show that training a simple deep neural network with supervision using the annotated depth maps yields a higher reconstruction performance than training with the previously applied weakly supervised approach. We also show how instance segmentation and depth completion datasets generated using NeuralLabeling can be incorporated into a robot application for grasping transparent objects placed in a dishwasher with an accuracy of 83.3%, compared to 16.3% without depth completion. Supplementary URI: https://florise.github.io/neural_labeling_web/.},
  archive   = {C_IROS},
  author    = {Floris Erich and Naoya Chiba and Abdullah Mustafa and Yusuke Yoshiyasu and Noriaki Ando and Ryo Hanai and Yukiyasu Domae},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801449},
  month     = {10},
  pages     = {11086-11093},
  title     = {NeuralLabeling: A versatile toolset for labeling vision datasets using neural radiance fields},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A sampling ensemble for asymptotically complete motion
planning with volume-reducing workspace constraints. <em>IROS</em>,
11070–11077. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many robot tasks impose constraints on the workspace. For example, a robot may need to move a container without spilling its contents or open a door following the doorknob’s arc. Such constraints may induce narrow volumes in the configuration space, traditionally a challenge for sampling-based methods, and further cause infeasibility. We extend sample-driven connectivity learning (SDCL), a robust approach for planning with narrow passages, to develop a sampling ensemble for workspace constraints. In particular, the ensemble combines SDCL, projection via dual quaternion optimization, and random sampling. These complementary sampling approaches support efficient and robust planning under workspace constraints. Further, this framework offers the ability to determine infeasibility under workspace constraints, which is unaddressed by previous constrained planning methods.},
  archive   = {C_IROS},
  author    = {Sihui Li and Matthew A. Schack and Aakriti Upadhyay and Neil T. Dantam},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801445},
  month     = {10},
  pages     = {11070-11077},
  title     = {A sampling ensemble for asymptotically complete motion planning with volume-reducing workspace constraints},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Roadmaps with gaps over controllers: Achieving efficiency in
planning under dynamics. <em>IROS</em>, 11064–11069. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper aims to improve the computational efficiency of motion planning for mobile robots with non-trivial dynamics through the use of learned controllers. Offline, a system-specific controller is first trained in an empty environment. Then, for the target environment, the approach constructs a data structure, a &quot;Roadmap with Gaps,&quot; to approximately learn how to solve planning queries using the learned controller. The roadmap nodes correspond to local regions. Edges correspond to applications of the learned controller that approximately connect these regions. Gaps arise as the controller does not perfectly connect pairs of individual states along edges. Online, given a query, a tree sampling-based motion planner uses the roadmap so that the tree’s expansion is informed towards the goal region. The tree expansion selects local subgoals given a wavefront on the roadmap that guides towards the goal. When the controller cannot reach a subgoal region, the planner resorts to random exploration to maintain probabilistic completeness and asymptotic optimality. The accompanying experimental evaluation shows that the approach significantly improves the computational efficiency of motion planning on various benchmarks, including physics-based vehicular models on uneven and varying friction terrains as well as a quadrotor under air pressure effects. Website: https://prx-kinodynamic.github.io/projects/rogue},
  archive   = {C_IROS},
  author    = {Aravind Sivaramakrishnan and Sumanth Tangirala and Edgar Granados and Noah R. Carver and Kostas E. Bekris},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802619},
  month     = {10},
  pages     = {11064-11069},
  title     = {Roadmaps with gaps over controllers: Achieving efficiency in planning under dynamics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BOMP: Bin-optimized motion planning. <em>IROS</em>,
11056–11063. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In logistics, the ability to quickly compute and execute pick-and-place motions from bins is critical to increasing productivity. We present Bin-Optimized Motion Planning (BOMP), a motion planning framework that plans arm motions for a six-axis industrial robot with a long-nosed suction tool to remove boxes from deep bins. BOMP considers robot arm kinematics, actuation limits, the dimensions of a grasped box, and a varying height map of a bin environment to rapidly generate time-optimized, jerk-limited, and collision-free trajectories. The optimization is warm-started using a deep neural network trained offline in simulation with 25,000 scenes and corresponding trajectories. Experiments with 96 simulated and 15 physical environments suggest that BOMP generates collision-free trajectories that are up to 58% faster than baseline sampling-based planners and up to 36% faster than an industry-standard Up-Over-Down algorithm, which has an extremely low 15% success rate in this context. BOMP also generates jerk-limited trajectories while baselines do not. Website: https://sites.google.com/berkeley.edu/bomp.},
  archive   = {C_IROS},
  author    = {Zachary Tam and Karthik Dharmarajan and Tianshuang Qiu and Yahav Avigal and Jeffrey Ichnowski and Ken Goldberg},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801312},
  month     = {10},
  pages     = {11056-11063},
  title     = {BOMP: Bin-optimized motion planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BEV-CV: Birds-eye-view transform for cross-view
geo-localisation. <em>IROS</em>, 11048–11055. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cross-view image matching for geo-localisation is a challenging problem due to the significant visual difference between aerial and ground-level viewpoints. The method provides localisation capabilities from geo-referenced images, eliminating the need for external devices or costly equipment. This enhances the capacity of agents to autonomously determine their position, navigate, and operate effectively in GNSS-denied environments. Current research employs a variety of techniques to reduce the domain gap such as applying polar transforms to aerial images or synthesising between perspectives. However, these approaches generally rely on having a 360° field of view, limiting real-world feasibility. We propose BEV-CV, an approach introducing two key novelties with a focus on improving the real-world viability of cross-view geo-localisation. Firstly bringing ground-level images into a semantic Birds-Eye-View before matching embeddings, allowing for direct comparison with aerial image representations. Secondly, we adapt datasets into application realistic format - limited Field-of-View images aligned to vehicle direction. BEV-CV achieves state-of-the-art recall accuracies, improving Top-1 rates of 70° crops of CVUSA and CVACT by 23% and 24% respectively. Also decreasing computational requirements by reducing floating point operations to below previous works, and decreasing embedding dimensionality by 33% - together allowing for faster localisation capabilities.},
  archive   = {C_IROS},
  author    = {Tavis Shore and Simon Hadfield and Oscar Mendez},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802566},
  month     = {10},
  pages     = {11048-11055},
  title     = {BEV-CV: Birds-eye-view transform for cross-view geo-localisation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3D-BLUE: Backscatter localization for underwater robotics.
<em>IROS</em>, 11040–11047. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present the design, implementation, and evaluation of 3D-BLUE, an ultra-low-power underwater 3D localization system that can be deployed on compact robots to accurately localize them in shallow underwater environments. 3D-BLUE’s design introduces two core components. First, it adapts a recent ultra-low-power underwater acoustic communication technology (called piezo-electric backscatter) to the underwater robotics localization problem; specifically, it integrates backscatter nodes into the underwater robot and uses them for localizing it. Second, it leverages the physical properties of the backscatter technology to efficiently extract spatio-temporal-spectral features from the backscatter signal; using these features, it devises a particle-filter-based algorithm to localize the corresponding robot accurately in challenging shallow-water environments. We implemented an end-to-end prototype of 3D-BLUE on a BlueROV2 robot and custombuilt backscatter localization system, and evaluated it in dozens of experimental trials in a pool. Our results demonstrate that 3D-BLUE can localize the robot with an accuracy of around 0.25m at close range and an accuracy of around 1.4m at a range of 10m. This high localization accuracy opens important commercial, naval, and environmental applications in challenging shallow-water environments such as shores, rivers, pools, and narrow waterways.},
  archive   = {C_IROS},
  author    = {Sayed Saad Afzal and Weitung Chen and Fadel Adib},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801869},
  month     = {10},
  pages     = {11040-11047},
  title     = {3D-BLUE: Backscatter localization for underwater robotics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BEVRender: Vision-based cross-view vehicle registration in
off-road GNSS-denied environment. <em>IROS</em>, 11032–11039. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce BEVRender, a novel learning-based approach for the localization of ground vehicles in Global Navigation Satellite System (GNSS)-denied off-road scenarios. These environments are typically challenging for conventional vision-based state estimation due to the lack of distinct visual landmarks and the instability of vehicle poses. To address this, BEVRender generates high-quality local bird’s-eye-view (BEV) images of the local terrain. Subsequently, these images are aligned with a georeferenced aerial map through template matching to achieve accurate cross-view registration. Our approach overcomes the inherent limitations of visual inertial odometry systems and the substantial storage requirements of image-retrieval localization strategies, which are susceptible to drift and scalability issues, respectively. Extensive experimentation validates BEVRender’s advancement over existing GNSS-denied visual localization methods, demonstrating notable enhancements in both localization accuracy and update frequency.},
  archive   = {C_IROS},
  author    = {Lihong Jin and Wei Dong and Wenshan Wang and Michael Kaess},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802702},
  month     = {10},
  pages     = {11032-11039},
  title     = {BEVRender: Vision-based cross-view vehicle registration in off-road GNSS-denied environment},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advancements in radar odometry. <em>IROS</em>, 11024–11031.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Radar odometry estimation has emerged as a critical technique in the field of autonomous navigation, providing robust and reliable motion estimation under various environmental conditions. Despite its potential, the complex nature of radar signals and the inherent challenges associated with processing these signals have limited the widespread adoption of this technology. This paper aims to address these challenges and simultaneously present an understanding about the current advancements in radar odometry estimation. First, we propose novel improvements to an existing state-of-the-art method, which are designed to enhance accuracy and reliability in diverse scenarios. Our pipeline consists of filtering, motion compensation, oriented surface points computation, smoothing, one-to-many radar scan registration, and pose refinement. In particular, we enforce local understanding of a scene by including additional information through smoothing (Gaussian kernels) and alignment (ICP), introduced by us in the existing pipeline. Then, we present an in-depth investigation of the contribution of each improvement to the localization accuracy. Lastly, we benchmark our system and state-of-the-art methods on all sequences of well-known datasets for radar understanding, i.e., the Oxford Radar RobotCar, MulRan, and Boreas datasets. In particular, Boreas includes scenarios with challenging weather conditions, such as snow or overcast, and, to our knowledge, it has never been used for evaluation or benchmarking in the literature. The effectiveness of the proposed improvements is proven by an increased translation and rotation accuracy on the majority of scenarios considered.},
  archive   = {C_IROS},
  author    = {Matteo Frosi and Mirko Usuelli and Matteo Matteucci},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802141},
  month     = {10},
  pages     = {11024-11031},
  title     = {Advancements in radar odometry},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatial spinal fixation: A transformative approach using a
unique robot-assisted steerable drilling system and flexible pedicle
screw. <em>IROS</em>, 11018–11023. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Spinal fixation procedures are currently limited by the rigidity of the existing instruments and pedicle screws leading to fixation failures and rigid pedicle screw pull out. Leveraging our recently developed Concentric Tube Steerable Drilling Robot (CT-SDR) in integration with a robotic manipulator, to address the aforementioned issue, here we introduce the transformative concept of Spatial Spinal Fixation (SSF) using a unique Flexible Pedicle Screw (FPS). The proposed SSF procedure enables planar and out-of-plane placement of the FPS throughout the full volume of the vertebral body. In other words, not only does our fixation system provide the option of drilling in-plane and out-of-plane trajectories, it also enables implanting the FPS inside linear (represented by an I-shape) and/or non-linear (represented by J-shape) trajectories. To thoroughly evaluate the functionality of our proposed robotic system and the SSF procedure, we have performed various experiments by drilling different I-J and J-J drilling trajectory pairs into our custom-designed L3 vertebral phantoms and analyzed the accuracy of the procedure using various metrics.},
  archive   = {C_IROS},
  author    = {Susheela Sharma and Yash Kulkarni and Sarah Go and Jeff Bonyun and Jordan P. Amadio and Maryam Tilton and Mohsen Khadem and Farshid Alambeigi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801485},
  month     = {10},
  pages     = {11018-11023},
  title     = {Spatial spinal fixation: A transformative approach using a unique robot-assisted steerable drilling system and flexible pedicle screw},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An MR safe double-arch needle insertion robot with
scissor-folding mechanism for abdominal percutaneous interventions*.
<em>IROS</em>, 11010–11017. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tumors affecting abdominal organs rank among the deadliest malignancies. In this context, Magnetic Resonance Imaging (MRI) serves as an effective diagnostic tool with a strong potential to support image-guided minimally invasive interventions for treating these tumours, offering an ionizing-radiation-free medical modality. MRI provides exceptional soft tissue contrast and multi-angle imaging, enabling accurate intraoperative localisation of target tumours within these vital organs. Nevertheless, MRI-guided minimally invasive interventions still encounter significant challenges due to the strong magnetic field environment and the narrow and deep bore of MRI machines. This paper proposes a novel MR safe 5-degrees-of-freedom (DoFs) parallel table-mounted double-arch needle insertion robot with a scissor-folding mechanism (SFM) for abdominal interventions. The proposed robot is designed to fit a standard 70-cm MRI bore. Initial evaluation experiments indicate mean errors of 3.14 mm for the proposed robotic arch and 2.23 mm for the full needle insertion robot, respectively. Additionally, preliminary testing of the system in an MRI environment resulted in unaltered MRI imaging output, with negligible artefacts associated with the presence of the robot within the bore.},
  archive   = {C_IROS},
  author    = {Ziting Liang and Chuang Lu and Haoqian Yang and Ryman Hashem and Mohamed E.M.K. Abdelaziz and Lukas Lindenroth and Steve Bandula and Danail Stoyanov and Agostino Stilli},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802516},
  month     = {10},
  pages     = {11010-11017},
  title     = {An MR safe double-arch needle insertion robot with scissor-folding mechanism for abdominal percutaneous interventions*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continuum robot shape estimation using magnetic ball chains.
<em>IROS</em>, 11004–11009. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Shape sensing of medical continuum robots is important both for closed-loop control as well as for enabling the clinician to visualize the robot inside the body. There is a need for inexpensive, but accurate shape sensing technologies. This paper proposes the use of magnetic ball chains as a means of generating shape-specific magnetic fields that can be detected by an external array of Hall effect sensors. Such a ball chain, encased in a flexible polymer sleeve, could be inserted inside the lumen of any continuum robot to provide real-time shape feedback. The sleeve could be removed, as needed, during the procedure to enable use of the entire lumen. To investigate this approach, a shape-sensing model for a steerable catheter tip is derived and an observability and sensitivity analysis are presented. Experiments show maximum estimation errors of 7.1% and mean of 2.9% of the tip position with respect to total length.},
  archive   = {C_IROS},
  author    = {Giovanni Pittiglio and Abdulhamit Donder and Pierre E. Dupont},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801952},
  month     = {10},
  pages     = {11004-11009},
  title     = {Continuum robot shape estimation using magnetic ball chains},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time robotic flexible needle insertion in deformable
living organs using isolated objective constraint. <em>IROS</em>,
10998–11003. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an innovative approach for executing robotic needle insertion within deformable living organs. The objective is to maintain the insertion pivot point on the skin, which remains stationary. At the same time, the organs undergo displacement and deformation due to respiration. Therefore, real-time control and precise needle steering are crucial. The proposed method relies on isolated objective constraints to ensure the objectives while steering the needle along a predefined trajectory. The needle insertion process benefits from Finite Element (FE) models to simulate the environment and address the inverse problem to drive the robot’s end effector (EE) by re-evaluating the objective functions in the constraint space for each time step. So, the desired motion of the robot’s EE could be calculated at a small cost for non-linear functions in real-time, resulting in better precision and reducing stress caused to the organs.},
  archive   = {C_IROS},
  author    = {Thuc Long Ha and Julien Bert and Hadrien Courtecuisse},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802810},
  month     = {10},
  pages     = {10998-11003},
  title     = {Real-time robotic flexible needle insertion in deformable living organs using isolated objective constraint},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Greedy perspectives: Multi-drone view planning for
collaborative perception in cluttered environments. <em>IROS</em>,
10990–10997. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deployment of teams of aerial robots could enable large-scale filming of dynamic groups of people (actors) in complex environments for applications in areas such as team sports and cinematography. Toward this end, methods for submodular maximization via sequential greedy planning can enable scalable optimization of camera views across teams of robots but face challenges with efficient coordination in cluttered environments. Obstacles can produce occlusions and increase chances of inter-robot collision which can violate requirements for near-optimality guarantees. To coordinate teams of aerial robots in filming groups of people in dense environments, a more general view-planning approach is required. We explore how collision and occlusion impact performance in filming applications through the development of a multi-robot multi-actor view planner with an occlusion-aware objective for filming groups of people and compare with a formation planner and a greedy planner that ignores inter-robot collisions. We evaluate our approach based on five test environments and complex multi-actor behaviors. Compared with a formation planner, our sequential planner generates 14% greater view reward for filming the actors in three scenarios and comparable performance to formation planning on two others. We also observe near identical view rewards for sequential planning both with and without inter-robot collision constraints which indicates that robots are able to avoid collisions without impairing performance in the perception task. Overall, we demonstrate effective coordination of teams of aerial robots in environments cluttered with obstacles that may cause collisions or occlusions and for filming groups that may split, merge, or spread apart. Our implementation and the data used to produce results for this paper are available via the companion website: https://greedyperspectives.github.io/},
  archive   = {C_IROS},
  author    = {Krishna Suresh and Aditya Rauniyar and Micah Corah and Sebastian Scherer},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802601},
  month     = {10},
  pages     = {10990-10997},
  title     = {Greedy perspectives: Multi-drone view planning for collaborative perception in cluttered environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data-driven koopman operator-based error-state kalman filter
for enhanced state estimation of quadrotors in agile flight.
<em>IROS</em>, 10983–10989. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Highly dynamic maneuvers pose a challenge to conventional state estimators of quadrotors in rapidly tracking the pose. This paper proposes a data-driven Koopman operator-based error-state Kalman filter (K-ESKF) to enhance pose estimation in agile flight. Our method uses the Koopman operator theory to transform the full-state nonlinear quadrotor dynamics into a lifted bilinear control system driven by accelerations and angular rates. A deep neural network (DNN) is used to represent the Koopman observable functions. Our proposed K-ESKF extends the propagation step of a standard error-state Kalman filter (ESKF) using the lifted bilinear control system. An open-source quadrotor dataset, NeuroBEM, is used for training and evaluating the DNN and for testing the K-ESKF. The learned Koopman bilinear system demonstrates a 60% less attitude errors compared to the first-order Euler method in terms of model accuracy. Using real trajectories from the dataset, our proposed K-ESKF can estimate the pose as accurately as the ESKF during normal flight. More importantly, our proposed approach outperforms the ESKF by achieving about 50% less attitude and velocity estimation errors in a highly agile flight. During drastic attitude and velocity changes, the K-ESKF can still estimate the pose while the ESKF loses tracking.},
  archive   = {C_IROS},
  author    = {Peng Huang and Ketong Zheng and Gerhard Fettweis},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802457},
  month     = {10},
  pages     = {10983-10989},
  title     = {Data-driven koopman operator-based error-state kalman filter for enhanced state estimation of quadrotors in agile flight},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SOAR: Simultaneous exploration and photographing with
heterogeneous UAVs for fast autonomous reconstruction. <em>IROS</em>,
10975–10982. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unmanned Aerial Vehicles (UAVs) have gained significant popularity in scene reconstruction. This paper presents SOAR, a LiDAR-Visual heterogeneous multi-UAV system specifically designed for fast autonomous reconstruction of complex environments. Our system comprises a LiDAR-equipped explorer with a large field-of-view (FoV), alongside photographers equipped with cameras. To ensure rapid acquisition of the scene’s surface geometry, we employ a surface frontier-based exploration strategy for the explorer. As the surface is progressively explored, we identify the uncovered areas and generate viewpoints incrementally. These viewpoints are then assigned to photographers through solving a Consistent Multiple Depot Multiple Traveling Salesman Problem (Consistent-MDMTSP), which optimizes scanning efficiency while ensuring task consistency. Finally, photographers utilize the assigned viewpoints to determine optimal coverage paths for acquiring images. We present extensive benchmarks in the realistic simulator, which validates the performance of SOAR compared with classical and state-of-the-art methods. For more details, please see our project page at sysu-star.github.io/SOAR.},
  archive   = {C_IROS},
  author    = {Mingjie Zhang and Chen Feng and Zengzhi Li and Guiyong Zheng and Yiming Luo and Zhu Wang and Jinni Zhou and Shaojie Shen and Boyu Zhou},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801474},
  month     = {10},
  pages     = {10975-10982},
  title     = {SOAR: Simultaneous exploration and photographing with heterogeneous UAVs for fast autonomous reconstruction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SCP: Soft conditional prompt learning for aerial video
action recognition. <em>IROS</em>, 10967–10974. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a new learning approach, Soft Conditional Prompt Learning (SCP), which leverages the strengths of prompt learning for aerial video action recognition. Our approach is designed to predict the action of each agent by helping the models focus on the descriptions or instructions associated with actions in the input videos for aerial/robot visual perception. Our formulation supports various prompts, including learnable prompts, auxiliary visual information, and large vision models to improve the recognition performance. We present a soft conditional prompt method that learns to dynamically generate prompts from a pool of prompt experts under different video inputs. By sharing the same objective with the task, our proposed SCP can optimize prompts that guide the model’s predictions while explicitly learning input-invariant (prompt experts pool) and input-specific (data-dependent) prompt knowledge. In practice, we observe a 3.17 − 10.2% accuracy improvement on the aerial video datasets (Okutama [1], NECDrone [2]), which consist of scenes with single-agent and multi-agent actions. We further evaluate our approach on ground camera videos to verify the effectiveness and generalization and achieve a 1.0 − 3.6% improvement on SSV2 [3]. We integrate our method into the ROS2 as well.},
  archive   = {C_IROS},
  author    = {Xijun Wang and Ruiqi Xian and Tianrui Guan and Fuxiao Liu and Dinesh Manocha},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802074},
  month     = {10},
  pages     = {10967-10974},
  title     = {SCP: Soft conditional prompt learning for aerial video action recognition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-assessment of robotic laboratory and equipment
readiness using large language models and robotic data capture.
<em>IROS</em>, 10961–10966. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study explores the potential of automating robotic laboratory readiness assessment by integrating Large Language Models (LLMs) with robotic data acquisition. It investigates the capability of LLMs to detect equipment motion and operational status using visual and auditory information. Despite the challenges LLMs face in spatial analysis, this study also investigates LLM grounding methods to ensure accurate workspace assessment. By inspecting a robotic cooking setup with camera-equipped robotic arm, LLMs can detect the motion of custom equipment via color-coded marks, and identify the operational status of kitchen appliances from a single image without any physical augmentations. Additionally, device operation perceived through the emission of loud noises can be assessed by post-processing sound recordings and analyzing loudness and sound frequency metrics presented in a visual plot form. For simple spatial tasks like saucepan positioning, LLM provides accurate assessments when grounded with a single image, while complex workspace safety assessment task requires extensive knowledge of past experiences. By reviewing status of each checklist item, the LLM can decide whether experiment needs to be halted or requires human intervention, offering a set of troubleshooting steps. These findings demonstrate feasibility of the self-assessment approach for robotic laboratory systems, paving the way for future deployments.},
  archive   = {C_IROS},
  author    = {Stefan Ilić and Josie Hughes},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802667},
  month     = {10},
  pages     = {10961-10966},
  title     = {Self-assessment of robotic laboratory and equipment readiness using large language models and robotic data capture},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Novel design of reconfigurable tracked robot with
geometry-changing tracks. <em>IROS</em>, 10953–10960. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tracked robots with reconfigurable mechanisms exhibit great maneuverability due to their adaptability to complex ground conditions. Reconfigurable tracked robots with geometry-changing tracks show further obstacle-crossing capabilities with compact dimensions. However, existing systems face deployment limitations due to either complex transmission mechanisms or unsustainable designs when maintaining the tension in the tracks. To address these challenges, we introduce a novel design of a reconfigurable tracked robot with geometry-changing tracks, which achieves strong terrain traversability with good mechanical properties. We achieve the elliptical trajectory of key planetary wheels through a novel Quad-slider Elliptical Trammel Mechanism (Qs-ETM), allowing the tracks to maintain fixed tension while changing their geometry. Furthermore, the combination of direct drive motors significantly enhances its mechanical properties and agility. A detailed analysis of the kinematic and dynamic characteristics has been conducted and proved with a series of simulations. We built a fully functional prototype of the design and tested it in real-world experiments to validate its advantages. The result shows that our design can reduce the torque required by up to 68.3% and the shear stress of the flipper by up to 67.1%.},
  archive   = {C_IROS},
  author    = {Chice Xuan and Jiadong Lu and Zhihao Tian and Jiacheng Li and Mengke Zhang and Hanbin Xie and Jianxiong Qiu and Chao Xu and Yanjun Cao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802301},
  month     = {10},
  pages     = {10953-10960},
  title     = {Novel design of reconfigurable tracked robot with geometry-changing tracks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design of a fully actuated drone with non-isotropic wrench
shape. <em>IROS</em>, 10945–10952. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a novel design framework for a fully actuated drone with non-isotropic wrench shape. Conventional fully actuated drones face challenges related to high energy consumption during aerial contact manipulation, particularly when force exertion along a specific direction is required. This challenge arises from the balanced tilting of propellers, which leads to an isotropic wrench shape. To address this limitation, we explicitly define the required wrench set (RW) for aerial contact manipulation and integrate it into an optimization problem. To ensure the generation of the RW, we employ the hyperplane shifting method, commonly used for verifying wrench feasibility in cable-driven robots. The optimization aims to minimize hovering energy consumption while ensuring wrench feasibility. Consequently, the proposed design demonstrates a significant improvement over the typical fully actuated drone, with hovering and contact force efficiency more than doubled and nearly 1.4 times higher, respectively. The effectiveness of our design is also validated through simulation.},
  archive   = {C_IROS},
  author    = {Seongsu Park and Min Jun Kim},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801419},
  month     = {10},
  pages     = {10945-10952},
  title     = {Design of a fully actuated drone with non-isotropic wrench shape},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design of a soft shell for a spherical exploration robot
traversing varying terrain. <em>IROS</em>, 10939–10944. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Exploration robots are typically equipped with either wheels or legs for mobility. However, these conventional designs inherently possess certain limitations, such as restricted ability to navigate through steep slopes, soft soil or muddy watery terrain. Despite the advancements in robotics technoloy, these challenges persist in both terrestrial and extraterrestrial environments. To overcome these obstacles, we have proposed a novel solution in the form of a spherical robot with a soft shell. This paper provides an overview of the construction of this spherical robot and the design of its soft shell for adapting to varying terrain. Furthermore, we analyze and characterize its performance across various terrain types as the shell pressure is changed. By employing this approach, the paper establishes the suitability and efficiency of the spherical robot in traversing different terrain by adapting the shell pressure.},
  archive   = {C_IROS},
  author    = {Meghali Prashant Dravid and Micah Oevermann and David McDougall and David Dugas and Robert Ambrose},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802644},
  month     = {10},
  pages     = {10939-10944},
  title     = {Design of a soft shell for a spherical exploration robot traversing varying terrain},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). S-BUN: Soft bifunctional utility module for robot sensing
and signaling. <em>IROS</em>, 10932–10938. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Conventional approaches in robotics for perceiving the environment and signaling the robot’s state or intention for human-robot interaction involve the use of separate sensing and signaling systems. This can sometimes result in high costs and complex system installations. In this study, we propose an alternative approach, integrating both robot sensing and signaling mechanisms into a single utility module (called S-BUN, Soft Bifunctional Utility module for robot sensing aNd signaling). Soft material (Ecoflex 00-10 silicone) is used to form its bun-like structure with a central cavity filled with a NaCl solution. Inspired by honeycombs, the module’s surface incorporates a hexagonal pattern to enhance structural robustness. The design of S-BUN enables it to function as a sensor for both non-contact proximity and touch sensing, utilizing the NaCl solution. Additionally, it serves as a signaling mechanism to indicate the robot’s state through the active inflation and deflation dynamics of the module, simulating lifelike breathing patterns. Through our experiments, we present S-BUN’s capabilities in proximity and touch sensing, including its ability to discern various touch intensities. Finally, we demonstrate the application of S-BUN in the context of reactive behavioral control for a crawling robot and human-robot interaction scenarios.},
  archive   = {C_IROS},
  author    = {Suksakaow Mahuttanatan and Naris Asawalertsak and Jinjuta Paripurana and Kanut Tarapongnivat and Thirawat Chuthong and Poramate Manoonpong},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801310},
  month     = {10},
  pages     = {10932-10938},
  title     = {S-BUN: Soft bifunctional utility module for robot sensing and signaling},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Strain-based modeling of rod-driven soft continuum robots
with co-located embedded sensors. <em>IROS</em>, 10926–10931. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Rod-driven soft robots (RDSR) with a well-balanced performance in terms of perception, precision, and intelligence have a great potential for application. Mathematical description and predicted sensing of deformable soft bodies are crucial to achieve controllable and intelligent behaviors of these robots. In this work, we propose a kinetostatic model for RDSR embedded with co-located sensors based on the Geometric Variable Strain (GVS) approach where local deformations, actuation lengths and external interactions are included. This approach allows us to estimate the shape of RDSR and predict the strain variation of soft bodies under internal and external interactions. Simulations and experimental results show that tip position errors are not greater than 1.8% with respect to the whole body length under different loads (0, 100, 200, 300 gf). The maximum error of predicted sensor length change is up to 2 mm and its percentage relative to the actual length does not exceed 4%. The results demonstrate the accuracy and effectiveness of the proposed model.},
  archive   = {C_IROS},
  author    = {Peiyi Wang and Daniel Feliu-Talegon and Sheng Guo and Federico Renda and Cecilia Laschi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801729},
  month     = {10},
  pages     = {10926-10931},
  title     = {Strain-based modeling of rod-driven soft continuum robots with co-located embedded sensors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Soft finger rotational stability for precision grasps.
<em>IROS</em>, 10919–10925. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft robotic fingers can safely grasp fragile or variable form objects, but their force capacity is limited, especially with less contact area: precision grasps and when objects are smaller or not spherical. Although most research focuses on improving force capacity through mechanical design modifications, optimizing grasping parameters for precision tasks remains crucial. To address this problem, this paper proposes an analytical rotational stability model for soft fingers’ precision grasping, considering grip failure involving slip and dynamic rotational stability. Comprehensive experiments across various objects, grip condition and types of fingers (PneuNet and commercial fingers) are conducted by examining the relationship between grasp parameters and model variables including coulomb friction, bulk stiffness, and dynamic stability. The findings demonstrate the model’s utility in identifying optimal grip parameters that enhance the force capacity of soft fingers without causing dynamic instability. This research contributes to the development of more effective and stable soft robotic fingers for precision grasping tasks.},
  archive   = {C_IROS},
  author    = {Hun Jang and Valentyn Petrichenko and Joonbum Bae and Kevin Haninger},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802720},
  month     = {10},
  pages     = {10919-10925},
  title     = {Soft finger rotational stability for precision grasps},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Embedded 3D printing of silicone for soft actuator with
stiffness gradient and programmable workspace. <em>IROS</em>,
10913–10918. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft pneumatic actuators can accomplish various customizable deformation/motion through the distribution of cavities and gradients in stiffness. However, traditional manufacturing methods, say molding, struggle to produce soft actuators with both complex cavities and desirable stiffness distributions. Regular 3D printing methods usually need extra printheads for support materials to fabricate soft actuators with cavities. In addition, the printing quality and fidelity of the whole structure cannot be uniform due to the effect of gravity, especially for a soft actuator with overhang features. To fabricate a soft actuator of uniform fidelity but desirable stiffness distributions, we propose an embedded 3D printing approach with only one active mixing printhead. By adjusting the mixing ratio of the dual-component silicone, we can achieve designated stiffness gradients, ranging from 30.2 kPa to 198 kPa. With this approach, we successfully fabricate soft pneumatic actuators with overhang features, which exhibit programmable elongation and radial expansion. Additionally, we fabricate soft bending actuators which can achieve programmable workspaces due to their predetermined stiffness distribution.},
  archive   = {C_IROS},
  author    = {Fei Xiao and Zhuoheng Wei and Hao Wang and Jisen Li and Jian Zhu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801545},
  month     = {10},
  pages     = {10913-10918},
  title     = {Embedded 3D printing of silicone for soft actuator with stiffness gradient and programmable workspace},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ManiFoundation model for general-purpose robotic
manipulation of contact synthesis with arbitrary objects and robots.
<em>IROS</em>, 10905–10912. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To substantially enhance robot intelligence, there is a pressing need to develop a large model that enables general-purpose robots to proficiently undertake a broad spectrum of manipulation tasks, akin to the versatile task-planning ability exhibited by LLMs. The vast diversity in objects, robots, and manipulation tasks presents huge challenges. Our work introduces a comprehensive framework to develop a foundation model for general robotic manipulation that formalizes a manipulation task as contact synthesis. Specifically, our model takes as input object and robot manipulator point clouds, object physical attributes, target motions, and manipulation region masks. It outputs contact points on the object and associated contact forces or post-contact motions for robots to achieve the desired manipulation task. We perform extensive experiments both in the simulation and real-world settings, manipulating articulated rigid objects, rigid objects, and deformable objects that vary in dimensionality, ranging from one-dimensional objects like ropes to two-dimensional objects like cloth and extending to three-dimensional objects such as plasticine. Our model achieves average success rates of around 90%. Supplementary materials and videos are available on our project website at https://manifoundationmodel.github.io/.},
  archive   = {C_IROS},
  author    = {Zhixuan Xu and Chongkai Gao and Zixuan Liu and Gang Yang and Chenrui Tie and Haozhuo Zheng and Haoyu Zhou and Weikun Peng and Debang Wang and Tianrun Hu and Tianyi Chen and Zhouliang Yu and Lin Shao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801782},
  month     = {10},
  pages     = {10905-10912},
  title     = {ManiFoundation model for general-purpose robotic manipulation of contact synthesis with arbitrary objects and robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adapting skills to novel grasps: A self-supervised approach.
<em>IROS</em>, 10897–10904. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we study the problem of adapting manipulation trajectories involving grasped objects (e.g. tools) defined for a single grasp pose to novel grasp poses. A common approach to address this is to define a new trajectory for each possible grasp explicitly, but this is highly inefficient. Instead, we propose a method to adapt such trajectories directly while only requiring a period of self-supervised data collection, during which a camera observes the robot’s end-effector moving with the object rigidly grasped. Importantly, our method requires no prior knowledge of the grasped object (such as a 3D CAD model), it can work with RGB images, depth images, or both, and it requires no camera calibration. Through a series of real-world experiments involving 1360 evaluations, we find that self-supervised RGB data consistently outperforms alternatives that rely on depth images including several state-of-the-art pose estimation methods. Compared to the best-performing baseline, our method results in an average of 28.5% higher success rate when adapting manipulation trajectories to novel grasps on several everyday tasks. The appendix accompanying the paper and videos of the experiments are available on our webpage at www.robot-learning.uk/adapting-skills.},
  archive   = {C_IROS},
  author    = {Georgios Papagiannis and Kamil Dreczkowski and Vitalis Vosylius and Edward Johns},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802112},
  month     = {10},
  pages     = {10897-10904},
  title     = {Adapting skills to novel grasps: A self-supervised approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RelationGrasp: Object-oriented prompt learning for
simultaneously grasp detection and manipulation relationship in open
vocabulary. <em>IROS</em>, 10890–10896. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous robotic grasping under complex, clustered, and unstructured environments is a fundamental but challenging task. To achieve human-like rationality in dealing with the grasping task, the agent requires hybrid intelligence from multilateral aspects. This paper introduces RelationGrasp, a unified framework employing a transformer encoder-decoder structure to simultaneously achieve open-vocabulary object detection, manipulation relationship inference, and grasp pose detection. A unique object-oriented prompt learning mechanism is designed to seamlessly bridge the grasp pose and manipulation relationship branches, delivering high fidelity of object-grasp affiliation for object-aware grasping and grasp sequence planning. By formulating the relationship detection as an adjacency matrix regression task under multi-task learning, our framework significantly increases the relationship accuracy with reduced computational overhead. Moreover, to facilitate the robust and adaptive deployment of the proposed RelationGrasp to novel environments, we propose a consistency-based self-supervised adaptation strategy to adapt the pre-trained network to new scenarios and improve grasp accuracy on unseen objects. Our proposed network achieved state-of-the-art performance on various public dataset such as VMRD, OCID, etc., in both grasp detection and manipulation relationship classification, and real-world robot experiments has also been conducted to show the practical usages.},
  archive   = {C_IROS},
  author    = {Songting Liu and Tat Joo Teo and Zhiping Lin and Haiyue Zhu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802508},
  month     = {10},
  pages     = {10890-10896},
  title     = {RelationGrasp: Object-oriented prompt learning for simultaneously grasp detection and manipulation relationship in open vocabulary},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Tactile active inference reinforcement learning for
efficient robotic manipulation skill acquisition. <em>IROS</em>,
10884–10889. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic manipulation holds the potential to replace humans in the execution of tedious or dangerous tasks. However, control-based approaches are not suitable due to the difficulty of formally describing open-world manipulation in reality, and the inefficiency of existing learning methods. Therefore, applying manipulation in a wide range of scenarios presents significant challenges. In this study, we propose a novel framework for skill learning in robotic manipulation called Tactile Active Inference Reinforcement Learning (TactileAIRL), aimed at achieving efficient learning. To enhance the performance of reinforcement learning (RL), we introduce active inference, which integrates model-based techniques and intrinsic curiosity into the RL process. This integration improves the algorithm’s training efficiency and adaptability to sparse rewards. Additionally, we have designed universal tactile static and dynamic features based on vision-based tactile sensors, making our framework scalable to many manipulation tasks learning involving tactile feedback. Simulation results demonstrate that our method achieves significantly high training efficiency in objects pushing tasks. It enables agents to excel in both dense and sparse reward tasks with just few interaction episodes, surpassing the SAC baseline. Furthermore, we conduct physical experiments on a gripper screwing task using our method, which showcases the algorithm’s rapid learning capability and its potential for practical applications.},
  archive   = {C_IROS},
  author    = {Zihao Liu and Xing Liu and Yizhai Zhang and Zhengxiong Liu and Panfeng Huang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802750},
  month     = {10},
  pages     = {10884-10889},
  title     = {Tactile active inference reinforcement learning for efficient robotic manipulation skill acquisition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hybrid human tracking system using UWB sensors and
monocular visual data fusion for human following robots. <em>IROS</em>,
10878–10883. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to follow people can benefit the human-robot interaction of mobile robots. This work proposes a hybrid human tracking system for human following robots, integrating sensor fusion of Ultra-Wideband (UWB) and monocular visual positioning to enhance tracking accuracy and precision. At the same time, UWB and the visual positioning system can operate independently, thereby creating a redundancy in the system. Based on our previous study of UWB-positioning, this article elaborates on a visual positioning system that employs human detection using a pre-trained Convolutional Neural Network (CNN), coupled with data fusion process based on experimental assessments. The hybrid human tracking system achieves a 2D Euclidean accuracy RMS of 7.4 cm, demonstrating sufficient accuracy for human following and improving the following performance in real-world experiments compared to our previous study.},
  archive   = {C_IROS},
  author    = {Dingzhi Zhang and Lukas Birner and Felix Pancheri and Christoph Rehekampff and Darius Burschka and Tim C. Lueth},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801756},
  month     = {10},
  pages     = {10878-10883},
  title     = {A hybrid human tracking system using UWB sensors and monocular visual data fusion for human following robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SFTrack: A robust scale and motion adaptive algorithm for
tracking small and fast moving objects. <em>IROS</em>, 10870–10877. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the problem of multi-object tracking in Unmanned Aerial Vehicle (UAV) footage. It plays a critical role in various UAV applications, including traffic monitoring systems and real-time suspect tracking by the police. However, this task is highly challenging due to the fast motion of UAVs, as well as the small size of target objects in the videos caused by the high-altitude and wide-angle views of drones. In this study, we thus introduce a simple yet more effective method compared to previous work to overcome these challenges. Our approach involves a new tracking strategy, which initiates the tracking of target objects from low-confidence detections commonly encountered in UAV application scenarios. Additionally, we propose revisiting traditional appearance-based matching algorithms to improve the association of low-confidence detections. To evaluate the effectiveness of our method, we conducted benchmark evaluations on two UAV-specific datasets (VisDrone2019, UAVDT) and one general object tracking dataset (MOT17). The results demonstrate that our approach surpasses current state-of-the-art methodologies, highlighting its robustness and adaptability in diverse tracking environments. Furthermore, we have improved the annotation of the UAVDT dataset by rectifying several errors and addressing omissions found in the original annotations. We will provide this refined version of the dataset to facilitate better benchmarking in the field.},
  archive   = {C_IROS},
  author    = {Inpyo Song and Jangwon Lee},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802537},
  month     = {10},
  pages     = {10870-10877},
  title     = {SFTrack: A robust scale and motion adaptive algorithm for tracking small and fast moving objects},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A real-time filter for human pose estimation based on
denoising diffusion models for edge devices. <em>IROS</em>, 10864–10869.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human Pose Estimation (HPE) is increasingly utilized across various sectors, from healthcare to Industry 5.0. To address the inherent inaccuracies in CNN-based HPE systems, filtering models are commonly employed to refine and improve inference results. However, state-of-the-art filtering models often require substantial computational resources, limiting their applicability in resource-constrained environments. To overcome this limitation, we propose a real-time filtering approach based on denoising diffusion models (DM) specifically optimized for edge devices. Through a micro-benchmarking process, we analyze the DM adaptability to different types and levels of noise and determine the optimal setup for specific application scenarios. We present a real-time filter that takes advantage of the DM setup with two configurations to address different application scenarios. Using a widespread edge device, we evaluate the model’s effectiveness in handling both synthetic and real noise generated by state-of-the-art HPE systems. The results demonstrate a significant improvement in real-time filtering performance with minimal computational overhead. The code is available on github.com/PARCO-LAB/LUT-DM-filters.},
  archive   = {C_IROS},
  author    = {Chiara Bozzini and Michele Boldo and Enrico Martini and Nicola Bombieri},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802213},
  month     = {10},
  pages     = {10864-10869},
  title     = {A real-time filter for human pose estimation based on denoising diffusion models for edge devices},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Autonomous behavior planning for humanoid loco-manipulation
through grounded language model. <em>IROS</em>, 10856–10863. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Enabling humanoid robots to perform autonomously loco-manipulation in unstructured environments is crucial and highly challenging for achieving embodied intelligence. This involves robots being able to plan their actions and behaviors in long-horizon tasks while using multi-modality to perceive deviations between task execution and high-level planning. Recently, large language models (LLMs) have demonstrated powerful planning and reasoning capabilities for comprehension and processing of semantic information through robot control tasks, as well as the usability of analytical judgment and decision-making for multi-modal inputs. To leverage the power of LLMs towards humanoid loco-manipulation, we propose a novel language-model based framework that enables robots to autonomously plan behaviors and low-level execution under given textual instructions, while observing and correcting failures that may occur during task execution. To systematically evaluate this framework in grounding LLMs, we created the robot ’action’ and ’sensing’ behavior library for task planning, and conducted mobile manipulation tasks and experiments in both simulated and real environments using the CENTAURO robot, and verified the effectiveness and application of this approach in robotic tasks with autonomous behavioral planning. Video: https://youtu.be/mmnaxthEX34},
  archive   = {C_IROS},
  author    = {Jin Wang and Arturo Laurenzi and Nikos Tsagarakis},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802344},
  month     = {10},
  pages     = {10856-10863},
  title     = {Autonomous behavior planning for humanoid loco-manipulation through grounded language model},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). STL-SLAM: A structured-constrained RGB-d SLAM approach to
texture-limited environments. <em>IROS</em>, 10850–10855. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most RGB-D-based SLAM methods assume texture-rich environments, making them susceptible to significant tracking errors or complete failures in the absence of texture features. Moreover, many existing methods encounter substantial rotation estimation errors, leading to long-term drift in tracking. This paper proposes a novel structured-constrained RGB-D SLAM method (STL-SLAM) for texture-limited environments. Compared to the existing methods, STL-SLAM can deal with environments without abundant texture information and significantly reduce long-term drift caused by rotation estimation errors. We assess the distribution complexity of pixels in an image by calculating the information entropy and pre-processing accordingly. We also present an efficient Manhattan Frames (MF) detection strategy based on orthogonal planes and lines. If MF is detected, we decouple rotation and translation, estimate drift-free rotation based on the Manhattan World (MW) coordinate system, and then estimate translation by minimizing the re-projection error of point, line, and plane features. In non-Manhattan Frames, the 6-DoF pose estimation is performed holistically, with the incorporation of structural constraints of parallel and perpendicular planes, as well as parallel and vertical lines, into the optimization process. Finally, we evaluate our method on public datasets and in real-world environments, which shows that our proposed method achieves superior performance compared to its counterparts.},
  archive   = {C_IROS},
  author    = {Juan Dong and Maobin Lu and Chen Chen and Fang Deng and Jie Chen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801465},
  month     = {10},
  pages     = {10850-10855},
  title     = {STL-SLAM: A structured-constrained RGB-D SLAM approach to texture-limited environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MUP-LIO: Mapping uncertainty-aware point-wise lidar inertial
odometry. <em>IROS</em>, 10843–10849. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a mapping uncertainty-aware point-wise Lidar Inertial Odometry (LIO), which synthesizes the point-wise point-to-plane match and map refreshment into a probabilistic model. As a result, it can address the issue of mismatching during point registration and remove in-frame motion distortion of Lidar sensors. Specifically, the uncertainty-aware map is designed to embody the uncertainty of map geometric features (points and planes), which comes from the Lidar point measurement and pose estimation. Then the map can be modeled in a probabilistic form. In addition, the proposed framework refreshes map at each Lidar point measurement to timely revise geometric features and provide non-delayed map. On the basis, the probabilistic point-to-plane match method is designed to seek a corresponding plane for each Lidar point in point registration, which can enhance the effectiveness of match and provide adaptive observation noises for more accurate state estimation. Comparative experiments on various public datasets are conducted to demonstrate the superior performance of the proposed framework in terms of higher accuracy and better robustness.},
  archive   = {C_IROS},
  author    = {Hekai Yao and Xuetao Zhang and Gang Sun and Yisha Liu and Xuebo Zhang and Yan Zhuang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801762},
  month     = {10},
  pages     = {10843-10849},
  title     = {MUP-LIO: Mapping uncertainty-aware point-wise lidar inertial odometry},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DDS-SLAM: Dense semantic neural SLAM for deformable
endoscopic scenes. <em>IROS</em>, 10837–10842. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Estimating camera motion and continuously reconstructing dense scenes in deformable environments presents a complex and open challenge. Many existing approaches tend to rely on assumptions about the scene’s topology or the nature of deformable motion. However, these assumptions do not hold true in medical endoscopy applications. To address these challenges, we introduce DDS-SLAM, a novel dense deformable semantic neural SLAM that achieves accurate camera tracking, continuous dense scene reconstruction, and high-quality image rendering in deformable scenes. First, we propose a novel hybrid neural scene representation method capable of capturing both natural and artificial deformations. Additionally, by leveraging the 2D semantic information of the scene, we introduce a semantic loss function based on semantic distance fields. This approach guides network optimization at a higher level, thereby enhancing system performance. Furthermore, we validate our method through a series of experiments conducted on several representative medical datasets, demonstrating its superiority over other state-of-the-art approaches. The code is available at: https://github.com/IRMVLab/DDS-SLAM.},
  archive   = {C_IROS},
  author    = {Jiwei Shan and Yirui Li and Lujia Yang and Qiyu Feng and Lijun Han and Hesheng Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802442},
  month     = {10},
  pages     = {10837-10842},
  title     = {DDS-SLAM: Dense semantic neural SLAM for deformable endoscopic scenes},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). AS-LIO: Spatial overlap guided adaptive sliding window
LiDAR-inertial odometry for aggressive FOV variation. <em>IROS</em>,
10829–10836. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {LiDAR-Inertial Odometry (LIO) demonstrates outstanding accuracy and stability in general low-speed and smooth motion scenarios. However, in high-speed and intense motion scenarios, such as sharp turns, two primary challenges arise: firstly, due to the limitations of IMU frequency, the error in estimating significantly non-linear motion states escalates; secondly, drastic changes in the Field of View (FOV) may diminish the spatial overlap between LiDAR frame and pointcloud map (or between frames), leading to insufficient data association and constraint degradation.To address these issues, we propose a novel Adaptive Sliding window LIO framework (AS-LIO) guided by the Spatial Overlap Degree (SOD). Initially, we assess the SOD between the LiDAR frames and the registered map, directly evaluating the adverse impact of current FOV variation on pointcloud alignment. Subsequently, we design an adaptive sliding window to manage the continuous LiDAR stream and control state updates, dynamically adjusting the update step according to the SOD. This strategy enables our odometry to adaptively adopt higher update frequency to precisely characterize trajectory during aggressive FOV variation, thus effectively reducing the non-linear error in positioning. Meanwhile, the historical constraints within the sliding window reinforce the frame-to-map data association, ensuring the robustness of state estimation. Experiments show that our AS-LIO framework can quickly perceive and respond to challenging FOV change, outperforming other state-of-the-art LIO frameworks in terms of accuracy and robustness.},
  archive   = {C_IROS},
  author    = {Tianxiang Zhang and Xuanxuan Zhang and Zongbo Liao and Xin Xia and You Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801561},
  month     = {10},
  pages     = {10829-10836},
  title     = {AS-LIO: Spatial overlap guided adaptive sliding window LiDAR-inertial odometry for aggressive FOV variation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Outlier-robust geometric perception: A novel
thresholding-based estimator with intra-class variance maximization.
<em>IROS</em>, 10821–10828. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Geometric perception problems are fundamental tasks in robotics and computer vision. In real-world applications, they often encounter the inevitable issue of outliers, preventing traditional algorithms from making correct estimates. In this paper, we present a novel general-purpose robust estimator TIVM (Thresholding with Intra-class Variance Maximization) that can collaborate with standard non-minimal solvers to efficiently reject outliers for geometric perception problems. First, we introduce the technique of intra-class variance maximization to design a dynamic 2-group thresholding method on the measurement residuals, aiming to distinctively separate inliers from outliers. Then, we develop an iterative framework that robustly optimizes the model by approaching the pure-inlier group using a multi-layered dynamic thresholding strategy as subroutine, in which a self-adaptive mechanism for layer-number tuning is further employed to minimize the user-defined parameters. We validate the proposed estimator on 3 classic geometric perception problems: rotation averaging, point cloud registration and category-level perception, and experiments show that it is robust against 7090% of outliers and can converge typically in only 315 iterations, much faster than state-of-the-art robust solvers such as RANSAC, GNC and ADAPT. Furthermore, another highlight is that: our estimator can retain approximately the same level of robustness even when the inlier-noise statistics of the problem are fully unknown.},
  archive   = {C_IROS},
  author    = {Lei Sun},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801956},
  month     = {10},
  pages     = {10821-10828},
  title     = {Outlier-robust geometric perception: A novel thresholding-based estimator with intra-class variance maximization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adv3D: Generating 3D adversarial examples for 3D object
detection in driving scenarios with NeRF. <em>IROS</em>, 10813–10820.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep neural networks (DNNs) have been proven extremely susceptible to adversarial examples, which raises special safety-critical concerns for DNN-based autonomous driving stacks (i.e., 3D object detection). Although there are extensive works on image-level attacks, most are restricted to 2D pixel spaces, and such attacks are not always physically realistic in our 3D world. Here we present Adv3D, the first exploration of modeling adversarial examples as Neural Radiance Fields (NeRFs) in driving scenarios. Advances in NeRF provide photorealistic appearances and 3D accurate generation, yielding a more realistic and realizable adversarial example. We train our adversarial NeRF by minimizing the surrounding objects’ confidence predicted by 3D detectors on the training set. Then we evaluate Adv3D on the unseen validation set and show that it can cause a large performance reduction when rendering NeRF in any sampled pose. To enhance physical effectiveness, we propose primitive-aware sampling and semantic-guided regularization that enable 3D patch attacks with camouflage adversarial texture. Experimental results demonstrate that our method surpasses the mesh baseline and generalizes well to different poses, scenes, and 3D detectors. Finally, we provide a defense method to our attacks that improves both the robustness and clean performance of 3D detectors.},
  archive   = {C_IROS},
  author    = {Leheng Li and Qing Lian and Ying-Cong Chen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801323},
  month     = {10},
  pages     = {10813-10820},
  title     = {Adv3D: Generating 3D adversarial examples for 3D object detection in driving scenarios with NeRF},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Progressive query refinement framework for bird’s-eye-view
semantic segmentation from surrounding images. <em>IROS</em>,
10807–10812. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Expressing images with Multi-Resolution (MR) features has been widely adopted in many computer vision tasks. In this paper, we introduce the MR concept into Bird&#39;s-Eye-View (BEV) semantic segmentation for autonomous driving. This introduction enhances our model&#39;s ability to capture both global and local characteristics of driving scenes through our proposed residual learning. Specifically, given a set of MR BEV query maps, the lowest resolution query map is initially updated using a View Transformation (VT) encoder. This updated query map is then upscaled and merged with a higher resolution query map to undergo further updates in a subsequent VT encoder. This process is repeated until the resolution of the updated query map reaches the target. Finally, the lowest resolution map is added to the target resolution to generate the final query map. During training, we enforce both the lowest and final query maps to align with the ground-truth BEV semantic map to help our model effectively capture the global and local characteristics. We also propose a visual feature interaction network that promotes interactions between features across images and across feature levels, thus highly contributing to the performance improvement. We evaluate our model on a large-scale real-world dataset. The experimental results show that our model outperforms the SOTA models in terms of IoU metric. Codes are available at https://github.com/d1024choi/ProgressiveQueryReneNet},
  archive   = {C_IROS},
  author    = {Dooseop Choi and Jungyu Kang and Taeghyun An and Kyounghwan Ahn and KyoungWook Min},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801357},
  month     = {10},
  pages     = {10807-10812},
  title     = {Progressive query refinement framework for bird&#39;s-eye-view semantic segmentation from surrounding images},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual quality inspection planning: A model-based framework
for generating optimal and feasible inspection poses. <em>IROS</em>,
10799–10806. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automatic visual quality inspection is pivotal in both computer vision and robotics. It plays a crucial role in manufacturing, where robotic systems are increasingly employed to enhance the speed and efficiency of visual quality assessments. Several inspection planning methodologies have been developed; however, they often address the inspection challenge from a singular perspective of robotics or computer vision. This work introduces a comprehensive approach that synergistically integrates principles from both domains. We present an innovative algorithm designed to generate optimal inspection poses by considering the interplay between the inspected object’s geometry and the kinematics of the robotic setup used for inspection. This is accomplished by taking advantage of the concept of visibility. The effectiveness of our algorithm is demonstrated through simulations and experiments, revealing complete coverage for diverse geometries and materials with a small number of inspection poses. Moreover, we benchmark our framework against box constraints and workspace sampling techniques to generate feasible inspection poses. The results indicate superior performance in achieving extensive coverage and reducing the number of required optimal inspection poses, enhancing the overall inspection process.},
  archive   = {C_IROS},
  author    = {Vanessa Staderini and Tobias Glück and Philipp Schneider and Andreas Kugi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802791},
  month     = {10},
  pages     = {10799-10806},
  title     = {Visual quality inspection planning: A model-based framework for generating optimal and feasible inspection poses},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A cascaded broad learning system for manipulator motion
control. <em>IROS</em>, 10792–10798. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Intelligent control methods have led to a significant simplification of the robotic arm modeling and control tuning process, and thus they have been widely used. To further improve the precision of robotic arm motion control, this paper proposes a robotic arm motion control strategy based on a cascaded feature-enhanced elastic-net broad learning system (CFE-EN-BLS). This will fully extract data features to improve motion control accuracy. Moreover, ElasticNet regression is introduced to reduce feature redundancy. Finally, Lyapunov stability theory is introduced to constrain the learning parameters of the proposed learning method to enhance the convergence of the control strategy. The simulation and experiment show that the proposed control strategy can realize high-precision trajectory tracking control of the robotic arm.},
  archive   = {C_IROS},
  author    = {Guoyu Zuo and Shuaifeng Dong and Jiyong Zhou and Shuangyue Yu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802175},
  month     = {10},
  pages     = {10792-10798},
  title     = {A cascaded broad learning system for manipulator motion control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bridging the sim-to-real gap with bayesian inference.
<em>IROS</em>, 10784–10791. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present Sim-FSVGD for learning robot dynamics from data. As opposed to traditional methods, Sim-FSVGD leverages low-fidelity physical priors, e.g., in the form of simulators, to regularize the training of neural network models. While learning accurate dynamics already in the low data regime, Sim-FSVGD scales and excels also when more data is available. We empirically show that learning with implicit physical priors results in accurate mean model estimation as well as precise uncertainty quantification. We demonstrate the effectiveness of Sim-FSVGD in bridging the sim-to-real gap on a high-performance RC racecar system. Using model-based RL, we demonstrate a highly dynamic parking maneuver with drifting, using less than half the data compared to the state of the art.},
  archive   = {C_IROS},
  author    = {Jonas Rothfuss and Bhavya Sukhija and Lenart Treven and Florian Dörfler and Stelian Coros and Andreas Krause},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801505},
  month     = {10},
  pages     = {10784-10791},
  title     = {Bridging the sim-to-real gap with bayesian inference},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Control-oriented reinforcement active modeling scheme for
hysteresis compensation of flexible endoscopic robot. <em>IROS</em>,
10777–10783. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hysteresis has posed significant challenges to the modeling and control of flexible endoscopic robots, which impedes the advancement of automated endoscopic operation. Despite numerous hysteresis modeling approaches aimed at improving accuracy, there are still several unresolved issues, such as inappropriate model selection and non-ideal assumption of noise. Focusing on these challenges, a novel reinforcement active modeling (RAM) scheme is proposed in this paper. By incorporating reinforcement learning, this method augments an Extended Kalman Filter (EKF)-based active modeling strategy, which improves the insensitivity and generalization ability to non-Gaussian noise that is not introduced in training. Finally, a series of comparative experiments are conducted on the self-built flexible endoscopic robot to validate the improvement achieved by the proposed scheme. Compared with some widely-applied methods, the proposed scheme achieved at least 63.8% improvement in the root mean square error (RMSE) in modeling accuracy under Gaussian noise conditions, and at least 36.5% improvement in RMSE under Poisson noise conditions.},
  archive   = {C_IROS},
  author    = {Fan Ren and Xiangyu Wang and Yongchun Fang and Yanding Qin and Hongpeng Wang and Ningbo Yu and Jianda Han},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802764},
  month     = {10},
  pages     = {10777-10783},
  title     = {Control-oriented reinforcement active modeling scheme for hysteresis compensation of flexible endoscopic robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Arm-constrained curriculum learning for loco-manipulation
of a wheel-legged robot. <em>IROS</em>, 10770–10776. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Incorporating a robotic manipulator into a wheellegged robot enhances its agility and expands its potential for practical applications. However, the presence of potential instability and uncertainties presents additional challenges for control objectives. In this paper, we introduce an arm-constrained curriculum learning architecture to tackle the issues introduced by adding the manipulator. Firstly, we develop an arm-constrained reinforcement learning algorithm to ensure safety and reliability in control performance after equipping the manipulator. Additionally, to address discrepancies in reward settings between the arm and the base, we propose a reward-aware curriculum learning method. The policy is first trained in Isaac gym and transferred to the physical robot to complete grasping tasks, including the door-opening task, fan-twitching task and the relay-baton-picking and following task. The results demonstrate that our proposed approach effectively controls the arm-equipped wheel-legged robot to master grasping abilities including the dynamic grasping skills, allowing it to chase and catch a moving object while in motion. Please refer to our website (https://acodedog.github.io/wheel-legged-loco-manipulation/) for the code and supplemental videos.},
  archive   = {C_IROS},
  author    = {Zifan Wang and Yufei Jia and Lu Shi and Haoyu Wang and Haizhou Zhao and Xueyang Li and Jinni Zhou and Jun Ma and Guyue Zhou},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802062},
  month     = {10},
  pages     = {10770-10776},
  title     = {Arm-constrained curriculum learning for loco-manipulation of a wheel-legged robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Energy sharing mechanism for freeform robots utilizing
conductive spherical sliding surfaces. <em>IROS</em>, 10762–10769. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Energy sharing among modular robots enables sustainable operation of the system by maintaining energy balance among the modules. In this paper, we propose a novel energy sharing mechanism for FreeSN, a modular self-reconfigurable robot consisting of node and strut modules. Utilizing the feature that our modules are connected in a face-to-face manner, our method successfully establishes an energy sharing channel at almost any point on a sphere by placing transmission intermediaries at the interfacing face between modules, which is facilitated by the combination of brush contact and shell decomposition. Such mechanism also allows the utilization of the node module’s inner space for extra energy storage. A prototype of this energy sharing system has been implemented on FreeSN and rigorously tested. Our findings indicate that energy sharing is reliably established between modules; for strut modules positioned randomly on a node module’s surface, the probability of forming a valid connection is 56.6%. With orientation adjustment, a connection is achievable at nearly any position on the sphere, barring a few exceptional points. As a result, the operational endurance of the strut modules, which provide all the driving forces in the system, is markedly enhanced. This technique also holds potential for broader application across other freeform robotic platforms that incorporate conductive spherical surfaces for sliding connections.},
  archive   = {C_IROS},
  author    = {Xinzhuo Li and Yuxiao Tu and Guanqi Liang and Di Wu and Tin Lun Lam},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801706},
  month     = {10},
  pages     = {10762-10769},
  title     = {Energy sharing mechanism for freeform robots utilizing conductive spherical sliding surfaces},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning from demonstration framework for multi-robot
systems using interaction keypoints and soft actor-critic methods.
<em>IROS</em>, 10754–10761. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning from Demonstration (LfD) is a promising approach to enable Multi-Robot Systems (MRS) to acquire complex skills and behaviors. However, the intricate interactions and coordination challenges in MRS pose significant hurdles for effective LfD. In this paper, we present a novel LfD framework specifically designed for MRS, which leverages visual demonstrations to capture and learn from robot-robot and robot-object interactions. Our framework introduces the concept of Interaction Keypoints (IKs) to transform the visual demonstrations into a representation that facilitates the inference of various skills necessary for the task. The robots then execute the task using sensorimotor actions and reinforcement learning (RL) policies when required. A key feature of our approach is the ability to handle unseen contact-based skills that emerge during the demonstration. In such cases, RL is employed to learn the skill using a classifier-based reward function, eliminating the need for manual reward engineering and ensuring adaptability to environmental changes. We evaluate our framework across a range of mobile robot tasks, covering both behavior-based and contact-based domains. The results demonstrate the effectiveness of our approach in enabling robots to learn complex multi-robot tasks and behaviors from visual demonstrations.},
  archive   = {C_IROS},
  author    = {Vishnunandan L. N. Venkatesh and Byung-Cheol Min},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802324},
  month     = {10},
  pages     = {10754-10761},
  title     = {Learning from demonstration framework for multi-robot systems using interaction keypoints and soft actor-critic methods},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-goal path planning in cluttered environments with
PRM-guided self-organising maps. <em>IROS</em>, 10746–10753. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problem of multi-robot, multi-goal path planning in cluttered environments, motivated by scenarios including surveillance, object search, and package delivery in crowded office spaces and urban environments. While many solutions have been proposed for related vehicle routing problems, they typically do not generalise well for cluttered environments with obstacles due to the introduction of non-Euclidean point-to-point distances. We consider a Self-Organising Map (SOM) algorithm due to its versatility in optimising waypoints within region-based goals. Since standard SOM heavily relies on Euclidean distance-based operations, we propose a generalised SOM with several new innovations: Probabilistic Roadmap (PRM)-guided adaptation and winner selection rules, a two-level path representation for effective routing between goals, and caching operations to overcome the increased computational demands. We present simulation experiments in office and maze environments with one to three robots that show that our approach significantly outperforms standard SOM algorithms as it explicitly reasons over collision avoidance. These results demonstrate the viability of our PRM-guided SOM algorithm for tasks including surveillance in cluttered environments.},
  archive   = {C_IROS},
  author    = {Benjamin R. Davis and Edward Bray and Graeme Best},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802120},
  month     = {10},
  pages     = {10746-10753},
  title     = {Multi-goal path planning in cluttered environments with PRM-guided self-organising maps},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning coordinated maneuver in adversarial environments.
<em>IROS</em>, 10740–10745. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper aims to solve the coordination of a team of robots traversing a route in the presence of adversaries with random positions. Our goal is to minimize the overall cost of the team, which is determined by (i) the accumulated risk when robots stay in adversary-impacted zones and (ii) the mission completion time. During traversal, robots can reduce their speed and act as a ‘guard’ (the slower, the better), which will decrease the risks certain adversary incurs. This leads to a trade-off between the robots’ guarding behaviors and their travel speeds. The formulated problem is highly non-convex and cannot be efficiently solved by existing algorithms. We employ reinforcement learning techniques by developing new encoding and policy-generating methods. Simulations demonstrate that our learning methods can efficiently produce team coordination behaviors. We discuss the reasoning behind these behaviors and explain why they reduce the overall team cost.},
  archive   = {C_IROS},
  author    = {Zechen Hu and Manshi Limbu and Daigo Shishika and Xuesu Xiao and Xuan Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802227},
  month     = {10},
  pages     = {10740-10745},
  title     = {Learning coordinated maneuver in adversarial environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OTVIC: A dataset with online transmission for
vehicle-to-infrastructure cooperative 3D object detection.
<em>IROS</em>, 10732–10739. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vehicle-to-infrastructure cooperative 3D object detection (VIC3D) is a task that leverages both vehicle and roadside sensors to jointly perceive the surrounding environment. However, considering the high speed of vehicles, the real-time requirements, and the limitations of communication bandwidth, roadside devices transmit the results of perception rather than raw sensor data or feature maps in our real-world scenarios. And affected by various environmental factors, the transmission delay is dynamic. To meet the needs of practical applications, we present OTVIC, which is the first multi-modality and multi-view dataset with online transmission from real scenes for vehicle-to-infrastructure cooperative 3D object detection. The ego-vehicle receives the results of infrastructure perception in real-time, collected from a section of highway in Chengdu, China. Moreover, we propose LfFormer, which is a novel end-to-end multi-modality late fusion framework with transformer for VIC3D task as a baseline based on OTVIC. Experiments prove our fusion framework’s effectiveness and robustness. Our project is available at https://sites.google.com/view/otvic.},
  archive   = {C_IROS},
  author    = {He Zhu and Yunkai Wang and Quyu Kong and Yufei Wei and Xunlong Xia and Bing Deng and Rong Xiong and Yue Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802656},
  month     = {10},
  pages     = {10732-10739},
  title     = {OTVIC: A dataset with online transmission for vehicle-to-infrastructure cooperative 3D object detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual place recognition in unstructured driving
environments. <em>IROS</em>, 10724–10731. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The problem of determining geolocation through visual inputs, known as Visual Place Recognition (VPR), has attracted significant attention in recent years owing to its potential applications in autonomous self-driving systems. The rising interest in these applications poses unique challenges, particularly the necessity for datasets encompassing unstructured environmental conditions to facilitate the development of robust VPR methods. In this paper, we address the VPR challenges by proposing an Indian driving VPR dataset that caters to the semantic diversity of unstructured driving environments like occlusions due to dynamic environments, variations in traffic density, viewpoint variability, and variability in lighting conditions. In unstructured driving environments, GPS signals are unreliable often affecting the vehicle to accurately determine location. To address this challenge, we develop an interactive image-to-image tagging annotation tool to annotate large datasets with ground truth annotations for VPR training. Evaluation of the state-of-the-art methods on our dataset shows a significant performance drop of up to 15%, defeating a large number of standard VPR datasets. We also provide an exhaustive quantitative and qualitative experimental analysis of frontal-view, multi-view, and sequence-matching methods. We believe that our dataset will open new challenges for the VPR research community to build robust models. Project Page: https://cvit.iiit.ac.in/research/projects/cvit-projects/iddvpr},
  archive   = {C_IROS},
  author    = {Utkarsh Rai and Shankar Gangisetty and A. H. Abdul Hafez and Anbumani Subramanian and C. V. Jawahar},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802708},
  month     = {10},
  pages     = {10724-10731},
  title     = {Visual place recognition in unstructured driving environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two teachers are better than one: Leveraging depth in
training only for unsupervised obstacle segmentation. <em>IROS</em>,
10716–10723. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel unsupervised obstacle segmentation architecture that follows a novel Relation Distillation (RD) paradigm. Our architecture design was inspired by a self-supervised teacher-student approach that relies on the Semantic Distillation originally devised for representation learning. While the teacher in the Semantic Distillation considers a single patch at a time, the teacher within RD takes a ‘pair of patches’ instead to transfer the local Semantic Co-occurrence Localization (SCooL) relationship that focuses more on the segmentation-boosting signals. To further improve the proposed architecture, we introduce the utilization of another teacher that leverages the depth information which inherently separates the entities at different physical distances, often tied with the boundaries of the obstacles. As the depth is distilled towards the student network only at the time of training, it adds zero computational/hardware cost at run-time. As no relevant public dataset is available, we have curated the Avoiding Obstacles In unstructured Driving (AvOID) dataset as a new testbed for unsupervised obstacle segmentation. We have validated that both the Relation Distillation and depth contribute to boosting the no-annotation segmentation performance on AvOID and KITTI-Obstacles.},
  archive   = {C_IROS},
  author    = {Sungmin Eum and Hyungtae Lee and Heesung Kwon and Phil Osteen and Andre Harrison},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801483},
  month     = {10},
  pages     = {10716-10723},
  title     = {Two teachers are better than one: Leveraging depth in training only for unsupervised obstacle segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PEGASUS: Physically enhanced gaussian splatting simulation
system for 6DoF object pose dataset generation. <em>IROS</em>,
10710–10715. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce Physically Enhanced Gaussian Splatting Simulation System (PEGASUS) for 6DoF object pose dataset generation, a versatile dataset generator based on 3D Gaussian Splatting. Environment and object representations can be easily obtained using commodity cameras to reconstruct with Gaussian Splatting. PEGASUS allows the composition of new scenes by merging the respective underlying Gaussian Splatting point cloud of an environment with one or multiple objects. Leveraging a physics engine enables the simulation of natural object placement within a scene through interaction between meshes extracted for the objects and the environment. Consequently, an extensive amount of new scenes - static or dynamic - can be created by combining different environments and objects. By rendering scenes from various perspectives, diverse data points such as RGB images, depth maps, semantic masks, and 6DoF object poses can be extracted. Our study demonstrates that training on data generated by PEGASUS enables pose estimation networks to successfully transfer from synthetic data to real-world data. Moreover, we introduce the Ramen dataset, comprising 30 Japanese cup noodle items. This dataset includes spherical scans that capture images from both the object hemisphere and the Gaussian Splatting reconstruction, making them compatible with PEGASUS.},
  archive   = {C_IROS},
  author    = {Lukas Meyer and Floris Erich and Yusuke Yoshiyasu and Marc Stamminger and Noriaki Ando and Yukiyasu Domae},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802037},
  month     = {10},
  pages     = {10710-10715},
  title     = {PEGASUS: Physically enhanced gaussian splatting simulation system for 6DoF object pose dataset generation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IDb-RRT: Sampling-based kinodynamic motion planning with
motion primitives and trajectory optimization. <em>IROS</em>,
10702–10709. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Rapidly-exploring Random Trees (RRT) and its variations have emerged as a robust and efficient tool for finding collision-free paths in robotic systems. However, adding dynamic constraints makes the motion planning problem significantly harder, as it requires solving two-value boundary problems (computationally expensive) or propagating random control inputs (uninformative). Alternatively, Iterative Discontinuity Bounded A* (iDb-A*), introduced in our previous study, combines search and optimization iteratively. The search step connects short trajectories (motion primitives) while allowing a bounded discontinuity between the motion primitives, which is later repaired in the trajectory optimization step.Building upon these foundations, in this paper, we present iDb-RRT, a sampling-based kinodynamic motion planning algorithm that combines motion primitives and trajectory optimization within the RRT framework. iDb-RRT is probabilistically complete and can be implemented in forward or bidirectional mode. We have tested our algorithm across a benchmark suite comprising 30 problems, spanning 8 different systems, and shown that iDb-RRT can find solutions up to 10x faster than previous methods, especially in complex scenarios that require long trajectories or involve navigating through narrow passages.},
  archive   = {C_IROS},
  author    = {Joaquim Ortiz-Haro and Wolfgang Hönig and Valentin N. Hartmann and Marc Toussaint and Ludovic Righetti},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802168},
  month     = {10},
  pages     = {10702-10709},
  title     = {IDb-RRT: Sampling-based kinodynamic motion planning with motion primitives and trajectory optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GSRM: Building roadmaps for query-efficient and near-optimal
path planning using a reaction diffusion system. <em>IROS</em>,
10694–10701. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile robots frequently navigate on roadmaps, i.e., graphs where edges represent safe motions, in applications such as healthcare, hospitality, and warehouse automation. Often the environment is quasi-static, i.e., it is sufficient to construct a roadmap once and then use it for any future planning queries. Roadmaps are typically used with graph search algorithm to find feasible paths for the robots. Therefore, the roadmap should be well-connected, and graph searches should produce near-optimal solutions with short solution paths while simultaneously be computationally efficient to execute queries quickly.We propose a new method to construct roadmaps based on the Gray-Scott reaction diffusion system and Delaunay triangulation. Our approach, GSRM, produces roadmaps with evenly distributed vertices and edges that are well-connected even in environments with challenging narrow passages. Empirically, we compare to classical roadmaps generated by 8-connected grids, probabilistic roadmaps (PRM, SPARS2), and optimized roadmap graphs (ORM). Our results show that GSRM consistently produces superior roadmaps that are well-connected, have high query efficiency, and result in short solution paths.},
  archive   = {C_IROS},
  author    = {Christian Henkel and Marc Toussaint and Wolfgang Hönig},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801293},
  month     = {10},
  pages     = {10694-10701},
  title     = {GSRM: Building roadmaps for query-efficient and near-optimal path planning using a reaction diffusion system},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robot active vision-based path planning for localization
improvement in indoor environments. <em>IROS</em>, 10686–10693. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reliable and robust navigation of autonomous mobile robots in indoor environments faces significant challenges due to the absence of GPS, visual degradation, repetitive structures, illumination variations, and low texture. These factors adversely affect localization systems. Current robots often use a uniform navigation approach, regardless of the varying localization uncertainties within different indoor environments. In this paper, we propose a holistic, active vision-based path planning method that produces efficient trajectories, aiming to minimize localization error and enhance navigation performance. Specifically, we utilize a 3D model of an indoor environment to derive an Artificial Potential Field (APF) with its associated localizability scores that encapsulate both visual features’ richness and fiducial markers’ placement. APF is employed to direct a Kinematically Constrained Bi-directional Rapidly Exploring Random Tree (KB-RRT) planner towards the calculation of optimal paths, prioritizing high localization areas. Subsequently, we use an online weight-adaptive MPC-based approach that, apart from robust path planning and obstacle avoidance, guides the robot towards areas with the most robust visual features in order to further refine the localization error. The proposed framework has been extensively tested in both simulation and real-world experiments with a mobile robot in a visually challenging indoor environment.},
  archive   = {C_IROS},
  author    = {Sotirios Barlakas and Dimitrios Alexiou and Kosmas Tsiakas and Dimitrios Katsatos and Ioannis Kostavelis and Dimitrios Giakoumis and Antonios Gasteratos and Dimitrios Tzovaras},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802041},
  month     = {10},
  pages     = {10686-10693},
  title     = {Robot active vision-based path planning for localization improvement in indoor environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fast motion and foothold planning framework for legged
robots on discrete terrain. <em>IROS</em>, 10678–10685. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Legged robot proved their capability to cross complex terrain in recent research, yet the autonomy of robots on discrete terrain still needs to be enhanced since it requires a full stack framework. This paper introduces a real-time motion and foothold planning framework tailored for legged robots navigating uneven terrains, such as stepping stones. Our approach addresses the critical challenges of determining feasible global paths and local footholds to enhance autonomous mobility across complex landscapes. By using a sampling-based global path planner integrated with terrain segmentation and the robot’s kinematic model, our framework swiftly generates viable navigation paths. Concurrently, it utilizes a Mixed Integer Programming (MIP) methodology for real-time foothold optimization, ensuring the robot’s stability and safety through dynamic terrain interaction. Finally, an execution layer including Model Predictive Control (MPC) and Whole-Body Control (WBC) generates the robots’ motion. Simulation and real-world experiments demonstrate that our framework improves legged robots’ adaptability on discrete terrains.},
  archive   = {C_IROS},
  author    = {Jiyu Yu and Dongqi Wang and Zhenghan Chen and Ci Chen and Shuangpeng Wu and Yue Wang and Rong Xiong},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802606},
  month     = {10},
  pages     = {10678-10685},
  title     = {A fast motion and foothold planning framework for legged robots on discrete terrain},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DSLO: Deep sequence LiDAR odometry based on inconsistent
spatio-temporal propagation. <em>IROS</em>, 10672–10677. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a 3D point cloud sequence learning model based on inconsistent spatio-temporal propagation for LiDAR odometry, termed DSLO. It consists of a pyramid structure with a spatial information reuse strategy, a sequential pose initialization module, a gated hierarchical pose refinement module, and a temporal feature propagation module. First, spatial features are encoded using a point feature pyramid, with features reused in successive pose estimations to reduce computational overhead. Second, a sequential pose initialization method is introduced, leveraging the high-frequency sampling characteristic of LiDAR to initialize the LiDAR pose. Then, a gated hierarchical pose refinement mechanism refines poses from coarse to fine by selectively retaining or discarding motion information from different layers based on gate estimations. Finally, temporal feature propagation is proposed to incorporate the historical motion information from point cloud sequences, and address the spatial inconsistency issue when transmitting motion information embedded in point clouds between frames. Experimental results on the KITTI odometry dataset and Argoverse dataset demonstrate that DSLO outperforms state-of-the-art methods, achieving at least a 15.67% improvement on RTE and a 12.64% improvement on RRE, while also achieving a 34.69% reduction in runtime compared to baseline methods. Our implementation will be available at https://github.com/IRMVLab/DSLO.},
  archive   = {C_IROS},
  author    = {Huixin Zhang and Guangming Wang and Xinrui Wu and Chenfeng Xu and Mingyu Ding and Masayoshi Tomizuka and Wei Zhan and Hesheng Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802130},
  month     = {10},
  pages     = {10672-10677},
  title     = {DSLO: Deep sequence LiDAR odometry based on inconsistent spatio-temporal propagation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MICP-l: Mesh-based ICP for robot localization using
hardware-accelerated ray casting. <em>IROS</em>, 10664–10671. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Triangle mesh maps are a versatile 3D environment representation for robots to navigate in challenging indoor and outdoor environments exhibiting tunnels, hills and varying slopes. To make use of these mesh maps, methods are needed to accurately localize robots in such maps to perform essential tasks like path planning and navigation. We present Mesh ICP Localization (MICP-L), a novel and computationally efficient method for registering one or more range sensors to a triangle mesh map to continuously localize a robot in 6D, even in GPS-denied environments. We accelerate the computation of ray casting correspondences (RCC) between range sensors and mesh maps by supporting different parallel computing devices like multicore CPUs, GPUs and the latest NVIDIA RTX hardware. By additionally transforming the covariance computation into a reduction operation, we can optimize the initial guessed poses in parallel on CPUs or GPUs, making our implementation applicable in real-time on many architectures. We demonstrate the robustness of our localization approach with datasets from agricultural, aerial, and automotive domains.},
  archive   = {C_IROS},
  author    = {Alexander Mock and Thomas Wiemann and Sebastian Pütz and Joachim Hertzberg},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802360},
  month     = {10},
  pages     = {10664-10671},
  title     = {MICP-L: Mesh-based ICP for robot localization using hardware-accelerated ray casting},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EVIT: Event-based visual-inertial tracking in semi-dense
maps using windowed nonlinear optimization. <em>IROS</em>, 10656–10663.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Event cameras are an interesting visual exteroceptive sensor that reacts to brightness changes rather than integrating absolute image intensities. Owing to this design, the sensor exhibits strong performance in situations of challenging dynamics and illumination conditions. While event-based simultaneous tracking and mapping remains a challenging problem, a number of recent works have pointed out the sensor’s suitability for prior map-based tracking. By making use of cross-modal registration paradigms, the camera’s ego-motion can be tracked across a large spectrum of illumination and dynamics conditions on top of accurate maps that have been created a priori by more traditional sensors. The present paper follows up on a recently introduced event-based geometric semi-dense tracking paradigm, and proposes the addition of inertial signals in order to robustify the estimation. More specifically, the added signals provide strong cues for pose initialization as well as regularization during windowed, multi-frame tracking. As a result, the proposed framework achieves increased performance under challenging illumination conditions as well as a reduction of the rate at which intermediate event representations need to be registered in order to maintain stable tracking across highly dynamic sequences. Our evaluation focuses on a diverse set of real world sequences and comprises a comparison of our proposed method against a purely event-based alternative running at different rates.},
  archive   = {C_IROS},
  author    = {Runze Yuan and Tao Liu and Zijia Dai and Yi-Fan Zuo and Laurent Kneip},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802117},
  month     = {10},
  pages     = {10656-10663},
  title     = {EVIT: Event-based visual-inertial tracking in semi-dense maps using windowed nonlinear optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-modal visual relocalization in prior LiDAR maps
utilizing intensity textures. <em>IROS</em>, 10650–10655. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cross-modal localization has drawn increasing attention in recent years, while the visual relocalization in prior LiDAR maps is less studied. Related methods usually suffer from inconsistency between the 2D texture and 3D geometry, neglecting the intensity features in the LiDAR point cloud. In this paper, we propose a cross-modal visual relocalization system in prior LiDAR maps utilizing intensity textures, which consists of three main modules: map projection, coarse retrieval, and fine relocalization. In the map projection module, we construct the database of intensity channel map images leveraging the dense characteristic of panoramic projection. The coarse retrieval module retrieves the top-K most similar map images to the query image from the database, and retains the top-K’ results by covisibility clustering. The fine relocalization module applies a two-stage 2D-3D association and a covisibility inlier selection method to obtain robust correspondences for 6DoF pose estimation. The experimental results on our self-collected datasets demonstrate the effectiveness in both place recognition and pose estimation tasks.},
  archive   = {C_IROS},
  author    = {Qiyuan Shen and Hengwang Zhao and Weihao Yan and Chunxiang Wang and Tong Qin and Ming Yang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801498},
  month     = {10},
  pages     = {10650-10655},
  title     = {Cross-modal visual relocalization in prior LiDAR maps utilizing intensity textures},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hybrid vision/force control strategy for handheld robotic
devices enhancing probe-based confocal laser endomicroscopy.
<em>IROS</em>, 10642–10649. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a novel hybrid vision/force control strategy for robotic devices designed to obtain clear and consistent images using probe-based confocal laser endomicroscopy (pCLE). Due to the variable nature of tissue characteristics encountered during pCLE imaging, the conventional approach of pre-setting forces or focus metrics for either force or vision control is often impractical and inadequate. To address this, our strategy employs a blur metric called the CR score to assess the level of blur in pCLE images, enabling the attainment of clear and focused images. At the onset of a pCLE scan, the system autonomously determines the target CR score for vision control, in tandem with a real-time peak detection algorithm. Concurrently, force control is applied judiciously to prevent excessive force on the tissue, adjusting to ensure minimal force is applied, thus preserving image focus. This innovative approach facilitates seamless transitions between vision and force control, depending on the imaging conditions, thereby ensuring the acquisition of consistent pCLE images with minimal force. Our method marks a notable improvement over conventional PID force control techniques. By dynamically adjusting target forces and minimizing force application during operation, we not only enhance the precision and quality of pCLE imaging but also eliminate the dependency on manual pre-settings.},
  archive   = {C_IROS},
  author    = {Ingu Choi and Eunchan Kim and Sungwook},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801436},
  month     = {10},
  pages     = {10642-10649},
  title     = {A hybrid Vision/Force control strategy for handheld robotic devices enhancing probe-based confocal laser endomicroscopy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward micro eye movement detection in practice: Stand-alone
eye tracker with high resolution and wide measurement range.
<em>IROS</em>, 10634–10641. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Detecting the micromovements of eyes that reflect a person’s inner state can be an essential step in many applications, but most eye trackers need to fix the subject’s head using a chin rest to obtain sufficient data quality. We propose a stand-alone eye tracker that utilizes two 500 fps cameras, a pair of rotating mirrors for gaze control, a liquid lens for focus control and an intensity-controllable light source, and describe how the proposed system works in real-time. The experimental results show that our system covers more than twice as wide a measurement range in the depth direction as the conventional eye tracker while achieving sufficient data quality to analyze microsaccades with an amplitude of down to 0.2 deg. We also examine the practical use of the proposed system for microsaccade detection.},
  archive   = {C_IROS},
  author    = {Keiko Yokoyama and Tomohiro Sueishi and Michiaki Inoue and Shoji Yachida and Toshinori Hosoi and Masatoshi Ishikawa},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801891},
  month     = {10},
  pages     = {10634-10641},
  title     = {Toward micro eye movement detection in practice: Stand-alone eye tracker with high resolution and wide measurement range},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An octopus-inspired-configuration sensor array concept
toward torso-oriented magnetic localization task and simulation
verification. <em>IROS</em>, 10628–10633. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In response to torso-oriented magnetic localization tasks that require the system to have interactivity and flexibility with guaranteed accuracy, a novel bio-inspired magnetic sensor array configuration is proposed in this paper. Precisely, the ideas of the natural characteristics of octopus flexible tentacles and the &quot;wrap&quot; morphology are integrated into the design of the magnetic localization system based on the sensor array method. It is worth mentioning that such a design enhances the interactivity and flexibility of the localization system compared to the general planar sensor array strategy. Apart from the concept introduction, the geometry analysis of the proposed configuration is presented based on the constant curvature model. Besides, the magnetic localization algorithm for the system is presented by constructing a magnetic tracking optimization function. Eventually, the proposed concept and developed algorithm are examined in the sensor-array-simulation environment to manifest their effectiveness and applicability. The experimental results indicate that the octopus-inspired-configuration sensor array achieves a mean accuracy at a centimeter-level in our cases, and has better accuracy with a mean value of ē as 0.0178 m and ${\overline {SQR} _{ave{\text{ }}}}$ as 0.0883 for the center interest space compared to general planar configuration one. Moreover, the effect of the configuration error is analyzed. These results verify the feasibility and superiority of the proposed concept and hold significant practical significance in addressing the challenge associated with magnetic localization tasks toward the clinical application scenarios.},
  archive   = {C_IROS},
  author    = {Yichong Sun and Wai Shing Chan and Yehui Li and Heng Zhang and Yisen Huang and Haochen Hu and Philip Wai Yan Chiu and Zheng Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802250},
  month     = {10},
  pages     = {10628-10633},
  title     = {An octopus-inspired-configuration sensor array concept toward torso-oriented magnetic localization task and simulation verification},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advanced handheld micro-surgical system using an hall sensor
and a magnet trocar for retinal microsurgery. <em>IROS</em>,
10621–10627. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Diseases affecting the retina, such as retinal detachment, diabetic retinopathy, and macular degeneration, are significant contributors to blindness globally, with a substantial risk of vision loss among those afflicted. Surgical treatment of these conditions is complex due to the delicate nature of retinal tissue and the challenges posed by involuntary hand movements. While existing methods aim to compensate for hand tremors using sensor-based systems, they are hindered by limitations in accurately tracking retinal surface movement during surgery, particularly in response to patient movements under anesthesia. To address these issues, this study proposes a novel handheld micro-surgical tool equipped with a 1-degree of freedom (DOF) mechanism and a 3-axis Hall sensor to mitigate physiological hand tremors effectively. By utilizing magnetic flux density measurements, the tool can pinpoint the position of a magnet embedded within the surgical instrument, enabling precise tremor compensation without reliance on a global coordinate system. The design incorporates a piezoelectric (PZT) linear actuator and a Hall sensor for compactness and sensitivity. Optimization of the magnet’s dimensions through simulation ensures optimal sensor performance. Experimental validation using artificial and ex-vivo porcine eye models demonstrates the tool’s effectiveness in reducing hand tremors, suggesting potential enhancements in the safety and accuracy of retinal surgeries. For the desired positions from 4000 µm to 1000 µm, the RMS error of the synthetic eye model and porcine eye decreased from 71.10 µm to 33.27 µm and 71.36 µm to 33.39 µm, respectively.},
  archive   = {C_IROS},
  author    = {Myung Ho Lee and Jintaek Im and Cheol Song},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802576},
  month     = {10},
  pages     = {10621-10627},
  title     = {Advanced handheld micro-surgical system using an hall sensor and a magnet trocar for retinal microsurgery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Structure-invariant range-visual-inertial odometry.
<em>IROS</em>, 10613–10620. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Mars Science Helicopter (MSH) mission aims to deploy the next generation of unmanned helicopters on Mars, targeting landing sites in highly irregular terrain such as Valles Marineris, the largest canyons in the Solar system with elevation variances of up to 8000 meters. Unlike its predecessor, the Mars 2020 mission, which relied on a state estimation system assuming planar terrain, MSH requires a novel approach due to the complex topography of the landing site. This paper introduces a novel range-visual-inertial odometry system tailored for the unique challenges of the MSH mission. Our system extends the state-of-the-art xVIO framework by fusing consistent range information with visual and inertial measurements, preventing metric scale drift in the absence of visual-inertial excitation (mono camera and constant velocity descent), and enabling landing on any terrain structure, without requiring any planar terrain assumption. Through extensive testing in image-based simulations using actual terrain structure and textures collected in Mars orbit, we demonstrate that our range-VIO approach estimates terrain-relative velocity meeting the stringent mission requirements, and outperforming existing methods.},
  archive   = {C_IROS},
  author    = {Ivan Alberico and Jeff Delaune and Giovanni Cioffi and Davide Scaramuzza},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801775},
  month     = {10},
  pages     = {10613-10620},
  title     = {Structure-invariant range-visual-inertial odometry},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OmniNxt: A fully open-source and compact aerial robot with
omnidirectional visual perception. <em>IROS</em>, 10605–10612. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Adopting omnidirectional Field of View (FoV) cameras in aerial robots vastly improves perception ability, significantly advancing aerial robotics’s capabilities in inspection, reconstruction, and rescue tasks. However, such sensors also elevate system complexity, e.g., hardware design, and corresponding algorithm, which limits researchers from utilizing aerial robots with omnidirectional FoV in their research. To bridge this gap, we propose OmniNxt, a fully open-source aerial robotics platform with omnidirectional perception. We design a high-performance flight controller Nxt-FC and a multi-fisheye camera set for OmniNxt. Meanwhile, the compatible software is carefully devised, which empowers OmniNxt to achieve accurate localization and real-time dense mapping with limited computation resource occupancy. We conducted extensive real-world experiments to validate the superior performance of OmniNxt in practical applications. All the hardware and software are open-access at3, and we provide docker images of each crucial module in the proposed system. Project page: https://hkust-aerial-robotics.github.io/OmniNxt.},
  archive   = {C_IROS},
  author    = {Peize Liu and Chen Feng and Yang Xu and Yan Ning and Hao Xu and Shaojie Shen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802134},
  month     = {10},
  pages     = {10605-10612},
  title     = {OmniNxt: A fully open-source and compact aerial robot with omnidirectional visual perception},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Drones guiding drones: Cooperative navigation of a
less-equipped micro aerial vehicle in cluttered environments.
<em>IROS</em>, 10597–10604. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reliable deployment of Unmanned Aerial Vehicles (UAVs) in cluttered unknown environments requires accurate sensors for Global Navigation Satellite System (GNSS)-denied localization and obstacle avoidance. Such a requirement limits the usage of cheap and micro-scale vehicles with constrained payload capacity if industrial-grade reliability and precision are required. This paper investigates the possibility of offloading the necessity to carry heavy sensors to another member of the UAV team while preserving the desired capability of the smaller robot intended for exploring narrow passages. A novel cooperative guidance framework offloading the sensing requirements from a minimalistic secondary UAV to a superior primary UAV is proposed. The primary UAV constructs a dense occupancy map of the environment and plans collision-free paths for both UAVs to ensure reaching the desired secondary UAV’s goals even in areas not accessible by the bigger robot. The primary UAV guides the secondary UAV to follow the planned path while tracking the UAV using Light Detection and Ranging (LiDAR)-based relative localization. The proposed approach was verified in real-world experiments with a heterogeneous team of a 3D LiDAR-equipped primary UAV and a micro-scale camera-equipped secondary UAV moving autonomously through unknown cluttered GNSS-denied environments with the proposed framework running fully on board the UAVs.},
  archive   = {C_IROS},
  author    = {Václav Pritzl and Matouš Vrba and Yurii Stasinchuk and Vít Krátký and Jiří Horyna and Petr Štěpán and Martin Saska},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802770},
  month     = {10},
  pages     = {10597-10604},
  title     = {Drones guiding drones: Cooperative navigation of a less-equipped micro aerial vehicle in cluttered environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trajectory optimization with global yaw parameterization for
field-of-view constrained autonomous flight. <em>IROS</em>, 10590–10596.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trajectory generation for quadrotors with limited field-of-view sensors has numerous applications such as aerial exploration, coverage, inspection, videography, and target tracking. Most previous works simplify the task of optimizing yaw trajectories by either aligning the heading of the robot with its velocity, or potentially restricting the feasible space of candidate trajectories by using a limited yaw domain to circumvent angular singularities. In this paper, we propose a novel global yaw parameterization method for trajectory optimization that allows a 360-degree yaw variation as demanded by the underlying algorithm. This approach effectively bypasses inherent singularities by including supplementary quadratic constraints and transforming the final decision variables into the desired state representation. This method significantly reduces the needed control effort, and improves optimization feasibility. Furthermore, we apply the method to several examples of different applications that require jointly optimizing over both the yaw and position trajectories. Ultimately, we present a comprehensive numerical analysis and evaluation of our proposed method in both simulation and real-world experiments.},
  archive   = {C_IROS},
  author    = {Yuwei Wu and Yuezhan Tao and Igor Spasojevic and Vijay Kumar},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802607},
  month     = {10},
  pages     = {10590-10596},
  title     = {Trajectory optimization with global yaw parameterization for field-of-view constrained autonomous flight},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). A high-performance anthropomorphic robotic arm for
household applications. <em>IROS</em>, 10582–10589. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Anthropomorphic robotic arms, mimicking the structure and function of human arms, show great potential for helping people in various tedious and repetitive household tasks. However, such arms mostly consist of multiple serial links controlled independently by actuators at joints with high reduction ratios, posing challenges in household services in terms of load capacity, responsiveness, and safety. In this paper, we propose a high-performance anthropomorphic arm called TRX-Arm based on differential cable transmission, characterized by features of high dynamics, high load capacity, and inherent compliance. TRX-Arm is composed of three deferential cable-driven coupling joints and one independent roll joint. Thanks to the cable differential transmission, the joints are capable of achieving doubled torque and stiffness without replacing motors. To enhance safety in human-robot interaction, the actuators including motors, reducer, belt, and pulley are mounted at the shoulder near the base and drive the joints remotely using cables, thereby minimizing the inertia of the whole arm. The workspace of TRX-Arm has a volume of 1.56 m3, much larger than that of the human arm. Real experiments show its capabilities including high repeatability and load capacity as well as high dynamic behavior of a dual-arm robot platform built with TRX-Arms.},
  archive   = {C_IROS},
  author    = {Tianliang Liu and Sicheng Yang and Jingchen Li and Xiangchi Chen and Shuai Wang and Xiao Teng and Wangwei Lee and Xiong Li and Yu Zheng},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802154},
  month     = {10},
  pages     = {10582-10589},
  title     = {A high-performance anthropomorphic robotic arm for household applications},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Synergizing morphological computation and generative design:
Automatic synthesis of tendon-driven grippers. <em>IROS</em>,
10576–10581. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The design process of robotic systems is a complex journey that involves multiple phases. Throughout this process, the aim is to tackle various criteria simultaneously, even though they often contradict each other. The ultimate goal is to uncover the optimal solution that resolves these conflicting factors. Within this paper we propose a design methodology to generate linkage mechanisms for robots with morphological computation. We use a graph grammar and a heuristic search algorithm to create robot mechanism graphs that are converted into simulation models for testing the design output. To verify the design methodology we have applied it to a relatively simple quasi-static problem of object grasping. Designing a fully actuated gripper may seem simple, but we found a way to automatically design an underactuated tendon-driven gripper that can grasp a wide range of objects. This is possible because of its structure, not because of sophisticated planning or learning. To test the applicability of the proposed method in real engineering practice, we used it to create physical prototypes. Simulation results together with results of testing of physical prototypes are given at the end of the paper. The framework is open source and the link to GitHub is given in the paper.},
  archive   = {C_IROS},
  author    = {Kirill D. Zharkov and Mikhail E. Chaikovskii and Yefim V. Osipov and Rahaf Alshaowa and Ivan I. Borisov and Sergey A. Kolyubin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801489},
  month     = {10},
  pages     = {10576-10581},
  title     = {Synergizing morphological computation and generative design: Automatic synthesis of tendon-driven grippers},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Frozen assets: Leveraging ice, water, and phase transitions
in robots. <em>IROS</em>, 10570–10575. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots are especially useful in cold, remote, and inhospitable environments such as polar regions and extraterrestrial settings. Due to subfreezing temperatures and limited resources in these environments, robots made of ice are particularly advantageous. In this paper we demonstrate how the solid and liquid phases of water, and transitions between these phases, can be leveraged into common robot designs for modular robots, robot arms, rovers, and soft robots. We explore how robots can utilize structural elements made of ice and exploit the phase change between ice and water to augment their capabilities. Additionally, we do a scaling analysis of ice structural elements to provide insight on their performance at different length scales and ambient temperatures.},
  archive   = {C_IROS},
  author    = {Aaron Wilhelm and Andrew Wilhelm and Lydia Isabela Calderon-Aceituno and Nils Napp and Kirstin Petersen and E. Farrell Helbling},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801495},
  month     = {10},
  pages     = {10570-10575},
  title     = {Frozen assets: Leveraging ice, water, and phase transitions in robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and preliminary validation of a multi-mode quadrotor
aerial-aquatic vehicle with tilting mechanism. <em>IROS</em>,
10564–10569. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents the development of a quadrotor vehicle capable of movement between air and water environments. The design challenges, including torque matching in the propulsion system, balance maintenance in different mediums, and waterproofing requirements, are discussed. A prototype is constructed to demonstrate the feasibility of the proposed design. Through the tilting mechanism in four rotors, the vehicle can not only fly in air like a normal quadrotor vehicle, but also propel in water with two configured rear thrusters. Moreover, the four tilted rotors cooperate in aligning the center of gravity and buoyancy, addressing limitations observed in traditional aerial-aquatic vehicles. The prototype has been verified through experiments in both air and water.},
  archive   = {C_IROS},
  author    = {Mengyao Liu and Bai Chen and Lingyu Wang and Zebing Mao and Yayi Shen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801901},
  month     = {10},
  pages     = {10564-10569},
  title     = {Design and preliminary validation of a multi-mode quadrotor aerial-aquatic vehicle with tilting mechanism},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SoftNeRF: A self-modeling soft robot plugin for various
tasks. <em>IROS</em>, 10558–10563. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Building a self-model for robots, enabling them to simulate their physical selves and predict future states without direct interaction with the physical world, is crucial for robot motion planning and control. Existing self-modeling methods primarily focus on rigid robots and typically require significant time, effort, and resources to gather training data. In this study, we introduce SoftNeRF, a self-supervised visual self-model designed for soft robots. We use a hybrid neural shape representation based on the Signed Distance Function (SDF) to capture both the geometry and complex nonlinear motion of soft robots. By leveraging differentiable rendering, our method learns a self-model from readily available RGB images, similar to how humans understand their physical state through reflection. To improve training efficiency and model accuracy, we propose an error-guided adaptive sampling strategy. SoftNeRF can serve as a plug-in for various downstream tasks, even when trained with data unrelated to those tasks. We demonstrate SoftNeRF’s ability to support shape prediction and motion planning for robots in both simulated and real-world environments. Furthermore, SoftNeRF excels in detecting and recovering from damage, thereby enhancing machine resilience. Code is available at: https://github.com/irmvlab/soft-nerf.},
  archive   = {C_IROS},
  author    = {Jiwei Shan and Yirui Li and Qiyu Feng and Ditao Li and Lijun Han and Hesheng Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801344},
  month     = {10},
  pages     = {10558-10563},
  title     = {SoftNeRF: A self-modeling soft robot plugin for various tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and control of a novel soft-rigid lower limb
exoskeleton robot. <em>IROS</em>, 10550–10557. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a study on the design and control of a novel soft-rigid lower limb exoskeleton robot. First, based on anatomy, a novel exoskeleton structure design is proposed that applies Curl Pneumatic Artificial Muscles (CPAMs) to lower limb joints to actuate lower limb movement, and transmit force and motion through rigid parts. A phenomenological characteristic model of the CPAM based on experimental tests and fitting methods is constructed for exoskeleton control. Then, a feedforward-feedback hybrid control method based on four CPAM characteristic models is proposed to improve the control accuracy and stability of the exoskeleton. A human-in-the-loop control method based on human-robot hybrid dynamics modeling and human intentions and states is proposed to improve the human-robot interaction performance of exoskeletons. Experimental results show that feedforward-feedback hybrid control can reduce the maximum tracking error of the exoskeleton to 3.4% for hip, 2.9% for knee, and 4.7% for ankle joint. The exoskeleton can achieve intentional control based on EMG signals. With the assistance of the exoskeleton, the muscle activity of the human lower limbs is reduced by an average of 32.2%. The proposed soft-rigid lower limb exoskeleton robot has the advantages of being lightweight, having good flexibility, being comfortable wearing and having good human-computer interaction, which can improve efficiency. In the future, it will provide more effective intelligent rehabilitation equipment for patients with lower limb movement disorders.},
  archive   = {C_IROS},
  author    = {Yuxuan Wang and Shaoke Yuan and Zihan Pu and Jiangbei Wang and Yanqiong Fei},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802692},
  month     = {10},
  pages     = {10550-10557},
  title     = {Design and control of a novel soft-rigid lower limb exoskeleton robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tunable stiffness glove for tremor suppression based on 3D
printed structured fabrics. <em>IROS</em>, 10543–10549. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tremors, which are prevalent symptoms in both Parkinson’s disease (PD) and essential tremor (ET), substantially diminish the quality of life for those affected. Traditional treatments, including pharmaceutical medications and invasive surgical procedures, often come with limitations and side effects, prompting the need for alternative solutions. In this paper, the Tunable Stiffness Glove (TSG) is developed as a non-invasive exoskeleton to suppress wrist tremors. By employing chain mail fabrics with tunable stiffness, the TSG permits natural wrist movement when in a soft state, while transitioning to a jammed state upon the application of negative pressure, effectively suppressing tremors in two directions. Evaluation of the TSG’s efficacy was conducted through comprehensive three-point bending tests and human trials, utilizing commercial mechanical tester, inertial measurement unit (IMU), and electromyography (EMG) sensors. Weighing a mere 92 grams, the TSG demonstrated remarkable tremor suppression rates of 74.86%±5.52% (in the flexion-extension direction) and 66.80%±15.47% (in the adduction-abduction direction). Future enhancements aim to optimize the design for increased damping force and integrate sophisticated control strategies for improved user-exoskeleton interaction.},
  archive   = {C_IROS},
  author    = {Yu Chen and Junwei Li and Xudong Yang and Yifan Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801707},
  month     = {10},
  pages     = {10543-10549},
  title     = {Tunable stiffness glove for tremor suppression based on 3D printed structured fabrics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vine robots that evert through bending. <em>IROS</em>,
10535–10542. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite the elegantly simple operation principle, the design of everting vine robots still presents two complexities: a tip-device required for buckle-free retraction; a pressurised base chamber to maintain inflation while allowing the everted body to be winded onto a motorised reel. We create a new type of vine robots with a unique eversion method: instead of using a single everting tube, our design comprises multiple interconnected tubes that bend to induce an overall eversion. Our design eliminates the pressurised base and the tip-device, while allowing buckle-free retraction. It also enables new designs and functionalities beyond conventional vine robots, including drone-based vine robots capable of cantilevered operations, everting grippers that grasp then transport objects into the robot body, and solid-state vine robots without the need for inflation.},
  archive   = {C_IROS},
  author    = {Rui Wu and Stefano Mintchev},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801415},
  month     = {10},
  pages     = {10535-10542},
  title     = {Vine robots that evert through bending},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ShapeGrasp: Zero-shot task-oriented grasping with large
language models through geometric decomposition. <em>IROS</em>,
10527–10534. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Task-oriented grasping of unfamiliar objects is a necessary skill for robots in dynamic in-home environments. Inspired by the human capability to grasp such objects through intuition about their shape and structure, we present a novel zero-shot task-oriented grasping method leveraging a geometric decomposition of the target object into simple, convex shapes that we represent in a graph structure, including geometric attributes and spatial relationships. Our approach employs minimal essential information – the object’s name and the intended task – to facilitate zero-shot task-oriented grasping. We utilize the commonsense reasoning capabilities of large language models to dynamically assign semantic meaning to each decomposed part and subsequently reason over the utility of each part for the intended task. Through extensive experiments on a real-world robotics platform, we demonstrate that our grasping approach’s decomposition and reasoning pipeline is capable of selecting the correct part in 92% of the cases and successfully grasping the object in 82% of the tasks we evaluate. Additional videos, experiments, code, and data are available on our project website: https://shapegrasp.github.io/.},
  archive   = {C_IROS},
  author    = {Samuel Li and Sarthak Bhagat and Joseph Campbell and Yaqi Xie and Woojun Kim and Katia Sycara and Simon Stepputtis},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801661},
  month     = {10},
  pages     = {10527-10534},
  title     = {ShapeGrasp: Zero-shot task-oriented grasping with large language models through geometric decomposition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). ToolEENet: Tool affordance 6D pose estimation.
<em>IROS</em>, 10519–10526. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The exploration of robotic dexterous hands utilizing tools has recently attracted considerable attention. A significant challenge in this field is the precise awareness of a tool’s pose when grasped, as occlusion by the hand often degrades the quality of the estimation. Additionally, the tool’s overall pose often fails to accurately represent the contact interaction, thereby limiting the effectiveness of vision-guided, contact-dependent activities. To overcome this limitation, we present the innovative TOOLEE dataset, which, to the best of our knowledge, is the first to feature affordance segmentation of a tool’s end-effector (EE) along with its defined 6D pose based on its usage. Furthermore, we propose the ToolEENet framework for accurate 6D pose estimation of the tool’s EE. This framework begins by segmenting the tool’s EE from raw RGB-D data, then uses a diffusion model-based pose estimator for 6D pose estimation at a category-specific level. Addressing the issue of symmetry in pose estimation, we introduce a symmetry-aware pose representation that enhances the consistency of pose estimation. Our approach excels in this field, demonstrating high levels of precision and generalization. Furthermore, it shows great promise for application in contact-based manipulation scenarios. All data and codes are available on the project website: https://tooleenet-iros2024.github.io/},
  archive   = {C_IROS},
  author    = {Yunlong Wang and Lei Zhang and Yuyang Tu and Hui Zhang and Kaixin Bai and Zhaopeng Chen and Jianwei Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801680},
  month     = {10},
  pages     = {10519-10526},
  title     = {ToolEENet: Tool affordance 6D pose estimation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Touch-GS: Visual-tactile supervised 3D gaussian splatting.
<em>IROS</em>, 10511–10518. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we propose a novel method to supervise 3D Gaussian Splatting (3DGS) scenes using optical tactile sensors. Optical tactile sensors have become widespread in their use in robotics for manipulation and object representation; however, raw optical tactile sensor data is unsuitable to directly supervise a 3DGS scene. Our representation leverages a Gaussian Process Implicit Surface to implicitly represent the object, combining many touches into a unified representation with uncertainty. We merge this model with a monocular depth estimation network, which is aligned in a two stage process, coarsely aligning with a depth camera and then finely adjusting to match our touch data. For every training image, our method produces a corresponding fused depth and uncertainty map. Utilizing this additional information, we propose a new loss function, variance-weighted depth supervised loss, for training the 3DGS scene model. We leverage the DenseTact optical tactile sensor and RealSense RGB-D camera to show that combining touch and vision in this manner leads to quantitatively and qualitatively better results than vision or touch alone in few-view scene synthesis on opaque, reflective and transparent objects. Please see our project page at armlabstanford.github.io/touchgs.},
  archive   = {C_IROS},
  author    = {Aiden Swann and Matthew Strong and Won Kyung Do and Gadiel Sznaier Camps and Mac Schwager and Monroe Kennedy},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802412},
  month     = {10},
  pages     = {10511-10518},
  title     = {Touch-GS: Visual-tactile supervised 3D gaussian splatting},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TřiVis: Versatile, reliable, and high-performance tool for
computing visibility in polygonal environments. <em>IROS</em>,
10503–10510. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visibility is a fundamental concept in computational geometry, with numerous applications in surveillance, robotics, and games. This software paper presents TřiVis, a C++ library developed by the authors for computing numerous visibility-related queries in highly complex polygonal environments. Adapting the triangular expansion algorithm, TřiVis stands out as a versatile, high-performance, more reliable and easy-to-use alternative to current solutions that is also free of heavy dependencies. Through evaluation on a challenging dataset, TřiVis has been benchmarked against existing visibility libraries. The results demonstrate that TřiVis outperforms the competing solutions by at least an order of magnitude in query times, while exhibiting more reliable runtime behavior. TřiVis is freely available for private, research, and institutional use at https://github.com/janmikulacz/trivis.},
  archive   = {C_IROS},
  author    = {Jan Mikula and Miroslav Kulich and Libor Přeučil},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801476},
  month     = {10},
  pages     = {10503-10510},
  title     = {TřiVis: Versatile, reliable, and high-performance tool for computing visibility in polygonal environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Text-to-drive: Diverse driving behavior synthesis via large
language models. <em>IROS</em>, 10495–10502. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generating varied scenarios through simulation is crucial for training and evaluating safety-critical systems, such as autonomous vehicles. Yet, the task of modeling the trajectories of other vehicles to simulate diverse and meaningful close interactions remains prohibitively costly. Adopting language descriptions to generate driving behaviors emerges as a promising strategy, offering a scalable and intuitive method for human operators to simulate a wide range of driving interactions. However, the scarcity of large-scale annotated language-trajectory data makes this approach challenging. To address this gap, we propose Text-to-Drive (T2D) to synthesize diverse driving behaviors via Large Language Models (LLMs). We introduce a knowledge-driven approach that operates in two stages. In the first stage, we employ the embedded knowledge of LLMs to generate diverse language descriptions of driving behaviors for a scene. Then, we leverage LLM’s reasoning capabilities to synthesize these behaviors in simulation. At its core, T2D employs an LLM to construct a state chart that maps low-level states to high-level abstractions. This strategy aids in downstream tasks such as summarizing low-level observations, assessing policy alignment with behavior description, and shaping the auxiliary reward, all without needing human supervision. With our knowledge-driven approach, we demonstrate that T2D generates more diverse trajectories compared to other baselines and offers a natural language interface that allows for interactive incorporation of human preference. Please check our website for more examples: here},
  archive   = {C_IROS},
  author    = {Phat Nguyen and Tsun-Hsuan Wang and Zhang-Wei Hong and Sertac Karaman and Daniela Rus},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801754},
  month     = {10},
  pages     = {10495-10502},
  title     = {Text-to-drive: Diverse driving behavior synthesis via large language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High rate mechanical coupling of interacting objects in the
context of needle insertion simulation with haptic feedback.
<em>IROS</em>, 10489–10494. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Needle-based procedures such as biopsies or radiofrequency ablation (RFA) of tumors are often considered to diagnose and treat liver cancer for their low invasiveness but raise difficulties for practitioners related to needle placement and visibility of internal anatomical structures. Efforts are being conducted to build real-time needle insertion simulators with both visual and haptic rendering, facing challenges related to model accuracy and real-time computational performance. This work focuses on the contact model involved in needle-tissue interactions in order to improve the realism of the resulting haptic rendering. We present a novel method to update the compliant coupling at high rates of a complete contact system involving the mechanics of a large object and the complete model of a flexible needle. These updates allow to adapt the contact directions to the needle deformations in the haptic thread, with the aim of improving the resulting haptic feedback. Updates of contact directions and the related mechanical system according to high-rate deformations decrease force feedback artifacts associated with low-rate mechanics while maintaining high-rate performances for the haptic loop.},
  archive   = {C_IROS},
  author    = {Claire Martin and Christian Duriez and Hadrien Courtecuisse},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801450},
  month     = {10},
  pages     = {10489-10494},
  title     = {High rate mechanical coupling of interacting objects in the context of needle insertion simulation with haptic feedback},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Solving dynamic cosserat rods with frictional contact using
the shooting method and implicit surfaces. <em>IROS</em>, 10483–10488.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a method for solving the strong form of the dynamic Cosserat rod clamped-free boundary value problem (BVP) under frictional contact. The shooting method is employed for BVP solving, and contact response is computed using the penalty method. The proposed method is tailored for accurate and rapid simulation of catheterization-like medical procedures. Our contact scenarios involve complex shapes and real patient geometry, modeled with implicit surfaces as blobby models. The results show that using blobby models for contact detection is orders of magnitude faster than using triangle meshes. This approach provides smoother contact forces, accelerating the convergence of the shooting method. We have made our code available as open source at https://gitlab.inria.fr/rjilani/IROS2024/.},
  archive   = {C_IROS},
  author    = {Radhouane Jilani and Pierre-Frédéric Villard and Erwan Kerrien},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801774},
  month     = {10},
  pages     = {10483-10488},
  title     = {Solving dynamic cosserat rods with frictional contact using the shooting method and implicit surfaces},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-fidelity SLAM using gaussian splatting with
rendering-guided densification and regularized optimization.
<em>IROS</em>, 10476–10482. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a dense RGBD SLAM system based on 3D Gaussian Splatting that provides metrically accurate pose tracking and visually realistic reconstruction. To this end, we first propose a Gaussian densification strategy based on the rendering loss to map unobserved areas and refine reobserved areas. Second, we introduce extra regularization parameters to alleviate the &quot;forgetting&quot; problem during contiunous mapping, where parameters tend to overfit the latest frame and result in decreasing rendering quality for previous frames. Both mapping and tracking are performed with Gaussian parameters by minimizing re-rendering loss in a differentiable way. Compared to recent neural and concurrently developed Gaussian splatting RGBD SLAM baselines, our method achieves state-of-the-art results on the synthetic dataset Replica and competitive results on the real-world dataset TUM. The code is released on https://github.com/ljjTYJR/HF-SLAM.},
  archive   = {C_IROS},
  author    = {Shuo Sun and Malcolm Mielle and Achim J. Lilienthal and Martin Magnusson},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802373},
  month     = {10},
  pages     = {10476-10482},
  title     = {High-fidelity SLAM using gaussian splatting with rendering-guided densification and regularized optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). HSS-SLAM: Human-in-the-loop semantic SLAM represented by
superquadrics. <em>IROS</em>, 10469–10475. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The advancement of object detection algorithms has catalyzed the development of object-level semantic SLAM. However, due to missed and false detections, object-level semantic SLAM fails to represent the objects within the scene adequately. Therefore, this paper proposes a novel object-level semantic SLAM termed HSS-SLAM. We incorporate human-in-the-loop into our method, establishing an interaction module to facilitate human editing and rectifying semantic information. Additionally, to minimize the manual correction workload, a lightweight and intuitive method for semantic extension is proposed, augmenting the semantic richness of the global map with a few operations. Furthermore, our method adopts superquadrics for object representation, enabling detailed descriptions of various object shapes. This mitigates the limitation of conventional semantic mapping, where objects are difficult to distinguish due to the reliance on a single-shape representation. Subsequently, precise estimation of superquadric parameters and camera poses is achieved through joint optimization. Extensive experiments conducted on TUM RGB-D and Scenes V2 datasets demonstrate that the proposed approach exhibits competitive performance, surpassing current methods in both object representation and camera localization accuracy.},
  archive   = {C_IROS},
  author    = {Yulong Li and Yunzhou Zhang and Bin Zhao and Zhiyao Zhang and You Shen and Tengda Zhang and Guolu Chen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801400},
  month     = {10},
  pages     = {10469-10475},
  title     = {HSS-SLAM: Human-in-the-loop semantic SLAM represented by superquadrics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Physically-based photometric bundle adjustment in
non-lambertian environments. <em>IROS</em>, 10461–10468. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Photometric bundle adjustment (PBA) is widely used in estimating the camera pose and 3D geometry by assuming a Lambertian world. However, the assumption of photometric consistency is often violated since the non-diffuse reflection is common in real-world environments. The photometric inconsistency significantly affects the reliability of existing PBA methods. To solve this problem, we propose a novel physically-based PBA method. Specifically, we introduce the physically-based weights regarding material, illumination, and light path. These weights distinguish the pixel pairs with different levels of photometric inconsistency. We also design corresponding models for material estimation based on sequential images and illumination estimation based on point clouds. In addition, we establish the first SLAM-related dataset of non-Lambertian scenes with complete ground truth of illumination and material. Extensive experiments demonstrated that our PBA method outperforms existing approaches in accuracy.},
  archive   = {C_IROS},
  author    = {Lei Cheng and Junpeng Hu and Haodong Yan and Mariia Gladkova and Tianyu Huang and Yun-Hui Liu and Daniel Cremers and Haoang Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802236},
  month     = {10},
  pages     = {10461-10468},
  title     = {Physically-based photometric bundle adjustment in non-lambertian environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). I2EKF-LO: A dual-iteration extended kalman filter based
LiDAR odometry. <em>IROS</em>, 10453–10460. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {LiDAR odometry is a pivotal technology in the fields of autonomous driving and autonomous mobile robotics. However, most of the current works focus on nonlinear optimization methods, and still existing many challenges in using the traditional Iterative Extended Kalman Filter (IEKF) framework to tackle the problem: IEKF only iterates over the observation equation, relying on a rough estimate of the initial state, which is insufficient to fully eliminate motion distortion in the input point cloud; the system process noise is difficult to be determined during state estimation of the complex motions; and the varying motion models across different sensor carriers. To address these issues, we propose the Dual-Iteration Extended Kalman Filter (I2EKF) and the LiDAR odometry based on I2EKF (I2EKF-LO). This approach not only iterates over the observation equation but also leverages state updates to iteratively mitigate motion distortion in LiDAR point clouds. Moreover, it dynamically adjusts process noise based on the confidence level of prior predictions during state estimation and establishes motion models for different sensor carriers to achieve accurate and efficient state estimation. Comprehensive experiments demonstrate that I2EKF-LO achieves outstanding levels of accuracy and computational efficiency in the realm of LiDAR odometry. Additionally, to foster community development, our code is open-sourced.1},
  archive   = {C_IROS},
  author    = {Wenlu Yu and Jie Xu and Chengwei Zhao and Lijun Zhao and Thien-Minh Nguyen and Shenghai Yuan and Mingming Bai and Lihua Xie},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801455},
  month     = {10},
  pages     = {10453-10460},
  title     = {I2EKF-LO: A dual-iteration extended kalman filter based LiDAR odometry},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel variable step-size path planning framework with
step-consistent markov decision process for large-scale UAV swarm.
<em>IROS</em>, 10447–10452. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, Deep Reinforcement Learning (DRL) has been a key approach to solving Unmanned Aerial Vehicle (UAV) swarm path planning problems. However, traditional DRL methods often face challenges in the initial learning stage and struggle to learn from variable step-size tasks. This paper introduces a novel training framework for large-scale UAV swarm variable step-size path planning: Rapidly-exploring Variable Step-size Deep Reinforcement Learning (RVSDRL). This framework involves common training on the ground local server and decentralized training on distributed UAVs. In the common training stage, we generate rapidly-exploring random graph samples to accelerate the common agent explore environment. In the decentralized training stages, we utilize the priority replay mechanism to improve efficiency. To enhance convergence stability, we restrict the returns of the equivalent paths and propose the Step-size Consistent Markov Decision Process (SCMDP) path planning model. Our method is compared with traditional methods, and the experiments demonstrate its superior performance in complex obstacle environments.},
  archive   = {C_IROS},
  author    = {Dan Xu and Yunxiao Guo and Han Long and Chang Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802011},
  month     = {10},
  pages     = {10447-10452},
  title     = {A novel variable step-size path planning framework with step-consistent markov decision process for large-scale UAV swarm},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explainable artificial intelligence for autonomous UAV
navigation. <em>IROS</em>, 10439–10446. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unmanned Aerial Vehicles (UAVs) with limited computational, perception and power resources face significant challenges when navigating autonomously in unfamiliar environments. While artificial intelligence (AI)-assisted algorithms have been used to address these limitations, transparency of the underlying AI models remains a concern, hindering user trust. To address this limitation, this research study proposes a novel, explainable AI-based navigation approach for UAVs to navigate them through unknown environments autonomously. The soft actor-critic (SAC) algorithm and multilayer perceptron (MLP) policies integrated deep reinforcement learning algorithm is developed to derive control actions. This controller is integrated with a novel moving-window gradient-based explainable artificial intelligence (XAI) framework to shed light on the UAV’s decision-making process. The proposed XAI algorithm provides granular insights into how various factors, such as image segments and UAV state features, influence the UAV’s actions. It lays the groundwork for a novel visual explanation approach that segments input depth images to highlight critical navigational cues, augmented by a dynamic color map for precise obstacle identification. Additionally, the study introduces comprehensive textual explanations to provide an in-depth understanding of the UAV’s decision processes, thereby improving the model’s transparency and explainability. The simulation results indicate that the proposed DRL model achieves over 95% success rate. Moreover, evaluations conducted in two distinct environments demonstrate the model’s capability to generate effective and reliable explanations.},
  archive   = {C_IROS},
  author    = {Didula Dissanayaka and Thumeera R. Wanasinghe and Raymond G. Gosine},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801529},
  month     = {10},
  pages     = {10439-10446},
  title     = {Explainable artificial intelligence for autonomous UAV navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Targeted image transformation for improving robustness in
long range aircraft detection. <em>IROS</em>, 10431–10438. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the field of aviation, the Detect and Avoid (DAA) problem deals with incorporating collision avoidance capabilities into current autopilot navigation systems. As an application of the Small Object Detection (SOD) problem, DAA presents the difficulties of a low signal-to-noise ratio and far range detection. Visual DAA is also susceptible to changing weather and lighting conditions at deployment. While current literature has presented many solutions for this, prior work has yet to study the robustness of the learning-based models for DAA. In this work, we show that standard techniques for improving robustness for object detection do not produce the desired results for DAA given the SOD constraints. We present targeted transformations, a zero-shot technique that can significantly improve robustness with minimal impact on accuracy. We demonstrate how to construct these transformations and evaluate our method on the current SOTA model for DAA, showing a 53.6% increase in recall. This makes our pipeline more robust to changes in lighting and environmental factors, and better able to detect potential threats. In the future, we hope to automate the transformation selection process, making it easier to adopt in different use cases.},
  archive   = {C_IROS},
  author    = {Rebecca Martin and Clement Fung and Nikhil Keetha and Lujo Bauer and Sebastian Scherer},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801304},
  month     = {10},
  pages     = {10431-10438},
  title     = {Targeted image transformation for improving robustness in long range aircraft detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Preserving relative localization of FoV-limited drone swarm
via active mutual observation. <em>IROS</em>, 10423–10430. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Relative state estimation is crucial for vision-based swarms to estimate and compensate for the unavoidable drift of visual odometry. For autonomous drones equipped with the most compact sensor setting — a stereo camera that provides a limited field of view (FoV), the demand for mutual observation for relative state estimation conflicts with the demand for environment observation. To balance the two demands for FoV-limited swarms by acquiring mutual observations with a safety guarantee, this paper proposes an active localization correction system, which plans camera orientations via a yaw planner during the flight. The yaw planner manages the contradiction by calculating suitable timing and yaw angle commands based on the evaluation of localization uncertainty estimated by the Kalman Filter. Simulation validates the scalability of our algorithm. In real-world experiments, we reduce positioning drift by up to 65% and managed to maintain a given formation in both indoor and outdoor GPS-denied flight, from which the accuracy, efficiency, and robustness of the proposed system are verified.},
  archive   = {C_IROS},
  author    = {Lianjie Guo and Zaitian Gongye and Ziyi Xu and Yingjian Wang and Xin Zhou and Jinni Zhou and Fei Gao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801634},
  month     = {10},
  pages     = {10423-10430},
  title     = {Preserving relative localization of FoV-limited drone swarm via active mutual observation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural control barrier functions for safe navigation.
<em>IROS</em>, 10415–10422. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous robot navigation can be particularly demanding, especially when the surrounding environment is not known and safety of the robot is crucial. This work relates to the synthesis of Control Barrier Functions (CBFs) through data for safe navigation in unknown environments. A novel methodology to jointly learn CBFs and corresponding safe controllers, in simulation, inspired by the State Dependent Riccati Equation (SDRE) is proposed. The CBF is used to obtain admissible commands from any nominal, possibly unsafe controller. An approach to apply the CBF inside a safety filter without the need for a consistent map or position estimate is developed. Subsequently, the resulting reactive safety filter is deployed on a multirotor platform integrating a LiDAR sensor both in simulation and real-world experiments.},
  archive   = {C_IROS},
  author    = {Marvin Harms and Mihir Kulkarni and Nikhil Khedekar and Martin Jacquet and Kostas Alexis},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802694},
  month     = {10},
  pages     = {10415-10422},
  title     = {Neural control barrier functions for safe navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantics from space: Satellite-guided thermal semantic
segmentation annotation for aerial field robots. <em>IROS</em>,
10407–10414. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a new method to automatically generate semantic segmentation annotations for thermal imagery captured from an aerial vehicle by utilizing satellite-derived data products alongside onboard global positioning and attitude estimates. This new capability overcomes the challenge of developing thermal semantic perception algorithms for field robots due to the lack of annotated thermal field datasets and the time and costs of manual annotation, enabling precise and rapid annotation of thermal data from field collection efforts at a massively-parallelizable scale. By incorporating a thermal-conditioned refinement step with visual foundation models, our approach can produce highly-precise semantic segmentation labels using low-resolution satellite land cover data for little-tono cost. It achieves 98.5% of the performance from using costly high-resolution options and demonstrates between 70-160% improvement over popular zero-shot semantic segmentation methods based on large vision-language models currently used for generating annotations for RGB imagery. Code will be available at: https://github.com/connorlee77/aerial-auto-segment.},
  archive   = {C_IROS},
  author    = {Connor Lee and Saraswati Soedarmadji and Matthew Anderson and Anthony J. Clark and Soon-Jo Chung},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801479},
  month     = {10},
  pages     = {10407-10414},
  title     = {Semantics from space: Satellite-guided thermal semantic segmentation annotation for aerial field robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active scout: Multi-target tracking using neural radiance
fields in dense urban environments. <em>IROS</em>, 10399–10406. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study pursuit-evasion games in highly occluded urban environments, e.g. tall buildings in a city, where a scout (quadrotor) tracks multiple dynamic targets on the ground. We show that we can build a neural radiance field (NeRF) representation of the city—online—using RGB and depth images from different vantage points. This representation is used to calculate the information gain to both explore unknown parts of the city and track the targets—thereby giving a completely first-principles approach to actively tracking dynamic targets. We demonstrate, using a custom-built simulator using Open Street Maps data of Philadelphia and New York City, that we can explore and locate 20 stationary targets within 300 steps. This is slower than a greedy baseline, which does not use active perception. But for dynamic targets that actively hide behind occlusions, we show that our approach maintains, at worst, a tracking error of 200m; the greedy baseline can have a tracking error as large as 600m. We observe a number of interesting properties in the scout’s policies, e.g., it switches its attention to track a different target periodically, as the quality of the NeRF representation improves over time, the scout also becomes better in terms of target tracking.},
  archive   = {C_IROS},
  author    = {Christopher D. Hsu and Pratik Chaudhari},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802565},
  month     = {10},
  pages     = {10399-10406},
  title     = {Active scout: Multi-target tracking using neural radiance fields in dense urban environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Photometric consistency for precise drone rephotography.
<em>IROS</em>, 10391–10398. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a precise drone rephotography system for fixed-domain patrolling scenarios. The proposed system integrates computer-vision-based localization and fine-tuned pixel-level dense flow prediction to achieve consistent and precise rephotography images with viewpoints that closely align with those of target images. The proposed Keypoints Alignment Through Dense Flow Prediction (KADFP) model effectively handles challenges arising from lighting variations and background differences. Moreover, a novel flight procedure is implemented in the proposed system. This procedure involves using an Interleaved Drone Controller to alternate between translation and rotation adjustments to ensure smooth flight dynamics during rephotography. Experiments indicated that the proposed system provided considerably more precise rephotography results (error of 4.72 pixels indoors) than did an existing localization approach (error of 35.56 pixels).},
  archive   = {C_IROS},
  author    = {Hsaun-Jui Chang and Tzu-Chun Huang and Hao-Liang Xu and Kuan-Wen Chen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801840},
  month     = {10},
  pages     = {10391-10398},
  title     = {Photometric consistency for precise drone rephotography},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quaternion-based sliding mode control for six degrees of
freedom flight control of quadrotors. <em>IROS</em>, 10385–10390. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite extensive research on sliding mode control (SMC) design for quadrotors, the existing approaches have certain limitations. Euler angle-based SMC formulations suffer from poor performance in high-pitch or -roll maneuvers. Quaternion-based SMC approaches have unwinding issues and complex architecture. Coordinate-free methods are slow and only almost globally stable. This paper presents a new six degrees of freedom SMC flight controller to address the above limitations. We use a cascaded architecture with a position controller in the outer loop and a quaternion-based attitude controller in the inner loop. The position controller generates the desired trajectory for the attitude controller using a coordinate-free approach. The quaternion-based attitude controller uses the natural characteristics of the quaternion hypersphere, featuring a simple structure while providing global stability and avoiding unwinding issues. We compare our controller with three other common control methods conducting challenging maneuvers like flip-over and high-speed trajectory tracking in the presence of model uncertainties and disturbances. Our controller consistently outperforms the benchmark approaches with less control effort and actuator saturation, offering highly effective and efficient flight control.},
  archive   = {C_IROS},
  author    = {Amin Yazdanshenas and Reza Faieghi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801815},
  month     = {10},
  pages     = {10385-10390},
  title     = {Quaternion-based sliding mode control for six degrees of freedom flight control of quadrotors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Tracking control with uncertainty smoothing estimation
under aggressive maneuvers of aerial vehicles. <em>IROS</em>,
10378–10384. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Aggressive maneuvering is crucial for aerial vehicles to execute adversarial and penetration missions. However, this challenges the accurate tracking control of drones due to uncertainties induced by high-speed flight. Therefore, firstly, a highly dynamic tracking control framework is proposed to actualize the accurate tracking of aggressive trajectories with velocities up to 15 m/s (i.e., 54 km/h) and acceleration of 2 g. Secondly, in order to mitigate the impact of conjoint effects on uncertainty estimation during aggressive flights and to ensure that uncertainty is smoothly compensated, a novel adaptive nonlinear extended state observer (ANESO) with noise suppression and peak attenuation capabilities is designed. Finally, extensive comparative simulation and real-world practical experimental results certify the superiority of the proposed control strategy in tracking aggressive trajectories.},
  archive   = {C_IROS},
  author    = {Hao Zhang and Tao Jiang and Jianchuan Ye and Senqi Tan and Zhi Zheng},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802393},
  month     = {10},
  pages     = {10378-10384},
  title     = {Tracking control with uncertainty smoothing estimation under aggressive maneuvers of aerial vehicles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning to walk and fly with adversarial motion priors.
<em>IROS</em>, 10370–10377. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot multimodal locomotion encompasses the ability to transition between walking and flying, representing a significant challenge in robotics. This work presents an approach that enables automatic smooth transitions between legged and aerial locomotion. Leveraging the concept of Adversarial Motion Priors, our method allows the robot to imitate motion datasets and accomplish the desired task without the need for complex reward functions. The robot learns walking patterns from human-like gaits and aerial locomotion patterns from motions obtained using trajectory optimization. Through this process, the robot adapts the locomotion scheme based on environmental feedback using reinforcement learning, with the spontaneous emergence of mode-switching behavior. The results highlight the potential for achieving multimodal locomotion in aerial humanoid robotics through automatic control of walking and flying modes, paving the way for applications in diverse domains such as search and rescue, surveillance, and exploration missions. This research contributes to advancing the capabilities of aerial humanoid robots in terms of versatile locomotion in various environments. Video: https://youtu.be/mi6Do-x67CM},
  archive   = {C_IROS},
  author    = {Giuseppe L’Erario and Drew Hanover and Ángel Romero and Yunlong Song and Gabriele Nava and Paolo Maria Viceconte and Daniele Pucci and Davide Scaramuzza},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802391},
  month     = {10},
  pages     = {10370-10377},
  title     = {Learning to walk and fly with adversarial motion priors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Energy-optimal planning of waypoint-based UAV missions -
does minimum distance mean minimum energy? <em>IROS</em>, 10362–10369.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multirotor unmanned aerial vehicle is a prevailing type of aerial robots with wide real-world applications. The energy efficiency of the robot is a critical aspect of its performance, determining the range and duration of missions that can be performed. This paper studies the energy-optimal planning of the multirotor, which aims at finding the optimal ordering of waypoints with the minimum energy consumption for missions in 3D space. The study is performed based on a previously developed model capturing first-principle energy dynamics of the multirotor. We found that in majority of the cases (up to 95%) the solutions of the energy-optimal planning are different from those of the traditional traveling salesman problem which minimizes the total distance. The difference can be as high as 14.9%, with the average at 1.6%-3.3% and 90th percentile at 3.7%-6.5% depending on the range and number of waypoints in the mission. We then identified and explained the key features of the minimum-energy order by correlating to the underlying flight energy dynamics. It is shown that instead of minimizing the distance, coordination of vertical and horizontal motion to promote aerodynamic efficiency is the key to optimizing energy consumption.},
  archive   = {C_IROS},
  author    = {Nicolas Michel and Ayush Patnaik and Zhaodan Kong and Xinfan Lin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801888},
  month     = {10},
  pages     = {10362-10369},
  title     = {Energy-optimal planning of waypoint-based UAV missions - does minimum distance mean minimum energy?},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AirCrab: A hybrid aerial-ground manipulator with an active
wheel. <em>IROS</em>, 10356–10361. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inspired by the behavior of birds, we present AirCrab, a hybrid aerial ground manipulator (HAGM) with a single active wheel and a 3-degree of freedom (3-DoF) manipulator. AirCrab leverages a single point of contact with the ground to reduce position drift and improve manipulation accuracy. The single active wheel enables locomotion on narrow surfaces without adding significant weight to the robot. To realize accurate attitude maintenance using propellers on the ground, we design a control allocation method for AirCrab that prioritizes attitude control and dynamically adjusts the thrust input to reduce energy consumption. Experiments verify the effectiveness of the proposed control method and the gain in manipulation accuracy with ground contact. A series of operations to complete the letters ‘NTU’ demonstrates the capability of the robot to perform challenging hybrid aerial-ground manipulation missions.},
  archive   = {C_IROS},
  author    = {Muqing Cao and Jiayan Zhao and Xinhang Xu and Lihua Xie},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802472},
  month     = {10},
  pages     = {10356-10361},
  title     = {AirCrab: A hybrid aerial-ground manipulator with an active wheel},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Control of unknown quadrotors from a single throw.
<em>IROS</em>, 10350–10355. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a method to recover quadrotor Unmanned Air Vehicles (UAVs) from a throw, when no control parameters are known before the throw. We leverage the availability of high-frequency rotor speed feedback available in racing drone hardware and software to find control effectiveness values and fit a motor model using recursive least squares (RLS) estimation. Furthermore, we propose an excitation sequence that provides large actuation commands while guaranteeing to stay within gyroscope sensing limits. After 450ms of excitation, an Incremental Nonlinear Dynamic Inversion (INDI) attitude controller uses the 52 fitted parameters to arrest rotational motion and recover an upright attitude. Finally, a Nonlinear Dynamic Inversion (NDI) position controller drives the craft to a position setpoint. The proposed algorithm runs efficiently on microcontrollers found in common UAV flight controllers, and was shown to recover an agile quadrotor every time in live experiments with as low as 3.5m throw height, demonstrating robustness against initial rotations and noise. We also demonstrate control of randomized quadrotors in simulated throws, where the parameter fitting Root-Mean-Square (RMS) error is typically within 10% of the true value.},
  archive   = {C_IROS},
  author    = {Till M. Blaha and Ewoud J. J. Smeur and Bart D. W. Remes},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801514},
  month     = {10},
  pages     = {10350-10355},
  title     = {Control of unknown quadrotors from a single throw},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A ducted fan UAV for safe aerial grabbing and transfer of
multiple loads using electromagnets. <em>IROS</em>, 10342–10349. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, research on aerial grasping, manipulation, and transportation of objects has garnered significant attention. These tasks often require UAVs to operate safely close to environments or objects and to efficiently grasp payloads. However, current widely adopted flying platforms pose safety hazards: unprotected high-speed rotating propellers can cause harm to the surroundings. Additionally, the space for carrying payloads on the fuselage is limited, and the restricted position of the payload also hinders efficient grasping. To address these issues, this paper presents a coaxial ducted fan UAV which is equipped with electromagnets mounted externally on the fuselage, enabling safe grasping and transfer of multiple loads in midair without complex additional actuators. It also has the capability to achieve direct human-UAV cargo transfer in the air. The forces acting on the loads during magnetic attachment and their influencing factors were analyzed. An ADRC controller is utilized to counteract disturbances during grasping and achieve attitude control. Finally, flight tests are conducted to verify the UAV’s ability to directly grasp multiple loads from human hands in flight while maintaining attitude tracking.},
  archive   = {C_IROS},
  author    = {Zhong Yin and Hailong Pei},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802207},
  month     = {10},
  pages     = {10342-10349},
  title     = {A ducted fan UAV for safe aerial grabbing and transfer of multiple loads using electromagnets},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Biodegradable gliding paper flyers fabricated through inkjet
printing. <em>IROS</em>, 10334–10341. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Seed-inspired minimalist microflyers are showing potential as dispersal platforms for sensor networks and seeds. Their function relies on the sheer number of low-cost flyers, inevitably raising concerns about the post-operation environmental impact. We propose a biodegradable paper glider platform fabricated through origami inkjet printing. This method can fold origami along printing patterns on different paper varieties. We created a printing pattern that allows 2D paper sheets to self-fold into a 3D flying wing glider with designable wing geometry and center of gravity (CG). The design allows stable and repeatable gliding behavior, proven in our gliding tests. It successfully achieves the dispersal of multiple gliders from a hovering drone, covering an average horizontal distance larger than the release height. We also tested the biodegradation of different paper types compatible with the printing method, showing near-complete degradation after 15 weeks in moist soil. Our study presents a novel, potentially scalable approach for fabricating environmentally friendly microflyers, offering new avenues for remote environmental sensing and automated forest restoration programs.},
  archive   = {C_IROS},
  author    = {Luca Girardi and Rui Wu and Yuki Fukatsu and Hiroki Shigemune and Stefano Mintchev},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801580},
  month     = {10},
  pages     = {10334-10341},
  title     = {Biodegradable gliding paper flyers fabricated through inkjet printing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PEERNet: An end-to-end profiling tool for real-time
networked robotic systems. <em>IROS</em>, 10326–10333. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Networked robotic systems balance compute, power, and latency constraints in applications such as self-driving vehicles, drone swarms, and teleoperated surgery. A core problem in this domain is deciding when to offload a computationally expensive task to the cloud, a remote server, at the cost of communication latency. Task offloading algorithms often rely on precise knowledge of system-specific performance metrics, such as sensor data rates, network bandwidth, and machine learning model latency. While these metrics can be modeled during system design, uncertainties in connection quality, server load, and hardware conditions introduce real-time performance variations, hindering overall performance. We introduce PEERNet, an end-to-end and real-time profiling tool for cloud robotics. PEERNet enables performance monitoring on heterogeneous hardware through targeted yet adaptive profiling of system components such as sensors, networks, deep-learning pipelines, and devices. We showcase PEERNet’s capabilities through networked robotics tasks, such as image-based teleoperation of a Franka Emika Panda arm and querying vision language models using an Nvidia Jetson Orin. PEERNet reveals non-intuitive behavior in robotic systems, such as asymmetric network transmission and bimodal language model output. Our evaluation underscores the effectiveness and importance of benchmarking in networked robotics, demonstrating PEERNet’s adaptability. Our code is open-source and available at github.com/UTAustin-SwarmLab/PEERNet.},
  archive   = {C_IROS},
  author    = {Aditya Narayanan and Pranav Kasibhatla and Minkyu Choi and Po-Han Li and Ruihan Zhao and Sandeep Chinchali},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801747},
  month     = {10},
  pages     = {10326-10333},
  title     = {PEERNet: An end-to-end profiling tool for real-time networked robotic systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Extending task and motion planning with feasibility
prediction: Towards multi-robot manipulation planning of realistic
objects. <em>IROS</em>, 10318–10325. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The hybrid discrete/continuous nature of task and motion planning (TAMP) results often in a combinatorial explosion. This challenge is even more pronounced in multi-robot TAMP problems due to the increase in dimensionality of the action space. Previous works use action feasibility prediction as a heuristic to accelerate TAMP. However, these methods are limited to box-shaped objects and specific single or dual robot settings. In this paper, we propose a feasibility-enabled multi-robot TAMP algorithm capable of tackling complex multi-robot manipulation problems. Also, we expand on our previous work on action and grasp feasibility prediction [1] by extending its use to mesh-shaped objects. We demonstrate the performance of our method compared to a non feasibility-informed baseline, and show its ability to handle TAMP problems requiring the collaboration of multiple robots.},
  archive   = {C_IROS},
  author    = {Smail Ait Bouhsain and Rachid Alami and Thierry Siméon},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802307},
  month     = {10},
  pages     = {10318-10325},
  title     = {Extending task and motion planning with feasibility prediction: Towards multi-robot manipulation planning of realistic objects},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leader-follower cooperative manipulation under
spatio-temporal constraints. <em>IROS</em>, 10312–10317. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we develop a control algorithm for mobile manipulators manipulating an object within a leader-follower framework. Unlike existing literature, we avoid the knowledge of the object’s dynamics, and only the leader is aware of the tasks to be executed by the object. The followers are primarily tasked to lift the object and maintain a desired posture while the leader manipulates the object despite its unknown dynamic parameters. We employ a stiffness-based controller for the followers, allowing set-point stabilisation with permissible flexibility and a high-gain prescribed performance controller for the leader to facilitate manipulation from the object&#39;s equilibrium state. We present simulation results with two followers and one leader KUKA youbots to validate our proposed framework.},
  archive   = {C_IROS},
  author    = {Mayank Sewlia and Christos K. Verginis and Dimos V. Dimarogonas},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802449},
  month     = {10},
  pages     = {10312-10317},
  title     = {Leader-follower cooperative manipulation under spatio-temporal constraints},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalability of platoon-based coordination for mixed autonomy
intersections. <em>IROS</em>, 10304–10311. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As transportation systems see gradual deployment of connected and automated vehicles (CAVs), there is increasing opportunity for intelligent coordination of CAVs towards system-wide objectives. While numerous previous works have modeled single junctions (e.g. intersections and merges) and investigated control theory-based strategies for vehicle-based coordination, this work investigates the scalability of vehicular control approaches to large networks of intersections, where interactions among multiple intersections may amplify traffic disturbances. Moreover, this work focuses on mixed autonomy networks where the highly nonlinear behavior of human-driven vehicles (HDVs) complicate overall system dynamics, and where the formation of CAV-led platoons may be advantageous. Two approaches are considered for the studied settings: model predictive control (MPC) and model-free reinforcement learning (RL), both adapted from previous methods designed for single intersection and/or full autonomy settings. Results in a network of two intersections demonstrate that MPC faces significant challenges in low-level nonlinear trajectory optimization as well as high-level crossing scheduling, while the RL policies implicitly optimizes for both low-level control and high-level coordination. Scalability analysis in large networks with hundred of intersections reveal that policies derived from additional finetuning only suffer mild degradation in performance despite the numerous out-of-distribution traffic conditions that may emerge under large scale.},
  archive   = {C_IROS},
  author    = {Zhongxia Yan and Cathy Wu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801701},
  month     = {10},
  pages     = {10304-10311},
  title     = {Scalability of platoon-based coordination for mixed autonomy intersections},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Consistent distributed cooperative localization: A
coordinate transformation approach. <em>IROS</em>, 10297–10303. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the consistency issue of multi-robot distributed cooperative localization. We introduce a consistent distributed cooperative localization algorithm conducting state estimation in a transformed coordinate. The core idea involves a linear time-varying coordinated transformation to render the propagation Jacobian independent of the state and make it suitable for a distributed manner. This transformation is seamlessly integrated into a server-based distributed cooperative localization framework, in which each robot estimates its own state while the server maintains the cross-correlations. The transformation ensures the correct observability property of the entire framework. Moreover, the algorithm accommodates various types of robot-to-robot relative measurements, broadening its applicability. Through simulations and real-world dataset experiments, the proposed algorithm has demonstrated better performance in terms of both consistency and accuracy compared to existing algorithms.},
  archive   = {C_IROS},
  author    = {Chungeng Tian and Ning Hao and Fenghua He and Haodi Yao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802078},
  month     = {10},
  pages     = {10297-10303},
  title     = {Consistent distributed cooperative localization: A coordinate transformation approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decentralized multi-robot navigation coupled with
spatial-temporal RetNet based on deep reinforcement learning.
<em>IROS</em>, 10289–10296. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Navigating robots through dynamic multi-robot environments, avoiding collisions with both other robots and obstacles, has emerged as a central challenge in robotics. The existing approaches fall short in allowing the policy network to effectively capture spatial-temporal reciprocal collision avoidance in multi-robot environments, comprising both static and dynamic obstacles, resulting in inadequate safety and efficiency in directing robot movement. In this study, we introduce a novel policy neural network called Spatial-Temporal RetNet (STR), designed to encode reciprocal collision avoidance states between robots in spatial and temporal dimensions. The goal is to improve the safety and efficacy of the policy neural network in directing robots to complete assigned tasks. The spatial state encoder module is built upon a parallel RetNet structure, which strengthens the neural network&#39;s capacity in extracting reciprocal collision avoidance states between robots in spatial dimensions. This module addresses the limitations of position encoding in transformer-based multi-robot navigation policy neural networks. We design a temporal state encoder utilizing a recurrent RetNet structure. This innovation bolsters the multi-robot navigation policy neural network&#39;s capability to capture features in the temporal dimension of multi-robot movements. It addresses the limitations of transformer-based multi-robot navigation policy neural networks, particularly in recurrently inferring information across time dimensions. Simulation experiments were conducted to showcase the superior safety and effectiveness of our proposed method compared to previous state-of-the-art approaches in guiding robots to accomplish tasks.},
  archive   = {C_IROS},
  author    = {Lin Chen and Yaonan Wang and Zhiqiang Miao and Mingtao Feng and Yuanzhe Wang and Yang Mo and Zhen Zhou and Hesheng Wang and Danwei Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802823},
  month     = {10},
  pages     = {10289-10296},
  title     = {Decentralized multi-robot navigation coupled with spatial-temporal RetNet based on deep reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatiotemporal co-design enabling prioritized multi-agent
motion planning. <em>IROS</em>, 10281–10288. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces an innovative planner for prioritized multi-agent motion planning, employing a sequential integration of spatial and temporal designs. The planner initiates a smooth trajectory in space for each agent, ignoring the presence of other agents. Subsequently, by treating spatial collisions as 2D obstacles from a temporal perspective, the planner dynamically fine-tunes the trajectory-tracking speed of agents to avoid collisions, ensuring optimal time consumption for the last agent to reach the target as well. Additionally, the proposed approach systematically coordinates priority for each agent. The efficacy of the approach is validated through both simulations and comparative experiments with a recent planner.},
  archive   = {C_IROS},
  author    = {Yunshen Huang and Wenbo He and Yiannis Kantaros and Shen Zeng},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801559},
  month     = {10},
  pages     = {10281-10288},
  title     = {Spatiotemporal co-design enabling prioritized multi-agent motion planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cooperative path planning for four-way shuttle vehicles in
storage and retrieval systems: A hierarchically dynamic graph-based
approach. <em>IROS</em>, 10275–10280. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, Shuttle-based Storage and Retrieval Systems (SBS/RSs) have garnered significant attention from both academia and industry, owing to their high spatial utilization and rapid response speed. However, the weak connectivity of roadmaps in densely stored environments increases the likelihood of congestion and deadlocks when multiple four-way shuttles operate simultaneously, thereby imposing greater demands for collaborative path planning. Instead of exhaustively coordinating the shuttle motions during the off-line planning or online local control stages, we solve the cooperative path planning challenge from the perspective of altering the road graph structure dynamically. More specifically, we propose an approach to automatically transfer the typical road graph of SBS/RSs into a hierarchical graph with a reduced size, and then dynamically adjust its edge properties to prohibit any motion conflicts. In this manner, the planning problem of large-scale shuttle groups can be easily resolved and all the potential congestions can be eliminated inherently. Finally, we build a complete multi-shuttle cooperative path planning system adaptable for large-scale problems.},
  archive   = {C_IROS},
  author    = {Xingyao Han and Yuhong Tan and Siyuan Chen and Zhe Liu and Hesheng Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801443},
  month     = {10},
  pages     = {10275-10280},
  title     = {Cooperative path planning for four-way shuttle vehicles in storage and retrieval systems: A hierarchically dynamic graph-based approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Highly efficient observation process based on FFT filtering
for robot swarm collaborative navigation in unknown environments*.
<em>IROS</em>, 10267–10274. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collaborative path planning for robot swarms in complex, unknown environments without external positioning is a challenging problem. This requires robots to find safe directions based on real-time environmental observations, and to efficiently transfer and fuse these observations within the swarm. This study presents a filtering method based on Fast Fourier Transform (FFT) to address these two issues. We treat sensors’ environmental observations as a digital sampling process. Then, we design two different types of filters for safe direction extraction, as well as for the compression and reconstruction of environmental data. The reconstructed data is mapped to probabilistic domain, achieving efficient fusion of swarm observations and planning decision. The computation time is only on the order of microseconds, and the transmission data in communication systems is in bit-level. The performance of our algorithm in sensor data processing was validated in real world experiments, and the effectiveness in swarm path optimization was demonstrated through extensive simulations.},
  archive   = {C_IROS},
  author    = {Chenxi Li and Weining Lu and Zhihao Ma and Litong Meng and Bin Liang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801770},
  month     = {10},
  pages     = {10267-10274},
  title     = {Highly efficient observation process based on FFT filtering for robot swarm collaborative navigation in unknown environments*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). K-robust conflict-based search with continuous time for
multi-robot coordination. <em>IROS</em>, 10260–10266. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Coordinating multiple robots is crucial for various real-life applications. Many Multi-Agent Path Finding (MAPF) algorithms have been proven to be successful in addressing this challenge. Nevertheless, some problems, such as unexpected delays during navigation, commonly arise when handling highlevel abstractions, potentially leading to failures or collisions in live executions. This paper proposes k-Robust Continuoustime Conflict-Based Search (kR-CCBS), a novel algorithm that overcomes some of these limitations. Our approach offers path planning with continuous time, leading to more precise routes than discrete time approaches. Additionally, we increase safety by incorporating k-robustness, enabling the system to adapt to agent failures due to delays and minimize collision risks. Comparative evaluations demonstrate that kR-CCBS outperforms similar works in effectiveness while maintaining reasonable costs, making it a promising solution for real-world multi-agent coordination scenarios.},
  archive   = {C_IROS},
  author    = {Guilherme Daudt and Alleff Dymytry and Mariana Kolberg and Renan Maffei},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801503},
  month     = {10},
  pages     = {10260-10266},
  title     = {K-robust conflict-based search with continuous time for multi-robot coordination},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-agent vulcan: An information-driven multi-agent path
finding approach. <em>IROS</em>, 10253–10259. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Scientists often search for phenomenon of interest while exploring new environments. Autonomous vehicles are deployed to explore such areas where human-operated vehicles would be costly or dangerous. Online control of autonomous vehicles for information-gathering is called adaptive sampling and can be framed as a Partially Observable Markov Decision Process (POMDPs) that uses information gain as its principal objective. While prior work focuses largely on single-agent scenarios, this paper confronts challenges unique to multi-agent adaptive sampling, such as avoiding redundant observations, preventing vehicle collision, and facilitating path planning under limited communication. We start with Multi-Agent Path Finding (MAPF) methods, which address collision avoidance by decomposing the multi-agent path planning problem into a series of single-agent path planning problems. We present an extension to these methods called information-driven MAPF which addresses multi-agent information gain under limited communication. First, we introduce an admissible heuristic that relaxes mutual information gain to an additive function that can be evaluated as a set of independent single agent path planning problems. Second, we extend our approach to a distributed system that is robust to limited communication. When all agents are in range, the group plans jointly to maximize information. When some agents move out of range, communicating subgroups are formed and the subgroups plan independently. Since redundant observations are less likely when vehicles are far apart, this approach only incurs a small loss in information gain, resulting in an approach that gracefully transitions from full to partial communication. We evaluate our method against other adaptive sampling strategies across various scenarios, including real-world robotic applications. Our method was able to locate up to 200% more unique phenomena in certain scenarios, and each agent located its first unique phenomenon faster by up to 50%.},
  archive   = {C_IROS},
  author    = {Jake Olkin and Viraj Parimi and Brian Williams},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801571},
  month     = {10},
  pages     = {10253-10259},
  title     = {Multi-agent vulcan: An information-driven multi-agent path finding approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IMTSP: Solving min-max multiple traveling salesman problem
with imperative learning. <em>IROS</em>, 10245–10252. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper considers a Min-Max Multiple Traveling Salesman Problem (MTSP), where the goal is to find a set of tours, one for each agent, to collectively visit all the cities while minimizing the length of the longest tour. Though MTSP has been widely studied, obtaining near-optimal solutions for large-scale problems is still challenging due to its NP-hardness. Recent efforts in data-driven methods face challenges of the need for hard-to-obtain supervision and issues with high variance in gradient estimations, leading to slow convergence and highly sub-optimal solutions. We address these issues by reformulating MTSP as a bilevel optimization problem, using the concept of imperative learning (IL). This involves introducing an allocation network that decomposes the MTSP into multiple single-agent traveling salesman problems (TSPs). The longest tour from these TSP solutions is then used to self-supervise the allocation network, resulting in a new self-supervised, bilevel, end-to-end learning framework, which we refer to as imperative MTSP (iMTSP). Additionally, to tackle the high-variance gradient issues during the optimization, we introduce a control variate-based gradient estimation algorithm. Our experiments showed that these innovative designs enable our gradient estimator to converge 20× faster than the advanced reinforcement learning baseline, and find up to 80% shorter tour length compared with Google OR-Tools MTSP solver, especially in large-scale problems (e.g. 1000 cities and 15 agents).},
  archive   = {C_IROS},
  author    = {Yifan Guo and Zhongqiang Ren and Chen Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802039},
  month     = {10},
  pages     = {10245-10252},
  title     = {IMTSP: Solving min-max multiple traveling salesman problem with imperative learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A non-homogeneity mapless navigation based on hierarchical
safe reinforcement learning in dynamic complex environments.
<em>IROS</em>, 10237–10244. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Addressing safe and efficient navigation in dynamic, realistic, and complex environments stands as a pivotal inquiry within the realm of robotics. Recently, numerous learning-based methods are introduced into the field of navigation, yielding notable outcomes. In this letter, we propose a hierarchical safe reinforcement learning navigation approach (HSRLN) for mapless navigation. It trains mapless navigation policies for non-homogeneous complex scenarios in a hierarchical manner through a kind of three-stage learning, global planning reinforcement learning (RL) + expert imitation learning (IL) + transfer RL (TRL). The innovations of this work are fourfold: a) It effectively reduces the difficulty of training for complex navigation by effectively narrowing the task horizon of RL through a hierarchical framework. b) We designed an imitation learning method based on Relative Driving Safety Index (RDSI) [1] to focus on learning critical expert actions. c) It employs a TRL approach to improve generalization under non-homogeneity assumptions by fine-tuning the policy. d) HSRLN extracts significant features important for navigation decisions from raw observations via velocity obstacle modeling. Experiments indicate that it has performs better than existing hierarchical RL navigation methods (HDRL [2], SRL-ORCA [3]). Relative to SRL-ORCA, it improves navigation success by 12.1% under the non-homogeneity assumption. Videos are available at https://youtu.be/24h9JmcIfMw.},
  archive   = {C_IROS},
  author    = {Jianmin Qin and Qingchen Liu and Qichao Ma and Zipeng Wu and Jiahu Qin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802193},
  month     = {10},
  pages     = {10237-10244},
  title     = {A non-homogeneity mapless navigation based on hierarchical safe reinforcement learning in dynamic complex environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-robot active graph exploration with reduced pose-SLAM
uncertainty via submodular optimization. <em>IROS</em>, 10229–10236. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper considers the multi-robot active graph exploration problem, where robots need to collaboratively cover a graph environment while maintaining reliable pose estimation in collaborative Simultaneous Localization and Mapping (SLAM). Considering both objectives presents challenges for multi-robot pathfinding, as it involves the expensive covariance propagation for SLAM uncertainty evaluation, especially when considering various combinations of robots’ paths. To reduce the computational complexity, we propose an efficient two-stage strategy where exploration paths are first generated for quick coverage, and then enhanced by adding informative loop-closing actions along the paths for reliable pose estimation. We formulate the latter problem as a non-monotone submodular maximization problem by relating SLAM uncertainty with pose graph topology, which (1) facilitates a more efficient evaluation of SLAM uncertainty than covariance inference, and (2) allows the employment of approximation algorithms in submodular optimization to provide suboptimality guarantees. We further introduce ordering heuristics to improve the objective values while preserving the optimality bound. Simulation experiments over randomly generated graph environments verify the effectiveness of our methods to achieve quick coverage and enhanced pose graph reliability, and benchmark the performance of the approximation algorithms and the greedy-based algorithm in the loop edge selection problem. Our implementations will be open-source at https://github.com/bairuofei/CGE.},
  archive   = {C_IROS},
  author    = {Ruofei Bai and Shenghai Yuan and Hongliang Guo and Pengyu Yin and Wei-Yun Yau and Lihua Xie},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802691},
  month     = {10},
  pages     = {10229-10236},
  title     = {Multi-robot active graph exploration with reduced pose-SLAM uncertainty via submodular optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SwarmPRM: Probabilistic roadmap motion planning for
large-scale swarm robotic systems. <em>IROS</em>, 10222–10228. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large-scale swarm robotic systems consisting of numerous cooperative agents show considerable promise for performing autonomous tasks across various sectors. Nonetheless, traditional motion planning approaches often face a trade-off between scalability and solution quality due to the exponential growth of the joint state space of robots. In response, this work proposes SwarmPRM, a hierarchical, scalable, computationally efficient, and risk-aware sampling-based motion planning approach for large-scale swarm robots. SwarmPRM utilizes a Gaussian Mixture Model (GMM) to represent the swarm’s macroscopic state and constructs a Probabilistic Roadmap in Gaussian space, referred to as the Gaussian roadmap, to generate a transport trajectory of GMM. This trajectory is then followed by each robot at the microscopic stage. To enhance trajectory safety, SwarmPRM incorporates the conditional value-at-risk (CVaR) in the collision checking process to impart the property of risk awareness to the constructed Gaussian roadmap. SwarmPRM then crafts a linear programming formulation to compute the optimal GMM transport trajectory within this roadmap. Extensive simulations demonstrate that SwarmPRM outperforms state-of-the-art methods in computational efficiency, scalability, and trajectory quality while offering the capability to adjust the risk tolerance of generated trajectories.},
  archive   = {C_IROS},
  author    = {Yunze Hu and Xuru Yang and Kangjie Zhou and Qinghang Liu and Kang Ding and Han Gao and Pingping Zhu and Chang Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801905},
  month     = {10},
  pages     = {10222-10228},
  title     = {SwarmPRM: Probabilistic roadmap motion planning for large-scale swarm robotic systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online planning for multi agent path finding in inaccurate
maps. <em>IROS</em>, 10214–10221. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In multi-agent path finding (MAPF), agents navigate to their target positions without conflict within an environment, typically represented as a graph. Traditionally, the input graph is assumed to be accurate. We investigate MAPF scenarios where the input graph may be inaccurate, containing non-existent edges or missing edges present in the environment. Agents can verify the existence or non-existence of an edge only by moving close to it. To navigate such maps, we propose an online approach where planning and execution are interleaved. As agents gather new information about the environment over time, they replan accordingly. To minimize replanning efforts, we developed methods to identify and replan only for agents affected by observed changes. To scale to larger problems, we defer conflicts resolution expected only in the distant future and adapt single-agent path-finding algorithms to account for map inaccuracies. Experimental results show impressive scalability, solving problems involving over 1000 agents in under 3 minutes.},
  archive   = {C_IROS},
  author    = {Nir Malka and Guy Shani and Roni Stern},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801975},
  month     = {10},
  pages     = {10214-10221},
  title     = {Online planning for multi agent path finding in inaccurate maps},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SwiftBase: A ddataset based on high-frequency visual
measurement for visual-inertial localization in high-speed motion
scenes. <em>IROS</em>, 10206–10213. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Localizing an aggressively moving platform is a considerable challenge in the SLAM domain. This paper presents a dataset, SwiftBase, crafted to facilitate research into precise localization under such conditions. It includes high-speed cameras with over 200Hz sampling rate, capturing detailed visual data for analyzing rapid external dynamics. The dataset features two IDS high-speed cameras, a low-frequency camera, and a high-precision integrated inertial measurement unit (IMU). Calibration parameters are provided, and sensor data is synchronized using ROS system time. SwiftBase is recorded in indoor environments, utilizing pulleys and suspension ropes to simulate high-speed conditions, with ground truth data supplied by OptiTrack. SwiftBase has been instrumental in evaluating advanced VI-SLAM algorithms. However, there is still an urgent need for new algorithms capable of robust and real-time tracking in High-Speed localization.1},
  archive   = {C_IROS},
  author    = {Zhenghao Zou and Yang Lyu and Chunhui Zhao and Xirui Kao and Jiangbo Liu and Haochen Chai},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802203},
  month     = {10},
  pages     = {10206-10213},
  title     = {SwiftBase: A ddataset based on high-frequency visual measurement for visual-inertial localization in high-speed motion scenes},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Object-based SLAM using superquadrics. <em>IROS</em>,
10198–10205. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual SLAM uses visual information, typically point features, to localise a camera and, at the same time, map the environment. In recent years, there has been interest in using scene-understanding capabilities to enhance the mapping process and object-level SLAM systems have appeared in response. However, most of the previous work is limited to prestored object models or pre-trained networks to represent the objects, which limits working scenarios or uses representations with limited scope, such as cubes or quadrics. To address this, we propose to use superquadrics as the object representation and, in this paper, present a proof of principle SLAM system in which object-based mapping is fully integrated with camera tracking via keyframe optimisation. The system was tested on simulated and real datasets, and the results show that the system can achieve lightweight and comparatively good object representation whilst also giving good camera trajectories estimates under certain scenarios.},
  archive   = {C_IROS},
  author    = {Yifan Xing and Noe Samano and Wen Fan and Andrew Calway},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801995},
  month     = {10},
  pages     = {10198-10205},
  title     = {Object-based SLAM using superquadrics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NF-SLAM: Effective, normalizing flow-supported neural field
representations for object-level visual SLAM in automotive applications.
<em>IROS</em>, 10190–10197. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel, vision-only object-level SLAM framework for automotive applications representing 3D shapes by implicit signed distance functions. Our key innovation consists of augmenting the standard neural representation by a normalizing flow network. As a result, achieving strong representation power on the specific class of road vehicles is made possible by compact networks with only 16-dimensional latent codes. Furthermore, the newly proposed architecture exhibits a significant performance improvement in the presence of only sparse and noisy data, which is demonstrated through comparative experiments on synthetic data. The module is embedded into the back-end of a stereo-vision based framework for joint, incremental shape optimization. The loss function is given by a combination of a sparse 3D point-based SDF loss, a sparse rendering loss, and a semantic mask-based silhouette-consistency term. We furthermore leverage semantic information to determine keypoint extraction density in the front-end. Finally, experimental results on real-world data reveal accurate and reliable performance comparable to alternative frameworks that make use of direct depth readings. The proposed method performs well with only sparse 3D points obtained from bundle adjustment, and eventually continues to deliver stable results even under exclusive use of the mask-consistency term.},
  archive   = {C_IROS},
  author    = {Li Cui and Yang Ding and Richard Hartley and Zirui Xie and Laurent Kneip and Zhenghua Yu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801421},
  month     = {10},
  pages     = {10190-10197},
  title     = {NF-SLAM: Effective, normalizing flow-supported neural field representations for object-level visual SLAM in automotive applications},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two-stage pose optimization algorithm using color
information for underwater SLAM with light-sectioning-based 3D scanning
method. <em>IROS</em>, 10182–10189. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The demand for 3D shape measurement of underwater scene is increasing in various applications. Especially, simultaneous localization and mapping (SLAM) technique utilizing remotely operated vehicle (ROV) attached with 3D sensors has been intensively researched. This paper focuses on solving pose optimization problem for underwater robots with camera/multiple-line-lasers setup, especially for the scene with some textures (color information). To this end, a two-stage pose optimization technique is proposed. In the first stage, due to the sparse nature of the reconstructed shape in the light-sectioning method consisting of several 3D curves, we bundle 10 to 20 consecutive frames to form a block shape, refining significant errors in the initial sensor poses using a novel bundle adjustment algorithm. In the second stage, remaining pose errors are corrected by a block-based matching algorithm utilizing iterative closest point (ICP) algorithm with color information. Through experiments in underwater environment with a real system, it was validated that the proposed method demonstrates superior performance compared to past underwater SLAM techniques.},
  archive   = {C_IROS},
  author    = {Takaki Ikeda and Takafumi Iwaguchi and Diego Thomas and Hiroshi Kawasaki},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801859},
  month     = {10},
  pages     = {10182-10189},
  title     = {Two-stage pose optimization algorithm using color information for underwater SLAM with light-sectioning-based 3D scanning method},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards long term SLAM on thermal imagery. <em>IROS</em>,
10174–10181. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual SLAM with thermal imagery remains a difficult problem for many state of the art (SOTA) algorithms. Compared with visible spectrum imagery, thermal imagery generally has lower contrast, higher noise, and tends to have lower resolution, making for challenging front-end data association. Thermal imagery also presents a difficult problem for long term relocalization and map reuse, because the relative temperatures of objects in thermal imagery tend to change dramatically from day to night. Feature descriptors typically used for relocalization in SLAM are unable to maintain consistency over these diurnal changes. We show that learned feature descriptors can be used within existing bag of word based localization schemes to dramatically improve place recognition across large temporal gaps in thermal imagery. In order to demonstrate the effectiveness of our trained vocabulary, we have developed a baseline SLAM system, integrating learned features and matching into a classical SLAM algorithm. Our system demonstrates good local tracking on challenging thermal imagery, and relocalization that overcomes dramatic day to night thermal appearance changes. Our code and datasets are available here: https://github.com/neufieldrobotics/IRSLAM_Baseline},
  archive   = {C_IROS},
  author    = {C Keil and A. Gupta and P. Kaveti and H. Singh},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802139},
  month     = {10},
  pages     = {10174-10181},
  title     = {Towards long term SLAM on thermal imagery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PickScan: Object discovery and reconstruction from handheld
interactions. <em>IROS</em>, 10167–10173. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reconstructing compositional 3D representations of scenes, where each object is represented with its own 3D model, is a highly desirable capability in robotics and augmented reality. However, most existing methods rely heavily on strong appearance priors for object discovery, therefore only working on those classes of objects on which the method has been trained, or do not allow for object manipulation, which is necessary to scan objects fully and to guide object discovery in challenging scenarios. We address these limitations with a novel interaction-guided and class-agnostic method based on object displacements that allows a user to move around a scene with an RGB-D camera, hold up objects, and finally outputs one 3D model per held-up object. Our main contribution to this end is a novel approach to detecting user-object interactions and extracting the masks of manipulated objects. On a custom-captured dataset, our pipeline discovers manipulated objects with 78.3% precision at 100% recall and reconstructs them with a mean chamfer distance of 0.90cm. Compared to Co-Fusion, the only comparable interaction-based and class-agnostic baseline, this corresponds to a reduction in chamfer distance of 73% while detecting 99% fewer false positives.},
  archive   = {C_IROS},
  author    = {Vincent van der Brugge and Marc Pollefeys and Joshua B. Tenenbaum and Krishna Murthy Jatavallabhula and Ayush Tewari},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802190},
  month     = {10},
  pages     = {10167-10173},
  title     = {PickScan: Object discovery and reconstruction from handheld interactions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MM3DGS SLAM: Multi-modal 3D gaussian splatting for SLAM
using vision, depth, and inertial measurements. <em>IROS</em>,
10159–10166. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simultaneous localization and mapping is essential for position tracking and scene understanding. 3D Gaussian-based map representations enable photorealistic reconstruction and real-time rendering of scenes using multiple posed cameras. We show for the first time that using 3D Gaussians for map representation with unposed camera images and inertial measurements can enable accurate SLAM. Our method, MM3DGS, addresses the limitations of prior neural radiance field-based representations by enabling faster rendering, scale awareness, and improved trajectory tracking. Our framework enables keyframe-based mapping and tracking utilizing loss functions that incorporate relative pose transformations from pre-integrated inertial measurements, depth estimates, and measures of photometric rendering quality. We also release a multi-modal dataset, UT-MM, collected from a mobile robot equipped with a camera and an inertial measurement unit. Experimental evaluation on several scenes from the dataset shows that MM3DGS achieves nearly 3x improvement in tracking and 5% improvement in photometric rendering quality compared to the current 3DGS SLAM state-of-the-art, while allowing real-time rendering of a high-resolution dense 3D map.},
  archive   = {C_IROS},
  author    = {Lisong C. Sun and Neel P. Bhatt and Jonathan C. Liu and Zhiwen Fan and Zhangyang Wang and Todd E. Humphreys and Ufuk Topcu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802389},
  month     = {10},
  pages     = {10159-10166},
  title     = {MM3DGS SLAM: Multi-modal 3D gaussian splatting for SLAM using vision, depth, and inertial measurements},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Visual loop closure detection with thorough temporal and
spatial context exploitation. <em>IROS</em>, 10153–10158. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite advancements in visual Simultaneous Localization and Mapping (SLAM), prevailing visual Loop Closure Detection (LCD) methods primarily rely on computationally intensive image similarity comparisons, neglecting temporal-spatial context during long-term exploration. To address this issue, we propose TOSA, a novel visual LCD algorithm harnessing TempOral and SpAtial context for efficient LCD. Specifically, as the agent explores through time, our approach recurrently updates a latent feature incorporating historical information via a Long Short-Term Memory (LSTM) module. Upon receiving a query frame, TOSA seamlessly fuses the latent feature with the query feature to predict the candidates’ distribution, thus averting intensive similarity computation. Additionally, TOSA integrates a temporal-spatial convolution for candidate refinement by thoroughly exploiting the temporal consistency and spatial correlation to enhance selected candidates, further boosting the performance. Extensive experiments across four standard datasets showcase the superiority of our method over existing state-of-the-art techniques, demonstrating the effectiveness of utilizing rich temporal-spatial contexts.},
  archive   = {C_IROS},
  author    = {Jiaxin Li and Zan Wang and Huijun Di and Jian Li and Wei Liang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802228},
  month     = {10},
  pages     = {10153-10158},
  title     = {Visual loop closure detection with thorough temporal and spatial context exploitation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LA-LIO: Robust localizability-aware LiDAR-inertial odometry
for challenging scenes. <em>IROS</em>, 10145–10152. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern robotic systems are increasingly deployed in complex and diverse environments, and reliable localization under challenging conditions becomes crucial for the safe and efficient operation of these systems. The odometry based on LiDAR is prone to system collapse caused by computational divergence under conditions of aggressive motion and information deficiency in spatial geometry. To enhance the robustness of systems in challenging scenes, this work proposes LA-LIO, robust localizability-aware LiDAR inertial odometry. It mainly consists of three parts. Firstly, this paper presents a LiDAR degeneration detection method that enables stable degeneration assessment. Secondly, a method for segmenting LiDAR point clouds is proposed to alleviate the issue of excessive distortion in point clouds under aggressive motion scenes. The last is an Errors State Kalman Filter (ESKF) method with adaptive weights to utilize the existing spatial information as much as possible to improve the stability of the system in degenerated scenarios. The proposed method is evaluated and compared in multiple experiments, demonstrating the performance and reliability improvements of this approach in challenging environments.},
  archive   = {C_IROS},
  author    = {Junjie Huang and Yunzhou Zhang and Qingdong Xu and Song Wu and Jun Liu and Guiyuan Wang and Wei Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802825},
  month     = {10},
  pages     = {10145-10152},
  title     = {LA-LIO: Robust localizability-aware LiDAR-inertial odometry for challenging scenes},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient dynamic LiDAR odometry for mobile robots with
structured point clouds. <em>IROS</em>, 10137–10144. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a real-time dynamic LiDAR odometry pipeline for mobile robots in Urban Search and Rescue (USAR) scenarios. Existing approaches to dynamic object detection often rely on pretrained learned networks or computationally expensive volumetric maps. To enhance efficiency on computationally limited robots, we reuse data between the odometry and detection module. Utilizing a range image segmentation technique and a novel residual-based heuristic, our method distinguishes dynamic from static objects before integrating them into the point cloud map. The approach demonstrates robust object tracking and improved map accuracy in environments with numerous dynamic objects. Even highly non-rigid objects, such as running humans, are accurately detected at point level without prior downsampling of the point cloud and hence, without loss of information. Evaluation on simulated and real-world data validates its computational efficiency. Compared to a stateof-the-art volumetric method, our approach shows comparable detection performance at a fraction of the processing time, adding only 14 ms to the odometry module for dynamic object detection and tracking. The implementation and a new realworld dataset are available as open-source for further research.},
  archive   = {C_IROS},
  author    = {Jonathan Lichtenfeld and Kevin Daun and Oskar von Stryk},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802045},
  month     = {10},
  pages     = {10137-10144},
  title     = {Efficient dynamic LiDAR odometry for mobile robots with structured point clouds},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ASML-VDIO: Visual-depth-inertial odometry using selected
accurate and stable multi-modal landmarks in structural environments.
<em>IROS</em>, 10129–10136. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In complex indoor structural scenes such as shopping centers and malls, camera pose estimation using pure point features is easy to fail due to the difficulty in extracting sufficient and stable point features from weak textures or dynamic environments. Recent works have attempted to address these challenges by introducing line features. However, the addition of line features increases the number of parameters and landmarks for BA (Bundle Adjustment), leading to efficiency reduction. This is a common issue in multi-modal SLAM (Simultaneous Localization And Mapping). To address this issue, this paper proposes a novel visual-depth-inertial odometry (ASML-VDIO) framework by combining RGB-D and IMU sensors. To improve the efficiency of BA, the proposed landmark classification method classifies 3D landmarks into accurate landmarks and other landmarks based on spatial consistency verification and depth range limitation. Then, accurate landmarks are fixed, and only other landmarks are optimized in the optimization of BA. Furthermore, to remove line features extracted from dynamic objects (pedestrian, shopping-car, etc), we propose a dynamic line removal method that combines geometric constraints and motion constraints of line features. Finally, the method is evaluated on public and author-collected datasets, showing competitive accuracy and robustness in complex indoor structural scenes while 71% speedup on optimization thread with same constraints.},
  archive   = {C_IROS},
  author    = {Xingjian Luo and Chenglin Pang and Xuankang Wu and Zheng Fang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801551},
  month     = {10},
  pages     = {10129-10136},
  title     = {ASML-VDIO: Visual-depth-inertial odometry using selected accurate and stable multi-modal landmarks in structural environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Grid-based submap joining: An efficient algorithm for
simultaneously optimizing global occupancy map and local submap frames.
<em>IROS</em>, 10121–10128. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Optimizing robot poses and the map simultaneously has been shown to provide more accurate SLAM results. However, for non-feature based SLAM approaches, directly optimizing all the robot poses and the whole map will greatly increase the computational cost, making SLAM problems difficult to solve in large-scale environments. To solve the 2D non-feature based SLAM problem in large-scale environments more accurately and efficiently, we propose the grid-based submap joining method. Specifically, we first formulate the 2D grid-based submap joining problem as a non-linear least squares (NLLS) form to optimize the global occupancy map and local submap frames simultaneously. We then prove that in solving the NLLS problem using Gauss-Newton (GN) method, the increments of the poses in each iteration are independent of the occupancy values of the global occupancy map. Based on this property, we propose a pose-only GN algorithm equivalent to full GN method to solve the NLLS problem. The proposed submap joining algorithm is very efficient due to the independent property and the pose-only solution. Evaluations using simulations and publicly available practical 2D laser datasets confirm the outperformance of our proposed method compared to the state-of-the-art methods in terms of efficiency and accuracy, as well as the ability to solve the grid-based SLAM problem in very large-scale environments.},
  archive   = {C_IROS},
  author    = {Yingyu Wang and Liang Zhao and Shoudong Huang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802536},
  month     = {10},
  pages     = {10121-10128},
  title     = {Grid-based submap joining: An efficient algorithm for simultaneously optimizing global occupancy map and local submap frames},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A low-texture robust hybrid feature based visual odometry.
<em>IROS</em>, 10113–10120. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In low-texture scenes, Visual Odometry (VO) algorithms often encounter challenges stemming from sparse feature sets and reduced accuracy in feature matching. To overcome this, integrating plane features and vanishing point characteristics can provide additional constraints for refining camera poses. Optical flow-based tracking methods may also offer improved matching precision compared to traditional feature-based approaches. Motivated by these challenges, we present a robust Visual Odometry system tailored for low-texture environments. Our system combines a vanishing point-based approach for camera pose optimization with a Manhattan-aided algorithm for matching line segments using optical flow. By incorporating planes and vanishing points as supplementary features for pose estimation, we enhance overall accuracy without significant time overhead. We utilize detected line features to compute vanishing points, improving accuracy without compromising efficiency. In addition, our Manhattan-aided optical flow technique supplements and refines the results of line feature matching, further enhancing the accuracy of vanishing points. Evaluation on various public datasets demonstrates the superior accuracy and robustness of our system compared to state-of-the-art Simultaneous Localization And Mapping (SLAM) and VO methods. Notably, our method effectively addresses issues of failure in low-texture scenes and improves the accuracy of line feature matching compared to baseline methods. We will release our source code upon paper acceptance.},
  archive   = {C_IROS},
  author    = {He Wang and Qi Zhang and Zhiwen Zheng and Xiaoli Li and Hongye Tan and Ru Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802809},
  month     = {10},
  pages     = {10113-10120},
  title     = {A low-texture robust hybrid feature based visual odometry},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BE-SLAM: BEV-enhanced dynamic semantic SLAM with static
object reconstruction. <em>IROS</em>, 10105–10112. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The quality of a robot’s environmental perception determines whether it can achieve more intelligent applications, such as semantic interaction with humans. SLAM, on the other hand, is one of the crucial capabilities for a robot to perceive its environment. However, when only a monocular image is provided, dynamic objects in the environment significantly impact the accuracy of map construction by the robot, leading to erroneous perception results. To address this issue, we propose a Visual SLAM framework based on BEV perception results, named BE-SLAM. With this framework, we can handle dynamic objects, occlusions, and incompletely observed objects. It can construct a stable static map by strengthening trust in static objects. Considering that object-level semantic maps can enhance a robot’s perception abilities, we also reconstruct static objects in the map and use them to optimize the pose. Through experiments on existing publicly available dataset, we compare BE-SLAM with several existing methods that have shown good performance. The experimental results demonstrate that BE-SLAM performs exceptionally well on high-dynamic sequences and achieves comparable results on static or low-dynamic sequences.},
  archive   = {C_IROS},
  author    = {Jun Luo and Gang Wang and Hongliang Liu and Lang Wu and Tao Huang and Dengyu Xiao and Huayan Pu and Jun Luo},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802013},
  month     = {10},
  pages     = {10105-10112},
  title     = {BE-SLAM: BEV-enhanced dynamic semantic SLAM with static object reconstruction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). ESO-SLAM: Tightly-coupled and simultaneous estimation of
self and multi-object pose via sensor fusion. <em>IROS</em>,
10097–10104. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simultaneous Localization and Mapping (SLAM) is widely used in applications such as robotics and autonomous driving, with methods involving multi-sensor fusion demonstrating excellent performance. However, they simply reject dynamic features and ignore the mutual benefits of self and dynamic objects, which greatly limits their application in actual high-dynamic scenes. To address this issue, we propose ESO-SLAM, a tightly-coupled system for simultaneous self and multi-object pose estimation achieved through sensor fusion. This system employs a multi-probability fusion tracker based on filter to establish more robust object-level data association. Building upon this, we introduce a method that combines 3D Kalman filter velocity priors and camera optical flow decoupling for dynamic point cloud removal, aiming at improving the accuracy of self-pose estimation in odometry. Finally, we jointly refine the poses of the robot and objects using multiple constraint factors within our proposed framework. Experimental results on the KITTI raw dataset demonstrate that our approach achieves better pose accuracy for both self and tracked objects compared to baseline and state-of-the-art techniques. Furthermore, the proposed method exhibits feasibility in real-time performance to ensure its practical application value.},
  archive   = {C_IROS},
  author    = {Wu Li and Yunzhou Zhang and Yuezhang Lv and Tingting Wang and Sizhan Wang and Guiyuan Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802118},
  month     = {10},
  pages     = {10097-10104},
  title     = {ESO-SLAM: Tightly-coupled and simultaneous estimation of self and multi-object pose via sensor fusion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inline photometrically calibrated hybrid visual SLAM.
<em>IROS</em>, 10089–10096. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an integrated approach to Visual SLAM, merging online sequential photometric calibration within a Hybrid direct-indirect visual SLAM (H-SLAM). Photometric calibration helps normalize pixel intensity values under different lighting conditions, and thereby improves the direct component of our H-SLAM. A tangential benefit also results to the indirect component of H-SLAM given that the detected features are more stable across variable lighting conditions. Our proposed photometrically calibrated H-SLAM is tested on several datasets, including the TUM monoVO as well as on a dataset we created. Calibrated H-SLAM outperforms other state of the art direct, indirect, and hybrid Visual SLAM systems in all the experiments. Furthermore, in online SLAM tested at our site, it also significantly outperformed the other SLAM Systems.},
  archive   = {C_IROS},
  author    = {Nicolas Abboud and Malak Sayour and Imad H. Elhajj and John Zelek and Daniel Asmar},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802153},
  month     = {10},
  pages     = {10089-10096},
  title     = {Inline photometrically calibrated hybrid visual SLAM},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Time-ordered ad-hoc resource sharing for independent robotic
agents. <em>IROS</em>, 10081–10088. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Resource sharing is a crucial part of a multi-robot system. We propose a Boolean satisfiability problem (SAT) based approach to resource sharing. Our key contributions are an algorithm for converting any constrained assignment to a weighted-SAT based optimization. We propose a theorem that allows optimal resource assignment problems to be solved via repeated application of a SAT solver. Additionally we show a way to encode continuous time ordering constraints using Conjunctive Normal Form (CNF). We benchmark our new algorithms and show that they can be used in an ad-hoc setting. We test our algorithms on a fleet of simulated and real world robots and show that the algorithms are able to handle real world situations. Our algorithms and test harnesses are open source and build on Open-RMF’s fleet management system.},
  archive   = {C_IROS},
  author    = {Arjo Chakravarty and Michael X. Grey and M. A. Viraj J. Muthugala and Mohan Rajesh Elara},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802365},
  month     = {10},
  pages     = {10081-10088},
  title     = {Time-ordered ad-hoc resource sharing for independent robotic agents},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Privacy-preserving map-free exploration for confirming the
absence of a radioactive source. <em>IROS</em>, 10073–10080. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Performing an inspection task while maintaining the privacy of the inspected site is a challenging balancing act. In this work, we are motivated by the future of nuclear arms control verification, which requires both a high level of privacy and guaranteed correctness. For scenarios with limitations on sensors and stored information due to the potentially secret nature of observable features, we propose a robotic verification procedure that provides map-free exploration to perform a source verification task without requiring, nor revealing, any task-irrelevant, site-specific information. We provide theoretical guarantees on the privacy and correctness of our approach, validated by extensive simulated and hardware experiments.},
  archive   = {C_IROS},
  author    = {Eric Lepowsky and David Snyder and Alexander Glaser and Anirudha Majumdar},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802428},
  month     = {10},
  pages     = {10073-10080},
  title     = {Privacy-preserving map-free exploration for confirming the absence of a radioactive source},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual-geometry GP-based navigable space for autonomous
navigation. <em>IROS</em>, 10066–10072. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous navigation in unknown environments is challenging and requires the consideration of both geometric and semantic information to assess the navigability of the environment. In this work, we propose a novel space modeling framework, Visual-Geometry Sparse Gaussian Process (VG-SGP), that simultaneously considers the semantics and geometry of the scene. Our proposed approach can overcome the limitation of visual planners that fail to recognize geometry associated with the semantic and the geometric planners that completely overlook the semantic information which is very critical in real-world navigation. The proposed method leverages dual Sparse Gaussian Processes in an integrated manner; the first is trained to forecast geometrically navigable spaces while the second predicts the semantically navigable areas. This integrated model is able to pinpoint the overlapping (geometric and semantic) navigable space. The simulation and real-world experiments demonstrate that the proposed VG-SGP model, coupled with our innovative navigation strategy, outperforms models solely reliant on visual or geometric navigation algorithms, highlighting a superior adaptive behavior. We provided a demonstration video1 and open-sourced our code 2.},
  archive   = {C_IROS},
  author    = {Mahmoud Ali and Durgkant Pushp and Zheng Chen and Lantao Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801801},
  month     = {10},
  pages     = {10066-10072},
  title     = {Visual-geometry GP-based navigable space for autonomous navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LAC-net: Linear-fusion attention-guided convolutional
network for accurate robotic grasping under the occlusion.
<em>IROS</em>, 10059–10065. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the challenge of perceiving complete object shapes through visual perception. While prior studies have demonstrated encouraging outcomes in segmenting the visible parts of objects within a scene, amodal segmentation, in particular, has the potential to allow robots to infer the occluded parts of objects. To this end, this paper introduces a new framework that explores amodal segmentation for robotic grasping in cluttered scenes, thus greatly enhancing robotic grasping abilities. Initially, we use a conventional segmentation algorithm to detect the visible segments of the target object, which provides shape priors for completing the full object mask. Particularly, to explore how to utilize semantic features from RGB images and geometric information from depth images, we propose a Linear-fusion Attention-guided Convolutional Network (LAC-Net). LAC-Net utilizes the linear-fusion strategy to effectively fuse this cross-modal data, and then uses the prior visible mask as attention map to guide the network to focus on target feature locations for further complete mask recovery. Using the amodal mask of the target object provides advantages in selecting more accurate and robust grasp points compared to relying solely on the visible segments. The results on different datasets show that our method achieves state-of-the-art performance. Furthermore, the robot experiments validate the feasibility and robustness of this method in the real world. Our code and demonstrations are available on the project page: https://jrryzh.github.io/LAC-Net.},
  archive   = {C_IROS},
  author    = {Jinyu Zhang and Yongchong Gu and Jianxiong Gao and Haitao Lin and Qiang Sun and Xinwei Sun and Xiangyang Xue and Yanwei Fu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802054},
  month     = {10},
  pages     = {10059-10065},
  title     = {LAC-net: Linear-fusion attention-guided convolutional network for accurate robotic grasping under the occlusion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Boosting generalizability towards zero-shot cross-dataset
single-image indoor depth by meta-initialization. <em>IROS</em>,
10051–10058. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Indoor robots rely on depth to perform tasks like navigation or obstacle detection, and single-image depth estimation is widely used to assist perception. Most indoor single-image depth prediction focuses less on model generalizability to unseen datasets, concerned with in-the-wild robustness for system deployment. This work leverages gradient-based meta-learning to gain higher generalizability on zero-shot cross-dataset inference. Unlike the most-studied meta-learning of image classification associated with explicit class labels, no explicit task boundaries exist for continuous depth values tied to highly varying indoor environments regarding object arrangement and scene composition. We propose fine-grained task that treats each RGB-D mini-batch as a task in our meta-learning formulation. We first show that our method on limited data induces a much better prior (max 27.8% in RMSE). Then, finetuning on meta-learned initialization consistently outperforms baselines without the meta approach. Aiming at generalization, we propose zero-shot cross-dataset protocols and validate higher generalizability induced by our meta-initialization, as a simple and useful plugin to many existing depth estimation methods. The work at the intersection of depth and meta-learning potentially drives both research to step closer to practical robotic and machine perception usage.},
  archive   = {C_IROS},
  author    = {Cho-Ying Wu and Yiqi Zhong and Junying Wang and Ulrich Neumann},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801638},
  month     = {10},
  pages     = {10051-10058},
  title     = {Boosting generalizability towards zero-shot cross-dataset single-image indoor depth by meta-initialization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards cross-view-consistent self-supervised surround depth
estimation. <em>IROS</em>, 10043–10050. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Depth estimation is a cornerstone for autonomous driving, yet acquiring per-pixel depth ground truth for supervised learning is challenging. Self-Supervised Surround Depth Estimation (SSSDE) from consecutive images offers an economical alternative. While previous SSSDE methods have proposed different mechanisms to fuse information across images, few of them explicitly consider the cross-view constraints, leading to inferior performance, particularly in overlapping regions. This paper proposes an efficient and consistent pose estimation design and two loss functions to enhance cross-view consistency for SSSDE. For pose estimation, we propose to use only front-view images to reduce training memory and sustain pose estimation consistency. The first loss function is the dense depth consistency loss, which penalizes the difference between predicted depths in overlapping regions. The second one is the multi-view reconstruction consistency loss, which aims to maintain consistency between reconstruction from spatial and spatial-temporal contexts. Additionally, we introduce a novel flipping augmentation to improve the performance further. Our techniques enable a simple neural model to achieve state-of-the-art performance on the DDAD and nuScenes datasets. Last but not least, our proposed techniques can be easily applied to other methods. The code is available at https://github.com/denyingmxd/CVCDepth.},
  archive   = {C_IROS},
  author    = {Laiyan Ding and Hualie Jiang and Jie Li and Yongquan Chen and Rui Huang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802436},
  month     = {10},
  pages     = {10043-10050},
  title     = {Towards cross-view-consistent self-supervised surround depth estimation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). TriHelper: Zero-shot object navigation with dynamic
assistance. <em>IROS</em>, 10035–10042. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Navigating toward specific objects in unknown environments without additional training, known as Zero-Shot object navigation, poses a significant challenge in the field of robotics, which demands high levels of auxiliary information and strategic planning. Traditional works have focused on holistic solutions, overlooking the specific challenges agents encounter during navigation such as collision, low exploration efficiency, and misidentification of targets. To address these challenges, our work proposes TriHelper, a novel framework designed to assist agents dynamically through three primary navigation challenges: collision, exploration, and detection. Specifically, our framework consists of three innovative components: (i) Collision Helper, (ii) Exploration Helper, and (iii) Detection Helper. These components work collaboratively to solve these challenges throughout the navigation process. Experiments on the Habitat-Matterport 3D (HM3D) and Gibson datasets demonstrate that TriHelper significantly outperforms all existing baseline methods in Zero-Shot object navigation, showcasing superior success rates and exploration efficiency. Our ablation studies further underscore the effectiveness of each helper in addressing their respective challenges, notably enhancing the agent’s navigation capabilities. By proposing TriHelper, we offer a fresh perspective on advancing the object navigation task, paving the way for future research in the domain of Embodied AI and visual-based navigation.},
  archive   = {C_IROS},
  author    = {Lingfeng Zhang and Qiang Zhang and Hao Wang and Erjia Xiao and Zixuan Jiang and Honglei Chen and Renjing Xu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802670},
  month     = {10},
  pages     = {10035-10042},
  title     = {TriHelper: Zero-shot object navigation with dynamic assistance},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Monocular depth estimation for drone obstacle avoidance in
indoor environments. <em>IROS</em>, 10027–10034. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous nano-quadcopters possess large potential for indoor use. Existing works on autonomous flight however rely on large amounts of compute, therefore resulting in heavy and bulky platforms that can only be safely deployed outdoors. We present a monocular depth estimation method for autonomous indoor obstacle avoidance and waypoint navigation of nano-quadcopters demonstrated on the Bitcraze Crazyflie 2.1 which weighs a mere 33g. Our depth estimation model has 1.56 million parameters and is 4 MB, which after quantization becomes 1 MB. We transmit the images via WiFi from the onboard grayscale camera on the Bitcraze to a laptop, which then runs the 1 MB quantized model to generate small-size depth maps. Subsequently, we run our navigation algorithms on a laptop and transmit high-level motion commands back to the drone. We demonstrate obstacle avoidance capability of this end-to-end system through real-world flights in a variety of indoor environments.},
  archive   = {C_IROS},
  author    = {Haokun Zheng and Sidhant Rajadnya and Avideh Zakhor},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802577},
  month     = {10},
  pages     = {10027-10034},
  title     = {Monocular depth estimation for drone obstacle avoidance in indoor environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LeGo-drive: Language-enhanced goal-oriented closed-loop
end-to-end autonomous driving. <em>IROS</em>, 10020–10026. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing Vision-Language Models (VLMs) produce long-term trajectory waypoints or directly control actions based on their perception input and language prompt. However, these VLMs are not explicitly aware of the constraints imposed by the scene or kinematics of the vehicle. As a result, the generated trajectories or control inputs are likely to be unsafe and/or infeasible. In this paper, we introduce LeGo-Drive†, which aims to address these issues. Our key idea is to use the VLM to just predict a goal location based on the given language command and perception input, which is then fed to a downstream differentiable trajectory optimizer with learnable components. We train the VLM and the trajectory optimizer in an end-to-end fashion using a loss function that captures the ego-vehicle’s ability to reach the predicted goal while satisfying safety and kinematic constraints. The gradients during the back-propagation flow through the optimization layer and make the VLM aware of the planner’s capabilities, making more feasible goal predictions. We compare our end-to-end approach with a decoupled framework where the planner is just used at the inference time to drive to the VLM-predicted goal location and report a goal reaching Success Rate of 81%. We demonstrate the versatility of LeGo-Drive† across various driving scenarios and navigation commands, highlighting its potential for practical deployment in autonomous vehicles.},
  archive   = {C_IROS},
  author    = {Pranjal Paul and Anant Garg and Tushar Choudhary and Arun Kumar Singh and K. Madhava Krishna},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801870},
  month     = {10},
  pages     = {10020-10026},
  title     = {LeGo-drive: Language-enhanced goal-oriented closed-loop end-to-end autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Map-based modular approach for zero-shot embodied question
answering. <em>IROS</em>, 10013–10019. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Embodied Question Answering (EQA) serves as a benchmark task to evaluate the capability of robots to navigate within novel environments and identify objects in response to human queries. However, existing EQA methods often rely on simulated environments and operate with limited vocabularies. This paper presents a map-based modular approach to EQA, enabling real-world robots to explore and map unknown environments. By leveraging foundation models, our method facilitates answering a diverse range of questions using natural language. We conducted extensive experiments in both virtual and real-world settings, demonstrating the robustness of our approach in navigating and comprehending queries within unknown environments.},
  archive   = {C_IROS},
  author    = {Koya Sakamoto and Daichi Azuma and Taiki Miyanishi and Shuhei Kurita and Motoaki Kawanabe},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801314},
  month     = {10},
  pages     = {10013-10019},
  title     = {Map-based modular approach for zero-shot embodied question answering},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active learning-augmented intention-aware obstacle avoidance
of autonomous surface vehicles in high-traffic waters. <em>IROS</em>,
10005–10012. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper enhances the obstacle avoidance of Autonomous Surface Vehicles (ASVs) for safe navigation in high-traffic waters with an active state estimation of obstacle’s passing intention and reducing its uncertainty. We introduce a topological modeling of passing intention of obstacles, which can be applied to varying encounter situations based on the inherent embedding of topological concepts in COLREGs. With a Long Short-Term Memory (LSTM) neural network, we classify the passing intention of obstacles. Then, for determining the ASV maneuver, we propose a multi-objective optimization framework including information gain about the passing obstacle intention and safety. We validate the proposed approach under extensive Monte Carlo simulations (2,400 runs) with a varying number of obstacles, dynamic properties, encounter situations, and different behavioral patterns of obstacles (cooperative, non-cooperative). We also present the results from a real marine accident case study as well as real-world experiments of a real ASV with environmental disturbances, showing successful collision avoidance with our strategy in real-time.},
  archive   = {C_IROS},
  author    = {Mingi Jeong and Arihant Chadda and Alberto Quattrini Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802205},
  month     = {10},
  pages     = {10005-10012},
  title     = {Active learning-augmented intention-aware obstacle avoidance of autonomous surface vehicles in high-traffic waters},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FDNet: Feature decoupling framework for trajectory
prediction. <em>IROS</em>, 9997–10004. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trajectory prediction plays a significant role in autonomous driving, with current challenges primarily focused on capturing complex interactions in traffic scenes. Previous methods usually directly encode non-interactive and interactive information together, and then decode them for trajectory prediction. However, given the complexity inherent property in the trajectory generation process (e.g., the generation of trajectory points are influenced by the interactions among multiple moving agents, as well as the interactions between agents and the static environment), previous approaches fail to precisely capture separate variations of the trajectory generation process. In this paper, we propose a general and plug-and-play feature decoupling framework for trajectory prediction called FDNet, which can learn the interactive and non-interactive factors in the latent space to capture separate variations of the trajectory generation process. At its core, FDNet is comprised of a Non-interactive Feature Extraction Module to extract non-interactive features, and an Interactive Feature Decoupling Module to decouple interactive features. Extensive experiments conducted on Argoverse and nuScenes demonstrate that FDNet significantly improves the performance of existing methods.},
  archive   = {C_IROS},
  author    = {Yuhang Li and Changsheng Li and Baoyu Fan and Rongqing Li and Ziyue Zhang and Dongchun Ren and Ye Yuan and Guoren Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802470},
  month     = {10},
  pages     = {9997-10004},
  title     = {FDNet: Feature decoupling framework for trajectory prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OSM vs HD maps: Map representations for trajectory
prediction. <em>IROS</em>, 9990–9996. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {High Definition (HD) Maps have long been favored for their precise depictions of static road elements. However, their accessibility constraints and vulnerability to rapid environmental changes impede the widespread deployment of highly map-reliant autonomous driving tasks, such as motion forecasting. In this context, we propose to leverage OpenStreetMap (OSM) as a promising alternative to HD Maps for long-term motion forecasting. The contributions of this work are threefold: firstly, we extend the application of OSM to long-horizon forecasting, doubling the forecasting horizon compared to previous studies. Secondly, through an expanded observation landscape and the integration of intersection priors, our OSM-based approach exhibits competitive performance, narrowing the gap with HD-map-based models. Lastly, we conduct an exhaustive context-aware analysis, providing deeper insights in motion forecasting across diverse scenarios as well as conducting class-aware comparisons. This research not only advances long-term motion forecasting with coarse map representations but additionally offers a scalable solution within the domain of autonomous driving.},
  archive   = {C_IROS},
  author    = {Jing-Yan Liao and Parth Doshi and Zihan Zhang and David Paz and Henrik Christensen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802482},
  month     = {10},
  pages     = {9990-9996},
  title     = {OSM vs HD maps: Map representations for trajectory prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AdvDiffuser: Generating adversarial safety-critical driving
scenarios via guided diffusion. <em>IROS</em>, 9983–9989. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safety-critical scenarios are infrequent in natural driving environments but hold significant importance for the training and testing of autonomous driving systems. The prevailing approach involves generating safety-critical scenarios automatically in simulation by introducing adversarial adjustments to natural environments. These adjustments are often tailored to specific tested systems, thereby disregarding their transferability across different systems. In this paper, we propose AdvDiffuser, an adversarial framework for generating safety-critical driving scenarios through guided diffusion. By incorporating a diffusion model to capture plausible collective behaviors of background vehicles and a lightweight guide model to effectively handle adversarial scenarios, AdvDiffuser facilitates transferability. Experimental results on the nuScenes dataset demonstrate that AdvDiffuser, trained on offline driving logs, can be applied to various tested systems with minimal warm-up episode data and outperform other existing methods in terms of realism, diversity, and adversarial performance.},
  archive   = {C_IROS},
  author    = {Yuting Xie and Xianda Guo and Cong Wang and Kunhua Liu and Long Chen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802408},
  month     = {10},
  pages     = {9983-9989},
  title     = {AdvDiffuser: Generating adversarial safety-critical driving scenarios via guided diffusion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Synergistic reinforcement and imitation learning for
vision-driven autonomous flight of UAV along river. <em>IROS</em>,
9976–9982. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision-driven autonomous flight and obstacle avoidance of Unmanned Aerial Vehicles (UAVs) along complex riverine environments for tasks like rescue and surveillance requires a robust navigation policy, which is yet difficult to obtain due to the shortage of trainable riverine environment simulators. To easily verify the vision-based navigation controller performance for the river following task before real-world deployment, we developed a trainable photo-realistic dynamics-free riverine simulation environment using Unity. In this paper, we address the shortcomings that vanilla Reinforcement Learning (RL) algorithm encounters in learning a navigation policy within this partially observable, non-Markovian environment. We propose a synergistic approach that integrates RL and Imitation Learning (IL). Initially, an IL expert is trained on manually collected demonstrations, which then guides the RL policy training process. Concurrently, experiences generated by the RL agent are utilized to re-train the IL expert, enhancing its ability to generalize to unseen data. By leveraging the strengths of both RL and IL, this framework achieves a faster convergence rate and higher performance compared to pure RL, pure IL, and RL combined with static IL algorithms. The results validate the efficacy of the proposed method in terms of both task completion and efficiency. The code and trainable environments are available1.},
  archive   = {C_IROS},
  author    = {Zihan Wang and Jianwen Li and Nina Mahmoudian},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801487},
  month     = {10},
  pages     = {9976-9982},
  title     = {Synergistic reinforcement and imitation learning for vision-driven autonomous flight of UAV along river},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring latent pathways: Enhancing the interpretability of
autonomous driving with a variational autoencoder. <em>IROS</em>,
9968–9975. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous driving presents a complex challenge, which is usually addressed with artificial intelligence models that are end-to-end or modular in nature. Within the landscape of modular approaches, a bio-inspired neural circuit policy model has emerged as an innovative control module, offering a compact and inherently interpretable system to infer a steering wheel command from abstract visual features. Here, we take a leap forward by integrating a variational autoencoder with the neural circuit policy controller, forming a solution that directly generates steering commands from input camera images. By substituting the traditional convolutional neural network approach to feature extraction with a variational autoencoder, we enhance the system’s interpretability, enabling a more transparent and understandable decision-making process.In addition to the architectural shift toward a variational autoencoder, this study introduces the automatic latent perturbation tool, a novel contribution designed to probe and elucidate the latent features within the variational autoencoder. The automatic latent perturbation tool automates the interpretability process, offering granular insights into how specific latent variables influence the overall model’s behavior. Through a series of numerical experiments, we demonstrate the interpretative power of the variational autoencoder-neural circuit policy model and the utility of the automatic latent perturbation tool in making the inner workings of autonomous driving systems more transparent.},
  archive   = {C_IROS},
  author    = {Anass Bairouk and Mirjana Maras and Simon Herlin and Alexander Amini and Marc Blanchon and Ramin Hasani and Patrick Chareyre and Daniela Rus},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801915},
  month     = {10},
  pages     = {9968-9975},
  title     = {Exploring latent pathways: Enhancing the interpretability of autonomous driving with a variational autoencoder},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Path-parameterised RRTs for underactuated systems.
<em>IROS</em>, 9960–9967. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a sample-based motion planning algorithm specialised to a class of underactuated systems using path parameterisation. The structure this class presents under a path parameterisation enables the trivial computation of dynamic feasibility along a path. Using this, a specialised state-based steering mechanism within an RRT motion planning algorithm is developed, enabling the generation of both geometric paths and their time parameterisations without introducing excessive computational overhead. We find with two systems that our algorithm computes feasible trajectories with higher rates of success and lower mean computation times compared to existing approaches.},
  archive   = {C_IROS},
  author    = {Damian Abood and Ian R. Manchester},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802123},
  month     = {10},
  pages     = {9960-9967},
  title     = {Path-parameterised RRTs for underactuated systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Camera-based belief space planning in discrete
partially-observable domains. <em>IROS</em>, 9953–9959. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots often have to operate in discrete partially observable worlds, where the state of the world is only observable at runtime. To react to different world states, robots need contingencies. To find contingencies, prior work developed the path tree optimization (PTO) method, which computes motion contingencies by constructing a tree of motion paths in belief space. In this paper, we extend upon PTO by enabling camera-based belief space planning through an extension of the open motion planning library (OMPL). By leveraging this extension, we develop an improved camera-based state sampler and an efficient open-source implementation of PTO. This version of PTO supports a virtual camera, non-euclidean state spaces, and different state samplers. We evaluate this improved version of PTO on four realistic scenarios with a virtual camera in up to 10-dimensional state spaces. In our evaluations, we compare PTO both with a default and with the new camera-based state sampler. The results indicate that the camera-based state sampler improves success rates in 3 out of 4 scenarios while having a significant lower memory footprint. Our work thus makes an important step in advancing belief-space planning and provides researchers with an open source tool to use, modify, and benchmark belief-space planning methods.},
  archive   = {C_IROS},
  author    = {Janis Eric Freund and Camille Phiquepal and Andreas Orthey and Marc Toussaint},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802363},
  month     = {10},
  pages     = {9953-9959},
  title     = {Camera-based belief space planning in discrete partially-observable domains},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The effectiveness of state representation model in
multi-agent proximal policy optimization for multi-agent path finding.
<em>IROS</em>, 9947–9952. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent pathfinding plays a crucial role in various robot applications. Recently, deep reinforcement learning methods have been adopted to solve large-scale planning problems in a decentralized manner. Nonetheless, such approaches pose challenges such as non-stationarity and partial observability. In this paper, we address these challenges by integrating a state representation model into a multi-agent proximal policy optimization framework. To do so, we propose to utilize a state representation model which extracts representation features from the global map and leverages this information to enhance the training process. Our approach involves decoupling the feature extractor from the agent training process, enabling a more accurate representation of the global state that remains unbiased by the actions of the agents. Furthermore, our modularized approach offers the flexibility to replace the representation model with another model or modify tasks within the global map, without the retraining of the agents. We demonstrated the effectiveness of our approach by comparing three multi-agent proximal policy optimization frameworks. Our experimental results demonstrate that our approach improves the average episode reward compared to the other approaches.},
  archive   = {C_IROS},
  author    = {Jaehoon Chung and Jamil Fayyad and Mehran Ghafarian Tamizi and Homayoun Najjaran},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802643},
  month     = {10},
  pages     = {9947-9952},
  title     = {The effectiveness of state representation model in multi-agent proximal policy optimization for multi-agent path finding},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Trajectory planning for non-prehensile object
transportation. <em>IROS</em>, 9939–9946. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Non-prehensile transportation of unstable objects presents a challenging task in robotics. To ensure the success of the transportation, it is necessary to consider both the object’s stability via contact dynamics and the motion constraints of the robot. We propose two novel trajectory planning methods derived from sampling and dynamic programming algorithms, tested on a 7-DoF Franka Emika robot against common strategies like Model Predictive Control (MPC) and S-curve planning, particularly under the constraint of a non-rotating tray. The results demonstrate the effectiveness of our methodologies in improving transportation speed. This research contributes to advancements in robotic manipulation techniques by tackling non-prehensile manipulation of dynamically unstable objects.},
  archive   = {C_IROS},
  author    = {Lingyun Chen and Haoyu Yu and Liding Zhang and Abdeldjallil Naceri and Abdalla Swikir and Sami Haddadin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801587},
  month     = {10},
  pages     = {9939-9946},
  title     = {Trajectory planning for non-prehensile object transportation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A general formulation for path constrained time-optimized
trajectory planning with environmental and object contacts.
<em>IROS</em>, 9931–9938. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A typical manipulation task consists of a manipulator equipped with a gripper to grasp and move an object with constraints on the motion of the hand-held object, which may be due to the nature of the task itself or from object-environment contacts. In this paper, we study the problem of computing joint torques and grasping forces for time-optimal motion of an object, while ensuring that the grasp is not lost and any constraints on the motion of the object, either due to dynamics, environment contact, or no-slip requirements, are also satisfied. We present a second-order cone program (SOCP) formulation of the time-optimal trajectory planning problem that considers nonlinear friction cone constraints at the hand-object and object-environment contacts. Since SOCPs are convex optimization problems that can be solved optimally in polynomial time using interior point methods, we can solve the trajectory optimization problem efficiently. We present simulation results on three examples, including a non-prehensile manipulation task, which shows the generality and effectiveness of our approach.},
  archive   = {C_IROS},
  author    = {Dasharadhan Mahalingam and Aditya Patankar and Riddhiman Laha and Srinivasan Lakshminarayanan and Sami Haddadin and Nilanjan Chakraborty},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801794},
  month     = {10},
  pages     = {9931-9938},
  title     = {A general formulation for path constrained time-optimized trajectory planning with environmental and object contacts},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Task-driven manipulation with reconfigurable parallel
robots. <em>IROS</em>, 9924–9930. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {ReachBot, a proposed robotic platform, employs extendable booms as limbs for mobility in challenging environments, such as martian caves. When attached to the environment, ReachBot acts as a parallel robot, with reconfiguration driven by the ability to detach and re-place the booms. This ability enables manipulation-focused scientific objectives: for instance, through operating tools, or handling and transporting samples. To achieve these capabilities, we develop a two-part solution, optimizing for robustness against task uncertainty and stochastic failure modes. First, we present a mixed-integer stance planner to determine the positioning of ReachBot’s booms to maximize the task wrench space about the nominal point(s). Second, we present a convex tension planner to determine boom tensions for the desired task wrenches, accounting for the probabilistic nature of microspine grasping. We demonstrate improvements in key robustness metrics from the field of dexterous manipulation, and show a large increase in the volume of the manipulation workspace. Finally, we employ Monte-Carlo simulation to validate the robustness of these methods, demonstrating good performance across a range of randomized tasks and environments, and generalization to cable-driven morphologies. We make our code available at our project webpage, https://stanfordasl.github.io/reachbot_manipulation/},
  archive   = {C_IROS},
  author    = {Daniel Morton and Mark Cutkosky and Marco Pavone},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801313},
  month     = {10},
  pages     = {9924-9930},
  title     = {Task-driven manipulation with reconfigurable parallel robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ContactHandover: Contact-guided robot-to-human object
handover. <em>IROS</em>, 9916–9923. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot-to-human object handover is an important step in many human robot collaboration tasks. A successful handover requires the robot to maintain a stable grasp on the object while making sure the human receives the object in a natural and easy-to-use manner. We propose ContactHandover, a robot to human handover system that consists of two phases: a contact-guided grasping phase and an object delivery phase. During the grasping phase, ContactHandover predicts both 6DoF robot grasp poses and a 3D affordance map of human contact points on the object. The robot grasp poses are reranked by penalizing those that block human contact points, and the robot executes the highest ranking grasp. During the delivery phase, the robot end effector pose is computed by maximizing human contact points close to the human while minimizing the human arm joint torques and displacements. We evaluate our system on 27 diverse household objects and show that our system achieves better visibility and reachability of human contacts to the receiver compared to several baselines. More results can be found on the project website.},
  archive   = {C_IROS},
  author    = {Zixi Wang and Zeyi Liu and Nicolas Ouporov and Shuran Song},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801777},
  month     = {10},
  pages     = {9916-9923},
  title     = {ContactHandover: Contact-guided robot-to-human object handover},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Clutter-aware spill-free liquid transport via learned
dynamics. <em>IROS</em>, 9908–9915. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we present a novel algorithm to perform spill-free handling of open-top liquid-filled containers that operates in cluttered environments. By allowing liquid-filled containers to be tilted at higher angles and enabling motion along all axes of end-effector orientation, our work extends the reachable space and enhances maneuverability around obstacles, broadening the range of feasible scenarios. Our key contributions include: i) generating spill-free paths through the use of RRT* with an informed sampler that leverages container properties to avoid spill-inducing states (such as an upside-down container), ii) parameterizing the resulting path to generate spill-free trajectories through the implementation of a time parameterization algorithm, coupled with a transformer-based machine-learning model capable of classifying trajectories as spill-free or not. We validate our approach in real-world, obstacle-rich task settings using containers of various shapes and fill levels and demonstrate an extended solution space that is at least 3x larger than an existing approach.},
  archive   = {C_IROS},
  author    = {Ava Abderezaei and Anuj Pasricha and Alex Klausenstock and Alessandro Roncone},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802215},
  month     = {10},
  pages     = {9908-9915},
  title     = {Clutter-aware spill-free liquid transport via learned dynamics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Combining sampling- and gradient-based planning for
contact-rich manipulation. <em>IROS</em>, 9901–9907. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Planning for contact-rich manipulation involves discontinuous dynamics, which presents challenges to planning methods. Sampling-based planners have higher sample complexity in high-dimensional problems and cannot efficiently handle state constraints such as force limits. Gradient-based solvers can suffer from local optima and their convergence rate is often worse on non-smooth problems. We propose a planning method that is both sampling- and gradient-based, using the Cross-entropy Method to initialize a gradient-based solver, providing better initialization to the gradient-based method and allowing explicit handling of state constraints. The sampling-based planner also allows direct integration of a particle filter, which is here used for online contact mode estimation. The approach is shown to improve performance in MuJoCo environments and the effects of problem stiffness and planing horizon are investigated. The estimator and planner are then applied to an impedance-controlled robot, showing a reduction in solve time in contact transitions to only gradient-based.},
  archive   = {C_IROS},
  author    = {Filippo Rozzi and Loris Roveda and Kevin Haninger},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802369},
  month     = {10},
  pages     = {9901-9907},
  title     = {Combining sampling- and gradient-based planning for contact-rich manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UNO push: Unified nonprehensile object pushing via
non-parametric estimation and model predictive control. <em>IROS</em>,
9893–9900. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Nonprehensile manipulation through precise pushing is an essential skill that has been commonly challenged by perception and physical uncertainties, such as those associated with contacts, object geometries, and physical properties. For this, we propose a unified framework that jointly addresses system modeling, action generation, and control. While most existing approaches either heavily rely on a priori system information for analytic modeling, or leverage a large dataset to learn dynamic models, our framework approximates a system transition function via non-parametric learning only using a small number of exploratory actions (ca. 10). The approximated function is then integrated with model predictive control to provide precise pushing manipulation. Furthermore, we show that the approximated system transition functions can be robustly transferred across novel objects while being online updated to continuously improve the manipulation accuracy. Through extensive experiments on a real robot platform with a set of novel objects and comparing against a state-of-the-art baseline, we show that the proposed unified framework is a light-weight and highly effective approach to enable precise pushing manipulation all by itself. Our evaluation results illustrate that the system can robustly ensure millimeter-level precision and can straightforwardly work on any novel object.},
  archive   = {C_IROS},
  author    = {Gaotian Wang and Kejia Ren and Kaiyu Hang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802843},
  month     = {10},
  pages     = {9893-9900},
  title     = {UNO push: Unified nonprehensile object pushing via non-parametric estimation and model predictive control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Grasping trajectory optimization with point clouds.
<em>IROS</em>, 9885–9892. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a new trajectory optimization method for robotic grasping based on a point-cloud representation of robots and task spaces. In our method, robots are represented by 3D points on their link surfaces. The task space of a robot is represented by a point cloud that can be obtained from depth sensors. Using the point-cloud representation, goal reaching in grasping can be formulated as point matching, while collision avoidance can be efficiently achieved by querying the signed distance values of the robot points in the signed distance field of the scene points. Consequently, a constrained nonlinear optimization problem is formulated to solve the joint motion and grasp planning problem. The advantage of our method is that the point-cloud representation is general to be used with any robot in any environment. We demonstrate the effectiveness of our method by performing experiments on a tabletop scene and a shelf scene for grasping with a Fetch mobile manipulator and a Franka Panda arm.1},
  archive   = {C_IROS},
  author    = {Yu Xiang and Sai Haneesh Allu and Rohith Peddi and Tyler Summers and Vibhav Gogate},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802826},
  month     = {10},
  pages     = {9885-9892},
  title     = {Grasping trajectory optimization with point clouds},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Self-reconfiguration strategies for space-distributed
spacecraft. <em>IROS</em>, 9879–9884. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a distributed on-orbit spacecraft assembly algorithm, where future spacecraft can assemble modules with different functions on orbit to form a spacecraft structure with specific functions. This form of spacecraft organization has the advantages of reconfigurability, fast mission response and easy maintenance. Reasonable and efficient on-orbit self-reconfiguration algorithms play a crucial role in realizing the benefits of distributed spacecraft. This paper adopts the framework of imitation learning combined with reinforcement learning for strategy learning of module handling order. A robot arm motion algorithm is then designed to execute the handling sequence. We achieve the self-reconfiguration handling task by creating a map on the surface of the module, completing the path point planning of the robotic arm using A*. The joint planning of the robotic arm is then accomplished through forward and reverse kinematics. Finally, the results are presented in Unity3D.},
  archive   = {C_IROS},
  author    = {Tianle Liu and Zhixiang Wang and Yongwei Zhang and Ziwei Wang and Zihao Liu and Yizhai Zhang and Panfeng Huang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802829},
  month     = {10},
  pages     = {9879-9884},
  title     = {Self-reconfiguration strategies for space-distributed spacecraft},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Demonstration to adaptation: A user-guided framework for
sequential and real-time planning. <em>IROS</em>, 9871–9878. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a comprehensive user-guided planning framework designed for robots operating in dynamic, human-centered environments – where the ability to execute sequential tasks flexibly and adaptively is paramount. Our planner enables robots to (i) encode object-centric constraints and user preferences via multiple demonstrations, (ii) transfer geometric features and implicit relaxations to novel scenarios while reacting to unforeseen events, and (iii) adapt to changing task conditions in real-time, including the real-time replanning and tracking of moving targets. Our approach relies on C1 screw linear interpolation, which generates smooth paths satisfying the underlying geometric constraints that characterize the task. The prescribed path is combined with a hierarchical quadratic programming-based controller which explores the user demonstrations&#39;s stochastic variability to relax task constraints while ensuring real-time whole-body collision avoidance. Our framework continuously checks for dynamic changes in task targets, ensuring appropriate planning or control actions, and tending to the prescribed screw path. This comprehensive approach is deployed in different task conditions which are available at https://youtu.be/F0cMr1n1D9k.},
  archive   = {C_IROS},
  author    = {Kuanqi Cai and Riddhiman Laha and Yuhe Gong and Lingyun Chen and Liding Zhang and Luis F.C. Figueredo and Sami Haddadin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802661},
  month     = {10},
  pages     = {9871-9878},
  title     = {Demonstration to adaptation: A user-guided framework for sequential and real-time planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Using graphs of convex sets to guide nonconvex trajectory
optimization. <em>IROS</em>, 9863–9870. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collision-free motion planning with trajectory optimization is inherently nonconvex. Some of this nonconvexity is fundamental: the robot might need to make a discrete decision to go left around an obstacle or right around an obstacle. Some of the nonconvexity is potentially more benign: we might want to penalize high-order derivatives of our continuous trajectories in order to encourage smoothness. Recently, Graphs of Convex Sets (GCS) have been applied to trajectory optimization, addressing the fundamental nonconvexity with efficient online optimization over a &quot;roadmap&quot; represented by an approximate convex decomposition of the configuration space. In this paper, we explore some of the most useful nonconvex costs and constraints and the suitability of combining convex &quot;global&quot; optimization using GCS with nonconvex trajectory optimization for rounding the local solutions. We find that for many applications, this combination can lead to a small number of nonconvex optimizations finding extremely good solutions to the nonconvex trajectory optimization problem.},
  archive   = {C_IROS},
  author    = {David von Wrangel and Russ Tedrake},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802426},
  month     = {10},
  pages     = {9863-9870},
  title     = {Using graphs of convex sets to guide nonconvex trajectory optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ontology based AI planning and scheduling for robotic
assembly. <em>IROS</em>, 9855–9862. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The rising demand for customized products necessitates the integration of multiple robotic systems, underscoring the need for advanced production planning and scheduling. This paper introduces an ontology-based, artificial intelligence-enhanced method for dynamic task planning and scheduling, aimed at improving the efficiency of production process, reducing machine downtime, and consequently increasing throughput in assembly operations. Designed to generate and execute feasible production plans dynamically, this method minimizes manual planning and scheduling efforts. We evaluate its effectiveness using two gear assembly use cases with various robot skills, highlighting its flexibility in planning and scheduling and its contributions to the evolution of smart manufacturing. The method’s adaptability suggests its applicability across diverse smart factory environments.},
  archive   = {C_IROS},
  author    = {Jingyun Zhao and Birgit Vogel-Heuser and Jicong Ao and Yansong Wu and Liding Zhang and Fandi Hartl and Dominik Hujo and Zhenshan Bing and Fan Wu and Alois Knoll and Sami Haddadin and Bernd Vojanec and Timo Markert and André Kraft},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802295},
  month     = {10},
  pages     = {9855-9862},
  title     = {Ontology based AI planning and scheduling for robotic assembly},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal robotic assembly sequence planning (ORASP): A
sequential decision-making approach. <em>IROS</em>, 9847–9854. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The optimal robotic assembly planning problem entails determining the sequence of actions for a robot to feasibly assemble a product from its components which minimizes a given objective. This problem is made especially challenging as the number of potential sequences increase exponentially with respect to the number of parts in the assembly. Additionally, the optimal sequence must also consider and satisfy a selection of constraints such as attachment precedence or a maximum robot carry weight. Traditionally, robotic assembly planning problems have been solved using heuristics, but these methods are specific to a given robot or cost structure. In this paper, we propose to model robotic assembly planning as a decision-making problem which enables us to use tools from shortest path algorithms and reinforcement learning. We formulate assembly sequencing as a Markov Decision Process and use Dynamic Programming (DP) to find optimal assembly policies that far outperform the state-of-the-art in terms of speed. We further exploit the deterministic nature of assembly planning to introduce a class of optimal Graph Exploration Assembly Planners (GEAPs) and even propose our own ORASP Search Method. We further showcase how we can produce high-reward assembly plans for larger structures using our deep Reinforcement Learning (RL) method. We evaluate this method on large robotic assembly problems such as the assembly of the Hubble Space Telescope, the International Space Station, and the James Webb Space Telescope. We further discuss how our DP, GEAP, and RL methods are capable of finding optimal solutions under a variety of different objective functions and how we translate any form of precedence constraints to branch pruning and further improve performance. We have published our code at https://github.com/labicon/ORASP-Code.},
  archive   = {C_IROS},
  author    = {Kartik Nagpal and Negar Mehr},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802475},
  month     = {10},
  pages     = {9847-9854},
  title     = {Optimal robotic assembly sequence planning (ORASP): A sequential decision-making approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EVSMap: An efficient volumetric-semantic mapping approach
for embedded systems. <em>IROS</em>, 9839–9846. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite significant progress in perception tasks such as 3D scene mapping and semantic information extraction using SLAM and deep learning, applying these techniques within computationally constrained embedded systems remains a challenge. In this work, we introduce a novel end-to-end framework for efficient and real-time volumetric-semantic mapping. We have developed a lightweight and robust RGB-D segmentation network for extracting semantic information. Through the introduction of three distinct modules—CFIM, DAPPF, and LAD—our network significantly enhances real-time performance while achieving Mean Intersection over Union (MIoU) scores comparable to state-of-the-art (SOTA) models. Our model reduces the parameters by 8 to 26 times compared to similar networks and improves inference speed by 2 to 3 times. Additionally, we improved a multi-class bayesian updating strategy by refining penalty function to reduce the memory size of the semantic map and enhance the mapping speed. Compared with other volumetric-semantic mapping approaches, our work maintains the same level of detail in semantic information representation, while increasing mapping speed by 1.3 to 9.6 times and reducing memory size of the map by up to 2.6 times. Finally, we applied our work to real-world mobile robot exploration scenarios, demonstrating the efficiency of the proposed framework.},
  archive   = {C_IROS},
  author    = {Jiyuan Qiu and Chen Jiang and Pengfei Zhang and Haowen Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801849},
  month     = {10},
  pages     = {9839-9846},
  title     = {EVSMap: An efficient volumetric-semantic mapping approach for embedded systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantic layering in room segmentation via LLMs.
<em>IROS</em>, 9831–9838. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce Semantic Layering in Room Segmentation via LLMs (SeLRoS), an advanced method for semantic room segmentation by integrating Large Language Models (LLMs) with traditional 2D map-based segmentation. Unlike previous approaches that solely focus on the geometric segmentation of indoor environments, our work enriches segmented maps with semantic data, including object identification and spatial relationships, to enhance robotic navigation. By leveraging LLMs, we provide a novel framework that interprets and organizes complex information about each segmented area, thereby improving the accuracy and contextual relevance of room segmentation. Furthermore, SeLRoS overcomes the limitations of existing algorithms by using a semantic evaluation method to accurately distinguish true room divisions from those erroneously generated by furniture and segmentation inaccuracies. The effectiveness of SeLRoS is verified through its application across 30 different 3D environments. Source code and experiment videos for this work are available at: https://sites.google.com/view/selros.},
  archive   = {C_IROS},
  author    = {Taehyeon Kim and Byung-Cheol Min},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801361},
  month     = {10},
  pages     = {9831-9838},
  title     = {Semantic layering in room segmentation via LLMs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Volumetric mapping with panoptic refinement using kernel
density estimation for mobile robots. <em>IROS</em>, 9824–9830. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reconstructing three-dimensional (3D) scenes with semantic understanding is vital in many robotic applications. Robots need to identify which objects, along with their positions and shapes, to manipulate them precisely with given tasks. Mobile robots, especially, usually use lightweight networks to segment objects on RGB images and then localize them via depth maps; however, they often encounter out-of-distribution scenarios where masks over-cover the objects. In this paper, we address the problem of panoptic segmentation quality in 3D scene reconstruction by refining segmentation errors using non-parametric statistical methods. To enhance mask precision, we map the predicted masks into a depth frame to estimate their distribution via kernel densities. The outliers in depth perception are then rejected without the need for additional parameters in an adaptive manner to out-of-distribution scenarios, followed by 3D reconstruction using projective signed distance functions (SDFs). We validate our method on a synthetic dataset, which shows improvements in both quantitative and qualitative results for panoptic mapping. Through real-world testing, the results furthermore show our method’s capability to be deployed on a real-robot system. Our source code is available at: https://github.com/mkhangg/refined_panoptic_mapping.},
  archive   = {C_IROS},
  author    = {Khang Nguyen and Tuan Dang and Manfred Huber},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802224},
  month     = {10},
  pages     = {9824-9830},
  title     = {Volumetric mapping with panoptic refinement using kernel density estimation for mobile robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SePaint: Semantic map inpainting via multinomial diffusion.
<em>IROS</em>, 9817–9823. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Prediction beyond partial observations is crucial for robots to navigate in unknown environments because it can provide extra information regarding the surroundings beyond the current sensing range or resolution. In this work, we consider the inpainting of semantic Bird’s-Eye-View maps. We propose SePaint, an inpainting model for semantic data based on generative multinomial diffusion. To maintain semantic consistency, we need to condition the prediction for the missing regions on the known regions. We propose a novel and efficient condition strategy, Look-Back Condition (LB-Con), which performs one-step look-back operations during the reverse diffusion process. By doing so, we are able to strengthen the harmonization between unknown and known parts, leading to better completion performance. We have conducted extensive experiments on different datasets, showing our proposed model outperforms commonly used interpolation methods in various robotic applications.},
  archive   = {C_IROS},
  author    = {Zheng Chen and Deepak Duggirala and David Crandall and Lei Jiang and Lantao Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802192},
  month     = {10},
  pages     = {9817-9823},
  title     = {SePaint: Semantic map inpainting via multinomial diffusion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EMBOSR: Embodied spatial reasoning for enhanced situated
question answering in 3D scenes. <em>IROS</em>, 9811–9816. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D Embodied Spatial Reasoning, emphasizing an agent’s interaction with its surroundings for spatial information inference, is adeptly facilitated by the process of Situated Question Answering in 3D Scenes (SQA3D). SQA3D requires an agent to comprehend its position and orientation within a 3D scene based on a textual situation and then utilize this understanding to answer questions about the surrounding environment in that context. Previous methods in this field face substantial challenges, including a dependency on constant retraining on limited datasets, which leads to poor performance in unseen scenarios, limited expandability, and inadequate generalization. To address these challenges, we present a new embodied spatial reasoning paradigm for enhanced SQA3D, fusing the capabilities of foundation models with the chain of thought methodology. This approach is designed to elevate adaptability and scalability in a wide array of 3D environments. A new aspect of our model is the integration of a chain of thought reasoning process, which significantly augments the model’s capability for spatial reasoning and complex query handling in diverse 3D environments. In our structured experiments, we compare our approach against other methods with varying architectures, demonstrating its efficacy in multiple tasks including SQA3D and 3D captioning. We also assess the informativeness contained in the generated answers for complex queries. Ablation studies further delineate the individual contributions of our method to its overall performance. The results consistently affirm our proposed method’s effectiveness and efficiency.},
  archive   = {C_IROS},
  author    = {Yu Hao and Fan Yang and Nicholas Fang and Yu-Shen Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801720},
  month     = {10},
  pages     = {9811-9816},
  title     = {EMBOSR: Embodied spatial reasoning for enhanced situated question answering in 3D scenes},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning high-level semantic-relational concepts for SLAM.
<em>IROS</em>, 9803–9810. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent works on SLAM extend their pose graphs with higher-level semantic concepts like Rooms exploiting relationships between them, to provide, not only a richer representation of the situation/environment but also to improve the accuracy of its estimation. Concretely, our previous work, Situational Graphs (S-Graphs+), a pioneer in jointly leveraging semantic relationships in the factor optimization process, relies on semantic entities such as Planes and Rooms, whose relationship is mathematically defined. Nevertheless, there is no unique approach to finding all the hidden patterns in lower-level factor-graphs that correspond to high-level concepts of different natures. It is currently tackled with ad-hoc algorithms, which limits its graph expressiveness.To overcome this limitation, in this work, we propose an algorithm based on Graph Neural Networks for learning high-level semantic-relational concepts that can be inferred from the low-level factor graph. Given a set of mapped Planes our algorithm is capable of inferring Room entities relating to the Planes. Additionally, to demonstrate the versatility of our method, our algorithm can infer an additional semantic-relational concept, i.e. Wall, and its relationship with its Planes. We validate our method in both simulated and real datasets demonstrating improved performance over two baseline approaches. Furthermore, we integrate our method into the S-Graphs+ algorithm providing improved pose and map accuracy compared to the baseline while further enhancing the scene representation.},
  archive   = {C_IROS},
  author    = {Jose Andres Millan-Romera and Hriday Bavle and Muhammad Shaheer and Martin R. Oswald and Holger Voos and Jose Luis Sanchez-Lopez},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801511},
  month     = {10},
  pages     = {9803-9810},
  title     = {Learning high-level semantic-relational concepts for SLAM},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging computation of expectation models for commonsense
affordance estimation on 3D scene graphs. <em>IROS</em>, 9797–9802. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This article studies the commonsense object affordance concept for enabling close-to-human task planning and task optimization of embodied robotic agents in urban environments. The focus of the object affordance is on reasoning how to effectively identify object’s inherent utility during the task execution, which in this work is enabled through the analysis of contextual relations of sparse information of 3D scene graphs. The proposed framework develops a Correlation Information (CECI) model to learn probability distributions using a Graph Convolutional Network, allowing to extract the commonsense affordance for individual members of a semantic class. The overall framework was experimentally validated in a real-world indoor environment, showcasing the ability of the method to level with human commonsense. For a video of the article, showcasing the experimental demonstration, please refer to the following link: https://youtu.be/BDCMVx2GiQE},
  archive   = {C_IROS},
  author    = {Mario A.V. Saucedo and Nikolaos Stathoulopoulos and Akash Patel and Christoforos Kanellakis and George Nikolakopoulos},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802560},
  month     = {10},
  pages     = {9797-9802},
  title     = {Leveraging computation of expectation models for commonsense affordance estimation on 3D scene graphs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DiffPrompter: Differentiable implicit visual prompts for
semantic-segmentation in adverse conditions. <em>IROS</em>, 9791–9796.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Semantic segmentation in adverse weather scenarios is a critical task for autonomous driving systems. While foundation models have shown promise, the need for specialized adaptors becomes evident for handling more challenging scenarios. We introduce DiffPrompter, a novel differentiable visual and latent prompting mechanism aimed at expanding the learning capabilities of existing adaptors in foundation models. Our proposed ∇HFC (High Frequency Components) based image processing block excels particularly in adverse weather conditions, where conventional methods often fall short. Furthermore, we investigate the advantages of jointly training visual and latent prompts, demonstrating that this combined approach significantly enhances performance in out-of-distribution scenarios. Our differentiable visual prompts leverage parallel and series architectures to generate prompts, effectively improving object segmentation tasks in adverse conditions. Through a comprehensive series of experiments and evaluations, we provide empirical evidence to support the efficacy of our approach. Project page: diffprompter.github.io},
  archive   = {C_IROS},
  author    = {Sanket Kalwar and Mihir Ungarala and Shruti Jain and Aaron Monis and Krishna Reddy Konda and Sourav Garg and K Madhava Krishna},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802718},
  month     = {10},
  pages     = {9791-9796},
  title     = {DiffPrompter: Differentiable implicit visual prompts for semantic-segmentation in adverse conditions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weakly scene segmentation using efficient transformer.
<em>IROS</em>, 9784–9790. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current methods for large-scale point cloud scene semantic segmentation rely on manually annotated dense point-wise labels, which are costly, labor-intensive, and prone to errors. Consequently, gathering point cloud scenes with billions of labeled points is impractical in real-world scenarios. In this paper, we introduce a novel weak supervision approach to semantically segment large-scale indoor scenes, requiring only 1‰ of the points to be labeled. Specifically, we develop an efficient point neighbor Transformer to capture the geometry of local point cloud patches. To address the quadratic complexity of self-attention computation in Transformers, particularly for large-scale point clouds, we propose approximating the self-attention matrix using low-rank and sparse decomposition. Building on the point neighbor Transformer as foundational blocks, we design a Low-rank Sparse Transformer Network (LST-Net) for weakly supervised large-scale point cloud scene semantic segmentation. Experimental results on two commonly used indoor point cloud scene segmentation benchmarks demonstrate that our model achieves performance comparable to those of both weakly supervised and fully supervised methods. Our code can be found in https://github.com/hhuang-code/LST-Net.},
  archive   = {C_IROS},
  author    = {Hao Huang and Shuaihang Yuan and CongCong Wen and Yu Hao and Yi Fang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802479},
  month     = {10},
  pages     = {9784-9790},
  title     = {Weakly scene segmentation using efficient transformer},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Indoor scene change understanding (SCU): Segment, describe,
and revert any change. <em>IROS</em>, 9777–9783. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Understanding of scene changes is crucial for embodied AI applications, such as visual room rearrangement, where the agent must revert changes by restoring the objects to their original locations or states. Visual changes between two scenes, pre- and post-rearrangement, encompass two tasks: scene change detection (locating changes) and image difference captioning (describing changes). While previous methods, focused on sequential 2D images, have addressed these tasks separately, it is essential to emphasize the significance of their combination. Therefore, we propose a new Scene Change Understanding (SCU) task for simultaneous change detection and description. Moreover, we go beyond change language description generation and aim to generate rearrangement instructions for the robotic agent to revert changes. To solve this task, we propose a novel method - EmbSCU, which allows to compare instance-level change object masks (for 53 frequently-seen indoor object classes) before and after changes and generate rearrangement language instructions for the agent. EmbSCU is built on our Segment Any Object Model (SAOMv2) - a fine-tuned version of Segment Anything Model (SAM), adapted to obtain instance-level object masks for both foreground and background objects in indoor embodied environments. EmbSCU is evaluated on our own dataset of sequential 2D image pairs before and after changes, collected from the Ai2Thor simulator. The proposed framework achieves promising results in both change detection and change description. Moreover, EmbSCU demonstrates positive generalization results on real-world scenes without using any real-life data during training. The dataset and the code are available here.},
  archive   = {C_IROS},
  author    = {Mariia Khan and Yue Qiu and Yuren Cong and Bodo Rosenhahn and David Suter and Jumana Abu-Khalaf},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801354},
  month     = {10},
  pages     = {9777-9783},
  title     = {Indoor scene change understanding (SCU): Segment, describe, and revert any change},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DVT: Decoupled dual-branch view transformation for monocular
bird’s eye view semantic segmentation. <em>IROS</em>, 9769–9776. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Monocular Bird’s Eye View (BEV) semantic segmentation is critical for autonomous driving for its inherent advantages in spatial representation and downstream tasks. However, it is challenging to simultaneously learn view transformation and pixel-wise classification. Previous works suffer from non-flat region distortion, distant depth ambiguity, and visual occlusion. To address these aforementioned concerns, we propose dual-branch view transformation (DVT), a novel framework for monocular BEV semantic segmentation. Our method consists of: (i) A dual-branch view transformation to decouple features into flat region and non-flat region and process them independently. (ii) A depth-aware weighting method to make the model pay more attention to the distant depth. (iii) An auxiliary task to introduce more inductive biases to alleviate the inaccuracy caused by visual occlusion. Furthermore, we design a class-aware weighting method to address the class and size imbalance of datasets. Experimental results on nuScenes and KITTI-360 datasets demonstrate that DVT outperforms previous state-of-the-art (SOTA). Our codes are available at https://github.com/MrPicklesGG/DVT.},
  archive   = {C_IROS},
  author    = {Jiayuan Du and Xianghui Pan and Mengjiao Shen and Shuai Su and Jingwei Yang and Chengju Liu and Qijun Chen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802126},
  month     = {10},
  pages     = {9769-9776},
  title     = {DVT: Decoupled dual-branch view transformation for monocular bird’s eye view semantic segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A context-enhanced full-resolution floor plan segmentation
network for topological semantic mapping. <em>IROS</em>, 9761–9768. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Topological semantic maps provide a practical solution to enhance indoor navigation for the Partially Sighted or Visually Impaired (PSVI). Segmenting indoor floor plans and extracting boundaries are key to constructing these maps. The existing methods exhibit low accuracy in segmentation. To achieve desired high segmentation accuracy, we introduce a Context-Enhanced Full-Resolution Network (CEFRN) for floor plan segmentation. It is designed to harness the shallow detailed features and inter-category contextual dependencies inherent in floor plans. CEFRN integrates modified residual blocks to capture the low-stage full-resolution features while maintaining its compactness. A position attention module is employed to refine the deep-stage contextual information. We also propose a two-dimensional deep supervision method to merge features from both stages, which significantly boosts the feature representation ability of CEFRN. Finally, a practical topological semantic mapping method for PSVI indoor navigation is introduced. Experimental results demonstrate that CEFRN’s segmentation accuracy well exceeds the state-of-the-art methods’. It can be used to well support accurate topological semantic mapping.},
  archive   = {C_IROS},
  author    = {Zhengcai Cao and Yiyang Sun and Zhe Ma and MengChu Zhou},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801649},
  month     = {10},
  pages     = {9761-9768},
  title     = {A context-enhanced full-resolution floor plan segmentation network for topological semantic mapping},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A language-driven navigation strategy integrating semantic
maps and large language models. <em>IROS</em>, 9753–9760. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate perception of semantic and spatial information is crucial for robots performing language-driven navigation tasks. Existing approaches utilize visual-language models to extract semantic information from the environment and construct maps. However, constrained by the generalization and accuracy of these models themselves, the constructed maps may not be accurate and comprehensive, thereby affecting the accuracy of navigation tasks. Inspired by foundational models’ outstanding classification and segmentation capabilities, this study introduces a semantic map constructed using foundational models. We leverage a foundational model to semantically segment objects in the robot’s video stream and fuse semantics onto the map. Furthermore, this map is used in conjunction with large language models (LLMs) that receive natural language instructions to complete the navigation task. A substantial number of experiments in a simulated environment demonstrate that our method outperforms existing ones in language-driven navigation tasks.},
  archive   = {C_IROS},
  author    = {Zhengjun Zhong and Ying He and Pengteng Li and Fei Yu and Fei Ma},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802631},
  month     = {10},
  pages     = {9753-9760},
  title     = {A language-driven navigation strategy integrating semantic maps and large language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual preference inference: An image sequence-based
preference reasoning in tabletop object manipulation. <em>IROS</em>,
9745–9752. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In robotic object manipulation, human preferences can often be influenced by the visual attributes of objects, such as color and shape. These properties play a crucial role in operating a robot to interact with objects and align with human intention. In this paper, we focus on the problem of inferring underlying human preferences from a sequence of raw visual observations in tabletop manipulation environments with a variety of object types, named Visual Preference Inference (VPI). To facilitate visual reasoning in the context of manipulation, we introduce the Chain-of-Visual-Residuals (CoVR) method. CoVR employs a prompting mechanism that describes the difference between the consecutive images (i.e., visual residuals) and incorporates such texts with a sequence of images to infer the user’s preference. This approach significantly enhances the ability to understand and adapt to dynamic changes in its visual environment during manipulation tasks. Furthermore, we incorporate such texts along with a sequence of images to infer the user’s preferences. Our method outperforms baseline methods in terms of extracting human preferences from visual sequences in both simulation and real-world environments. Code and videos are available at: https://joonhyung-lee.github.io/vpi/},
  archive   = {C_IROS},
  author    = {Joonhyung Lee and Sangbeom Park and Yongin Kwon and Jemin Lee and Minwook Ahn and Sungjoon Choi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801806},
  month     = {10},
  pages     = {9745-9752},
  title     = {Visual preference inference: An image sequence-based preference reasoning in tabletop object manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transcrib3D: 3D referring expression resolution through
large language models. <em>IROS</em>, 9737–9744. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {If robots are to work effectively alongside people, they must be able to interpret natural language references to objects in their 3D environment. Understanding 3D referring expressions is challenging—it requires the ability to both parse the 3D structure of the scene and correctly ground free-form language in the presence of distraction and clutter. We introduce Transcrib3D, an approach that brings together 3D detection methods and the emergent reasoning capabilities of large language models (LLMs). Transcrib3D uses text as the unifying medium, which allows us to sidestep the need to learn shared representations connecting multi-modal inputs, which would require massive amounts of annotated 3D data. As a demonstration of its effectiveness, Transcrib3D achieves state-of-the-art results on 3D reference resolution benchmarks, with a great leap in performance from previous multi-modality baselines. To improve upon zero-shot performance and facilitate local deployment on edge computers and robots, we propose self-correction for fine-tuning that trains smaller models, resulting in performance close to that of large models. We show that our method enables a real robot to perform pick-and-place tasks given queries that contain challenging referring expressions. Code will be available at https://ripl.github.io/Transcrib3D.},
  archive   = {C_IROS},
  author    = {Jiading Fang and Xiangshan Tan and Shengjie Lin and Igor Vasiljevic and Vitor Guizilini and Hongyuan Mei and Rares Ambrus and Gregory Shakhnarovich and Matthew R. Walter},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802793},
  month     = {10},
  pages     = {9737-9744},
  title     = {Transcrib3D: 3D referring expression resolution through large language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Object-oriented material classification and 3D clustering
for improved semantic perception and mapping in mobile robots.
<em>IROS</em>, 9729–9736. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Classification of different object surface material types can play a significant role in the decision-making algorithms for mobile robots and autonomous vehicles. RGB-based scene-level semantic segmentation has been well-addressed in the literature. However, improving material recognition using the depth modality and its integration with SLAM algorithms for 3D semantic mapping could unlock new potential benefits in the robotics perception pipeline. To this end, we propose a complementarity-aware deep learning approach for RGB-D-based material classification built on top of an object-oriented pipeline. The approach further integrates the ORB-SLAM2 method for 3D scene mapping with multiscale clustering of the detected material semantics in the point cloud map generated by the visual SLAM algorithm. Extensive experimental results with existing public datasets and newly contributed real-world robot datasets demonstrate a significant improvement in material classification and 3D clustering accuracy compared to state-of-the-art approaches for 3D semantic scene mapping.},
  archive   = {C_IROS},
  author    = {Siva Krishna Ravipati and Ehsan Latif and Ramviyas Parasuraman and Suchendra M. Bhandarkar},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801936},
  month     = {10},
  pages     = {9729-9736},
  title     = {Object-oriented material classification and 3D clustering for improved semantic perception and mapping in mobile robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ODTFormer: Efficient obstacle detection and tracking with
stereo cameras based on transformer. <em>IROS</em>, 9721–9728. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Obstacle detection and tracking represent a critical component in robot autonomous navigation. In this paper, we propose ODTFormer, a Transformer-based model that addresses both obstacle detection and tracking problems. For the detection task, our approach leverages deformable attention to construct a 3D cost volume, which is decoded progressively in the form of voxel occupancy grids. We further track the obstacles by matching the voxels between consecutive frames. The entire model can be optimized in an end-to-end manner. Through extensive experiments on DrivingStereo and KITTI benchmarks, our model achieves state-of-the-art performance in the obstacle detection task. We also report comparable accuracy to state-of-the-art obstacle tracking models while requiring only a fraction of their computation cost, typically ten-fold to twenty-fold less. Our code is available on https://github.com/neu-vi/ODTFormer.},
  archive   = {C_IROS},
  author    = {Tianye Ding and Hongyu Li and Huaizu Jiang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802476},
  month     = {10},
  pages     = {9721-9728},
  title     = {ODTFormer: Efficient obstacle detection and tracking with stereo cameras based on transformer},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Do one thing and do it well: Delegate responsibilities in
classical planning. <em>IROS</em>, 9714–9720. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel framework and algorithm for solving classical planning problems with an implicit hierarchical solver based on the principle of delegation. This framework, the Markov Intent Process, features a collection of skills that are each specialised to perform a single task well. Skills are aware of their intended effects and are able to analyse planning goals to delegate planning to the best-suited skill. This principle dynamically creates a hierarchy of plans, in which each skill plans for sub-goals for which it is specialised. Our method performs robustly in noisy environments with non-deterministic action effects and features on-demand execution—skill policies are only evaluated when needed. Plans are only generated at the highest level, then expanded and optimised when the latest state information is available. The high-level plan retains the initial planning intent and previously computed skills, effectively reducing the computation needed to adapt to environmental changes. We show this planning approach is experimentally very competitive to classic planning and reinforcement learning techniques on a variety of domains, both in terms of solution length and planning time.},
  archive   = {C_IROS},
  author    = {Tin Lai and Philippe Morere},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802245},
  month     = {10},
  pages     = {9714-9720},
  title     = {Do one thing and do it well: Delegate responsibilities in classical planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prompt-driven temporal domain adaptation for nighttime UAV
tracking. <em>IROS</em>, 9706–9713. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Nighttime UAV tracking under low-illuminated scenarios has achieved great progress by domain adaptation (DA). However, previous DA training-based works are deficient in narrowing the discrepancy of temporal contexts for UAV trackers. To address the issue, this work proposes a prompt-driven temporal domain adaptation training framework to fully utilize temporal contexts for challenging nighttime UAV tracking, i.e., TDA. Specifically, the proposed framework aligns the distribution of temporal contexts from daytime and nighttime domains by training the temporal feature generator against the discriminator. The temporal-consistent discriminator progressively extracts shared domain-specific features to generate coherent domain discrimination results in the time series. Additionally, to obtain high-quality training samples, a prompt-driven object miner is employed to precisely locate objects in unannotated nighttime videos. Moreover, a new benchmark for long-term nighttime UAV tracking is constructed. Exhaustive evaluations on both public and self-constructed nighttime benchmarks demonstrate the remarkable performance of the tracker trained in TDA framework, i.e., TDA-Track. Real-world tests at nighttime also show its practicality. The code and demo videos are available at https://github.com/vision4robotics/TDA-Track.},
  archive   = {C_IROS},
  author    = {Changhong Fu and Yiheng Wang and Liangliang Yao and Guangze Zheng and Haobo Zuo and Jia Pan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801335},
  month     = {10},
  pages     = {9706-9713},
  title     = {Prompt-driven temporal domain adaptation for nighttime UAV tracking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The power of the senses: Generalizable manipulation from
vision and touch through masked multimodal learning. <em>IROS</em>,
9698–9705. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans rely on the synergy of their senses for most essential tasks. For tasks requiring object manipulation, we seamlessly and effectively exploit the complementarity of our senses of vision and touch. This paper draws inspiration from such capabilities and aims to find a systematic approach to fuse visual and tactile information in a reinforcement learning setting. We propose Masked Multimodal Learning (M3L), which jointly learns a policy and visual-tactile representations based on masked autoencoding. The representations jointly learned from vision and touch improve sample efficiency, and unlock generalization capabilities beyond those achievable through each of the senses separately. Remarkably, representations learned in a multimodal setting also benefit vision-only policies at test time. We evaluate M3L on three simulated environments with both visual and tactile observations: robotic insertion, door opening, and dexterous in-hand manipulation, demonstrating the benefits of learning a multimodal policy. Videos of the experiments and the open-source code are available at https://sferrazza.cc/m3l_site.},
  archive   = {C_IROS},
  author    = {Carmelo Sferrazza and Younggyo Seo and Hao Liu and Youngwoon Lee and Pieter Abbeel},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802719},
  month     = {10},
  pages     = {9698-9705},
  title     = {The power of the senses: Generalizable manipulation from vision and touch through masked multimodal learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-modal motion prediction using temporal ensembling with
learning-based aggregation. <em>IROS</em>, 9691–9697. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent years have seen a shift towards learning-based methods for trajectory prediction, with challenges remaining in addressing uncertainty and capturing multi-modal distributions. This paper introduces Temporal Ensembling with Learning-based Aggregation, a meta-algorithm designed to mitigate the issue of missing behaviors in trajectory prediction, which leads to inconsistent predictions across consecutive frames. Unlike conventional model ensembling, temporal ensembling leverages predictions from nearby frames to enhance spatial coverage and prediction diversity. By confirming predictions from multiple frames, temporal ensembling compensates for occasional errors in individual frame predictions. Furthermore, trajectory-level aggregation, often utilized in model ensembling, is insufficient for temporal ensembling due to a lack of consideration of traffic context and its tendency to assign candidate trajectories with incorrect driving behaviors to final predictions. We further emphasize the necessity of learning-based aggregation by utilizing mode queries within a DETR-like architecture for our temporal ensembling, leveraging the characteristics of predictions from nearby frames. Our method, validated on the Argoverse 2 dataset, shows notable improvements: a 4% reduction in minADE, a 5% decrease in minFDE, and a 1.16% reduction in the miss rate compared to the strongest baseline, QCNet, highlighting its efficacy and potential in autonomous driving.},
  archive   = {C_IROS},
  author    = {Kai-Yin Hong and Chieh-Chih Wang and Wen-Chieh Lin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802445},
  month     = {10},
  pages     = {9691-9697},
  title     = {Multi-modal motion prediction using temporal ensembling with learning-based aggregation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BTGenBot: Behavior tree generation for robotic tasks with
lightweight LLMs. <em>IROS</em>, 9684–9690. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel approach to generating behavior trees for robots using lightweight large language models (LLMs) with a maximum of 7 billion parameters. The study demonstrates that it is possible to achieve satisfying results with compact LLMs when fine-tuned on a specific dataset. The key contributions of this research include the creation of a fine-tuning dataset based on existing behavior trees using GPT-3.5 and a comprehensive comparison of multiple LLMs (namely llama2, llama-chat, and code-llama) across nine distinct tasks. To be thorough, we evaluated the generated behavior trees using static syntactical analysis, a validation system, a simulated environment, and a real robot. Furthermore, this work opens the possibility of deploying such solutions directly on the robot, enhancing its practical applicability. Findings from this study demonstrate the potential of LLMs with a limited number of parameters in generating effective and efficient robot behaviors.},
  archive   = {C_IROS},
  author    = {Riccardo Andrea Izzo and Gianluca Bardaro and Matteo Matteucci},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802304},
  month     = {10},
  pages     = {9684-9690},
  title     = {BTGenBot: Behavior tree generation for robotic tasks with lightweight LLMs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Polaris: Open-ended interactive robotic manipulation via
Syn2Real visual grounding and large language models. <em>IROS</em>,
9676–9683. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper investigates the task of the open-ended interactive robotic manipulation on table-top scenarios. While recent Large Language Models (LLMs) enhance robots&#39; comprehension of user instructions, their lack of visual grounding constrains their ability to physically interact with the environment. This is because the robot needs to locate the target object for manipulation within the physical workspace. To this end, we introduce an interactive robotic manipulation framework called Polaris, which integrates perception and interaction by utilizing GPT-4 alongside grounded vision models. For precise manipulation, it is essential that such grounded vision models produce detailed object pose for the target object, rather than merely identifying pixels belonging to them in the image. Consequently, we propose a novel Synthetic-to-Real (Syn2Real) pose estimation pipeline. This pipeline utilizes rendered synthetic data for training and is then transferred to real-world manipulation tasks. The real-world performance demonstrates the efficacy of our proposed pipeline and underscores its potential for extension to more general categories. Moreover, real-robot experiments have showcased the impressive performance of our framework in grasping and executing multiple manipulation tasks. This indicates its potential to generalize to scenarios beyond the tabletop. More information and video results are available here: https://star-uu-wang.github.io/Polaris/.},
  archive   = {C_IROS},
  author    = {Tianyu Wang and Haitao Lin and Junqiu Yu and Yanwei Fu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801446},
  month     = {10},
  pages     = {9676-9683},
  title     = {Polaris: Open-ended interactive robotic manipulation via Syn2Real visual grounding and large language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TempBEV: Improving learned BEV encoders with combined image
and BEV space temporal aggregation. <em>IROS</em>, 9668–9675. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous driving requires an accurate representation of the environment. A strategy toward high accuracy is to fuse data from several sensors. Learned Bird’s-Eye View (BEV) encoders can achieve this by mapping data from individual sensors into one joint latent space. For cost-efficient camera-only systems, this provides an effective mechanism to fuse data from multiple cameras with different views. Accuracy can further be improved by aggregating sensor information over time. This is especially important in monocular camera systems to account for the lack of explicit depth and velocity measurements, such that decision-critical information about distance distances and motions of other objects is easily accessible. Thereby, the effectiveness of developed BEV encoders crucially depends on the operators used to aggregate temporal information and on the used latent representation spaces.We analyze BEV encoders proposed in the literature and compare their effectiveness, quantifying the effects of aggregation operators and latent representations. While most existing approaches aggregate temporal information either in image or in BEV latent space, our analyses and performance comparisons suggest that these latent representations exhibit complementary strengths. Therefore, we develop a novel temporal BEV encoder, TempBEV, which integrates aggregated temporal information from both latent spaces. We consider subsequent image frames as stereo through time and leverage methods from optical flow estimation for temporal stereo encoding.Empirical evaluation on the NuScenes dataset shows a significant improvement by TempBEV over the baseline for 3D object detection and BEV segmentation. The ablation uncovers a strong synergy of joint temporal aggregation in the image and BEV latent space. These results indicate the overall effectiveness of our approach and make a strong case for aggregating temporal information in both image and BEV latent spaces.},
  archive   = {C_IROS},
  author    = {Thomas Monninger and Vandana Dokkadi and Md Zafar Anwar and Steffen Staab},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801646},
  month     = {10},
  pages     = {9668-9675},
  title     = {TempBEV: Improving learned BEV encoders with combined image and BEV space temporal aggregation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-modal representation learning with tactile data.
<em>IROS</em>, 9660–9667. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Advancements in embodied language models like PALM-E and RT-2 have significantly enhanced language-conditioned robotic manipulation. However, these advances remain predominantly focused on vision and language, often overlooking the pivotal role of tactile feedback which is advantageous in contact-rich interactions. Our research introduces a novel approach that synergizes tactile information with vision and language. We present the Multi-Modal Wand (MMWand) dataset enriched with linguistic descriptions and tactile data. By integrating tactile feedback, we aim to bridge the divide between human linguistic understanding and robotic sensory interpretation. Our multi-modal representation model is trained on these datasets by employing the multi-modal embedding alignment principle from ImageBind which has shown promising results, emphasizing the potential of tactile data in robotic applications. The validation of our approach in downstream robotics tasks, such as texture-based object classification, cross-modality retrieval, and the dense reward function for visuomotor control, attests to its effectiveness. Our contributions underscore the importance of tactile feedback in multi-modal robotic learning and its potential to enhance robotic tasks. The MMWand dataset is publicly available at https://hyung-gun.me/mmwand/.},
  archive   = {C_IROS},
  author    = {Hyung-Gun Chi and Jose Barreiros and Jean Mercat and Karthik Ramani and Thomas Kollar},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802699},
  month     = {10},
  pages     = {9660-9667},
  title     = {Multi-modal representation learning with tactile data},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vision-language model-based physical reasoning for robot
liquid perception. <em>IROS</em>, 9652–9659. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There is a growing interest in applying large language models (LLMs) in robotic tasks, due to their remarkable reasoning ability and extensive knowledge learned from vast training corpora. Grounding LLMs in the physical world remains an open challenge as they can only process textual input. Recent advancements in large vision-language models (LVLMs) have enabled a more comprehensive understanding of the physical world by incorporating visual input, which provides richer contextual information than language alone. In this work, we proposed a novel paradigm that leveraged GPT-4V(ision), the state-of-the-art LVLM by OpenAI, to enable embodied agents to perceive liquid objects via image-based environmental feedback. Specifically, we exploited the physical understanding of GPT-4V to interpret the visual representation (e.g., time-series plot) of non-visual feedback (e.g., F/T sensor data), indirectly enabling multimodal perception beyond vision and language using images as proxies. We evaluated our method using 10 common household liquids with containers of various geometry and material. Without any training or fine-tuning, we demonstrated that our method can enable the robot to indirectly perceive the physical response of liquids and estimate their viscosity. We also showed that by jointly reasoning over the visual and physical attributes learned through interactions, our method could recognize liquid objects in the absence of strong visual cues (e.g., container labels with legible text or symbols), increasing the accuracy from 69.0%—achieved by the best-performing vision-only variant—to 86.0%.},
  archive   = {C_IROS},
  author    = {Wenqiang Lai and Tianwei Zhang and Tin Lun Lam and Yuan Gao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801833},
  month     = {10},
  pages     = {9652-9659},
  title     = {Vision-language model-based physical reasoning for robot liquid perception},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MuTT: A multimodal trajectory transformer for robot skills.
<em>IROS</em>, 9644–9651. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {High-level robot skills represent an increasingly popular paradigm in robot programming. However, configuring the skills’ parameters for a specific task remains a manual and time-consuming endeavor. Existing approaches for learning or optimizing these parameters often require numerous real-world executions or do not work in dynamic environments. To address these challenges, we propose Multimodal Trajectory Transformer (MuTT), a novel encoder-decoder transformer architecture designed to predict environment-aware executions of robot skills by integrating vision, trajectory, and robot skill parameters. Notably, we pioneer the fusion of vision and trajectory, introducing a novel trajectory projection. Furthermore, we illustrate MuTT’s efficacy as a predictor when combined with a model-based robot skill optimizer. This approach facilitates the optimization of robot skill parameters for the current environment, without the need for real-world executions during optimization. Designed for compatibility with any representation of robot skills, MuTT demonstrates its versatility across three comprehensive experiments, showcasing superior performance across two different skill representations.},
  archive   = {C_IROS},
  author    = {Claudius Kienle and Benjamin Alt and Onur Celik and Philipp Becker and Darko Katic and Rainer Jäkel and Gerhard Neumann},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802198},
  month     = {10},
  pages     = {9644-9651},
  title     = {MuTT: A multimodal trajectory transformer for robot skills},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CoT-TL: Low-resource temporal knowledge representation of
planning instructions using chain-of-thought reasoning. <em>IROS</em>,
9636–9643. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous agents often face the challenge of interpreting uncertain natural language instructions for planning tasks. Representing these instructions as Linear Temporal Logic (LTL) enables planners to synthesize actionable plans. We introduce CoT-TL, a data-efficient in-context learning framework for translating natural language specifications into LTL representations. CoT-TL addresses the limitations of large language models, which typically rely on extensive fine-tuning data, by extending chain-of-thought reasoning and semantic roles to align with the requirements of formal logic creation. This approach enhances the transparency and rationale behind LTL generation, fostering user trust. CoT-TL achieves state-of-the-art accuracy across three diverse datasets in low-data scenarios, outperforming existing methods without fine-tuning or intermediate translations. To improve reliability and minimize hallucinations, we incorporate model checking to validate the syntax of the generated LTL output. We further demonstrate CoT-TL’s effectiveness through ablation studies and evaluations on unseen LTL structures and formulas in a new dataset. Finally, we validate CoT-TL’s practicality by integrating it into a QuadCopter for multi-step drone planning based on natural language instructions.},
  archive   = {C_IROS},
  author    = {Kumar Manas and Stefan Zwicklbauer and Adrian Paschke},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801817},
  month     = {10},
  pages     = {9636-9643},
  title     = {CoT-TL: Low-resource temporal knowledge representation of planning instructions using chain-of-thought reasoning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NARRATE: Versatile language architecture for optimal control
in robotics. <em>IROS</em>, 9628–9635. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The impressive capabilities of Large Language Models (LLMs) have led to various efforts in enabling robots to be controlled through natural language instructions, opening exciting possibilities for human-robot interaction. The goal is for the motor-control task to be performed accurately, efficiently and safely while also enjoying the flexibility imparted by LLMs to specify and adjust the task through natural language. In this work, we demonstrate how a careful layering of an LLM in combination with a Model Predictive Control (MPC) formulation allows for accurate and flexible robotic control via natural language while taking into consideration safety constraints. In particular, we rely on the LLM to effectively frame constraints and objective functions as mathematical expressions, which are later used in the motor-control module via MPC. The transparency of the optimization formulation allows for interpretability of the task and enables adjustments through human feedback. We demonstrate the validity of our method through extensive experiments on long-horizon reasoning, contact-rich, and multi-object interaction tasks. Our evaluations show that NARRATE outperforms current existing methods on these benchmarks and effectively transfers to the real world on two different embodiments.Videos, Code and Prompts at narrate-mpc.github.io},
  archive   = {C_IROS},
  author    = {Seif Ismail and Antonio Arbues and Ryan Cotterell and René Zurbrügg and Carmen Amo Alonso},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801425},
  month     = {10},
  pages     = {9628-9635},
  title     = {NARRATE: Versatile language architecture for optimal control in robotics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ensuring safety in LLM-driven robotics: A cross-layer
sequence supervision mechanism. <em>IROS</em>, 9620–9627. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Integrating Large Language Models (LLMs) into robotics significantly enhances autonomous task planning. However, ensuring that multi-step task plans (action sequence) generated by LLMs comply with pre-defined safety constraints during planning and execution remains a challenge, limiting their adaptability in complex environments. To address this issue, a mechanism that can monitor and adjust the plan generated by the LLM-driven task planner and guide the motion planner to avoid potential risks during action execution is required. Therefore, this paper proposes a cross-layer sequence supervision mechanism. Specifically, we employ linear temporal logic syntax to express safety constraints and convert them into a set of nondeterministic Büchi automatons to build a cross-layer safety supervisor. For the task planning layer, the safety supervisor provides a closed-loop correction mechanism that can identify violations in the task plan in real time and guide LLM-driven planners to correct this plan to ensure compliance. For the motion planning layer, the safety supervisor introduces virtual &quot;obstacle&quot; information into the task plan to form the task plan tuple. Based on this plan tuple, the motion planner can proactively prevent unsafe behaviors during action execution. Extensive experimentation demonstrates significant improvements in safety with this cross-layer supervision mechanism, highlighting its potential to enhance LLM-driven robotic technology. Experiment details can be found in https://youtu.be/BDdSSEP6HJw.},
  archive   = {C_IROS},
  author    = {Ziming Wang and Qingchen Liu and Jiahu Qin and Man Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801576},
  month     = {10},
  pages     = {9620-9627},
  title     = {Ensuring safety in LLM-driven robotics: A cross-layer sequence supervision mechanism},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LANCAR: Leveraging language for context-aware robot
locomotion in unstructured environments. <em>IROS</em>, 9612–9619. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Navigating robots through unstructured terrains is challenging, primarily due to the dynamic environmental changes. While humans adeptly navigate such terrains by using context from their observations, creating a similar context-aware navigation system for robots is difficult. The essence of the issue lies in the acquisition and interpretation of context information, a task complicated by the inherent ambiguity of human language. In this work, we introduce LANCAR, which addresses this issue by combining a context translator with reinforcement learning (RL) agents for context-aware locomotion. LANCAR allows robots to comprehend context information through Large Language Models (LLMs) sourced from human observers and convert this information into actionable context embeddings. These embeddings, combined with the robot’s sensor data, provide a complete input for the RL agent’s policy network. We provide an extensive evaluation of LANCAR under different levels of context ambiguity and compare with alternative methods. The experimental results showcase the superior generalizability and adaptability across different terrains. Notably, LANCAR shows at least a 7.4% increase in episodic reward over the best alternatives, highlighting its potential to enhance robotic navigation in unstructured environments. More details and experiment videos could be found in this link.},
  archive   = {C_IROS},
  author    = {Chak Lam Shek and Xiyang Wu and Wesley A. Suttle and Carl Busart and Erin Zaroukian and Dinesh Manocha and Pratap Tokekar and Amrit Singh Bedi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802075},
  month     = {10},
  pages     = {9612-9619},
  title     = {LANCAR: Leveraging language for context-aware robot locomotion in unstructured environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LLaKey: Follow my basic action instructions to your next key
state. <em>IROS</em>, 9604–9611. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In 3D object manipulation, collecting expert data for end-to-end imitation learning becomes a mainstream method. Though successful, previous works neglect the guiding role of language in action execution. These methods lack the understanding of action semantics, in which multiple action sequences are guided by a category of instructions, resulting in overlearned object semantics and vague action semantics. To address the above limitation, we introduce a novel framework named LLaKey, which breaks down skill commands into more detailed action instructions based on key states for fine-grained action control. Specifically, LLaKey first leverages the knowledge encoded in pre-trained large-scale models to fine-tune an action instruction conductor. Then, these instructions are executed by a downstream action model. Comprehensive experiments show that LLaKey significantly surpasses baselines with a relative improvement of 15% in nine complex and varied skill tasks, demonstrating the superiority of our method.},
  archive   = {C_IROS},
  author    = {Zheyi Zhao and Ying He and Fei Yu and Pengteng Li and Fan Zhuo and Xilong Sun},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802579},
  month     = {10},
  pages     = {9604-9611},
  title     = {LLaKey: Follow my basic action instructions to your next key state},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GenCHiP: Generating robot policy code for high-precision and
contact-rich manipulation tasks. <em>IROS</em>, 9596–9603. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large Language Models (LLMs) have been successful at generating robot policy code, but so far these results have been limited to high-level tasks that do not require precise movement. It is an open question how well such approaches work for tasks that require reasoning over contact forces and working within tight success tolerances. We find that, with the right action space, LLMs are capable of successfully generating policies for a variety of contact-rich and high-precision manipulation tasks, even under noisy conditions, such as perceptual errors or grasping inaccuracies. Specifically, we reparameterize the action space to include compliance with constraints on the interaction forces and stiffnesses involved in reaching a target pose. We validate this approach on subtasks derived from the Functional Manipulation Benchmark (FMB) and NIST Task Board Benchmarks. Exposing this action space alongside methods for estimating object poses improves policy generation with an LLM by greater than 3x and 4x when compared to non-compliant action spaces. More material is available on our project webpage: https://dex-code-gen.github.io/dex-code-gen/},
  archive   = {C_IROS},
  author    = {Kaylee Burns and Ajinkya Jain and Keegan Go and Fei Xia and Michael Stark and Stefan Schaal and Karol Hausman},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801525},
  month     = {10},
  pages     = {9596-9603},
  title     = {GenCHiP: Generating robot policy code for high-precision and contact-rich manipulation tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning-on-the-drive: Self-supervised adaptive long-range
perception for high-speed offroad driving. <em>IROS</em>, 9588–9595. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous offroad driving is essential for applications like emergency rescue, military operations, and agriculture. Despite progress, systems struggle with high-speed vehicles exceeding 10m/s due to the need for accurate long-range (&gt; 50m) perception for safe navigation. Current approaches are limited by sensor constraints; LiDAR-based methods offer precise short-range data but are noisy beyond 30m, while visual models provide dense long-range measurements but falter with unseen scenarios. To overcome these issues, we introduce ALTER, a learning-on-the-drive perception framework that leverages both sensor types. ALTER uses a self-supervised visual model to learn and adapt from near-range LiDAR measurements, improving long-range prediction in new environments without manual labeling. It also includes a model selection module for better sensor failure response and adaptability to known environments. Testing in two real-world settings showed on average 43.4% better traversability prediction than LiDAR-only and 164% over non-adaptive state-of-the-art (SOTA) visual semantic methods after 45 seconds of online learning.},
  archive   = {C_IROS},
  author    = {Eric Chen and Cherie Ho and Mukhtar Maulimov and Chen Wang and Sebastian Scherer},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801365},
  month     = {10},
  pages     = {9588-9595},
  title     = {Learning-on-the-drive: Self-supervised adaptive long-range perception for high-speed offroad driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). QuerySOD: A small object detection algorithm based on sparse
convolutional network and query mechanism. <em>IROS</em>, 9581–9587. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although remarkable advances have been achieved in generic object detection, small object detection (SOD) remains challenging owing to small objects’ information loss and noisy representation caused by their non-uniform distribution. Their limited width and height, scale variations, and redundant computation make SOD hard. To overcome them, this work proposes a new SOD method based on sparse convolutional network (SCNet) and Query Mechanism called QuerySOD. First, an extended feature pyramid network is constructed for extracting feature maps of small objects with more regional details. Then, a Sparse Head is neatly designed by using SCNet for accelerating the interfering speed and obtaining weights of each layer. After that, a Query Mechanism is innovatively introduced for harvesting the benefit of sparse value feature maps from the Sparse Head. QuerySOD is evaluated on public benchmarks including COCO and VisDrone. Finally, we apply it on ‘Jinghai’ unmanned survey vehicles and receive excellent SOD performance from this real-world application.},
  archive   = {C_IROS},
  author    = {Zhengcai Cao and Junnian Li and Jie Niu and MengChu Zhou},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801539},
  month     = {10},
  pages     = {9581-9587},
  title     = {QuerySOD: A small object detection algorithm based on sparse convolutional network and query mechanism},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Density-aware domain generalization for LiDAR semantic
segmentation. <em>IROS</em>, 9573–9580. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D LiDAR-based perception has made remarkable advancements, leading to the widespread adoption of LiDAR in autonomous driving systems. Despite these technological strides, variations in LiDAR sensors and environmental conditions can significantly deteriorate the performance of perception models, primarily due to changes in the density of point clouds. Recent studies in domain generalization have aimed to mitigate this challenge; however, they often rely on the availability of sequential data and ego-motion, which limits their applicability. To address these limitations, we propose two novel methods that enable network operation in a density-aware fashion without any constraints, thereby ensuring consistent performance despite fluctuations in point cloud density. First, we design the network to be density-aware by utilizing the kernel occupancy information from the 3D sparse convolution as geometric features. Subsequently, we further enhance density awareness by incorporating voxel-wise density prediction as an auxiliary task in a self-supervised manner. Our method demonstrates superior performance over current state-of-the-art approaches, achieving this without the need for specific data prerequisites. Our approach is compatible with a variety of 3D backbone architectures, enhancing domain generalization performance by 18.4% while adding a minimal computational overhead of only 7ms.},
  archive   = {C_IROS},
  author    = {Jaeyeul Kim and Jungwan Woo and Ukcheol Shin and Jean Oh and Sunghoon Im},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801829},
  month     = {10},
  pages     = {9573-9580},
  title     = {Density-aware domain generalization for LiDAR semantic segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). InverseMatrixVT3D: An efficient projection matrix-based
approach for 3D occupancy prediction. <em>IROS</em>, 9565–9572. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces InverseMatrixVT3D, an efficient method for transforming multi-view image features into 3D feature volumes for 3D semantic occupancy prediction. Existing methods for constructing 3D volumes often rely on depth estimation, device-specific operators, or transformer queries, which hinders the widespread adoption of 3D occupancy models. In contrast, our approach leverages two projection matrices to store the static mapping relationships and matrix multiplications to efficiently generate global Bird’s Eye View (BEV) features and local 3D feature volumes. Specifically, we achieve this by performing matrix multiplications between multi-view image feature maps and two sparse projection matrices. We introduce a sparse matrix handling technique for the projection matrices to optimize GPU memory usage. Moreover, a global-local attention fusion module is proposed to integrate the global BEV features with the local 3D feature volumes to obtain the final 3D volume. We also employ a multi-scale supervision mechanism to enhance performance further. Extensive experiments performed on the nuScenes and SemanticKITTI datasets reveal that our approach not only stands out for its simplicity and effectiveness but also achieves the top performance in detecting vulnerable road users (VRU), crucial for autonomous driving and road safety. The code has been made available at: https://github.com/DanielMing123/InverseMatrixVT3D},
  archive   = {C_IROS},
  author    = {Zhenxing Ming and Julie Stephany Berrio and Mao Shan and Stewart Worrall},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802434},
  month     = {10},
  pages     = {9565-9572},
  title     = {InverseMatrixVT3D: An efficient projection matrix-based approach for 3D occupancy prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Channel-wise motion features for efficient motion
segmentation. <em>IROS</em>, 9557–9564. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For safety-critical robotics applications such as autonomous driving, it is important to detect all required objects accurately in real-time. Motion segmentation offers a solution by identifying dynamic objects from the scene in a class-agnostic manner. Recently, various motion segmentation models have been proposed, most of which jointly use subnetworks to estimate Depth, Pose, Optical Flow, and Scene Flow. As a result, the overall computational cost of the model increases, hindering real-time performance.In this paper, we propose a novel cost-volume-based motion feature representation, Channel-wise Motion Features. By extracting depth features of each instance in the feature map and capturing the scene’s 3D motion information, it offers enhanced efficiency. The only subnetwork used to build Channel-wise Motion Features is the Pose Network, and no others are required. Our method not only achieves about 4 times the FPS of state-of-the-art models in the KITTI Dataset and Cityscapes of the VCAS-Motion Dataset, but also demonstrates equivalent accuracy while reducing the parameters to about 25%.},
  archive   = {C_IROS},
  author    = {Riku Inoue and Masamitsu Tsuchiya and Yuji Yasui},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802584},
  month     = {10},
  pages     = {9557-9564},
  title     = {Channel-wise motion features for efficient motion segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Object segmentation from open-vocabulary manipulation
instructions based on optimal transport polygon matching with multimodal
foundation models. <em>IROS</em>, 9549–9556. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the task of generating segmentation masks for the target object from an object manipulation instruction, which allows users to give open vocabulary instructions to domestic service robots. Conventional segmentation generation approaches often fail to account for objects outside the camera’s field of view and cases in which the order of vertices differs but still represents the same polygon, which leads to erroneous mask generation. In this study, we propose a novel method that generates segmentation masks from open vocabulary instructions. We implement a novel loss function using optimal transport to prevent significant loss where the order of vertices differs but still represents the same polygon. To evaluate our approach, we constructed a new dataset based on the REVERIE dataset and Matterport3D dataset. The results demonstrated the effectiveness of the proposed method compared with existing mask generation methods. Remarkably, our best model achieved a +16.32% improvement on the dataset compared with a representative polygon-based method.},
  archive   = {C_IROS},
  author    = {Takayuki Nishimura and Katsuyuki Kuyo and Motonari Kambara and Komei Sugiura},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802596},
  month     = {10},
  pages     = {9549-9556},
  title     = {Object segmentation from open-vocabulary manipulation instructions based on optimal transport polygon matching with multimodal foundation models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pseudo-rigid body networks: Learning interpretable
deformable object dynamics from partial observations. <em>IROS</em>,
9542–9548. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurately predicting deformable linear object (DLO) dynamics is challenging, especially when the task requires a model that is both human-interpretable and computationally efficient. In this work, we draw inspiration from the pseudo-rigid body method (PRB) and model a DLO as a serial chain of rigid bodies whose internal state is unrolled through time by a dynamics network. This dynamics network is trained jointly with a physics-informed encoder that maps observed motion variables to the DLO&#39;s hidden state. To encourage the state to acquire a physically meaningful representation, we leverage the forward kinematics of the PRB model as a decoder. We demonstrate in robot experiments that the proposed DLO dynamics model provides physically interpretable predictions from partial observations while being on par with black-box models regarding prediction accuracy. The project code is available at: tinyurl.com/prb-networks},
  archive   = {C_IROS},
  author    = {Shamil Mamedov and A. René Geist and Jan Swevers and Sebastian Trimpe},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802305},
  month     = {10},
  pages     = {9542-9548},
  title     = {Pseudo-rigid body networks: Learning interpretable deformable object dynamics from partial observations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inverse kinematics of robotic manipulators using a new
learning-by-example method. <em>IROS</em>, 9534–9541. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inverse Kinematics (IK) is one of the most fundamental challenges in robotics. It refers to the process of determining the joint configurations required to achieve the desired position and orientation (pose) of a robot end-effector. Although numerous Data-Driven (DD) IK solvers have demonstrated encouraging results, they have not achieved the same accuracy when compared to other IK methods for complex robot configurations (e.g., numerical methods for higher Degrees of Freedom (DoF)). In this work, we propose a new Learning-by-Example method, and show that such a scheme considerably improves the IK learning results when compared to other DD learners. In our approach, the network input incorporates an example of joint-pose pair along with the query pose to predict the desired robot joint configuration. We show that the example joint-pose pair does not need to be too close to the query – i.e. example and query can be as far as 20 degrees apart in the joint configuration space. Furthermore, we investigate the utilization of residual and dense skip connections in Multilayer Perceptron for DDIK solvers and employ the resulting networks for two redundant robotic manipulators: a 7-DoF-7R commensurate robot and a 7DoF-2RP4R incommensurate robot. Our experimental results show that the resulting DDIK solver can reliably predict IK solutions with accuracy better than 1mm in position and 1deg in orientation.},
  archive   = {C_IROS},
  author    = {Jacket Demby’s and Ramy Farag and Guilherme N. DeSouza},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802048},
  month     = {10},
  pages     = {9534-9541},
  title     = {Inverse kinematics of robotic manipulators using a new learning-by-example method},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). APEX: Ambidextrous dual-arm robotic manipulation using
collision-free generative diffusion models. <em>IROS</em>, 9526–9533.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dexterous manipulation, particularly adept coordinating and grasping, constitutes a fundamental and indispensable capability for robots, facilitating the emulation of human-like behaviors. Integrating this capability into robots empowers them to supplement and even supplant humans in undertaking increasingly intricate tasks in both daily life and industrial settings. Unfortunately, contemporary methodologies encounter serious challenges in devising manipulation trajectories owing to the intricacies of tasks, the expansive robotic manipulation space, and dynamic obstacles. We propose a novel approach, APEX, to address all these difficulties by introducing a collision-free latent diffusion model for both robotic motion planning and manipulation. Firstly, we simplify the complexity of real-life ambidextrous dual-arm robotic manipulation tasks by abstracting them as aligning two vectors. Secondly, we devise latent diffusion models to produce a variety of robotic manipulation trajectories. Furthermore, we integrate obstacle information utilizing a classifier-guidance technique, thereby guaranteeing both the feasibility and safety of the generated manipulation trajectories. Lastly, we validate our proposed algorithm through extensive experiments conducted on the hardware platform of ambidextrous dual-arm robots. Our algorithm consistently generates successful and seamless trajectories across diverse tasks, surpassing conventional robotic motion planning algorithms. These results carry significant implications for the future design of diffusion robots, enhancing their capability to tackle more intricate robotic manipulation tasks with increased efficiency and safety. Complete video demonstrations of our experiments can be found in https://sites.google.com/view/apex-dual-arm/home.},
  archive   = {C_IROS},
  author    = {Apan Dastider and Hao Fang and Mingjie Lin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802655},
  month     = {10},
  pages     = {9526-9533},
  title     = {APEX: Ambidextrous dual-arm robotic manipulation using collision-free generative diffusion models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Avoiding object damage in robotic manipulation.
<em>IROS</em>, 9518–9525. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The large-scale deployment of robotic manipulation systems in warehouses has highlighted the rare but costly problem of robot-induced object damage. We present a system that uses a classification model to predict whether an object will get damaged during robotic manipulation. The model uses object attributes retrieved from warehouse information systems as well as attributes available at our robotic workcell. We evaluated different classical machine learning models, as well as a large language model (BERT) and a multimodal-transformer for our task. We show that the multi-modal transformer model that is able to leverage text and image data outperforms models that only rely on categorical and numerical data. Furthermore, our comparative analysis equips the selection the optimal model for an application. We validate our system during an experiment in which the output of the damage prediction system is used to avoid picking objects that are likely to get damaged. In over 50k pick-and-place activities, our system reduces damage rate by 64%.},
  archive   = {C_IROS},
  author    = {Erica Aduh and Fan Wang and Dylan Randle and Kaiwen Wang and Priyesh Shah and Chaitanya Mitash and Manikantan Nambi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801408},
  month     = {10},
  pages     = {9518-9525},
  title     = {Avoiding object damage in robotic manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). KOSMOS-e: Learning to follow instruction for robotic
grasping. <em>IROS</em>, 9510–9517. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tuning on instruction-following data has been shown to enhance the capabilities and controllability of language models, but the idea is less explored in the robotic field. In this work, we introduce KOSMOS-E, a Multimodal Large Language Model (MLLM) that leverages instruction-following robotic grasping data to enhance capabilities for precise and intricate robotic grasping maneuvers. To achieve this, we craft a large-scale instruction-following robotic grasping dataset, termed INSTRUCT-GRASP, primarily comprising two aspects: (i) grasp a single object following varying levels of granularity descriptions, e.g., different angles and aspects, and (ii) grasp a specific object within a multi-object environment following specific attributes, e.g., color and shape. Extensive experiments show the effectiveness of KOSMOS-E on robotic grasping tasks across a variety of environments.},
  archive   = {C_IROS},
  author    = {Zhi Wang and Xun Wu and Shaohan Huang and Li Dong and Wenhui Wang and Shuming Ma and Furu Wei},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802219},
  month     = {10},
  pages     = {9510-9517},
  title     = {KOSMOS-E: Learning to follow instruction for robotic grasping},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discretizing SO(2)-equivariant features for robotic kitting.
<em>IROS</em>, 9502–9509. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic kitting has attracted considerable attention in logistics and industrial settings. However, existing kitting methods encounter challenges such as low precision and poor efficiency, limiting their widespread applications. To address these issues, we present a novel kitting framework that improves both the precision and computational efficiency of complex kitting tasks. Firstly, our approach introduces a fine-grained orientation estimation technique in the picking module, significantly enhancing orientation precision while effectively decoupling computational load from orientation granularity. This technique combines an SO(2)-equivariant network with a group discretization operation to preciously predict discrete orientation distributions. Secondly, we develop the Hand-Tool Kitting Dataset (HTKD) to evaluate different solutions in handling orientation-sensitive kitting tasks. This dataset comprises a diverse collection of hand tools and synthetically created kits, which reflects the complexities of real-world kitting scenarios. Finally, a series of experiments is conducted to evaluate the performance of the proposed method. The results demonstrate that our approach offers an excellent balance between success rates and computational efficiency in high-precision robotic kitting tasks.},
  archive   = {C_IROS},
  author    = {Jiadong Zhou and Yadan Zeng and Huixu Dong and I-Ming Chen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801879},
  month     = {10},
  pages     = {9502-9509},
  title     = {Discretizing SO(2)-equivariant features for robotic kitting},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MultipleCupSuctionNet: Deep neural network for detecting
grasp pose of a vacuum gripper with multiple suction cups based on YOLO
feature map affine transformation. <em>IROS</em>, 9496–9501. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multiple-suction-cup grasp is preferable for picking large and heavy objects in warehouses. Deep learning methods have been widely used to predict grasp point position for single-suction-cup grasp, but few studies have examined grasp pose detection for a gripper with multiple suction cups. This study proposes MultipleCupSuctionNet, which is a deep neural network for detecting multiple cup grasp pose. To address the challenge of direct regression of poses, this neural network first infers the surface mask to compute the surface normal to obtain the direction of the gripper z-axis. The feature maps of the surfaces are then affine-transformed to surface image coordinates, based on which gripper position and rotation angle around the z-axis are predicted. Such a neural network design makes grasp pose learning easier because there is no need to consider the orientation of the surfaces so that 2D poses respective to the surface are learned. Feature map affine transformation saves computation cost because there is no need to first transform images and then extract the features to obtain surface features. Further, for each predicted grasp pose, the overlap area between cup and surface is calculated to determine which cup should be used when grasping. MultipleCupSuctionNet exhibited competitive performance (80.1% prediction accuracy) particularly in dense scenes compared with state-of-the-art planners (Dex-Net and a model-free multiple-suction-cup grasp planner). Physical picking experiments were conducted using a robot employing the proposed neural network. The experimental results showed that our robot achieved an average success rate of 94.5% for picking common objects in warehouses.},
  archive   = {C_IROS},
  author    = {Ping Jiang and Komoda Kazuma and Haifeng Han and Ooga Junichiro},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802723},
  month     = {10},
  pages     = {9496-9501},
  title     = {MultipleCupSuctionNet: Deep neural network for detecting grasp pose of a vacuum gripper with multiple suction cups based on YOLO feature map affine transformation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CoPa: General robotic manipulation through spatial
constraints of parts with foundation models. <em>IROS</em>, 9488–9495.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Foundation models pre-trained on web-scale data are shown to encapsulate extensive world knowledge beneficial for robotic manipulation in the form of task planning. However, the actual physical implementation of these plans often relies on task-specific learning methods, which require significant data collection and struggle with generalizability. In this work, we introduce Robotic Manipulation through Spatial Constraints of Parts (CoPa), a novel framework that leverages the common sense knowledge embedded within foundation models to generate a sequence of 6-DoF end-effector poses for open-world robotic manipulation. Specifically, we decompose the manipulation process into two phases: task-oriented grasping and task-aware motion planning. In the task-oriented grasping phase, we employ foundation vision-language models (VLMs) to select the object’s grasping part through a novel coarse-to-fine grounding mechanism. During the task-aware motion planning phase, VLMs are utilized again to identify the spatial geometry constraints of task-relevant object parts, which are then used to derive post-grasp poses. We also demonstrate how CoPa can be seamlessly integrated with existing robotic planning algorithms to accomplish complex, long-horizon tasks. Our comprehensive real-world experiments show that CoPa possesses a fine-grained physical understanding of scenes, capable of handling open-set instructions and objects with minimal prompt engineering and without additional training. Project page: copa-2024.github.io},
  archive   = {C_IROS},
  author    = {Haoxu Huang and Fanqi Lin and Yingdong Hu and Shengjie Wang and Yang Gao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801352},
  month     = {10},
  pages     = {9488-9495},
  title     = {CoPa: General robotic manipulation through spatial constraints of parts with foundation models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Object pose estimation by camera arm control based on the
next viewpoint estimation. <em>IROS</em>, 9482–9487. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We have developed a new method to estimate a Next Viewpoint (NV) which is effective for pose estimation of simple-shaped products for product display robots in retail stores. Pose estimation methods using Neural Networks (NN) based on an RGBD camera are highly accurate, but their accuracy significantly decreases when the camera acquires few texture and shape features at a current view point. However, it is difficult for previous mathematical model-based methods to estimate effective NV which is because the simple shaped objects have few shape features. Therefore, we focus on the relationship between the pose estimation and NV estimation. When the pose estimation is more accurate, the NV estimation is more accurate. Therefore, we develop a new pose estimation NN that estimates NV simultaneously. Experimental results showed that our NV estimation realized a pose estimation success rate 77.3%, which was 7.4pt higher than the mathematical model-based NV calculation did. Moreover, we verified that the robot using our method displayed 84.2% of products.},
  archive   = {C_IROS},
  author    = {Tomoki Mizuno and Kazuya Yabashi and Tsuyoshi Tasaki},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801633},
  month     = {10},
  pages     = {9482-9487},
  title     = {Object pose estimation by camera arm control based on the next viewpoint estimation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DAP: Diffusion-based affordance prediction for
multi-modality storage. <em>IROS</em>, 9476–9481. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Solving storage problems—where objects must be accurately placed into containers with precise orientations and positions—presents a distinct challenge that extends beyond traditional rearrangement tasks. These challenges are primarily due to the need for fine-grained 6D manipulation and the inherent multi-modality of solution spaces, where multiple viable goal configurations exist for the same storage container. We present a novel Diffusion-based Affordance Prediction (DAP) pipeline for the multi-modal object storage problem. DAP leverages a two-step approach, initially identifying a placeable region on the container and then precisely computing the relative pose between the object and that region. Existing methods either struggle with multi-modality issues or computation-intensive training. Our experiments demonstrate DAP’s superior performance and training efficiency over the current state-of-the-art RPDiff, achieving remarkable results on the RPDiff benchmark. Additionally, our experiments showcase DAP’s data efficiency in real-world applications, an advancement over existing simulation-driven approaches. Our contribution fills a gap in robotic manipulation research by offering a solution that is both computationally efficient and capable of handling real-world variability. Code and supplementary material can be found at: https://github.com/changhaonan/DPS.git.},
  archive   = {C_IROS},
  author    = {Haonan Chang and Kowndinya Boyalakuntla and Yuhan Liu and Xinyu Zhang and Liam Schramm and Abdeslam Boularias},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802575},
  month     = {10},
  pages     = {9476-9481},
  title     = {DAP: Diffusion-based affordance prediction for multi-modality storage},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-modal self-supervised learning with effective
contrastive units for LiDAR point clouds. <em>IROS</em>, 9468–9475. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D perception in LiDAR point clouds is crucial for a self-driving vehicle to properly act in 3D environment. However, manually labeling point clouds is hard and costly. There has been a growing interest in self-supervised pre-training of 3D perception models. Following the success of contrastive learning in images, current methods mostly conduct contrastive pre-training on point clouds only. Yet an autonomous driving vehicle is typically supplied with multiple sensors including cameras and LiDAR. In this context, we systematically study single modality, cross-modality, and multi-modality for contrastive learning of point clouds, and show that cross-modality wins over other alternatives. In addition, considering the huge difference between the training sources in 2D images and 3D point clouds, it remains unclear how to design more effective contrastive units for LiDAR. We therefore propose the instance-aware and similarity-balanced contrastive units that are tailored for self-driving point clouds. Extensive experiments reveal that our approach achieves remarkable performance gains over various point cloud models across the downstream perception tasks of LiDAR based 3D object detection and 3D semantic segmentation on the four popular benchmarks including Waymo Open Dataset, nuScenes, SemanticKITTI and ONCE.},
  archive   = {C_IROS},
  author    = {Mu Cai and Chenxu Luo and Yong Jae Lee and Xiaodong Yang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802675},
  month     = {10},
  pages     = {9468-9475},
  title     = {Cross-modal self-supervised learning with effective contrastive units for LiDAR point clouds},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DSVT: Dynamic 3D surround view for tractor-trailer vehicles
based on real-time pose estimation with drop model. <em>IROS</em>,
9461–9467. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, 3D surround view systems have attracted a lot of attention in the field of advanced driver assistance systems (ADAS). However, the foundational assumption of unchanging camera poses in traditional 3D surround view systems, which is designed for single-unit vehicles, results in a failure to manage the non-rigid connections characteristic of tractor-trailer vehicles. Moreover, tractor-trailer vehicles have the feature of long bodies and large wheelbases, leading to severe distortions and abrupt changes in the rendering results of previous 3D texture mapping models. In this paper, we propose DSVT, a dynamic 3D surround view system for tractor-trailer vehicles, designed to address the aforementioned issues. Specifically, we develop a dynamic surround image stitching algorithm based on relative pose estimation, which estimates the relative poses between cameras and stitches all images together to generate a 2D panoramic image. Subsequently, a novel 3D drop model is proposed, mapping the 2D panoramic image onto the 3D model for panoramic viewing. Our system can run in real time on Nvidia AGX Orin. Experimental results in real tractor-trailer scenes show that our system can achieve more accurate and natural visual effects.},
  archive   = {C_IROS},
  author    = {Zhipeng Dong and Mengyin Fu and Hao Liang and Chunhui Zhu and Yi Yang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801679},
  month     = {10},
  pages     = {9461-9467},
  title     = {DSVT: Dynamic 3D surround view for tractor-trailer vehicles based on real-time pose estimation with drop model},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LDIP: Real-time on-road object detection with depth
estimation from a single image. <em>IROS</em>, 9455–9460. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Detecting on-road objects with absolute depth information is one of the most crucial tasks in autonomous driving to ensure safety. Traditional 2D object detection aims to classify and locate objects in image space, but it cannot acquire in-depth information. While 3D object detection and pixel-level depth detection tasks can provide accurate depth information for objects, they are challenging to deploy in real-world scenarios due to their significant inference overhead. This paper proposes a novel deep learning-based model named the Location and Depth Information Perceptron (LDIP), designed to provide positional, categorical, and absolute depth information for given objects in the images.We first conducted model training and validation on the vehicle-side autonomous driving dataset—KITTI. The experimental results show that we achieved a 68.6% mAP in object recognition tasks and an RMSE of 0.101 and AbsRel of 2.327 in depth estimation tasks, all of which represent state-of-the-art performance in comparable tasks. Subsequently, we fine-tuned the trained model on DAIR, where the validated mAP, AbsRel, and RMSE reached 65.4%, 0.092, and 2.461 respectively. This demonstrates the robustness and generalization of our model across different types of road datasets.Moreover, in comparison to other models, our model is more compact while maintaining accuracy, achieving an inference speed of 70 frames per second on an NVIDIA 4060 GPU, thus making it deployable in practical scenarios. Relevant code is available at https://github.com/xcp-ustc/LDIP.},
  archive   = {C_IROS},
  author    = {Chengpeng Xu and Xiao Sun and Yangyang Xu and Ruolin Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801572},
  month     = {10},
  pages     = {9455-9460},
  title     = {LDIP: Real-time on-road object detection with depth estimation from a single image},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LiDAR-camera online calibration by representing local
feature and global spatial context. <em>IROS</em>, 9447–9454. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {LiDAR-camera calibration plays a crucial role in autonomous driving. However, operation-induced factors such as physical vibrations and temperature variations degrade the pre-deployment calibration accuracy, leading to the environmental perception performance deterioration. Recent recalibration methods have achieved online calibration without a target board by leveraging the relative attributes of LiDAR and camera. Nevertheless, we proposes a novel framework for LiDAR-camera online calibration which employs a Transformer network to learn crucial interactions between cameras and LiDAR sensors. Additionally, our novel framework design enables the effective calibration by utilizing correspondence point information between the two sensors. This allows the utilization of global spatial context and achieves high performance by integrating information across modalities. Experimental results indicate that our method demonstrates superior performance compared to state-of-the-art benchmarks.},
  archive   = {C_IROS},
  author    = {SeongJoo Moon and Sebin Lee and Dong He and Sung-Eui Yoon},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801723},
  month     = {10},
  pages     = {9447-9454},
  title     = {LiDAR-camera online calibration by representing local feature and global spatial context},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EC-IoU: Orienting safety for object detectors via
ego-centric intersection-over-union. <em>IROS</em>, 9439–9446. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents Ego-Centric Intersection-over-Union (EC-IoU), addressing the limitation of the standard IoU measure in characterizing safety-related performance for object detectors in navigating contexts. Concretely, we propose a weighting mechanism to refine IoU, allowing it to assign a higher score to a prediction that covers closer points of a ground-truth object from the ego agent’s perspective. The proposed EC-IoU measure can be used in typical evaluation processes to select object detectors with better safety-related performance for downstream tasks. It can also be integrated into common loss functions for model fine-tuning. While geared towards safety, our experiment with the KITTI dataset demonstrates the performance of a model trained on EC-IoU can be better than that of a variant trained on IoU in terms of mean Average Precision as well.},
  archive   = {C_IROS},
  author    = {Brian Hsuan-Cheng Liao and Chih-Hong Cheng and Hasan Esen and Alois Knoll},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801740},
  month     = {10},
  pages     = {9439-9446},
  title     = {EC-IoU: Orienting safety for object detectors via ego-centric intersection-over-union},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised motion segmentation with confidence-aware
loss functions for handling occluded pixels and uncertain optical flow
predictions. <em>IROS</em>, 9432–9438. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In driving scenarios, motion segmentation is a crucial and fundamental component that is needed for many tasks. Recently, a self-supervised multitasking framework was proposed for driving scenarios. It simultaneously trains motion segmentation, optical flow, depth, and ego-motion models without annotated data. The self-supervised architecture derives training signals from training data via loss functions. If these loss functions lack robustness, they may result in model inaccuracies. To reduce the bad influences of occlusion and optical flow estimation errors on motion segmentation, we propose two loss functions: (1) Soft-Per-Pixel-Minimum (Soft-PPM) loss that excludes occluded pixels while balancing the contribution of each frame on the loss function temporally; (2) Flow difference loss that excludes pixels with unclear motion states to diminish the effect of optical flow estimation errors. Our loss function design is based on the key insight that information such as depth and optical flow can be used to train motion segmentation models and act as a reliable measure for pixels during training. Our approach can improve segmentation accuracy for both moving and static objects and has achieved IoU scores on moving and static classes comparable to the state-of-the-art methods on the KITTI dataset.},
  archive   = {C_IROS},
  author    = {Chung-Yu Chen and Bo-Yun Lai and Ying-Shiuan Huang and Wen-Chieh Lin and Chieh-Chih Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801964},
  month     = {10},
  pages     = {9432-9438},
  title     = {Self-supervised motion segmentation with confidence-aware loss functions for handling occluded pixels and uncertain optical flow predictions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). All-day depth completion. <em>IROS</em>, 9425–9431. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a method for depth estimation under different illumination conditions, i.e., day and night time. As photometry is uninformative in regions under low-illumination, we tackle the problem through a multi-sensor fusion approach, where we take as input an additional synchronized sparse point cloud (i.e., from a LiDAR) projected onto the image plane as a sparse depth map, along with a camera image. The crux of our method lies in the use of the abundantly available synthetic data to first approximate the 3D scene structure by learning a mapping from sparse to (coarse) dense depth maps along with their predictive uncertainty – we term this, SpaDe. In poorly illuminated regions where photometric intensities do not afford the inference of local shape, the coarse approximation of scene depth serves as a prior; the uncertainty map is then used with the image to guide refinement through an uncertainty-driven residual learning (URL) scheme. The resulting depth completion network leverages complementary strengths from both modalities – depth is sparse but insensitive to illumination and in metric scale, and image is dense but sensitive with scale ambiguity. SpaDe can be used in a plug-and-play fashion, which allows for 24% improvement when augmented onto existing methods to preprocess sparse depth. We demonstrate URL on the nuScenes dataset where we improve over all baselines by an average 12.39% in all-day scenarios, 12.02% when tested specifically for daytime, and 14.95% for nighttime scenes. Code available at : https://github.com/ezhovv/all-day-depth},
  archive   = {C_IROS},
  author    = {Vadim Ezhov and Hyoungseob Park and Zhaoyang Zhang and Rishi Upadhyay and Howard Zhang and Chethan Chinder Chandrappa and Achuta Kadambi and Yunhao Ba and Julie Dorsey and Alex Wong},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802376},
  month     = {10},
  pages     = {9425-9431},
  title     = {All-day depth completion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing LiDAR scene upsampling with instance-aware
feature-embedding and attention mechanism. <em>IROS</em>, 9418–9424. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Scanning LiDAR is one of the widely used sensors in autonomous vehicles; however, the inherent sparsity of LiDAR point clouds often affects its performance. To address this issue, upsampling methods could be employed to enhance low-resolution LiDAR data. Although there have been methods on upsampling of single-object point clouds recently in computer vision, they tend to generate a considerable amount of artifacts when dealing with real-world LiDAR scenes consisting of multiple objects. In this paper, we propose a solution to tackle this problem by introducing an instance embedding auxiliary task and a context attention module. With our auxiliary learning architecture, the network can learn features that benefit both the primary upsampling task and the auxiliary instance embedding task. This training design enables the point generation process to be carried out separately and significantly reduces artifacts of the upsampling results on the SemanticKITTI dataset, particularly in areas surrounding instances. By leveraging these techniques to improve the model’s understanding of the relationship between objects and the background in LiDAR scenes, we achieve an overall 4% to 10% improvement in whole-scene upsampling.},
  archive   = {C_IROS},
  author    = {Wei-Jen Wang and You-Sheng Do and Wen-Chieh Lin and Chieh-Chih Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801358},
  month     = {10},
  pages     = {9418-9424},
  title     = {Enhancing LiDAR scene upsampling with instance-aware feature-embedding and attention mechanism},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient motion prediction: A lightweight &amp; accurate
trajectory prediction model with fast training and inference speed.
<em>IROS</em>, 9411–9417. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For efficient and safe autonomous driving, it is essential that autonomous vehicles can predict the motion of other traffic agents. While highly accurate, current motion prediction models often impose significant challenges in terms of training resource requirements and deployment on embedded hardware. We propose a new efficient motion prediction model, which achieves highly competitive benchmark results while training only a few hours on a single GPU. Due to our lightweight architectural choices and the focus on reducing the required training resources, our model can easily be applied to custom datasets. Furthermore, its low inference latency makes it particularly suitable for deployment in autonomous applications with limited computing resources.},
  archive   = {C_IROS},
  author    = {Alexander Prutsch and Horst Bischof and Horst Possegger},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802425},
  month     = {10},
  pages     = {9411-9417},
  title     = {Efficient motion prediction: A lightweight &amp; accurate trajectory prediction model with fast training and inference speed},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vehicle trajectory prediction with soft behavior
constraints. <em>IROS</em>, 9404–9410. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trajectory prediction plays a crucial role in autonomous driving, but it is challenging due to the multi-modal nature of future trajectories. Behavior information is frequently employed to capture more diverse modalities of future trajectories. Traditional behavior information is typically hard-encoded, which is often inaccurate and inadequate for reflecting future multimodality. Therefore, we introduce the concept of soft vehicle behavior, which is represented as a probability distribution over a predefined comprehensive set of behaviors. This approach allows for a more rational depiction of vehicle behavior and captures potential future driving modalities. Based on it, we propose a new soft-behavior-constrained vehicle trajectory prediction framework. The framework consists of a backbone and a lightweight and plug-and-play behavior prediction module, which is used to imbue soft behavior constraints to assist in representation learning. We integrated the behavior prediction module into five representative trajectory predictors and achieved improvements of at least 4.2% in minFDE(K=5) on the nuScenes dataset and 0.5% in minFDE(K=6) on the Argoverse 1 motion forecasting dataset. These universal increments prove the effectiveness and generalizability of soft behavior constraints in vehicle trajectory prediction.},
  archive   = {C_IROS},
  author    = {Ke Ye and Sanping Zhou and Miao Kang and Jingwen Fu and Nanning Zheng},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802739},
  month     = {10},
  pages     = {9404-9410},
  title     = {Vehicle trajectory prediction with soft behavior constraints},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised monocular depth estimation in challenging
environments based on illumination compensation PoseNet. <em>IROS</em>,
9396–9403. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-supervised depth estimation has attracted much attention due to its ability to improve the 3D perception capabilities of unmanned systems. However, existing unsupervised frameworks rely on the assumption of photometric consistency, which may not hold in challenging environments such as night-time, rainy nights, or snowy winters due to complex lighting and reflections, resulting in inconsistent photometry across different frames for the same pixel. To address this problem, we propose a self-supervised monocular depth estimation unified framework that can handle these complex scenarios, which has the following characteristics: (1) an Illumination Compensation PoseNet (ICP) is designed, which is based on the classic Phong illumination theory and compensates for lighting changes in adjacent frames by estimating per-pixel transformations; (2) a Dual-Axis Transformer (DAT) block is proposed as the backbone network of the depth encoder, which infers the depth of local repeat-texture areas through spatial-channel dual-dimensional global context information of images. Experimental results demonstrate that our approach achieves state-of-the-art depth estimation results in complex environments on the challenging Oxford RobotCar dataset.},
  archive   = {C_IROS},
  author    = {Shengyu Hou and Wenjie Song and Rongchuan Wang and Meiling Wang and Yi Yang and Mengyin Fu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801331},
  month     = {10},
  pages     = {9396-9403},
  title     = {Self-supervised monocular depth estimation in challenging environments based on illumination compensation PoseNet},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SGOR: Outlier removal by leveraging semantic and geometric
information for robust point cloud registration. <em>IROS</em>,
9388–9395. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce a new outlier removal method that fully leverages geometric and semantic information, to achieve robust registration. Current semantic-based registration methods only use semantics for point-to-point or instance semantic correspondence generation, which has two problems. First, these methods are highly dependent on the correctness of semantics. They perform poorly in scenarios with incorrect semantics and sparse semantics. Second, the use of semantics is limited only to the correspondence generation, resulting in bad performance in the weak geometry scene. To solve these problems, on the one hand, we propose secondary ground segmentation and loose semantic consistency based on regional voting. It improves the robustness to semantic correctness by reducing the dependence on single-point semantics. On the other hand, we propose semantic-geometric consistency for outlier removal, which makes full use of semantic information and significantly improves the quality of correspondences. In addition, a two-stage hypothesis verification is proposed, which solves the problem of incorrect transformation selection in the weak geometry scene. In the outdoor dataset, our method demonstrates superior performance, boosting a 22.5 percentage points improvement in registration recall and achieving better robustness under various conditions. Our code is available.},
  archive   = {C_IROS},
  author    = {Guiyu Zhao and Zhentao Guo and Hongbin Ma},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801642},
  month     = {10},
  pages     = {9388-9395},
  title     = {SGOR: Outlier removal by leveraging semantic and geometric information for robust point cloud registration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Supervised articulation angles estimation for
multi-articulated vehicles based on panoramic camera system.
<em>IROS</em>, 9381–9387. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Articulation angle plays a significant role in determining the motion of a complex dynamic system such as a multi-articulated vehicle. By engineering practice, articulation angles are measured using mechanical angle sensors that are delicate to physical damage. To overcome this problem, this study proposed a supervised articulation angle estimation method based on the panoramic camera system of multi-articulated vehicles. By constructing neural network that takes images of surrounding environment captured by spatially adjacent cameras as input, and takes temporal dependency as well as data imbalanced distribution into consideration, we show that the proposed vision-only method could make accurate estimations either on collected dataset or field experiment. Results of our experiments verified the validity and feasibility of the proposed method in playing as an alternative to mechanical angle sensors without bringing additional hardware setting expenses.},
  archive   = {C_IROS},
  author    = {Weimin Liu and Wenjun Wang and Zhaocong Sun},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802842},
  month     = {10},
  pages     = {9381-9387},
  title     = {Supervised articulation angles estimation for multi-articulated vehicles based on panoramic camera system},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AutoJoin: Efficient adversarial training against
gradient-free perturbations for robust maneuvering via denoising
autoencoder and joint learning. <em>IROS</em>, 9374–9380. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the growing use of machine learning algorithms and ubiquitous sensors, many ‘perception-to-control’ systems are being developed and deployed. To ensure their trustworthiness, improving their robustness through adversarial training is one potential approach. We propose a gradient-free adversarial training technique, named AutoJoin, to effectively and efficiently produce robust models for image-based maneuvering. Compared to other state-of-the-art methods with testing on over 5M images, AutoJoin achieves significant performance increases up to the 40% range against perturbations while improving on clean performance up to 300%. AutoJoin is also highly efficient, saving up to 86% time per training epoch and 90% training data over other state-of-the-art techniques. The core idea of AutoJoin is to use a decoder attachment to the original regression model creating a denoising autoencoder within the architecture. This architecture allows the tasks ‘maneuvering’ and ‘denoising sensor input’ to be jointly learnt and reinforce each other’s performance.},
  archive   = {C_IROS},
  author    = {Michael Villarreal and Bibek Poudel and Ryan Wickman and Yu Shen and Weizi Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802857},
  month     = {10},
  pages     = {9374-9380},
  title     = {AutoJoin: Efficient adversarial training against gradient-free perturbations for robust maneuvering via denoising autoencoder and joint learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatic image annotation for mapped features detection.
<em>IROS</em>, 9367–9373. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Detecting road features is a key enabler for autonomous driving and localization. For instance, a reliable detection of poles which are widespread in road environments can improve localization. Modern deep learning-based perception systems need a significant amount of annotated data. Automatic annotation avoids time-consuming and costly manual annotation. Because automatic methods are prone to errors, managing annotation uncertainty is crucial to ensure a proper learning process. Fusing multiple annotation sources on the same dataset can be an efficient way to reduce the errors. This not only improves the quality of annotations, but also improves the learning of perception models. In this paper, we consider the fusion of three automatic annotation methods in images: feature projection from a high accuracy vector map combined with a lidar, image segmentation and lidar segmentation. Our experimental results demonstrate the significant benefits of multi-modal automatic annotation for pole detection through a comparative evaluation on manually annotated images. Finally, the resulting multi-modal fusion is used to fine-tune an object detection model for pole base detection using unlabeled data, showing overall improvements achieved by enhancing network specialization. The dataset is publicly available.},
  archive   = {C_IROS},
  author    = {Maxime Noizet and Philippe Xu and Philippe Bonnifait},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801773},
  month     = {10},
  pages     = {9367-9373},
  title     = {Automatic image annotation for mapped features detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mini-PointNetPlus: A local feature descriptor in deep
learning model for real-time 3D environment perception. <em>IROS</em>,
9362–9366. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Common deep learning models for 3D real-time environment perception often use pillarization/voxelization methods to convert point cloud data into pillars/voxels and then process it with a 2D/3D convolutional neural network (CNN). The pioneer work PointNet has been widely applied as a local feature descriptor, a fundamental component in deep learning models for 3D perception, to extract features of a point cloud. This is achieved by using a symmetric max-pooling operator which provides unique pillar/voxel features. However, by ignoring most of the points, the max-pooling operator causes an information loss, which reduces the model performance. To address this issue, we propose a novel local feature descriptor, mini-PointNetPlus, as an alternative for plug-and-play to PointNet. Our basic idea is to separately project the data points to the individual features considered, each leading to a permutation invariant. Thus, the proposed descriptor transforms an unordered point cloud to a stable order. The vanilla PointNet is proved to be a special case of our mini-PointNetPlus. Due to fully utilizing the features by the proposed descriptor, we demonstrate in experiment a considerable performance improvement for 3D perception.},
  archive   = {C_IROS},
  author    = {Chuanyu Luo and Nuo Cheng and Sikun Ma and Jun Xiang and Xiaohan Li and Shengguang Lei and Pu Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801871},
  month     = {10},
  pages     = {9362-9366},
  title     = {Mini-PointNetPlus: A local feature descriptor in deep learning model for real-time 3D environment perception},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3D object visibility prediction in autonomous driving.
<em>IROS</em>, 9355–9361. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the rapid advancement of hardware and software technologies, research in autonomous driving has seen significant growth. The prevailing framework for multi-sensor autonomous driving encompasses sensor installation, perception, path planning, decision-making, and motion control. At the perception phase, a common approach involves utilizing neural networks to infer 3D bounding box (Bbox) attributes from raw sensor data, including classification, size, and orientation.In this paper, we present a novel attribute and its corresponding algorithm: 3D object visibility. By incorporating multi-task learning, the introduction of this attribute, visibility, negligibly affects the model&#39;s effectiveness and efficiency. Our proposal of this attribute and its computational strategy aims to expand the capabilities for downstream tasks, thereby enhancing the safety and reliability of real-time autonomous driving in real-world scenarios.},
  archive   = {C_IROS},
  author    = {Chuanyu Luo and Nuo Cheng and Ren Zhong and Haipeng Jiang and Wenyu Chen and Aoli Wang and Pu Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802014},
  month     = {10},
  pages     = {9355-9361},
  title     = {3D object visibility prediction in autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). SmartKit: User-friendly robot with multiple operating
systems. <em>IROS</em>, 9348–9354. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile robots have become extensively involved in human activities, taking on arduous tasks and providing significant assistance. Robot capabilities have been continuously enhanced, from simple chassis control to path planning and SLAM. Mixed criticality systems enable mobile robots to handle tasks of varying criticality by integrating multiple operating systems, allowing them to accomplish a wide range of tasks. However, besides improving robot computing performance, we should remember that robots are designed to serve humans. Reliability, usability, and affordability are all critical factors for robot design.We introduce SmartKit, a mixed criticality system (MCS) for mobile robots. Leveraging the efficiency in hardware utilization brought by virtualization, SmartKit can execute tasks of different criticality efficiently and securely. This paper will present the software and hardware architecture of SmartKit and provide performance and functionality validation of the robot system.},
  archive   = {C_IROS},
  author    = {Guanyu Chen and Yiqun Zhou and Guoqing Yang and Hong Li and Pan Lv},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802835},
  month     = {10},
  pages     = {9348-9354},
  title     = {SmartKit: User-friendly robot with multiple operating systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and validation of flexible aerial robotics for safe
human-robot interaction. <em>IROS</em>, 9342–9347. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work addresses the critical challenge of integrating drones into human-aerial robot interaction by presenting a novel Soft Flexible Aerial Robotics (SFAR) design. SFAR features an innovative low-pressure inflatable airbag structure that replaces traditional rigid frames, enhancing safety by mitigating collision risks with humans and payloads. To control this unconventional aerial platform, we present a control strategy based on a virtual link dynamics model that exploits the drone’s unique design. Our contributions include the pioneering design of an aerial robot specifically for Human-Aerial Robot Interaction (HARI), a novel control framework that balances flight performance with passive safety, and the validation of SFAR through real-world experiments, demonstrating its ability to perform at par with traditional rigid-body drones while offering enhanced safety features for seamless and safe integration into human environments.},
  archive   = {C_IROS},
  author    = {Fuhua Jia and Zihao Zheng and Cheng’ao Li and Junlin Xiao and Rui Li and Xiaoying Yang and Adam Rushworth and Salman Ijaz},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801991},
  month     = {10},
  pages     = {9342-9347},
  title     = {Design and validation of flexible aerial robotics for safe human-robot interaction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transparency evaluation for the kinematic design of the
harnesses through human-exoskeleton interaction modeling. <em>IROS</em>,
9335–9341. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Lower Limb Exoskeletons (LLEs) are wearable robotic systems that provide mechanical power to the user. Human-exoskeleton (HE) connections must guarantee the subsistence of the user’s natural behavior during the interaction, avoiding the exertion of undesired forces, i.e., the robot must be transparent. Since transparency is an essential feature of exoskeletons’ design, numerous works focus on its maximization, e.g., employing passive joints at the HE interfaces. Given the inherent complications of repeatedly prototyping and experimentally testing a device, modeling the exoskeleton and its physical interaction with the user emerges as an extremely valuable approach for assessing the design effects. This paper proposes a novel method to compare different exoskeleton configurations with a flexible simulation tool. This approach contemplates simulating the dynamics of the device, including its interaction with the wearer, to evaluate multiple connection mechanism designs along with the kinematics and actuation of the LLE. This evaluation is based on the minimization of the interaction wrenches through an optimization process that includes the impedance parameters at the interfaces as optimization variables and the similarity of the LLE’s joint variables trajectories with the motion of the wearer’s articulations. Exploratory tests are conducted using the Wearable Walker LLE in different configurations and measuring the interaction forces. Experimental data are then compared to the optimization outcomes, proving that the proposed method provides contact wrench estimations consistent with the collected measurements and previous outcomes from the literature.},
  archive   = {C_IROS},
  author    = {Riccardo Bezzini and Carlo Alberto Avizzano and Francesco Porcini and Alessandro Filippeschi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802797},
  month     = {10},
  pages     = {9335-9341},
  title     = {Transparency evaluation for the kinematic design of the harnesses through human-exoskeleton interaction modeling},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Perception-driven shared control architecture for
agricultural robots performing harvesting tasks. <em>IROS</em>,
9328–9334. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a shared control framework designed specifically for agricultural mobile manipulators engaged in harvesting operations. The shared control strategy allows for achieving such operations by dynamically exchanging the control between the robotic system and a human operator depending on the uncertainty in the environment perception. For this purpose, the robot’s behavior is dynamically adapted to switch between two control modes with a different level of autonomy of the robot. The level of autonomy is encoded in two different admittance behaviors which are included in a first-order Hierarchical Quadratic Programming (HQP) control framework, that allows the robot to simultaneously address other control objectives at the same time. Experimental results with a dual-arm mobile robot, developed as part of the EU-funded CANOPIES project, demonstrate the effectiveness of the proposed method in real conditions.},
  archive   = {C_IROS},
  author    = {Jozsef Palmieri and Paolo Di Lillo and Alberto Sanfeliu and Alessandro Marino},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802454},
  month     = {10},
  pages     = {9328-9334},
  title     = {Perception-driven shared control architecture for agricultural robots performing harvesting tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Compliant blind handover control for human-robot
collaboration. <em>IROS</em>, 9321–9327. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a Human-Robot Blind Handover architecture within the context of Human-Robot Collaboration (HRC). The focus lies on a blind handover scenario where the operator is intentionally faced away, focused in a task, and requires an object from the robot. In this context, it is imperative for the robot to autonomously manage the entire handover process. Key considerations include ensuring safety while handing the object to the operator’s hand, and detect the proper timing to release the object. The article explores strategies to navigate these challenges, emphasizing the need for a robot to operate safely and independently in facilitating blind handovers, thereby contributing to the advancement of HRC protocols and fostering a natural and efficient collaboration between humans and robots.},
  archive   = {C_IROS},
  author    = {Davide Ferrari and Andrea Pupa and Cristian Secchi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801955},
  month     = {10},
  pages     = {9321-9327},
  title     = {Compliant blind handover control for human-robot collaboration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Development of a super-thin and fast omnidirectional
treadmill through a novel helical transmission mechanism. <em>IROS</em>,
9314–9320. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To effectively enhance the spatial sensation for an immersive user experience (UX) in virtual reality (VR), the locomotion interface (LI) is one of the most critical factors. To offer a room-scale LI suitable for use in typical office or home environments, it is imperative that the LI device enables a natural walking experience while occupying minimal space. To realize LI for all directions, an omnidirectional treadmill (ODT) can successfully achieve 2-dimensional holonomic motion (X and Y axes) and provide the most natural walking experience, such as walking on real ground. However, a problem arises due to the excessive system thickness caused by the torus structure of a large treadmill (X-axis) carrying several small treadmills (Y-axis), along with the installation of a complex power transmission mechanism inside the ODT. To reduce the thickness of its double-layer and complex structure, we propose an ODT with a novel transmission mechanism. The proposed ODT utilizes helical timing pulleys (HTPs) to generate Y-axis motion and helical gears (HGs) to synchronized-actuate the HTPs. As a result, the proposed ODT achieves a super-thin configuration and fast actuation performance. A pilot test of the proposed ODT was conducted to assess its maximum performance. The results indicate achievable speeds of 3.175 m/s and 4 m/s, along with an acceleration of 5 m/s2 for both the X and Y axes, respectively.},
  archive   = {C_IROS},
  author    = {Sanghun Pyo and Jinsun Choi and Jungwon Yoon},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802204},
  month     = {10},
  pages     = {9314-9320},
  title     = {Development of a super-thin and fast omnidirectional treadmill through a novel helical transmission mechanism},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Force and velocity prediction in human-robot collaborative
transportation tasks through video retentive networks. <em>IROS</em>,
9307–9313. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this article, we propose a generalization of a Deep Learning State-of-the-Art architecture such as Retentive Networks so that it can accept video sequences as input. With this generalization, we design a force/velocity predictor applied to the medium-distance Human-Robot collaborative object transportation task. We achieve better results than with our previous predictor by reaching success rates in testset of up to 93.7% in predicting the force to be exerted by the human and up to 96.5% in the velocity of the human-robot pair during the next 1 s, and up to 91.0% and 95.0% respectively in real experiments. This new architecture also manages to improve inference times by up to 32.8% with different graphics cards. Finally, an ablation test allows us to detect that one of the input variables used so far, such as the position of the task goal, could be discarded allowing this goal to be chosen dynamically by the human instead of being pre-set.},
  archive   = {C_IROS},
  author    = {J. E. Domínguez-Vidal and Alberto Sanfeliu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801981},
  month     = {10},
  pages     = {9307-9313},
  title     = {Force and velocity prediction in human-robot collaborative transportation tasks through video retentive networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning-based adaptive admittance controller for efficient
and safe pHRI in contact-rich manufacturing tasks. <em>IROS</em>,
9299–9306. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes an adaptive admittance controller for improving efficiency and safety in physical human-robot interaction (pHRI) tasks in small-batch manufacturing that involve contact with stiff environments, such as drilling, polishing, cutting, etc. We aim to minimize human effort and task completion time while maximizing precision and stability during the contact of the machine tool attached to the robot’s end-effector with the workpiece. To this end, a two-layered learning-based human intention recognition mechanism is proposed, utilizing only the kinematic and kinetic data from the robot and two force sensors. A &quot;subtask detector&quot; recognizes the human intent by estimating which phase of the task is being performed, e.g., Idle, Tool-Attachment, Driving, and Contact. Simultaneously, a &quot;motion estimator&quot; continuously quantifies intent more precisely during the Driving to predict when Contact will begin. The controller is adapted online according to the subtask while allowing early adaptation before the Contact to maximize precision and safety and prevent potential instabilities. Three sets of pHRI experiments were performed with multiple subjects under various conditions. Spring compression experiments were performed in virtual environments to train the data-driven models and validate the proposed adaptive system, and drilling experiments were performed in the physical world to test the proposed methods’ efficacy in real-life scenarios. Experimental results show subtask classification accuracy of 84% and motion estimation R2 score of 0.96. Furthermore, 57% lower human effort was achieved during Driving as well as 53% lower oscillation amplitude at Contact as a result of the proposed system.},
  archive   = {C_IROS},
  author    = {Pouya P. Niaz and Engin Erzin and Cagatay Basdogan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802530},
  month     = {10},
  pages     = {9299-9306},
  title     = {Learning-based adaptive admittance controller for efficient and safe pHRI in contact-rich manufacturing tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive passivation of admittance controllers by bypassing
power to null space on redundant manipulators. <em>IROS</em>, 9292–9298.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The significance of physical human-robot interaction (pHRI) with collaborative robots in the industry is growing steadily. Within this domain, an admittance controller is crucial for enabling robots to follow or assist human intentions. However, a persistent challenge with admittance controllers is ensuring their passivity. Various strategies have been developed to address this issue by adjusting the control signals derived from the admittance model. While these strategies achieve passivity, they often inadvertently impact collaborative performance, preventing the system from accurately aligning with the intended dynamics. Accordingly, this paper introduces an adaptive hierarchical control approach for redundant robots to handle this problem. This approach diverts non-passive power into the null space without diminishing the robot’s responsiveness to human input. Implementing this null-space controller involves the dynamic adjustment of compliance control error, ensuring joint limit avoidance while facilitating integration with energy tanks for enhanced reliability. Moreover, the method enables the calculation of adaptive error gain in a closed form, simplifying its real-time application. Experimental validation with a 7-DOF manipulator showed a reduction of non-passive energy from 1.61 J to 0.12 J without compromising task performance.},
  archive   = {C_IROS},
  author    = {Yeoil Yun and DongJun Oh and Eun Jeong Song and Hyouk Ryeol Choi and Hyungpil Moon and Ja Choon Koo},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801296},
  month     = {10},
  pages     = {9292-9298},
  title     = {Adaptive passivation of admittance controllers by bypassing power to null space on redundant manipulators},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Boosting 3D visual grounding by object-centric referring
network. <em>IROS</em>, 9285–9291. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D visual grounding is tasked with locating a specific object within a 3D scene, as described by a given textual reference. This task is challenging because it requires (1) the accurate recognition of various objects in a 3D scene and (2) the understanding of spatial relations in the description. However, current studies encounter difficulties in situations where multiple similar objects are present or when the descriptions involve intricate and abstract relations. In this paper, a novel, simple, and efficient Object-Centric Referring network, namely 3D-OCR, is presented to take high-quality semantic representation and deep relation modeling into account. Specifically, an offline Fine-grained Semantic Enhancement (FSE) module is designed to reinforce the object-centric semantic awareness with fine-grained high-quality object semantic representations. To achieve superior object-centric relation awareness, we propose a Deep Relation Modeling (DRM) module with the explicit and implicit relation self-attention module, enriching object features with relational context. Moreover, we utilize a vision-language contrastive loss to further improve the matching process between point cloud and language. Comprehensive experiments conducted on the challenging ScanRefer and Nr3D datasets corroborate the exceptional performance of our method, with an increase of +1.47% on ScanRefer and +1.2% on Nr3D.},
  archive   = {C_IROS},
  author    = {Ruilong Ren and Jian Cao and Weichen Xu and Tianhao Fu and Yilei Dong and Xinxin Xu and Zicong Hu and Xing Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802292},
  month     = {10},
  pages     = {9285-9291},
  title     = {Boosting 3D visual grounding by object-centric referring network},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PGA: Personalizing grasping agents with single human-robot
interaction. <em>IROS</em>, 9277–9284. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Language-Conditioned Robotic Grasping (LCRG) aims to develop robots that comprehend and grasp objects based on natural language instructions. While the ability to understand personal objects like my wallet facilitates more natural interaction with human users, current LCRG systems only allow generic language instructions, e.g., the black-colored wallet next to the laptop. To this end, we introduce a task scenario GraspMine alongside a novel dataset aimed at pinpointing and grasping personal objects given personal indicators via learning from a single human-robot interaction, rather than a large labeled dataset. Our proposed method, Personalized Grasping Agent (PGA), addresses GraspMine by leveraging the unlabeled image data of the user’s environment, called Reminiscence. Specifically, PGA acquires personal object information by a user presenting a personal object with its associated indicator, followed by PGA inspecting the object by rotating it. Based on the acquired information, PGA pseudo-labels objects in the Reminiscence by our proposed label propagation algorithm. Harnessing the information acquired from the interactions and the pseudo-labeled objects in the Reminiscence, PGA adapts the object grounding model to grasp personal objects. This results in significant eﬃciency while previous LCRG systems rely on resource-intensive human annotations—necessitating hundreds of labeled data to learn my wallet. Moreover, PGA outperforms baseline methods across all metrics and even shows comparable performance compared to the fully-supervised method, which learns from 9k annotated data samples. We further validate PGA’s real-world applicability by employing a physical robot to execute GrsapMine. Code and data are publicly available at https://github.com/JHKim-snu/PGA.},
  archive   = {C_IROS},
  author    = {Junghyun Kim and Gi-Cheon Kang and Jaein Kim and Seoyun Yang and Minjoon Jung and Byoung-Tak Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801347},
  month     = {10},
  pages     = {9277-9284},
  title     = {PGA: Personalizing grasping agents with single human-robot interaction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast explicit-input assistance for teleoperation in clutter.
<em>IROS</em>, 9270–9276. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The performance of prediction-based assistance for robot teleoperation degrades in unseen or goal-rich environments due to incorrect or quickly-changing intent inferences. Poor predictions can confuse operators or cause them to change their control input to implicitly signal their goal. We present a new assistance interface for robotic manipulation where an operator can explicitly communicate a manipulation goal by pointing the end-effector. The pointing target specifies a region for local pose generation and optimization, providing interactive control over grasp and placement pose candidates. We evaluate this explicit pointing interface against an implicit inference-based assistance scheme and an unassisted control condition in a within-subjects user study (N=20), where participants teleoperate a simulated robot to complete a multi-step singulation and stacking task in cluttered environments. We find that operators prefer the explicit interface, experience fewer pick failures and report lower cognitive workload. Our code is available at: github.com/NVlabs/fast-explicit-teleop.},
  archive   = {C_IROS},
  author    = {Nick Walker and Xuning Yang and Animesh Garg and Maya Cakmak and Dieter Fox and Claudia Pérez-D’Arpino},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802138},
  month     = {10},
  pages     = {9270-9276},
  title     = {Fast explicit-input assistance for teleoperation in clutter},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Foot arch stiffness-based dynamic plantar support control of
human walking gait with active pneumatic insoles. <em>IROS</em>,
9262–9269. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The human foot arch plays a significant role in bearing weight, keeping balance, and walking efficiently. In this study, we present a pneumatic arch support insole (PASI) and a foot arch stiffness-based dynamic plantar support control to reduce the metabolic cost of walking. We first obtain the foot arch quasi-stiffness estimation over the gait phase by conducting multiple subject experiments. The design and modeling of the PASI and foot-insole interactions are then discussed. A model predictive control scheme is presented to dynamically regulate the foot arch stiffness over the gait phase by using the active PASI. We validate the system experimentally and conduct multi-subject walking tests. The results show that dynamic plantar support control of the foot arch stiffness reduces the metabolic cost by 7.99% compared to regular walking. In contrast, passive support without dynamic regulation increases the metabolic cost by 6.60%. The new pneumatic insoles and dynamic support method demonstrate promising potential for everyday and medical applications.},
  archive   = {C_IROS},
  author    = {Chenhao Liu and Jingang Yi and Long He and Yijun Zhang and Xiufeng Zhang and Tao Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802837},
  month     = {10},
  pages     = {9262-9269},
  title     = {Foot arch stiffness-based dynamic plantar support control of human walking gait with active pneumatic insoles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interactive reward tuning: Interactive visualization for
preference elicitation. <em>IROS</em>, 9254–9261. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In reinforcement learning, tuning reward weights in the reward function is necessary to align behavior with user preferences. However, current approaches, which use pairwise comparisons for preference elicitation, are inefficient, because they miss much of the human ability to explore and judge groups of candidate solutions. The paper presents a novel visualization-based approach that better exploits the user’s ability to quickly recognize interesting directions for reward tuning. It breaks down the tuning problem by using the visual information-seeking principle: overview first, zoom and filter, then details-on-demand. Following this principle, we built a visualization system comprising two interactively linked views: 1) an embedding view showing a contextual overview of all sampled behaviors and 2) a sample view displaying selected behaviors and visualizations of the detailed time-series data. A user can efficiently explore large sets of samples by iterating between these two views. The paper demonstrates that the proposed approach is capable of tuning rewards for challenging behaviors. The simulation-based evaluation shows that the system can reach optimal solutions with fewer queries relative to baselines.},
  archive   = {C_IROS},
  author    = {Danqing Shi and Shibei Zhu and Tino Weinkauf and Antti Oulasvirta},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801540},
  month     = {10},
  pages     = {9254-9261},
  title     = {Interactive reward tuning: Interactive visualization for preference elicitation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatic dietary monitoring using inertial sensor in
smartwatch. <em>IROS</em>, 9248–9253. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper investigates the problem of eating activity detection using motion data from an off-the-shelf smartwatch. The development and integration of the algorithm for detecting eating activity will make it easier for users to monitor their eating habits. For development of a such algorithm about 27500 hours of data were collected from 91 participants. Moreover, a reliable and interpreted approach with adjustable tolerance for model quality estimation in real-world conditions is proposed in this work. The algorithm based on end-to-end neural network (NN) for eating events detection with special postprocessing was developed by our research group. It recognizes eating events with 1 minute delay from the beginning of food intake. For a such tolerance it achieves F1-score of 0.90 in average (at &quot;free-living&quot; scenario test) for users wearing smartwatches either on dominant or on non-dominant hand. To the best of authors’ knowledge, the algorithm provides the best performance of any existing solution or described in the literature.},
  archive   = {C_IROS},
  author    = {K. Pavlov and V. Tsepulin and N. Lutsyak and R. Khasianov and E. Simchuk and A. Perchik and E. Volkova},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802031},
  month     = {10},
  pages     = {9248-9253},
  title     = {Automatic dietary monitoring using inertial sensor in smartwatch},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and evaluation of a prototype tactile scanner for
active sensing of proximal objects. <em>IROS</em>, 9241–9247. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tactile interfaces, that can convey information to humans via tactile feedback, are still relatively rare. In this study we present a prototype ‘tactile scanner’, that fixes onto a user’s arm, and using arrays of capacitive sensors and vibratory motors, provides users with a sense of the proximity of objects near their arm. Through two experiments, we show that the device enables users to detect not just the position of objects, but also estimate their shapes and orientations. Finally, in a third experiment, we compare the user accuracy with the tactile scanner and their accuracy with real touch.},
  archive   = {C_IROS},
  author    = {A. Dechaux and M. Kitazaki and J. Lagarde and G. Ganesh},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801696},
  month     = {10},
  pages     = {9241-9247},
  title     = {Design and evaluation of a prototype tactile scanner for active sensing of proximal objects},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effects of fiber number and density on fiber jamming:
Towards follow-the-leader deployment of a continuum robot.
<em>IROS</em>, 9235–9240. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fiber jamming modules (FJMs) offer flexibility and quick stiffness variation, making them suitable for followthe-leader (FTL) motions in continuum robots, which is ideal for minimally invasive surgery (MIS). However, their potential has not been fully exploited, particularly in designing and manufacturing small-sized FJMs with high stiffness variation. Although existing research has focused on factors like fiber materials and geometry to maximize stiffness variation, the results often do not apply to FJMs for MIS due to size constraints. Meanwhile, other factors such as fiber number and packing density, less significant to large FJMs but critical to smallsized FJMs, have received insufficient investigation regarding their impact on the stiffness variation for FTL deployment. In this paper, we design and fabricate FJMs with a diameter of 4mm. Through theoretical and experimental analysis, we find that fiber number and packing density significantly affect both absolute stiffness and stiffness variation. Our experiments confirm the feasibility of using FJMs in a medical FTL robot design. The optimal configuration is a 4mm FJM with 0.4mm fibers at a 56% packing density, achieving up to 3400% stiffness variation. A video demonstration of a prototype robot using the suggested parameters for achieving FTL motions can be found at https://youtu.be/7pI5U0z7kcE.},
  archive   = {C_IROS},
  author    = {Chen Qian and Tangyou Liu and Liao Wu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802851},
  month     = {10},
  pages     = {9235-9240},
  title     = {Effects of fiber number and density on fiber jamming: Towards follow-the-leader deployment of a continuum robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enabling maintainablity of robot programs in assembly by
extracting compositions of force- and position-based robot skills from
learning-from-demonstration models. <em>IROS</em>, 9227–9234. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To this day, only a small number of industrial robots is used in assembly. One key reason for this is that specific contact situations require the introduction of force-control schemes. The parameters for those schemes are hard to select in practice, because they require in-depth expertise about the robot and the process. Learning-from-Demonstration (LfD) provides a powerful approach to intuitively parameterize robot programs by demonstrating the task at hand. However, when dimensions increase by including force or orientation, many LfD algorithms are hard to verify, understand and maintain, requiring expert knowledge to make adaptions, effectively making it a &quot;black-box&quot;. This property renders them ineffective for usage in industrial applications. We build upon a system of composable skills, that can be easily adapted by experts without the need to demonstrate the task again. This approach to skill-based robot programming promises to address the issues of readability and maintainability by sequencing robot movements in skills and breaking them down into understandable (sub-)goals. In this paper, we combine skill-based programming with LfD, preserving both maintainability and intuitive parameterization. We present (a) an approach to parameterize and create sequences of hierarchies of force- and/or position-controlled robot skills from a LfD model, (b) which can be adapted by a user by hand with few, basic and understandable parameters, and (c) show its applicability on the real-world example of terminal clamp assembly. We achieve a reduction in teach-in time of 53.8% for variants, increased robustness against variance, and efficient tight stacking of clamps with a gap of ≤ 1mm.},
  archive   = {C_IROS},
  author    = {Daniel Bargmann and Werner Kraus and Marco F. Huber},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802802},
  month     = {10},
  pages     = {9227-9234},
  title     = {Enabling maintainablity of robot programs in assembly by extracting compositions of force- and position-based robot skills from learning-from-demonstration models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design of a pneumatically driven 3D-printed under-actuated
soft robot with programmable stiffness. <em>IROS</em>, 9221–9226. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft robotic technologies are laying the foundations for a wide range of applications involving manipulation, gripping, and locomotion. In particular, soft pneumatic actuators have been very popular for their high power-to-weight ratio and ability to yield significantly large motions. However, these actuators are usually limited in delivering reconfigurable deformation, limiting their versatility. A recent direction to rectify this limitation is to couple pneumatic actuation with active materials. Proof-of-the-concept designs has been developed to demonstrate that the actuator’s deformation characteristics could be configured by selectively softening structural elements in an ON-OFF manner. In this paper, following this idea, we propose a novel soft pneumatic actuator coupled with stiffness programmable polylactic acid (PLA) elements. The proposed design is based on heating wires embedded in the PLA elements. Changes in the elastic properties of PLA near the glass transition temperature (Tg) coupled with pneumatic actuation are exploited to activate the actuator’s variable stiffness features. This study shows that the stiffness of 3D-printed elements made of PLA can be tuned by means of temperature controlled heaters. The proposed actuator is able to extend by 19.5% of its original length and produce 15◦ angular deformation. We demonstrate the actuator’s ability to perform different reconfigurable orientations based on different temperature and pressure inputs. The proposed idea could be considered as an actuation unit that can deliver complex deformations when configured in multitudes.},
  archive   = {C_IROS},
  author    = {Zaid Mustafa and Melih Türkseven},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802121},
  month     = {10},
  pages     = {9221-9226},
  title     = {Design of a pneumatically driven 3D-printed under-actuated soft robot with programmable stiffness},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Agonist-antagonist pouch motors: Bidirectional soft
actuators enhanced by thermally responsive peltier elements.
<em>IROS</em>, 9214–9220. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this study, we introduce a novel Mylar-based pouch motor design that leverages the reversible actuation capabilities of Peltier junctions to enable agonist-antagonist muscle mimicry in soft robotics. Addressing the limitations of traditional silicone-based materials, such as leakage and phase-change fluid degradation, our pouch motors filled with Novec 7000 provide a durable and leak-proof solution for geometric modeling. The integration of flexible Peltier junctions offers a significant advantage over conventional Joule heating methods by allowing active and reversible heating and cooling cycles. This innovation not only enhances the reliability and longevity of soft robotic applications but also broadens the scope of design possibilities, including the development of agonist-antagonist artificial muscles, grippers with can manipulate through flexion and extension, and an anchor-slip style simple crawler design. Our findings indicate that this approach could lead to more efficient, versatile, and durable robotic systems, marking a significant advancement in the field of soft robotics.},
  archive   = {C_IROS},
  author    = {Trevor Exley and Rashmi Wijesundara and Nathan Tan and Akshay Sunkara and Xinyu He and Shuopu Wang and Bonnie Chan and Aditya Jain and Luis Espinosa and Amir Jafari},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802253},
  month     = {10},
  pages     = {9214-9220},
  title     = {Agonist-antagonist pouch motors: Bidirectional soft actuators enhanced by thermally responsive peltier elements},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal sensing in soft pneumatic actuators via stretchable
optical waveguides. <em>IROS</em>, 9208–9213. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Stretchable optical waveguides have been explored as a route to enhancing the sensing capabilities of soft actuators. Certain properties and qualities they possess recommend them for this task – their biological plausibility, compliance, low power consumption, and heightened responsiveness to external stimuli. Though well regarded for their efficiency, their practical application warrants a more detailed examination as regards their sensitivity, robustness, and resilience when integrated with various manipulators. There is a dearth of comprehensive, wide-ranging studies that investigate the relationship between soft sensors and actuators, both in terms of integration and sensor performance – the present study endeavours to fill this void. Here we present a series of findings as to the interdependent relationship at the nexus of soft actuator sensorisation, sensitivity and responsiveness. Building on our previous work and prior waveguide designs, we examine the influence of sensor location and placement along the deformation axis on responsiveness, repeatability, and longevity. Location is key as, during bending, one side experiences tension, while the other compression. Placement is identified as ’straight’ or ’loose’. Three PneuNet-based actuators were used in three design configurations: one without any additional modifications, one with a few rigid exoskeleton reinforcements, and one covered fully with rigid exoskeleton reinforcements. The purpose of applying the exoskeletons is to hold the waveguide-based sensors and to suppress any bubble formation. Each design enables the straightforward integration of sensors, so that the relationship between soft actuator design and sensor performance is easy to assess when applying various pressure intakes (from 0 to 7.1 psi) to actuate the bending motion.},
  archive   = {C_IROS},
  author    = {Faisal Aljaber and Ahmed Hassan and Ivan Vitanov and Noora Almeadadi and Hind AlHajri and Sara AlEnazi and Rashid Al-Marri and Kaspar Althoefer and Pilsung Choe},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801812},
  month     = {10},
  pages     = {9208-9213},
  title     = {Optimal sensing in soft pneumatic actuators via stretchable optical waveguides},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrated electronic circuitry for soft robots using
multi-material FDM printing. <em>IROS</em>, 9202–9207. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The integration of electronics into compliant materials is typically complex, cumbersome, and jeopardizes system-level compliance. Using multi-material fused deposition modeling, we introduce a framework in which components of a soft robot and conductive traces are deposited in a single print. Our novel procedure for attaching discrete electronic components to printed conductive traces using toluene solvent ensures reliable electrical connections by significantly reducing contact resistance by over an order of magnitude compared to existing methods. This fabrication pipeline is an additional key component that contributes to the broader objective of establishing a fully automated fabrication process for soft robots with integrated electronics. We demonstrate a complete assembly of a terrestrial soft robot and showcase its resilience against physical impacts.},
  archive   = {C_IROS},
  author    = {Cem Aygül and Ritwik Pandey and Krishram Kothimbakam and Ceren Yılmaz Akkaya and Pratap M. Rao and Markus P. Nemitz},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802422},
  month     = {10},
  pages     = {9202-9207},
  title     = {Integrated electronic circuitry for soft robots using multi-material FDM printing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reconfigurable soft gripper based on eversion and
electroadhesion for cluttered environments. <em>IROS</em>, 9195–9201.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic grasping in cluttered and real-world human environments is a challenging task. It requires unique kinematic capabilities to deal with spatial constraints as well as compliance and softness to offer collision safety and safe manipulation of sensitive objects. To address this challenge, we propose a novel robotic gripper with two steerable fingers whose lengths can be adjusted by way of a soft eversion mechanism. This enables the gripper to work in confined spaces while interacting safely with the environment. We also developed a new Electroadhesion (EA) pad design with a multilayer structure and a single insulating layer that can be safely integrated with the evertable fingers avoiding short-circuiting or dielectric breakdown to enhance the gripper payload. The resulting gripper can retrieve an object in a confined space and partially occluded by a barrier. It exhibits remarkable versatility in terms of object sizes, grasping objects with varying widths at least ranging from 70 mm to 600 mm. These results provide a promising avenue for new robotic applications in real-world environments.},
  archive   = {C_IROS},
  author    = {Dana Ragab and Elizabeth Rendon-Morales and Kaspar Althoefer and Hareesh Godaba},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802690},
  month     = {10},
  pages     = {9195-9201},
  title     = {Reconfigurable soft gripper based on eversion and electroadhesion for cluttered environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A laser-induced graphene-based flexible multimodal sensor
for material and texture perception. <em>IROS</em>, 9189–9194. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans can perceive and interact with their surroundings through multiple senses. For intelligent robots, multimodal sensors are crucial for them to perceive and understand the environment. In this work, we propose a multi-layered flexible multimodal sensor based on laser-induced graphene, capable of detecting both touchless signals (such as the distance from external objects and their material) and tactile signals (three-dimensional force). The sensor has advantages in durability and stability. Under normal force, the sensitivity is –7.449% N–1 in the range of 0 N to 1.5 N and –0.273% N–1 in the range of 1.5 N to 30 N, with fast response (17 ms) and recovery (37 ms). Furthermore, using the Convolutional Neural Networks (CNN) model, we develop an intelligent soft robot system capable of distinguishing objects of different materials and fabric textures with accuracies of 99% and 88.75%, respectively. The proposed flexible multimodal sensor holds a significant effect on the perception and interaction of intelligent robots with the environment.},
  archive   = {C_IROS},
  author    = {Youning Duo and Jinxi Duan and Xingyu Chen and Wenbo Liu and Shengxue Wang and Li Wen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802515},
  month     = {10},
  pages     = {9189-9194},
  title     = {A laser-induced graphene-based flexible multimodal sensor for material and texture perception},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A circular soft pneumatic actuator with bi-directional
bending behavior. <em>IROS</em>, 9182–9188. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most existing soft robotic actuators require multiple chambers to achieve multi-directional bending. In this paper, we present the design, fabrication, and analysis of a novel circular actuator. The actuator is made from a single soft material and is capable of bi-directional bending using only positive pressure in a single chamber. To accurately predict the required pressure for all bending angles, we developed an analytical model for the full range of motion - both uncurling and curling. We tested and analyzed actuators fabricated out of four types of silicone and found that the softest actuators have a bending range from the initial bending angle, 210°, to 0°, and to 225° in the other direction in 112 kPa. When constraining the actuators, we found they can create up to 5.1 N of blocked force in 210 kPa. Additionally, we demonstrate that a single circular actuator can grasp objects of various weights up to 800 g from both the inside and outside.},
  archive   = {C_IROS},
  author    = {Jeannette Circe and Michael Giglia and Isaiah Rivera and Ani Vardanyan and Brandon Bunt and Michelle H. Rosen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802703},
  month     = {10},
  pages     = {9182-9188},
  title     = {A circular soft pneumatic actuator with bi-directional bending behavior},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Programming passive fingertip deformation for improved
grasping and manipulation. <em>IROS</em>, 9175–9181. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft robots exhibit complex behaviors despite simple control, due to their inherently compliant hardware which passively deforms upon contact ± a concept commonly referred to as morphological computation. To fully determine the behavior of soft robots, not only their control software but also their passive behavior needs to be programmed. We show that deliberate programming of passive deformation in soft fingertips can significantly influence the grasping and manipulation performance of various robotic grippers. For this, the fingertips display strategically modulated compliance levels across their palmar surface, realized through adjustments to the local thickness of a lattice structure within their soft material, resulting in desired passive deformation. The grippers are operated by human participants, solving diverse tasks involving a variety of objects. We analyze 2025 human trials and show that the distinct passive behaviors programmed into the fingertips significantly affect grasping and manipulation performance. Furthermore, we discovered that specific compliance profiles consistently demonstrate superior performance, indicating that not merely inherent softness by itself, but a purposeful combination of varying compliance levels plays a pivotal role in successful soft interaction.},
  archive   = {C_IROS},
  author    = {Steffen Puhlmann and Lion-Constantin Weber and Hannes Höppner},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801459},
  month     = {10},
  pages     = {9175-9181},
  title     = {Programming passive fingertip deformation for improved grasping and manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and control of a soft supernumerary robotic limb
based on fiber-reinforced actuator. <em>IROS</em>, 9167–9174. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Supernumerary robotic limbs (SRLs) provide additional wearable limbs to enhance the user’s physical abilities. Most SRLs employ rigid structures, resulting in uncomfortable wearing experience and insufficient flexible manipulation. As a new type of SRL, soft SRLs offer operational flexibility, lightweight structure, and wearing safety, compensating for the shortcomings of rigid SRLs. However, due to the complex actuation mechanisms, soft SRLs pose challenges in multiple deformations and accurate controlling. In this paper, a soft SRL actuated by fiber-reinforced actuators (FRAs) is proposed. A kinematic model is established to capture the posture of the SRL. A control system is proposed to adjust the SRL posture precisely by configuration of the FRAs. Finally, the accuracy of the proposed control strategy is verified through experiments, and the SRL prototype exhibits flexibility and adaptability to various scenarios, effectively assisting users in accomplishing complex tasks.},
  archive   = {C_IROS},
  author    = {Tianyi Zhang and Jiajun Xu and Yonghua Lu and Mengcheng Zhao and Kaizhen Huang and Bai Chen and Xuyan Hou and Youfu Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801825},
  month     = {10},
  pages     = {9167-9174},
  title     = {Design and control of a soft supernumerary robotic limb based on fiber-reinforced actuator},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robotic object insertion with a soft wrist through
sim-to-real privileged training. <em>IROS</em>, 9159–9166. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study addresses contact-rich object insertion tasks under unstructured environments using a robot with a soft wrist, enabling safe contact interactions. For the unstructured environments, we assume that there are uncertainties in object grasp and hole pose and that the soft wrist pose cannot be directly measured. Recent methods employ learning approaches and force/torque sensors for contact localization; however, they require data collection in the real world. This study proposes a sim-to-real approach using a privileged training strategy. This method has two steps. 1) The teacher policy is trained to complete the task with sensor inputs and ground truth privileged information such as the peg pose, and then 2) the student encoder is trained with data produced from teacher policy rollouts to estimate the privileged information from sensor history. We performed sim-to-real experiments under grasp and hole pose uncertainties. This resulted in 100%, 95%, and 80% success rates for circular peg insertion with 0°, +5°, and -5° peg misalignments, respectively, and start positions randomly shifted ± 10 mm from a default position. Also, we tested the proposed method with a square peg that was never seen during training. Additional simulation evaluations revealed that using the privileged strategy improved success rates compared to training with only simulated sensor data. Our results demonstrate the advantage of using sim-to-real privileged training for soft robots, which has the potential to alleviate human engineering efforts for robotic assembly.},
  archive   = {C_IROS},
  author    = {Yuni Fuchioka and Cristian C. Beltran-Hernandez and Hai Nguyen and Masashi Hamaya},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801575},
  month     = {10},
  pages     = {9159-9166},
  title     = {Robotic object insertion with a soft wrist through sim-to-real privileged training},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic model and experimental validation of a haptic robot
based on a flexible antenna mounted on an omnidirectional platform.
<em>IROS</em>, 9152–9158. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The development of new measurement systems for mobile robots has been a growing interest in recent years, particularly tactile systems based on bioinspired sensor antennae, such as whiskers and antennae found in animals and insects. This work focuses on studying a mobile robot equipped with such systems. Specifically, a dynamic model is developed for an omnidirectional robot with a sensing antenna, considering the planar motion of the system and taking into account the gravity effect. The extended Hamilton principle is applied to derive the equations of motion for the mobile platform, while the boundary-value problem is formulated for the antenna. Subsequently, modal analysis is employed to obtain a unique solution for the sensor antenna model, which is then validated using experimental data.},
  archive   = {C_IROS},
  author    = {Luis Mérida-Calvo and María Isabel Haro-Olmo and Vicente Feliu-Batlle},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802846},
  month     = {10},
  pages     = {9152-9158},
  title     = {Dynamic model and experimental validation of a haptic robot based on a flexible antenna mounted on an omnidirectional platform},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive smith predictor fractional control of a
tele-operated flexible link robot *. <em>IROS</em>, 9144–9151. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work addresses the adaptive Smith predictor control of a tele-operated robot composed of a flexible link. Measurements of the angle of the motor that moves the arm by using an encoder and the moment at the base of the arm by using two strain gauges are fed back. These strain gauges normally present noticeable offset and high-frequency noise. In order to implement such control, the simultaneous real-time characterizations of the main vibration mode - which changes with the carried payload - and the delay of the tele-operated robot - which is time-varying - are carried out using a new algorithm based on an algebraic identification technique, which is robust to the previous strain gauge disturbances. This algorithm is faster than others previously developed, being especially suited to implement an adaptive version of the Smith predictor. The control of the tip position of the robot is closed using a fractional order controller, which has the advantage of removing the steady state error introduced by the strain gauges offset on the tip position. This adaptive control system is subsequently evaluated on a prototype. Simulated and experimental results are presented demonstrating the speed, accuracy, and robustness of the performed control system.},
  archive   = {C_IROS},
  author    = {Saddam Gharab and Selma Benftima and Vicente Feliu Batlle},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802381},
  month     = {10},
  pages     = {9144-9151},
  title     = {Adaptive smith predictor fractional control of a tele-operated flexible link robot *},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modelling and analysis of joint-to-end variable stiffness
for cable-driven hyper-redundant manipulator. <em>IROS</em>, 9138–9143.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To ensure operational accuracy and flexibility in confined environments, the cable-driven hyper-redundant manipulator needs to take into account both compliance and stiffness. Although the cable-driven method enables the manipulator to have adjustable stiffness, the theoretical analyses and studies on the effects of various factors on the stiffness are insufficient, leading to the possibility that the existing variable stiffness strategies may disrupt the equilibrium state of the manipulator. Accordingly, this paper presents the modeling and analysis of joint-to-end variable stiffness for cable-driven hyper-redundant manipulator. First, the multi-layered static models are constructed to decouple and characterise the complex robotic arm system by combining the manipulator kinematics with the virtual work principle. Then, the joint-to-end analytical stiffness models are developed to explore the influencing factors of stiffness, and relevant stiffness indicators are designed to evaluate the stiffness level. Finally, with platform validation and numerical method, the connections between cable tension, joint angle, joint stiffness and end stiffness are analysed, thereby summarising the variable stiffness characteristics of the cable-driven super-redundant manipulator.},
  archive   = {C_IROS},
  author    = {Hongyang Zhang and Shuting Wang and Hu Li and Yuanlong Xie},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801726},
  month     = {10},
  pages     = {9138-9143},
  title     = {Modelling and analysis of joint-to-end variable stiffness for cable-driven hyper-redundant manipulator},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). To help or not to help: LLM-based attentive support for
human-robot group interactions. <em>IROS</em>, 9130–9137. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {How can a robot provide unobtrusive physical support within a group of humans? We present Attentive Support, a novel interaction concept for robots to support a group of humans. It combines scene perception, dialogue acquisition, situation understanding, and behavior generation with the common-sense reasoning capabilities of Large Language Models (LLMs). In addition to following user instructions, Attentive Support is capable of deciding when and how to support the humans, and when to remain silent to not disturb the group. With a diverse set of scenarios, we show and evaluate the robot’s attentive behavior, which supports and helps the humans when required, while not disturbing if no help is needed.},
  archive   = {C_IROS},
  author    = {Daniel Tanneberg and Felix Ocker and Stephan Hasler and Joerg Deigmoeller and Anna Belardinelli and Chao Wang and Heiko Wersing and Bernhard Sendhoff and Michael Gienger},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801517},
  month     = {10},
  pages     = {9130-9137},
  title     = {To help or not to help: LLM-based attentive support for human-robot group interactions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Preliminary result of cury: A backdrivable leg design using
linear actuators. <em>IROS</em>, 9124–9129. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper reports the design, simulation, and experiment of a robotic leg prototype named Cury, which has the potential to achieve minimal clearance and excellent backdrivability. Inspired by human walking data, the actuator design incorporates four-bar linkages and ball screws and is further optimized to meet the torque requirement. The Webots simulation is used to obtain the closed-loop chain description of the robotic leg from fits the URDF specification, and this simulation is used to assess the actuator output requirements at a given predefined trajectory. Leveraging customized ac motors and drives, Cury demonstrates satisfactory trajectory tracking performance using a simple controller. The motor drive design files and Webots simulation files are open-sourced.},
  archive   = {C_IROS},
  author    = {Zhongtao Guan and Yiming Chen and Junlei Zhu and Yu Hu and Weibang Bai and Jiahao Chen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801612},
  month     = {10},
  pages     = {9124-9129},
  title     = {Preliminary result of cury: A backdrivable leg design using linear actuators},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust agility via learned zero dynamics policies.
<em>IROS</em>, 9116–9123. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the design of robust and agile controllers for hybrid underactuated systems. Our approach breaks down the task of creating a stabilizing controller into: 1) learning a mapping that is invariant under optimal control, and 2) driving the actuated coordinates to the output of that mapping. This approach, termed Zero Dynamics Policies, exploits the structure of underactuation by restricting the inputs of the target mapping to the subset of degrees of freedom that cannot be directly actuated, thereby achieving significant dimension reduction. Furthermore, we retain the stability and constraint satisfaction of optimal control while reducing the online computational overhead. We prove that controllers of this type stabilize hybrid underactuated systems and experimentally validate our approach on the 3D hopping platform, ARCHER. Over the course of 3000 hops the proposed framework demonstrates robust agility, maintaining stable hopping while rejecting disturbances on rough terrain.},
  archive   = {C_IROS},
  author    = {Noel Csomay-Shanklin and William D. Compton and Ivan Dario Jimenez Rodriguez and Eric R. Ambrose and Yisong Yue and Aaron D. Ames},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801322},
  month     = {10},
  pages     = {9116-9123},
  title     = {Robust agility via learned zero dynamics policies},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PA-LOCO: Learning perturbation-adaptive locomotion for
quadruped robots. <em>IROS</em>, 9110–9115. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Locomotion control is still a challenging task for quadruped robots traversing diverse terrains amidst unforeseen disturbances. Recently, privileged learning has been employed to learn reliable and robust quadrupedal locomotion over various terrains based on a teacher-student architecture. However, its one-encoder structure is not adequate in addressing external force perturbations. The student policy would experience inevitable performance degradation due to the feature embedding discrepancy between the feature encoder of the teacher policy and the one of the student policy. Hence, this paper presents a privileged learning framework with multiple feature encoders and a residual policy network for robust and reliable quadruped locomotion subject to various external perturbations. The multi-encoder structure can decouple latent features from different privileged information, ultimately leading to enhanced performance of the learned policy in terms of robustness, stability, and reliability. The efficiency of the proposed feature encoding module is analyzed in depth using extensive simulation data. The introduction of the residual policy network helps mitigate the performance degradation experienced by the student policy that attempts to clone the behaviors of a teacher policy. The proposed framework is evaluated on a Unitree GO1 robot, showcasing its performance enhancement over the state-of-the-art privileged learning algorithm through extensive experiments conducted on diverse terrains. Ablation studies are conducted to illustrate the efficiency of the residual policy network.},
  archive   = {C_IROS},
  author    = {Zhiyuan Xiao and Xinyu Zhang and Xiang Zhou and Qingrui Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801753},
  month     = {10},
  pages     = {9110-9115},
  title     = {PA-LOCO: Learning perturbation-adaptive locomotion for quadruped robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning visual quadrupedal loco-manipulation from
demonstrations. <em>IROS</em>, 9102–9109. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Quadruped robots are progressively being integrated into human environments. Despite the growing locomotion capabilities of quadrupedal robots, their interaction with objects in realistic scenes is still limited. While additional robotic arms on quadrupedal robots enable manipulating objects, they are sometimes redundant given that a quadruped robot is essentially a mobile unit equipped with four limbs, each possessing 3 degrees of freedom (DoFs). Hence, we aim to empower a quadruped robot to execute real-world manipulation tasks using only its legs. We decompose the loco-manipulation process into a low-level reinforcement learning (RL)-based controller and a high-level Behavior Cloning (BC)-based planner. By parameterizing the manipulation trajectory, we synchronize the efforts of the upper and lower layers, thereby leveraging the advantages of both RL and BC. Our approach is validated through simulations and real-world experiments, demonstrating the robot’s ability to perform tasks that demand mobility and high precision, such as lifting a basket from the ground while moving, closing a dishwasher, pressing a button, and pushing a door.},
  archive   = {C_IROS},
  author    = {Zhengmao He and Kun Lei and Yanjie Ze and Koushil Sreenath and Zhongyu Li and Huazhe Xu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802742},
  month     = {10},
  pages     = {9102-9109},
  title     = {Learning visual quadrupedal loco-manipulation from demonstrations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Safety-critical autonomous inspection of distillation
columns using quadrupedal robots equipped with roller arms.
<em>IROS</em>, 9094–9101. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a comprehensive framework designed for the autonomous inspection of complex environments, with a specific focus on multi-tiered settings such as distillation column trays. Leveraging quadruped robots equipped with roller arms, and through the use of onboard perception, we integrate essential motion components including: locomotion, safe and dynamic transitions between trays, and intermediate motions that bridge a variety of motion primitives. Given the slippery and confined nature of column trays, it is critical to ensure safety of the robot during inspection, therefore we employ a safety filter and footstep re-planning based upon control barrier function representations of the environment. Our framework integrates all system components into a state machine encoding the developed safety-critical planning and control elements to guarantee safety-critical autonomy, enabling autonomous and safe navigation and inspection of distillation columns. Experimental validation in an environment, consisting of industrial-grade chemical distillation trays, highlights the effectiveness of our multi-layered architecture.},
  archive   = {C_IROS},
  author    = {Jaemin Lee and Jeeseop Kim and Aaron D. Ames},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802679},
  month     = {10},
  pages     = {9094-9101},
  title     = {Safety-critical autonomous inspection of distillation columns using quadrupedal robots equipped with roller arms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluation and design recommendations for a folding
morphing-wheg robot for nuclear characterisation. <em>IROS</em>,
9088–9093. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper explores the design and development of a folding robot required to survey and characterize nuclear facilities only accessible via 150 mm diameter entry ducts. The enclosed legacy facilities at old nuclear sites like Sellafield in the UK have this sort of limited access. When a site reaches the end of its operational life, it must be decommissioned and the resulting waste material must be safely disposed of. The condition, radioactive characteristics, and accessibility of the enclosed environments are unknown; for decommissioning to occur, these environments must be mapped and characterized. For a robot to carry out this task, one of the key requirements is the ability of the robot to traverse rough terrain and obstacles that could be found inside the facility. To accommodate this, while fitting through the entry duct, the chosen design utilizes morphing whegs (i.e., wheel-legs) for locomotion. These are shape-changing wheels that can open out into a set of legs that rotate around an axle, allowing greater traction, diameter, and object traversal ability than wheels alone. The design and morphology of a folding morphing-wheg robot for nuclear characterization, as well as the manufacture and testing of a prototype, is discussed in this paper. A preliminary evaluation of the robot has shown it is capable of climbing up a maximum step height of 150 mm while having a wheel dimension of 100 mm and being able to fit through a 150 mm duct.},
  archive   = {C_IROS},
  author    = {Dominic Murphy and Manuel Giuliani and Paul Bremner},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802195},
  month     = {10},
  pages     = {9088-9093},
  title     = {Evaluation and design recommendations for a folding morphing-wheg robot for nuclear characterisation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Development of bidirectional series elastic actuator with
torsion coil spring and implementation to the legged robot.
<em>IROS</em>, 9082–9087. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many studies have been conducted on Series Elastic Actuators (SEA) for robot joints because they are effective in terms of flexibility, safety, and energy efficiency. The ability of SEA to robustly handle unexpected disturbances has raised expectations for practical applications in environments where robots interact with humans. On the other hand, the development and commercialization of small robots for indoor entertainment applications is also actively underway, and it is thought that by using SEA in these robots, dynamic movements such as jumping and running can be realized. In this work, we developed a small and lightweight SEA using coil springs as elastic elements. By devising a method for fixing the coil spring, it is possible to absorb shock and perform highly accurate force measurement in both rotational directions with a simple structure. In addition, to verify the effectiveness of the developed SEA, we created a small single-legged robot with SEA implemented in the three joints of the hip, knee, and ankle, and we conducted a drop test. By adjusting the initial posture and control gain of each joint, we confirmed that flexible landing and continuous hopping are possible with simple PD position control. The measurement results showed that SEA is effective in terms of shock absorption and energy reuse.This work was performed for research purposes only.},
  archive   = {C_IROS},
  author    = {Yuta Koda and Hiroshi Osawa and Norio Nagatsuka and Shinichi Kariya and Taeko Inagawa and Kensaku Ishizuka},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802064},
  month     = {10},
  pages     = {9082-9087},
  title     = {Development of bidirectional series elastic actuator with torsion coil spring and implementation to the legged robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quadruped robot traversing 3D complex environments with
limited perception. <em>IROS</em>, 9074–9081. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traversing 3-D complex environments has always been a significant challenge for legged locomotion. Existing methods typically rely on external sensors such as vision and lidar to preemptively react to obstacles by acquiring environmental information. However, in scenarios like nighttime or dense forests, external sensors often fail to function properly, necessitating robots to rely on proprioceptive sensors to perceive diverse obstacles in the environment and respond promptly. This task is undeniably challenging. Our research finds that methods based on collision detection can enhance a robot’s perception of environmental obstacles. In this work, we propose an end-to-end learning-based quadruped robot motion controller that relies solely on proprioceptive sensing. This controller can accurately detect, localize, and agilely respond to collisions in unknown and complex 3D environments, thereby improving the robot’s traversability in complex environments. We demonstrate in both simulation and real-world experiments that our method enables quadruped robots to successfully traverse challenging obstacles in various complex environments. The videos and appendix can be found at Quad-Traverse-Go2.github.io},
  archive   = {C_IROS},
  author    = {Yi Cheng and Hang Liu and Guoping Pan and Houde Liu and Linqi Ye},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801507},
  month     = {10},
  pages     = {9074-9081},
  title     = {Quadruped robot traversing 3D complex environments with limited perception},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Structural optimization of lightweight bipedal robot via
SERL. <em>IROS</em>, 9066–9073. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Designing a bipedal robot is a complex and challenging task, especially when dealing with a multitude of structural parameters. Traditional design methods often rely on human intuition and experience. However, such approaches are time-consuming, labor-intensive, lack theoretical guidance and hard to obtain optimal design results within vast design spaces, thus failing to full exploit the inherent performance potential of robots. In this context, this paper introduces the SERL (Structure Evolution Reinforcement Learning) algorithm, which combines reinforcement learning for locomotion tasks with evolution algorithms. The aim is to identify the optimal parameter combinations within a given multidimensional design space. Through the SERL algorithm, we successfully designed a bipedal robot named Wow Orin, where the optimal leg length are obtained through optimization based on body structure and motor torque. We have experimentally validated the effectiveness of the SERL algorithm, which is capable of optimizing the best structure within specified design space and task conditions. Additionally, to assess the performance gap between our designed robot and the current state-of-the-art robots, we compared Wow Orin with mainstream bipedal robots Cassie and Unitree H1. A series of experimental results demonstrate the Outstanding energy efficiency and performance of Wow Orin, further validating the feasibility of applying the SERL algorithm to practical design.},
  archive   = {C_IROS},
  author    = {Yi Cheng and Chenxi Han and Yuheng Min and Houde Liu and Linqi Ye and Hang Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801349},
  month     = {10},
  pages     = {9066-9073},
  title     = {Structural optimization of lightweight bipedal robot via SERL},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). StaccaToe: A single-leg robot that mimics the human leg and
toe. <em>IROS</em>, 9058–9065. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce StaccaToe, a human-scale, electric motor-powered single-leg robot designed to rival the agility of human locomotion through two distinctive attributes: an actuated toe and a co-actuation configuration inspired by the human leg. Leveraging the foundational design of HyperLeg’s lower leg mechanism, we develop a stand-alone robot by incorporating new link designs, custom-designed power electronics, and a refined control system. Unlike previous jumping robots that rely on either special mechanisms (e.g., springs and clutches) or hydraulic/pneumatic actuators, StaccaToe employs electric motors without energy storage mechanisms. This choice underscores our ultimate goal of developing a practical, high-performance humanoid robot capable of human-like, stable walking as well as explosive dynamic movements. In this paper, we aim to empirically evaluate the balance capability and the exertion of explosive ground reaction forces of our toe and co-actuation mechanisms. Throughout extensive hardware and controller development, StaccaToe showcases its control fidelity by demonstrating a balanced tip-toe stance and dynamic jump. This study is significant for three key reasons: 1) StaccaToe represents the first human-scale, electric motor-driven single-leg robot to execute dynamic maneuvers without relying on specialized mechanisms; 2) our research provides empirical evidence of the benefits of replicating critical human leg attributes in robotic design; and 3) we explain the design process for creating agile legged robots, the details that have been scantily covered in academic literature.},
  archive   = {C_IROS},
  author    = {Nisal Perera and Shangqun Yu and Daniel Marew and Mack Tang and Ken Suzuki and Aidan McCormack and Shifan Zhu and Yong-Jae Kim and Donghyun Kim},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801444},
  month     = {10},
  pages     = {9058-9065},
  title     = {StaccaToe: A single-leg robot that mimics the human leg and toe},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HiLMa-res: A general hierarchical framework via residual RL
for combining quadrupedal locomotion and manipulation. <em>IROS</em>,
9050–9057. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents HiLMa-Res, a hierarchical framework leveraging reinforcement learning to tackle manipulation tasks while performing continuous locomotion using quadrupedal robots. Unlike most previous efforts that focus on solving a specific task, HiLMa-Res is designed to be general for various loco-manipulation tasks that require quadrupedal robots to maintain sustained mobility. The novel design of this framework tackles the challenges of integrating continuous locomotion control and manipulation using legs. It develops an operational space locomotion controller that can track arbitrary robot end-effector (toe) trajectories while walking at different velocities. This controller is designed to be generic to different downstream tasks, and therefore, can be utilized in high-level manipulation planning policy to address specific tasks. To demonstrate the versatility of this framework, we utilize HiLMa-Res to tackle several challenging loco-manipulation tasks using a quadrupedal robot in the real world. These tasks span from leveraging state-based policy to vision-based policy, from training purely from the simulation data to learning from real-world data. In these tasks, HiLMa-Res shows better performance than other methods.},
  archive   = {C_IROS},
  author    = {Xiaoyu Huang and Qiayuan Liao and Yiming Ni and Zhongyu Li and Laura Smith and Sergey Levine and Xue Bin Peng and Koushil Sreenath},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802517},
  month     = {10},
  pages     = {9050-9057},
  title     = {HiLMa-res: A general hierarchical framework via residual RL for combining quadrupedal locomotion and manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online determination of legged kinematics. <em>IROS</em>,
9043–9049. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Legged robots are emerging, and legged locomotion is in critical need, which requires precise leg-body kinematics to execute control commands or plan motion trajectories. This paper proposes online state estimation to determine legged kinematics of robots with an arbitrary number of legs, which includes the kinematic parameters of the leg-body transformation, time offset and the leg link lengths. In particular, we advocate an in-place dance gait for kinematic determination where the toes remain static on the ground and serve as static landmarks as in SLAM. As a visual-inertial sensor is typically available onboard robot and located at the floating base, we leverage efficient MSCKF-based visual-inertial navigation to estimate legged kinematics. To this end, we analytically derive the legged kinematic measurements and tightly fuse them along with visual-inertial measurements for MSCKF update of both the leg’s kinematics and body’s motion. The proposed method has been extensively validated in both simulations and experiments with different quadrupeds, showing its robustness and accuracy.},
  archive   = {C_IROS},
  author    = {Chinmay Burgul and Woosik Lee and Patrick Geneva and Guoquan Huang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801595},
  month     = {10},
  pages     = {9043-9049},
  title     = {Online determination of legged kinematics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Understanding how a 3-dimensional ZMP exactly decouples the
horizontal and vertical dynamics of the CoM-ZMP model. <em>IROS</em>,
9036–9042. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The CoM-ZMP model represents the dominant behaviour of bipedal locomotion with surface contact. However, once the centre of mass (CoM) position goes out of a predefined spatial plane, the horizontal dynamics of the model can couple with its vertical dynamics to be nonlinear. This study theoretically investigates the properties of the 3-dimensional zero moment point (ZMP), lying apart from the actual ground to resolve the coupling. The presented discussion includes the compatibility of the 3D ZMP with ZMPs used in preceding research, such as the linear inverted pendulum mode, the existence of a virtual repellent point considering the arbitrary vertical CoM motion, the parameter invariance of the CoM-ZMP model, and feasible regions of the ZMP.},
  archive   = {C_IROS},
  author    = {Yuki Onishi and Shuuji Kajita},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802151},
  month     = {10},
  pages     = {9036-9042},
  title     = {Understanding how a 3-dimensional ZMP exactly decouples the horizontal and vertical dynamics of the CoM-ZMP model},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time coupled centroidal motion and footstep planning
for biped robots. <em>IROS</em>, 9030–9035. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an algorithm that finds a centroidal motion and footstep plan for a Spring-Loaded Inverted Pendulum (SLIP)-like bipedal robot model substantially faster than real-time. This is achieved with a novel representation of the dynamic footstep planning problem, where each point in the environment is considered a potential foothold that can apply a force to the center of mass to keep it on a desired trajectory. For a biped, up to two such footholds per time step must be selected, and we approximate this cardinality constraint with an iteratively reweighted l1-norm minimization. Along with a linearizing approximation of an angular momentum constraint, this results in a quadratic program can be solved for a contact schedule and center of mass trajectory with automatic gait discovery. A 2 s planning horizon with 13 time steps and 20 surfaces available at each time is solved in 142 ms, roughly ten times faster than comparable existing methods in the literature. We demonstrate the versatility of this program in a variety of simulated environments.},
  archive   = {C_IROS},
  author    = {Tara Bartlett and Ian R. Manchester},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802557},
  month     = {10},
  pages     = {9030-9035},
  title     = {Real-time coupled centroidal motion and footstep planning for biped robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Research on autonomous navigation of dual-mode wheel-legged
robot. <em>IROS</em>, 9022–9029. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In order to improve the terrain adaptability and energy efficiency of wheel-legged robot in complex environment, a dual-mode navigation system based on robot energy consumption model is proposed. Firstly, the obstacle trafficability is evaluated according to the maximum obstacle crossing capability of the robot, and the two-dimensional grid map is preprocessed. Secondly, the established energy consumption model is integrated into the evaluation function of A* algorithm, and the eight-adjacency expansion mode is improved to search the surrounding nodes according to the obstacle characteristics of the robot. In the obstacle-crossing area, the obstacle-crossing and obstacle-bypassing modes are intelligently switched based on the principle of minimum energy consumption. Finally, a dual-mode robot navigation system is built, and the experimental results show that the proposed navigation system reduces the average energy consumption, path length, and steering angle by 16.8%, 24.7%, and 31.18%, respectively.},
  archive   = {C_IROS},
  author    = {Wen Wang and Xiaobin Xu and ZiHeng Chen and Jian Yang and Yingying Ran and Zhiying Tan and Minzhou Luo},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801460},
  month     = {10},
  pages     = {9022-9029},
  title     = {Research on autonomous navigation of dual-mode wheel-legged robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Miscommunication between robots can improve group accuracy
in best-of-n decision-making. <em>IROS</em>, 9014–9021. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Making fast and accurate consensus decisions through local communication and decentralised control in a swarm of simple robots can be a very challenging endeavour. In swarms of robots with limited capabilities, consensus decisions can be made using simple voting rules. In our study, the robots use rules based on the cross-inhibition model, which describes a voting mechanism observed in the house-hunting honeybee, that has been shown to efficiently allow consensus achievement in distributed robotic systems. The cross-inhibition mechanism has been shown to lead to a highly stable consensus, preventing the correction of possible group decision errors which can happen, for example, due to high noise in robots’ estimations. In this paper, we investigate the impact of miscommunication on the speed-accuracy trade-off in consensus decision-making in the context of a binary discrimination problem—i.e., choosing collectively the best of two options. We evaluate the accuracy of decision-making theoretically, using continuous and finite-size models, and experimentally in a collective perception scenario, using swarms of 100 simulated robots and 50 real Kilobots. Our study suggests that a certain level of miscommunication (or communication noise) among agents can increase the decision’s accuracy and, thus, can serve an important functional role in making collective decisions in robot swarms.},
  archive   = {C_IROS},
  author    = {Raina Zakir and Marco Dorigo and Andreagiovanni Reina},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802464},
  month     = {10},
  pages     = {9014-9021},
  title     = {Miscommunication between robots can improve group accuracy in best-of-n decision-making},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robot swarm control based on smoothed particle hydrodynamics
for obstacle-unaware navigation. <em>IROS</em>, 9006–9013. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot swarms hold immense potential for performing complex tasks far beyond the capabilities of individual robots. However, the challenge in unleashing this potential is the robots’ limited sensory capabilities, which hinder their ability to detect and adapt to unknown obstacles in real-time. To overcome this limitation, we introduce a novel robot swarm control method with an indirect obstacle detector using a smoothed particle hydrodynamics (SPH) model. The indirect obstacle detector can predict the collision with an obstacle and its collision point solely from the robot’s velocity information. This approach enables the swarm to effectively and accurately navigate environments without the need for explicit obstacle detection, significantly enhancing their operational robustness and efficiency. Our method’s superiority is quantitatively validated through a comparative analysis, showcasing its significant navigation and pattern formation improvements under obstacle-unaware conditions.},
  archive   = {C_IROS},
  author    = {Michikuni Eguchi and Mai Nishimura and Shigeo Yoshida and Takefumi Hiraki},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801800},
  month     = {10},
  pages     = {9006-9013},
  title     = {Robot swarm control based on smoothed particle hydrodynamics for obstacle-unaware navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Language-guided pattern formation for swarm robotics with
multi-agent reinforcement learning. <em>IROS</em>, 8998–9005. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper explores leveraging the vast knowledge encoded in Large Language Models (LLMs) to tackle pattern formation challenges for swarm robotics systems. A new framework, named LGPF (Language-Guided Pattern Formation), is proposed to address these challenges. The framework breaks down the pattern formation into two key components: pattern synthesis and swarm robotics control. For the former, this study utilizes the exceptional few-shot generalizability of LLMs to translate high-level natural language descriptions into the desired spatial pattern coordinates. This approach allows for overcoming previous limitations in representing and designing complex patterns. The framework further employs a centralized training with decentralized execution (CTDE) based multiagent reinforcement learning (MARL) approach to control the swarm robots in forming the specified pattern while avoiding collisions. The decentralized policies learned with the CTDE-based MARL algorithm consider coordination between robots without direct communication under a partially observable setup. To validate the effectiveness of our framework, we perform extensive experiments in both simulation and real-world environments. These experiments validate LGPF’s effectiveness in accurately and safely forming diverse user-specified patterns.},
  archive   = {C_IROS},
  author    = {Hsu-Shen Liu and So Kuroki and Tadashi Kozuno and Wei-Fang Sun and Chun-Yi Lee},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801665},
  month     = {10},
  pages     = {8998-9005},
  title     = {Language-guided pattern formation for swarm robotics with multi-agent reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decentralized trajectory planning for formation flight in
unknown and dense environments. <em>IROS</em>, 8990–8997. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For aerial swarms, formation flight has been applied in various scenes. However, most existing works do not consider balancing the conflicting requirements among keeping formation, keeping the smoothness of trajectories, and obstacle avoidance within the limited time. To address this issue, we propose a decentralized trajectory planning framework for formation flight in unknown and dense environments. To ensure that feasible trajectories can be found within the limited time, the formation optimization problem is decoupled into formation affine transformation and iterative trajectory generation. Firstly, the optimization problem based on affine transformation is designed to obtain the optimal affine transformation sequence, which provides the formation reference of trajectory optimization. Secondly, the iterative optimization framework of trajectory planning is designed, which balances the conflicting requirements of formation, smooth flight, and obstacle avoidance. Besides, to escape the local minima caused by non-convex dense environments, the method of topological path planning is designed to provide distinctive initial solutions for trajectory optimization. Finally, the proposed methods are proven to be effective through the simulations and real-world experiments.},
  archive   = {C_IROS},
  author    = {Jianxin Zeng and Yaonan Wang and Zhiqiang Miao and Wei He and Hesheng Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802409},
  month     = {10},
  pages     = {8990-8997},
  title     = {Decentralized trajectory planning for formation flight in unknown and dense environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reality fusion: Robust real-time immersive mobile robot
teleoperation with volumetric visual data fusion. <em>IROS</em>,
8982–8989. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce Reality Fusion, a novel robot teleoperation system that localizes, streams, projects, and merges a typical onboard depth sensor with a photorealistic, high resolution, high framerate, and wide FoV rendering of the complex remote environment represented as 3D Gaussian splats (3DGS). Our framework enables robust egocentric and exocentric robot teleoperation in immersive VR, with the 3DGS effectively extending spatial information of a depth sensor with limited FoV and balancing the trade-off between data streaming costs and data visual quality. We evaluated our framework through a user study with 24 participants, which revealed that Reality Fusion leads to significantly better user performance, situation awareness, and user preferences. To support further research and development, we provide an open-source implementation with an easy-to-replicate custom-made telepresence robot, a high-performance virtual reality 3DGS renderer, and an immersive robot control package.1},
  archive   = {C_IROS},
  author    = {Ke Li and Reinhard Bacher and Susanne Schmidt and Wim Leemans and Frank Steinicke},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802431},
  month     = {10},
  pages     = {8982-8989},
  title     = {Reality fusion: Robust real-time immersive mobile robot teleoperation with volumetric visual data fusion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient-PIP: Large-scale pixel-level aligned image pair
generation for cross-time infrared-RGB translation. <em>IROS</em>,
8974–8981. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generative models are gaining momentum in both academic and industrial applications driven by the availability of large-scale datasets, especially in tasks involving Image-to-Image Translation. Meanwhile, poor human perception of nighttime environment has led to a demand for translation from night-vision infrared to day-vision RGB images. However, collecting such cross-modal training data at the same time is impossible due to the thermal imaging properties of infrared cameras, the challenge lies in constructing image pairs during the day and at night respectively, where the requirement for data alignment poses significant difficulties. In this paper, we propose a Pixel-level aligned Image Pair generation framework PIP to explore efficient colorization of high-resolution infrared images. Specifically, we first construct a 3D high-precision point cloud map for the purpose of establishing the correlation between day and night scenes. Corresponding point clouds of modal images are collected simultaneously during data acquisition to obtain image sensor poses by Global Matching with the map, which allows us to calculate the transformation relationship from infrared to RGB image coordinate systems based on the sensor parameters and depth information of the map. Leveraging the relationship, the pixel values of RGB image is projected onto the infrared image followed by optimization as the colored image. Accordingly, we present a dataset NUDT-PIP, the first of its kind containing large-scale pixel-level aligned cross-time infrared-RGB image pairs of complicated real road scenes. Experimental results demonstrate the reliability and strong applicability of our dataset in Image-to-Image Translation. Our code will be released at https://github.com/wjjjjyourFA/NUDT-PIP.},
  archive   = {C_IROS},
  author    = {Jian Li and Kexin Fei and Yi Sun and Jie Wang and Bokai Liu and Zongtan Zhou and Yongbin Zheng and Zhenping Sun},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802486},
  month     = {10},
  pages     = {8974-8981},
  title     = {Efficient-PIP: Large-scale pixel-level aligned image pair generation for cross-time infrared-RGB translation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep visual odometry with events and frames. <em>IROS</em>,
8966–8973. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual Odometry (VO) is crucial for autonomous robotic navigation, especially in GPS-denied environments like planetary terrains. To improve robustness, recent model-based VO systems have begun combining standard and event-based cameras. While event cameras excel in low-light and high-speed motion, standard cameras provide dense and easier-to-track features. However, the field of image- and event-based VO still predominantly relies on model-based methods and is yet to fully integrate recent image-only advancements leveraging end-to-end learning-based architectures. Seamlessly integrating the two modalities remains challenging due to their different nature, one asynchronous, the other not, limiting the potential for a more effective image- and event-based VO. We introduce RAMP-VO, the first end-to-end learned image- and event-based VO system. It leverages novel Recurrent, Asynchronous, and Massively Parallel (RAMP) encoders capable of fusing asynchronous events with image data, providing 8× faster inference and 33% more accurate predictions than existing solutions. Despite being trained only in simulation, RAMP-VO outperforms previous methods on the newly introduced Apollo and Malapert datasets, and on existing benchmarks, where it improves image- and event-based methods by 58.8% and 30.6%, paving the way for robust and asynchronous VO in space.Multimedial Material: For code and datasets visit https://github.com/uzh-rpg/rampvo.},
  archive   = {C_IROS},
  author    = {Roberto Pellerito and Marco Cannici and Daniel Gehrig and Joris Belhadj and Olivier Dubois-Matra and Massimo Casasco and Davide Scaramuzza},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801307},
  month     = {10},
  pages     = {8966-8973},
  title     = {Deep visual odometry with events and frames},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Event-free moving object segmentation from moving ego
vehicle. <em>IROS</em>, 8960–8965. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Moving object segmentation (MOS) in dynamic scenes is an important, challenging, but under-explored research topic for autonomous driving, especially for sequences obtained from moving ego vehicles. Most segmentation methods leverage motion cues obtained from optical flow maps. However, since these methods are often based on optical flows that are pre-computed from successive RGB frames, this neglects the temporal consideration of events occurring within the inter-frame, consequently constraining its ability to discern objects exhibiting relative staticity but genuinely in motion. To address these limitations, we propose to exploit event cameras for better video understanding, which provide rich motion cues without relying on optical flow. To foster research in this area, we first introduce a novel large-scale dataset called DSEC-MOS for moving object segmentation from moving ego vehicles, which is the first of its kind. For benchmarking, we select various mainstream methods and rigorously evaluate them on our dataset. Subsequently, we devise EmoFormer, a novel network able to exploit the event data. For this purpose, we fuse the event temporal prior with spatial semantic maps to distinguish genuinely moving objects from the static background, adding another level of dense supervision around our object of interest. Our proposed network relies only on event data for training but does not require event input during inference, making it directly comparable to frame-only methods in terms of efficiency and more widely usable in many application cases. The exhaustive comparison highlights a significant performance improvement of our method over all other methods. The source code and dataset are publicly available at: https://github.com/ZZYZhou/DSEC-MOS.},
  archive   = {C_IROS},
  author    = {Zhuyun Zhou and Zongwei Wu and Danda Pani Paudel and Rémi Boutteau and Fan Yang and Luc Van Gool and Radu Timofte and Dominique Ginhac},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801383},
  month     = {10},
  pages     = {8960-8965},
  title     = {Event-free moving object segmentation from moving ego vehicle},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Translating agent-environment interactions from humans to
robots. <em>IROS</em>, 8952–8959. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans are remarkably adept at imitating other people performing tasks, afforded by their ability to abstract away irrelevant details and focus on the task strategy of the demonstrator. In this paper, we take steps towards enabling robots with this ability, and present a framework, TransAct to do so. TransAct first builds on prior skill learning work to learn temporally abstract representations of common agent-environment interactions in manipulation tasks, e.g., a robot pouring from a cup. Given a human demonstration of an unseen unknown task, TransAct then translates the underlying sequence of interactions (i.e., the human task strategy) to a robot learner. Through experiments on real-world human and robot datasets, we demonstrate TransAct’s ability to accurately represent diverse agent-environment interactions. Moreover, TransAct empowers robots to consume human task demonstrations and compose corresponding interactions with similar environmental effects to perform the tasks themselves in a zero shot manner, without access to paired demonstrations or dense annotations. We present visualizations of our results at https://sites.google.com/view/interaction-abstractions.},
  archive   = {C_IROS},
  author    = {Tanmay Shankar and Chaitanya Chawla and Almutwakel Hassan and Jean Oh},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802752},
  month     = {10},
  pages     = {8952-8959},
  title     = {Translating agent-environment interactions from humans to robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning human-to-humanoid real-time whole-body
teleoperation. <em>IROS</em>, 8944–8951. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present Human to Humanoid (H2O), a reinforcement learning (RL) based framework that enables real-time whole-body teleoperation of a full-sized humanoid robot with only an RGB camera. To create a large-scale retargeted motion dataset of human movements for humanoid robots, we propose a scalable &quot;sim-to-data&quot; process to filter and pick feasible motions using a privileged motion imitator. Afterwards, we train a robust real-time humanoid motion imitator in simulation using these refined motions and transfer it to the real humanoid robot in a zero-shot manner. We successfully achieve teleoperation of dynamic whole-body motions in real-world scenarios, including walking, back jumping, kicking, turning, waving, pushing, boxing, etc. To the best of our knowledge, this is the first demonstration to achieve learning-based real-time whole-body humanoid teleoperation.},
  archive   = {C_IROS},
  author    = {Tairan He and Zhengyi Luo and Wenli Xiao and Chong Zhang and Kris Kitani and Changliu Liu and Guanya Shi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801984},
  month     = {10},
  pages     = {8944-8951},
  title     = {Learning human-to-humanoid real-time whole-body teleoperation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning bimanual manipulation policies for bathing
bed-bound people. <em>IROS</em>, 8936–8943. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Assistive robots hold promise in enhancing the quality of life for older adults and people with mobility impairments in daily bed bathing routines. When providing bathing assistance to bed-bound people, human caregivers often support the joints when lifting the arms and legs to properly wash and dry occluded areas. This research introduces a novel approach to robotic bed bathing manipulation, where a bimanual robot learns to lift a target limb while controlling a cleaning tool to bath the surface within safe force bounds. To ensure safe, cooperative bath manipulation, our work combines Multi-Agent Reinforcement Learning (MARL) framework with a variable impedance action space enabling adaptive interaction with the environment and carefully-designed reward functions regulating contact force on the human body. Simulation results demonstrate improved bathing area coverage compared to unimanual models and exhibit great adaptability to contact-rich interaction within a safe force boundary. We validate our approach across various human body sizes, showcasing its generalizability. We also transfer our models to a physical Baxter robot bathing a medical-grade manikin. We further incorporate a force tracking controller with the trained models to enhance adaptation to noisy real-world bathing scenarios. To the best of our knowledge, this is the first robot-assisted bed bathing application that performs autonomous bathing around the human body using bimanual robot arms.},
  archive   = {C_IROS},
  author    = {Yijun Gu and Yiannis Demiris},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801478},
  month     = {10},
  pages     = {8936-8943},
  title     = {Learning bimanual manipulation policies for bathing bed-bound people},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LAVA: Long-horizon visual action based food acquisition.
<em>IROS</em>, 8929–8935. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic Assisted Feeding (RAF) addresses the fundamental need for individuals with mobility impairments to regain autonomy in feeding themselves. The goal of RAF is to use a robot arm to acquire and transfer food to individuals from the table. Existing RAF methods primarily focus on solid foods, leaving a gap in manipulation strategies for semisolid and deformable foods. We present Long-horizon Visual Action-based (LAVA) food acquisition of liquid, semisolid, and deformable foods. Long-horizon refers to the goal of &quot;clearing the bowl&quot; by sequentially acquiring the food from the bowl. LAVA is hierarchical: (1) At the highest level, we determine primitives using ScoopNet. (2) At the mid-level, LAVA finds parameters for the low-level primitives. (3) At the lowest level, LAVA carries out action execution using behavior cloning. We validate LAVA on real-world acquisition trials involving granular, liquid, semisolid, and deformable foods along with fruit chunks and soup. Across 46 bowls, LAVA acquires much more efficiently than baselines with a success rate of 89±4%, and generalizes across realistic plate variations such as varying positions, varieties, and amount of food in the bowl. Datasets and supplementary materials can be found on our website.},
  archive   = {C_IROS},
  author    = {Amisha Bhaskar and Rui Liu and Vishnu D. Sharma and Guangyao Shi and Pratap Tokekar},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802738},
  month     = {10},
  pages     = {8929-8935},
  title     = {LAVA: Long-horizon visual action based food acquisition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inverse submodular maximization with application to
human-in-the-loop multi-robot multi-objective coverage control.
<em>IROS</em>, 8921–8928. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider a new type of inverse combinatorial optimization, Inverse Submodular Maximization (ISM), for human-in-the-loop multi-robot coordination. Forward combinatorial optimization - solving a combinatorial problem given the reward (cost)-related parameters - is widely used in multi-robot coordination. In the standard pipeline, the reward (cost)-related parameters are designed offline by domain experts. These parameters are utilized for coordinating robots online. What if non-expert human supervisors desire to change these parameters during task execution to adapt to some new requirements? We are interested in the case where human supervisors can suggest what actions to take, and the robots need to change these internal parameters accordingly. We study such problems from the perspective of inverse combinatorial optimization, i.e., the process of finding parameters given solutions to the problem. Specifically, we propose a new formulation for ISM, in which we aim to find a new set of parameters that minimally deviate from the current parameters while causing a greedy algorithm to output actions which are the same as those desired by the human supervisors. We show that such problems can be formulated as a Mixed Integer Quadratic Program (MIQP) which is intractable for existing solvers when the problem size is large. We propose a new Branch &amp; Bound algorithm to solve such problems. In numerical simulations, we demonstrate how to use ISM in multi-robot multi-objective coverage control, and we show that the proposed algorithm provides significant advantages in running time and peak memory usage compared to directly using an existing solver.},
  archive   = {C_IROS},
  author    = {Guangyao Shi and Gaurav S. Sukhatme},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801908},
  month     = {10},
  pages     = {8921-8928},
  title     = {Inverse submodular maximization with application to human-in-the-loop multi-robot multi-objective coverage control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing crowd-aware multi-agent path finding through
local communication with graph neural networks. <em>IROS</em>, 8913. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-Agent Path Finding (MAPF) in crowded environments presents a challenging problem in motion planning, aiming to find collision-free paths for all agents in the system. MAPF finds a wide range of applications in various domains, including aerial swarms, autonomous warehouse robotics, and self-driving vehicles. Current approaches to MAPF generally fall into two main categories: centralized and decentralized planning. Centralized planning suffers from the curse of dimensionality when the number of agents or states increases and thus does not scale well in large and complex environments. On the other hand, decentralized planning enables agents to engage in real-time path planning within a partially observable environment, demonstrating implicit coordination. However, they suffer from slow convergence and performance degradation in dense environments. In this paper, we introduce CRAMP, a novel crowd-aware decentralized reinforcement learning approach to address this problem by enabling efficient local communication among agents via Graph Neural Networks (GNNs), facilitating situational awareness and decision-making capabilities in congested environments. We test CRAMP on simulated environments and demonstrate that our method outperforms the state-of-the-art decentralized methods for MAPF on various metrics. CRAMP improves the solution quality up to 59% measured in makespan and collision count, and up to 35% improvement in success rate in comparison to previous methods.},
  archive   = {C_IROS},
  author    = {Phu Pham and Aniket Bera},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801969},
  month     = {10},
  pages     = {8913},
  title     = {Optimizing crowd-aware multi-agent path finding through local communication with graph neural networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SiCP: Simultaneous individual and cooperative perception for
3D object detection in connected and automated vehicles. <em>IROS</em>,
8905–8912. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cooperative perception for connected and automated vehicles is traditionally achieved through the fusion of feature maps from two or more vehicles. However, the absence of feature maps shared from other vehicles can lead to a significant decline in 3D object detection performance for cooperative perception models compared to standalone 3D detection models. This drawback impedes the adoption of cooperative perception as vehicle resources are often insufficient to concurrently employ two perception models. To tackle this issue, we present Simultaneous Individual and Cooperative Perception (SiCP), a generic framework that supports a wide range of the state-of-the-art standalone perception backbones and enhances them with a novel Dual-Perception Network (DP-Net) designed to facilitate both individual and cooperative perception. In addition to its lightweight nature with only 0.13M parameters, DP-Net is robust and retains crucial gradient information during feature map fusion. As demonstrated in a comprehensive evaluation on the V2V4Real and OPV2V datasets, thanks to DP-Net, SiCP surpasses state-of-the-art cooperative perception solutions while preserving the performance of standalone perception solutions. The source code can be found at https://github.com/DarrenQu/SiCP.},
  archive   = {C_IROS},
  author    = {Deyuan Qu and Qi Chen and Tianyu Bai and Hongsheng Lu and Heng Fan and Hao Zhang and Song Fu and Qing Yang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801398},
  month     = {10},
  pages     = {8905-8912},
  title     = {SiCP: Simultaneous individual and cooperative perception for 3D object detection in connected and automated vehicles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). D3G: Learning multi-robot coordination from demonstrations.
<em>IROS</em>, 8898–8904. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper develops a new Distributed approach for solving the inverse problem of a Differentiable Dynamic Game (D3G), which enables robots to learn multi-robot coordination from given demonstrations. We formulate multi-robot coordination as the Nash equilibrium of a parameterized dynamic game, where the behavior of each robot is dictated by an objective function that also depends on the behavior of its neighboring robots. The coordination thus can be adapted by tuning the parameters of the objective and the local dynamics of each robot. The proposed algorithm enables each robot to automatically tune such parameters in a distributed and coordinated fashion — only using the data of its neighbors without global information. Its key novelty is the development of a distributed solver for a diff-KKT condition that can enhance scalability and reduce the computational load for gradient computation. We test the proposed algorithm in simulation with heterogeneous robots given different task configurations. The results demonstrate its effectiveness and generalizability for learning multi-robot coordination from demonstrations.},
  archive   = {C_IROS},
  author    = {Yizhi Zhou and Wanxin Jin and Xuan Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801743},
  month     = {10},
  pages     = {8898-8904},
  title     = {D3G: Learning multi-robot coordination from demonstrations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Recurrent non-rigid point cloud registration. <em>IROS</em>,
8890–8897. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Non-rigid point cloud registration remains a significant challenge in 3D computer vision due to the complexity of structural deforms, lack of overlaps, and sensitivity to initialization. This paper introduces a framework inspired by the recent success in recurrent architecture, adapted to accommodate the unique characteristics of point clouds. More specifically, we design a recurrent update network block for progressively refining local registration results under a local rigidity assumption, starting from an initial global SE(3) alignment. Through comparison, our method consistently outperforms competing methods in standard metrics, achieving a 33% reduction in EPE on the 4DLoMatch benchmark compared to the second-best method. To the best of our knowledge, the proposed method is the first to successfully demonstrate that the recurrent update strategy can effectively address the non-rigid registration task with large displacement, significant deform, and low overlap. The source code and the model will be released at http://dummy.url/.},
  archive   = {C_IROS},
  author    = {Yue Cao and Ziang Cheng and Hongdong Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801631},
  month     = {10},
  pages     = {8890-8897},
  title     = {Recurrent non-rigid point cloud registration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NeuSurfEmb: A complete pipeline for dense
correspondence-based 6D object pose estimation without CAD models.
<em>IROS</em>, 8882–8889. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {State-of-the-art approaches for 6D object pose estimation assume the availability of CAD models and require the user to manually set up physically-based rendering (PBR) pipelines for synthetic training data generation. Both factors limit the application of these methods in real-world scenarios. In this work, we present a pipeline that does not require CAD models and allows training a state-of-the-art pose estimator requiring only a small set of real images as input. Our method is based on a NeuS2 [1] object representation, that we learn through a semi-automated procedure based on Structure-from-Motion (SfM) and object-agnostic segmentation. We exploit the novel-view synthesis ability of NeuS2 and simple cut-and-paste augmentation to automatically generate photorealistic object renderings, which we use to train the correspondence-based SurfEmb [2] pose estimator. We evaluate our method on the LINEMOD-Occlusion dataset, extensively studying the impact of its individual components and showing competitive performance with respect to approaches based on CAD models and PBR data. We additionally demonstrate the ease of use and effectiveness of our pipeline on self-collected real-world objects, showing that our method outperforms state-of-the-art CAD-model-free approaches, with better accuracy and robustness to mild occlusions. To allow the robotics community to benefit from this system, we will publicly release it at https://www.github.com/ethz-asl/neusurfemb.},
  archive   = {C_IROS},
  author    = {Francesco Milano and Jen Jen Chung and Hermann Blum and Roland Siegwart and Lionel Ott},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801399},
  month     = {10},
  pages     = {8882-8889},
  title     = {NeuSurfEmb: A complete pipeline for dense correspondence-based 6D object pose estimation without CAD models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LiOn-XA: Unsupervised domain adaptation via LiDAR-only
cross-modal adversarial training. <em>IROS</em>, 8875–8881. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose LiOn-XA, an unsupervised domain adaptation (UDA) approach that combines LiDAR-Only Cross-Modal (X) learning with Adversarial training for 3D LiDAR point cloud semantic segmentation to bridge the domain gap arising from environmental and sensor setup changes. Unlike existing works that exploit multiple data modalities like point clouds and RGB image data, we address UDA in scenarios where RGB images might not be available and show that two distinct LiDAR data representations can learn from each other for UDA. More specifically, we leverage 3D voxelized point clouds to preserve important geometric structure in combination with 2D projection-based range images that provide information such as object orientations or surfaces. To further align the feature space between both domains, we apply adversarial training using both features and predictions of both 2D and 3D neural networks. Our experiments on 3 real-to-real adaptation scenarios demonstrate the effectiveness of our approach, achieving new state-of-the-art performance when compared to previous uni- and multi-model UDA methods. Our source code is publicly available at https://github.com/JensLe97/lion-xa.},
  archive   = {C_IROS},
  author    = {Thomas Kreutz and Jens Lemke and Max Mühlhäuser and Alejandro Sanchez Guinea},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801916},
  month     = {10},
  pages     = {8875-8881},
  title     = {LiOn-XA: Unsupervised domain adaptation via LiDAR-only cross-modal adversarial training},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GenerOcc: Self-supervised framework of real-time 3D
occupancy prediction for monocular generic cameras. <em>IROS</em>,
8868–8874. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the context of 3D scene perception tasks, the significance of 3D occupancy prediction has been progressively growing, aiming to forecast the occupancy state of voxels in a discrete 3D space. However, existing methods typically exhibit several limitations, such as restricted adaptability to non-pinhole cameras due to fixed camera parameters, heavy reliance on 3D annotations because of the inability to project 3D output back to the camera plane, and inferior real-time inference performance resulting from the conversion process from 2D to 3D features. To address these constrains, we introduce GenerOcc, a self-supervised framework of real-time 3D occupancy prediction for monocular generic cameras. We have collected the fisheye Dominant dataset to confirm the compatibility of our ray-based camera model with non-pinhole cameras. By transforming the occupancy prediction task into a depth estimation task in a self-supervised manner, we eliminate dependency on 3D annotations. Furthermore, we propose a parametric voxel probability distribution module that leverages 2D features to quickly predict 3D occupancy without 3D representations of the scene. Additionally, our GenerOcc has been extensively evaluated on public pinhole Occ3D-nuScenes dataset and our proprietary fisheye Dominant dataset, both yielding impressive performance.},
  archive   = {C_IROS},
  author    = {Xianghui Pan and Jiayuan Du and Shuai Su and Wenhao Zong and Xiao Wang and Chengju Liu and Qijun Chen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802337},
  month     = {10},
  pages     = {8868-8874},
  title     = {GenerOcc: Self-supervised framework of real-time 3D occupancy prediction for monocular generic cameras},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Asymptotically optimal lazy lifelong sampling-based
algorithm for efficient motion planning in dynamic environments.
<em>IROS</em>, 8861–8867. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The paper introduces an asymptotically optimal lifelong sampling-based path planning algorithm that combines the merits of lifelong planning algorithms and lazy search algorithms for rapid replanning in dynamic environments where edge evaluation is expensive. By evaluating only sub-path candidates for the optimal solution, the algorithm saves considerable evaluation time and thereby reduces the overall planning cost. It employs a novel informed rewiring cascade to efficiently repair the search tree when the underlying search graph changes. Simulation results demonstrate that the algorithm outperforms various state-of-the-art sampling-based planners in addressing both static and dynamic motion planning problems.},
  archive   = {C_IROS},
  author    = {Lu Huang and Xingjian Jing},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802657},
  month     = {10},
  pages     = {8861-8867},
  title     = {Asymptotically optimal lazy lifelong sampling-based algorithm for efficient motion planning in dynamic environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Expansion-GRR: Efficient generation of smooth global
redundancy resolution roadmaps. <em>IROS</em>, 8854–8860. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Global redundancy resolution (GRR) roadmap is a novel concept in robotics that facilitates the mapping from task space paths to configuration space paths in a legible, predictable, and repeatable way. Such roadmaps could find widespread utility in applications such as safe teleoperation, consistent path planning, and motion primitives generation. However, previous methods to compute GRR roadmaps often necessitate a lengthy computation time and produce non-smooth paths, limiting their practical efficacy. To address this challenge, we introduce a novel method EXPANSION-GRR that leverages efficient configuration space projections and enables a rapid generation of smooth roadmaps that satisfy the task constraints. Additionally, we propose a simple multi-seed strategy that further enhances the final quality. We conducted experiments in simulation with a 5-link planar manipulator and a Kinova arm. We were able to generate the GRR roadmaps up to 2 orders of magnitude faster while achieving higher smoothness. We also demonstrate the utility of the GRR roadmaps in teleoperation tasks where our method outperformed prior methods and reactive IK solvers in terms of success rate and solution quality.},
  archive   = {C_IROS},
  author    = {Zhuoyun Zhong and Zhi Li and Constantinos Chamzas},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801917},
  month     = {10},
  pages     = {8854-8860},
  title     = {Expansion-GRR: Efficient generation of smooth global redundancy resolution roadmaps},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A mixed-integer conic program for the moving-target
traveling salesman problem based on a graph of convex sets.
<em>IROS</em>, 8847–8853. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a new formulation that finds the optimum for the Moving-Target Traveling Salesman Problem (MT-TSP), which seeks to find a shortest path for an agent, that starts at a depot, visits a set of moving targets exactly once within their assigned time-windows, and returns to the depot. The formulation relies on the key idea that when the targets move along lines, their trajectories become convex sets within the space-time coordinate system. The problem then reduces to finding the shortest path within a graph of convex sets, subject to some speed constraints. We compare our formulation with the current state-of-the-art Mixed Integer Conic Program (MICP) formulation for the MT-TSP. The experimental results show that our formulation outperforms the MICP for instances with up to 20 targets, with up to two orders of magnitude reduction in runtime, and up to a 60% tighter optimality gap. We also show that the solution cost from the convex relaxation of our formulation provides significantly tighter lower-bounds for the MT-TSP than the ones from the MICP.},
  archive   = {C_IROS},
  author    = {Allen George Philip and Zhongqiang Ren and Sivakumar Rathinam and Howie Choset},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802374},
  month     = {10},
  pages     = {8847-8853},
  title     = {A mixed-integer conic program for the moving-target traveling salesman problem based on a graph of convex sets},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Agile and safe trajectory planning for quadruped navigation
with motion anisotropy awareness. <em>IROS</em>, 8839–8846. (<a
href="https://doi.org/10.1109/IROS58592.2024.10803105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Quadruped robots demonstrate robust and agile movements in various terrains; however, their navigation autonomy is still insufficient. One of the challenges is that the motion capabilities of the quadruped robot are anisotropic along different directions, which significantly affects the safety of quadruped robot navigation. This paper proposes a navigation framework that takes into account the motion anisotropy of quadruped robots including kinodynamic trajectory generation, nonlinear trajectory optimization, and nonlinear model predictive control. In simulation and real robot tests, we demonstrate that our motion-anisotropy-aware navigation framework could: (1) generate more efficient trajectories and realize more agile quadruped navigation; (2) significantly improve the navigation safety in challenging scenarios. The implementation is realized as an open-source package at https://github.com/ZWT006/agile_navigation.},
  archive   = {C_IROS},
  author    = {Wentao Zhang and Shaohang Xu and Peiyuan Cai and Lijun Zhu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10803105},
  month     = {10},
  pages     = {8839-8846},
  title     = {Agile and safe trajectory planning for quadruped navigation with motion anisotropy awareness},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast global point cloud registration using semantic NDT.
<em>IROS</em>, 8831–8838. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robust and accurate point cloud registration is an essential part of many robotic tasks such as SLAM or object pose retrieval. In this paper, we address the problem of global 3D point cloud registration, i.e., the task of estimating the 3D rigid body transform between a source and a target point cloud without any initial guess. Typically, the problem is solved by extracting and matching features to find a data association and then computing a transform that minimizes the squared distance between points. Our approach combines the normal distributions transform and oriented point pair framework and introduces the NDT distance histogram to quickly generate and test candidate transforms. Our method further exploits semantic information if available for greater speed. We implement our algorithm in C++ and compare it to other state-of-the-art approaches on a diverse set of environments. Our evaluation shows that our method outperforms the other approaches, especially concerning run-time and compute efficiency.},
  archive   = {C_IROS},
  author    = {Robert Schirmer and Narunas Vaskevicius and Peter Biber and Cyrill Stachniss},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801863},
  month     = {10},
  pages     = {8831-8838},
  title     = {Fast global point cloud registration using semantic NDT},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CATOA: Cooperative calibration of timestamp measurements for
distributed multi-robot localization. <em>IROS</em>, 8825–8830. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ultra-wideband (UWB) is a popular technology for robotic localization in global positioning system (GPS)-challenged and vision-obstructed scenarios. In UWB localization systems, distance information is extracted from ToA and ToD timestamp measurements. However, these measurements are easily influenced by hardware limitations and complex propagation environments, making an effective calibration method crucial for achieving high-accuracy ranging. This paper proposes a cooperative timestamp calibration method, which effectively mitigates ranging errors with scalability, adaptability, and flexibility. Our approach reduces the calibration complexity from O(N2) to O(N) for networks within N nodes and allows for distributed implementation to lower communication costs. The enabler is developing a new timestamp measurement model that can rectify all timestamps across different devices in a unified manner, coupled with the introduction of cooperative model training techniques that accommodate both feasible and infeasible scenarios for precisely labeling node positions. Real-world experimental results show that our method reduces the ranging error from 38.02 cm to 8.17 cm within a fully labeled 4-node network and from 16.77 cm to 9.61 cm in an 8-node network without labeling.},
  archive   = {C_IROS},
  author    = {Feiyang Wen and Hanying Zhao and Jincheng Yu and Shulin Cui and Yuan Shen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801560},
  month     = {10},
  pages     = {8825-8830},
  title     = {CATOA: Cooperative calibration of timestamp measurements for distributed multi-robot localization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SDFT: Structural discrete fourier transform for place
recognition and traversability analysis. <em>IROS</em>, 8817–8824. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to associate the current location with previously visited places is an essential aspect of autonomous ground robots. Unstructured environments such as planetary surfaces pose a significant challenge for robots because their terrain is less distinctive. Meanwhile, traversability must be analyzed simultaneously for safe navigation. In the past, place recognition research has rarely considered traversability analysis despite its significance. This is because the structural information of terrains becomes quickly implicit during the encoding process. This paper provides a method that explicitly addresses both problems: place recognition and traversability analysis. It proposes a discrete Fourier transform (DFT) to represent the frequency components embedded in ground curvature, which underlies both concepts. Our place recognition function demonstrates excellent performance in extensive experiments using challenging planetary &amp; urban datasets while estimating traversability that other approaches find difficult to handle.},
  archive   = {C_IROS},
  author    = {Ayumi Umemura and Ken Sakurada and Masaki Onishi and Kazuya Yoshida},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802244},
  month     = {10},
  pages     = {8817-8824},
  title     = {SDFT: Structural discrete fourier transform for place recognition and traversability analysis},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UWB-based localization system considering antenna anisotropy
and NLOS/multipath conditions. <em>IROS</em>, 8810–8816. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ultra-wideband (UWB) communication technology has gained attention in robotics due to its ability to provide range measurements possibly with centimeter-level accuracy. Nevertheless, practical UWB range measurements are susceptible to disturbances from multiple sources, including the anisotropic characteristics of antennas, non-line-of-sight (NLOS) conditions, and multipath propagation. In this paper, we introduce a UWB range measurement model that addresses these sources of error. To accommodate the effects of antenna anisotropy, we adopt real spherical harmonics to represent directional bias in the UWB range measurement model. To handle delayed measurements induced by NLOS conditions and multipath propagation, an asymmetric heavy-tailed distribution is utilized to model the measurement noise. We calibrate this measurement model based on the maximum likelihood estimation method and propose a UWB-based localization system based on that. Our localization system provides: 1) anchor self-calibration, which identifies anchor placement by fusing visual-inertial-ranging measurements based on continuous-time state representation; and 2) filtering-based state estimation, which applies our measurement model into Kalman filtering framework via an iterative update algorithm. Experimental validation is conducted to demonstrate the effectiveness of the measurement model for our localization system. We open source our implementation of the proposed UWB-based localization system at https://github.com/INRoL/inrol_uwb_localization.},
  archive   = {C_IROS},
  author    = {Taekyun Kim and Byoungkwon Yoon and Dongjun Lee},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802170},
  month     = {10},
  pages     = {8810-8816},
  title     = {UWB-based localization system considering antenna anisotropy and NLOS/Multipath conditions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comparison of audible, visual, and multi-modal
communication for multi-robot supervision and situational awareness.
<em>IROS</em>, 8802–8809. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-robot supervision becomes increasingly cognitively demanding as the ratio of robots to human supervisors rises, potentially leading to situational awareness (SA) losses and robot system failures. Nonverbal cues have been employed to direct supervisor attention and prevent awareness loss in diverse human-computer interaction (HCI) settings. This paper compares the effects of uni-modal and multi-modal audiovisual nonverbal cues on supervisor SA in a multi-robot supervision task. In a simulation-based navigation scenario, 50 participants monitored a multi-robot mission and responded to supervision requests from the robots. We evaluated supervisor SA using response speed and the situational awareness global assessment technique. Results demonstrate that supervisor awareness hinges on the communication method employed by the robots, with greater significance observed at higher awareness levels and when the robot-to-human ratio is higher. Findings also indicate the utility of sonification mapping in human-multirobot interactions and the benefits of multi-modal cues for sustaining awareness during multi-robot supervision.},
  archive   = {C_IROS},
  author    = {Richard Attfield and Elizabeth Croft and Dana Kulić},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801305},
  month     = {10},
  pages     = {8802-8809},
  title     = {A comparison of audible, visual, and multi-modal communication for multi-robot supervision and situational awareness},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TrustNavGPT: Modeling uncertainty to improve trustworthiness
of audio-guided LLM-based robot navigation. <em>IROS</em>, 8794–8801.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large language models (LLMs) exhibit a wide range of promising capabilities – from step-by-step planning to commonsense reasoning –that provide utility for robot navigation. However, as humans communicate with robots in the real world, ambiguity and uncertainty may be embedded inside spoken instructions. While LLMs are proficient at processing text in human conversations, they often encounter difficulties with the nuances of verbal instructions and, thus, remain prone to hallucinate trust in human command. In this work, we present TrustNavGPT, an LLM-based audio-guided navigation agent that uses affective cues in spoken communication—elements such as tone and inflection that convey meaning beyond words—allowing it to assess the trustworthiness of human commands and make effective, safe decisions. Experiments across a variety of simulation and real-world setups show a 70.46% success rate in catching command uncertainty and an 80% success rate in finding the target, 48.30%, and 55% outperform existing LLM-based navigation methods, respectively. Additionally, TrustNavGPT shows remarkable resilience against adversarial attacks, highlighted by a 22%+ less decrease ratio than the existing LLM navigation method in success rate. Our approach provides a lightweight yet effective approach that extends existing LLMs to model audio vocal features embedded in the voice command and model uncertainty for safe robotic navigation. For more information, visit the TrustNav project page.},
  archive   = {C_IROS},
  author    = {Xingpeng Sun and Yiran Zhang and Xindi Tang and Amrit Singh Bedi and Aniket Bera},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801932},
  month     = {10},
  pages     = {8794-8801},
  title     = {TrustNavGPT: Modeling uncertainty to improve trustworthiness of audio-guided LLM-based robot navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Using augmented reality in human-robot assembly: A
comparative study of eye-gaze and hand-ray pointing methods.
<em>IROS</em>, 8786–8793. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collaborative robots (cobots) are a promising technology for frontline workers in industry. They can support tasks that cannot be fully automated but are repetitive, fatiguing, boring, or dangerous for humans. Although cobots are explicitly designed to work with humans, they remain primarily non-intuitive and difficult to collaborate with. Thus, there is a need for new interaction approaches to facilitate efficient human-robot collaboration. Recently, we could see emerging examples of using augmented reality (AR) to assist a worker in collaborative task execution with a cobot. However, for such an approach to provide truly efficient support for the seamless bimanual task execution, we need to first investigate interaction methods offered by an AR interface. To that end, we performed a study with sixteen participants to compare eye-gaze and hand-ray pointing methods for part selection in collaborative, manual assembly tasks. The results of our study show that both techniques provide similar perceived usability, with the eye-gaze selection leading to significantly shorter completion times.},
  archive   = {C_IROS},
  author    = {Sławomir K. Tadeja and Tianye Zhou and Matteo Capponi and Krzysztof Walas and Thomas Bohné and Fulvio Forni},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802282},
  month     = {10},
  pages     = {8786-8793},
  title     = {Using augmented reality in human-robot assembly: A comparative study of eye-gaze and hand-ray pointing methods},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Probabilistic inference of human capabilities from passive
observations. <em>IROS</em>, 8779–8785. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern robots need to adapt to diverse human partners with whom they collaborate. To this end, learning a representation of human capabilities enables the robot to personalize their behaviour to their collaborators across multiple tasks. We propose CApability Modeling from Observations (CAMO), a model-based estimation algorithm, in which human capabilities that parameterize a given model are inferred from observations of the human behaviour on known collaborative tasks. We apply the method to joint limit learning in order to predict future trajectories of a 7-DOF manipulator arm. Furthermore, we show that CAMO can be used as a sub-task assignment routine in a simulated human–robot collaboration scenario, allowing the robot to adapt its task allocation to perform tasks that the person is not able to do.},
  archive   = {C_IROS},
  author    = {Peter Tisnikar and Gerard Canal and Matteo Leonetti},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801640},
  month     = {10},
  pages     = {8779-8785},
  title     = {Probabilistic inference of human capabilities from passive observations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). State estimation of an adaptive 3-finger gripper using
recurrent neural networks. <em>IROS</em>, 8740–8746. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Adaptive grippers enable easy and robust grasping of diverse objects by adapting to their shapes and enclosing them. However, determining the exact state of the hand remains challenging. This is not always straightforward but is often necessary to assess grip success, quality, or the pose of the object. In this work, we present two deep learning approaches using recurrent neural networks to successfully estimate the joint states of the Robotiq 3-Finger Adaptive Gripper. The models are compared with an existing analytical approach, which does not distinguish between the fingers of the hand and calculates the three angles of their respective joints using joint limits, contact information, and motor position in a transition model. We test the differences in accuracy with our networks by not distinguishing between the fingers as the analytical approach for the first model and by looking at the entire hand in the second model. Our experiments demonstrate that the model considering the entire hand outperforms the other two approaches, is more robust against object movements and achieves an average joint position accuracy of 2.29 degrees.},
  archive   = {C_IROS},
  author    = {Yannick Jonetzko and Theresa Alexandra Aurelia Naß and Niklas Fiedler and Jianwei Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802760},
  month     = {10},
  pages     = {8740-8746},
  title     = {State estimation of an adaptive 3-finger gripper using recurrent neural networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Look gauss, no pose: Novel view synthesis using gaussian
splatting without accurate pose initialization. <em>IROS</em>,
8732–8739. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D Gaussian Splatting has recently emerged as a powerful tool for fast and accurate novel-view synthesis from a set of posed input images. However, like most novel-view synthesis approaches, it relies on accurate camera pose information, limiting its applicability in real-world scenarios where acquiring accurate camera poses can be challenging or even impossible. We propose an extension to the 3D Gaussian Splatting framework by optimizing the extrinsic camera parameters with respect to photometric residuals. We derive the analytical gradients and integrate their computation with the existing high-performance CUDA implementation. This enables downstream tasks such as 6-DoF camera pose estimation as well as joint reconstruction and camera refinement. In particular, we achieve rapid convergence and high accuracy for pose estimation on real-world scenes. Our method enables fast reconstruction of 3D scenes without requiring accurate pose information by jointly optimizing geometry and camera poses, while achieving state-of-the-art results in novel-view synthesis. Our approach is considerably faster to optimize than most competing methods, and several times faster in rendering. We show results on real-world scenes and complex trajectories through simulated environments, achieving state-of-the-art results on LLFF while reducing runtime by two to four times compared to the most efficient competing method. Source code will be available at https://github.com/Schmiddo/noposegs.},
  archive   = {C_IROS},
  author    = {Christian Schmidt and Jens Piekenbrinck and Bastian Leibe},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801639},
  month     = {10},
  pages     = {8732-8739},
  title     = {Look gauss, no pose: Novel view synthesis using gaussian splatting without accurate pose initialization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A robot kinematics model estimation using inertial sensors
for on-site building robotics. <em>IROS</em>, 8724–8731. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In order to make robots more useful in a variety of environments, they need to be highly portable so that they can be transported to wherever they are needed, and highly storable so that they can be stored when not in use. We propose &quot;on-site robotics&quot;, which uses parts procured at the location where the robot will be active, and propose a new solution to the problem of portability and storability. In this paper, as a proof of concept for on-site robotics, we describe a method for estimating the kinematic model of a robot by using inertial measurement units (IMU) sensor module on rigid links, estimating the relative orientation between modules from angular velocity, and estimating the relative position from the measurement of centrifugal force.At the end of this paper, as an evaluation for this method, we present an experiment in which a robot made up of wooden sticks reaches a target position. In this experiment, even if the combination of the links is changed, the robot is able to reach the target position again immediately after estimation, showing that it can operate even after being reassembled. Our implementation is available on https://github.com/hiroya1224/urdf_estimation_with_imus.},
  archive   = {C_IROS},
  author    = {Hiroya Sato and Tasuku Makabe and Iori Yanokura and Naoya Yamaguchi and Kei Okada and Masayuki Inaba},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802591},
  month     = {10},
  pages     = {8724-8731},
  title     = {A robot kinematics model estimation using inertial sensors for on-site building robotics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient estimation of frequency response functions of
industrial robots using the local rational method. <em>IROS</em>,
8717–8723. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Non-parametric estimates of frequency response functions (FRFs) are often suitable for describing the dynamics of a mechanical system. If treating these estimates as measurements, they can be used for parametric identification of, e.g., a gray-box model. This paper shows that a more accurate parametric model can be identified based on local parametric FRF estimates, giving a shorter total experiment time, compared to classical methods. Classical methods for non-parametric FRF estimation of MIMO (Multiple Input Multiple Output) systems require at least as many experiments as the system has inputs. Local parametric FRF estimation methods have been developed for avoiding multiple experiments. In this paper, these local methods are adapted and applied for estimating the FRFs of a 6-axes robotic manipulator, which is a nonlinear MIMO system operating in closed loop. The aim is to reduce the experiment time and amount of data needed for identification. The resulting FRFs are analyzed in an experimental study and compared to estimates obtained by classical MIMO techniques.},
  archive   = {C_IROS},
  author    = {Stefanie A. Zimmermann and Stig Moberg},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802182},
  month     = {10},
  pages     = {8717-8723},
  title     = {Efficient estimation of frequency response functions of industrial robots using the local rational method},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring modal switch in metamaterial-based robots.
<em>IROS</em>, 8711–8716. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mechanical metamaterials are microscale patterned structures that are designed to have specific mechanical properties at a macro-scale that are atypical of natural materials. Robotic manipulators composed of these materials can exhibit deformation and motion capabilities that can be customized and easily fabricated. However, as of now, the motion capability of such manipulators are encoded in their physical composition and cannot be changed. This paper presents multimodal metamaterial-based robot prototypes which can switch between the behaviors found in two different metamaterials. Two such robots are explored, a bending/shearing robot and a bending/twisting robot. The robot design is described in detail, including how the robots toggle between behavior modes via mechanical actuation of a sliding rod insert. Multi-modal robots are compared to their single-mode equivalents to characterize their capabilities. The single-mode behaviors are largely preserved in the multi-modal innovations. The multi-modal prototypes also demonstrate variable rigidity. We discuss the feasibility of using robots of this design as part of a robotic surgical system.},
  archive   = {C_IROS},
  author    = {Britton Jordan and Daniel S. Esser and Jeonghyeon Kim and Brian Y. Cho and Robert J. Webster and Alan Kuntz},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802506},
  month     = {10},
  pages     = {8711-8716},
  title     = {Exploring modal switch in metamaterial-based robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identification of flexible joint robot inertia matrix using
frequency response analysis. <em>IROS</em>, 8704–8710. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel, nonlinearity robust identification method for deriving the inertia matrix of multi-DOF Flexible Joint Robots (FJR), utilizing resonance and anti-resonance frequencies in the Frequency Response Functions (FRF). Our proposed method overcomes the limitations of conventional approaches, which are susceptible to mechanical nonlinearities, leading to inaccurate models. By leveraging frequency domain techniques, our approach effectively mitigates the influence of nonlinear characteristics, providing a more accurate and reliable means of robot control. Furturmore, the paper highlights the benefits of frequency domain system identification, including nonlinear robustness and the ability to decompose the flexible joint into motor and load components. Finally, a novel sequential excitation algorithm is proposed to obtain the inertia matrix of a multi-DOF robot manipulator without relying on complex theories or optimizations. The effectiveness of the proposed algorithm is verified through simulation and experiment.},
  archive   = {C_IROS},
  author    = {Kiyoung Choi and Junho Song and Wonbum Yun and Deokjin Lee and Sehoon Oh},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802372},
  month     = {10},
  pages     = {8704-8710},
  title     = {Identification of flexible joint robot inertia matrix using frequency response analysis},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Strong compliant grasps using a cable-driven soft gripper.
<em>IROS</em>, 8696–8703. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The natural flexibility of soft robotic grippers allows for versatile and compliant grasping. However, this same flexibility can restrict the gripper’s strength. Striking a balance between compliance and strength is essential for effective soft grippers. In this work, we present Flexible Robust Observant Gripper (FROG), a soft gripper that is both compliant and strong. We describe the mechanical design of the gripper, characterize the soft flexures used in the design, and analyze the grasp forces generated by the gripper. Utilizing the structure of the gripper, we develop feedforward grasp controllers and a classifier to distinguish between grasp types. Grasping experiments show that FROG can effectively grasp a variety of objects, including very soft or delicate items. Holding force tests show that our gripper can conform to the grasped object and exert large grasp forces.},
  archive   = {C_IROS},
  author    = {Gregory Xie and Lillian Chin and Byungchul Kim and Rachel Holladay and Daniela Rus},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801693},
  month     = {10},
  pages     = {8696-8703},
  title     = {Strong compliant grasps using a cable-driven soft gripper},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamics-based trajectory planning for vibration suppression
of a flexible long-reach robotic manipulator system. <em>IROS</em>,
8690–8695. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the unique challenge of vibration suppression for a flexible long-reach robotic manipulator system, namely, the through-wall deployment (TWD) system that is used in nuclear environments. This paper proposes a novel dynamics-based trajectory optimization approach, which minimizes both the acceleration and the jerk at the manipulator’s joints, as well as the vibrations of the flexible long-reach boom where the manipulator’s base is mounted. Firstly, we create an integrated model for the system dynamics based on the knowledge of the robotic manipulator and the acceleration data from the vibration tests. We then develop an original procedure for generating the high-order polynomial trajectory that guarantees the zero-boundary condition for a flexible number of optimization parameters and waypoints. Following the simulation of a multi-objective optimization scheme, the optimized trajectory is experimentally validated on the practical TWD system with around 28% vibration reduction on average compared to the benchmark. Importantly, this reduction is achieved without compromising on the average speed of motion. The methodology is transferable to a wider range of flexible robotic manipulator systems with similar characteristics.},
  archive   = {C_IROS},
  author    = {Anthony Siming Chen and Erwin Jose Lopez Pulgarin and Guido Herrmann and Alexander Lanzon and Joaquin Carrasco and Barry Lennox and Benji Carrera-Knowles and John Brotherhood and Tomoki Sakaue and Kaiqiang Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801636},
  month     = {10},
  pages     = {8690-8695},
  title     = {Dynamics-based trajectory planning for vibration suppression of a flexible long-reach robotic manipulator system},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Underwater hyperspectral imaging for measuring seafloor
reflectance. <em>IROS</em>, 8658–8665. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A known challenge for computer vision methods applied to the underwater domain is that nonlinear attenuation of light in underwater environments distorts the color signal in captured imagery, resulting in inconsistent color and contrast at varying distances to an imaged target. While surface reflectance can provide a useful cue for classifying imagery of the seafloor by object or substrate types, color inconsistency makes robust classification challenging. We introduce a method that leverages hyperspectral imagery with an underwater light formation model and structure from motion to estimate the intrinsic optical properties of the underwater environment and correct seafloor reflectance estimates from radiance measurements. We show that our method enables consistent surface reflectance estimates under both artificial and ambient lighting conditions and is readily integrated on small underwater vehicle platforms, such as a BlueROV.},
  archive   = {C_IROS},
  author    = {Hongjie Zhang and Gideon Billings and Jackson Shields and Stefan Williams},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801500},
  month     = {10},
  pages     = {8658-8665},
  title     = {Underwater hyperspectral imaging for measuring seafloor reflectance},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive multi-altitude search and sampling of sparsely
distributed natural phenomena. <em>IROS</em>, 8650–8657. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel method for autonomously seeking out sparsely distributed targets in an unknown underwater environment. Our Sparse Adaptive Search and Sample (SASS) algorithm mixes low-altitude observations of discrete targets with high-altitude observations of the surrounding substrates. By using prior information about the distribution of targets across substrate types in combination with belief modelling over these substrates in the environment, high-altitude observations provide information that allows SASS to quickly guide the robot to areas with high target densities. A maximally informative path is autonomously constructed online using Monte Carlo Tree Search with a novel acquisition function to guide the search to maximise observations of unique targets. We demonstrate our approach in a set of simulated trials using a novel generative species model. SASS consistently outperforms the canonical boustrophedon planner by up to 36% in seeking out unique targets in the first 75-90% of time it takes for a boustrophedon survey. Additionally, we verify the performance of SASS on two real world coral reef datasets.},
  archive   = {C_IROS},
  author    = {Jessica E. Todd and Seth McCammon and Yogesh Girdhar and Nicholas Roy and Dana Yoerger},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802325},
  month     = {10},
  pages     = {8650-8657},
  title     = {Adaptive multi-altitude search and sampling of sparsely distributed natural phenomena},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). QO-net: Query optimization underwater object detection
network. <em>IROS</em>, 8642–8649. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Underwater object detection has attracted increasing interest for its wide application in various underwater tasks. However, due to underwater image quality degradation and the lack of large-scale underwater object datasets, many underwater detectors suffer from low detection performance. To address the issues, we not only propose a novel underwater transformer detector with multi-scale feature enhancement and query optimization, named QO-Net, but also construct a new underwater object detection dataset, called UODD. Specifically, a Conv-Trans Layer is developed as the unit of QO-Net, which effectively learns multi-scale image feature representation through CNN and simultaneously captures the dependencies among different positions in the sequence data through Transformer, enabling QO-Net to process underwater image sequence information over longer distances. An effective combination can enhance the representation of multi-scale features. Then, QO-Net develops a positional query enhancement strategy to optimize the spatial prior of positional queries, thereby speeding up the convergence of the network training. In addition, UODD also contains more than 20,000 underwater images for training and validation, with a variety of rich underwater categories. Extensive experiments on UODD, Brackish, and TrashCan datasets demonstrate that QO-Net presents favorable detection performance against state-of-the-art methods in terms of robustness and accuracy.},
  archive   = {C_IROS},
  author    = {Jiandong Tian and Hongyang Sun and Baojie Fan and Hongxin Xu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802136},
  month     = {10},
  pages     = {8642-8649},
  title     = {QO-net: Query optimization underwater object detection network},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic SpectraFormer for ultra-high-definition underwater
image enhancement. <em>IROS</em>, 8634–8641. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Underwater images suffer from color distortion, haze, and poor visibility due to light refraction and absorption in water. These challenges significantly impact the utilization of Autonomous Underwater Vehicles (AUVs) or marine robots. Typically, color and brightness distortions manifest at lower frequencies, while edge and texture distortions are prevalent at higher frequencies. Traditional methods struggle to concurrently rectify these mixed distortions as they primarily concentrate on the spatial domain. To address these issues, we introduce the Dynamic SpectraFormer, which enhances under-water images through a frequency domain transformer. The Dynamic SpectraFormer introduces an ultra-high-resolution sparse spectrum attention module, which could capture the long-term dependency without losing the universal approximating power. Additionally, we have developed a dynamic spectrum weight generation layer that serves as an adaptive spectrum band selector, accentuating critical frequency bands and suppressing less relevant ones. Consequently, this method significantly improves underwater image quality by addressing both high-and low-frequency distortions. Our extensive ablation studies and comparative evaluations consolidate the Dynamic SpectraFormer’s efficacy across multiple underwater image enhancement benchmarks. The source code is available at https://github.com/arifence2024/DynamicSpectraFormer.git.},
  archive   = {C_IROS},
  author    = {Zhiqiang Hu and Tao Yu and Shouren Huang and Masatoshi Ishikawa},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802529},
  month     = {10},
  pages     = {8634-8641},
  title     = {Dynamic SpectraFormer for ultra-high-definition underwater image enhancement},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Skill transfer and discovery for sim-to-real learning: A
representation-based viewpoint. <em>IROS</em>, 8603–8609. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study sim-to-real skill transfer and discovery in the context of robotics control using representation learning. We draw inspiration from spectral decomposition of Markov decision processes. The spectral decomposition brings about representation that can linearly represent the state-action value function induced by any policies, thus can be regarded as skills. The skill representations are transferable across arbitrary tasks with the same transition dynamics. Moreover, to handle the sim-to-real gap in the dynamics, we propose a skill discovery algorithm that learns new skills caused by the sim-to-real gap from real-world data. We promote the discovery of new skills by enforcing orthogonal constraints between the skills to learn and the skills from simulators, and then synthesize the policy using the enlarged skill sets. We demonstrate our methodology by transferring quadrotor controllers from simulators to Crazyflie 2.1 quadrotors. We show that we can learn the skill representations from a single simulator task and transfer these to multiple different real-world tasks including hovering, taking off, landing and trajectory tracking. Our skill discovery approach helps narrow the sim-to-real gap and improve the real-world controller performance by up to 30.2%.},
  archive   = {C_IROS},
  author    = {Haitong Ma and Zhaolin Ren and Bo Dai and Na Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801637},
  month     = {10},
  pages     = {8603-8609},
  title     = {Skill transfer and discovery for sim-to-real learning: A representation-based viewpoint},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sim2Real transfer for audio-visual navigation with
frequency-adaptive acoustic field prediction. <em>IROS</em>, 8595–8602.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sim2real transfer has received increasing attention lately due to its success in transferring robotic policies learned in simulation to the real world. While significant progress has been made in transferring vision-based navigation policies, the current sim2real strategy for audio-visual navigation remains limited to basic data augmentation. Sound differs from light in that it spans across much wider frequencies and thus requires a different solution for sim2real. To understand how the acoustic sim2real gap varies with frequencies, we first define a novel acoustic field prediction (AFP) task that predicts the local sound pressure field. We then train frequency-specific AFP models in simulation and measure the prediction errors on collected real data. We propose a frequency-adaptive strategy that intelligently selects the best frequency band for prediction based on both the measured prior and the energy distribution of the received audio, which improves the generalization on real data. Coupled with waypoint navigation, we show the navigation policy not only improves navigation performance in simulation but also transfers successfully to real robots. This work demonstrates the potential of building autonomous agents that can see, hear, and act entirely from simulation, and transferring them to the real world.},
  archive   = {C_IROS},
  author    = {Changan Chen and Jordi Ramos and Anshul Tomar and Kristen Grauman},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802636},
  month     = {10},
  pages     = {8595-8602},
  title     = {Sim2Real transfer for audio-visual navigation with frequency-adaptive acoustic field prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-architecture auxiliary feature space translation for
efficient few-shot personalized object detection. <em>IROS</em>,
8587–8594. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent years have seen object detection robotic systems deployed in several personal devices (e.g., home robots and appliances). This has highlighted a challenge in their design, i.e., they cannot efficiently update their knowledge to distinguish between general classes and user-specific instances (e.g., a dog vs. user’s dog). We refer to this challenging task as Instance-level Personalized Object Detection (IPOD). The personalization task requires many samples for model tuning and optimization in a centralized server, raising privacy concerns. An alternative is provided by approaches based on recent large-scale Foundation Models, but their compute costs preclude on-device applications. In our work we tackle both problems at the same time, designing a Few-Shot IPOD strategy called AuXFT. We introduce a conditional coarse-to-fine few-shot learner to refine the coarse predictions made by an efficient object detector, showing that using an off-the-shelf model leads to poor personalization due to neural collapse. Therefore, we introduce a Translator block that generates an auxiliary feature space where features generated by a self-supervised model (e.g., DINOv2) are distilled without impacting the performance of the detector. We validate AuXFT on three publicly available datasets and one in-house benchmark designed for the IPOD task, achieving remarkable gains in all considered scenarios with excellent time-complexity trade-off: AuXFT reaches a performance of 80% its upper bound at just 32% of the inference time, 13% of VRAM and 19% of the model size.},
  archive   = {C_IROS},
  author    = {Francesco Barbato and Umberto Michieli and Jijoong Moon and Pietro Zanuttigh and Mete Ozay},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802460},
  month     = {10},
  pages     = {8587-8594},
  title     = {Cross-architecture auxiliary feature space translation for efficient few-shot personalized object detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Latent object characteristics recognition with visual to
haptic-audio cross-modal transfer learning. <em>IROS</em>, 8579–8586.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recognising the characteristics of objects while a robot handles them is crucial for adjusting motions that ensure stable and efficient interactions with containers. Ahead of realising stable and efficient robot motions for handling/transferring the containers, this work aims to recognise the unobservable latent object characteristics. While vision is commonly used for object recognition by robots, it is ineffective for detecting hidden objects. However, recognising objects indirectly using other sensors is a challenging task. To address this challenge, we propose a cross-modal transfer learning approach from vision to haptic-audio. We initially train the model with vision, directly observing the target object. Subsequently, we transfer the latent space learned from vision to a second module, trained only with haptic-audio and motor data. This transfer learning framework facilitates the representation of object characteristics using indirect sensor data, thereby improving recognition accuracy. For evaluating the recognition accuracy of our proposed learning framework we selected shape, position, and orientation as the object characteristics. Finally, we demonstrate online recognition of both trained and untrained objects using the humanoid robot Nextage Open. See our accompanying video here: https://www.youtube.com/watch?v=sOHqPC1uusg},
  archive   = {C_IROS},
  author    = {Namiko Saito and João Moura and Hiroki Uchida and Sethu Vijayakumar},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801583},
  month     = {10},
  pages     = {8579-8586},
  title     = {Latent object characteristics recognition with visual to haptic-audio cross-modal transfer learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VIVO: A visual-inertial-velocity odometry with online
calibration in challenging condition. <em>IROS</em>, 8571–8578. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {State estimation is a central component of autonomous navigation. To date, many methods presented have a disruptive potential for application, such as visual-inertial odometry (VIO), wheel and leg odometry (for short, body odometry). However, most of them are prone to fail in some challenging conditions like high-dynamic street scenes and sustain aggressive movements. To this end, in this paper, we present a novel visual-inertial-velocity odometry (VIVO) framework which incorporates velocity measurement provided by the proprioceptive sensing into the MSCKF-based VIO in a tightly coupled fashion. Furthermore, considering that the imprecise extrinsic parameters can severely undermine the state estimation performance, we hence perform VIVO along with online calibration of the body odometry’s extrinsic parameters by adding them to the estimated state vector. The generic VIVO can be deployed for a broad spectrum of robot models ranging from wheeled robots to legged robots. Both simulation and real-world experiments are performed to extensively validate the robustness and accuracy of the proposed method in challenging scenarios using wheeled and legged robot models, respectively.},
  archive   = {C_IROS},
  author    = {Fuzhang Han and Shenhan Jia and Jiyu Yu and Yufei Wei and Wenjun Huang and Yue Wang and Rong Xiong},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801486},
  month     = {10},
  pages     = {8571-8578},
  title     = {VIVO: A visual-inertial-velocity odometry with online calibration in challenging condition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). FlexLoc: Conditional neural networks for zero-shot sensor
perspective invariance in object localization with distributed
multimodal sensors. <em>IROS</em>, 8563–8570. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Localization is a critical technology for various applications ranging from navigation and surveillance to assisted living. Localization systems typically fuse information from sensors viewing the scene from different perspectives to estimate the target location while also employing multiple modalities for enhanced robustness and accuracy. Recently, such systems have employed end-to-end deep neural models trained on large datasets due to their superior performance and ability to handle data from diverse sensor modalities. However, such neural models are often trained on data collected from a particular set of sensor poses (i.e., locations and orientations). During real-world deployments, slight deviations from these sensor poses can result in extreme inaccuracies. To address this challenge, we introduce FlexLoc, which employs conditional neural networks to inject node perspective information to adapt the localization pipeline. Specifically, a small subset of model weights are derived from node poses at run time, enabling accurate generalization to unseen perspectives with minimal additional overhead. Our evaluations on a multimodal, multi-view indoor tracking dataset showcase that FlexLoc improves the localization accuracy by almost 50% in the zero-shot case (no calibration data available) compared to the baselines. The source code of FlexLoc is available in https://github.com/nesl/FlexLoc.},
  archive   = {C_IROS},
  author    = {Jason Wu and Ziqi Wang and Xiaomin Ouyang and Ho Lyun Jeong and Colin Samplawski and Lance M. Kaplan and Benjamin Marlin and Mani Srivastava},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802096},
  month     = {10},
  pages     = {8563-8570},
  title     = {FlexLoc: Conditional neural networks for zero-shot sensor perspective invariance in object localization with distributed multimodal sensors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GMMCalib: Extrinsic calibration of LiDAR sensors using
GMM-based joint registration. <em>IROS</em>, 8555–8562. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {State-of-the-art LiDAR calibration frameworks mainly use non-probabilistic registration methods such as Iterative Closest Point (ICP) and its variants. These methods suffer from biased results due to their pair-wise registration procedure as well as their sensitivity to initialization and parameterization. This often leads to misalignments in the calibration process. Probabilistic registration methods compensate for these drawbacks by specifically modeling the probabilistic nature of the observations. This paper presents GMMCalib, an automatic target-based extrinsic calibration approach for multi-LiDAR systems. Using an implementation of a Gaussian Mixture Model (GMM)-based registration method that allows joint registration of multiple point clouds, this data-driven approach is compared to ICP algorithms. We perform simulation experiments using the digital twin of the EDGAR research vehicle and validate the results in a real-world environment. We also address the local minima problem of local registration methods for extrinsic sensor calibration and use a distance-based metric to evaluate the calibration results. Our results show that an increase in robustness against sensor miscalibrations can be achieved by using GMM-based registration algorithms. The code is open source and available on GitHub3.},
  archive   = {C_IROS},
  author    = {Ilir Tahiraj and Felix Fent and Philipp Hafemann and Egon Ye and Markus Lienkamp},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801262},
  month     = {10},
  pages     = {8555-8562},
  title     = {GMMCalib: Extrinsic calibration of LiDAR sensors using GMM-based joint registration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DeRO: Dead reckoning based on radar odometry with
accelerometers aided for robot localization. <em>IROS</em>, 8547–8554.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a radar odometry structure that directly utilizes radar velocity measurements for dead reckoning while maintaining its ability to update estimations within the Kalman filter framework. Specifically, we employ the Doppler velocity obtained by a 4D Frequency Modulated Continuous Wave (FMCW) radar in conjunction with gyroscope data to calculate poses. This approach helps mitigate high drift resulting from accelerometer biases and double integration. Instead, tilt angles measured by gravitational force are utilized alongside relative distance measurements from radar scan matching for the filter’s measurement update. Additionally, to further enhance the system’s accuracy, we estimate and compensate for the radar velocity scale factor. The performance of the proposed method is verified through five real-world open-source datasets. The results demonstrate that our approach reduces position error by 62% and rotation error by 66% on average compared to the state-of-the-art radar-inertial fusion method in terms of absolute trajectory error.},
  archive   = {C_IROS},
  author    = {Hoang Viet Do and Yong Hun Kim and Joo Han Lee and Min Ho Lee and Jin Woo Song},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801645},
  month     = {10},
  pages     = {8547-8554},
  title     = {DeRO: Dead reckoning based on radar odometry with accelerometers aided for robot localization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From LLMs to actions: Latent codes as bridges in
hierarchical robot control. <em>IROS</em>, 8539–8546. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hierarchical control for robotics has long been plagued by the need to have a well defined interface layer to communicate between high-level task planners and low-level policies. With the advent of LLMs, language has been emerging as a prospective interface layer. However, this has several limitations. Not all tasks can be decomposed into steps that are easily expressible in natural language (e.g. performing a dance routine). Further, it makes end-to-end finetuning on embodied data challenging due to domain shift and catastrophic forgetting. We introduce our method – Latent Codes as Bridges (LCB) – as an alternate architecture to overcome these limitations. LCB uses a learnable latent code to act as a bridge between LLMs and low-level policies. This enables LLMs to flexibly communicate goals in the task plan without being entirely constrained by language limitations. Additionally, it enables end-to-end finetuning without destroying the embedding space of word tokens learned during pre-training. Through experiments on Language Table and Calvin, two common language based benchmarks for embodied agents, we find that LCB outperforms baselines (including those w/ GPT-4V) that leverage pure language as the interface layer on tasks that require reasoning and multi-step behaviors.},
  archive   = {C_IROS},
  author    = {Yide Shentu and Philipp Wu and Aravind Rajeswaran and Pieter Abbeel},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801683},
  month     = {10},
  pages     = {8539-8546},
  title     = {From LLMs to actions: Latent codes as bridges in hierarchical robot control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data efficient behavior cloning for fine manipulation via
continuity-based corrective labels. <em>IROS</em>, 8531–8538. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider imitation learning with access only to expert demonstrations, whose real-world application is often limited by covariate shift due to compounding errors during execution. We investigate the effectiveness of the Continuity-based Corrective Labels for Imitation Learning (CCIL) framework in mitigating this issue for real-world fine manipulation tasks. CCIL generates corrective labels by learning a locally continuous dynamics model from demonstrations to guide the agent back toward expert states. Through extensive experiments on insertion and fine grasping tasks, we provide the first empirical validation that CCIL can significantly improve imitation learning performance despite discontinuities present in contact-rich manipulation. We find that: (1) real-world manipulation exhibits sufficient local smoothness to apply CCIL, (2) generated corrective labels are most beneficial in low-data regimes, and (3) label filtering based on estimated dynamics model error enables performance gains. To effectively apply CCIL to robotic domains, we offer a practical instantiation of the framework and insights into design choices and hyperparameter selection. Our work demonstrates CCIL’s practicality for alleviating compounding errors in imitation learning on physical robots.},
  archive   = {C_IROS},
  author    = {Abhay Deshpande and Liyiming Ke and Quinn Pfeifer and Abhishek Gupta and Siddhartha S. Srinivasa},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801414},
  month     = {10},
  pages     = {8531-8538},
  title     = {Data efficient behavior cloning for fine manipulation via continuity-based corrective labels},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural ODE-based imitation learning (NODE-IL):
Data-efficient imitation learning for long-horizon multi-skill robot
manipulation. <em>IROS</em>, 8524–8530. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In robotics, acquiring new skills through Imitation Learning (IL) is crucial for handling diverse complex tasks. However, model-free IL faces challenges of data inefficiency and prolonged training time, whereas model-based methods struggle to obtain accurate nonlinear models. To address these challenges, we developed Neural ODE-based Imitation Learning (NODE-IL), a novel model-based imitation learning framework that employs Neural Ordinary Differential Equations (Neural ODEs) for learning task dynamics and control policies. NODE-IL comprises (1) Dynamic-NODE for learning the continuous differentiable task’s transition dynamics model, and (2) Control-NODE for learning a long-horizon control policy in an MPC fashion, which are trained holistically. Extensively evaluated on challenging manipulation tasks, NODE-IL demonstrates significant advantages in data efficiency, requiring less than 70 samples to achieve robust performance. It outperforms Behavioral Cloning from Observation (BCO) and Gaussian Process Imitation Learning (GP-IL) methods, achieving 70% higher average success rate, and reducing translation errors for high-precision tasks, which demonstrates its robustness and accuracy, as an effective and efficient imitation learning approach for learning complex manipulation tasks.},
  archive   = {C_IROS},
  author    = {Shiyao Zhao and Yucheng Xu and Mohammadreza Kasaei and Mohsen Khadem and Zhibin Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802736},
  month     = {10},
  pages     = {8524-8530},
  title     = {Neural ODE-based imitation learning (NODE-IL): Data-efficient imitation learning for long-horizon multi-skill robot manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-task real-robot data with gaze attention for dual-arm
fine manipulation. <em>IROS</em>, 8516–8523. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep imitation learning is a promising approach in robotic manipulation, enabling robots to acquire versatile and adaptable skills. In such research, by learning various tasks, robots achieved generality across multiple objects. However, such multi-task robot datasets have mainly focused on single-arm tasks that are relatively imprecise and not addressed the fine-grained object manipulation that robots are expected to perform in the real world. In this study, we introduce a dataset for diverse object manipulation that includes dual-arm tasks and/or tasks that require fine manipulation. We generated a dataset containing 224k episodes (150 hours, 1,104 language instructions) that includes dual-arm fine tasks, such as bowl-moving, pencil-case opening, and banana-peeling. This dataset is publicly available 1. Additionally, this dataset includes visual attention signals, dual-action labels that separate actions into robust reaching trajectories or precise interactions with objects, and language instructions, all aimed at achieving robust and precise object manipulation. We applied the dataset to our Dual-Action and Attention, which is a model that we designed for fine-grained dual-arm manipulation tasks that is robust to covariate shift. We tested the model in over 7k trials for real robot manipulation tasks, which demonstrated its capability to perform fine manipulation.},
  archive   = {C_IROS},
  author    = {Heecheol Kim and Yoshiyuki Ohmura and Yasuo Kuniyoshi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802034},
  month     = {10},
  pages     = {8516-8523},
  title     = {Multi-task real-robot data with gaze attention for dual-arm fine manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BuzzRacer: A palm-sized autonomous vehicle platform for
testing multi-agent adversarial decision-making. <em>IROS</em>,
8510–8515. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present BuzzRacer, a palm-sized autonomous vehicle platform suitable for multi-agent autonomous racing. BuzzRacer consists of two parts. First, a software framework with multiple racetrack environments, dynamic simulation, visualization, and control pipelines. Second, a miniature autonomous vehicle platform capable of 1g acceleration and 3.5m/s top speed. BuzzRacer is an open-source project currently used at Georgia Tech in a project-based robotics course and research projects for experimental validation and benchmarking of novel planning and control algorithms.},
  archive   = {C_IROS},
  author    = {Zhiyuan Zhang and Panagiotis Tsiotras},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802853},
  month     = {10},
  pages     = {8510-8515},
  title     = {BuzzRacer: A palm-sized autonomous vehicle platform for testing multi-agent adversarial decision-making},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Asynchronous spatial-temporal allocation for trajectory
planning of heterogeneous multi-agent systems. <em>IROS</em>, 8504–8509.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To plan the trajectories of a large-scale heterogeneous swarm, sequentially or synchronously distributed methods usually become intractable due to the lack of global clock synchronization. To this end, we provide a novel asynchronous spatial-temporal allocation method. Specifically, between a pair of agents, the allocation is proposed to determine their corresponding derivable time-stamped space and can be updated in an asynchronous way, by inserting a waiting duration between two consecutive replanning steps. It is theoretically shown that the inter-agent collision is avoided and the allocation ensures timely updates. Comprehensive simulations and comparisons with state-of-the-art baselines validate the effectiveness of the proposed method and illustrate its improvement in completion time and moving distance. Finally, hardware experiments are carried out, where 8 heterogeneous unmanned ground vehicles with onboard computation navigate in cluttered scenarios with high agility.},
  archive   = {C_IROS},
  author    = {Yuda Chen and Haoze Dong and Zhongkui Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801393},
  month     = {10},
  pages     = {8504-8509},
  title     = {Asynchronous spatial-temporal allocation for trajectory planning of heterogeneous multi-agent systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive model predictive control for differential-algebraic
systems towards a higher path accuracy for physically coupled robots.
<em>IROS</em>, 8497–8503. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The physical coupling between robots has the potential to improve the capabilities of multi-robot systems in challenging manufacturing processes. However, the path tracking accuracy of physically coupled robots is not studied adequately, especially considering the uncertain kinematic parameters, the mechanical elasticity, and the built-in controllers of off-the-shelf robots. This paper addresses these issues with a novel differential-algebraic system model which is verified against measurement data from real execution. The uncertain kinematic parameters are estimated online to adapt the model. Consequently, an adaptive model predictive controller is designed as a coordinator between the robots. The controller achieves a path tracking error reduction of 88.6% compared to the state-of-the-art benchmark in the simulation.},
  archive   = {C_IROS},
  author    = {Xin Ye and Karl Handwerker and Sören Hohmann},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802620},
  month     = {10},
  pages     = {8497-8503},
  title     = {Adaptive model predictive control for differential-algebraic systems towards a higher path accuracy for physically coupled robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time bandwidth-efficient occupancy grid map
synchronization for multi-robot systems. <em>IROS</em>, 8489–8496. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot swarms are increasingly being applied in various domains. However, due to the inherent limitation imposed by low real-time communication bandwidth, the synchronization of environmental information among multiple robots remains a persistent and challenging problem in practical applications. In response to this challenge, we introduce a comprehensive framework for synchronizing occupancy grid maps (OGMs) in practical multi-robot systems that operate under communication bandwidth constraints. In our research, we elaborately design the data structure of transmitted local OGMs and employ the Hilbert space-filling curve for voxel sorting. By adopting this approach, data redundancy is effectively increased, resulting in lower information entropy for compression and significantly reducing the volume of communication data. Finally, our framework outperforms the benchmark method by reducing the average and maximum bandwidth usage by more than 10 times in high-resolution scenarios. Moreover, our method has been successfully applied in the multi-UAV autonomous navigation application, demonstrating its real-time and bandwidth-efficient nature, as well as its practical value.},
  archive   = {C_IROS},
  author    = {Liuyu Shi and Longji Yin and Fanze Kong and Yunfan Ren and Fangcheng Zhu and Benxu Tang and Fu Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802281},
  month     = {10},
  pages     = {8489-8496},
  title     = {Real-time bandwidth-efficient occupancy grid map synchronization for multi-robot systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MonoPlane: Exploiting monocular geometric cues for
generalizable 3D plane reconstruction. <em>IROS</em>, 8481–8488. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a generalizable 3D plane detection and reconstruction framework named MonoPlane. Unlike previous robust estimator-based works (which require multiple images or RGB-D input) and learning-based works (which suffer from domain shift), MonoPlane combines the best of two worlds and establishes a plane reconstruction pipeline based on monocular geometric cues, resulting in accurate, robust and scalable 3D plane detection and reconstruction in the wild. Specifically, we first leverage large-scale pre-trained neural networks to obtain the depth and surface normals from a single image. These monocular geometric cues are then incorporated into a proximity-guided RANSAC framework to sequentially fit each plane instance. We exploit effective 3D point proximity and model such proximity via a graph within RANSAC to guide the plane fitting from noisy monocular depths, followed by image-level multi-plane joint optimization to improve the consistency among all plane instances. We further design a simple but effective pipeline to extend this single-view solution to sparse-view 3D plane reconstruction. Extensive experiments on a list of datasets demonstrate our superior zero-shot generalizability over baselines, achieving state-of-the-art plane reconstruction performance in a transferring setting. Our code is available at https://github.com/thuzhaowang/MonoPlane.},
  archive   = {C_IROS},
  author    = {Wang Zhao and Jiachen Liu and Sheng Zhang and Yishu Li and Sili Chen and Sharon X Huang and Yong-Jin Liu and Hengkai Guo},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802672},
  month     = {10},
  pages     = {8481-8488},
  title     = {MonoPlane: Exploiting monocular geometric cues for generalizable 3D plane reconstruction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SNF-feat: Semantic-guided negative-sample-free
representation learning for local feature extraction. <em>IROS</em>,
8473–8480. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Local feature extraction constitutes a foundational module crucial for numerous downstream tasks of computer vision. Its primary challenge lies in the generation of discriminative feature representations. Prior methodologies have employed contrastive learning within their pipelines, yet have encountered limitations stemming from inherent conflicts within their training data, including the ambiguity of negative samples and the distortion of positive samples. In this study, we propose a semantic-guided negative-sample-free method for local feature learning, denoted as SNF-Feat. Our framework entails dense patch-level representation learning without reliance on negative samples, aiming to ensure that descriptors derived from transformed views of the same local area exhibit predictive capability towards each other. To assess the impact of positive sample distortion, we harness high-level semantic information to derive point-wise loss weights. Furthermore, we establish a self-supervised feature learning paradigm that extends our utilization of datasets. Experimental results demonstrate the superior performance of our method across a range of typical datasets and tasks in comparison to state-of-the-art approaches.},
  archive   = {C_IROS},
  author    = {Xun Zhou and Qingqing Yan and Minghao Zhu and Mengxian Hu and Chengju Liu and Qijun Chen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802559},
  month     = {10},
  pages     = {8473-8480},
  title     = {SNF-feat: Semantic-guided negative-sample-free representation learning for local feature extraction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ARDuP: Active region video diffusion for universal policies.
<em>IROS</em>, 8465–8472. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sequential decision-making can be formulated as a text-conditioned video generation problem, where a video planner, guided by a text-defined goal, generates future frames visualizing planned actions, from which control actions are subsequently derived. In this work, we introduce Active Region Video Diffusion for Universal Policies (ARDuP), a novel framework for video-based policy learning that emphasizes the generation of active regions, i.e. potential interaction areas, enhancing the conditional policy’s focus on interactive areas critical for task execution. This innovative framework integrates active region conditioning with latent diffusion models for video planning and employs latent representations for direct action decoding during inverse dynamic modeling. By utilizing motion cues in videos for automatic active region discovery, our method eliminates the need for manual annotations of active regions. We validate ARDuP’s efficacy via extensive experiments on simulator CLIPort and the real-world dataset BridgeData v2, achieving notable improvements in success rates and generating convincingly realistic video plans.},
  archive   = {C_IROS},
  author    = {Shuaiyi Huang and Mara Levy and Zhenyu Jiang and Anima Anandkumar and Yuke Zhu and Linxi Fan and De-An Huang and Abhinav Shrivastava},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802264},
  month     = {10},
  pages     = {8465-8472},
  title     = {ARDuP: Active region video diffusion for universal policies},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WidthFormer: Toward efficient transformer-based BEV view
transformation. <em>IROS</em>, 8457–8464. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present WidthFormer, a novel transformer-based module to compute Bird’s-Eye-View (BEV) representations from multi-view cameras for real-time autonomous-driving applications. WidthFormer is computationally efficient, robust and does not require any special engineering effort to deploy. We first introduce a novel 3D positional encoding mechanism capable of accurately encapsulating 3D geometric information, which enables our model to compute high-quality BEV representations with only a single transformer decoder layer. This mechanism is also beneficial for existing sparse 3D object detectors. Inspired by the recently proposed works, we further improve our model’s efficiency by vertically compressing the image features when serving as attention keys and values, and then we develop two modules to compensate for potential information loss due to feature compression. Experimental evaluation on the widely-used nuScenes 3D object detection benchmark demonstrates that our method outperforms previous approaches across different 3D detection architectures. More importantly, our model is highly efficient. For example, when using 256 × 704 input images, it achieves 1.5 ms and 2.8 ms latency on NVIDIA 3090 GPU and Horizon Journey-5 computation solutions. Furthermore, WidthFormer also exhibits strong robustness to different degrees of camera perturbations. Our study offers valuable insights into the deployment of BEV transformation methods in real-world, complex road environments. Code is available at https://github.com/ChenhongyiYang/WidthFormer.},
  archive   = {C_IROS},
  author    = {Chenhongyi Yang and Tianwei Lin and Lichao Huang and Elliot J. Crowley},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801452},
  month     = {10},
  pages     = {8457-8464},
  title     = {WidthFormer: Toward efficient transformer-based BEV view transformation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RaceMOP: Mapless online path planning for multi-agent
autonomous racing using residual policy learning. <em>IROS</em>,
8449–8456. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The interactive decision-making in multi-agent autonomous racing offers insights valuable beyond the domain of self-driving cars. Mapless online path planning is particularly of practical appeal but poses a challenge for safely overtaking opponents due to the limited planning horizon. To address this, we introduce RaceMOP, a novel method for mapless online path planning designed for multi-agent racing of F1TENTH cars. Unlike classical planners that rely on predefined racing lines, RaceMOP operates without a map, utilizing only local observations to execute high-speed overtaking maneuvers. Our approach combines an artificial potential field method as a base policy with residual policy learning to enable long-horizon planning. We advance the field by introducing a novel approach for policy fusion with the residual policy directly in probability space. Extensive experiments on twelve simulated racetracks validate that RaceMOP is capable of long-horizon decision-making with robust collision avoidance during overtaking maneuvers. RaceMOP demonstrates superior handling over existing mapless planners and generalizes to unknown racetracks, affirming its potential for broader applications in robotics. Our code is available at http://github.com/raphajaner/racemop.},
  archive   = {C_IROS},
  author    = {Raphael Trumpp and Ehsan Javanmardi and Jin Nakazato and Manabu Tsukada and Marco Caccamo},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801657},
  month     = {10},
  pages     = {8449-8456},
  title     = {RaceMOP: Mapless online path planning for multi-agent autonomous racing using residual policy learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Smooth invariant interpolation on lie groups with prescribed
terminal conditions for robot motion planning and modeling of soft
robots. <em>IROS</em>, 8442–8448. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Interpolation of rigid body motions, or a general frame motion in Euclidean space, is a recurring topic in robotics. It boils down to generating trajectories in a Lie group, either SE (3) or SO (3) × ℝ3, with given initial and/or terminal values. To this end, spline interpolation schemes were developed where canonical coordinates are represented by cubic splines. They allow for prescribing initial velocity and acceleration only. In many robotic applications, terminal conditions are prescribed, however. In this paper, a novel interpolation scheme is presented that admits prescribing the terminal pose, velocity and acceleration, or the initial condition. As example, the scheme is applied to a rendezvous task of a UAV and describing the deformation of a Cosserat beam as relevant for soft robotics. The presented interpolation scheme can be directly applied to the motion parameterization in terms of (dual) quaternions.},
  archive   = {C_IROS},
  author    = {Andreas Müller and Tobias Marauli and Hubert Gattringer},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802790},
  month     = {10},
  pages     = {8442-8448},
  title     = {Smooth invariant interpolation on lie groups with prescribed terminal conditions for robot motion planning and modeling of soft robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MARPF: Multi-agent and multi-rack path finding.
<em>IROS</em>, 8435–8441. (<a
href="https://doi.org/10.1109/IROS58592.2024.10803053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In environments where many automated guided vehicles (AGVs) operate, planning efficient, collision-free paths is essential. Related research has mainly focused on environments with pre-defined passages, resulting in space inefficiency. We attempt to relax this assumption. In this study, we define multi-agent and multi-rack path finding (MARPF) as the problem of planning paths for AGVs to convey target racks to their designated locations in environments without passages. In such environments, an AGV without a rack can pass under racks, whereas one with a rack cannot pass under racks to avoid collisions. MARPF entails conveying the target racks without collisions, while the obstacle racks are relocated to prevent any interference with the target racks. We formulated MARPF as an integer linear programming problem in a network flow. To distinguish situations in which an AGV is or is not loading a rack, the proposed method introduces two virtual layers into the network. We optimized the AGVs’ movements to move obstacle racks and convey the target racks. The formulation and applicability of the algorithm were validated through numerical experiments. The results indicated that the proposed algorithm addressed issues in environments with dense racks.},
  archive   = {C_IROS},
  author    = {Hiroya Makino and Yoshihiro Ohama and Seigo Ito},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10803053},
  month     = {10},
  pages     = {8435-8441},
  title     = {MARPF: Multi-agent and multi-rack path finding},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online multi-agent pickup and delivery with task deadlines.
<em>IROS</em>, 8428–8434. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Managing delivery deadlines in automated warehouses and factories is crucial for maintaining customer satisfaction and ensuring seamless production. This study introduces the problem of online multi-agent pickup and delivery with task deadlines (MAPD-D), an advanced variant of the online MAPD problem incorporating delivery deadlines. In the MAPD problem, agents must manage a continuous stream of delivery tasks online. Tasks are added at any time. Agents must complete their tasks while avoiding collisions with each other. MAPD-D introduces a dynamic, deadline-driven approach that incorporates task deadlines, challenging the conventional MAPD frameworks. To tackle MAPD-D, we propose a novel algorithm named deadline-aware token passing (D-TP). The D-TP algorithm calculates pickup deadlines and assigns tasks while balancing execution cost and deadline proximity. Additionally, we introduce the D-TP with task swaps (D-TPTS) method to further reduce task tardiness, enhancing flexibility and efficiency through task-swapping strategies. Numerical experiments were conducted in simulated warehouse environments to showcase the effectiveness of the proposed methods. Both D-TP and D-TPTS demonstrated significant reductions in task tardiness compared to existing methods. Our methods contribute to efficient operations in automated warehouses and factories with delivery deadlines.},
  archive   = {C_IROS},
  author    = {Hiroya Makino and Seigo Ito},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802597},
  month     = {10},
  pages     = {8428-8434},
  title     = {Online multi-agent pickup and delivery with task deadlines},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CRPlace: Camera-radar fusion with BEV representation for
place recognition. <em>IROS</em>, 8421–8427. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The integration of complementary characteristics from camera and radar data has emerged as an effective approach in 3D object detection. However, such fusion-based methods remain unexplored for place recognition, an equally important task for autonomous systems. Given that place recognition relies on the similarity between a query scene and the corresponding candidate scene, the stationary background of a scene is expected to play a crucial role in the task. As such, current well-designed camera-radar fusion methods for 3D object detection can hardly take effect in place recognition because they mainly focus on dynamic foreground objects. In this paper, a background-attentive camera-radar fusion-based method, named CRPlace, is proposed to generate background-attentive global descriptors from multi-view images and radar point clouds for accurate place recognition. To extract stationary background features effectively, we design an adaptive module that generates the background-attentive mask by utilizing the camera BEV feature and radar dynamic points. With the guidance of a background mask, we devise a bidirectional cross-attention-based spatial fusion strategy to facilitate comprehensive spatial interaction between the background information of the camera BEV feature and the radar BEV feature. As the first camera-radar fusion-based place recognition network, CRPlace has been evaluated thoroughly on the nuScenes dataset. The results show that our algorithm outperforms a variety of baseline methods across a comprehensive set of metrics (recall@1 reaches 91.2%).},
  archive   = {C_IROS},
  author    = {Shaowei Fu and Yifan Duan and Yao Li and Chengzhen Meng and Yingjie Wang and Jianmin Ji and Yanyong Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802267},
  month     = {10},
  pages     = {8421-8427},
  title     = {CRPlace: Camera-radar fusion with BEV representation for place recognition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WSCLoc: Weakly-supervised sparse-view camera relocalization
via radiance field. <em>IROS</em>, 8414–8420. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite the advancements in deep learning for camera relocalization tasks, obtaining ground truth pose labels required for the training process remains a costly endeavor. While current weakly supervised methods excel in lightweight label generation, their performance notably declines in scenarios with sparse views. In response to this challenge, we introduce WSCLoc, a system capable of being customized to various deep learning-based relocalization models to enhance their performance under weakly-supervised and sparse view conditions. This is realized with two stages. In the initial stage, WSCLoc employs a multilayer perceptron-based structure called WFT-NeRF to co-optimize image reconstruction quality and initial pose information. To ensure a stable learning process, we incorporate temporal information as input. Furthermore, instead of optimizing SE(3), we opt for sim(3) optimization to explicitly enforce a scale constraint. In the second stage, we co-optimize the pre-trained WFT-NeRF and WFT-Pose. This optimization is enhanced by Time-Encoding based Random View Synthesis and supervised by inter-frame geometric constraints that consider pose, depth, and RGB information. We validate our approaches on two publicly available datasets, one outdoor and one indoor. Our experimental results demonstrate that our weakly-supervised relocalization solutions achieve superior pose estimation accuracy in sparse-view scenarios, comparable to state-of-the-art camera relocalization methods. We will make our code publicly available.},
  archive   = {C_IROS},
  author    = {Jialu Wang and Kaichen Zhou and Andrew Markham and Niki Trigoni},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801669},
  month     = {10},
  pages     = {8414-8420},
  title     = {WSCLoc: Weakly-supervised sparse-view camera relocalization via radiance field},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pos2VPR: Fast position consistency validation with positive
sample mining for hierarchical place recognition. <em>IROS</em>,
8408–8413. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual place recognition (VPR) is a challenging issue for robotics and autonomous systems, focusing on utilizing visual information for robot localization. Currently, hierarchical architecture is being employed by growing works, which embraces RANSAC-based geometric verification for re-ranking. However, RANSAC is time-consuming and only employs geometric information while neglecting other potential information that could be useful for re-ranking. Here we propose a fast position consistency via local patch (PCLP) algorithm to take the position of task-relevant patch-descriptor into account. Without training, it only costs little time but performs better than other re-ranking methods that rely on geometric consistency verification. In this paper, we present a unified place recognition framework that incorporates an aggregation module to extract global features for retrieval and a PCLP validation module to filter local patch for reranking. Meanwhile, we propose a RANSAC-based tightly coupled learning (R-TCL) strategy to discover the best positive sample for training robust models. Unlike common sample mining methods, we introduce RANSAC into the sample mining process, achieving trade-off between efficiency and accuracy. Due to improved positive sample mining strategy and novel position validation module, our model is named as Pos2VPR. Remarkably, Pos2VPR outperforms state-of-the-art methods on four major datasets with extremely short running time.},
  archive   = {C_IROS},
  author    = {Dehao Zou and Xiaolong Qian and Yunzhou Zhang and Xinge Zhao and Zhuo Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801788},
  month     = {10},
  pages     = {8408-8413},
  title     = {Pos2VPR: Fast position consistency validation with positive sample mining for hierarchical place recognition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Representing 3D sparse map points and lines for camera
relocalization. <em>IROS</em>, 8400–8407. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advancements in visual localization and mapping have demonstrated considerable success in integrating point and line features. However, expanding the localization framework to include additional mapping components frequently results in increased demand for memory and computational resources dedicated to matching tasks. In this study, we show how a lightweight neural network can learn to represent both 3D point and line features, and exhibit leading pose accuracy by harnessing the power of multiple learned mappings. Specifically, we utilize a single transformer block to encode line features, effectively transforming them into distinctive point-like descriptors. Subsequently, we treat these point and line descriptor sets as distinct yet interconnected feature sets. Through the integration of self- and cross-attention within several graph layers, our method effectively refines each feature before regressing 3D maps using two simple MLPs. In comprehensive experiments, our indoor localization findings surpass those of Hloc and Limap across both point-based and line-assisted configurations. Moreover, in outdoor scenarios, our method secures a significant lead, marking the most considerable enhancement over state-of-the-art learning-based methodologies. The source code and demo videos of this work are publicly available at: https://thpjp.github.io/pl2map/.},
  archive   = {C_IROS},
  author    = {Bach-Thuan Bui and Huy-Hoang Bui and Dinh-Tuan Tran and Joo-Ho Lee},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802402},
  month     = {10},
  pages     = {8400-8407},
  title     = {Representing 3D sparse map points and lines for camera relocalization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An optical interferometer-based force sensor system for
enhancing precision in epidural injection procedure. <em>IROS</em>,
8393–8399. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In minimally invasive pain management procedures, precise needle positioning is paramount for effective treatment and patient safety. Traditional techniques like the loss-of-resistance (LOR) method may be insufficient, especially in patients with narrowed epidural spaces. The use of imaging tools such as C-arms carries risks due to radiation exposure for medical professionals. A new system for detecting the epidural space based on optical interferometry is proposed to tackle this issue. Prior research has focused on force measurement systems to identify tissue puncture or rupture. Although mechanical sensors have been utilized, they add bulk and complexity to systems. Optical sensors like Fiber Bragg grating (FBG) and Fabry-Pérot interferometer (FPI) offer stable, high-resolution measurements suitable for complex biological tissues. This study aims to develop a sensor and needle system for epidural injections, incorporating quantitative metrics for validation. An optical interferometer-based force measurement sensor was integrated into a commercial epidural needle, and calibration was performed to establish a correlation between system output and actual force. The system employs a graphical user interface (GUI) to identify puncture points based on abrupt force decreases. A user study involving interventionalists assessed the system’s performance by measuring invasive depth and success rates. The user study demonstrated that the proposed sensorized system could detect the puncture with an average success rate of 72.63 %. This study represents a significant advancement toward safer and more precise epidural procedures, addressing critical clinical considerations for practical applications.},
  archive   = {C_IROS},
  author    = {Gichan Cho and Jintaek Im and Hyunjung Kwon and Cheol Song},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802501},
  month     = {10},
  pages     = {8393-8399},
  title     = {An optical interferometer-based force sensor system for enhancing precision in epidural injection procedure},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An online rcm adjusting system for robot-assisted retinal
surgeries. <em>IROS</em>, 8385–8392. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In robot-assisted retinal surgery, a Remote Center of Motion (Rcm) allows the surgical instrument to rotate around a distal fixed point without any lateral translations. The Rcm point should be perfectly aligned inside the trocar. Otherwise, unexpected tool translations at the expected remote center will enlarge the force applied to the trocar and consequently result in post-operative complications. Due to the narrow size of the trocar and the lack of real-time detection equipment, the Rcm point is hard to be perfectly located inside the trocar. Even if the Rcm is perfectly aligned, the movement of the tissue around the eyeball could make it inappropriate again. In this paper, inspired by the control strategy of surgeons, an online Rcm adjusting strategy is proposed. Instead of only using one fixed Rcm point, to restrict the force between the surgical tool and the trocar, the proposed strategy adjusts the position of the Rcm point during the motion. The results show our approach significantly reduces the force between the robot end-effector and surgical port by 64.2%. In addition, the results also demonstrate that our approach complies the Rcm trajectories without deforming or spoiling the working space, which is significantly important for obeying surgeon’s instructions in practice.},
  archive   = {C_IROS},
  author    = {Jun Xia and Ting Wang and Huanqi Ni and Yanlin Li and Ruoxi Chen and M. Ali Nasseri and Haotian Lin and Kai Huang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802804},
  month     = {10},
  pages     = {8385-8392},
  title     = {An online rcm adjusting system for robot-assisted retinal surgeries},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FF-SRL: High performance GPU-based surgical simulation for
robot learning. <em>IROS</em>, 8378–8384. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic surgery is a rapidly developing field that can greatly benefit from the automation of surgical tasks. However, training techniques such as Reinforcement Learning (RL) require a high number of task repetitions, which are generally unsafe and impractical to perform on real surgical systems. This stresses the need for simulated surgical environments, which are not only realistic, but also computationally efficient and scalable. We introduce FF-SRL (Fast and Flexible Surgical Reinforcement Learning), a high-performance learning environment for robotic surgery. In FF-SRL both physics simulation and RL policy training reside entirely on a single GPU. This avoids typical bottlenecks associated with data transfer between the CPU and GPU, leading to accelerated learning rates. Our results show that FF-SRL reduces the training time of a complex tissue manipulation task by an order of magnitude, down to a couple of minutes, compared to a common CPU/GPU simulator. Such speed-up may facilitate the experimentation with RL techniques and contribute to the development of new generation of surgical systems. To this end, we make our code publicly available to the community.},
  archive   = {C_IROS},
  author    = {Diego Dall’Alba and Michał Naskręt and Sabina Kamińska and Przemysław Korzeniowski},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801658},
  month     = {10},
  pages     = {8378-8384},
  title     = {FF-SRL: High performance GPU-based surgical simulation for robot learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Saturation in the null-space (SNS) for tele-operated
surgery: Prioritized motion control for RCM and joint limit constraints.
<em>IROS</em>, 8370–8377. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper showcases the application of the Saturation in the Null Space (SNS) algorithm to establish task prioritization and coordination within a tele-operated minimally invasive robotic surgical setting. In our work, SNS prioritizes achieving Remote Center of Motion (RCM) constraint, ensuring safe instrument manipulation, while respecting joint constraints for uninterrupted robot operation. This prioritization allows for accommodating the tracking of the surgeon’s motion, within the capabilities defined by RCM and joint constraints. We investigate both the velocity and acceleration control variants of the SNS algorithm, incorporating bespoke adjustments to tailor the original algorithm to the intricate requirements of surgical applications. Through simulations and experiments, this work aims to demonstrate the effectiveness of SNS in enhancing the safety and controllability of tele-operated surgery, paving the way for its integration in various surgical procedures.},
  archive   = {C_IROS},
  author    = {Sreekanth Kana and Antonia Perez Arias and Robert Kahlau and Pavan Kanajar and Shashank Sharma},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801508},
  month     = {10},
  pages     = {8370-8377},
  title     = {Saturation in the null-space (SNS) for tele-operated surgery: Prioritized motion control for RCM and joint limit constraints},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Identification and validation of the dynamic model of a
tendon-driven anthropomorphic finger. <em>IROS</em>, 8330–8337. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study addresses the absence of an identification framework to quantify a comprehensive dynamic model of human and anthropomorphic tendon-driven fingers, which is necessary to investigate the physiological properties of human fingers and improve the control of robotic hands. First, a generalized dynamic model was formulated, which takes into account the inherent properties of such a mechanical system. This includes rigid-body dynamics, coupling matrix, joint viscoelasticity, and tendon friction. Then, we propose a methodology comprising a series of experiments, for step-wise identification and validation of this dynamic model. Moreover, an experimental setup was designed and constructed that features actuation modules and peripheral sensors to facilitate the identification process. To verify the proposed methodology, a 3D-printed robotic finger based on the index finger design of the Dexmart hand was developed, and the proposed experiments were executed to identify and validate its dynamic model. This study could be extended to explore the identification of cadaver hands, aiming for a consistent dataset from a single cadaver specimen to improve the development of musculoskeletal hand models.},
  archive   = {C_IROS},
  author    = {Junnan Li and Lingyun Chen and Johannes Ringwald and Edmundo Pozo Fortunić and Amartya Ganguly and Sami Haddadin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801538},
  month     = {10},
  pages     = {8330-8337},
  title     = {Identification and validation of the dynamic model of a tendon-driven anthropomorphic finger},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatic spatial calibration of near-field MIMO radar with
respect to optical depth sensors. <em>IROS</em>, 8322–8329. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite an emerging interest in MIMO radar, the utilization of its complementary strengths in combination with optical depth sensors has so far been limited to far-field applications, due to the challenges that arise from mutual sensor calibration in the near field. In fact, most related approaches in the autonomous industry propose target-based calibration methods using corner reflectors that have proven to be unsuitable for the near field. In contrast, we propose a novel, joint calibration approach for optical RGB-D sensors and MIMO radars that is designed to operate in the radar’s near-field range, within decimeters from the sensors. Our pipeline consists of a bespoke calibration target, allowing for automatic target detection and localization, followed by the spatial calibration of the two sensor coordinate systems through target registration. We validate our approach using two different depth sensing technologies from the optical domain. The experiments show the efficiency and accuracy of our calibration for various target displacements, as well as its robustness of our localization in terms of signal ambiguities.},
  archive   = {C_IROS},
  author    = {Vanessa Wirth and Johanna Bräunig and Danti Khouri and Florian Gutsche and Martin Vossiek and Tim Weyrich and Marc Stamminger},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801705},
  month     = {10},
  pages     = {8322-8329},
  title     = {Automatic spatial calibration of near-field MIMO radar with respect to optical depth sensors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3DGS-calib: 3D gaussian splatting for multimodal
SpatioTemporal calibration. <em>IROS</em>, 8315–8321. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reliable multimodal sensor fusion algorithms require accurate spatiotemporal calibration. Recently, targetless calibration techniques based on implicit neural representations have proven to provide precise and robust results. Nevertheless, such methods are inherently slow to train given the high computational overhead caused by the large number of sampled points required for volume rendering. With the recent introduction of 3D Gaussian Splatting as a faster alternative to implicit representation methods, we propose to leverage this new rendering approach to achieve faster multi-sensor calibration. We introduce 3DGS-Calib, a new calibration method that relies on the speed and rendering accuracy of 3D Gaussian Splatting to achieve multimodal spatiotemporal calibration that is accurate, robust, and with a substantial speed-up compared to methods relying on implicit neural representations. We demonstrate the superiority of our proposal with experimental results on sequences from KITTI-360, a widely used driving dataset.},
  archive   = {C_IROS},
  author    = {Quentin Herau and Moussab Bennehar and Arthur Moreau and Nathan Piasco and Luis Roldão and Dzmitry Tsishkou and Cyrille Migniot and Pascal Vasseur and Cédric Demonceaux},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801360},
  month     = {10},
  pages     = {8315-8321},
  title     = {3DGS-calib: 3D gaussian splatting for multimodal SpatioTemporal calibration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interactive robot-environment self-calibration via compliant
exploratory actions. <em>IROS</em>, 8307–8314. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Calibrating robots into their workspaces is crucial for manipulation tasks. Existing calibration techniques often rely on sensors external to the robot (cameras, laser scanners, etc.) or specialized tools. This reliance complicates the calibration process and increases the costs and time requirements. Furthermore, the associated setup and measurement procedures require significant human intervention, which makes them more challenging to operate. Using the built-in force-torque sensors, which are nowadays a default component in collaborative robots, this work proposes a self-calibration framework where robot-environmental spatial relations are automatically estimated through compliant exploratory actions by the robot itself. The self-calibration approach converges, verifies its own accuracy, and terminates upon completion, autonomously purely through interactive exploration of the environment’s geometries. Extensive experiments validate the effectiveness of our self-calibration approach in accurately establishing the robot-environment spatial relationships without the need for additional sensing equipment or any human intervention.},
  archive   = {C_IROS},
  author    = {Podshara Chanrungmaneekul and Kejia Ren and Joshua T. Grace and Aaron M. Dollar and Kaiyu Hang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802330},
  month     = {10},
  pages     = {8307-8314},
  title     = {Interactive robot-environment self-calibration via compliant exploratory actions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-frequency capacitive sensing for electrohydraulic soft
actuators. <em>IROS</em>, 8299–8306. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The need for compliant and proprioceptive actuators has grown more evident in pursuing more adaptable and versatile robotic systems. Hydraulically Amplified Self-Healing Electrostatic (HASEL) actuators offer distinctive advantages with their inherent softness and flexibility, making them promising candidates for various robotic tasks, including delicate interactions with humans and animals, biomimetic locomotion, prosthetics, and exoskeletons. This has resulted in a growing interest in the capacitive self-sensing capabilities of HASEL actuators to create miniature displacement estimation circuitry that does not require external sensors. However, achieving HASEL self-sensing for actuation frequencies above 1 Hz and with miniature high-voltage power supplies has remained limited. In this paper, we introduce the F-HASEL actuator, which adds an additional electrode pair used exclusively for capacitive sensing to a Peano-HASEL actuator. We demonstrate displacement estimation of the F-HASEL during high-frequency actuation up to 20 Hz and during external loading using miniaturized circuitry comprised of low-cost off-the-shelf components and a miniature high-voltage power supply. Finally, we propose circuitry to estimate the displacement of multiple F-HASELs and demonstrate it in a wearable application to track joint rotations of a virtual reality user in real-time.},
  archive   = {C_IROS},
  author    = {Michel R. Vogt and Maximilian Eberlein and Clemens C. Christoph and Felix Baumann and Fabrice Bourquin and Wim Wende and Fabio Schaub and Amirhossein Kazemipour and Robert K. Katzschmann},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802777},
  month     = {10},
  pages     = {8299-8306},
  title     = {High-frequency capacitive sensing for electrohydraulic soft actuators},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). An origami-inspired pneumatic continuum module with active
variable stiffness. <em>IROS</em>, 8293–8298. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel pneumatic continuum module featuring high contraction-ratio, bidirectional actuation, and active stiffness regulation. The module comprises four linear pneumatic actuators integrating rigid polygon origami frame into soft bellow. This integration not only helps to regulate motion but also enhances structural strength, facilitating contraction and bending performance. The contraction resulting from vacuum pressure serves as a limiting layer for one opposing pair of actuators, while the other pair operates in a virtual antagonistic configuration, allowing for active regulation of joint stiffness through pressure control. The paper provides a detailed workflow covering the design, fabrication, and mathematical modeling of the pneumatic module. Furthermore, the paper presents verifications of the module’s actuation capacity and active variable stiffness. The findings of this study can serve as valuable references for the design of manipulators.},
  archive   = {C_IROS},
  author    = {Zhuowen Li and Huaiyuan Chen and Fan Xu and Hesheng Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801420},
  month     = {10},
  pages     = {8293-8298},
  title     = {An origami-inspired pneumatic continuum module with active variable stiffness},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Embedded valves for distributed control of soft pneumatic
actuators. <em>IROS</em>, 8286–8292. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft robotic systems are inherently compliant, giving them unique capabilities not possessed by traditional rigid-bodied robot systems. Many soft systems rely on soft pneumatic actuators. One of the biggest downsides of such actuators is the need for bulky pressure-regulating devices and individual pneumatic supply lines. In this work, a miniaturized pressure-regulating system is developed and embedded into the unused space inside of a soft pneumatic McKibben actuator, enabling the simultaneous pressure control of multiple actuators connected to a single pneumatic supply line. This &quot;valve-embedded&quot; actuator is capable of regulating its internal pressure within 0.05 psi of a desired set point, even under external load. Compared to a conventional McKibben actuator driven by external valves, the valve-embedded actuator is experimentally shown to consistently achieve faster settling times. To showcase the practical application of the valve-embedded actuator on a robotic system, a 0.9m serial-linked robot driven by five independently controlled valve-embedded actuators was assembled, and was shown to achieve an average root mean square error of less than 1.5cm in a waypoint tracking experiment. The miniaturized pressure control system developed in this work is open source and could be embedded in any fluid-driven actuator, enabling more capable and densely actuated pneumatic soft robots.},
  archive   = {C_IROS},
  author    = {Runze Zuo and Mayank Mehta and Dong Heon Han and Daniel Bruder},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801494},
  month     = {10},
  pages     = {8286-8292},
  title     = {Embedded valves for distributed control of soft pneumatic actuators},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FOCWS: A high sensitive flexible optical curvature sensor
inspired by arthropod sensory systems. <em>IROS</em>, 8280–8285. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Flexible sensors for joint angle measurement play a crucial role in various human-robot interaction applications. In previous studies, sensors with various sensing mechanisms have been developed. Among them, optical waveguide sensors exhibit high resistance to environmental factors (such as temperature and humidity) and low sensitivity to electromagnetic interference. Researchers have enhanced the sensitivity of optical waveguide sensors to tensile strain by doping other substances (such as graphite) into the optical core material of the optical waveguide. However, the sensitivity of measuring joint angles based on tensile strain principles remains relatively low. In nature, arthropods utilize crack-like structures near their leg joints to perceive minute mechanical stress changes. Here, we propose a curvature sensor based on a Flexible Optical Crack Waveguide Structure (FOCWS) inspired by the arthropod sensory systems. By cutting the optical core, we increase its light power loss during bending strain, thereby enhancing the sensor’s sensitivity to angle measurement. The characteristics of light propagation and geometric parameters were studied through simulation, and experiments were designed to validate the simulation results. The average sensitivity is 0.068 dB/°, which is nearly 300 times higher compared to uncut optical waveguide.},
  archive   = {C_IROS},
  author    = {Jiachen Wei and Zhengwei Li and Zeyu Liu and Wei He and Long Cheng and Yanhong Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802017},
  month     = {10},
  pages     = {8280-8285},
  title     = {FOCWS: A high sensitive flexible optical curvature sensor inspired by arthropod sensory systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Small multi-rotor UAV oriented direct thrust sensor based on
lightweight barometers. <em>IROS</em>, 8175–8182. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The multirotor unmanned aerial vehicle (UAV) requires precise control over thrust output when operating in wind-disturbed environments or executing intricate flight missions. Although current commercial force sensors offer high sensitivity and accuracy, they are often heavy and costly. These characteristics restrict their applicability in weight-sensitive and cost-sensitive scenarios, such as thrust measurement in UAVs. To overcome this difficulty, we have developed an embedded barometric force sensor (BFS) that mounts between the UAV’s airframe and motor, allowing direct measurement of the force exerted by the rotor on the UAV’s rigid body. The BFS is designed using low-cost MEMS barometers as tactile force sensors, encased in polyurethane rubber. Subsequently, we established its parameter model and devised a stability improvement strategy to reduce the impact of temperature. Additionally, we designed a structure suitable for mounting the BFS on the UAV to safeguard the rubber module from damage and reconstructed the thrust model to account for the impact of weight and friction on thrust measurement. Finally, we assembled testing platforms to validate the performance of the BFS. Experimental results demonstrate the BFS’s excellent linearity, wide range, adequate bandwidth to respond to UAV thrust variations, and confirm the feasibility of mounting the BFS on the UAV for thrust measurement and force feedback control.},
  archive   = {C_IROS},
  author    = {Han Jiang and Yanchun Chang and Liying Yang and Yuqing He},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802315},
  month     = {10},
  pages     = {8175-8182},
  title     = {Small multi-rotor UAV oriented direct thrust sensor based on lightweight barometers},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data-driven modeling of ground effect for UAV landing on a
vertical oscillating platform. <em>IROS</em>, 8169–8174. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Landing on a vertically oscillating platform poses a significant challenge for multi-rotor unmanned aerial vehicle (UAVs) due to the time-varying ground effect (GE). In this work, we formulated a data-driven GE dynamic model that accurately describes the complex interactions between UAVs and both stationary and oscillating platforms. Integrating this model with a feedforward controller effectively compensates for GE, resulting in improved landing performance. The proposed GE model elucidates the relationship between GE and factors such as UAVs’ velocity, throttle magnitude, and the motion of the landing platform. It highlights that the GE experienced during the landing process of UAVs is not only contingent on the current state but also related to past states. The resulting GE model is parsimonious and suitable for onboard computers with limited computational power, and its accuracy has been confirmed through a series of flight experiments. To demonstrate the effectiveness of the developed UAVs landing scheme, we compared our approach with robust control and internal model control methods. Experimental results indicate that the proposed landing strategy achieves faster and smoother landings, with at least a 22% improvement in smoothness and a 25% reduction in landing time.},
  archive   = {C_IROS},
  author    = {Binglin He and Heng Zhang and Baisheng Lai and Song Liu and Yang Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802474},
  month     = {10},
  pages     = {8169-8174},
  title     = {Data-driven modeling of ground effect for UAV landing on a vertical oscillating platform},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AGL-net: Aerial-ground cross-modal global localization with
varying scales. <em>IROS</em>, 8161. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present AGL-NET, a novel learning-based method for global localization using LiDAR point clouds and satellite maps. AGL-Net tackles two critical challenges: bridging the representation gap between image and points modalities for robust feature matching, and handling inherent scale discrepancies between global view and local view. To address these challenges, AGL-Net leverages a unified network architecture with a novel two-stage matching design. The first stage extracts informative neural features directly from raw sensor data and performs initial feature matching. The second stage refines this matching process by extracting informative skeleton features and incorporating a novel scale alignment step to rectify scale variations between LiDAR and map data. Furthermore, a novel scale and skeleton loss function guides the network toward learning scale-invariant feature representations, eliminating the need for pre-processing satellite maps. This significantly improves real-world applicability in scenarios with unknown map scales. To facilitate rigorous performance evaluation, we introduce a meticulously designed dataset within the CARLA simulator specifically tailored for metric localization training and assessment.},
  archive   = {C_IROS},
  author    = {Tianrui Guan and Ruiqi Xian and Xijun Wang and Xiyang Wu and Mohamed Elnoor and Daeun Song and Dinesh Manocha},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802020},
  month     = {10},
  pages     = {8161},
  title     = {AGL-net: Aerial-ground cross-modal global localization with varying scales},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intention-aware planner for robust and safe aerial tracking.
<em>IROS</em>, 8153–8160. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous target tracking with quadrotors has wide applications in many scenarios, such as cinematographic follow-up shooting or suspect chasing. Target motion prediction is necessary when designing the tracking planner. However, the widely used constant velocity or constant rotation assumption can not fully capture the dynamics of the target. The tracker may fail when the target happens to move aggressively, such as sudden turn or deceleration. In this paper, we propose an intention-aware planner by additionally considering the intention of the target to enhance safety and robustness in aerial tracking applications. Firstly, a designated intention prediction method is proposed, which combines a user-defined potential assessment function and a state observation function. A reachable region is generated to speci cally evaluate the turning intentions. Then we design an intention-driven hybrid A* method to predict the future possible positions for the target. Finally, an intention-aware optimization approach is designed to generate a spatial-temporal optimal trajectory, allowing the tracker to perceive unexpected situations from the target. Benchmark comparisons and real-world experiments are conducted to validate the performance of our method.},
  archive   = {C_IROS},
  author    = {Qiuyu Ren and Huan Yu and Jiajun Dai and Zhi Zheng and Jun Meng and Li Xu and Chao Xu and Fei Gao and Yanjun Cao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802015},
  month     = {10},
  pages     = {8153-8160},
  title     = {Intention-aware planner for robust and safe aerial tracking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attainable force approximation and full-pose tracking
control of an over-actuated thrust-vectoring modular team UAV.
<em>IROS</em>, 8147–8152. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traditional vertical take-off and landing (VTOL) aircraft can not achieve optimal efficiency for various payload weights and has limited mobility due to its under-actuation. With the thrust-vectoring mechanism, the proposed modular team UAV is fully actuated at certain attitudes. However, the attainable force space (AFS) differs according to the team configuration, which makes the controller design difficult. We propose an approximation to the AFS and a full-pose tracking controller with an attitude planner and a force projection, which guarantees the control force is feasible. The proposed approach can be applied to UAVs having multiple thrust-vectoring effectors with homogeneous agents. The simulation and experiment demonstrate a tilting motion during hovering for a 4-agent team.},
  archive   = {C_IROS},
  author    = {Yen-Cheng Chu and Kai-Cheng Fang and Feng-Li Lian},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802026},
  month     = {10},
  pages     = {8147-8152},
  title     = {Attainable force approximation and full-pose tracking control of an over-actuated thrust-vectoring modular team UAV},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hardware-software co-design for path planning by drones.
<em>IROS</em>, 8141–8146. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work consists of two main components: designing a hardware-software co-design, MT+, for adapting the Mikami-Tabuchi algorithm for on-board path planning by drones in a 3D environment; and development of a specialized custom hardware accelerator CDU, as a part of MT+, for parallel collision detection. Collision detection is a performance bottleneck in path planning. MT+ reduces the delay in path planning without using any heuristic. A comparative analysis between the state-of-the-art path planning algorithm A* and Mikami-Tabuchi is performed to show that Mikami-Tabuchi is faster than A* in typical real-world environments. In custom-generated environments, path planning using Mikami-Tabuchi shows a latency improvement of 1.7× across varying average sizes of obstacles and 2.7× across varying obstacle density over state-of-the-art path planning algorithm, A*. Further, the experiments show that the co-design achieves speedups over a full software implementation on CPU, averaging between 10% to 60% across different densities and sizes of obstacles. CDU area and power overheads are negligible against a conventional single-core processor.},
  archive   = {C_IROS},
  author    = {Ayushi Dube and Omkar Patil and Gian Singh and Nakul Gopalan and Sarma Vrudhula},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802753},
  month     = {10},
  pages     = {8141-8146},
  title     = {Hardware-software co-design for path planning by drones},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SwiftEagle: An advanced open-source, miniaturized FPGA UAS
platform with dual DVS/frame camera for cutting-edge low-latency
autonomous algorithms. <em>IROS</em>, 8134–8140. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Low-latency sensing and decision-making processing are critical requirements for the highly dynamic control and perception applications often found in Unmanned Areal Systems (UASs). Novel sensors such as Dynamic Vision Sensors (DVSs) are enhancing the pure performance of the perception component with orders of magnitude lower latency. However, they are typically not optimally integrated with the computing hardware, which effectively reduces the potential in both latency and power consumption. In addition to the non-optimal integration, low latency processing of such high data rate generating sensors is often challenging on resource-constrained platforms. Here, Field Programmable Gate Arrays (FPGAs) platforms offer a promising set of features, including low-level access to hardware and memories, as well as high-speed interfaces. On the other hand, FPGA platforms are not as popular on UASs due to their complex programming and development environments, steep initial learning curve, and challenges in achieving optimal performance. To accelerate future development, this paper presents SwiftEagle, an open-source, cutting-edge 720g FPGA based UAS based on a custom hardware design including a dual camera interface RGB/DVSs on a multi-sensors subsystem and an initial software and firmware stack designed for high precision recording of machine learning datasets in-flight with sub-micro-second time resolution and on-FPGA rendering of DVS event frames. Utilizing this developed platform, as proof-of-concept the end-to-end latency of a novel, just released DVS is shown to be below 210µs as a worst-case scenario, enabling future cutting-edge autonomous algorithms.},
  archive   = {C_IROS},
  author    = {Christian Vogt and Michael Jost and Michele Magno},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802491},
  month     = {10},
  pages     = {8134-8140},
  title     = {SwiftEagle: An advanced open-source, miniaturized FPGA UAS platform with dual DVS/Frame camera for cutting-edge low-latency autonomous algorithms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DOB-based wind estimation of a UAV using its onboard sensor.
<em>IROS</em>, 8126–8133. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unmanned Aerial Vehicles (UAVs) play a crucial role in meteorological research, particularly in environmental wind field measurements. However, several challenges exist in current wind measurement methods using UAVs that need to be addressed. Firstly, the accuracy of measurement is low, and the measurement range is limited. Secondly, the algorithms employed lack robustness and adaptability across different UAV platforms. Thirdly, there are limited approaches available for wind estimation during dynamic flight. Finally, while horizontal plane measurements are feasible, vertical direction estimation is often missing. To tackle these challenges, we present and implement a comprehensive wind estimation algorithm. Our algorithm offers several key features, including the capability to estimate the 3-D wind vector, enabling wind estimation even during dynamic flight of the UAV. Furthermore, our algorithm exhibits adaptability across various UAV platforms. Experimental results in the wind tunnel validate the effectiveness of our algorithm, showcasing improvements such as wind speed accuracy of 0.11 m/s and wind direction errors of less than 2.8°. Additionally, our approach extends the measurement range to 10 m/s.Wind Estimation, Disturbance Observer, Aerial Robotics, Field Robots},
  archive   = {C_IROS},
  author    = {Haowen Yu and Xianqi Liang and Ximin Lyu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801906},
  month     = {10},
  pages     = {8126-8133},
  title     = {DOB-based wind estimation of a UAV using its onboard sensor},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ROG-map: An efficient robocentric occupancy grid map for
large-scene and high-resolution LiDAR-based motion planning.
<em>IROS</em>, 8119–8125. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in LiDAR technology have opened up new possibilities for robotic navigation. Given the widespread use of occupancy grid maps (OGMs) in robotic motion planning, this paper aims to address the challenges of integrating LiDAR with OGMs. To this end, we propose ROG-Map, a uniform grid-based OGM that maintains a local map moving along with the robot to enable efficient map operation and reduce memory costs for large-scene autonomous flight. Moreover, we present a novel incremental obstacle inflation method that significantly reduces the computational cost of inflation. The proposed method outperforms state-of-the-art methods on various public datasets. To demonstrate the effectiveness and efficiency of ROG-Map, we integrate it into a complete quadrotor system and perform autonomous flights against both small obstacles and large-scale scenes. During real-world flight tests with a 0.05 m resolution local map and 30 m×30 m×6 m local map size, ROG-Map takes only 29.8 % of frame time on average to update the map at a frame rate of 50 Hz (i.e., 5.96 ms in 20 ms), including 0.33 % (i.e., 0.66 ms) to perform obstacle inflation, which represents only half of the total map updating time when compared to the state-of-the-art baseline. We release ROG-Map as an open-source ROS package1 to promote the development of LiDAR-based motion planning.},
  archive   = {C_IROS},
  author    = {Yunfan Ren and Yixi Cai and Fangcheng Zhu and Siqi Liang and Fu Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802303},
  month     = {10},
  pages     = {8119-8125},
  title     = {ROG-map: An efficient robocentric occupancy grid map for large-scene and high-resolution LiDAR-based motion planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online rotor fault detection and isolation for vertical
takeoff and landing vehicles. <em>IROS</em>, 8111–8118. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vertical take-off and landing (VTOL) vehicles are becoming increasingly popular for real-world transport; but, as with any vehicle, guaranteeing safety is both extremely critical and highly challenging due to issues like rotor faults. Existing fault detection and isolation (FDI) techniques usually focus on multirotor systems or fixed wing systems, rather than the hybrid VTOLs. Since VTOLs have both rotors and ailerons, a fault in a rotor may be masked by the (correctly working) ailerons, making it much more difficult to detect faults. However, this masking only works when ailersons are used (e.g., during cruising), leaving the takeoff and landing vulnerable to crashes.This paper presents an online rotor fault detection and isolation (FDI) method for VTOLs. The approach uses pose analysis and aileron command data to quickly and accurately identify the faulty rotor and to compute the severity of the fault. Our method works for hard-to-detect fault scenarios, such as small-severity faults that are masked during cruise flight but not during vertical motion. We evaluated our technique in a SITL PX4 simulation of a modified Deltaquad QuadPlane. The results show that our FDI technique can quickly detect and isolate faults in real time (within 1s-2.5s) and achieve high isolation success rate (91.67%) across six rotors, and that it can estimate the severity of faults to within 2%. When applying a simple recovery process post-isolation, the system consistently achieved safe landing.},
  archive   = {C_IROS},
  author    = {Jiaqi Lian and Neeraj Gandhi and Yifan Wang and Linh Thi Xuan Phan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802021},
  month     = {10},
  pages     = {8111-8118},
  title     = {Online rotor fault detection and isolation for vertical takeoff and landing vehicles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tactile odometry in aerial physical interaction.
<em>IROS</em>, 8103–8110. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Aerial robots are well-established technologies in environments characterized by reliable GNSS signals and favorable conditions for navigation based on cameras or LiDARs. However, their robustness is significantly challenged whenever ambient lighting is insufficient, GNSS signals are blocked, and range measurements are corrupted, for example, in underground, dark, or foggy environments. There, conventional navigation methods solely based on computer vision are very limited. This work proposes a completely novel approach to Aerial Tactile Odometry for pose estimation of aerial robots exploiting contact to precisely determine the system’s pose. By employing a compliant end-effector design with onboard tactile information by means of a trackball, we infer the complete UAV’s pose with respect to the environment, and the path traveled during contact. Through a large set of experiments, the proposed method shows centimeter-level accuracy for various relative orientations between the environment and the robot as well as for different trajectories. Akin to conventional dead-reckoning odometry methods in wheeled robotics, this method provides a valuable additional source of pose estimation, increasing the robustness of aerial robots – especially aerial manipulators – in the real world.},
  archive   = {C_IROS},
  author    = {Micha Schuster and Anton Bredenbeck and Michael Beitelschmidt and Salua Hamaza},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802497},
  month     = {10},
  pages     = {8103-8110},
  title     = {Tactile odometry in aerial physical interaction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data-driven system identification of quadrotors subject to
motor delays. <em>IROS</em>, 8095–8102. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently non-linear control methods like Model Predictive Control (MPC) and Reinforcement Learning (RL) have attracted increased interest in the quadrotor control community. In contrast to classic control methods like cascaded PID controllers, MPC and RL heavily rely on an accurate model of the system dynamics. The process of quadrotor system identification is notoriously tedious and is often pursued with additional equipment like a thrust stand. Furthermore, low-level details like motor delays which are crucial for accurate end-to-end control are often neglected. In this work, we introduce a data-driven method to identify a quadrotor’s inertia parameters, thrust curves, torque coefficients, and first-order motor delay purely based on proprioceptive data. The estimation of the motor delay is particularly challenging as usually, the RPMs can not be measured. We derive a Maximum A Posteriori (MAP)-based method to estimate the latent time constant. Our approach only requires about a minute of flying data that can be collected without any additional equipment and usually consists of three simple maneuvers. Experimental results demonstrate the ability of our method to accurately recover the parameters of multiple quadrotors. It also facilitates the deployment of RL-based, end-to-end quadrotor control of a large quadrotor under harsh, outdoor conditions.},
  archive   = {C_IROS},
  author    = {Jonas Eschmann and Dario Albani and Giuseppe Loianno},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801441},
  month     = {10},
  pages     = {8095-8102},
  title     = {Data-driven system identification of quadrotors subject to motor delays},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Autonomous power line tracking with mmWave radar.
<em>IROS</em>, 8088–8094. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work proposes a novel drone system designed to autonomously track and follow power lines and reconstruct them in 3D with a point cloud representation based on mmWave radar measurements. The system is composed of a GNSS-enabled quadrotor UAV equipped with a combined mmWave radar sensor and onboard compute module payload and has been designed to be small, lightweight, and low-cost. MmWave radar sensors offer great range and sensitivity in the task of power line detection with a high level of sparsity in the produced data when compared to traditional sensors such as LiDARs. The proposed system overcomes the radar sensor’s shortcomings by building up a point cloud representing the power line environment as the drone moves around in it. The built-up point cloud is analyzed using the onboard computer to detect the cables in the power line environment and to produce pose-estimates of each line. The system has been tested in a variety of scenarios and has been shown to be able to accurately detect and track power lines in varying weather conditions.},
  archive   = {C_IROS},
  author    = {Nicolaj Haarhøj Malle and Emad Ebeid},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802812},
  month     = {10},
  pages     = {8088-8094},
  title     = {Autonomous power line tracking with mmWave radar},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VRExplorer: An efficient view-region based autonomous
exploration method in unknown environments for UAV. <em>IROS</em>,
8081–8087. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous exploration plays a crucial role in robotics applications like rescue and scene reconstruction. This work addresses the challenges of autonomous exploration in intricate unknown environments by presenting a novel UAV autonomous exploration method based on a new concept of the view-region. Our proposed approach leverages the view-region to replace the conventional viewpoint generation and selection process, streamlining the planning process for exploration. Simultaneously, we model the problem of maximizing frontier coverage within the field of view during exploration, and jointly optimize it with the exploration path optimization problem. This approach ensures exploration path safety and effectiveness while being aggressive. Additionally, a gimbal is incorporated beneath the camera, with an associated optimization problem designed to minimize UAV self-rotation and enhance exploration efficiency. Simulations and real-world experiments demonstrate that the proposed method outperforms existing state-of-the-art methods in terms of runtime and distance traveled.},
  archive   = {C_IROS},
  author    = {Kai Xu and Lanxiang Zheng and Mingxin Wei and Hui Cheng},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802710},
  month     = {10},
  pages     = {8081-8087},
  title     = {VRExplorer: An efficient view-region based autonomous exploration method in unknown environments for UAV},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Time-varying control barrier function for safe and precise
landing of a UAV on a moving target. <em>IROS</em>, 8075–8080. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this article, we present a control barrier function (CBF)-based control strategy for safe and precise landing of an unmanned aerial vehicle (UAV) on a moving target. The CBF is time-varying, as it depends on the velocity of the landing platform and captures three crucial safety constraints: (a) collision avoidance with the landing platform, (b) precise vertical descent on a narrow landing platform, and (c) ground clearance throughout the landing maneuver. The proposed CBF’s parameters can be adjusted to set the desired width and height of the descending cone. A quadratic programbased CBF safety filter is designed, which takes a nominal position tracking control input and yields a minimally invasive control input that enforces the safety constraints throughout the landing maneuver. The controller’s feasibility is analyzed and its performance is validated through multiple experiments using a quadrotor UAV and an unmanned ground vehicle.},
  archive   = {C_IROS},
  author    = {Viswa Narayanan Sankaranarayanan and Akshit Saradagi and Sumeet Satpute and George Nikolakopoulos},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802827},
  month     = {10},
  pages     = {8075-8080},
  title     = {Time-varying control barrier function for safe and precise landing of a UAV on a moving target},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reconfigurable multi-rotor for high-precision physical
interaction. <em>IROS</em>, 8069–8074. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unmanned aerial vehicles (UAVs) for contact-based tasks at height can greatly improve the safety of the human workers involved. However, performing contact-based tasks with typical under-actuated UAVs is non-trivial. Due to their coupled translational and rotational dynamics and their limited station-keeping performance under physical disturbances, it is difficult to maintain precise and consistent contact. We address these problems in the context of physical interaction with vertical, cylindrical target objects, such as trees. We present a novel UAV design with a pair of tilt-rotors and a landing gear that can reconfigure into a front-mounted, two-fingered gripper. While the tilt-rotors provide horizontal force toward the target object without pitching the UAV forward, the reconfigurable landing gear enables the UAV to obtain support from the target object. Such support results in an approximately 80% improvement in position- and heading-keeping performance. Moreover, the landing gear is designed as a cable-driven under-actuated system, which requires only one actuator to control both the reconfiguration and the grasping (i.e., five degrees of freedom in total). Such a minimalist design helps keep the UAV power consumption for interactions low. This marks progress towards safe, high-precision physical interaction against vertical, cylindrical target objects. Our UAV in action: https://youtu.be/D-65vldox_A.},
  archive   = {C_IROS},
  author    = {Joshua Taylor and Nursultan Imanberdiyev and Meng Yee Michael Chuah and Wei-Yun Yau and Guillaume Sartoretti and Efe Camci},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802586},
  month     = {10},
  pages     = {8069-8074},
  title     = {Reconfigurable multi-rotor for high-precision physical interaction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Streamlining forest wildfire surveillance: AI-enhanced UAVs
utilizing the FLAME aerial video dataset for lightweight and efficient
monitoring. <em>IROS</em>, 8063–8068. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, unmanned aerial vehicles (UAVs) have played an increasingly crucial role in supporting disaster emergency response efforts by analyzing aerial images. While current deep-learning models focus on improving accuracy, they often overlook the limited computing resources of UAVs. This study recognizes the imperative for real-time data processing in disaster response scenarios and introduces a lightweight and efficient approach for aerial video understanding. Our methodology identifies redundant portions within the video through policy networks and eliminates this excess information using frame compression techniques. Additionally, we introduced the concept of a station point, which leverages future information in the sequential policy network, thereby enhancing accuracy. To validate our method, we employed the wildfire FLAME dataset. Compared to the baseline, our approach reduces computation costs by more than 10 times while improving accuracy by 3%. Moreover, our method can intelligently select salient frames from the video, refining the dataset. This feature enables sophisticated models to be effectively trained on a smaller dataset, significantly reducing the time spent during the training process.},
  archive   = {C_IROS},
  author    = {Lemeng Zhao and Junjie Hu and Jianchao Bi and Yanbing Bai and Erick Mas and Shunichi Koshimura},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801629},
  month     = {10},
  pages     = {8063-8068},
  title     = {Streamlining forest wildfire surveillance: AI-enhanced UAVs utilizing the FLAME aerial video dataset for lightweight and efficient monitoring},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Hierarchical search-based cooperative motion planning.
<em>IROS</em>, 8055–8062. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cooperative path planning, a crucial aspect of multi-agent systems research, serves a variety of sectors, including military, agriculture, and industry. Many existing algorithms, however, come with certain limitations, such as simplified kinematic models and inadequate support for multiple group scenarios. Focusing on the planning problem associated with a nonholonomic Ackermann model for Unmanned Ground Vehicles (UGV), we propose a leaderless, hierarchical Search-Based Cooperative Motion Planning (SCMP) method. The high-level utilizes a binary conflict search tree to minimize runtime, while the low-level fabricates kinematically feasible, collision-free paths that are shape-constrained. Our algorithm can adapt to scenarios featuring multiple groups with different shapes, outlier agents, and elaborate obstacles. We conduct algorithm comparisons, performance testing, simulation, and real-world testing, verifying the effectiveness and applicability of our algorithm. The implementation of our method will be open-sourced at https://github.com/WYCUniverStar/SCMP.},
  archive   = {C_IROS},
  author    = {Yuchen Wu and Yifan Yang and Gang Xu and Junjie Cao and Yansong Chen and Licheng Wen and Yong Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801442},
  month     = {10},
  pages     = {8055-8062},
  title     = {Hierarchical search-based cooperative motion planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ensembling prioritized hybrid policies for multi-agent
pathfinding. <em>IROS</em>, 8047–8054. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-Agent Reinforcement Learning (MARL) based Multi-Agent Path Finding (MAPF) has recently gained attention due to its efficiency and scalability. Several MARL-MAPF methods choose to use communication to enrich the information one agent can perceive. However, existing works still struggle in structured environments with high obstacle density and a high number of agents. To further improve the performance of the communication-based MARL-MAPF solvers, we propose a new method, Ensembling Prioritized Hybrid Policies (EPH). We first propose a selective communication block to gather richer information for better agent coordination within multi-agent environments and train the model with a Q-learning-based algorithm. We further introduce three advanced inference strategies aimed at bolstering performance during the execution phase. First, we hybridize the neural policy with single-agent expert guidance for navigating conflict-free zones. Secondly, we propose Q value-based methods for prioritized resolution of conflicts as well as deadlock situations. Finally, we introduce a robust ensemble method that can efficiently collect the best out of multiple possible solutions. We empirically evaluate EPH in complex multi-agent environments and demonstrate competitive performance against state-of-the-art neural methods for MAPF. We open-source our code at https://github.com/ai4co/eph-mapf.},
  archive   = {C_IROS},
  author    = {Huijie Tang and Federico Berto and Jinkyoo Park},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801914},
  month     = {10},
  pages     = {8047-8054},
  title     = {Ensembling prioritized hybrid policies for multi-agent pathfinding},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Mastering scene rearrangement with expert-assisted
curriculum learning and adaptive trade-off tree-search. <em>IROS</em>,
8039–8046. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Scene Rearrangement Planning (SRP) has recently emerged as a crucial interior scene task; however, current approaches still face two primary issues. First, prior works define the action space of SRP using handcrafted coarse-grained actions, which are inflexible for scene arrangement transition and impractical for real-world deployment. Secondly, the scarcity of realistic indoor scene rearrangement data hinders popular data-hungry learning approaches and quantitative evaluation. To tackle these issues, we propose a fine-grained action space definition and curate a large-scale scene rearrangement dataset to facilitate the training of learning approaches and comprehensive benchmarking. Building upon this dataset, we introduce a novel framework, PLATO, designed for efficient agent training and inference. Our approach features an exPert-assisted curriculum Learning (PL) paradigm that possesses a Behavior Cloning (BC) and an offline Reinforcement Learning (RL) curriculum for agent training, along with an advanced tree-search-based planner enhanced by an Adaptive Trade-Off (ATO) strategy to improve expert agent performance further. We demonstrate the superior performance of our method over baseline agents through extensive experiments and provide a detailed analysis to elucidate its rationale. Our project website can be accessed at plato.github.io.},
  archive   = {C_IROS},
  author    = {Zan Wang and Hanqing Wang and Wei Liang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802526},
  month     = {10},
  pages     = {8039-8046},
  title     = {Mastering scene rearrangement with expert-assisted curriculum learning and adaptive trade-off tree-search},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CASRL: Collision avoidance with spiking reinforcement
learning among dynamic, decision-making agents. <em>IROS</em>,
8031–8038. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Developing an efficient collision avoidance policy with Spiking Reinforcement Learning for dynamic, decision-making agents remains challenging. Moreover, the implementation of energy-efficient collision avoidance is important for mobile robots that operate with limited on-board computing resources. Most existing energy-efficient methods via spiking reinforcement learning are predominately concerned with the navigational capabilities of a single agent, and are unable to handle a large, and possibly varying number of agents. To overcome these limitations, we propose a model called collision avoidance with spiking reinforcement learning (CASRL), based on proximal policy optimization algorithms. This proposed model consists of an actor with spiking neural networks (SNNs) and a critic with deep neural networks (DNNs). Our spiking reinforcement learning algorithm is advantageous to handle an arbitrary number of other agents by virtue of a spiking-gated transformer (SpikeGTr) architecture and an accumulate-to-fire (ATF) module. Extensive experimental results demonstrate that CASRL obtains a competitive success rate of navigation and exhibits higher time-efficiency for navigation in crowded scenarios compared to traditional DNN-based methods.},
  archive   = {C_IROS},
  author    = {Chengjun Zhang and Ka-Wa Yip and Bo Yang and Zhiyong Zhang and Mengwen Yuan and Rui Yan and Huajin Tang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802416},
  month     = {10},
  pages     = {8031-8038},
  title     = {CASRL: Collision avoidance with spiking reinforcement learning among dynamic, decision-making agents},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A decentralized partially observable markov decision process
for dynamic obstacle avoidance and complete area coverage using multiple
reconfigurable robots. <em>IROS</em>, 8023–8030. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Achieving complete area coverage in robotics is an essential aspect for applications such as cleaning and patrolling. While multi-agent frameworks have been implemented to address the challenge of complete coverage, the area coverage performances are hindered by physical constraints and dynamic obstacles that cause inaccessibility to certain areas of the environment. Reconfigurable robots have been adopted to mitigate this issue as the independent alteration of the morphologies during deployments enables overcoming tight spaces to access obstructed areas. Hence, this paper proposes a Multi-Agent Reinforcement Learning (MARL) framework leveraging the Decentralized Partially Observable Markov Decision Process (Dec-POMDP) to enable a team of reconfigurable robots to achieve complete coverage under the presence of dynamic obstacles. The framework is modelled to allow the robots to coordinate and plan their motions effectively while using shape adaptability to access narrow spaces while avoiding dynamic obstacles. Experimental results demonstrated the framework’s ability to be generalised even when scaled up to a different number of agents across larger environments.},
  archive   = {C_IROS},
  author    = {J. J. J. Pey and S. M. Bhagya P. Samarakoon and M. A. Viraj J. Muthugala and Mohan Rajesh Elara},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801677},
  month     = {10},
  pages     = {8023-8030},
  title     = {A decentralized partially observable markov decision process for dynamic obstacle avoidance and complete area coverage using multiple reconfigurable robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Path re-planning with stochastic obstacle modeling: A monte
carlo tree search approach. <em>IROS</em>, 8017–8022. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Path re-planning and repairing are key topics for robust planning and navigation in open dynamic environments, finding applications in various domains such as fleet control of Unmanned Ground Vehicles (UGVs) in warehouses. The use of UGVs in open and dynamic environments requires flexible cooperation between human operators and the UGV fleet within a shared environment. In this paper, we propose a local strategy to re-plan the path of robots encountering unexpected and dynamic obstacles. Specifically, starting from a given Multi-Agent path, we model the re-planning problem as a Markov Decision Process (MDP) considering a stochastic obstacle lifespan, and we propose two local approaches based on Monte-Carlo Tree Search to re-plan the path of the robots that encounter obstacles. We compare these approaches with traditional Multi-Agent Path Finding (MAPF) algorithms to obtain new collision-free paths when an obstacle is detected. The evaluation is performed in simulation using benchmarking instances of warehouses and experimentally in a research facility with a scaled-down Industry 4.0 production line.},
  archive   = {C_IROS},
  author    = {Francesco Trotti and Alessandro Farinelli and Riccardo Muradore},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802254},
  month     = {10},
  pages     = {8017-8022},
  title     = {Path re-planning with stochastic obstacle modeling: A monte carlo tree search approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual-process optimization for multi-vehicle route planning
and parts collection sequencing. <em>IROS</em>, 8009–8016. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We proposed a novel dual-process optimization approach for parts collection order and route planning in parts warehouses. Conventional multi-agent parts collection typically uses the vehicle routing problem (VRP), which focuses on minimizing the number of agents and costs. However, the model does not fully leverage the vehicle’s potential. Moreover, multi-agent path finding (MAPF) focuses on route planning and avoiding path conflicts, ignoring the order of part collection. The proposed approach integrates algorithms from the traveling salesman problem (TSP) and path planning, and modifies them to suit the dynamic and complex environment of parts warehouses. This integration streamlines the collection process and considerably reduces the operational time. Thus, the study can improve automation and efficiency in parts warehouse management and improve optimization techniques. The proposed method achieved more than tenfold acceleration compared with the ideal centralized optimization, without cost increments. As the number of agents and part collections increases, centralized optimization requires a metaheuristic approach, which results in solution degradation. However, the proposed approach maintains over tenfold acceleration and produces solutions with shorter operational times. Furthermore, we conducted an ablation study comparing six methods, from entirely independent to centralized optimization, demonstrating that the proposed approach effectively balances computational time and solution accuracy.},
  archive   = {C_IROS},
  author    = {Ryota Higa and Takuro Kato and Florence Ho},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802316},
  month     = {10},
  pages     = {8009-8016},
  title     = {Dual-process optimization for multi-vehicle route planning and parts collection sequencing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bird’s-eye-view trajectory planning of multiple robots using
continuous deep reinforcement learning and model predictive control.
<em>IROS</em>, 8002–8008. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Efficient motion planning and control for multiple mobile robots in industrial automation and indoor logistics face challenges such as trajectory generation and collision avoidance in complex environments. We propose a hybrid, sequential method combining Bird’s-Eye-View vision-based continuous Deep Reinforcement Learning (DRL) with Model Predictive Control (MPC). DRL generates candidate trajectories in complex environments, while MPC refines these trajectories to ensure adherence to kinematic and dynamic constraints of the robot, as well as constraints modeling humans’ current and predicted future positions. In this study, the DRL utilizes a Deep Deterministic Policy Gradient model for trajectory generation, demonstrating its capability to navigate non-convex obstacles, a task that might pose challenges for MPC. We demonstrate that the proposed hybrid DRL-MPC model performs favorably in handling new scenarios, computational efficiency, time to destination, and adaptability to complex multi-robot situations when compared to pure DRL or pure MPC approaches.},
  archive   = {C_IROS},
  author    = {Kristian Ceder and Ze Zhang and Adam Burman and Ilya Kuangaliyev and Krister Mattsson and Gabriel Nyman and Arvid Petersén and Lukas Wisell and Knut Åkesson},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801434},
  month     = {10},
  pages     = {8002-8008},
  title     = {Bird’s-eye-view trajectory planning of multiple robots using continuous deep reinforcement learning and model predictive control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal and bounded suboptimal any-angle multi-agent
pathfinding. <em>IROS</em>, 7996–8001. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent pathfinding (MAPF) is the problem of finding a set of conflict-free paths for a set of agents. Typically, the agents&#39; moves are limited to a pre-defined graph of possible locations and allowed transitions between them, e.g. a 4-neighborhood grid. We explore how to solve MAPF problems when each agent can move between any pair of possible locations as long as traversing the line segment connecting them does not lead to a collision with the obstacles. This is known as any-angle pathfinding. We present the first optimal any-angle multi-agent pathfinding algorithm. Our planner is based on the Continuous Conflict-based Search (CCBS) algorithm and an optimal any-angle variant of the Safe Interval Path Planning (TO-AA-SIPP). The straightforward combination of those, however, scales poorly since any-angle path finding induces search trees with a very large branching factor. To mitigate this, we adapt two techniques from classical MAPF to the any-angle setting, namely Disjoint Splitting and Multi-Constraints. Experimental results on different combinations of these techniques show they enable solving over 30% more problems than the vanilla combination of CCBS and TO-AA-SIPP. In addition, we present a bounded-suboptimal variant of our algorithm, that enables trading runtime for solution cost in a controlled manner.},
  archive   = {C_IROS},
  author    = {Konstantin Yakovlev and Anton Andreychuk and Roni Stern},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801691},
  month     = {10},
  pages     = {7996-8001},
  title     = {Optimal and bounded suboptimal any-angle multi-agent pathfinding},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-agent teamwise cooperative path finding and traffic
intersection coordination. <em>IROS</em>, 7990–7995. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When coordinating the motion of connected autonomous vehicles at a signal-free intersection, the vehicles from each direction naturally forms a team and each team seeks to minimize their own traversal time through the intersection, without concerning the traversal times of other teams. Since the intersection is shared by all teams and agent-agent collision must be avoided, the coordination has to trade the traversal time of one team for the other. This paper thus investigates a problem called Multi-Agent Teamwise Cooperative Path Finding (TCPF), which seeks a set of collision-free paths for the agents from their respective start to goal locations, and agents are grouped into multiple teams with each team having its own objective function to optimize. In general, there are more than one teams and hence multiple objectives. TCPF thus seeks the Pareto-optimal front that represents possible trade-offs among the teams. We develop a centralized planner for TCPF by leveraging the Multi-Agent Path Finding techniques to resolve agent-agent collision, and Multi-Objective Optimization to find Pareto-optimal solutions. We analyze the completeness and optimality of the planner, which is then tested in various settings with up to 40 agents to verify the runtime efficiency and showcase the usage in intersection coordination.},
  archive   = {C_IROS},
  author    = {Zhongqiang Ren and Yilin Cai and Hesheng Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802803},
  month     = {10},
  pages     = {7990-7995},
  title     = {Multi-agent teamwise cooperative path finding and traffic intersection coordination},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). P4: Pruning and prediction-based priority planning.
<em>IROS</em>, 7982–7989. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, multi-agent path finding (MAPF) has attracted widespread attention in the fields of artificial intelligence and robotics. Its main goal is to find paths for multiple agents, each having specified start and end locations on a grid, without collisions, while minimizing the total travel time. In this work, we present a new algorithm called P4 (Pruning and Prediction-based Priority Planning), designed to accommodate large numbers of agents with enhanced scalability. The P4 method combines three key components: Point-to-Point (PnP) algorithm, dynamic window approach, and path direction prediction. In this way we reduce the search space and increase the speed of the computation. Our experiments show that P4 consistently achieves shorter execution times and produces solutions that are close to optimal. For example, for 200 agents and real map orz900d, P4 is 4× faster than optimal algorithm CBSH while the sum of delays is within 15% of optimal. The P4 method outperforms other existing suboptimal methods in both performance and solution quality. We also show that our approach exceeds existing methods in success rate under time constraints. As time limit is increased from 0.1 to 100 seconds, success rate of P4 increases from 50% to 100%. On the other hand, the success rate for alternative sub-optimal methods is less than that of P4.},
  archive   = {C_IROS},
  author    = {Rui Yang and Rajiv Gupta},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802763},
  month     = {10},
  pages     = {7982-7989},
  title     = {P4: Pruning and prediction-based priority planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Blending distributed NeRFs with tri-stage robust pose
optimization. <em>IROS</em>, 7975–7981. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Due to the limited model capacity, leveraging distributed Neural Radiance Fields (NeRFs) for modeling extensive urban environments has become a necessity. However, current distributed NeRF registration approaches encounter aliasing artifacts, arising from discrepancies in rendering resolutions and suboptimal pose precision. These factors collectively deteriorate the fidelity of pose estimation within NeRF frameworks, resulting in occlusion artifacts during the NeRF blending stage. In this paper, we present a distributed NeRF system with tri-stage pose optimization. In the first stage, precise poses of images are achieved by bundle adjusting Mip-NeRF 360 with a coarse-to-fine strategy. In the second stage, we incorporate the inverting Mip-NeRF 360, coupled with the truncated dynamic low-pass filter, to enable the achievement of robust and precise poses, termed Frame2Model optimization. On top of this, we obtain a coarse transformation between NeRFs in different coordinate systems. In the third stage, we fine-tune the transformation between NeRFs by Model2Model pose optimization. After obtaining precise transformation parameters, we proceed to implement NeRF blending, showcasing superior performance metrics in both real-world and simulation scenarios. Codes and data will be publicly available at https://github.com/boilcy/Distributed-NeRF.},
  archive   = {C_IROS},
  author    = {Baijun Ye and Caiyun Liu and Xiaoyu Ye and Yuantao Chen and Yuhai Wang and Zike Yan and Yongliang Shi and Hao Zhao and Guyue Zhou},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802232},
  month     = {10},
  pages     = {7975-7981},
  title     = {Blending distributed NeRFs with tri-stage robust pose optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CollabLoc: Collaborative information sharing for real-time
multiuser visual localization system. <em>IROS</em>, 7967–7974. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents CollabLoc, a novel approach for real-time multi-user visual localization. Typically, localization systems employ a client-server design for locating cameras. In these systems, lightweight simultaneous localization and mapping computations are performed on the client side, while the server handles intensive localization tasks. This approach harnesses the complementary capabilities of the client and server, resulting in accurate, real-time localization results. However, existing architectures primarily operate on a one-to-one client-server structure, limiting their scalability and multi-user capabilities. Therefore, CollabLoc is designed to accommodate multiple clients through collaborative information sharing to considerably reduce computational overhead and enhance overall efficiency and accuracy. We propose a tracking confidence module that evaluates the tracking quality of individual clients and plays a pivotal role in prioritizing client requests by the server-side algorithm. On the server, we utilize fused poses to accelerate image retrieval. Moreover, we enhance the efficiency of optical flow estimation by employing a simplified feature extraction module and leveraging spatial similarities among neighboring clients to improve its performance. Finally, via the Pose Fusion Module, the server can periodically adjust fused poses to mitigate accumulated errors. Experimental results indicate that compared with a baseline method, CollabLoc improves positioning efficiency by nearly twice and achieves higher accuracy in multi-user scenarios.},
  archive   = {C_IROS},
  author    = {Teng-Te Yu and Yo-Chung Lau and Kai-Li Wang and Kuan-Wen Chen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801941},
  month     = {10},
  pages     = {7967-7974},
  title     = {CollabLoc: Collaborative information sharing for real-time multiuser visual localization system},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards the new generation of smart home-care with
cloud-based internet of humans and robotic things. <em>IROS</em>,
7959–7966. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The burgeoning demand for home-care services, driven by a rapidly aging global population, necessitates innovative solutions to alleviate the burden on caregivers and enhance care quality. This paper introduces the development of an Inter-net of Human and Robotic Things (IoHRT) framework, which synergizes cloud computing and the Internet of Robotic Things (IoRT) with human-robot collaborative control mechanisms for home-care applications. The IoHRT framework is designed to enable the seamless integration of customizable robotic platforms with modular, scalable, and compatible features, thereby creating a dynamic and adaptable home-care ecosystem. By leveraging the scalability and computational power of cloud computing, the framework facilitates real-time data analysis and remote monitoring, thus enhancing the efficiency and effectiveness of home-care. We present an in-depth analysis of the key characteristics of IoHRT, supported by evidence embedded in our design, and conduct user studies to evaluate the framework from users’ perspectives. We demonstrate the performance and utility of our proposed framework for the future of home-care applications.},
  archive   = {C_IROS},
  author    = {Dandan Zhang and Jin Zheng},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802711},
  month     = {10},
  pages     = {7959-7966},
  title     = {Towards the new generation of smart home-care with cloud-based internet of humans and robotic things},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A cooperative recovery framework for resilient multi-robot
swarm operations under loss of localization in unknown environments.
<em>IROS</em>, 7952–7958. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Localization is one of the most important tasks for mobile robot operations. Without such capability, a robot may wander toward unsafe states and never complete a desired task. Such capability is even more important in multi-robot system (MRS) operations in which their motion is coordinated based on consensus schemes that leverage information from surrounding neighbors. Thus, in the event of compromised or malfunctioning on-board positioning sensing (e.g., due to cyber attacks or faults) on individual robots, the entire robotic system may be hijacked toward undesired states. In this work, we target this problem by proposing a decentralized framework where: i) robots with loss of localization capabilities detect the anomalous behavior then generate a notification signal within information exchanges to alert neighboring robots, and ii) neighboring robots leverage their mobility to aid in recovery allowing compromised robots to re-localize. Our framework is validated in simulations and lab experiments on proximity-based formations of homogeneous unmanned multi-robot swarms.},
  archive   = {C_IROS},
  author    = {Paul J Bonczek and Nicola Bezzo},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801367},
  month     = {10},
  pages     = {7952-7958},
  title     = {A cooperative recovery framework for resilient multi-robot swarm operations under loss of localization in unknown environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A collaborative stereo camera with two UAVs for
long-distance mapping of urban buildings. <em>IROS</em>, 7944–7951. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For a swarm of Unmanned Aerial Vehicle (UAV), long-distance visual mapping is advantageous for pre-planning navigation paths in unknown urban building environments. This work leverages two cameras on two UAVs to build a wide-baseline collaborative stereo camera, which can construct a navigable mesh map for remote building obstacles. We present a complete framework of the collaborative stereo camera for long-distance mapping, including online extrinsic parameter estimation of the stereo camera, real-time cross-camera feature association, and semantic mesh map generation of remote buildings. Extensive simulations and real-world experiments verify the effectiveness of the collaborative stereo camera. With a 3m baseline, the collaborative stereo camera achieves long-distance mapping of buildings (20m ~ 50m) away with a relative error of approximately 10%. The constructed remote map enables UAVs to pre-detect large obstacles and pre-plan navigation paths in large-scale building environments. Hopefully, this work can provide a novel and practical approach for collaborative visual tasks of UAV swarm.Video - https://youtu.be/a0kj-1zb6KI},
  archive   = {C_IROS},
  author    = {Zhaoying Wang and Wei Dong},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801758},
  month     = {10},
  pages     = {7944-7951},
  title     = {A collaborative stereo camera with two UAVs for long-distance mapping of urban buildings},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). AVM-SLAM: Semantic visual SLAM with multi-sensor fusion in
a bird’s eye view for automated valet parking. <em>IROS</em>, 7937–7943.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate localization in challenging garage environments—marked by poor lighting, sparse textures, repetitive structures, dynamic scenes, and the absence of GPS—is crucial for automated valet parking (AVP) tasks. Addressing these challenges, our research introduces AVM-SLAM, a cutting-edge semantic visual SLAM architecture with multi-sensor fusion in a bird’s eye view (BEV). This novel framework synergizes the capabilities of four fisheye cameras, wheel encoders, and an inertial measurement unit (IMU) to construct a robust SLAM system. Unique to our approach is the implementation of a flare removal technique within the BEV imagery, significantly enhancing road marking detection and semantic feature extraction by convolutional neural networks for superior mapping and localization. Our work also pioneers a semantic prequalification (SPQ) module, designed to adeptly handle the challenges posed by environments with repetitive textures, thereby enhancing loop detection and system robustness. To demonstrate the effectiveness and resilience of AVM-SLAM, we have released a specialized multi-sensor and high-resolution dataset of an underground garage, accessible at https://yale-cv.github.io/avm-slamdataset, encouraging further exploration and validation of our approach within similar settings.},
  archive   = {C_IROS},
  author    = {Ye Li and Wenchao Yang and Dekun Lin and Qianlei Wang and Zhe Cui and Xiaolin Qin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802668},
  month     = {10},
  pages     = {7937-7943},
  title     = {AVM-SLAM: Semantic visual SLAM with multi-sensor fusion in a bird’s eye view for automated valet parking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ternary-type opacity and hybrid odometry for RGB NeRF-SLAM.
<em>IROS</em>, 7929–7936. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we address the challenge of deploying Neural Radiance Field (NeRFs) in Simultaneous Localization and Mapping (SLAM) under the condition of lacking depth information, relying solely on RGB inputs. The key to unlocking the full potential of NeRF in such a challenging context lies in the integration of real-world priors. A crucial prior we explore is the binary opacity prior of 3D space with opaque objects. To effectively incorporate this prior into the NeRF framework, we introduce a ternary-type opacity (TT) model instead, which categorizes points on a ray intersecting a surface into three regions: before, on, and behind the surface. This enables a more accurate rendering of depth, subsequently improving the performance of image warping techniques. Therefore, we further propose a novel hybrid odometry (HO) scheme that merges bundle adjustment and warping-based localization. Our integrated approach of TT and HO achieves state-of-the-art performance on synthetic and real-world datasets, in terms of both speed and accuracy. This breakthrough underscores the potential of NeRF-SLAM in navigating complex environments with high fidelity.},
  archive   = {C_IROS},
  author    = {Junru Lin and Asen Nachkov and Songyou Peng and Luc Van Gool and Danda Pani Paudel},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802493},
  month     = {10},
  pages     = {7929-7936},
  title     = {Ternary-type opacity and hybrid odometry for RGB NeRF-SLAM},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GV-bench: Benchmarking local feature matching for geometric
verification of long-term loop closure detection. <em>IROS</em>,
7922–7928. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual loop closure detection is an important module in visual simultaneous localization and mapping (SLAM), which associates current camera observation with previously visited places. Loop closures correct drifts in trajectory estimation to build a globally consistent map. However, a false loop closure can be fatal, so verification is required as an additional step to ensure robustness by rejecting the false positive loops. Geometric verification has been a well-acknowledged solution that leverages spatial clues provided by local feature matching to find true positives. Existing feature matching methods focus on homography and pose estimation in long-term visual localization, lacking references for geometric verification. To fill the gap, this paper proposes a unified benchmark targeting geometric verification of loop closure detection under long-term conditional variations. Furthermore, we evaluate six representative local feature matching methods (handcrafted and learning-based) under the benchmark, with in-depth analysis for limitations and future directions.},
  archive   = {C_IROS},
  author    = {Jingwen Yu and Hanjing Ye and Jianhao Jiao and Ping Tan and Hong Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801481},
  month     = {10},
  pages     = {7922-7928},
  title     = {GV-bench: Benchmarking local feature matching for geometric verification of long-term loop closure detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). CurricularVPR: Curricular contrastive loss for visual place
recognition. <em>IROS</em>, 7916–7921. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual Place Recognition (VPR) techniques commonly utilize Contrastive Losses (CL) to train models that generate compact and discriminative global descriptors for images. These models often result in poor performance due to one of the following reasons during training: 1) loss functions that focus primarily on easier samples, 2) reliance on time-consuming hard sample mining methods to identify informative supervisory samples, which hinders effective learning from large-scale datasets. To enhance both learning efficiency and effectiveness, we propose a Curricular Contrastive Loss (CCL) and use graded similarity labels as a measure of sample difficulty. Inspired by human learning that begin with easier concepts and progressively tackle more challenging ones, our CCL dynamically emphasizes easier samples during the initial training stages to achieve rapid convergence. The learning gradually focuses on harder samples in later training stages to bolster robustness of the models under challenging conditions. Our proposed method has been extensively evaluated on popular datasets, and the results demonstrate its superior performance compared to the CL and Generalized CL functions.},
  archive   = {C_IROS},
  author    = {Dongshuo Zhang and Nanhua Chen and Meiqing Wu and Siew-Kei Lam},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802279},
  month     = {10},
  pages     = {7916-7921},
  title     = {CurricularVPR: Curricular contrastive loss for visual place recognition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). C3P-VoxelMap: Compact, cumulative and coalescible
probabilistic voxel mapping. <em>IROS</em>, 7908–7915. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents a compact, cumulative, and coalescible probabilistic voxel mapping method to enhance performance, accuracy, and memory efficiency in LiDAR odometry. Probabilistic voxel mapping requires storing past point clouds and re-iterating them to update the uncertainty at every iteration, which consumes large memory space and CPU cycles. To solve this problem, we propose a two-fold strategy. First, we introduce a compact point-free representation for probabilistic voxels and derive a cumulative update of the planar uncertainty without caching original point clouds. Our voxel structure only keeps track of a predetermined set of statistics for points that lie inside it. This method reduces the runtime complexity from O(MN) to O(N) and the space complexity from O(N) to O(1) where M is the number of iterations and N is the number of points. Second, to further minimize memory usage and enhance mapping accuracy, we provide a strategy to dynamically merge voxels associated with the same physical planes by taking advantage of the geometric features in the real world. Rather than constantly scanning for these coalescible voxels at every iteration, our merging strategy accumulates voxels in a locality-sensitive hash and triggers merging lazily. On-demand merging reduces memory footprint with minimal computational overhead and improves localization accuracy thanks to cross-voxel denoising. Experiments exhibit 20% higher accuracy, 20% faster performance, and 70% lower memory consumption than the state-of-the-art.},
  archive   = {C_IROS},
  author    = {Xu Yang and Wenhao Li and Qijie Ge and Lulu Suo and Weijie Tang and Zhengyu Wei and Longxiang Huang and Bo Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801418},
  month     = {10},
  pages     = {7908-7915},
  title     = {C3P-VoxelMap: Compact, cumulative and coalescible probabilistic voxel mapping},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantic SLAM fusing moving constraint for dynamic objects
under indoor environments. <em>IROS</em>, 7900–7907. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simultaneous Localization and Mapping (SLAM) technology is a rapidly developing field in robotics. Most existing SLAM algorithms lack robustness in dynamic environments because moving objects can influence mapping and localization accuracy, making it challenging for robots to identify moving objects and understand their surroundings. Though some works proposed semantic SLAM methods, they rely on point-based feature extraction and matching algorithms with semantic information and simply exclude dynamic objects. In this work, we proposed real-time RGB-D SLAM to combine point, line, and plane features with object detection to increase the robustness in dynamic environments. The proposed method combines object detection, feature points, and lines to identify moving objects and uses feature planes and semantic information to identify constrained moving objects. Thus, the localization accuracy can be improved under dynamic environments by excluding or using dynamic objects. Experiments were conducted on a public TUM dataset and in a real-world environment. The result shows that the proposed SLAM algorithm can increase the dynamic object detection speed and the robustness of SLAM performance compared to state-of-the-art in dynamic environments.},
  archive   = {C_IROS},
  author    = {Zhenyuan Yang and W. K. R. Sachinthana and S. M. Bhagya P. Samarakoon and Mohan Rajesh Elara},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802524},
  month     = {10},
  pages     = {7900-7907},
  title     = {Semantic SLAM fusing moving constraint for dynamic objects under indoor environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SDPL-SLAM: Introducing lines in dynamic visual SLAM and
multi-object tracking. <em>IROS</em>, 7893–7899. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The need for a robust visual SLAM system operating in real human environments has led to the gradual abandonment of the static world assumption and to the creation of many dynamic SLAM algorithms. Even though there have been many dynamic SLAM proposals, the vast majority of them relied on point features. However, research in static SLAM systems has demonstrated that the use of more complex geometric shapes such as lines can improve performance. Motivated by this we have created a new dynamic SLAM system that estimates the camera poses and the motion of rigid objects, by exploiting both static and dynamic points and lines. Line segments have been incorporated in a novel way in every aspect of our algorithm, by improving their correspondences through optical flow refinement, and by introducing line error terms in both camera and object motion, and in batch optimization. Our proposal has been tested extensively in indoor and outdoor datasets and has achieved significant improvement compared to other state-of-the-art dynamic SLAM systems. Our results demonstrated that line segments enhanced the robustness, thus contributing towards a fully operational SLAM system.Code is publicly available*.},
  archive   = {C_IROS},
  author    = {Argyris Manetas and Panagiotis Mermigkas and Petros Maragos},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802140},
  month     = {10},
  pages     = {7893-7899},
  title     = {SDPL-SLAM: Introducing lines in dynamic visual SLAM and multi-object tracking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MCGMapper: Light-weight incremental structure from motion
and visual localization with planar markers and camera groups.
<em>IROS</em>, 7885–7892. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Structure from Motion (SfM) and visual localization in indoor texture-less scenes and industrial scenarios present prevalent yet challenging research topics. Existing SfM methods designed for natural scenes typically yield low accuracy or map-building failures due to insufficient robust feature extraction in such settings. Visual markers, with their artificially designed features, can effectively address these issues. Nonetheless, existing marker-assisted SfM methods encounter problems like slow running speed and difficulties in convergence; and also, they are governed by the strong assumption of unique marker size. In this paper, we propose a novel SfM framework that utilizes planar markers and multiple cameras with known extrinsics to capture the surrounding environment and reconstruct the marker map. In our algorithm, the initial poses of markers and cameras are calculated with Perspective-n-Points (PnP) in the front-end, while bundle adjustment methods customized for markers and camera groups are designed in the back-end to optimize the 6-DOF pose directly. Our algorithm facilitates the reconstruction of large scenes with different marker sizes, and its accuracy and speed of map building are shown to surpass existing methods. Our approach is suitable for a wide range of scenarios, including laboratories, basements, warehouses, and other industrial settings. Furthermore, we incorporate representative scenarios into simulations and also supply our datasets with pose labels to address the scarcity of quantitative ground-truth datasets in this research field. The datasets and source code are available on GitHub1.},
  archive   = {C_IROS},
  author    = {Yusen Xie and Zhenmin Huang and Kai Chen and Lei Zhu and Jun Ma},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802612},
  month     = {10},
  pages     = {7885-7892},
  title     = {MCGMapper: Light-weight incremental structure from motion and visual localization with planar markers and camera groups},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sharing attention mechanism in v-SLAM: Relative pose
estimation with messenger tokens on small datasets. <em>IROS</em>,
7878–7884. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In V-SLAM, the estimation of relative camera pose is crucial to determine the spatial relationship between consecutive camera images, helping to accurately track the movement of the camera in its environment. In small indoor scenes, when the training set is limited, which is very common in robot SLAM, learning-based methods may fail to converge, especially the Transformer architecture, which requires a more substantial dataset to match the performance of the CNN architecture model. This work addresses this problem with the sharing attention mechanism, building on recent improvements in solving visual Transformer architectures on small datasets while incorporating messenger tokens. Besides, double-embedding is introduced to capture the spatial of images and order of images. In summary, we introduce an intuitive end-to-end relative pose estimation solution and prove its accuracy on the two smallest sub-datasets of 7Scenes. The proposed method is tested with a set of comparison experiments conducted across CNN-based, Transformer-based end-to-end relative pose estimation models, and the robust feature-matching non-learning method. Our model outperforms in all comparisons. Furthermore, ablation studies clearly illustrate that these innovations are crucial for the accuracy of relative pose estimation on small datasets.},
  archive   = {C_IROS},
  author    = {Dun Dai and Quan Quan and Kai-Yuan Cai},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801926},
  month     = {10},
  pages     = {7878-7884},
  title     = {Sharing attention mechanism in V-SLAM: Relative pose estimation with messenger tokens on small datasets},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). SMORE-SLAM: Semantic monocular SLAM with scale correction
and reverse loop utilization in outdoor environments. <em>IROS</em>,
7870–7877. (<a
href="https://doi.org/10.1109/IROS58592.2024.10803051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In large-scale outdoor environments, vehicles often encounter situations like retracing their path or turning around, leading to many reverse loop closures where the vehicles traverse previously covered paths from opposite viewpoints. Existing monocular SLAM methods, due to insufficient utilization of semantic information and neglect of leveraging reverse loop closures, result in significant scale drift and pose drift when confronted with such scenarios. In this paper, we introduce SMORE-SLAM, a semantic monocular SLAM with scale correction and reverse loop closure module. We constrain scale drift by harnessing semantic information across a wide spatial extent. Furthermore, we detect and correct reverse loop closures using semantic point cloud to reduce pose drift. Experimental results on the KITTI odometry dataset and the Oxford RobotCar dataset demonstrate the capability of our research in scale correction and reverse loop closure detection, enabling a reduction in trajectory errors of monocular SLAM.},
  archive   = {C_IROS},
  author    = {Yushi Chen and Fang Zhao and Yue Zhuge and Junxiong Liu and Jiaquan Yan and Haiyong Luo},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10803051},
  month     = {10},
  pages     = {7870-7877},
  title     = {SMORE-SLAM: Semantic monocular SLAM with scale correction and reverse loop utilization in outdoor environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SR-LIO: LiDAR-inertial odometry with sweep reconstruction.
<em>IROS</em>, 7862–7869. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a novel LiDAR-Inertial odometry (LIO), named SR-LIO, based on an error state iterated Kalman filter (ESIKF) framework. We adapt the sweep reconstruction method, which segments and reconstructs raw input sweeps from spinning LiDAR to obtain reconstructed sweeps with higher frequency. We found that such method can effectively reduce the time interval for each iterated state update, improving the state estimation accuracy and enabling the usage of ESIKF framework for fusing high-frequency IMU and low-frequency LiDAR. To prevent inaccurate trajectory caused by multiple distortion correction to a particular point, we further propose to perform distortion correction for each segment. Experimental results on four public datasets demonstrate that our SR-LIO outperforms all existing state-of-the-art methods on accuracy, and reducing the time interval of iterated state update via the proposed sweep reconstruction can improve the accuracy and frequency of estimated states. The source code of SR-LIO is publicly available for the development of the community.},
  archive   = {C_IROS},
  author    = {Zikang Yuan and Fengtian Lang and Tianle Xu and Xin Yang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802314},
  month     = {10},
  pages     = {7862-7869},
  title     = {SR-LIO: LiDAR-inertial odometry with sweep reconstruction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NDT-map-code: A 3D global descriptor for real-time loop
closure detection in lidar SLAM. <em>IROS</em>, 7854–7861. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Loop-closure detection, also known as place recognition, aiming to identify previously visited locations, is an essential component of a SLAM system. Existing research on lidar-based loop closure heavily relies on dense point cloud and 360 FOV lidars. This paper proposes an out-of-the-box NDT (Normal Distribution Transform) based global descriptor, NDT-Map-Code, designed for both on-road driving and underground valet parking scenarios. NDT-Map-Code can be directly extracted from the NDT map without the need for a dense point cloud, resulting in excellent scalability and low maintenance cost. The NDT representation is leveraged to identify representative patterns, which are further encoded according to their spatial location (bearing, range, and height). Experimental results on the NIO underground parking lot dataset and the KITTI dataset demonstrate that our method achieves significantly better performance compared to the state-of-the-art.},
  archive   = {C_IROS},
  author    = {Lizhou Liao and Wenlei Yan and Li Sun and Xinhui Bai and Zhenxing You and Hongyuan Yuan and Chunyun Fu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801384},
  month     = {10},
  pages     = {7854-7861},
  title     = {NDT-map-code: A 3D global descriptor for real-time loop closure detection in lidar SLAM},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). V3D-SLAM: Robust RGB-d SLAM in dynamic environments with 3D
semantic geometry voting. <em>IROS</em>, 7847–7853. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simultaneous localization and mapping (SLAM) in highly dynamic environments is challenging due to the correlation complexity between moving objects and the camera pose. Many methods have been proposed to deal with this problem; however, the moving properties of dynamic objects with a moving camera remain unclear. Therefore, to improve SLAM’s performance, minimizing disruptive events of moving objects with a physical understanding of 3D shapes and dynamics of objects is needed. In this paper, we propose a robust method, V3D-SLAM, to remove moving objects via two lightweight reevaluation stages, including identifying potentially moving and static objects using a spatial-reasoned Hough voting mechanism and refining static objects by detecting dynamic noise caused by intra-object motions using Chamfer distances as similarity measurements. Through our experiment on the TUM RGB-D benchmark on dynamic sequences with ground-truth camera trajectories, the results show that our methods outperform most other recent state-of-the-art SLAM methods. Our source code is available at https://github.com/tuantdang/v3d-slam.},
  archive   = {C_IROS},
  author    = {Tuan Dang and Khang Nguyen and Manfred Huber},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801333},
  month     = {10},
  pages     = {7847-7853},
  title     = {V3D-SLAM: Robust RGB-D SLAM in dynamic environments with 3D semantic geometry voting},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DNS-SLAM: Dense neural semantic-informed SLAM.
<em>IROS</em>, 7839–7846. (<a
href="https://doi.org/10.1109/IROS58592.2024.10803056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, coordinate-based neural implicit representations have shown promising results for the task of Simultaneous Localization and Mapping (SLAM). While achieving impressive performance on small synthetic scenes, these methods often suffer from losing details, especially for complex real-world scenes. In this work, we introduce DNS SLAM, a novel neural RGB-D semantic SLAM approach featuring a hybrid representation. Relying only on 2D semantic priors, we propose the first semantic neural SLAM method that trains class-wise scene representations while providing stable camera tracking at the same time. Our method integrates multi-view geometry constraints with image-based feature extraction to improve appearance details and to output color, occupancy, and semantic class information, enabling many downstream applications. To further enable fast tracking, we introduce a lightweight coarse scene representation which is trained in a self-supervised manner in latent space. Our experimental results achieve state-of-the-art performance on both synthetic data and real-world data tracking while maintaining a commendable operational speed on off-the-shelf hardware. Further, our method outputs class-wise decomposed reconstructions with better texture, capturing appearance and geometric details.},
  archive   = {C_IROS},
  author    = {Kunyi Li and Michael Niemeyer and Nassir Navab and Federico Tombari},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10803056},
  month     = {10},
  pages     = {7839-7846},
  title     = {DNS-SLAM: Dense neural semantic-informed SLAM},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RaNDT SLAM: Radar SLAM based on intensity-augmented normal
distributions transform. <em>IROS</em>, 7831–7838. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Rescue robotics sets high requirements to perception algorithms due to the unstructured and potentially vision-denied environments. Pivoting Frequency-Modulated Continuous Wave radars are an emerging sensing modality for SLAM in this kind of environment. However, the complex noise characteristics of radar SLAM makes, particularly indoor, applications computationally demanding and slow. In this work, we introduce a novel radar SLAM framework, RaNDT SLAM, that operates fast and generates accurate robot trajectories. The method is based on the Normal Distributions Transform augmented by radar intensity measures. Motion estimation is based on fusion of motion model, IMU data, and registration of the intensity-augmented Normal Distributions Transform. We evaluate RaNDT SLAM in a new benchmark dataset and the Oxford Radar RobotCar dataset. The new dataset contains indoor and outdoor environments besides multiple sensing modalities (LiDAR, radar, and IMU).},
  archive   = {C_IROS},
  author    = {Maximilian Hilger and Nils Mandischer and Burkhard Corves},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802458},
  month     = {10},
  pages     = {7831-7838},
  title     = {RaNDT SLAM: Radar SLAM based on intensity-augmented normal distributions transform},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing reinforcement learning in sensor fusion: A
comparative analysis of cubature and sampling-based integration methods
for rover search planning. <em>IROS</em>, 7825–7830. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study investigates the computational speed and accuracy of two numerical integration methods, cubature and sampling-based, for integrating an integrand over a 2D polygon. Using a group of rovers searching the Martian surface with a limited sensor footprint as a test bed, the relative error and computational time are compared as the area was sub-divided to improve accuracy in the sampling-based approach. The results show that the sampling-based approach exhibits a 14:75% deviation in relative error compared to cubature when it matches the computational performance at 100%. Furthermore, achieving a relative error below 1% necessitates a 10000% increase in relative time to calculate due to the $\mathcal{O}\left( {{N^2}} \right)$ complexity of the sampling-based method. It is concluded that for enhancing reinforcement learning capabilities and other high iteration algorithms, the cubature method is preferred over the sampling-based method.},
  archive   = {C_IROS},
  author    = {Jan-Hendrik Ewers and Sarah Swinton and David Anderson and Euan McGookin and Douglas Thomson},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801978},
  month     = {10},
  pages     = {7825-7830},
  title     = {Enhancing reinforcement learning in sensor fusion: A comparative analysis of cubature and sampling-based integration methods for rover search planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Object instance retrieval in assistive robotics: Leveraging
fine-tuned SimSiam with multi-view images based on 3D semantic map.
<em>IROS</em>, 7817–7824. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots that assist humans in their daily lives should be able to locate specific instances of objects in an environment that match a user’s desired objects. This task is known as instance-specific image goal navigation (InstanceImageNav), which requires a model that can distinguish different instances of an object within the same class. A significant challenge in robotics is that when a robot observes the same object from various 3D viewpoints, its appearance may differ significantly, making it difficult to recognize and locate accurately. In this paper, we introduce a method called SimView, which leverages multi-view images based on a 3D semantic map of an environment and self-supervised learning using SimSiam to train an instance-identification model on-site. The effectiveness of our approach was validated using a photorealistic simulator, Habitat Matterport 3D, created by scanning actual home environments. Our results demonstrate a 1.7-fold improvement in task accuracy compared with contrastive language-image pre-training (CLIP), a pre-trained multimodal contrastive learning method for object searching. This improvement highlights the benefits of our proposed fine-tuning method in enhancing the performance of assistive robots in InstanceImageNav tasks. The project website is https://emergentsystemlabstudent.github.io/MultiViewRetrieve/.},
  archive   = {C_IROS},
  author    = {Taichi Sakaguchi and Akira Taniguchi and Yoshinobu Hagiwara and Lotfi El Hafi and Shoichi Hasegawa and Tadahiro Taniguchi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802697},
  month     = {10},
  pages     = {7817-7824},
  title     = {Object instance retrieval in assistive robotics: Leveraging fine-tuned SimSiam with multi-view images based on 3D semantic map},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Embodiment: Self-supervised depth estimation based on camera
models. <em>IROS</em>, 7809–7816. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Depth estimationn is a critical topic for robotics and vision-related tasks. In monocular depth estimation, in comparison with supervised learning that requires expensive ground truth labeling, self-supervised methods possess great potential due to no labeling cost. However, self-supervised learning still has a large gap with supervised learning in 3D reconstruction and depth estimation performance. Meanwhile, scaling is also a major issue for monocular unsupervised depth estimation, which commonly still needs ground truth scale from GPS, LiDAR, or existing maps to correct. In the era of deep learning, existing methods primarily rely on exploring image relationships to train unsupervised neural networks, while the physical properties of the camera itself—such as intrinsics and extrinsics—are often overlooked. These physical properties are not just mathematical parameters; they are embodiments of the camera’s interaction with the physical world. By embedding these physical properties into the depth learning model, we can calculate depth priors for ground regions and regions connected to the ground based on physical principles, providing free supervision signals without the need for additional sensors. This approach is not only easy to implement but also enhances the effects of all unsupervised methods by embedding the camera’s physical properties into the model, thereby achieving an embodied understanding of the real world.},
  archive   = {C_IROS},
  author    = {Jinchang Zhang and Praveen Kumar Reddy and Xue-Iuan Wong and Yiannis Aloimonos and Guoyu Lu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801343},
  month     = {10},
  pages     = {7809-7816},
  title     = {Embodiment: Self-supervised depth estimation based on camera models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active human pose estimation via an autonomous UAV agent.
<em>IROS</em>, 7801–7808. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One of the core activities of an active observer involves moving to secure a &quot;better&quot; view of the scene, where the definition of &quot;better&quot; is task-dependent. This paper focuses on the task of human pose estimation from videos capturing a person’s activity. Self-occlusions within the scene can complicate or even prevent accurate human pose estimation. To address this, relocating the camera to a new vantage point is necessary to clarify the view, thereby improving 2D human pose estimation. This paper formalizes the process of achieving an improved viewpoint. Our proposed solution to this challenge comprises three main components: a NeRF-based Drone-View Data Generation Framework, an On-Drone Network for Camera View Error Estimation, and a Combined Planner for devising a feasible motion plan to reposition the camera based on the predicted errors for camera views. The Data Generation Framework utilizes NeRF-based methods to generate a comprehensive dataset of human poses and activities, enhancing the drone’s adaptability in various scenarios. The Camera View Error Estimation Network is designed to evaluate the current human pose and identify the most promising next viewing angles for the drone, ensuring a reliable and precise pose estimation from those angles. Finally, the combined planner incorporates these angles while considering the drone’s physical and environmental limitations, employing efficient algorithms to navigate safe and effective flight paths. This system represents a significant advancement in active 2D human pose estimation for an autonomous UAV agent, offering substantial potential for applications in aerial cinematography by improving the performance of autonomous human pose estimation and maintaining the operational safety and efficiency of UAVs.},
  archive   = {C_IROS},
  author    = {Jingxi Chen and Botao He and Chahat Deep Singh and Cornelia Fermüller and Yiannis Aloimonos},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801780},
  month     = {10},
  pages     = {7801-7808},
  title     = {Active human pose estimation via an autonomous UAV agent},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IN-sight: Interactive navigation through sight.
<em>IROS</em>, 7794–7800. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current visual navigation systems often treat the environment as static, lacking the ability to adaptively interact with obstacles. This limitation leads to navigation failure when encountering unavoidable obstructions. In response, we introduce IN-Sight, a novel approach to self-supervised path planning, enabling more effective navigation strategies through interaction with obstacles. Utilizing RGB-D observations, IN-Sight calculates traversability scores and incorporates them into a semantic map, facilitating long-range path planning in complex, maze-like environments. To precisely navigate around obstacles, IN-Sight employs a local planner, trained imperatively on a differentiable costmap using representation learning techniques. The entire framework undergoes end-to-end training within the state-of-the-art photorealistic Intel SPEAR Simulator. We validate the effectiveness of IN-Sight through extensive benchmarking in a variety of simulated scenarios and ablation studies. Moreover, we demonstrate the system’s real-world applicability with zero-shot sim-to-real transfer, deploying our planner on the legged robot platform ANYmal, showcasing its practical potential for interactive navigation in real environments.},
  archive   = {C_IROS},
  author    = {Philipp Schoch and Fan Yang and Yuntao Ma and Stefan Leutenegger and Marco Hutter and Quentin Leboutet},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801928},
  month     = {10},
  pages     = {7794-7800},
  title     = {IN-sight: Interactive navigation through sight},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DiPPeST: Diffusion-based path planner for synthesizing
trajectories applied on quadruped robots. <em>IROS</em>, 7787–7793. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present DiPPeST, a novel image and goal conditioned diffusion-based trajectory generator for quadrupedal robot path planning. DiPPeST is a zero-shot adaptation of our previously introduced diffusion-based 2D global trajectory generator (DiPPeR). The introduced system incorporates a novel strategy for local real-time path refinements, that is reactive to camera input, without requiring any further training, image processing, or environment interpretation techniques. DiPPeST achieves 92% success rate in obstacle avoidance for nominal environments and an average of 88% success rate when tested in environments that are up to 3.5 times more complex in pixel variation than DiPPeR. A visual-servoing framework is developed to allow for real-world execution, tested on the quadruped robot, achieving 80% success rate in different environments and showcasing improved behavior than complex state-of-the-art local planners, in narrow environments. Website: https://rpl-cs-ucl.github.io/DiPPeSTweb/},
  archive   = {C_IROS},
  author    = {Maria Stamatopoulou and Jianwei Liu and Dimitrios Kanoulas},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802677},
  month     = {10},
  pages     = {7787-7793},
  title     = {DiPPeST: Diffusion-based path planner for synthesizing trajectories applied on quadruped robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Environmental and behavioral imitation for autonomous
navigation. <em>IROS</em>, 7779–7786. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce a framework for imitation learning in navigation that enables policy learning from one-shot images without a physical robot and facilitates the transfer of this policy from simulation to reality. Utilizing Neural Radiance Fields (NeRF), our approach generates a simulated environment and simultaneously models expert behavior. This removes the necessity for a physical robot during both the expert teaching phase and the agent’s learning process, allowing for the application of policies learned within the NeRF simulation to real-world robots. We validate our method by demonstrating the navigation with an actual robot using the policy learned by our approach. Moreover, we present a method for adapting to changes in the robot configuration, such as camera parameters and robot dimensions, by simulating adjustments in the robot configuration throughout the learning and assessing its generalizability.},
  archive   = {C_IROS},
  author    = {Junki Aoki and Fumihiro Sasaki and Kohei Matsumoto and Ryota Yamashina and Ryo Kurazume},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801902},
  month     = {10},
  pages     = {7779-7786},
  title     = {Environmental and behavioral imitation for autonomous navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Asynchronous event-inertial odometry using a unified
gaussian process regression framework. <em>IROS</em>, 7773–7778. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent works have combined monocular event camera and inertial measurement unit to estimate the SE(3) trajectory. However, the asynchronicity of event cameras brings a great challenge to conventional fusion algorithms. In this paper, we present an asynchronous event-inertial odometry under a unified Gaussian Process (GP) regression framework to naturally fuse asynchronous data associations and inertial measurements. A GP latent variable model is leveraged to build data-driven motion prior and acquire the analytical integration capacity. Then, asynchronous event-based feature associations and integral pseudo measurements are tightly coupled using the same GP framework. Subsequently, this fusion estimation problem is solved by underlying factor graph in a sliding-window manner. With consideration of sparsity, those historical states are marginalized orderly. A twin system is also designed for comparison, where the traditional inertial preintegration scheme is embedded in the GP-based framework to replace the GP latent variable model. Evaluations on public event-inertial datasets demonstrate the validity of both systems. Comparison experiments show competitive precision compared to the state-of-the-art synchronous scheme.},
  archive   = {C_IROS},
  author    = {Xudong Li and Zhixiang Wang and Zihao Liu and Yizhai Zhang and Fan Zhang and Xiuming Yao and Panfeng Huang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802357},
  month     = {10},
  pages     = {7773-7778},
  title     = {Asynchronous event-inertial odometry using a unified gaussian process regression framework},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Perception-aware full body trajectory planning for
autonomous systems using motion primitives. <em>IROS</em>, 7765–7772.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many robotic systems rely on visual sensing to accomplish simultaneously the tasks of state estimation, mapping, and path planning. One one hand, the usage of camera sensors represents a power-efficient and lightweight option for solving this problem. On the other hand, these tasks pose requirements on the quality of the visual input (e.g. number of tracked features for Visual Odometry) that are often in contrast to the optimal viewpoint planning for local mapping and obstacle avoidance. Dealing with this constraint is actively researched in the field of perception-aware planning. The approaches delivered by this field mostly concern Micro air vehicles (MAVs), but could be applied to a larger group of robotic systems. We propose a perception-aware trajectory planner for a class of robotic systems that can orient their cameras independently from their direction of travel. By using motion primitives, our planner does not require differentiable models for motion and perception objectives. We evaluate our method in simulation, showing increased capabilities in localization-aware motions around obstacles, and demonstrate its run-time capability on a real planetary rover. The code is released publicly under github.com/DLR-RM/palp.},
  archive   = {C_IROS},
  author    = {Moritz Kuhne and Riccardo Giubilato and Martin J. Schuster and Maximo A. Roa},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801381},
  month     = {10},
  pages     = {7765-7772},
  title     = {Perception-aware full body trajectory planning for autonomous systems using motion primitives},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time bird’s-eye-view panoptic segmentation for
monocular-based indoor navigation. <em>IROS</em>, 7758–7764. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bird’s-Eye-View (BEV) segmentation is a essential technology for safe and efficient navigation. This is even more necessary in indoor driving, where there are dynamic and unstructured objects such as people and robot. However, since there is no way to generate training data, most of the researches have been conducted mainly in outdoor environments. In this paper, we propose an innovative approach to address this challenge. We automatically generate BEV training data for indoor environments based on the physics engine of a simulator. This eliminates the needs for tons of real data and virtual environments. We also propose a lightweight network architecture capable of BEV panoptic segmentation in real-time. Based on this network and simple image processing, our framework shows fast but also robust performance in real-world environments with no gap between simulation and reality. The network can inference at a speed of about 61 FPS on AGX Orin in FP16 mode. Furthermore, it outperforms existing algorithms in the semantic segmentation task, achieving 14% higher mIoU.},
  archive   = {C_IROS},
  author    = {Dawit Kim and Jungmo Koo and Jongseob Yun and Soonyong Park},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802038},
  month     = {10},
  pages     = {7758-7764},
  title     = {Real-time bird’s-eye-view panoptic segmentation for monocular-based indoor navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MG-VLN: Benchmarking multi-goal and long-horizon
vision-language navigation with language enhanced memory map.
<em>IROS</em>, 7750–7757. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision-Language Navigation (VLN) with high-level language instructions is a crucial task in robotics. Existing VLN benchmarks, such as the REVERIE challenge which has single-goal instructions and limited navigation steps, do not fully encapsulate the complexity of real-world navigation that often require multi-objective and long-horizon navigation. To address this, we propose a new benchmark task: Multi-Goal and Long-Horizon Vision-Language Navigation (MG-VLN), extending the REVERIE benchmark to encompass multi-objective and long-horizon navigation scenarios with sequences of high-level instructions. This task aims to provide a simulation benchmark to guide the design of lifelong and long-horizon navigation robots. To initiate the exploration in this newly proposed task, we first investigate the role of long-term memory in improving navigation performance by leveraging environmental information gathered during previous sub-goals. Additionally, we examine the types of knowledge that most effectively enrich this long-term memory. Specifically, we integrate the visual contents with linguistic knowledge such as object categories, visual captions, and object attributes/relationships. Our findings indicate that: 1) the explicit long-term memory map significantly enhances navigation performance in multi-goal and long-horizon scenarios; 2) incorporating object attributes and relationships information is the most advantageous for aligning environmental cues with high-level instructions.},
  archive   = {C_IROS},
  author    = {Junbo Zhang and Kaisheng Ma},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801689},
  month     = {10},
  pages     = {7750-7757},
  title     = {MG-VLN: Benchmarking multi-goal and long-horizon vision-language navigation with language enhanced memory map},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiple visual features in topological map for
vision-and-language navigation. <em>IROS</em>, 7742–7749. (<a
href="https://doi.org/10.1109/IROS58592.2024.10803061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision-and-Language Navigation (VLN) in continuous environments aims to navigate robot agents in unseen environments following natural language instructions. The majority of existing approaches rely on constructing semantic maps or topological maps to record information. However, semantic maps overlook the detailed information of objects and the correspondence among views during navigation, while topological maps lack the spatial representation between entities. To address these limitations, we propose a novel visual feature representation method for continuous VLN, called Multiple Visual Features in Topological Map (MV-Topo). MV-Topo utilizes three distinct visual encoders to extract visual features, which are integrated in the dynamically generated topological map. These fused features actively participate in the subsequent cross-modal planning to derive a long-term path towards a subgoal, effectively guiding the agent to reach the final location. We experimentally demonstrate the effectiveness of our approach and achieve competitive results on the full VLN-CE test splits. Notably, our method outperforms the state-of-the-art by 3.5% in terms of the Navigation Error (NE) metric, indicating that the utilization of multiple visual features significantly enhances the agent’s perception of semantic targets.},
  archive   = {C_IROS},
  author    = {Ruonan Liu and Ping Kong and Weidong Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10803061},
  month     = {10},
  pages     = {7742-7749},
  title     = {Multiple visual features in topological map for vision-and-language navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SWIFT: Strategic weather-informed image-based forecasting
for trajectories. <em>IROS</em>, 7734–7741. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Predicting agents’ trajectories in complex environments is critical for achieving safe autonomous robot navigation. Empirically, agents’ decisions and preferences are susceptible to changes in environmental factors (e.g., interactions with other agents, weather conditions, traffic rules). State-of-the-art methods rely on High-Definition (HD) or semantic maps to model the environment, but do not take into account unpredictable factors such as complex weather conditions. In addition, since HD maps are nontrivial to obtain, those methods are limited in the scope of environments they can be applied in. We propose a more flexible graph based trajectory prediction model that uses only images to model the environment, without requiring expensive map information. We experimentally validate our proposed model, demonstrating robust performances in trajectory prediction compared to state-of-the-art methods, and outperform in complex environments that cannot be modeled with purely map based methods, such as diverse weather conditions.},
  archive   = {C_IROS},
  author    = {Youya Xia and Jose Nino and Yutao Han and Mark Campbell},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802567},
  month     = {10},
  pages     = {7734-7741},
  title     = {SWIFT: Strategic weather-informed image-based forecasting for trajectories},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhanced language-guided robot navigation with panoramic
semantic depth perception and cross-modal fusion. <em>IROS</em>,
7726–7733. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Integrating visual observation with linguistic instruction holds significant promise for enhancing robot navigation across unstructured environments and enriches the human-robot interaction experience. However, while panoramic RGB views furnish robots with extensive environmental visuals, current methods significantly overlook crucial semantic and depth cues. This incomplete representation may lead to misinterpretation or inadequate execution of language instructions, thereby impeding navigation performance and adaptability. In this paper, we introduce SEAT, a semantic-depth aware cross-modal transformer model. Our approach incorporates an efficient panoramic multi-type visual encoder to capture comprehensive environmental details. To mitigate the rigidity of feature mapping stemming from the freezing of pre-training encoders, we propose a novel region query pre-training task. Additionally, we leverage an improved dual-scale cross-modal transformer to facilitate the integration of instructions, topological memory, and action prediction. Extensive experiments on three language-guided robot navigation datasets demonstrate the efficacy of our model, achieving competitive navigation success rates with fewer parameters and computational load. Furthermore, we validate SEAT’s effectiveness in real-world scenarios by deploying it on a mobile robot across various environments. The code is available at https://github.com/CrystalSixone/SEAT.},
  archive   = {C_IROS},
  author    = {Liuyi Wang and Jiagui Tang and Zongtao He and Ronghao Dang and Chengju Liu and Qijun Chen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801563},
  month     = {10},
  pages     = {7726-7733},
  title     = {Enhanced language-guided robot navigation with panoramic semantic depth perception and cross-modal fusion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PathFormer: A transformer-based framework for vision-centric
autonomous navigation in off-road environments. <em>IROS</em>,
7718–7725. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The efficient navigation of autonomous vehicles across rugged and unstructured terrains remains a significant challenge. Most existing research in this area emphasizes the need for complex mappings or intricate multi-step methodologies. However, these traditional approaches often struggle to adapt to dynamic changes in environmental conditions. In this paper, we introduce PathFormer, an end-to-end framework designed specifically to address these challenges. PathFormer utilizes transformers to decode free-space semantics and configurations directly from camera images, enabling efficient path planning without the reliance on detailed, pre-existing maps. The performance of PathFormer was rigorously evaluated across diverse datasets, where it demonstrated superior capabilities, outperforming other state-of-the-art methods by 3.68% in precisely segmenting free-space regions and showing a 13.65% improvement in correctly predicting traversable paths.},
  archive   = {C_IROS},
  author    = {Bilal Hassan and Nadya Abdel Madjid and Fatima Kashwani and Mohamad Alansari and Majid Khonji and Jorge Dias},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802399},
  month     = {10},
  pages     = {7718-7725},
  title     = {PathFormer: A transformer-based framework for vision-centric autonomous navigation in off-road environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-fidelity reinforcement learning for minimum energy
trajectory planning. <em>IROS</em>, 7710–7717. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modeling the energy consumption of a quadrotor involves complex electrical and physical dynamics, making it difficult to optimize. To address this challenge, this paper presents a multi-fidelity Gaussian process (MFGP) method that efficiently learns an accurate energy prediction model by combining many low-fidelity samples from a simple motor model with a few computationally expensive samples from a numerical battery simulation. We present extensive sample-efficiency experiments, demonstrating that a single-fidelity model often needs 10 times more high-fidelity data to match the accuracy achieved by the MFGP. The energy prediction model is then applied to a reinforcement learning (RL) agent, providing a reward signal to a minimum energy planning policy. The RL policy generates more energy efficient trajectories than those found by the minimum snap baseline method, achieving an average 3.6% energy reduction.},
  archive   = {C_IROS},
  author    = {Luke de Castro and Gilhyun Ryou and Hyungseuk Ohn and Sertac Karaman},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801931},
  month     = {10},
  pages     = {7710-7717},
  title     = {Multi-fidelity reinforcement learning for minimum energy trajectory planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Context-generative default policy for bounded rational
agent. <em>IROS</em>, 7703–7709. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bounded rational agents often make decisions by evaluating a finite selection of choices, typically derived from a reference point termed the ‘default policy,’ based on previous experience. However, the inherent rigidity of the static default policy presents significant challenges for agents when operating in unknown environment, that are not included in agent’s prior knowledge. In this work, we introduce a context-generative default policy that leverages the region observed by the robot to predict unobserved part of the environment, thereby enabling the robot to adaptively adjust its default policy based on both the actual observed map and the imagined unobserved map. Furthermore, the adaptive nature of the bounded rationality framework enables the robot to manage unreliable or incorrect imaginations by selectively sampling a few trajectories in the vicinity of the default policy. Our approach utilizes a diffusion model for map prediction and a sampling-based planning with B-spline trajectory optimization to generate the default policy. Extensive evaluations reveal that the context-generative policy outperforms the baseline methods in identifying and avoiding unseen obstacles. Additionally, real-world experiments conducted with the Crazyflie drones demonstrate the adaptability of our proposed method, even when acting in environments outside the domain of the training distribution.},
  archive   = {C_IROS},
  author    = {Durgakant Pushp and Junhong Xu and Zheng Chen and Lantao Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801923},
  month     = {10},
  pages     = {7703-7709},
  title     = {Context-generative default policy for bounded rational agent},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). When, what, and with whom to communicate: Enhancing RL-based
multi-robot navigation through selective communication. <em>IROS</em>,
7695. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Decentralized navigation methods rely primarily on local observations, lacking the global awareness needed to coordinate effectively within a multi-agent system. Exchanging relevant messages between agents can promote cooperation and improve navigation efficiency. We present a Reinforcement Learning (RL)-based decentralized navigation approach that learns ‘when,’ ‘what,’ and ‘with whom’ to communicate for safe and cooperative navigation. Our method leverages a visual transformer and self-attention mechanism to encode the local occupancy map and the state information of neighbors into fixed-length encodings, allowing it to handle an arbitrary number of neighbors for collision-free navigation. In addition, the network encodes the agent’s state information and observations of neighboring agents into a concise message vector by learning what information is crucial to communicate, which is shared with neighboring agents upon request. Moreover, to avoid indiscriminate broadcasting, the network learns when and with whom to communicate and request message vectors. Subsequently, the messages communicated alongside the local information are used to guide navigation decisions. We evaluate our method against state-of-the-art baselines in complex scenarios, including narrow corridors and environments with multiple agents. We observe considerable improvements in terms of navigation performance, showing up to ∼ 2× improvement in navigation success rates and a reduction of up to ∼ 20% in path length.},
  archive   = {C_IROS},
  author    = {Senthil Hariharan Arul and Amrit Singh Bedi and Dinesh Manocha},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802520},
  month     = {10},
  pages     = {7695},
  title     = {When, what, and with whom to communicate: Enhancing RL-based multi-robot navigation through selective communication},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AMCO: Adaptive multimodal coupling of vision and
proprioception for quadruped robot navigation in outdoor environments.
<em>IROS</em>, 7687–7694. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present AMCO, a novel navigation method for quadruped robots that adaptively combines vision-based and proprioception-based perception capabilities. Our approach uses three cost maps: general knowledge map; traversability history map; and current proprioception map; which are derived from a robot’s vision and proprioception data, and couples them to obtain a coupled traversability cost map for navigation. The general knowledge map encodes terrains semantically segmented from visual sensing, and represents a terrain’s typically expected traversability. The traversability history map encodes the robot’s recent proprioceptive measurements on a terrain and its semantic segmentation as a cost map. Further, the robot’s present proprioceptive measurement is encoded as a cost map in the current proprioception map. As the general knowledge map and traversability history map rely on semantic segmentation, we evaluate the reliability of the visual sensory data by estimating the brightness and motion blur of input RGB images and accordingly combine the three cost maps to obtain the coupled traversability cost map used for navigation. Leveraging this adaptive coupling, the robot can depend on the most reliable input modality available. Finally, we present a novel planner that selects appropriate gaits and velocities for traversing challenging outdoor environments using the coupled traversability cost map. We demonstrate AMCO’s navigation performance in different real-world outdoor environments and observe 10.8%-34.9% reduction w.r.t. two stability metrics, and up to 50% improvement in terms of success rate compared to current navigation methods.},
  archive   = {C_IROS},
  author    = {Mohamed Elnoor and Kasun Weerakoon and Adarsh Jagan Sathyamoorthy and Tianrui Guan and Vignesh Rajagopal and Dinesh Manocha},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801962},
  month     = {10},
  pages     = {7687-7694},
  title     = {AMCO: Adaptive multimodal coupling of vision and proprioception for quadruped robot navigation in outdoor environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Practical framework for path representation and following
control in mobile industrial robots. <em>IROS</em>, 7679–7686. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a practical framework for path representation and following control for mobile industrial robots. Mobile industrial robots perform various missions, such as navigating predefined routes and transporting cargo in industrial environments. Unlike traditional AGVs that rely on physical tapes installed densely along routes, our framework supports the generation of drivable paths by connecting waypoints specified by users. The proposed framework generates drivable work paths while considering the permissible deviation between waypoints and the path, as well as the maximum curvature, without nonlinear optimization techniques. Additionally, we introduce a &quot;stopover&quot; attribute for each waypoint to enhance usability in narrow workspaces. Furthermore, we have developed a practical path following control system that takes into account real-world challenges such as actuation delay and computational efficiency. The proposed speed planner incorporates predictive information without relying on optimization techniques, and supports robust following performance by braking intervention if path deviation increases. The effectiveness of the proposed framework has been validated through robot tests, demonstrating accurate path following performance and efficient computation.},
  archive   = {C_IROS},
  author    = {Youngil Koh and Woojeong Kim and MidEum Choi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801664},
  month     = {10},
  pages     = {7679-7686},
  title     = {Practical framework for path representation and following control in mobile industrial robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model predictive trees: Sample-efficient receding horizon
planning with reusable tree search. <em>IROS</em>, 7671–7678. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present Model Predictive Trees (MPT), a receding horizon tree search algorithm that improves its performance by reusing information efficiently. Whereas existing solvers reuse only the highest-quality trajectory from the previous iteration as a &quot;hotstart&quot;, our method reuses the entire optimal subtree, enabling the search to be simultaneously guided away from the low-quality areas and towards the high-quality areas. We characterize the restrictions on tree reuse by analyzing the induced tracking error under time-varying dynamics, revealing a tradeoff between the search depth and the timescale of the changing dynamics. In numerical studies, our algorithm outperforms state-of-the-art sampling-based cross-entropy methods with hotstarting. We demonstrate our planner on an autonomous vehicle testbed performing a nonprehensile manipulation task: pushing a target object through an obstacle field. Code associated with this work will be made available at https://github.com/jplathrop/mpt.},
  archive   = {C_IROS},
  author    = {John Lathrop and Benjamin Rivière and Jedidiah Alindogan and Soon-Jo Chung},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802673},
  month     = {10},
  pages     = {7671-7678},
  title     = {Model predictive trees: Sample-efficient receding horizon planning with reusable tree search},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive planning with generative models under uncertainty.
<em>IROS</em>, 7664–7670. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Planning with generative models has emerged as an effective decision-making paradigm across a wide range of domains, including reinforcement learning and autonomous navigation. While continuous replanning at each timestep might seem intuitive because it allows decisions to be made based on the most recent environmental observations, it results in substantial computational challenges, primarily due to the complexity of the generative model’s underlying deep learning architecture. Our work addresses this challenge by introducing a simple adaptive planning policy that leverages the generative model’s ability to predict long-horizon state trajectories, enabling the execution of multiple actions consecutively without the need for immediate replanning. We propose to use the predictive uncertainty derived from a Deep Ensemble of inverse dynamics models to dynamically adjust the intervals between planning sessions. In our experiments conducted on locomotion tasks within the OpenAI Gym framework, we demonstrate that our adaptive planning policy allows for a reduction in replanning frequency to only about 10% of the steps without compromising the performance. Our results underscore the potential of generative modeling as an efficient and effective tool for decision-making.},
  archive   = {C_IROS},
  author    = {Pascal Jutras-Dubé and Ruqi Zhang and Aniket Bera},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802624},
  month     = {10},
  pages     = {7664-7670},
  title     = {Adaptive planning with generative models under uncertainty},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A geometry-based approach for support-free additive
manufacturing of structures with large overhang angles and closed
features. <em>IROS</em>, 7656–7663. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Architected materials derive performance characteristics from material properties and internal geometry. These materials are increasingly prevalent across a wide variety of domains. Many intricate feature geometries associated with architected materials can be explored using additive manufacturing (AM) processes. However, current AM methods generally cannot fabricate geometries with completely closed voids without introducing a support structure. This paper describes a new, support-free approach to AM capable of creating structures with closed voids. This work limits part geometry to three-dimensional (3D) geometries defined by a revolution about a single axis. This limitation enables planar analysis within a three-degree-of-freedom (3-DoF) task space. Part geometry in 3-DoF task space is constrained to a convex arch. Task space geometry is divided into an ordered set of sub-regions, considering feasible deposition orientations and collision constraints. The use of 3-DoF task space provides planar translation and rotation of the component during fabrication. The introduction of this rotational DoF addresses AM overhang constraints imposed by gravity. Methods for generating, ordering, and layering sub-regions suitable for printing a part with a closed hole are presented. Layers derived in the 3-DoF task space analysis are then extended to 3D deposition paths using the axis of revolution defined by the original part. The method of hole closure relies on the concept of a &quot;keystone&quot; which requires a 45° nozzle offset for collision-free deposition within keystone-adjacent sub-regions. The feasibility of deposition using a 45° nozzle offset is explored experimentally, and results demonstrate feasibility.},
  archive   = {C_IROS},
  author    = {Jitian Liu and Zachary Cohen and Jin Seob Kim and Mehran Armand and Michael D. M. Kutzer},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801999},
  month     = {10},
  pages     = {7656-7663},
  title     = {A geometry-based approach for support-free additive manufacturing of structures with large overhang angles and closed features},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HP3: Hierarchical prediction-pretrained planning for
unprotected left turn. <em>IROS</em>, 7648–7655. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trajectory planning for unprotected left turns poses a significant challenge in autonomous driving. Reinforcement learning (RL) offers potential, but existing methods often rely on scenario-specific state representations, limiting their adaptability. This paper introduces Hierarchical Prediction-Pretrained Planning (HP3), a generalizable hierarchical RL framework designed for unprotected left turns. HP3 leverages historical trajectories of all vehicles and complete map information to achieve versatile state representation and generalizable scene understanding. Its two-layer architecture predicts semantic behavior (upper layer) and generates corresponding trajectories (lower layer). A scene encoder comprehends trajectories and roads, while a trajectory decoder outputs sequential points. To accelerate convergence, we pretrain the main network on a modified trajectory prediction dataset. Evaluation on a CARLA-based map with complex, unprotected left-turn intersections demonstrates HP3’s superiority over rule-based and simple RL-based methods, highlighting the effectiveness of our pretraining approach for this critical autonomous driving task.},
  archive   = {C_IROS},
  author    = {Zhihao Ou and Zhibo Wang and Yue Hua and Jinsheng Dou and Di Feng and Jian Pu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801989},
  month     = {10},
  pages     = {7648-7655},
  title     = {HP3: Hierarchical prediction-pretrained planning for unprotected left turn},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient coverage method for irregularly shaped
terrains. <em>IROS</em>, 7641–7647. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In mobile robotics, effectively covering known terrains is essential. While grid-based methods surpass exact cell decomposition in path length and multi-robot scalability, they face challenges in irregular areas. Here we develop a model for shortening coverage paths in arbitrary environments using grid-based methods, which redefines the path optimization problem as finding the largest Hamiltonian sub-graph of a given grid graph. Additionally, we present a Hamiltonian cycle expansion strategy to simplify the resolution process and propose a low-repetitive coverage path planner based on the strategy. Our path planner enables the quick finding of an efficient full coverage path in any region. Simulation results show that our algorithm consistently produces efficient coverage paths across diverse settings and demonstrates its adaptability in multi-robot systems.},
  archive   = {C_IROS},
  author    = {Yuxuan Tang and Qizhen Wu and Chunli Zhu and Lei Chen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801856},
  month     = {10},
  pages     = {7641-7647},
  title     = {An efficient coverage method for irregularly shaped terrains},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Valuing attrition in a fleet of robots used as path-based
sensors for gathering information in a communications restricted
environment. <em>IROS</em>, 7633–7640. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we propose a new algorithm for robots searching a hazardous, communications-denied area to gather information using a robot fleet that has a limited number of agents. The centralized algorithm uses robot survival along search paths as a sensor event for a distributed sensor network. As agents are lost to hazards, the search behavior adjusts to prioritize agent longevity in order to maximize information gain. In the past, related work solving this problem has assumed an infinite number of agents. In contrast, we assume that the number of agents is finite. We use Bayesian inference to update target and hazard belief maps of an area using data from the probability of survival of prior agents’ paths as well as sensor readings from the agents along those paths. Using those belief maps, the algorithm can construct paths that maximize information gain, in expectation, while taking into account the predicted decrease in future information collected when losing an agent. This behavior increases the likelihood that agents survive longer, allowing them to collect more data.Using simulations with various fleet sizes and probabilities for hazards disabling agents, we compare our algorithm to work that does not account for attrition. The results show an increase in the longevity of the fleet when hazards are more effective at disabling agents. In nearly all cases, this contributes to an increased rate in information gain when the fleet size is small. Small sized fleets, in our case 10 or less agents, do not meet a threshold of collected information necessary to direct agents away from hazards. Large fleets, over 200 agents in our scenario, collect most of the information before Our algorithm causes a noticeable change in agent behavior (as compared to existing techniques). We find that the proposed method provides the greatest advantage for mid-sized fleets, between 20 and 100 agents, and when hazards have an increased probability of immobilizing agents.},
  archive   = {C_IROS},
  author    = {Loy McGuire and Michael Otte and Donald Sofge},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802088},
  month     = {10},
  pages     = {7633-7640},
  title     = {Valuing attrition in a fleet of robots used as path-based sensors for gathering information in a communications restricted environment},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VLPG-nav: Object navigation using visual language pose graph
and object localization probability maps. <em>IROS</em>, 7625–7632. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present VLPG-Nav, a visual language navigation method for guiding robots to specified objects within household scenes. Unlike existing methods primarily focused on navigating the robot toward objects, our approach considers the additional challenge of centering the object within the robot’s camera view. Our method builds a visual language pose graph (VLPG) that functions as a spatial map of VL embeddings. Given an open-vocabulary object query, we plan a viewpoint for object navigation using the VLPG. Despite navigating to the viewpoint, real-world challenges such as object occlusion, displacement, and the robot’s localization errors can prevent visibility. We build an object localization probability map that leverages the robot’s current observations and prior VLPG. When the object is not visible, the probability map is updated, and an alternate viewpoint is computed. In addition, we propose an object-centering formulation that locally adjusts the robot’s pose to center the object in the camera view. We evaluate the effectiveness of our approach through simulations and real-world experiments, evaluating its ability to successfully view and center the object within the camera’s field of view. VLPG-Nav demonstrates improved performance in locating the object, navigating around occlusions, and centering the object within the robot’s camera view, outperforming selected baselines in the evaluation settings.},
  archive   = {C_IROS},
  author    = {Senthil Hariharan Arul and Dhruva Kumar and Vivek Sugirtharaj and Richard Kim and Xuewei Tony Qi and Rajasimman Madhivanan and Arnie Sen and Dinesh Manocha},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802008},
  month     = {10},
  pages     = {7625-7632},
  title     = {VLPG-nav: Object navigation using visual language pose graph and object localization probability maps},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PAAMP: Polytopic action-set and motion planning for long
horizon dynamic motion planning via mixed integer linear programming.
<em>IROS</em>, 7617–7624. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Optimization methods for long-horizon, dynamically feasible motion planning in robotics tackle challenging nonconvex and discontinuous optimization problems. Traditional methods often falter due to the nonlinear characteristics of these problems. We introduce a technique that utilizes learned representations of the system, known as Polytopic Action Sets, to efficiently compute long-horizon trajectories. By employing a suitable sequence of Polytopic Action Sets, we transform the long-horizon dynamically feasible motion planning problem into a Linear Program. This reformulation enables us to address motion planning as a Mixed Integer Linear Program (MILP). We demonstrate the effectiveness of a Polytopic Action-Set and Motion Planning (PAAMP) approach by identifying swing-up motions for a torque-constrained pendulum as fast as 0.75 milliseconds. This approach is well-suited for solving complex motion planning and long-horizon Constraint Satisfaction Problems (CSPs) in dynamic and underactuated systems such as legged and aerial robots.},
  archive   = {C_IROS},
  author    = {Akshay Jaitly and Siavash Farzan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802176},
  month     = {10},
  pages     = {7617-7624},
  title     = {PAAMP: Polytopic action-set and motion planning for long horizon dynamic motion planning via mixed integer linear programming},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GESCE: Graph-based ergodic search in cluttered environments.
<em>IROS</em>, 7611–7616. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a novel motion planning algorithm that inherits the strengths of both optimization and search-based planners. Optimization-based planners use the gradient of an objective function to generate a desired path, whereas search-based planners operate on a graph capturing the salient topology of a robot’s free space. A class of optimization-based planners leverages prior information, modeled as a probability distribution of target locations in an environment, to guide path generation. We embrace one specific measure, referred to as ergodicity, which encourages a robot to spend a proportion of its time, weighted by the distribution, where it is likely to find targets of interest. Methods that minimize ergodicity were not designed to handle obstacles in the environment, and augmented approaches that add &quot;soft&quot; constraints for obstacles to the cost function may still yield a path that collides with an obstacle. In this work, we present a hybrid approach that first generates a graph of the environment’s free space, followed by searching the graph with ergodicity as a heuristic. Our approach not only restricts the search to the free space, thereby avoiding obstacles by design, but also generates trajectories with low ergodicity values. Extensive testing on 125 test scenarios with varying degrees of clutter, information distribution, and robot start locations illustrate the efficacy of our algorithm.},
  archive   = {C_IROS},
  author    = {Burhanuddin Shirose and Adam Johnson and Bhaskar Vundurthy and Howie Choset and Matthew Travers},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802461},
  month     = {10},
  pages     = {7611-7616},
  title     = {GESCE: Graph-based ergodic search in cluttered environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantic belief behavior graph: Enabling autonomous robot
inspection in unknown environments. <em>IROS</em>, 7604–7610. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the problem of autonomous robotic inspection in complex and unknown environments. This capability is crucial for efficient and precise inspections in various real-world scenarios, even when faced with perceptual uncertainty and lack of prior knowledge of the environment. Existing methods for real-world autonomous inspections typically rely on predefined targets and waypoints and often fail to adapt to dynamic or unknown settings. In this paper, we introduce the Semantic Belief Behavior Graph (SB2G) framework as a new approach to semantic-aware autonomous robot inspection. SB2G generates a control policy for the robot, using behavior nodes that encapsulate various semantic-based policies designed for inspecting different classes of objects. We design an active semantic search behavior to guide the robot in locating objects for inspection while reducing semantic information uncertainty. The edges in the SB2G encode transitions between these behaviors. We validate our approach through simulation and real-world urban inspections using a legged robotic platform. Our results show that SB2G enables a more efficient object inspection policy, exhibiting similar behaviors comparable to human-operated inspections.},
  archive   = {C_IROS},
  author    = {Muhammad Fadhil Ginting and David D. Fan and Sung-Kyun Kim and Mykel J. Kochenderfer and Ali-Akbar Agha-Mohammadi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802336},
  month     = {10},
  pages     = {7604-7610},
  title     = {Semantic belief behavior graph: Enabling autonomous robot inspection in unknown environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interactive learning of physical object properties through
robot manipulation and database of object measurements. <em>IROS</em>,
7596–7603. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents a framework for automatically extracting physical object properties, such as material composition, mass, volume, and stiffness, through robot manipulation and a database of object measurements. The framework involves exploratory action selection to maximize learning about objects on a table. A Bayesian network models conditional dependencies between object properties, incorporating prior probability distributions and uncertainty associated with measurement actions. The algorithm selects optimal exploratory actions based on expected information gain and updates object properties through Bayesian inference. Experimental evaluation demonstrates effective action selection compared to a baseline and correct termination of the experiments if there is nothing more to be learned. The algorithm proved to behave intelligently when presented with trick objects with material properties in conflict with their appearance. The robot pipeline integrates with a logging module and an online database of objects, containing over 24,000 measurements of 63 objects with different grippers. All code and data are publicly available, facilitating automatic digitization of objects and their physical properties through exploratory manipulations.},
  archive   = {C_IROS},
  author    = {Andrej Kruzliak and Jiri Hartvich and Shubhan P. Patni and Lukas Rustler and Jan Kristof Behrens and Fares J. Abu-Dakka and Krystian Mikolajczyk and Ville Kyrki and Matej Hoffmann},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802249},
  month     = {10},
  pages     = {7596-7603},
  title     = {Interactive learning of physical object properties through robot manipulation and database of object measurements},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MV-ROPE: Multi-view constraints for robust category-level
object pose and size estimation. <em>IROS</em>, 7588–7595. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently there has been a growing interest in category-level object pose and size estimation, and prevailing methods commonly rely on single view RGB-D images. However, one disadvantage of such methods is that they require accurate depth maps which cannot be produced by consumer-grade sensors. Furthermore, many practical real-world situations involve a moving camera that continuously observes its surroundings, and the temporal information of the input video streams is simply overlooked by single-view methods. We propose a novel solution that makes use of RGB video streams. Our framework consists of three modules: a scale-aware monocular dense SLAM solution, a lightweight object pose predictor, and an object-level pose graph optimizer. The SLAM module utilizes a video stream and additional scale-sensitive readings to estimate camera poses and metric depth. The object pose predictor then generates canonical object representations from RGB images. The object pose is estimated through geometric registration of these canonical object representations with estimated object depth points. All per-view estimates finally undergo optimization within a pose graph, culminating in the output of robust and accurate canonical object poses. Our experimental results demonstrate that when utilizing public dataset sequences with high-quality depth information, the proposed method exhibits comparable performance to state-of-the-art RGB-D methods. We also collect and evaluate on new datasets containing depth maps of varying quality to further quantitatively benchmark the proposed method alongside previous RGB-D based methods. We demonstrate a significant advantage in scenarios where depth input is absent or the quality of depth sensing is limited.},
  archive   = {C_IROS},
  author    = {Jiaqi Yang and Yucong Chen and Xiangting Meng and Chenxin Yan and Min Li and Ran Cheng and Lige Liu and Tao Sun and Laurent Kneip},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801855},
  month     = {10},
  pages     = {7588-7595},
  title     = {MV-ROPE: Multi-view constraints for robust category-level object pose and size estimation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ManipVQA: Injecting robotic affordance and physically
grounded information into multi-modal large language models.
<em>IROS</em>, 7580–7587. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While the integration of Multi-modal Large Language Models (MLLMs) with robotic systems has significantly improved robots’ ability to understand and execute natural language instructions, their performance in manipulation tasks remains limited due to a lack of robotics-specific knowledge. Conventional MLLMs are typically trained on generic image-text pairs, leaving them deficient in understanding affordances and physical concepts crucial for manipulation. To address this gap, we propose ManipVQA, a novel framework that infuses MLLMs with manipulation-centric knowledge through a Visual Question-Answering (VQA) format. This approach encompasses tool detection, affordance recognition, and a broader understanding of physical concepts. We curated a diverse dataset of images depicting interactive objects, to challenge robotic understanding in tool detection, affordance prediction, and physical concept comprehension. To effectively integrate this robotics-specific knowledge with the inherent vision-reasoning capabilities of MLLMs, we leverage a unified VQA format and devise a fine-tuning strategy. This strategy preserves the original vision-reasoning abilities while incorporating the newly acquired robotic insights. Empirical evaluations conducted in robotic simulators and across various vision task benchmarks demonstrate the robust performance of ManipVQA. The code and dataset are publicly available at https://github.com/SiyuanHuang95/ManipVQA.},
  archive   = {C_IROS},
  author    = {Siyuan Huang and Iaroslav Ponomarenko and Zhengkai Jiang and Xiaoqi Li and Xiaobin Hu and Peng Gao and Hongsheng Li and Hao Dong},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801993},
  month     = {10},
  pages     = {7580-7587},
  title     = {ManipVQA: Injecting robotic affordance and physically grounded information into multi-modal large language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating perceptual uncertainty to predict robust motion
plans. <em>IROS</em>, 7573–7579. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A typical sense-plan-act robotics pipeline is brittle due to the inherent inaccuracies in the output of the sensing module and the lack of awareness of the planning module to those inaccuracies. This paper develops a framework to predict uncertainty estimates for neural network-based vision models used for state estimation in robotics pipelines. Our uncertainty estimates are based directly on the image observation data and are explicitly trained to model the error distribution on a held-out calibration set. We also demonstrate how predicted uncertainties can be used to select robust control strategies. We conduct experiments on the mobile manipulation problem of articulating everyday objects (e.g. opening a cupboard) and demonstrate the quality of estimated uncertainty and its downstream impact on robustness of inferred control strategies.},
  archive   = {C_IROS},
  author    = {Arjun Gupta and Michelle Zhang and Saurabh Gupta},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802308},
  month     = {10},
  pages     = {7573-7579},
  title     = {Estimating perceptual uncertainty to predict robust motion plans},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DITTO: Demonstration imitation by trajectory transformation.
<em>IROS</em>, 7565–7572. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Teaching robots new skills quickly and conveniently is crucial for the broader adoption of robotic systems. In this work, we address the problem of one-shot imitation from a single human demonstration, given by an RGB-D video recording. We propose a two-stage process. In the first stage we extract the demonstration trajectory offline. This entails segmenting manipulated objects and determining their relative motion in relation to secondary objects such as containers. In the online trajectory generation stage, we first re-detect all objects, then warp the demonstration trajectory to the current scene and execute it on the robot. To complete these steps, our method leverages several ancillary models, including those for segmentation, relative object pose estimation, and grasp prediction. We systematically evaluate different combinations of correspondence and re-detection methods to validate our design decision across a diverse range of tasks. Specifically, we collect and quantitatively test on demonstrations of ten different tasks including pick-and-place tasks as well as articulated object manipulation. Finally, we perform extensive evaluations on a real robot system to demonstrate the effectiveness and utility of our approach in real-world scenarios. We make the code publicly available at http://ditto.cs.uni-freiburg.de.},
  archive   = {C_IROS},
  author    = {Nick Heppert and Max Argus and Tim Welschehold and Thomas Brox and Abhinav Valada},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801982},
  month     = {10},
  pages     = {7565-7572},
  title     = {DITTO: Demonstration imitation by trajectory transformation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Potential field-based online path planning for robust cable
routing. <em>IROS</em>, 7558–7564. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper tackles the complex task of routing elastic deformable linear objects (DLOs) characterized by considerable stiffness, such as cables or hoses, which are already constrained at both ends. Specifically, a single arm robot is controlled to slide along the unknown contour of the cable, performing collision-free contour following, and to insert specific DLO segments into intermediate known clips. The contour following motion is executed avoiding both collisions with static obstacles and excessive deformation of the manipulated DLO. In particular, the path is defined considering an artificial potential field that is updated after each sliding motion along the DLO. This field accounts for static obstacles, the local cable shape (reconstructed using tactile sensors on the gripper fingertips) and the estimation of the global DLO shape obtained from a dynamic model of the DLO, accounting for the constraints imposed by the clips and the gripper. The proposed method is experimentally validated on an industrial robot executing cable routing in several DLO configurations.},
  archive   = {C_IROS},
  author    = {Andrea Monguzzi and Niccolò Mantegna and Andrea Maria Zanchettin and Paolo Rocco},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801463},
  month     = {10},
  pages     = {7558-7564},
  title     = {Potential field-based online path planning for robust cable routing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active pose refinement for textureless shiny objects using
the structured light camera. <em>IROS</em>, 7550–7557. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {6D pose estimation of textureless shiny objects has become an essential problem in many robotic applications. Many pose estimators require high-quality depth data, often measured by structured light cameras. However, when objects have shiny surfaces (e.g., metal parts), these cameras fail to sense complete depths from a single viewpoint due to the specular reflection, resulting in a significant drop in the final pose accuracy. To mitigate this issue, we present a complete active vision framework for 6D object pose refinement and next-best-view prediction. Specifically, we first develop an optimization-based pose refinement module for the structured light camera. Our system then selects the next best camera viewpoint to collect depth measurements by minimizing the predicted uncertainty of the object pose. Compared to previous approaches, we additionally predict measurement uncertainties of future viewpoints by online rendering, which significantly improves the next-best-view prediction performance. We test our method on the real-world ROBI dataset. The results show that our pose refinement module outperforms the traditional ICP-based approach when given the same input depth data, and our next-best-view strategy can achieve high object pose accuracy with significantly fewer viewpoints than the heuristic-based policies.},
  archive   = {C_IROS},
  author    = {Jun Yang and Jian Yao and Steven L. Waslander},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801300},
  month     = {10},
  pages     = {7550-7557},
  title     = {Active pose refinement for textureless shiny objects using the structured light camera},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning a pre-grasp manipulation policy to effectively
retrieve a target in dense clutter. <em>IROS</em>, 7543–7549. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic grasping of a target object in cluttered environments poses considerable challenges, often due to limited collision-free grasp affordances caused by the close proximity of other objects. To overcome this limitation, non-prehensile actions like pushing can be strategically employed to manipulate the environment and improve the chances of successful grasps. In this paper, we introduce a novel pre-grasp manipulation policy designed to efficiently retrieve a target object from dense clutter by leveraging pushing actions and considering the gripper’s kinematic capabilities to strategically position the target object within the gripper’s closing region for a secure grasp. Unlike conventional approaches, our policy incorporates sequential pushing, allowing the robot to make decisions while within the camera’s field of view without retracting to a home position, leading to significantly reduced execution time per action. Our policy, trained in simulation, seamlessly transfers to real-world scenarios. Extensive experimental evaluation demonstrates superior performance, faster completion times, and robust generalization to unseen objects compared to existing baselines.},
  archive   = {C_IROS},
  author    = {Marios Kiatos and Leonidas Koutras and Iason Sarantopoulos and Zoe Doulgeri},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801734},
  month     = {10},
  pages     = {7543-7549},
  title     = {Learning a pre-grasp manipulation policy to effectively retrieve a target in dense clutter},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SAID-NeRF: Segmentation-AIDed NeRF for depth completion of
transparent objects. <em>IROS</em>, 7535–7542. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Acquiring accurate depth information of transparent objects using off-the-shelf RGB-D cameras is a well-known challenge in Computer Vision and Robotics. Depth estimation/completion methods are typically employed and trained on datasets with quality depth labels acquired from either simulation, additional sensors or specialized data collection setups and known 3d models. However, acquiring reliable depth information for datasets at scale is not straightforward, limiting training scalability and generalization. Neural Radiance Fields (NeRFs) are learning-free approaches and have demonstrated wide success in novel view synthesis and shape recovery. However, heuristics and controlled environments (lights, backgrounds, etc) are often required to accurately capture specular surfaces. In this paper, we propose using Visual Foundation Models (VFMs) for segmentation in a zero-shot, label-free way to guide the NeRF reconstruction process for these objects via the simultaneous reconstruction of semantic fields and extensions to increase robustness. Our proposed method Segmentation-AIDed NeRF (SAID-NeRF) shows significant performance on depth completion datasets for transparent objects and robotic grasping.},
  archive   = {C_IROS},
  author    = {Avinash Ummadisingu and Jongkeum Choi and Koki Yamane and Shimpei Masuda and Naoki Fukaya and Kuniyuki Takahashi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801979},
  month     = {10},
  pages     = {7535-7542},
  title     = {SAID-NeRF: Segmentation-AIDed NeRF for depth completion of transparent objects},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3D affordance keypoint detection for robotic manipulation.
<em>IROS</em>, 7528–7534. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel approach for affordance-informed robotic manipulation by introducing 3D keypoints to enhance the understanding of object parts’ functionality. The proposed approach provides direct information about what the potential use of objects is, as well as guidance on where and how a manipulator should engage, whereas conventional methods treat affordance detection as a semantic segmentation task, focusing solely on answering the what question. To address this gap, we propose a Fusion-based Affordance Keypoint Network (FAKP-Net) by introducing 3D keypoint quadruplet that harnesses the synergistic potential of RGB and Depth image to provide information on execution position, direction, and extent. Benchmark testing demonstrates that FAKP-Net outperforms existing models by significant margins in affordance segmentation task and keypoint detection task. Real-world experiments also showcase the reliability of our method in accomplishing manipulation tasks with previously unseen objects. Our source code and video demo will be public.},
  archive   = {C_IROS},
  author    = {Zhiyang Liu and Ruiteng Zhao and Lei Zhou and Chengran Yuan and Yuwei Wu and Sheng Guo and Zhengshen Zhang and Chenchen Liu and Marcelo H. Ang and Francis EH Tay},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801792},
  month     = {10},
  pages     = {7528-7534},
  title     = {3D affordance keypoint detection for robotic manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A direct semi-exhaustive search method for robust,
partial-to-full point cloud registration. <em>IROS</em>, 7521–7527. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Point cloud registration refers to the problem of finding the rigid transformation that aligns two given point clouds, and is crucial for many applications in robotics and computer vision. The main insight of this paper is that we can directly optimize the point cloud registration problem without correspondences by utilizing an algorithmically simple, yet computationally complex, semi-exhaustive search approach that is very well-suited for parallelization on modern GPUs. Our proposed algorithm, Direct Semi-Exhaustive Search (DSES), iterates over potential rotation matrices and efficiently computes the inlier-maximizing translation associated with each rotation. It then computes the optimal rigid transformation based on any desired distance metric by directly computing the error associated with each transformation candidate {R,t}. By leveraging the parallelism of modern GPUs, DSES outperforms state-of-the-art methods for partial-to-full point cloud registration on the simulated ModelNet40 benchmark and demonstrates high performance and robustness for pose estimation on a real-world robotics problem (https://youtu.be/q0q2-s2KSuA).},
  archive   = {C_IROS},
  author    = {Richard Cheng and Chavdar Papozov and Dan Helmick and Mark Tjersland},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801518},
  month     = {10},
  pages     = {7521-7527},
  title     = {A direct semi-exhaustive search method for robust, partial-to-full point cloud registration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Raising body ownership in end-to-end visuomotor policy
learning via robot-centric pooling. <em>IROS</em>, 7514–7520. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present Robot-centric Pooling (RcP), a novel pooling method designed to enhance end-to-end visuomo-tor policies by enabling differentiation between the robots and similar entities or their surroundings. Given an image-proprioception pair, RcP guides the aggregation of image features by highlighting image regions correlating with the robot’s proprioceptive states, thereby extracting robot-centric image representations for policy learning. Leveraging contrastive learning techniques, RcP integrates seamlessly with existing visuomotor policy learning frameworks and is trained jointly with the policy using the same dataset, requiring no extra data collection involving self-distractors. We evaluate the proposed method with reaching tasks in both simulated and real-world settings. The results demonstrate that RcP significantly enhances the policies’ robustness against various unseen distractors, including self-distractors, positioned at different locations. Additionally, the inherent robot-centric characteristic of RcP enables the learnt policy to be far more resilient to aggressive pixel shifts compared to the baselines. Code available at: https://github.com/Zheyu-Zhuang/RcP},
  archive   = {C_IROS},
  author    = {Zheyu Zhuang and Ville Kyrki and Danica Kragic},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802462},
  month     = {10},
  pages     = {7514-7520},
  title     = {Raising body ownership in end-to-end visuomotor policy learning via robot-centric pooling},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OVGNet: A unified visual-linguistic framework for
open-vocabulary robotic grasping. <em>IROS</em>, 7507–7513. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recognizing and grasping novel-category objects remains a crucial yet challenging problem in real-world robotic applications. Despite its significance, limited research has been conducted in this specific domain. To address this, we seamlessly propose a novel framework that integrates open-vocabulary learning into the domain of robotic grasping, empowering robots with the capability to adeptly handle novel objects. Our contributions are threefold. Firstly, we present a large-scale benchmark dataset specifically tailored for evaluating the performance of open-vocabulary grasping tasks. Secondly, we propose a unified visual-linguistic framework that serves as a guide for robots in successfully grasping both base and novel objects. Thirdly, we introduce two alignment modules designed to enhance visual-linguistic perception in the robotic grasping process. Extensive experiments validate the efficacy and utility of our approach. Notably, our framework achieves an average accuracy of 71.2% and 64.4% on base and novel categories in our new dataset, respectively. Our code and dataset are available at https://github.com/cv516Buaa/OVGNet.},
  archive   = {C_IROS},
  author    = {Meng Li and Qi Zhao and Shuchang Lyu and Chunlei Wang and Yujing Ma and Guangliang Cheng and Chenguang Yang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802654},
  month     = {10},
  pages     = {7507-7513},
  title     = {OVGNet: A unified visual-linguistic framework for open-vocabulary robotic grasping},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HyperTaxel: Hyper-resolution for taxel-based tactile signals
through contrastive learning. <em>IROS</em>, 7499–7506. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To achieve dexterity comparable to that of humans, robots must intelligently process tactile sensor data. Taxel-based tactile signals often have low spatial-resolution, with non-standardized representations. In this paper, we propose a novel framework, HyperTaxel, for learning a geometrically-informed representation of taxel-based tactile signals to address challenges associated with their spatial resolution. We use this representation and a contrastive learning objective to encode and map sparse low-resolution taxel signals to high-resolution contact surfaces. To address the uncertainty inherent in these signals, we leverage joint probability distributions across multiple simultaneous contacts to improve taxel hyper-resolution. We evaluate our representation by comparing it with two baselines and present results that suggest our representation outperforms the baselines. Furthermore, we present qualitative results that demonstrate the learned representation captures the geometric features of the contact surface, such as flatness, curvature, and edges, and generalizes across different objects and sensor configurations. Moreover, we present results that suggest our representation improves the performance of various downstream tasks, such as surface classification, 6D in-hand pose estimation, and sim-to-real transfer.},
  archive   = {C_IROS},
  author    = {Hongyu Li and Snehal Dikhale and Jinda Cui and Soshi Iba and Nawid Jamali},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802001},
  month     = {10},
  pages     = {7499-7506},
  title     = {HyperTaxel: Hyper-resolution for taxel-based tactile signals through contrastive learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Language-driven grasp detection with mask-guided attention.
<em>IROS</em>, 7492–7498. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Grasp detection is an essential task in robotics with various industrial applications. However, traditional methods often struggle with occlusions and do not utilize language for grasping. Incorporating natural language into grasp detection remains a challenging task and largely unexplored. To address this gap, we propose a new method for language-driven grasp detection with mask-guided attention by utilizing the transformer attention mechanism with semantic segmentation features. Our approach integrates visual data, segmentation mask features, and natural language instructions, significantly improving grasp detection accuracy. Our work introduces a new framework for language-driven grasp detection, paving the way for language-driven robotic applications. Intensive experiments show that our method outperforms other recent baselines by a clear margin, with a 10.0% success score improvement. We further validate our method in real-world robotic experiments, confirming the effectiveness of our approach.},
  archive   = {C_IROS},
  author    = {Tuan Van Vo and Minh Nhat Vu and Baoru Huang and An Vuong and Ngan Le and Thieu Vo and Anh Nguyen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802256},
  month     = {10},
  pages     = {7492-7498},
  title     = {Language-driven grasp detection with mask-guided attention},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Precise pick-and-place using score-based diffusion networks.
<em>IROS</em>, 7484–7491. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel coarse-to-fine continuous pose diffusion method to enhance the precision of pick-and-place operations within robotic manipulation tasks. Leveraging the capabilities of diffusion networks, we facilitate the accurate perception of object poses. This accurate perception enhances both pick-and-place success rates and overall manipulation precision. Our methodology utilizes a top-down RGB image projected from an RGB-D camera and adopts a coarse-to-fine architecture. This architecture enables efficient learning of coarse and fine models. A distinguishing feature of our approach is its focus on continuous pose estimation, which enables more precise object manipulation, particularly concerning rotational angles. In addition, we employ pose and color augmentation techniques to enable effective training with limited data. Through extensive experiments in simulated and real-world scenarios, as well as an ablation study, we comprehensively evaluate our proposed methodology. Taken together, the findings validate its effectiveness in achieving high-precision pick-and-place tasks.},
  archive   = {C_IROS},
  author    = {Shih-Wei Guo and Tsu-Ching Hsiao and Yu-Lun Liu and Chun-Yi Lee},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801772},
  month     = {10},
  pages     = {7484-7491},
  title     = {Precise pick-and-place using score-based diffusion networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Steering decision transformers via temporal difference
learning. <em>IROS</em>, 7477–7483. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Decision Transformers (DTs) have been highly effective for offline reinforcement learning (RL) tasks, successfully modeling the sequences of actions in a given set of demonstrations. However, DTs may perform poorly in stochastic environments, which are prevalent in robotics scenarios. In this paper, we identify that the root cause of this performance degradation is the growing variance of returns-to-go, the signal used by DTs to predict actions, accumulated over the horizon. Building upon this insight, we propose an extension to DTs that allows them to be steered toward high-reward regions, where the expected returns are estimated using temporal difference learning. This way, we not only mitigate the growing variance problem but also eliminate the need for DTs to have access to returns-to-go during evaluation and deployment phases. We show that our method outperforms state-of-the-art offline RL methods in both simulated and real-world robotic arm environments.},
  archive   = {C_IROS},
  author    = {Hao-Lun Hsu and Alper Kamil Bozkurt and Juncheng Dong and Qitong Gao and Vahid Tarokh and Miroslav Pajic},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801303},
  month     = {10},
  pages     = {7477-7483},
  title     = {Steering decision transformers via temporal difference learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predictive coding for decision transformer. <em>IROS</em>,
7469–7476. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent work in offline reinforcement learning (RL) has demonstrated the effectiveness of formulating decision-making as return-conditioned supervised learning. Notably, the decision transformer (DT) architecture has shown promise across various domains. However, despite its initial success, DTs have underperformed on several challenging datasets in goal-conditioned RL. This limitation stems from the inefficiency of return conditioning for guiding policy learning, particularly in unstructured and suboptimal datasets, resulting in DTs failing to effectively learn temporal compositionality. Moreover, this problem might be further exacerbated in long-horizon sparse-reward tasks. To address this challenge, we propose the Predictive Coding for Decision Transformer (PCDT) framework, which leverages generalized future conditioning to enhance DT methods. PCDT utilizes an architecture that extends the DT framework, conditioned on predictive codings, enabling decision-making based on both past and future factors, thereby improving generalization. Through extensive experiments on eight datasets from the AntMaze and FrankaKitchen environments, our proposed method achieves performance on par with or surpassing existing popular value-based and transformer-based methods in offline goal-conditioned RL. Furthermore, we also evaluate our method on a goal-reaching task with a physical robot.},
  archive   = {C_IROS},
  author    = {Tung M. Luu and Donghoon Lee and Chang D. Yoo},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802437},
  month     = {10},
  pages     = {7469-7476},
  title     = {Predictive coding for decision transformer},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Off-dynamics conditional diffusion planners. <em>IROS</em>,
7461–7468. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Offline Reinforcement Learning (RL) offers an attractive alternative to interactive data acquisition by leveraging pre-existing datasets. However, its effectiveness hinges on the quantity and quality of the data samples. This work explores the use of more readily available, albeit off-dynamics datasets, to address the challenge of data scarcity in Offline RL. We propose a novel approach using conditional Diffusion Probabilistic Models (DPMs) to learn the joint distribution of the large-scale off-dynamics dataset and the limited target dataset. To enable the model to capture the underlying dynamics structure, we introduce two contexts for the conditional model: (1) a continuous dynamics score allows for partial overlap between trajectories from both datasets, providing the model with richer information; (2) an inverse-dynamics context guides the model to generate trajectories that adhere to the target environment’s dynamic constraints. Empirical results demonstrate that our method significantly outperforms several strong baselines. Ablation studies further reveal the critical role of each dynamics context. Additionally, our model demonstrates that by modifying the context, we can interpolate between source and target dynamics, making it more robust to subtle shifts in the environment.},
  archive   = {C_IROS},
  author    = {Wen Zheng Terence Ng and Jianda Chen and Tianwei Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802085},
  month     = {10},
  pages     = {7461-7468},
  title     = {Off-dynamics conditional diffusion planners},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Diff-control: A stateful diffusion-based policy for
imitation learning. <em>IROS</em>, 7453–7460. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While imitation learning provides a simple and effective framework for policy learning, acquiring consistent action during robot execution remains a challenging task. Existing approaches primarily focus on either modifying the action representation at data curation stage or altering the model itself, both of which do not fully address the scalability of consistent action generation. To overcome this limitation, we introduce the Diff-Control policy, which utilizes a diffusion-based model to learn action representation from a state-space modeling viewpoint. We demonstrate that diffusion-based policies can acquire statefulness through a Bayesian formulation facilitated by ControlNet, leading to improved robustness and success rates. Our experimental results demonstrate the significance of incorporating action statefulness in policy learning, where Diff-Control shows improved performance across various tasks. Specifically, Diff-Control achieves an average success rate of 72% and 84% on stateful and dynamic tasks, respectively. Notably, Diff-Control also shows consistent performance in the presence of perturbations, outperforming other state-of-the-art methods that falter under similar conditions. Project page: https://diff-control.github.io/},
  archive   = {C_IROS},
  author    = {Xiao Liu and Yifan Zhou and Fabian Weigend and Shubham Sonawani and Shuhei Ikemoto and Heni Ben Amor},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801557},
  month     = {10},
  pages     = {7453-7460},
  title     = {Diff-control: A stateful diffusion-based policy for imitation learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-fingered dragging of unknown objects and orientations
using distributed tactile information through vision-transformer and
LSTM. <em>IROS</em>, 7445–7452. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-fingered hands can be suitable for stable object manipulation. Furthermore, abundant tactile information can be acquired with multi-fingered hands, useful to recognize the object’s properties, which is beneficial to adapt the motion to the object. However, generating dexterous manipulation motions with multi-fingered hands with high density tactile sensors is challenging due to complex touch states. Hence, tasks that conventionally require a high level of active tactile sensing simultaneously with motion generation, such as pulling in the hand while recognizing the posture of an object are difficult to accomplish. In this letter, we propose a novel deep predictive learning approach using Vision-Transformer (ViT) and Long-Short Term Memory (LSTM). The ViT’s attention mechanism can spatially focus on specific fingers represented by distributed 3-axis tactile sensors (uSkin). The LSTM can preserve long time-series information of the manipulation which can realize changing the desired motion according to the initial touching position and orientation for the target object. Results showed that the ViT-LSTM is effective in performing adaptive finger movements according to the properties of the object, i.e. its hardness and relative posture.},
  archive   = {C_IROS},
  author    = {T. Ueno and S. Funabashi and H. Ito and A. Schmitz and S. Kulkarni and T. Ogata and S. Sugano},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802283},
  month     = {10},
  pages     = {7445-7452},
  title     = {Multi-fingered dragging of unknown objects and orientations using distributed tactile information through vision-transformer and LSTM},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fine-tuning the diffusion model and distilling informative
priors for sparse-view 3D reconstruction. <em>IROS</em>, 7437–7444. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D reconstruction methods such as Neural Radiance Fields (NeRFs) are capable of optimizing high-quality 3D representation from images. However, NeRF is limited by the requirement for a large number of multi-view images, making its application to real-world scenarios challenging. In this work, we propose a method that can reconstruct real-world scenes from a few input images and a simple text prompt. Specifically, we fine-tune a pretrained diffusion model to constrain its powerful priors to the visual inputs and generate 3D-aware images, leveraging the coarse renderings obtained from input images as the image condition, along with the text prompt as the text condition. Our fine-tuning method saves a significant amount of training time and GPU memory usage while also generating credible results. Moreover, to enable our method to have self-evaluation capabilities, we design a semantic switch to filter out generated images that do not match real scenes, ensuring that only informative priors from the fine-tuned diffusion model are distilled into the 3D model. The semantic switch we designed can be used as a plug-in and improve performance by 13%. We perform our approach on a real-world dataset and demonstrate competitive results compared to existing sparse-view 3D reconstruction methods. Please see our project page for more visualizations and code: https://bityia.github.io/FDfusion.},
  archive   = {C_IROS},
  author    = {Jiadong Tang and Yu Gao and Tianji Jiang and Yi Yang and Mengyin Fu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802155},
  month     = {10},
  pages     = {7437-7444},
  title     = {Fine-tuning the diffusion model and distilling informative priors for sparse-view 3D reconstruction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Temporal attention for cross-view sequential image
localization. <em>IROS</em>, 7429–7436. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a novel approach to enhancing cross-view localization, focusing on the fine-grained, sequential localization of street-view images within a single known satellite image patch, a significant departure from traditional one-to-one image retrieval methods. By expanding to sequential image fine-grained localization, our model, equipped with a novel Temporal Attention Module (TAM), leverages contextual information to significantly improve sequential image localization accuracy. Our method shows substantial reductions in both mean and median localization errors on the Cross-View Image Sequence (CVIS) dataset, outperforming current state-of-the-art single-image localization techniques. Additionally, by adapting the KITTI-CVL dataset into sequential image sets, we not only offer a more realistic dataset for future research but also demonstrate our model’s robust generalization capabilities across varying times and areas, evidenced by a 75.3% reduction in mean distance error in cross-view sequential image localization.},
  archive   = {C_IROS},
  author    = {Dong Yuan and Frederic Maire and Feras Dayoub},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802621},
  month     = {10},
  pages     = {7429-7436},
  title     = {Temporal attention for cross-view sequential image localization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A lightweight de-confounding transformer for image
captioning in wearable assistive navigation device. <em>IROS</em>,
7422–7428. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image captioning is a multi-modal task that enables the transformation from scene images to natural language, providing valuable insights for visually impaired individuals to understand their environment. Therefore, its application to wearable navigation devices for visually impaired individuals holds immense potential. However, in practical applications, confusion between scene visuals and semantics, coupled with model complexity, often leads to performance degradation, resulting in inaccurate environmental interpretation. In light of this, we introduce a Lightweight De-confounding Transformer Network (LDTNet) for image captioning equipped with a Causal Adjustment module to eliminate confounders. Moreover, we design a Suppression Gate Unit that efficiently integrates fine-grained information from shallow features, while reducing the number of network layers to have a lightweight model. Experimental results demonstrate that our approach not only addresses the visual-semantic confusion issue effectively but also improves the response speed of wearable devices in comparison with the state of the art. Twenty volunteers are recruited to evaluate LDTNet’s efficacy in real-world settings in terms of both response speed and generated outputs by wearing the resulting assistive navigation devices. The outcomes well show its outstanding performance and great potential for visualy impaired individuals to use.},
  archive   = {C_IROS},
  author    = {Zhengcai Cao and Ji Xia and Yinbin Shi and MengChu Zhou},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802814},
  month     = {10},
  pages     = {7422-7428},
  title     = {A lightweight de-confounding transformer for image captioning in wearable assistive navigation device},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3DR-DIFF: Blind diffusion inpainting for 3D point cloud
reconstruction and segmentation. <em>IROS</em>, 7414–7421. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {LiDAR-based 3D perception is a focal point in autonomous vehicle research due to its efficacy in real-world environments and falling costs. However, recent research reveals challenges with LiDAR sensing under corruptions that occur due to adverse weather conditions and sensor-level errors, known as common corruptions. In particular, the majority of these corruptions lead to sparsity or noise in LiDAR point clouds, degrading the performance of downstream perception tasks. To address this, we propose a blind inpainting method named 3DR-DIFF, utilizing diffusion networks to reconstruct and segment corrupted point clouds. 3DR-DIFF comprises two key components: a corrupted region prediction network, acting as a binary mask predictor, and a conditional diffusion network. The evaluation results demonstrate that the 3DR-DIFF is able to reconstruct the LiDAR samples with a depth error of less than 0.56 mean absolute error (MAE) and an intensity error of 0.02 MAE, along with an average segmentation performance of 0.43 mean intersection over union. Furthermore, benchmarking results highlight that 3DR-DIFF outperforms state-of-the-art methods in reconstructing LiDAR beam-missing scenarios, exhibiting an approximately 9.2% lower error for a degradation of 1 MAE.},
  archive   = {C_IROS},
  author    = {K. T. Yasas Mahima and Asanka G. Perera and Sreenatha Anavatti and Matt Garratt},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802338},
  month     = {10},
  pages     = {7414-7421},
  title     = {3DR-DIFF: Blind diffusion inpainting for 3D point cloud reconstruction and segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DiffusionNOCS: Managing symmetry and uncertainty in Sim2Real
multi-modal category-level pose estimation. <em>IROS</em>, 7406–7413.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the challenging problem of category-level pose estimation. Current state-of-the-art methods for this task face challenges when dealing with symmetric objects and when attempting to generalize to new environments solely through synthetic data training. In this work, we address these challenges by proposing a probabilistic model that relies on diffusion to estimate dense canonical maps crucial for recovering partial object shapes as well as establishing correspondences essential for pose estimation. Furthermore, we introduce critical components to enhance performance by leveraging the strength of the diffusion models with multi-modal input representations. We demonstrate the effectiveness of our method by testing it on a range of real datasets. Despite being trained solely on our generated synthetic data, our approach achieves state-of-the-art performance and unprecedented generalization qualities, outperforming baselines, even those specifically trained on the target domain. Our code and data for the generalization benchmark can be found at https://woven-planet.github.io/DiffusionNOCS/.},
  archive   = {C_IROS},
  author    = {Takuya Ikeda and Sergey Zakharov and Tianyi Ko and Muhammad Zubair Irshad and Robert Lee and Katherine Liu and Rares Ambrus and Koichi Nishiwaki},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802487},
  month     = {10},
  pages     = {7406-7413},
  title     = {DiffusionNOCS: Managing symmetry and uncertainty in Sim2Real multi-modal category-level pose estimation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lightweight fisheye object detection network with
transformer-based feature enhancement for autonomous driving.
<em>IROS</em>, 7399–7405. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fisheye cameras, offering a wide field of view (FOV) of 360◦, are extensively employed for surround-view perception in autonomous driving. Compared with the object detection on the standard images, it lacks studies for fisheye images. Moreover, efficient perception is crucial for autonomous vehicles with limited computational capability. In this work, we introduce a lightweight fisheye object detection network with transformer-based feature enhancement for autonomous driving. Specifically, we leverage ShuffleNet V2 as a feature extraction network to reduce computation complexity and develop a transformer-based feature enhancement module (TFEM) to integrate multi-level features. Notably, we observe that data augmentation methods like mix-up and mosaic, effective on standard images, do not yield positive results on fisheye images. The results on the WoodScape dataset demonstrate that our method can achieve better performance with fewer parameters and floating-point operations per second (FLOPs). Extending our evaluation to the Microsoft Common Objects in Context (MS COCO) dataset shows that the proposed method has excellent generalization capability.},
  archive   = {C_IROS},
  author    = {Hu Cao and Yanpeng Li and Yinlong Liu and Xinyi Li and Guang Chen and Alois Knoll},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802087},
  month     = {10},
  pages     = {7399-7405},
  title     = {Lightweight fisheye object detection network with transformer-based feature enhancement for autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How physics and background attributes impact video
transformers in robotic manipulation: A case study on planar pushing.
<em>IROS</em>, 7391–7398. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As model and dataset sizes continue to scale in robot learning, the need to understand how the composition and properties of a dataset affect model performance becomes increasingly urgent to ensure cost-effective data collection and model performance. In this work, we empirically investigate how physics attributes (color, friction coefficient, shape) and scene background characteristics, such as the complexity and dynamics of interactions with background objects, influence the performance of Video Transformers in predicting planar pushing trajectories. We investigate three primary questions: How do physics attributes and background scene characteristics influence model performance? What kind of changes in attributes are most detrimental to model generalization? What proportion of fine-tuning data is required to adapt models to novel scenarios? To facilitate this research, we present CloudGripper-Push-1K, a large real-world vision-based robot pushing dataset comprising 1278 hours and 460,000 videos of planar pushing interactions with objects with different physics and background attributes. We also propose Video Occlusion Transformer (VOT), a generic modular video-transformer-based trajectory prediction framework which features 3 choices of 2D-spatial encoders as the subject of our case study. The dataset and source code are available at https://cloudgripper.org.},
  archive   = {C_IROS},
  author    = {Shutong Jin and Ruiyu Wang and Muhammad Zahid and Florian T. Pokorny},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802583},
  month     = {10},
  pages     = {7391-7398},
  title     = {How physics and background attributes impact video transformers in robotic manipulation: A case study on planar pushing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SceneSense: Diffusion models for 3D occupancy synthesis from
partial observation. <em>IROS</em>, 7383–7390. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When exploring new areas, robotic systems generally exclusively plan and execute controls over geometry that has been directly measured. This planning paradigm can lead to unintuitive exploration or replanning latency when entering areas that were previous obstructed from view. To address this we present SceneSense, a real-time 3D diffusion model for synthesizing 3D occupancy information from partial observations that effectively predicts these occluded or out of view geometries for use in future planning and control frameworks. SceneSense uses a running occupancy map and a single RGB-D camera to generate predicted geometry around the platform at runtime, even when the geometry is occluded or out of view. Our architecture ensures that SceneSense never overwrites observed free or occupied space. By preserving the integrity of the observed map, SceneSense mitigates the risk of corrupting the observed space with generative predictions. While SceneSense is shown to operate well using a single RGB-D camera, the framework is flexible enough to extend to additional modalities. Unlike existing models that necessitate multiple views and offline scene synthesis, or are focused on filling gaps in observed data, our findings demonstrate that SceneSense is an effective approach to estimating unobserved local occupancy information at runtime. Local occupancy predictions from SceneSense are shown to better represent the ground truth occupancy distribution during the test exploration trajectories than the running occupancy map. The source code can be found on our website: https://arpg.github.io/scenesense/},
  archive   = {C_IROS},
  author    = {Alec Reed and Brendan Crowe and Doncey Albin and Lorin Achey and Bradley Hayes and Christoffer Heckman},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802589},
  month     = {10},
  pages     = {7383-7390},
  title     = {SceneSense: Diffusion models for 3D occupancy synthesis from partial observation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robot shape and location retention in video generation using
diffusion models. <em>IROS</em>, 7375–7382. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Diffusion models have marked a significant mile-stone in the enhancement of image and video generation technologies. However, generating videos that precisely retain the shape and location of moving objects such as robots remains a challenge. This paper presents diffusion models specifically tailored to generate videos that accurately maintain the shape and location of mobile robots. The proposed models incorporate techniques such as embedding accessible robot pose information and applying semantic mask regulation within the scalable and efficient ConvNext backbone network. These techniques are designed to refine intermediate outputs, therefore improving the retention performance of shape and location. Through extensive experimentation, our models have demonstrated notable improvements in maintaining the shape and location of different robots, as well as enhancing overall video generation quality, compared to the benchmark diffusion model. Codes will be open-sourced at: https://github.com/PengPaulWang/diffusion-robots.},
  archive   = {C_IROS},
  author    = {Peng Wang and Zhihao Guo and Abdul Latheef Sait and Minh Huy Pham},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802156},
  month     = {10},
  pages     = {7375-7382},
  title     = {Robot shape and location retention in video generation using diffusion models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Proposal and demonstration of a robot behavior planning
system utilizing video with open source models in real-world
environments. <em>IROS</em>, 7367–7374. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the field of robotics, researches have sought to control robots capable of dealing with a variety of environments and tasks generically, through the use of foundation models. Among these, the systems for robot behavior planning utilizing video have also been proposed. The system enables the generation of robot behaviors that are not dependent on specific environments or tasks. This is achieved by generating videos based on text input, which utilizes the vast knowledge inherent in the foundation models. Also, by using a visual interface such as video, it is possible to confirm the behavioral indicators on which the robot is operating. Although, there are few examples of research on robot behavior planning utilizing video. Previous studies have emphasized the verification of behavior generation utilizing video, with simplified object manipulation for testing on simulations. This is not enough to demonstrate the usefulness of robot behavior planning utilizing video in real-world environments. In addition, the systems from previous studies are not open, and such systems have not been sufficiently discussed. This paper attempts to construct robot behavior planning utilizing video as an open system, and to verify the validity of the behavior planning using actual machines. In this paper, we first focus on using Robotis&#39;s TURTLEBOT3 Waffle Pi and Mobile Manipulator(referred to as &quot;Waffle&quot;) to construct robot behavior planning system utilizing video. Second, we create planning videos targeting the pick-and-place motion using the proposed system, and control the arm part of Waffle in the actual machine verification. Finally, by comparing the target coordinates from the planning video with the coordinates observed from the actual machine, we can confirm whether it is possible to control Waffle as planned. Errors are calculated from the coordinate comparison, and the control is performed again. Based on the results, we verify whether the proposed system is useful for controlling robots in real-world environments.},
  archive   = {C_IROS},
  author    = {Yuki Akutsu and Takahiro Yoshida and Yuki Kato and Yuichiro Sueoka and Koichi Osuka},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801903},
  month     = {10},
  pages     = {7367-7374},
  title     = {Proposal and demonstration of a robot behavior planning system utilizing video with open source models in real-world environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Open6DOR: Benchmarking open-instruction 6-DoF object
rearrangement and a VLM-based approach. <em>IROS</em>, 7359–7366. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The integration of large-scale Vision-Language Models (VLMs) with embodied AI can greatly enhance the generalizability and the capacity to follow open instructions for robots. However, existing studies on object manipulation are not up to full consideration of the 6-DoF requirements, let alone establishing a comprehensive benchmark. In this paper, we propel the pioneer construction of the benchmark and approach for Open-instruction 6-DoF Object Rearrangement (Open6DOR). Specifically, we collect a synthetic dataset of 200+ objects and carefully design 5400+ Open6DOR tasks. These tasks are divided into the Position-track, Rotation-track, and 6-DoF-track for evaluating different embodied agents in predicting the positions and rotations of target objects.Besides, we also propose a VLM-based approach for Open6DOR, named Open6DOR-GPT, which empowers GPT-4V with 3D-awareness and simulation-assistance while exploiting its strengths in generalizability and instruction-following. We compare the existing embodied agents with our Open6DOR-GPT on the proposed Open6DOR benchmark and find that Open6DOR-GPT achieves the state-of-the-art performance. We further show the impressive performance of Open6DOR-GPT in diverse real-world experiments.},
  archive   = {C_IROS},
  author    = {Yufei Ding and Haoran Geng and Chaoyi Xu and Xiaomeng Fang and Jiazhao Zhang and Songlin Wei and Qiyu Dai and Zhizheng Zhang and He Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802733},
  month     = {10},
  pages     = {7359-7366},
  title     = {Open6DOR: Benchmarking open-instruction 6-DoF object rearrangement and a VLM-based approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging simulation-based model preconditions for fast
action parameter optimization with multiple models. <em>IROS</em>,
7351–7358. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Optimizing robotic action parameters is a significant challenge for manipulation tasks that demand high levels of precision and generalization. Using a model-based approach, the robot must quickly reason about the outcomes of different actions using a predictive model to find a set of parameters that will have the desired effect. The model may need to capture the behaviors of rigid and deformable objects, as well as objects of various shapes and sizes. Predictive models often need to trade-off speed for prediction accuracy and generalization. This paper proposes a framework that leverages the strengths of multiple predictive models, including analytical, learned, and simulation-based models, to enhance the efficiency and accuracy of action parameter optimization. Our approach uses Model Deviation Estimators (MDEs) to determine the most suitable predictive model for any given state-action parameters, allowing the robot to select models to make fast and precise predictions. We extend the MDE framework by not only learning sim-to-real MDEs, but also sim-to-sim MDEs. Our experiments show that these sim-to-sim MDEs provide significantly faster parameter optimization as well as a basis for efficiently learning sim-to-real MDEs through finetuning. The ease of collecting sim-to-sim training data also allows the robot to learn MDEs based directly on visual inputs and local material properties.},
  archive   = {C_IROS},
  author    = {M. Yunus Seker and Oliver Kroemer},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801532},
  month     = {10},
  pages     = {7351-7358},
  title     = {Leveraging simulation-based model preconditions for fast action parameter optimization with multiple models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Constrained 6-DoF grasp generation on complex shapes for
improved dual-arm manipulation. <em>IROS</em>, 7344–7350. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Efficiently generating grasp poses tailored to specific regions of an object is vital for various robotic manipulation tasks, especially in a dual-arm setup. This scenario presents a significant challenge due to the complex geometries involved, requiring a deep understanding of the local geometry to generate grasps efficiently on the specified constrained regions. Existing methods only explore settings involving table-top/small objects and require augmented datasets to train, limiting their performance on complex objects. We propose CGDF: Constrained Grasp Diffusion Fields, a diffusion-based grasp generative model that generalizes to objects with arbitrary geometries, as well as generates dense grasps on the target regions. CGDF uses a part-guided diffusion approach that enables it to get high sample efficiency in constrained grasping without explicitly training on massive constraint-augmented datasets. We provide qualitative and quantitative comparisons using analytical metrics and in simulation, in both unconstrained and constrained settings to show that our method can generalize to generate stable grasps on complex objects, especially useful for dual-arm manipulation settings, while existing methods struggle to do so. More results, code and an extended version of the paper can be found on the project page: https://constrained-grasp-diffusion.github.io/},
  archive   = {C_IROS},
  author    = {Gaurav Singh and Sanket Kalwar and Md Faizal Karim and Bipasha Sen and Nagamanikandan Govindan and Srinath Sridhar and K Madhava Krishna},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802268},
  month     = {10},
  pages     = {7344-7350},
  title     = {Constrained 6-DoF grasp generation on complex shapes for improved dual-arm manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gradual receptive expansion using vision transformer for
online 3D bin packing. <em>IROS</em>, 7338–7343. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The bin packing problem (BPP) is a challenging combinatorial optimization problem with a number of practical applications. This paper focuses on online 3D-BPP, where the packer makes immediate decisions for a loading position as items continually arrive. We propose a novel reinforcement learning algorithm, GREViT, which utilizes a vision transformer to tackle online 3D-BPP for the first time. By introducing the gradual receptive expansion technique, GREViT overcomes the limitations inherent in learning-based methods that only excel in their trained bins. As a result, GREViT surpasses existing BPP algorithms in packing ratio across various bin sizes. The effectiveness of GREViT in real-world scenarios is validated by its successful demonstrations using a real robot for solving 3D-BPP. The attached video demonstrates GREViT undertaking 3D-BPP in both simulated and real-world environments.},
  archive   = {C_IROS},
  author    = {Minjae Kang and Hogun Kee and Yoseph Park and Junseok Kim and Jaeyeon Jeong and Geunje Cheon and Jaewon Lee and Songhwai Oh},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801652},
  month     = {10},
  pages     = {7338-7343},
  title     = {Gradual receptive expansion using vision transformer for online 3D bin packing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A learning-based controller for multi-contact grasps on
unknown objects with a dexterous hand. <em>IROS</em>, 7331–7337. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing grasp controllers usually either only support finger-tip grasps or need explicit configuration of the inner forces. We propose a novel grasp controller that supports arbitrary grasp types, including power grasps with multi-contacts, while operating self-contained on before unseen objects. No detailed contact information is needed, but only a rough 3D model, e.g., reconstructed from a single depth image. First, the external wrench being applied to the object is estimated by using the measured torques at the joints. Then, the torques necessary to counteract the estimated wrench while keeping the object at its initial pose are predicted. The torques are commanded via desired joint angles to an underlying joint-level impedance controller. To reach real-time performance, we propose a learning-based approach that is based on a wrench estimator- and a torque predictor neural network. Both networks are trained in a supervised fashion using data generated via the analytical formulation of the controller. In an extensive simulation-based evaluation, we show that our controller is able to keep 83.1% of the tested grasps stable when applying external wrenches with up to 10 N. At the same time, we outperform the two tested baselines by being more efficient and inducing less involuntary object movement. Finally, we show that the controller also works on the real DLR-Hand II, reaching a cycle time of 6 ms. Website: aidx-lab.org/grasping},
  archive   = {C_IROS},
  author    = {Dominik Winkelbauer and Rudolph Triebel and Berthold Bäuml},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801894},
  month     = {10},
  pages     = {7331-7337},
  title     = {A learning-based controller for multi-contact grasps on unknown objects with a dexterous hand},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RT-grasp: Reasoning tuning robotic grasping via multi-modal
large language model. <em>IROS</em>, 7323–7330. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in Large Language Models (LLMs) have showcased their remarkable reasoning capabilities, making them influential across various fields. However, in robotics, their use has primarily been limited to manipulation planning tasks due to their inherent textual output. This paper addresses this limitation by investigating the potential of adopting the reasoning ability of LLMs for generating numerical predictions in robotics tasks, specifically for robotic grasping. We propose Reasoning Tuning, a novel method that integrates a reasoning phase before prediction during training, leveraging the extensive prior knowledge and advanced reasoning abilities of LLMs. This approach enables LLMs, notably with multi-modal capabilities, to generate accurate numerical outputs like grasp poses that are context-aware and adaptable through conversations. Additionally, we present the Reasoning Tuning VLM Grasp dataset, carefully curated to facilitate the adaptation of LLMs to robotic grasping. Extensive validation on both grasping datasets and real-world experiments underscores the adaptability of multi-modal LLMs for numerical prediction tasks in robotics. This not only expands their applicability but also bridges the gap between text-based planning and direct robot control, thereby maximizing the potential of LLMs in robotics. Our dataset will be released. More details and videos of this work are available on our project page: https://sites.google.com/view/rt-grasp.},
  archive   = {C_IROS},
  author    = {Jinxuan Xu and Shiyu Jin and Yutian Lei and Yuqian Zhang and Liangjun Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801718},
  month     = {10},
  pages     = {7323-7330},
  title     = {RT-grasp: Reasoning tuning robotic grasping via multi-modal large language model},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inverse kinematics for neuro-robotic grasping with humanoid
embodied agents. <em>IROS</em>, 7315–7322. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a novel zero-shot motion planning method that allows users to quickly design smooth robot motions in Cartesian space. A Bézier curve-based Cartesian plan is transformed into a joint space trajectory by our neuro-inspired inverse kinematics (IK) method CycleIK, for which we enable platform independence by scaling it to arbitrary robot designs. The motion planner is evaluated on the physical hardware of the two humanoid robots NICO and NICOL in a human-in-the-loop grasping scenario. Our method is deployed with an embodied agent that is a large language model (LLM) at its core. We generalize the embodied agent, that was introduced for NICOL, to also embody NICO. The agent can execute a discrete set of physical actions and allows the user to verbally instruct various different robots. We contribute a grasping primitive to its action space that allows for precise manipulation of household objects. The updated CycleIK2 method is compared to popular numerical IK solvers and state-of-the-art neural IK methods in simulation and is shown to be competitive with or outperform all evaluated methods when the algorithm runtime is very short. The grasping primitive is evaluated on both NICOL and NICO robots with a reported grasp success of 72% to 82% for each robot, respectively.},
  archive   = {C_IROS},
  author    = {Jan-Gerrit Habekost and Connor Gäde and Philipp Allgeuer and Stefan Wermter},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802010},
  month     = {10},
  pages     = {7315-7322},
  title     = {Inverse kinematics for neuro-robotic grasping with humanoid embodied agents},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SculptDiff: Learning robotic clay sculpting from humans with
goal conditioned diffusion policy. <em>IROS</em>, 7307–7314. (<a
href="https://doi.org/10.1109/IROS58592.2024.10803054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Manipulating deformable objects remains a challenge within robotics due to the difficulties of state estimation, long-horizon planning, and predicting how the object will deform given an interaction. These challenges are the most pronounced with 3D deformable objects. We propose SculptDiff, a goal-conditioned diffusion-based imitation learning framework that works with point cloud state observations to directly learn clay sculpting policies for a variety of target shapes. To the best of our knowledge this is the first real-world method that successfully learns manipulation policies for 3D deformable objects. For sculpting videos and access to our dataset and hardware CAD models, see the project website: https://sites.google.com/andrew.cmu.edu/imitation-sculpting/home},
  archive   = {C_IROS},
  author    = {Alison Bartsch and Arvind Car and Charlotte Avra and Amir Barati Farimani},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10803054},
  month     = {10},
  pages     = {7307-7314},
  title     = {SculptDiff: Learning robotic clay sculpting from humans with goal conditioned diffusion policy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforcement learning for active search and grasp in
clutter. <em>IROS</em>, 7301–7306. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an Active Search policy that balances between moving the camera and removing occluding objects to search for and retrieve a target object in clutter. While both types of action can reveal unobserved parts of a scene, they typically vary in execution complexity and time. Our proposed method explicitly reasons about the occluded spaces in the scene where the target object may be hidden, and uses reinforcement learning to compute the value of each action with the ultimate goal of finding and retrieving the target object in minimal time. Results in simulation and real-world experiments demonstrate a significant improvement in both task execution speed and success rate compared to baseline grasping strategies.},
  archive   = {C_IROS},
  author    = {Thomas Pitcher and Julian Förster and Jen Jen Chung},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801366},
  month     = {10},
  pages     = {7301-7306},
  title     = {Reinforcement learning for active search and grasp in clutter},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). GraspContrast: Self-supervised contrastive learning with
false negative elimination for 6-DoF grasp detection. <em>IROS</em>,
7294–7300. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic manipulation is a grand domain that primarily involves the use of robotic arms to interact with objects in the environment. While proposed methods have achieved advancements in grasping objects, they rely heavily on extensive training data that presents a significant challenge due to the labor-intensive process of human annotation. To address the issue, we propose GraspContrast, a self-supervised contrastive learning framework leveraging unlabeled RGB-D images to enhance point-wise feature representations for 6-DoF grasp detection. Our method designs a dual-branch network architecture to learn transformations that embed positive point pairs nearby, while pushing negative point pairs far apart. Specifically, we discuss a false negative elimination strategy to explicitly detect and remove the false negative samples that undesirably repel the point instances from the geometrically similar samples. Our method exhibits consistent improvements over existing learning-based grasp detection methods on both the GraspNet-1B benchmark and physical UR10e platform. These significant performance gains demonstrate the effectiveness of our proposed framework.},
  archive   = {C_IROS},
  author    = {Wenshuo Wang and Haiyue Zhu and Marcelo H. Ang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802171},
  month     = {10},
  pages     = {7294-7300},
  title     = {GraspContrast: Self-supervised contrastive learning with false negative elimination for 6-DoF grasp detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A contact model based on denoising diffusion to learn
variable impedance control for contact-rich manipulation. <em>IROS</em>,
7286–7293. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, a novel approach is proposed for learning robot control in contact-rich tasks such as wiping, by developing Diffusion Contact Model (DCM). Previous methods of learning such tasks relied on impedance control with time-varying stiffness tuning by performing Bayesian optimization by trial-and-error with robots. The proposed approach aims to reduce the cost of robot operation by predicting the robot contact trajectories from the variable stiffness inputs and using neural models. However, contact dynamics are inherently highly nonlinear, and their simulation requires iterative computations such as convex optimization. Moreover, approximating such computations by using finite-layer neural models is difficult. To overcome these limitations, the proposed DCM used the denoising diffusion models that could simulate the complex dynamics via iterative computations, thus improving the prediction accuracy. Stiffness tuning experiments conducted in simulated and real environments showed that the DCM achieved comparable performance to a conventional robot-based optimization method while reducing the number of robot trials.},
  archive   = {C_IROS},
  author    = {Masashi Okada and Mayumi Komatsu and Tadahiro Taniguchi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801976},
  month     = {10},
  pages     = {7286-7293},
  title     = {A contact model based on denoising diffusion to learn variable impedance control for contact-rich manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PreAfford: Universal affordance-based pre-grasping for
diverse objects and environments. <em>IROS</em>, 7278–7285. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic manipulation with two-finger grippers is challenged by objects lacking distinct graspable features. Traditional pre-grasping methods, which typically involve repositioning objects or utilizing external aids like table edges, are limited in their adaptability across different object categories and environments. To overcome these limitations, we introduce PreAfford, a novel pre-grasping planning framework incorporating a point-level affordance representation and a relay training approach. Our method significantly improves adaptability, allowing effective manipulation across a wide range of environments and object types. When evaluated on the ShapeNet-v2 dataset, PreAfford not only enhances grasping success rates by 69% but also demonstrates its practicality through successful real-world experiments. These improvements highlight PreAfford’s potential to redefine standards for robotic handling of complex manipulation tasks in diverse settings.},
  archive   = {C_IROS},
  author    = {Kairui Ding and Boyuan Chen and Ruihai Wu and Yuyang Li and Zongzheng Zhang and Huan-Ang Gao and Siqi Li and Guyue Zhou and Yixin Zhu and Hao Dong and Hao Zhao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802523},
  month     = {10},
  pages     = {7278-7285},
  title     = {PreAfford: Universal affordance-based pre-grasping for diverse objects and environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). RPMArt: Towards robust perception and manipulation for
articulated objects. <em>IROS</em>, 7270–7277. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Articulated objects are commonly found in daily life. It is essential that robots can exhibit robust perception and manipulation skills for articulated objects in real-world robotic applications. However, existing methods for articulated objects insufficiently address noise in point clouds and struggle to bridge the gap between simulation and reality, thus limiting the practical deployment in real-world scenarios. To tackle these challenges, we propose a framework towards Robust Perception and Manipulation for Articulated Objects (RPMArt), which learns to estimate the articulation parameters and manipulate the articulation part from the noisy point cloud. Our primary contribution is a Robust Articulation Network (RoArtNet) that is able to predict both joint parameters and affordable points robustly by local feature learning and point tuple voting. Moreover, we introduce an articulation-aware classification scheme to enhance its ability for sim-to-real transfer. Finally, with the estimated affordable point and articulation joint constraint, the robot can generate robust actions to manipulate articulated objects. After learning only from synthetic data, RPMArt is able to transfer zero-shot to real-world articulated objects. Experimental results confirm our approach’s effectiveness, with our framework achieving state-of-the-art performance in both noise-added simulation and real-world environments. Code, data and more results can be found on the project website at https://r-pmart.github.io.},
  archive   = {C_IROS},
  author    = {Junbo Wang and Wenhai Liu and Qiaojun Yu and Yang You and Liu Liu and Weiming Wang and Cewu Lu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802368},
  month     = {10},
  pages     = {7270-7277},
  title     = {RPMArt: Towards robust perception and manipulation for articulated objects},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Behavior-actor: Behavioral decomposition and
efficient-training for robotic manipulation. <em>IROS</em>, 7264–7269.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Language-conditioned Robotic manipulation demonstrates great potential in tackling various tasks. However, the generalization ability of this technique to unseen commands remains a challenge. Moreover, existing methods suffer from the burdensome overhead of data collection costs. Nowadays, Large language models (LLMs) have demonstrated impressive natural language understanding capabilities. In this work, we propose a novel scheme called Behavior-Actor(BehAct), which leverages the power of LLM to decompose language commands into executable behaviors in Retrieval-Augmented Generation(RAG) manner. A End-to-End actor is then trained to execute these identified behaviors. BehAct’s LLM acts as a &quot;brain&quot;, while the actor acts as a &quot;hand&quot;. A single actor model is trained from scratch on 11 real-world tasks, 40 behaviors using 276 demonstrations, only 7 for each behavior in average. We achieve a 68% average success rate on seen commands, which aligns comparably with recent works. Moreover, BehAct exhibits an impressive 45% average success rate on unseen commands, doubling the performance of the baseline approach. In the BehAct system, LLM-agnostic design enables flexibility in leveraging advanced LLMs without necessitating fine-tuning. Our code has been made publicly available here.},
  archive   = {C_IROS},
  author    = {Wenyi Jiang and Baowei Xv and Zhihao Cui},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801542},
  month     = {10},
  pages     = {7264-7269},
  title     = {Behavior-actor: Behavioral decomposition and efficient-training for robotic manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OPG-policy: Occluded push-grasp policy learning with amodal
segmentation. <em>IROS</em>, 7257–7263. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Goal-oriented grasping in dense clutter, a fundamental challenge in robotics, demands an adaptive policy to handle occluded target objects and diverse configurations. Previous methods typically learn policies based on partially observable segments of the occluded target to generate motions. However, these policies often struggle to generate optimal motions due to uncertainties regarding the invisible portions of different occluded target objects across various scenes, resulting in low motion efficiency. To this end, we propose OPG-Policy, a novel framework that leverages amodal segmentation to predict occluded portions of the target and develop an adaptive push-grasp policy for cluttered scenarios where the target object is partially observed. Specifically, our approach trains a dedicated amodal segmentation module for diverse target objects to generate amodal masks. These masks and scene observations are mapped to the future rewards of grasp and push motion primitives via deep Q-learning to learn the motion critic. Afterward, the push and grasp motion candidates predicted by the critic, along with the relevant domain knowledge, are fed into the coordinator to generate the optimal motion implemented by the robot. Extensive experiments conducted in both simulated and real-world environments demonstrate the effectiveness of our approach in generating motion sequences for retrieving occluded targets, outperforming other baseline methods in success rate and motion efficiency.},
  archive   = {C_IROS},
  author    = {Hao Ding and Yiming Zeng and Zhaoliang Wan and Hui Cheng},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802573},
  month     = {10},
  pages     = {7257-7263},
  title     = {OPG-policy: Occluded push-grasp policy learning with amodal segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Depth helps: Improving pre-trained RGB-based policy with
depth information injection. <em>IROS</em>, 7251–7256. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D perception ability is crucial for generalizable robotic manipulation. While recent foundation models have made significant strides in perception and decision-making with RGB-based input, their lack of 3D perception limits their effectiveness in fine-grained robotic manipulation tasks. To address these limitations, we propose a Depth Information Injection (DI2) framework that leverages the RGB-Depth modality for policy fine-tuning, while relying solely on RGB images for robust and efficient deployment. Concretely, we introduce the Depth Completion Module (DCM) to extract the spatial prior knowledge related to depth information and generate virtual depth information from RGB inputs to aid policy deployment. Further, we propose the Depth-Aware Codebook (DAC) to eliminate noise and reduce the cumulative error from the depth prediction. In the inference phase, this framework employs RGB inputs and accurately predicted depth data to generate the manipulation action. We conduct experiments on simulated LIBERO environments and real-world scenarios, and the experiment results prove that our method could effectively enhance the pre-trained RGB-based policy with 3D perception ability for robotic manipulation. The website is released at https://gewu-lab.github.io/DepthHelps-IROS2024.},
  archive   = {C_IROS},
  author    = {Xincheng Pang and Wenke Xia and Zhigang Wang and Bin Zhao and Di Hu and Dong Wang and Xuelong Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802706},
  month     = {10},
  pages     = {7251-7256},
  title     = {Depth helps: Improving pre-trained RGB-based policy with depth information injection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visuo-tactile zero-shot object recognition with
vision-language model. <em>IROS</em>, 7243–7250. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tactile perception is vital, especially when distinguishing visually similar objects. We propose an approach to incorporate tactile data into a Vision-Language Model (VLM) for visuo-tactile zero-shot object recognition. Our approach leverages the zero-shot capability of VLMs to infer tactile properties from the names of tactilely similar objects. The proposed method translates tactile data into a textual description solely by annotating object names for each tactile sequence during training, making it adaptable to various contexts with low training costs. The proposed method was evaluated on the FoodReplica and Cube datasets, demonstrating its effectiveness in recognizing objects that are difficult to distinguish by vision alone.},
  archive   = {C_IROS},
  author    = {Shiori Ueda and Atsushi Hashimoto and Masashi Hamaya and Kazutoshi Tanaka and Hideo Saito},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801766},
  month     = {10},
  pages     = {7243-7250},
  title     = {Visuo-tactile zero-shot object recognition with vision-language model},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OBHMR: Robust partial-to-full generalized point set
registration with overlap-guided bidirectional hybrid mixture model.
<em>IROS</em>, 7235–7242. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce a novel overlap-based bidirectional point set registration approach, i.e., Overlap-guided Bidirectional Hybrid Mixture Registration (OBHMR), which incorporates geometric information (i.e., normal vectors) in both the correspondence and transformation stages and formulates the optimization objective of registration in a bidirectional manner. More importantly, to address the issue of partial-to-full registration, OBHMR utilises the predicted point-wise overlap score using networks to formulate the overlap-guided Hybrid Mixture Model consisting of the Gaussian Mixture Model (GMM) and Fisher Mixture Model (FMM). OBHMR contains four components: (1) the overlap-guided correspondence network that estimates the correspondence probabilities and calculates the point-wise overlap score; (2) the learning posterior module that estimates the overlap-guided HMM parameters; (3) the transformation module that computes the rigid transformation by formulating the optimisation objective in a bidirectional registration way, given correspondences and overlap-guided HMM parameters. Experiments using 1457 human femur and 1301 human hip models demonstrate significant improvements in partial-to-full registration performance (p &lt; 0.01) under different overlapping ratios, compared to state-of-the-art registration approaches. Furthermore, individual contributions of three modules (i.e., additional normal vectors, overlap score estimation module and the bidirectional mechanism) in OBHMR have been validated in ablation studies. The results demonstrate OBHMR’s capability of tackling the challenging partial-to-full registration problems in computer-assisted orthopedic surgery. The codes are available at https://github.com/Dxinz/DeepOBHMR.},
  archive   = {C_IROS},
  author    = {Xinzhe Du and Zhengyan Zhang and Ang Zhang and Rui Song and Yibin Li and Max Q.-H. Meng and Zhe Min},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801428},
  month     = {10},
  pages     = {7235-7242},
  title     = {OBHMR: Robust partial-to-full generalized point set registration with overlap-guided bidirectional hybrid mixture model},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tracking tumors under deformation from partial point clouds
using occupancy networks. <em>IROS</em>, 7227–7234. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To track tumors during surgery, information from preoperative CT scans is used to determine their position. However, as the surgeon operates, the tumor may be deformed which presents a major hurdle for accurately resecting the tumor, and can lead to surgical inaccuracy, increased operation time, and excessive margins. This issue is particularly pronounced in robot-assisted partial nephrectomy (RAPN), where the kidney undergoes significant deformations during operation. Toward addressing this, we introduce a occupancy network-based method for the localization of tumors within kidney phantoms undergoing deformations at interactive speeds. We validate our method by introducing a 3D hydrogel kidney phantom embedded with exophytic and endophytic renal tumors. It closely mimics real tissue mechanics to simulate kidney deformation during in vivo surgery, providing excellent contrast and clear delineation of tumor margins to enable automatic threshold-based segmentation. Our findings indicate that the proposed method can localize tumors in moderately deforming kidneys with a margin of 6mm to 10mm, while providing essential volumetric 3D information at over 60Hz. This capability directly enables downstream tasks such as robotic resection.},
  archive   = {C_IROS},
  author    = {Pit Henrich and Jiawei Liu and Jiawei Ge and Samuel Schmidgall and Lauren Shepard and Ahmed Ezzat Ghazi and Franziska Mathis-Ullrich and Axel Krieger},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802114},
  month     = {10},
  pages     = {7227-7234},
  title     = {Tracking tumors under deformation from partial point clouds using occupancy networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Shadow maintenance for automatic light-probe control in
ophthalmic surgeries using only 2D information. <em>IROS</em>,
7220–7226. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In ophthalmic surgeries, the light probe is responsible for providing safe intraocular illumination and ensuring the visibility of the instrument and its shadow as the only available reference for qualitative depth estimation and landing point prediction in fundus microscopic images. To achieve sustainable shadow-based estimation during surgeries, we propose controlling the light probe automatically to limit the shadow position around the instrument tip using only 2D information from the microscope. We also integrate an intensity balancing sub-module to guarantee the normal intensity distribution and the safe depth of light-tip placement. Without motor-based pose coordination between the light probe and the instrument, experiments analyze the performance of our image-based shadow maintenance with only image information under the constraints of RCM and discuss the working volume and segmentation limitations during simulation and real-robot tests.},
  archive   = {C_IROS},
  author    = {Junjie Yang and Satoshi Inagaki and Zhihao Zhao and Daniel Zapp and Mathias Maier and Kai Huang and Nassir Navab and M. Ali Nasseri},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802617},
  month     = {10},
  pages     = {7220-7226},
  title     = {Shadow maintenance for automatic light-probe control in ophthalmic surgeries using only 2D information},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vertebrae-based global x-ray to CT registration for thoracic
surgeries. <em>IROS</em>, 7212–7219. (<a
href="https://doi.org/10.1109/IROS58592.2024.10803049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {X-ray to CT registration is an essential technique to provide on-site guidance for clinicians and medical robots by aligning preoperative information with intraoperative images. Current methods focus on local registration with small capture ranges and necessitate a manual initial alignment before precise registration. Some existing global methods are likely to fail in thoracic surgeries because of the respiratory motion and the nearly colinear nature of vertebrae landmarks. In this study, we propose a vertebrae-based global X-ray to CT registration method with the assistance of clinical setups for thoracic surgeries. Firstly, vertebrae centroids are automatically localized by CNN-based networks in CT and X-ray for establishing 2D/3-D correspondences. Then, inspired by clinical setup, we address the degradation of colinear landmarks of 6-DoF pose estimation by introducing a 4-DoF solver. Considering the inaccurate priori and landmark mislocalization, the solver is embedded into the Adaptive Error-Aware Estimator (AE2) to simultaneously estimate weights and aggregate candidate poses. Finally, the whole method is trained in an end-to-end manner for better performance. Evaluations on both the public LIDC-IDRI dataset and clinical dataset demonstrate that our method outperforms existing optimization-based and learningbased approaches in terms of registration accuracy and success rate. Our code: https://github.com/LiuLiluZJU/2P-AE2},
  archive   = {C_IROS},
  author    = {Lilu Liu and Yanmei Jiao and Zhou An and Honghai Ma and Chunlin Zhou and Haojian Lu and Jian Hu and Rong Xiong and Yue Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10803049},
  month     = {10},
  pages     = {7212-7219},
  title     = {Vertebrae-based global X-ray to CT registration for thoracic surgeries},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bidirectional partial-to-full non-rigid point set
registration with non-overlapping filtering. <em>IROS</em>, 7204–7211.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce Bidirectional Non-Overlapping Filtering Network (Bi-NOFNet), which registers the partial intraoperative point set with full preoperative point set for computer-assisted interventions (CAI). Our contributions are three-folds. First, Bi-NOFNet adopts customised feature extractor to extract distinctive features from both point sets, with which the per-point overlap mask is predicted and the overlapping region is segmented for the preoperative point set. Furthermore, we propose two methods to filter out the non-overlapping regions, at feature-level (i.e., Bi-NOFNet(Feature)) and point-level (i.e., Bi-NOFNet (Point)). For these two methods, we develop supervised registration strategy where the ground-truth overlap mask and displacement vectors are employed, and weakly-supervised registration strategies where only the ground-truth overlap mask is available. Additionally, to fully utilise the information in both space, we propose a bidirectional registration mechanism, which predicts the displacement vectors associated with the intraoperative point set (i.e., the forward way) and those warpping the preoperative point set (i.e., the backward way). Experiments have been conducted on the proposed DeformMedShapeNet dataset that contains 615 different liver shapes. Extensive results demonstrate that Bi-NOFNet performs well for partial-to-full registration tasks under various scenarios of noise, overlap ratios and deformation levels, outperforming existing non-rigid registration approaches.},
  archive   = {C_IROS},
  author    = {Hao Yu and Mingyang Liu and Rui Song and Yibin Li and Max Q.-H. Meng and Zhe Min},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801491},
  month     = {10},
  pages     = {7204-7211},
  title     = {Bidirectional partial-to-full non-rigid point set registration with non-overlapping filtering},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). REF2-NeRF: Reflection and refraction aware neural radiance
field. <em>IROS</em>, 7196–7203. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, significant progress has been made in the study of methods for 3D reconstruction from multiple images using implicit neural representations, exemplified by the neural radiance field (NeRF) method. Such methods, which are based on volume rendering, can model various light phenomena, and various extended methods have been proposed to accommodate different scenes and situations. However, when handling scenes with multiple glass objects, e.g., objects in a glass showcase, modeling the target scene accurately has been challenging due to the presence of multiple reflection and refraction effects. Thus, this paper proposes a NeRF-based modeling method for scenes containing a glass case. In the proposed method, refraction and reflection are modeled using elements that are dependent and independent of the viewer’s perspective. This approach allows us to estimate the surfaces where refraction occurs, i.e., glass surfaces, and enables the separation and modeling of both direct and reflected light components. The proposed method requires predetermined camera poses, but accurately estimating these poses in scenes with glass objects is difficult. Therefore, we used a robotic arm with an attached camera to acquire images with known poses. Compared to existing methods, the proposed method enables more accurate modeling of both glass refraction and the overall scene.},
  archive   = {C_IROS},
  author    = {Wooseok Kim and Taiki Fukiage and Takeshi Oishi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801830},
  month     = {10},
  pages     = {7196-7203},
  title     = {REF2-NeRF: Reflection and refraction aware neural radiance field},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards intelligent robotic sole deburring: From burrs
identification to path planning. <em>IROS</em>, 7189–7195. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Today, intelligent robotic manufacturing systems are reshaping the production industry. Using robots as actuators, multi-source sensors for perception, and Artificial Intelligence (AI) as decision-making systems, they can perform routine manufacturing tasks, surpassing the capabilities of traditional hard-programmed Computer Numerical Control (CNC) machinery. One specific challenge in footwear manufacturing is sole deburring, traditionally done manually by skilled workers. This paper focuses on developing a robust path-planning pipeline, comprising vision-based and Learning from Demonstrations (LfD) modules for autonomous deburring of soles. The vision-based module exploits Deep Learning (DL) techniques to handle key challenges such as precise segmentation of different soles types across diverse scenarios despite potential occlusions. Additionally, a novel method for burrs identification has been developed leveraging image processing and optimization techniques. Determining the optimal cutting tool orientation during sole deburring relies on human experience. The LfD module aims to impart this knowledge to the robot from videos of expert demonstrations, requiring adaptability to every new incoming sole that needs deburring. Experimental results showcase the method’s performance and flexibility, underlining the potential to advance the field of the proposed approach.},
  archive   = {C_IROS},
  author    = {Alessandra Tafuro and Luigi Cacciani and Andrea Maria Zanchettin and Paolo Rocco},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802795},
  month     = {10},
  pages     = {7189-7195},
  title     = {Towards intelligent robotic sole deburring: From burrs identification to path planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ParametricNet++: A 6DoF pose estimation network with sparse
keypoint recovery for parametric shapes in stacked scenarios.
<em>IROS</em>, 7181–7188. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most industrial parts are designed from parametric shapes with the properties of diversity and uncertainty. We propose a 6DoF pose estimation network, ParametricNet++, based on pointwise regression and sparse keypoint recovery, which is extended from ParametricNet to include optimizations of keypoint selection and prediction. Keypoint selection optimization selects geometrically unique keypoints and keypoint groups to reduce the difficulty of scene keypoint prediction and template keypoint recovery. Keypoint prediction optimization predicts keypoints from rough to precise, which improves the accuracy of scene keypoint prediction and template keypoint recovery. Compared with other state-of-the-art methods, the average of APs of ParametricNet++ is improved by over 15% on the public Siléane dataset, and the average of mAPs is improved by 12% and 14% on L-dataset and G-dataset from Parametric dataset, respectively. In particular, ParametricNet++ outperforms our original ParametricNet by 5% for both learning and generalization ability evaluation on the Parametric dataset. The experimental results demonstrate that ParametricNet++ lays a solid foundation for robot grasping in industrial scenarios.},
  archive   = {C_IROS},
  author    = {Yi Han Xie and Wei Jie Lv and Xin Yu Zhang and Yi Hong Chen and Long Zeng},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802005},
  month     = {10},
  pages     = {7181-7188},
  title     = {ParametricNet++: A 6DoF pose estimation network with sparse keypoint recovery for parametric shapes in stacked scenarios},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DuCAS: A knowledge-enhanced dual-hand compositional action
segmentation method for human-robot collaborative assembly.
<em>IROS</em>, 7175–7180. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recognising and tracking human actions from videos is crucial for human-robot collaborative assembly (HRCA). However, traditional action segmentation methods suffer from limited scene adaptability, partly because they conceptualise actions as unified verb-object entities with complete semantics. To overcome this, we propose a compositional action segmentation method. Following the human-robot shared assembly taxonomy, we deconstruct an assembly action into four elements: action verb, manipulated object, target object and tool. Our approach employs individual segmentation models for each action element, and then integrates general knowledge from large language models and domain-specific knowledge from predefined rules to form semantic-complete actions. Our method’s emphasis on general action elements and a modular design endows it with greater flexibility and adaptability than traditional approaches. Another attribute of our method is its capability to segment actions of each hand concurrently, facilitating more nuanced HRCA. Comparative experiments validate the superiority of our method over traditional action segmentation methods. More details can be found at https://github.com/LISMS-AKL-NZ/DuCAS.},
  archive   = {C_IROS},
  author    = {Hao Zheng and Regina Lee and Huachang Liang and Yuqian Lu and Xun Xu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802758},
  month     = {10},
  pages     = {7175-7180},
  title     = {DuCAS: A knowledge-enhanced dual-hand compositional action segmentation method for human-robot collaborative assembly},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PS6D: Point cloud based symmetry-aware 6D object pose
estimation in robot bin-picking. <em>IROS</em>, 7167–7174. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {6D object pose estimation holds essential roles in various fields, particularly in the grasping of industrial workpieces. Given challenges like rust, high reflectivity, and absent textures, this paper introduces a point cloud based pose estimation framework (PS6D). PS6D centers on slender and multi-symmetric objects. It extracts multi-scale features through an attention-guided feature extraction module, designs a symmetry-aware rotation loss and a center distance sensitive translation loss to regress the pose of each point to the centroid of the instance, and then uses a two-stage clustering method to complete instance segmentation and pose estimation. Objects from the Siléane and IPA datasets and typical workpieces from industrial practice are used to generate data and evaluate the algorithm. In comparison to the state-of-the-art approach, PS6D demonstrates an 11.5% improvement in ${{\text{F}}_{{1_{inst}}}}$ and a 14.8% improvement in Recall. The main part of PS6D has been deployed to the software of Mech-Mind, and achieves a 91.7% success rate in bin-picking experiments, marking its application in industrial pose estimation tasks.},
  archive   = {C_IROS},
  author    = {Yifan Yang and Zhihao Cui and Qianyi Zhang and Jingtai Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802046},
  month     = {10},
  pages     = {7167-7174},
  title     = {PS6D: Point cloud based symmetry-aware 6D object pose estimation in robot bin-picking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised monocular depth estimation with effective
feature fusion and self distillation. <em>IROS</em>, 7160–7166. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Monocular depth estimation obtaining scene depth information from a single image is an important task in the field of computer vision. Constrained by the limitations of convolutional networks in conducting long-distance modeling and the underutilization of datasets, the generalization of existing models is not satisfactory. In this paper, we propose an adaptive backbone named Internal Fusion Transformer to improve generalization ability compared to convolutional backbone, like HRNet, and a Bilateral Attention module which pays more attention to low-level semantic features compared to previous fuse methods. Meanwhile, we introduce three data augmentation methods, namely cropping-resizing (cr), cropping-shuffling (cs), and mirroring (mi), for self distillation, as well as discuss their contributions to model performance improvement. Our model is trained on the KITTI dataset, and without fine-tuning, tested on NYUv2 and Make3D datasets to confirm the generalization. The experimental results illustrate the effectiveness of our design. Our model also demonstrates better performance compared to other models on the KITTI dataset.},
  archive   = {C_IROS},
  author    = {Zhenfei Liu and Chengqun Song and Jun Cheng and Jiefu Luo and Xiaoyang Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802237},
  month     = {10},
  pages     = {7160-7166},
  title     = {Self-supervised monocular depth estimation with effective feature fusion and self distillation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active neural mapping at scale. <em>IROS</em>, 7152–7159.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a NeRF-based active mapping system that enables efficient and robust exploration of large-scale indoor environments. The key to our approach is the extraction of a generalized Voronoi graph (GVG) from the continually updated neural map, leading to the synergistic integration of scene geometry, appearance, topology, and uncertainty. Anchoring uncertain areas induced by the neural map to the vertices of GVG allows the exploration to undergo adaptive granularity along a safe path that traverses unknown areas efficiently. Harnessing a modern hybrid NeRF representation, the proposed system achieves competitive results in terms of reconstruction accuracy, coverage completeness, and exploration efficiency even when scaling up to large indoor environments. Extensive results at different scales validate the efficacy of the proposed system.},
  archive   = {C_IROS},
  author    = {Zijia Kuang and Zike Yan and Hao Zhao and Guyue Zhou and Hongbin Zha},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801913},
  month     = {10},
  pages     = {7152-7159},
  title     = {Active neural mapping at scale},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Finetuning pre-trained model with limited data for
LiDAR-based 3D object detection by bridging domain gaps. <em>IROS</em>,
7145–7151. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {LiDAR-based 3D object detectors have been largely utilized in various applications, including autonomous vehicles or mobile robots. However, LiDAR-based detectors often fail to adapt well to target domains with different sensor configurations (e.g., types of sensors, spatial resolution, or FOVs) and location shifts. Collecting and annotating datasets in a new setup is commonly required to reduce such gaps, but it is often expensive and time-consuming. Recent studies suggest that pre-trained backbones can be learned in a self-supervised manner with large-scale unlabeled LiDAR frames. However, despite their expressive representations, they remain challenging to generalize well without substantial amounts of data from the target domain. Thus, we propose a novel method, called Domain Adaptive Distill-Tuning (DADT), to adapt a pretrained model with limited target data (≈100 LiDAR frames), retaining its representation power and preventing it from overfitting. Specifically, we use regularizers to align object-level and context-level representations between the pre-trained and finetuned models in a teacher-student architecture. Our experiments with driving benchmarks, i.e., Waymo Open dataset and KITTI, confirm that our method effectively finetunes a pre-trained model, achieving significant gains in accuracy.},
  archive   = {C_IROS},
  author    = {Jiyun Jang and Mincheol Chang and Jongwon Park and Jinkyu Kim},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801433},
  month     = {10},
  pages     = {7145-7151},
  title     = {Finetuning pre-trained model with limited data for LiDAR-based 3D object detection by bridging domain gaps},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Coarse-to-fine detection of multiple seams for robotic
welding. <em>IROS</em>, 7138–7144. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Efficiently detecting target weld seams while ensuring sub-millimeter accuracy has always been an important challenge in autonomous welding, which has significant application in industrial practice. Previous works mostly focused on recognizing and localizing welding seams one by one, leading to inferior efficiency in modeling the workpiece. This paper proposes a novel framework capable of multiple weld seams extraction using both RGB images and 3D point clouds. The RGB image is used to obtain the region of interest by approximately localizing the weld seams, and the point cloud is used to achieve the fine-edge extraction of the weld seams within the region of interest using region growth. Our method is further accelerated by using a pre-trained deep learning model to ensure both efficiency and generalization ability. The proposed method was comprehensively tested on various workpieces featuring both linear and curved weld seams, as well as in physical experiment systems. The results showcase considerable potential for real-world industrial applications, emphasizing the method’s efficiency and effectiveness. Videos of the real-world experiments can be found at https://youtu.be/pq162HSP2D4.},
  archive   = {C_IROS},
  author    = {Pengkun Wei and Shuo Cheng and Dayou Li and Ran Song and Yipeng Zhang and Wei Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802546},
  month     = {10},
  pages     = {7138-7144},
  title     = {Coarse-to-fine detection of multiple seams for robotic welding},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CoBOS: Constraint-based online scheduler for human-robot
collaboration. <em>IROS</em>, 7131–7137. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Assembly processes involving humans and robots are challenging scenarios because the individual activities and access to shared workspace have to be coordinated. Fixed robot programs leave no room to diverge from a fixed protocol. Working on such a process can be stressful for the user and lead to ineffective behavior or failure. We propose a novel approach of online constraint-based scheduling in a reactive execution control framework facilitating behavior trees called CoBOS. This allows the robot to adapt to uncertain events such as delayed activity completions and activity selection (by the human). The user will experience less stress as the robotic coworkers adapt their behavior to best complement the human-selected activities to complete the common task. In addition to the improved working conditions, our algorithm leads to increased efficiency, even in highly uncertain scenarios. We evaluate our algorithm using a probabilistic simulation study with 56000 experiments. We outperform all other compared methods by a margin of 4−10%. Initial real robot experiments using a Franka Emika Panda robot and human tracking based on HTC Vive VR gloves look promising.},
  archive   = {C_IROS},
  author    = {Marina Ionova and Jan Kristof Behrens},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801885},
  month     = {10},
  pages     = {7131-7137},
  title     = {CoBOS: Constraint-based online scheduler for human-robot collaboration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and development of a work cell with a one-handed
soldering tool for enhanced human-robot collaboration. <em>IROS</em>,
7123–7130. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The challenge of human-robot collaboration, particularly in the context of enhancing the productivity of work processes, has been a pivotal area of research and development for many years. Despite significant advancements, there remains a substantial gap in the design of these systems to cater specifically to individuals with disabilities. This paper presents an innovative approach in assistive robotics, focusing on the development of a work cell designed to facilitate individuals with single-arm functionality. Through the integration of depth camera technology, machine learning algorithms, and Mediapipe human tracking, our system is capable of interpreting human intentions, thereby making interactions with robots more intuitive and effective. Central to our research is the design of a specialized workspace that assists in object handling and incorporates a fully functional One-Handed Soldering Tool, integrated within a robotic arm setup. This work cell is tailored for users with limited arm functionality, demonstrating the system’s versatility and providing invaluable insights into the practical implementation of applied robotics to bridge the theoretical and practical aspects of assistive technology.},
  archive   = {C_IROS},
  author    = {Natchanon Suppaadirek and Maximilien Sonnic and Raul Ariel Duran Jimenez and Tomohiro Shibata},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801987},
  month     = {10},
  pages     = {7123-7130},
  title     = {Design and development of a work cell with a one-handed soldering tool for enhanced human-robot collaboration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inferring belief states in partially-observable human-robot
teams. <em>IROS</em>, 7115–7122. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate the real-time estimation of human situation awareness using observations from a robot teammate with limited visibility. In human factors and human-autonomy teaming, it is recognized that individuals navigate their environments using an internal mental simulation, or mental model. The mental model informs cognitive processes including situation awareness, contextual reasoning, and task planning. In teaming domains, the mental model includes a team model of each teammate’s beliefs and capabilities, enabling fluent teamwork without the need for explicit communication. However, little work has applied team models to human-robot teaming. We compare the performance of two current methods at estimating user situation awareness over varying visibility conditions. Our results indicate that the methods are largely resilient to low-visibility conditions in our domain, however opportunities exist to improve their overall performance.},
  archive   = {C_IROS},
  author    = {Jack Kolb and Karen M. Feigh},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801316},
  month     = {10},
  pages     = {7115-7122},
  title     = {Inferring belief states in partially-observable human-robot teams},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SiSCo: Signal synthesis for effective human-robot
communication via large language models. <em>IROS</em>, 7107–7114. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Effective human-robot collaboration hinges on robust communication channels, with visual signaling playing a pivotal role due to its intuitive appeal. Yet, the creation of visually intuitive cues often demands extensive resources and specialized knowledge. The emergence of Large Language Models (LLMs) offers promising avenues for enhancing human-robot interactions and revolutionizing the way we generate context-aware visual cues. To this end, we introduce SiSCo–a novel framework that combines the computational power of LLMs with mixed-reality technologies to streamline the creation of visual cues for human-robot collaboration. Our results show that SiSCo improves the efficiency of communication in human-robot teaming tasks, reducing task completion time by approximately 73% and increasing task success rates by 18% compared to baseline natural language signals. Additionally, SiSCo reduces cognitive load for participants by 46%, as measured by the NASA-TLX subscale, and receives above-average user ratings for on-the-fly signals generated for unseen objects. To encourage further development and broader community engagement, we provide full access to SiSCo’s implementation and related materials on our GitHub repository.1},
  archive   = {C_IROS},
  author    = {Shubham Sonawani and Fabian Weigend and Heni Ben Amor},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802561},
  month     = {10},
  pages     = {7107-7114},
  title     = {SiSCo: Signal synthesis for effective human-robot communication via large language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GOMA: Proactive embodied cooperative communication via
goal-oriented mental alignment. <em>IROS</em>, 7099–7106. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Verbal communication plays a crucial role in human cooperation, particularly when the partners only have incomplete information about the task, environment, and each other’s mental state. In this paper, we propose a novel cooperative communication framework, Goal-Oriented Mental Alignment (GOMA). GOMA formulates verbal communication as a planning problem that minimizes the misalignment between the parts of agents’ mental states that are relevant to the goals. This approach enables an embodied assistant to reason about when and how to proactively initialize communication with humans verbally using natural language to help achieve better cooperation. We evaluate our approach against strong baselines in two challenging environments, Overcooked (a multiplayer game) and VirtualHome (a household simulator). Our experimental results demonstrate that large language models struggle with generating meaningful communication that is grounded in the social and physical context. In contrast, our approach can successfully generate concise verbal communication for the embodied assistant to effectively boost the performance of the cooperation as well as human users’ perception of the assistant.},
  archive   = {C_IROS},
  author    = {Lance Ying and Kunal Jha and Shivam Aarya and Joshua B. Tenenbaum and Antonio Torralba and Tianmin Shu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802144},
  month     = {10},
  pages     = {7099-7106},
  title     = {GOMA: Proactive embodied cooperative communication via goal-oriented mental alignment},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Open human-robot collaboration using decentralized inverse
reinforcement learning. <em>IROS</em>, 7092–7098. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The growing interest in human-robot collaboration (HRC), where humans and robots cooperate towards shared goals, has seen significant advancements over the past decade. While previous research has addressed various challenges, several key issues remain unresolved. Many domains within HRC involve activities that do not necessarily require human presence throughout the entire task. Existing literature typically models HRC as a closed system, where all agents are present for the entire duration of the task. In contrast, an open model offers flexibility by allowing an agent to enter and exit the collaboration as needed, enabling them to concurrently manage other tasks. In this paper, we introduce a novel multiagent framework called oDec-MDP, designed specifically to model open HRC scenarios where agents can join or leave tasks flexibly during execution. We generalize a recent multiagent inverse reinforcement learning method - Dec-AIRL to learn from open systems modeled using the oDec-MDP. Our method is validated through experiments conducted in both a simplified toy firefighting domain and a realistic dyadic human-robot collaborative assembly. Results show that our framework and learning method improves upon its closed system counterpart.},
  archive   = {C_IROS},
  author    = {Prasanth Sengadu Suresh and Siddarth Jain and Prashant Doshi and Diego Romeres},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801889},
  month     = {10},
  pages     = {7092-7098},
  title     = {Open human-robot collaboration using decentralized inverse reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DECAF: A discrete-event based collaborative human-robot
framework for furniture assembly. <em>IROS</em>, 7085–7091. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a task planning framework for collaborative Human-Robot scenarios, specifically focused on assembling complex systems such as furniture. The human is characterized as an uncontrollable agent, implying for example that the agent is not bound by a pre-established sequence of actions and instead acts according to its own preferences. Meanwhile, the task planner computes reactively the optimal actions for the collaborative robot to efficiently complete the entire assembly task in the least time possible.We formalize the problem as a Discrete Event Markov Decision Problem (DE-MDP), a comprehensive framework that incorporates a variety of asynchronous behaviors, human change of mind, and failure recovery as stochastic events. Although the problem could theoretically be addressed by constructing a graph of all possible actions, such an approach would be constrained by computational limitations. The proposed formulation offers an alternative solution utilizing Reinforcement Learning to derive an optimal policy for the robot. Experiments were conducted both in simulation and on a real system with human subjects assembling a chair in collaboration with a 7-DoF manipulator.},
  archive   = {C_IROS},
  author    = {Giulio Giacomuzzo and Matteo Terreran and Siddarth Jain and Diego Romeres},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802728},
  month     = {10},
  pages     = {7085-7091},
  title     = {DECAF: A discrete-event based collaborative human-robot framework for furniture assembly},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual attention based cognitive human–robot collaboration
for pedicle screw placement in robot-assisted orthopedic surgery.
<em>IROS</em>, 7078–7084. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current orthopedic robotic systems largely focus on navigation, aiding surgeons in positioning a guiding tube but still requiring manual drilling and screw placement. The automation of this task not only demands high precision and safety due to the intricate physical interactions between the surgical tool and bone but also poses significant risks when executed without adequate human oversight. As it involves continuous physical interaction, the robot should collaborate with the surgeon, understand the human intent, and always include the surgeon in the loop. To achieve this, this paper proposes a new cognitive human–robot collaboration framework, including the intuitive AR-haptic human–robot interface, the visual-attention-based surgeon model, and the shared interaction control scheme for the robot. User studies on a robotic platform for orthopedic surgery are presented to illustrate the performance of the proposed method. The results demonstrate that the proposed human– robot collaboration framework outperforms full robot and full human control in terms of safety and ergonomics.},
  archive   = {C_IROS},
  author    = {Chen Chen and Qikai Zou and Yuhang Song and Mingrui Yu and Senqiang Zhu and Shiji Song and Xiang Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801930},
  month     = {10},
  pages     = {7078-7084},
  title     = {Visual attention based cognitive Human–Robot collaboration for pedicle screw placement in robot-assisted orthopedic surgery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Collaborative conversation in safe multimodal human-robot
collaboration. <em>IROS</em>, 7071–7077. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the context of Human-Robot Collaboration (HRC), it is crucial that the two actors are able to communicate with each other in a natural and efficient manner. The absence of a communication interface is often a cause of undesired slowdowns. On one hand, this is because unforeseen events may occur, leading to errors. On the other hand, due to the close contact between humans and robots, the speed must be reduced significantly to comply with safety standard ISO/TS 15066. In this paper, we propose a novel architecture that enables operators and robots to communicate efficiently, emulating human-to-human dialogue, while addressing safety concerns. This approach aims to establish a communication framework that not only facilitates collaboration but also reduces undesired speed reduction. Through the use of a predictive simulator, we can anticipate safety-related limitations, ensuring smoother workflows, minimizing risks, and optimizing efficiency. The overall architecture has been validated with a UR10e and compared with a state of the art technique. The results show a significant improvement in user experience, with a corresponding 23% reduction in execution times and a 50% decrease in robot downtime.},
  archive   = {C_IROS},
  author    = {Davide Ferrari and Andrea Pupa and Cristian Secchi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802701},
  month     = {10},
  pages     = {7071-7077},
  title     = {Collaborative conversation in safe multimodal human-robot collaboration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RADAR: Robotics assembly by demonstration via augmented
reality. <em>IROS</em>, 7063–7070. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the widespread adoption of robots in high-mix, low-volume manufacturing, and the challenges posed by long-horizon assembly tasks, we introduce the RADAR system—an integrated human-robot collaboration system for Robotic Assembly by Demonstration via Augmented Reality. Existing frameworks lack a comprehensive, cross-task framework for effective assembly collaboration, limiting their applicability in complex tasks. We designed the RADAR system’s conceptual model, detailing its workflow and components. The system integrates human input into robotic metal beam assembly through augmented reality interactions and interfaces. We also developed a task planner that dynamically adjusts human-robot assembly tasks at coarse-fine resolutions. Validating through practical scenarios, particularly the RAMP assembly benchmark, showed that human involvement significantly enhances assembly precision and success rates, proving RADAR’s effectiveness and efficiency in human-robot collaborative assembly.},
  archive   = {C_IROS},
  author    = {Wenhao Yang and Shi Bai and Yunbo Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801493},
  month     = {10},
  pages     = {7063-7070},
  title     = {RADAR: Robotics assembly by demonstration via augmented reality},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DiaGBT: An explainable and evolvable robot control framework
using dialogue generative behavior trees. <em>IROS</em>, 7056–7062. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Manipulating robots using natural language is the preferred way for non-technical specialists. The challenge lies in reliability and adaptability especially when robots operate in unstructured surroundings. In this paper, we propose a novel framework called Dialogue Generative Behavior Trees (DiaGBT). Natural language instructions from human operators are transformed into behavior trees (BTs) and further executed by robots. Compared to the emerging Large Language Models (LLMs), DiaGBT is comparable in terms of semantic understanding but more lightweight, since the parsing rules are produced by LLM but tailored for task-correlated instructions. Besides, DiaGBT allows multi-round human-robot interaction, where robots learn reusable skills in real time. For evaluation, we generate a dataset with 4k instruction-BT pairs covering 4 different scenarios. On average, DiaGBT reaches over 90% parsability and 80% plausibility. Similar results on the VEIL-500 dataset outperform the current state of the art.},
  archive   = {C_IROS},
  author    = {Jinde Liang and Yuan Chang and Qian Wang and Yanzhen Wang and Xiaodong Yi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801472},
  month     = {10},
  pages     = {7056-7062},
  title     = {DiaGBT: An explainable and evolvable robot control framework using dialogue generative behavior trees},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). REPeat: A Real2Sim2Real approach for pre-acquisition of soft
food items in robot-assisted feeding. <em>IROS</em>, 7048–7055. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The paper presents REPeat, a Real2Sim2Real framework designed to enhance bite acquisition in robot-assisted feeding for soft foods. It uses ‘pre-acquisition actions’ such as pushing, cutting, and flipping to improve the success rate of bite acquisition actions such as skewering, scooping, and twirling. If the data-driven model predicts low success for direct bite acquisition, the system initiates a Real2Sim phase, reconstructing the food’s geometry in a simulation. The robot explores various pre-acquisition actions in the simulation, then a Sim2Real step renders a photorealistic image to reassess success rates. If the success improves, the robot applies the action in reality. We evaluate the system on 15 diverse plates with 10 types of food items for a soft food diet, showing improvement in bite acquisition success rates by 27% on average across all plates. See our project website at emprise.cs.cornell.edu/repeat.},
  archive   = {C_IROS},
  author    = {Nayoung Ha and Ruolin Ye and Ziang Liu and Shubhangi Sinha and Tapomayukh Bhattacharjee},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801368},
  month     = {10},
  pages     = {7048-7055},
  title     = {REPeat: A Real2Sim2Real approach for pre-acquisition of soft food items in robot-assisted feeding},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bridging the gap to natural language-based grasp predictions
through semantic information extraction. <em>IROS</em>, 7040–7047. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Enabling multi-fingered robots to choose an appropriate grasp on an object from natural language instructions poses great difficulties for such systems. The diversity, imprecision, and limited information contained in the language make this task particularly challenging. However, speech serves humans as a natural communication interface that can aid robots in adapting to the environment more easily. Therefore, providing robots with relevant data about the objects they interact with is essential for them to understand how to carry out object manipulation tasks. By leveraging Named Entity Recognition (NER) to automatically extract semantic data, our work introduces a novel approach to text-based grasp predictions. Our methodology involves a multistage learning approach using a semantic information extractor that provides significant features to a grasp prediction model. To assess the effectiveness of our approach, we conducted experiments on an existing corpus and two corpora generated by ChatGPT. Our results demonstrate superior performance compared to similar grasp prediction models while overcoming limitations in the literature. Additionally, we open-source our training data for reproducibility and future research advancement.},
  archive   = {C_IROS},
  author    = {Niko Kleer and Martin Feick and Amr Gomaa and Michael Feld and Antonio Krüger},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802058},
  month     = {10},
  pages     = {7040-7047},
  title     = {Bridging the gap to natural language-based grasp predictions through semantic information extraction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RobotGraffiti: An AR tool for semi-automated construction of
workcell models to optimize robot deployment. <em>IROS</em>, 7033–7039.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Improving robot deployment is a central step towards speeding up robot-based automation in manufacturing. A main challenge in robot deployment is how to best place the robot within the workcell. To tackle this challenge, we combine two knowledge sources: robotic knowledge of the system and workcell context awareness of the user, and intersect them with an Augmented Reality interface. RobotGraffiti is a unique tool that empowers the user in robot deployment tasks. One simply takes a 3D scan of the workcell with their mobile device, adds contextual data points that otherwise would be difficult to infer from the system, and receives a robot base position that satisfies the automation task. The proposed approach is an alternative to expensive and time-consuming digital twins, with a fast and easy-to-use tool that focuses on selected workcell features needed to run the placement optimization algorithm. The main contributions of this paper are the novel user interface for robot base placement data collection and a study comparing the traditional offline simulation with our proposed method. We showcase the method with a robot base placement solution and obtain up to 16 times reduction in time.},
  archive   = {C_IROS},
  author    = {Krzysztof Zieliński and Ryan Penning and Bruce Blumberg and Christian Schlette and Mikkel Baun Kjærgaard},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801402},
  month     = {10},
  pages     = {7033-7039},
  title     = {RobotGraffiti: An AR tool for semi-automated construction of workcell models to optimize robot deployment},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A voxel-enabled robotic assistant for omnidirectional
conveyance. <em>IROS</em>, 7027–7032. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Conventional bidirectional conveyance platforms use a flat translating belt or a series of spinning wheels or rollers to apply a shear force to payloads to move them. Wheel/roller-based conveyors in particular cannot double as a worktop when idle, do not support collision-free multi-object manipulation by default, and are not optimized to move objects that are either slippery or pliable—let alone both. This paper introduces a Voxel-Enabled Robotic Assistant (VERA), a network of intelligent table &quot;partitions&quot; whose topologically dynamic worktops enable omnidirectional conveyance; each partition is composed of a 2D array of &quot;quadrants,&quot; axisymmetric modules that can be hot-swapped for maintenance or repairs; each quadrant contains a 2D array of &quot;cells,&quot; unitary robotic submodules; each cell houses an independently controllable &quot;voxel,&quot; the motorized rotary element that conveys an overhead object. The efficacy of a VERA prototype was determined by evaluating waypoint error as a range of payloads were maneuvered between trajectory waypoints. By conveying both pliable and rigid payloads having slippery textures, the faceted voxels outperformed those augmented to mimic the circular-profiled wheels/rollers of competitor systems. VERA also successfully performed collision-free multi-object planar manipulations planned by its pathfinding algorithm. In light of these results, VERA emerges as a promising material handling platform for use in &quot;Future of Work&quot; settings as the need for multi-purpose collaborative industrial robots continues to grow.},
  archive   = {C_IROS},
  author    = {Michael Angelo Carvajal and Katiso Mabulu and Muneer Lalji and James Flanagan and Rui Luo and Samuel Hibbard and Tanav Chinthapatla and Rohan Bettadpur and Salah Bazzi and Mark Zolotas and Kristian Kloeckl and Taşkın Padır},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802540},
  month     = {10},
  pages     = {7027-7032},
  title     = {A voxel-enabled robotic assistant for omnidirectional conveyance},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pseudo-domain adversarial networks with electrical impedance
tomography for electrode offset error. <em>IROS</em>, 7020–7026. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper propose a novel transfer learning approach, Pseudo-Domain Adversarial Network (PDAN), to tackle the issue of electrode displacement in Electrical Impedance Tomography (EIT). Electrode displacement, caused by human movement or improper operation, significantly affects the accuracy of EIT by introducing data errors. Existing solutions either modify the electrode assembly at a high cost or employ recognition algorithms that require retraining from scratch. To overcome these limitations, our work leverages the power of transfer learning to enhance model performance in the target domain by utilizing knowledge from a related task in the source domain. PDAN extends the capabilities of deep adversarial learning by incorporating noisy images to simulate post-electrode rotation scenarios, aiding in the reduction of negative impacts caused by minor electrode displacements. Our method demonstrates superior performance in classifying leg posture data, achieving around 90% accuracy, and proving robust against sensor electrode offset. Experimental results across various datasets validate the effectiveness of PDAN, indicating its potential in addressing complex real-world situations with improved generalization capabilities.},
  archive   = {C_IROS},
  author    = {Gengchen Xu and Haofeng Chen and Xuanxuan Yang and Gang Ma and Xiaojie Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801895},
  month     = {10},
  pages     = {7020-7026},
  title     = {Pseudo-domain adversarial networks with electrical impedance tomography for electrode offset error},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Driving animatronic robot facial expression from speech.
<em>IROS</em>, 7012–7019. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Animatronic robots hold the promise of enabling natural human-robot interaction through lifelike facial expressions. However, generating realistic, speech-synchronized robot expressions poses significant challenges due to the complexities of facial biomechanics and the need for responsive motion synthesis. This paper introduces a novel, skinning-centric approach to drive animatronic robot facial expressions from speech input. At its core, the proposed approach employs linear blend skinning (LBS) as a unifying representation, guiding innovations in both embodiment design and motion synthesis. LBS informs the actuation topology, facilitates human expression retargeting, and enables efficient speech-driven facial motion generation. This approach demonstrates the capability to produce highly realistic facial expressions on an animatronic face in real-time at over 4000 fps on a single Nvidia RTX 4090, significantly advancing robots&#39; ability to replicate nuanced human expressions for natural interaction. To foster further research and development in this field, the code has been made publicly available at: https://github.com/library87/OpenRoboExp.},
  archive   = {C_IROS},
  author    = {Boren Li and Hang Li and Hangxin Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801970},
  month     = {10},
  pages     = {7012-7019},
  title     = {Driving animatronic robot facial expression from speech},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Miniaturisation and evaluation of the SoftSCREEN system in
colon phantoms. <em>IROS</em>, 7003–7010. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Screening of the lower gastrointestinal (GI) tract is of paramount importance for the early detection of precancerous lesions in the intestine, with an impact on reducing the high death rate of patients affected by cancer worldwide. Colonoscopy, i.e. standard procedure for screening the colon, is effective in reducing the incidence of colorectal cancer worldwide, nonetheless, this procedure remains an invasive method of screening, that typically causes discomfort and requires sedation for the patient. The SoftSCREEN system, a tethered robotic capsule designed for colonoscopy, aims to enable minimally invasive diagnosis of intestinal diseases through its innovative design that incorporates elastic tracks for locomotion and inflatable toroidal chambers for adaptable geometry to match the local lumen of the GI tract. After demonstrating the viability of the proposed design in a large-scale proof of concept in our previous work, the authors present here a miniaturised version of the SoftSCREEN system. We assess its performance in multiple phantom tests and evaluate the effect of pressure regulation on its locomotion. The conducted extensive tests demonstrate the capability of the soft robot to move inside intricate passages, capture internal images, and adjust its geometry to optimise traction. The results underscore the potential of the proposed design, offering promising advancements in the development of a robotic platform for efficient front-wheel locomotion and accurate intestinal screening.},
  archive   = {C_IROS},
  author    = {Vanni Consumi and Neri Niccolò Dei and Gastone Ciuti and Danail Stoyanov and Agostino Stilli},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802483},
  month     = {10},
  pages     = {7003-7010},
  title     = {Miniaturisation and evaluation of the SoftSCREEN system in colon phantoms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A 6-DOF double-layer programmable remote center of motion
robot for vitreoretinal surgery. <em>IROS</em>, 6997–7002. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {During vitreoretinal surgery, surgeons are required to precisely manipulate surgical tools within a confined workspace of an eye, which is roughly 2.5 cm spherical in shape. Because the surgical view can only be obtained by a microscope placed above the eyeball through the pupil, the eyeball needs to be moved or rotated during the operation to see a larger portion of the retina. At this point, general Remote Center of Motion (RCM) mechanisms require additional actuators or manual modification. On the other hand, a programmable RCM mechanism can reduce surgery time without a physical alignment procedure. This study introduces a novel six-degree-of-freedom (DoF) programmable RCM mechanism capable of generating the RCM at random positions in 3D space. Our approach combines two planar 5-bar linkage mechanisms placed in parallel, creating a double-layered configuration to establish the programmable RCM mechanism. We optimized the workspace of each planar mechanism to a customized workspace for a general eyeball model using genetic algorithms, focusing on maximizing the manipulability of the target workspace. The Phantom Omni device was utilized as a remote controller to remotely control the proposed mechanism in a transparent eyeball model with a diameter of 4 cm. Evaluation of the functionality of the programmable RCM mechanism at various RCM points showed that the overall error was less than 1 millimeter. The repeatability of the mechanism was tested and showed an accuracy of about 127 micrometers.},
  archive   = {C_IROS},
  author    = {Chenyu Wang and Seong Young Ko},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802216},
  month     = {10},
  pages     = {6997-7002},
  title     = {A 6-DOF double-layer programmable remote center of motion robot for vitreoretinal surgery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bifurcation identification for ultrasound-driven robotic
cannulation. <em>IROS</em>, 6990–6996. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In trauma and critical care settings, rapid and precise intravascular access is key to patients’ survival. Our research aims at ensuring this access, even when skilled medical personnel are not readily available. Vessel bifurcations are anatomical landmarks that can guide the safe placement of catheters or needles during medical procedures. Although ultrasound is advantageous in navigating anatomical landmarks in emergency scenarios due to its portability and safety, to our knowledge no existing algorithm can autonomously extract vessel bifurcations using ultrasound images. This is primarily due to the limited availability of ground truth data, in particular, data from live subjects, needed for training and validating reliable models. We introduce BIFURC (Bifurcation Identification For Ultrasound-driven Robot Cannulation), a novel algorithm that identifies vessel bifurcations and provides optimal needle insertion sites for an autonomous robotic cannulation system. BIFURC integrates expert knowledge with deep learning techniques to efficiently detect vessel bifurcations within the femoral region and can be trained on a limited amount of in-vivo data. We evaluated our algorithm using a medical phantom as well as real-world experiments involving live pigs. In all cases, BIFURC consistently identified bifurcation points and needle insertion locations in alignment with those identified by expert clinicians.},
  archive   = {C_IROS},
  author    = {Cecilia G. Morales and Dhruv Srikanth and Jack H. Good and Keith A. Dufendach and Artur Dubrawski},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801749},
  month     = {10},
  pages     = {6990-6996},
  title     = {Bifurcation identification for ultrasound-driven robotic cannulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DESectBot: Design and validation of a novel two-segment
decoupled continuum robotic system for endoscopic submucosal dissection.
<em>IROS</em>, 6983–6989. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Endoscopic Submucosal Dissection (ESD) is a minimally invasive procedure designed to remove precancerous and cancerous lesions from the gastrointestinal (GI) tract. Given the GI tract’s tortuous and narrow shape, along with the need for varied movements during dissection, this requires highly flexible and compact instruments, making flexible continuum robots suitable candidates. In this paper, we propose a novel two-segment continuum robot system named DESectBot, featuring a diameter of 5.5 mm and a total length of the active bending module of 48 mm, while the robot’s total length exceeds 1 m. We designed a novel joint combination structure called the spatial cross-curved disk skeleton for the robot, which addresses the mechanical coupling problem between flexible robot actuators. The DESectBot boasts six degrees of freedom, and its kinematic modeling has been derived and utilized in the closed-loop control of the DESectBot. The validation of the DESectBot was conducted through a two-stage test: first, the decoupling performance of the DESectBot was validated. The results show that when one active bending segment bends, the other segment remains almost uninfluenced, with a maximum variation of 1.15 degrees, demonstrating the robot’s effective decoupling capability. Secondly, the accuracy of DESectBot was validated through trajectory-following experiments. The results reveal that the average tracking error for both trajectories is less than 2 mm, and the maximum tracking error is below 2.5 mm. Taking marking, one of the ESD procedures with a 5mm tolerance, as an example, the DESectBot has the potential to be utilized for ESD procedure.},
  archive   = {C_IROS},
  author    = {Wenjie Liu and Yuancheng Shao and Yao Zhang and Zixi Chen and Di Wu and Yuqiao Chen and Cesare Stefanini and Ling Li and Peng Qi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802473},
  month     = {10},
  pages     = {6983-6989},
  title     = {DESectBot: Design and validation of a novel two-segment decoupled continuum robotic system for endoscopic submucosal dissection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FBG-based shape-sensing to enable lateral deflection methods
of autonomous needle insertion. <em>IROS</em>, 6977–6982. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In diagnosing and treating prostate cancer the flexible bevel tip needle insertion surgical technique is commonly used. Bevel tip needles experience asymmetric loading on the needle’s tip, inducing natural bending of the needle and enabling control mechanisms for precise placement of the needle during surgery. Several methods leverage the needles natural bending to provide autonomous control of needle insertion for accurate needle placement in an effort to reduce excess tissue damage and improve patient outcomes from needle insertion intraventions. Moreover, control methods using lateral deflection of the needle intra-operatively to steer the needle during insertion have been studied and have shown promising results. Thus, to enable these autonomous control methods, real-time, intra-operative shape-sensing feedback is pivotal for optimal performance of the needle insertion control. This work presents an extension of our proven Lie-group theoretic shape-sensing model to handle lateral deflection of the needle during needle insertion and validate this extension with robotic needle insertions in phantom tissue using stereo vision as a ground truth. Furthermore, the system configuration for real-time shape-sensing is implemented using ROS 2, demonstrating average feedback frequency of 15±8 Hz. Average needle shape errors realized from this extension under 1 mm, validating the shape-sensing models’ extension.},
  archive   = {C_IROS},
  author    = {Dimitri A. Lezcano and Iulian I. Iordachita and Jin Seob Kim},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801886},
  month     = {10},
  pages     = {6977-6982},
  title     = {FBG-based shape-sensing to enable lateral deflection methods of autonomous needle insertion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SuFIA: Language-guided augmented dexterity for robotic
surgical assistants. <em>IROS</em>, 6969–6976. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we present SuFIA, the first framework for natural language-guided augmented dexterity for robotic surgical assistants. SuFIA incorporates the strong reasoning capabilities of large language models (LLMs) with perception modules to implement high-level planning and low-level control of a robot for surgical sub-task execution. This enables a learning-free approach to surgical augmented dexterity without any in-context examples or motion primitives. SuFIA uses a human-in-the-loop paradigm by restoring control to the surgeon in the case of insufficient information, mitigating unexpected errors for mission-critical tasks. We evaluate SuFIA on four surgical sub-tasks in a simulation environment and two sub-tasks on a physical surgical robotic platform in the lab, demonstrating its ability to perform common surgical sub-tasks through supervised autonomous operation under challenging physical and workspace conditions.Project website: orbit-surgical.github.io/sufia},
  archive   = {C_IROS},
  author    = {Masoud Moghani and Lars Doorenbos and William Chung-Ho Panitch and Sean Huver and Mahdi Azizian and Ken Goldberg and Animesh Garg},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802053},
  month     = {10},
  pages     = {6969-6976},
  title     = {SuFIA: Language-guided augmented dexterity for robotic surgical assistants},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SeeBelow: Sub-dermal 3D reconstruction of tumors with
surgical robotic palpation and tactile exploration. <em>IROS</em>,
6961–6968. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Surgical scene understanding in Robot-assisted Minimally Invasive Surgery (RMIS) is highly reliant on visual cues and lacks tactile perception. Force-modulated surgical palpation with tactile feedback is necessary for localization, geometry/depth estimation, and dexterous exploration of abnormal stiff inclusions in subsurface tissue layers. Prior works explored surface-level tissue abnormalities or single layered tissue-tumor embeddings with more than 300 palpations for dense 2D stiffness mapping. Our approach focuses on 3D reconstructions of sub-dermal tumor surface profiles in multi-layered tissue (skin-fat-muscle) using a visually-guided novel tactile navigation policy. A robotic palpation probe with triaxial force sensing was leveraged for tactile exploration of the phantom. From a surface mesh of the surgical region initialized from a depth camera, the policy explores a surgeon’s region of interest through palpation, sampled from bayesian optimization. Each palpation includes contour following using a contact-safe impedance controller to trace the sub-dermal tumor geometry, until the underlying tumor-tissue boundary is reached. Projections of these contour following palpation trajectories allows 3D reconstruction of the subdermal tumor surface profile in less than 100 palpations. Our approach generates high-fidelity 3D surface reconstructions of rigid tumor embeddings in tissue layers with isotropic elasticities, although soft tumor geometries are yet to be explored. For more details, please refer to our open-source codebase1 and project website2.},
  archive   = {C_IROS},
  author    = {Raghava Uppuluri and Abhinaba Bhattacharjee and Sohel Anwar and Yu She},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801574},
  month     = {10},
  pages     = {6961-6968},
  title     = {SeeBelow: Sub-dermal 3D reconstruction of tumors with surgical robotic palpation and tactile exploration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SURESTEP: An uncertainty-aware trajectory optimization
framework to enhance visual tool tracking for robust surgical
automation. <em>IROS</em>, 6953–6960. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inaccurate tool localization is one of the main reasons for failures in automating surgical tasks. Imprecise robot kinematics and noisy observations caused by the poor visual acuity of an endoscopic camera make tool tracking challenging. Previous works in surgical automation adopt environment-specific setups or hard-coded strategies instead of explicitly considering motion and observation uncertainty of tool tracking in their policies. In this work, we present SURESTEP, an uncertainty-aware trajectory optimization framework for robust surgical automation.We model the uncertainty in tool tracking by considering noise sources that are typical in surgical environments.Using a Gaussian assumption to propagate our uncertainty models through a given tool trajectory, SURESTEP provides a general framework that minimizes the upper bound on the entropy of the final estimated tool distribution.We showcase our method by performing the first-ever, to our knowledge, needle regrasping with a moving endoscopic camera.We compare SURESTEP with a baseline method on a real-world suture needle regrasping task under challenging environmental conditions, such as poor lighting and a moving endoscopic camera. The results over 60 regrasps on the da Vinci Research Kit (dVRK) demonstrate that our optimized trajectories significantly outperform the un-optimized baseline.},
  archive   = {C_IROS},
  author    = {Nikhil U. Shinde and Zih-Yun Chiu and Florian Richter and Jason Lim and Yuheng Zhi and Sylvia Herbert and Michael C. Yip},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801845},
  month     = {10},
  pages     = {6953-6960},
  title     = {SURESTEP: An uncertainty-aware trajectory optimization framework to enhance visual tool tracking for robust surgical automation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pedicle drilling planning transfer for spine surgery using
functional map correspondences. <em>IROS</em>, 6947–6952. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Precise pedicle screw placement is crucial in spine surgery, where minor inaccuracies can result in significant complications. Despite introducing robot-assisted navigation systems to aid surgeons, accommodating the spine’s nonrigid movements (due to patient movement or interactions with the surgeon) often necessitates repeated intraoperative imaging, leading to increased radiation exposure. To address this challenge, we propose a novel method that utilizes the functional map (FM) framework to transfer drilling trajectories from preoperative CT scans to partially observed and noisy spine models. Specifically, the FM correspondences enhance the registration quality of pre-operative and perioperative 3D spine model data, even in the presence of non-rigid deformations. Through comprehensive simulations, we assess the method’s effectiveness across various cases of complex deformations using a spine model consisting of five lumbar vertebrae obtained through CT scans. Validation involves evaluating registration errors in translation and rotation and verifying the clinical validity of transferred drilling trajectories. The results demonstrate the method’s efficiency in transferring drilling trajectories onto noisy, partially observed, and deformed spine models.},
  archive   = {C_IROS},
  author    = {L. Leblanc and R. Vialle and C. De Farias and E. Saghbiny and N. Marturi and B. Tamadazte},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802159},
  month     = {10},
  pages     = {6947-6952},
  title     = {Pedicle drilling planning transfer for spine surgery using functional map correspondences},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards a surgeon-in-the-loop ophthalmic robotic apprentice
using reinforcement and imitation learning. <em>IROS</em>, 6939–6946.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot-assisted surgical systems have demonstrated significant potential in enhancing surgical precision and minimizing human errors. However, existing systems cannot accommodate individual surgeons’ unique preferences and requirements. Additionally, they primarily focus on general surgeries (e.g., laparoscopy) and are unsuitable for highly precise microsurgeries, such as ophthalmic procedures. Thus, we propose an image-guided approach for surgeon-centered autonomous agents that can adapt to the individual surgeon’s skill level and preferred surgical techniques during ophthalmic cataract surgery. Our approach trains reinforcement and imitation learning agents simultaneously using curriculum learning approaches guided by image data to perform all tasks of the incision phase of cataract surgery. By integrating the surgeon’s actions and preferences into the training process, our approach enables the robot to implicitly learn and adapt to the individual surgeon’s unique techniques through surgeon-in-the-loop demonstrations. This results in a more intuitive and personalized surgical experience for the surgeon while ensuring consistent performance for the autonomous robotic apprentice. We define and evaluate the effectiveness of our approach in a simulated environment using our proposed metrics and highlight the trade-off between a generic agent and a surgeon-centered adapted agent. Finally, our approach has the potential to extend to other ophthalmic and microsurgical procedures, opening the door to a new generation of surgeon-in-the-loop autonomous surgical robots. We provide an open-source simulation framework for future development and reproducibility at https://github.com/amrgomaaelhady/CataractAdaptSurgRobot.},
  archive   = {C_IROS},
  author    = {Amr Gomaa and Bilal Mahdy and Niko Kleer and Antonio Krüger},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802574},
  month     = {10},
  pages     = {6939-6946},
  title     = {Towards a surgeon-in-the-loop ophthalmic robotic apprentice using reinforcement and imitation learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Autonomous guidewire navigation in dynamic environments.
<em>IROS</em>, 6931–6938. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cardiovascular disease treatment involves the challenging task of navigating guidewires and catheters through the vascular anatomy. This often results in prolonged procedures where both the patient and clinician are subjected to X-ray radiation. As a potential solution, Deep Reinforcement Learning methods have demonstrated potential in learning this task, paving the way for automated catheter navigation during robotic interventions. However, current works show a limited ability to generalize to unseen and/or deforming anatomies.In this paper, we extend our previous reinforcement learning approach in two main areas: we improve the training strategy to learn a control of the device even when the vascular anatomy is deforming and we propose a method to estimate the motion of the anatomy from single view fluoroscopy images. The combination of these two contributions makes it possible to automatically navigate across a moving vascular anatomy under fluoroscopic imaging, even without injecting a contrast agent. We validate our method on two scenarios: a simulated beating heart and a liver subjected to breathing motion. Our approach leads to an average success rate of 95% in reaching random targets within these anatomies. Our framework is also computationally efficient, enabling the training of our controller to be completed in about 6 hours.},
  archive   = {C_IROS},
  author    = {Valentina Scarponi and François Lecomte and Michel Duprez and Florent Nageotte and Stephane Cotin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801651},
  month     = {10},
  pages     = {6931-6938},
  title     = {Autonomous guidewire navigation in dynamic environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BronchoCopilot: Towards autonomous robotic bronchoscopy via
multimodal reinforcement learning. <em>IROS</em>, 6923–6930. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bronchoscopy plays a significant role in the early diagnosis and treatment of lung diseases. This process demands physicians to maneuver the flexible endoscope for reaching distal lesions, particularly requiring substantial expertise when examining the airways of the upper lung lobe. With the development of artificial intelligence and robotics, reinforcement learning (RL) method has been applied to the manipulation of interventional surgical robots. However, unlike human physicians who utilize multimodal information, most of the current RL methods rely on a single modality, limiting their performance. In this paper, we propose BronchoCopilot, a multimodal RL agent designed to acquire manipulation skills for autonomous bronchoscopy. BronchoCopilot specifically integrates images from the bronchoscope camera and estimated robot poses, aiming for a higher success rate within challenging airway environment. We employ auxiliary reconstruction tasks to compress multimodal data and utilize attention mechanisms to achieve an efficient latent representation of this data, serving as input for the RL module. This framework adopts a stepwise training and fine-tuning approach to mitigate the challenges of training difficulty. Our evaluation in the realistic simulation environment reveals that BronchoCopilot, by effectively harnessing multimodal information, attains a success rate of approximately 90% in fifth generation airways with consistent movements. Additionally, it demonstrates a robust capacity to adapt to diverse cases.},
  archive   = {C_IROS},
  author    = {Jianbo Zhao and Hao Chen and Qingyao Tian and Jian Chen and Bingyu Yang and Zihui Zhang and Hongbin Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802152},
  month     = {10},
  pages     = {6923-6930},
  title     = {BronchoCopilot: Towards autonomous robotic bronchoscopy via multimodal reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 2mm diameter continuum robot tools for suturing in open
spina bifida repair. <em>IROS</em>, 6915–6922. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Open Spina Bifida (OSB) is a congenital neural tube defect where a major component of the procedure to repair the defect involves the closure of a lesion wound through suturing. For a minimally invasive approach, tools entering the uterus to access the fetus should be as thin as possible to minimize maternal risk. This work presents the design of a 3 degrees-of-freedom, 2mm diameter tool wrist with a bending range of motion from 0° to 90°. This wrist is capable of generating up to 2N of force measured from the end of the wrist and achieving a bending curvature of 107m-1 (9.35mm bending radius). A pseudo-rigid body kinematic model has been implemented for the control of this tool with a protocol for singularity mitigation and avoidance. Timed teleoperation studies explicitly demonstrate that the tool is able to reliably execute suturing with a fastest achieved time of under 3 minutes for a simple interrupted suturing technique.},
  archive   = {C_IROS},
  author    = {Arion Law and Nillan Nimal and Paul H. Kang and Radian Gondokaryono and James Drake and Tim Van Mieghem and Thomas Looi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801699},
  month     = {10},
  pages     = {6915-6922},
  title     = {2mm diameter continuum robot tools for suturing in open spina bifida repair},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing base placement of surgical robot: Kinematics
data-driven approach by analyzing working pattern. <em>IROS</em>,
6907–6914. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In robot-assisted minimally invasive surgery (RAMIS), optimal placement of the surgical robot base is crucial for successful surgery. Improper placement can hinder performance because of manipulator limitations and inaccessible workspaces. Conventional base placement relies on the experience of trained medical staff. This study proposes a novel method for determining the optimal base pose based on the surgeon’s working pattern. The proposed method analyzes recorded end-effector poses using a machine learning-based clustering technique to identify key positions and orientations preferred by the surgeon. We introduce two scoring metrics to address the joint limit and singularity issues: joint margin and manipulability scores. We then train a multi-layer perceptron regressor to predict the optimal base pose based on these scores. Evaluation in a simulated environment using the da Vinci Research Kit shows unique base pose score maps for four volunteers, highlighting the individuality of the working patterns. Results comparing with 20,000 randomly selected base poses suggest that the score obtained using the proposed method is 28.2% higher than that obtained by random base placement. These results emphasize the need for operator-specific optimization during base placement in RAMIS.},
  archive   = {C_IROS},
  author    = {Jeonghyeon Yoon and Junhyun Park and Hyojae Park and Hakyoon Lee and Sangwon Lee and Minho Hwang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802398},
  month     = {10},
  pages     = {6907-6914},
  title     = {Optimizing base placement of surgical robot: Kinematics data-driven approach by analyzing working pattern},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging symmetry in RL-based legged locomotion control.
<em>IROS</em>, 6899–6906. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Model-free reinforcement learning is a promising approach for autonomously solving challenging robotics control problems, but faces exploration difficulty without information about the robot’s morphology. The under-exploration of multiple modalities with symmetric states leads to behaviors that are often unnatural and sub-optimal. This issue becomes particularly pronounced in the context of robotic systems with morphological symmetries, such as legged robots for which the resulting asymmetric and aperiodic behaviors compromise performance, robustness, and transferability to real hardware. To mitigate this challenge, we can leverage symmetry to guide and improve the exploration in policy learning via equivariance / invariance constraints. We investigate the efficacy of two approaches to incorporate symmetry: modifying the network architectures to be strictly equivariant / invariant, and leveraging data augmentation to approximate equivariant / invariant actor-critics. We implement the methods on challenging loco-manipulation and bipedal locomotion tasks and compare with an unconstrained baseline. We find that the strictly equivariant policy consistently outperforms other methods in sample efficiency and task performance in simulation. Additionaly, symmetry-incorporated approaches exhibit better gait quality, higher robustness and can be deployed zero-shot to hardware.},
  archive   = {C_IROS},
  author    = {Zhi Su and Xiaoyu Huang and Daniel Ordoñez-Apraez and Yunfei Li and Zhongyu Li and Qiayuan Liao and Giulio Turrisi and Massimiliano Pontil and Claudio Semini and Yi Wu and Koushil Sreenath},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802439},
  month     = {10},
  pages     = {6899-6906},
  title     = {Leveraging symmetry in RL-based legged locomotion control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling and analysis of passive quadruped walker with
compliant torso on low-friction surface. <em>IROS</em>, 6893–6898. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The quadrupeds have wider active territory than humans. Their bodies can adapt various environments through evolution, enabling the efficient, elegant gait for their legged locomotion. Previous researches have indicated lots of examples of utilizing the advantages of body to achieve environment adaptive and stable gait, and for legged locomotion, especially with quadruped robot determining how to generate environment-adaptive mobile locomotion remains a significant challenge. In this study, we discussed the adaptability to environments of quadruped robots, specific walking stability and gait convergence in low-friction environments with compliant torso. The numerically simulations are proposed for observing the trend of walking performance with various friction coefficient. By analyzing the typical walking gait, the adaptability of quadruped walkers with compliant torso are found. These conclusions contribute to the design and development of compliant torso for quadruped walkers.},
  archive   = {C_IROS},
  author    = {Yuxuan Xiang and Yanqiu Zheng and Fumihiko Asano},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802724},
  month     = {10},
  pages     = {6893-6898},
  title     = {Modeling and analysis of passive quadruped walker with compliant torso on low-friction surface},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Versatile locomotion skills for hexapod robots.
<em>IROS</em>, 6885–6892. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hexapod robots are potentially suitable for carrying out tasks in cluttered environments since they are stable, compact, and light weight. They also have multi-joint legs and variable height bodies that make them good candidates for tasks such as stairs climbing and squeezing under objects in a typical home environment or an attic. Expanding on our previous work on joist climbing in attics, we train a legged hexapod equipped with a depth camera and visual inertial odometry (VIO) to perform three tasks: climbing stairs, avoiding obstacles, and squeezing under obstacles such as a table. Our policies are trained with simulation data only and can be deployed on low-cost hardware not requiring real-time joint state feedback. We train our model in a teacher-student model with 2 phases: In phase 1, we use reinforcement learning with access to privileged information such as height maps and joint feedback. In phase 2, we use supervised learning to distill the model into one with access to only onboard observations, consisting of egocentric depth images and robot pose captured by a tracking VIO camera. By manipulating available privileged information, constructing simulation terrains, and refining reward functions during phase 1 training, we are able to train the robots with skills that are robust in non-ideal physical environments. We demonstrate successful sim-to-real transfer and achieve high success rates across all three tasks in physical experiments.},
  archive   = {C_IROS},
  author    = {Tomson Qu and Dichen Li and Avideh Zakhor and Wenhao Yu and Tingnan Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801714},
  month     = {10},
  pages     = {6885-6892},
  title     = {Versatile locomotion skills for hexapod robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LocoMan: Advancing versatile quadrupedal dexterity with
lightweight loco-manipulators. <em>IROS</em>, 6877–6884. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Quadrupedal robots have emerged as versatile agents capable of locomoting and manipulating in complex environments. Traditional designs typically rely on the robot’s inherent body parts or incorporate top-mounted arms for manipulation tasks. However, these configurations may limit the robot’s operational dexterity, efficiency, and adaptability, particularly in cluttered or constrained spaces. In this work, we present LocoMan, a dexterous quadrupedal robot with a novel morphology to perform versatile manipulation in diverse constrained environments. By equipping a Unitree Go1 robot with two low-cost and lightweight modular 3-DoF loco-manipulators on its front calves, LocoMan leverages the combined mobility and functionality of the legs and grippers for complex manipulation tasks that require precise 6D positioning of the end effector in a wide workspace. To harness the loco-manipulation capabilities of LocoMan, we introduce a unified control framework that extends the whole-body controller (WBC) to integrate the dynamics of loco-manipulators. Through experiments, we validate that the proposed whole-body controller can accurately and stably follow desired 6D trajectories of the end effector and torso, which, when combined with the large workspace from our design, facilitates a diverse set of challenging dexterous loco-manipulation tasks in confined spaces, such as opening doors, plugging into sockets, picking objects in narrow and low-lying spaces, and bimanual manipulation.},
  archive   = {C_IROS},
  author    = {Changyi Lin and Xingyu Liu and Yuxiang Yang and Yaru Niu and Wenhao Yu and Tingnan Zhang and Jie Tan and Byron Boots and Ding Zhao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801980},
  month     = {10},
  pages     = {6877-6884},
  title     = {LocoMan: Advancing versatile quadrupedal dexterity with lightweight loco-manipulators},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning safe locomotion for quadrupedal robots by
derived-action optimization. <em>IROS</em>, 6870–6876. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep reinforcement learning controllers with exteroception have enabled quadrupedal robots to traverse terrain robustly. However, most of these controllers heavily depend on complex reward functions and suffer from poor convergence. This work proposes a novel learning framework called derived-action optimization. The derived action is defined as a high-level representation of a policy and can be introduced into the reward function to guide decision-making behaviors. The proposed derived-action optimization method is applied to learn safer quadrupedal locomotion, achieving fast convergence and better performance. Specifically, we choose the foothold as the derived action and optimize the flatness of the terrain around the foothold to reduce potential sliding and collisions. Extensive experiments demonstrate the high safety and effectiveness of our method.},
  archive   = {C_IROS},
  author    = {Deye Zhu and Chengrui Zhu and Zhen Zhang and Shuo Xin and Yong Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802725},
  month     = {10},
  pages     = {6870-6876},
  title     = {Learning safe locomotion for quadrupedal robots by derived-action optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Harnessing natural oscillations for high-speed, efficient
asymmetrical locomotion in quadrupedal robots. <em>IROS</em>, 6864–6869.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study explores the dynamics of asymmetrical bounding gaits in quadrupedal robots, focusing on the integration of torso pitching and hip motion to enhance speed and stability. Traditional control strategies often enforce a fixed posture, minimizing natural body movements to simplify the control problem. However, this approach may overlook the inherent dynamical advantages found in natural locomotion. By considering the robot as two interconnected segments, we concentrate on stance leg motion while allowing passive torso oscillation, drawing inspiration from natural dynamics and underactuated robotics principles. Our control scheme employs Linear Inverted Pendulum (LIP) and Spring-Loaded Inverted Pendulum (SLIP) models to govern front and rear leg movements independently. This approach has been validated through extensive simulations and hardware experiments, demonstrating successful high-speed locomotion with top speeds nearing 4 m/s and reduced ground reaction forces, indicating a more efficient gait. Furthermore, unlike conventional methods, our strategy leverages natural torso oscillations to aid leg circulation and stride length, aligning robot dynamics more closely with biological counterparts. Our findings suggest that embracing the natural dynamics of quadrupedal movement, particularly in asymmetrical gaits like bounding, can lead to more stable, efficient, and high-speed robotic locomotion. This investigation lays the groundwork for future studies on versatile and dynamic quadrupedal gaits and their potential applications in scenarios demanding rapid and effective locomotion.},
  archive   = {C_IROS},
  author    = {Jing Cheng and Yasser G. Alqaham and Zhenyu Gan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801432},
  month     = {10},
  pages     = {6864-6869},
  title     = {Harnessing natural oscillations for high-speed, efficient asymmetrical locomotion in quadrupedal robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving legged robot locomotion by quantifying
morphological computation. <em>IROS</em>, 6856–6863. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many robotic and biological systems exploit their morphology’s interaction with the environment to become more adaptable, more energy efficient, and to simplify their control. The principles of morphological computation (MC) have been increasingly studied in recent years and researchers have investigated theoretical approaches to quantify the contribution of MC for a variety of robotic systems using only simulated models. In this work, we quantify MC in a physical robotic system, utilizing position-controlled legs with two degrees of freedom in two designs of different elastic compliance, on a bespoke test rig to execute a walking gait. The contribution of morphology was estimated by applying a theoretical model at various stages within the gait cycle to quantify the MC. The relationships between MC and the ground reaction forces and actuator energy consumption are analyzed. The results indicating that increasing the compliance in the leg morphology increases the mean MC value (7.70±1.49) relative to a traditional leg design (5.03±2.27). Periods of high MC were found to occur during the swing phase of the walking gait and reduced during the stance phase with ground reaction forces, which correlates with the findings of prior theoretical studies of MC in hopping gaits. The benefits of refining the leg morphology for higher MC is demonstrated by the measurements of cost of transport (COT), where the leg with the higher mean MC of 7.70 has a lower mean COT of 102.8 compared to the other leg’s mean COT of 153.8. The results demonstrate how real-world measurements of MC may help design the morphology of improved robotics systems.},
  archive   = {C_IROS},
  author    = {Vijay Chandiramani and Helmut Hauser and Andrew T. Conn},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801821},
  month     = {10},
  pages     = {6856-6863},
  title     = {Improving legged robot locomotion by quantifying morphological computation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic object catching with quadruped robot front legs.
<em>IROS</em>, 6848–6855. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a framework for dynamic object catching using a quadruped robot’s front legs while it stands on its rear legs. The system integrates computer vision, trajectory prediction, and leg control to enable the quadruped to visually detect, track, and successfully catch a thrown object using an onboard camera. Leveraging a fine-tuned YOLOv8 model for object detection and a regression-based trajectory prediction module, the quadruped adapts its front leg positions iteratively to anticipate and intercept the object. The catching maneuver involves identifying the optimal catching position, controlling the front legs with Cartesian PD control, and closing the legs together at the right moment. We propose and validate three different methods for selecting the optimal catching position: 1) intersecting the predicted trajectory with a vertical plane, 2) selecting the point on the predicted trajectory with the minimal distance to the center of the robot’s legs in their nominal position, and 3) selecting the point on the predicted trajectory with the highest likelihood on a Gaussian Mixture Model (GMM) modelling the robot’s reachable space. Experimental results demonstrate robust catching capabilities across various scenarios, with the GMM method achieving the best performance, leading to an 80% catching success rate.},
  archive   = {C_IROS},
  author    = {André Schakkal and Guillaume Bellegarda and Auke Ijspeert},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801922},
  month     = {10},
  pages     = {6848-6855},
  title     = {Dynamic object catching with quadruped robot front legs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distilling reinforcement learning policies for interpretable
robot locomotion: Gradient boosting machines and symbolic regression.
<em>IROS</em>, 6840–6847. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advancements in reinforcement learning (RL) have led to remarkable achievements in robot locomotion capabilities. However, the complexity and &quot;black-box&quot; nature of neural network-based RL policies hinder their interpretability and broader acceptance, particularly in applications demanding high levels of safety and reliability. This paper introduces a novel approach to distill neural RL policies into more interpretable forms using Gradient Boosting Machines (GBMs), Explainable Boosting Machines (EBMs) and Symbolic Regression. By leveraging the inherent interpretability of generalized additive models, decision trees, and analytical expressions, we transform opaque neural network policies into more transparent &quot;glass-box&quot; models. We train expert neural network policies using RL and subsequently distill them into (i) GBMs, (ii) EBMs, and (iii) symbolic policies. To address the inherent distribution shift challenge of behavioral cloning, we propose to use the Dataset Aggregation (DAgger) algorithm with a curriculum of episode-dependent alternation of actions between expert and distilled policies, to enable efficient distillation of feedback control policies. We evaluate our approach on various robot locomotion gaits – walking, trotting, bounding, and pacing – and study the importance of different observations in joint actions for distilled policies using various methods. We train neural expert policies for 205 hours of simulated experience and distill interpretable policies with only 10 minutes of simulated interaction for each gait using the proposed method.},
  archive   = {C_IROS},
  author    = {Fernando Acero and Zhibin Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802433},
  month     = {10},
  pages     = {6840-6847},
  title     = {Distilling reinforcement learning policies for interpretable robot locomotion: Gradient boosting machines and symbolic regression},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing leg odometry in legged robots with learned contact
bias: An LSTM recurrent neural network approach. <em>IROS</em>,
6832–6839. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To address the leg odometry drift caused by the non-stationary foot contact, this paper introduces a novel data-driven based leg odometry technique for legged robots. By leveraging a Long Short-Term Memory (LSTM) Recurrent Neural Network (RNN), the method learns the biases in the robot’s foot contact locations from sequential IMU measurements and ground reaction forces (GRF). This learned contact bias is then incorporated into the state estimation process using a Kalman filter (KF), significantly improving the precision of leg odometry for legged robots operating in real time. This method, which combines deep learning approaches with conventional filtering techniques, is named the Deep Learning Kalman Filter (DLKF). The effectiveness of the DLKF is demonstrated through simulation and experimental trials using a Unitree Go1 robot across various challenging environments, including uneven terrain, slopes, and stairs, where foot slippage occurs frequently. Our results indicate an average 64.93% reduction in translational errors in leg odometry when the learned contact bias is applied. Further improvements are observed in a fused LiDAR and leg odometry state estimation system, especially in feature-deprived areas, indicating that the proposed leg odometry system can be easily fused with other sensor measurements to get a more precise state estimation.},
  archive   = {C_IROS},
  author    = {Yaru Gu and Ze Liu and Ting Zou},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802731},
  month     = {10},
  pages     = {6832-6839},
  title     = {Enhancing leg odometry in legged robots with learned contact bias: An LSTM recurrent neural network approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Task-space riccati feedback based whole body control for
underactuated legged locomotion. <em>IROS</em>, 6826–6831. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This manuscript primarily aims to enhance the performance of whole-body controllers(WBC) for underactuated legged locomotion. We introduce a systematic parameter design mechanism for the floating-base feedback control within the WBC. The proposed approach involves utilizing the linearized model of unactuated dynamics to formulate a Linear Quadratic Regulator(LQR) and solving a Riccati gain while accounting for potential physical constraints through a second-order approximation of the log-barrier function. And then the user-tuned feedback gain for the floating base task is replaced by a new one constructed from the solved Riccati gain. Extensive simulations conducted in MuJoCo with a point bipedal robot, as well as real-world experiments performed on a quadruped robot, demonstrate the effectiveness of the proposed method. In the different bipedal locomotion tasks, compared with the user-tuned method, the proposed approach is at least 12% better and up to 50% better at linear velocity tracking, and at least 7% better and up to 47% better at angular velocity tracking. In the quadruped experiment, linear velocity tracking is improved by at least 3% and angular velocity tracking is improved by at least 23% using the proposed method.},
  archive   = {C_IROS},
  author    = {Shunpeng Yang and Zejun Hong and Sen Li and Patrick Wensing and Wei Zhang and Hua Chen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801528},
  month     = {10},
  pages     = {6826-6831},
  title     = {Task-space riccati feedback based whole body control for underactuated legged locomotion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The design of the barkour benchmark for robot agility.
<em>IROS</em>, 6818–6825. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we describe the design of the Barkour benchmark for measuring robot agility in navigating complex environments. Despite the growing interest in developing agile robot locomotion skills, the field lacks systematic benchmarks to measure the performance of robotic control systems and hardware in agility-focused tasks. This motivated us to propose the Barkour benchmark, an obstacle course designed to quantify agility across various robotic platforms. Inspired by dog agility competitions, the course features diverse obstacles and a time-based scoring mechanism, encouraging researchers to develop controllers that enable robots to move quickly, precisely, and with adaptability. This benchmark is challenging as it demands diverse motion skills and the time-based scoring requires control precision at high speed. Along with the design details presented in the paper, we release our simulated environment setups in MuJoCo-XLA and the CAD model of a custom-designed quadruped robot to facilitate future research to reproduce the Barkour setup (available at sites.google.com/view/barkour). We hope these together will accelerate the pace of robot agility research.},
  archive   = {C_IROS},
  author    = {Wenhao Yu and Ken Caluwaerts and Atil Iscen and J. Chase Kew and Tingnan Zhang and Daniel Freeman and Lisa Lee and Stefano Saliceti and Vincent Zhuang and Nathan Batchelor and Steven Bohez and Federico Casarini and Jose Enrique Chen and Erwin Coumans and Adil Dostmohamed and Gabriel Dulac-Arnold and Alejandro Escontrela and Erik Frey and Roland Hafner and Deepali Jain and Bauyrjan Jyenis and Yuheng Kuang and Edward Lee and Ofir Nachum and Ken Oslund and Francesco Romano and Fereshteh Sadeghi and Baruch Tabanpour and Daniel Zheng and Michael Neunert and Raia Hadsell and Nicolas Heess and Francesco Nori and Jeff Seto and Carolina Parada and Vikas Sindhwani and Vincent Vanhoucke and Jie Tan and Kuang-Huei Lee},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801377},
  month     = {10},
  pages     = {6818-6825},
  title     = {The design of the barkour benchmark for robot agility},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). State estimation transformers for agile legged locomotion.
<em>IROS</em>, 6810–6817. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a state estimation method that can accurately predict the robot’s privileged states to push the limits of quadruped robots in executing advanced skills such as jumping in the wild. In particular, we present the State Estimation Transformers (SET), an architecture that casts the state estimation problem as conditional sequence modeling. SET outputs the robot states that are hard to obtain directly in the real world, such as the body height and velocities, by leveraging a causally masked Transformer. By conditioning an autoregressive model on the robot’s past states, our SET model can predict these privileged observations accurately even in highly dynamic locomotions. We evaluate our methods on three tasks — running jumping, running backflipping, and running sideslipping — on a low-cost quadruped robot, Cyberdog2. Results show that SET can outperform other methods in estimation accuracy and transferability in the simulation as well as success rates of jumping and triggering a recovery controller in the real world, suggesting the superiority of such a Transformer-based explicit state estimator in highly dynamic locomotion tasks.},
  archive   = {C_IROS},
  author    = {Chen Yu and Yichu Yang and Tianlin Liu and Yangwei You and Mingliang Zhou and Diyun Xiang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802640},
  month     = {10},
  pages     = {6810-6817},
  title     = {State estimation transformers for agile legged locomotion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time perceptive motion control using control barrier
functions with analytical smoothing for six-wheeled-telescopic-legged
robot tachyon 3. <em>IROS</em>, 6802–6809. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To achieve safe legged locomotion, it is crucial to generate motion in real-time considering various constraints in robots and environments. In this study, we propose a lightweight real-time perceptive motion control system for the newly developed six-wheeled-telescopic-legged robot, Tachyon 3. In the proposed method, analytically smoothed constraints including Smooth Separating Axis Theorem (SSAT) as a novel higher order differentiable collision detection for 3D shapes is applied to the Control Barrier Function (CBF). The proposed system integrating the CBF achieves online motion generation in a short control cycle of 1 ms that satisfies joint limitations, environmental collision avoidance and safe convex foothold constraints. The efficiency of SSAT is shown from the collision detection time of 1 µs or less and the CBF constraint computation time for Tachyon 3 of several µs. Furthermore, the effectiveness of the proposed system is verified through the stair-climbing motion, integrating online recognition in a simulation and a real machine.},
  archive   = {C_IROS},
  author    = {Noriaki Takasugi and Masaya Kinoshita and Yasuhisa Kamikawa and Ryoichi Tsuzaki and Atsushi Sakamoto and Toshimitsu Kai and Yasunori Kawanami},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802375},
  month     = {10},
  pages     = {6802-6809},
  title     = {Real-time perceptive motion control using control barrier functions with analytical smoothing for six-wheeled-telescopic-legged robot tachyon 3},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explosive legged robotic hopping: Energy accumulation and
power amplification via pneumatic augmentation. <em>IROS</em>,
6794–6801. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel pneumatic augmentation to traditional electric motor-actuated legged robot to increase intermittent power density to perform infrequent explosive hopping behaviors. The pneumatic system is composed of a pneumatic pump, a tank, and a pneumatic actuator. The tank is charged up by the pump during regular hopping motion that is created by the electric motors. At any time after reaching a desired air pressure in the tank, a solenoid valve is utilized to rapidly release the air pressure to the pneumatic actuator (piston) which is used in conjunction with the electric motors to perform explosive hopping, increasing maximum hopping height for one or subsequent cycles. We show that, on a custom-designed one-legged hopping robot, without any additional power source and with this novel pneumatic augmentation system, their associated system identification and optimal control, the robot is able to realize highly explosive hopping with power amplification per cycle by a factor of approximately 5.4 times the power of electric motor actuation alone.},
  archive   = {C_IROS},
  author    = {Yifei Chen and Arturo Gamboa-Gonzalez and Michael Wehner and Xiaobin Xiong},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801632},
  month     = {10},
  pages     = {6794-6801},
  title     = {Explosive legged robotic hopping: Energy accumulation and power amplification via pneumatic augmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Wheelchair maneuvering with a single-spherical-wheeled
balancing mobile manipulator. <em>IROS</em>, 6583–6589. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we present a control framework to effectively maneuver wheelchairs with a dynamically stable mobile manipulator. Wheelchairs are a type of nonholonomic cart system, maneuvering such systems with mobile manipulators (MM) is challenging mostly due to the following reasons: 1) These systems feature nonholonomic constraints and considerably varying inertial parameters that require online identification and adaptation. 2) These systems are widely used in human-centered environments, which demand the MM to operate in potentially crowded spaces while ensuring compliance for safe physical human-robot interaction (pHRI). We propose a control framework that plans whole-body motion based on quasi-static analysis to maneuver heavy nonholonomic carts while maintaining overall compliance. We validated our approach experimentally by maneuvering a wheelchair with a bimanual mobile manipulator, the CMU ballbot. The experiments demonstrate the proposed framework is able to track desired wheelchair velocity with loads varying from 11.8 kg to 79.4 kg at a maximum linear velocity of 0.45 m/s and angular velocity of 0.3 rad/s. Furthermore, we verified that the proposed method can generate human-like motion smoothness of the wheelchair while ensuring safe interactions with the environment.},
  archive   = {C_IROS},
  author    = {Cunxi Dai and Xiaohan Liu and Roberto Shu and Ralph Hollis},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802600},
  month     = {10},
  pages     = {6583-6589},
  title     = {Wheelchair maneuvering with a single-spherical-wheeled balancing mobile manipulator},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Safe and efficient auto-tuning to cross sim-to-real gap for
bipedal robot. <em>IROS</em>, 6383–6389. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in both legged robot locomotion and Reinforcement Learning have shown a promising path for developing bipedal robot controllers. While the difference in dynamics between real world and simulation, also known as reality gap, still hinders the use. In this paper, we focus on sim-to-real bipedal robot locomotion task. We leverage the recent advances in auto-tuning sim-to-real transfer and use it to address sim-to-real bipedal robot locomotion problem. Similar to existing work, we first train a parameter searching model with dataset collected from simulator and use real-world data to tune the simulation parameters. However, the prediction tuning can be unreliable if the training dataset distribution fails to cover the real-world data. We address this problem by formulating this problem as an Out-of-distribution problem and further extending the current framework with a dataset verification model. With extended module, our method is capable of tuning the simulation parameters safely and efficiently. We demonstrate our method outperforms existing work and achieves sim-to-real bipedal robot locomotion on bipedal robot BITeno.},
  archive   = {C_IROS},
  author    = {Yidong Du and Xuechao Chen and Zhangguo Yu and Yuanxi Zhang and Zishun Zhou and Jindai Zhang and Jintao Zhang and Botao Liu and Qiang Huang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801318},
  month     = {10},
  pages     = {6383-6389},
  title     = {Safe and efficient auto-tuning to cross sim-to-real gap for bipedal robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatio-temporal consistent mapping of growing plants for
agricultural robots in the wild. <em>IROS</em>, 6375–6382. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tracking changes in growing plants is important for automating phenotyping and robots managing crops. In this paper, we propose a system that uses a 3D model of plants along crop rows to enable a robotic platform to localize itself even in the presence of heavy changes and deforming the model to adapt the scene description to the new measurements. In particular, we focus on consumer RGB-D cameras due to their cost-effectiveness and ease of deployment on real platforms. Our approach exploits modern deep-learning-based feature descriptors and geometric information to obtain matches between 3D points corresponding to temporally distant sessions. We then use the associations in a non-rigid registration pipeline to obtain the final result, an updated representation of the 3D model that reflects plant changes. Using a standard RGB-D sensor, we validate our approach on a real-world dataset recorded in a glasshouse. We obtain accurate 4D models of the plants and track the plant traits’ evolution over time. We show, through experiments, that our method is applicable to interpolate plant organs’ evolution, a helpful result for phenotypic trait measurement. We see our approach as a relevant step toward 4D reconstruction for robotic agriculture in the wild.},
  archive   = {C_IROS},
  author    = {Luca Lobefaro and Meher V. R. Malladi and Tiziano Guadagnino and Cyrill Stachniss},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802321},
  month     = {10},
  pages     = {6375-6382},
  title     = {Spatio-temporal consistent mapping of growing plants for agricultural robots in the wild},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Revolutionizing battery disassembly: The design and
implementation of a battery disassembly autonomous mobile manipulator
robot(BEAM-1). <em>IROS</em>, 6367–6374. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The efficient disassembly of end-of-life electric vehicle batteries(EOL-EVBs) is crucial for green manufacturing and sustainable development. The current pre-programmed disassembly conducted by the Autonomous Mobile Manipulator Robot(AMMR) struggles to meet the disassembly requirements in dynamic environments, complex scenarios, and unstructured processes. In this paper, we propose a Battery Disassembly AMMR(BEAM-1) system based on NeuralSymbolic AI. It detects the environmental state by leveraging a combination of multi-sensors and neural predicates and then translates this information into a quasi-symbolic space. In real-time, it identifies the optimal sequence of action primitives through LLM-heuristic tree search, ensuring high-precision execution of these primitives. Additionally, it employs positional speculative sampling using intuitive networks and achieves the disassembly of various bolt types with a meticulously designed end-effector. Importantly, BEAM-1 is a continuously learning embodied intelligence system capable of subjective reasoning like a human, and possessing intuition. A large number of real scene experiments have proved that it can autonomously perceive, decide, and execute to complete the continuous disassembly of bolts in multiple, multi-category, and complex situations, with a success rate of 98.78%. This research attempts to use NeuroSymbolic AI to give robots real autonomous reasoning, planning, and learning capabilities. BEAM-1 realizes the revolution of battery disassembly. Its framework can be easily ported to any robotic system to realize different application scenarios, which provides a ground-breaking idea for the design and implementation of future embodied intelligent robotic systems.},
  archive   = {C_IROS},
  author    = {Yanlong Peng and Zhigang Wang and Yisheng Zhang and Shengmin Zhang and Nan Cai and Fan Wu and Ming Chen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802225},
  month     = {10},
  pages     = {6367-6374},
  title     = {Revolutionizing battery disassembly: The design and implementation of a battery disassembly autonomous mobile manipulator Robot(BEAM-1)},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An autonomous, 3D printed, waterjet-powered, open-source
robotic trimaran for environmental inspection and monitoring.
<em>IROS</em>, 6359–6366. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Versatile, autonomous robotic boats can offer excellent environmental inspection and monitoring solutions for remote, dangerous, hard to reach, or access protected water bodies. This paper introduces such a platform in the form of an autonomous, cost-effective, waterjet-powered robotic trimaran. Motivated by the need for an efficient aquatic monitoring, particularly in Aotearoa - New Zealand’s diverse environments, the trimaran provides an efficient, low-cost, and easy to replicate alternative to resource-intensive research vessels. The proposed platform, costs $600-1,500 USD to develop (depending on the sensing system configuration), weighs under 5 kg, and excels in bathymetry and water quality testing. The trimaran can reach speeds of up to 2 m/s offering obstacle avoidance of natural features, such as rocks. Utilizing off-the-shelf components and 3D printing technology, the proposed platform offers excellent reproducibility and robustness while operating in shallow waters with its jet propulsion system. The paper presents in detail the design characteristics, the sensing system employed, testing results focusing on bathymetry, and highlights the ability of vessel and the potential for future research and data collection.},
  archive   = {C_IROS},
  author    = {Reuben O’Brien and Martin Lambrechtse-Reid and Minas Liarokapis},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801622},
  month     = {10},
  pages     = {6359-6366},
  title     = {An autonomous, 3D printed, waterjet-powered, open-source robotic trimaran for environmental inspection and monitoring},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robot synesthesia: A sound and emotion guided robot painter.
<em>IROS</em>, 5986–5992. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {If a picture paints a thousand words, sound may voice a million. While recent robotic painting and image synthesis methods have achieved progress in generating visuals from text inputs, the translation of sound into images is vastly unexplored. Generally, sound-based interfaces and sonic interactions have the potential to expand accessibility and control for the user and provide a means to convey complex emotions and the dynamic aspects of the real world. In this paper, we propose an approach for using sound and speech to guide a robotic painting process, known here as robot synesthesia. For general sound, we encode the simulated paintings and input sounds into the same latent space. For speech, we decouple speech audio into its transcribed text and the tone of the speech. Whereas we use the text to control the content, we estimate the emotions from the tone to guide the mood of the painting. Our approach has been fully integrated with FRIDA, a robotic painting framework, adding sound and speech to FRIDA’s existing input modalities such as text and style. In two surveys, participants were able to correctly guess the emotion or natural sound used to generate a given painting more than twice as likely as random chance. On our sound-guided image manipulation and music-guided paintings, we discuss the results qualitatively.},
  archive   = {C_IROS},
  author    = {Vihaan Misra and Peter Schaldenbrand and Jean Oh},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802780},
  month     = {10},
  pages     = {5986-5992},
  title     = {Robot synesthesia: A sound and emotion guided robot painter},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Low-cost air hockey robot using a five-bar linkage mechanism
driven by position-control servomotors. <em>IROS</em>, 5978–5985. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In human-robot interaction (HRI) research, ball games pose significant challenges that demand robotic solutions that are both cost-effective and user-friendly for non-experts. Air hockey, characterized by safe, non-direct-contact play and a simplified state-action space, emerges as an ideal platform for such research. Despite the availability of various air hockey robots, their high cost and complexity have limited widespread use among researchers requiring robotics expertise. Addressing this gap, we introduce a low-cost, accessible air hockey robot designed to facilitate HRI studies. Featuring a lightweight five-bar linkage mechanism powered by low-cost servomotors for position control, this robot combines efficiency with ease of use. The complete robot’s cost is estimated at $346.8, with the arm weighing a mere 19 grams. The robot precisely returns the puck by intermittently adjusting its target joint positions, achieving a play with an average return error of 42.6 mm. These characteristics affirm the robot’s potential as a valuable tool for advancing HRI research.},
  archive   = {C_IROS},
  author    = {Mirai Shinjo and Cristian C. Beltran-Hernandez and Masashi Hamaya and Kazutoshi Tanaka},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801458},
  month     = {10},
  pages     = {5978-5985},
  title     = {Low-cost air hockey robot using a five-bar linkage mechanism driven by position-control servomotors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An intelligent robotic system for perceptive pancake batter
stirring and precise pouring. <em>IROS</em>, 5970–5977. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cooking robots have long been desired by the commercial market, while the technical challenge is still significant. A major difficulty comes from the demand of perceiving and handling liquid with different properties. This paper presents a robot system that mixes batter and makes pancakes out of it, where understanding and handling the viscous liquid is an essential component. The system integrates Haptic Sensing and control algorithms to autonomously stir flour and water to achieve the desired batter uniformity, estimate the batter’s properties such as the water-flour ratio and liquid level, as well as perform precise manipulations to pour the batter into any specified shape. Experimental results show the system’s capability to always produce batter of desired uniformity, estimate water-flour ratio and liquid level precisely, and accurately pour it into complex shapes. This research showcases the potential for robots to assist in kitchens and step towards commercial culinary automation.},
  archive   = {C_IROS},
  author    = {Xinyuan Luo and Shengmiao Jin and Hung-Jui Huang and Wenzhen Yuan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802841},
  month     = {10},
  pages     = {5970-5977},
  title     = {An intelligent robotic system for perceptive pancake batter stirring and precise pouring},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flying robotics art: ROS-based drone draws the
record-breaking mural. <em>IROS</em>, 5964–5969. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents the innovative design and successful deployment of a pioneering autonomous unmanned aerial system developed for executing the world’s largest mural painted by a drone. Addressing the dual challenges of maintaining artistic precision and operational reliability under adverse outdoor conditions such as wind and direct sunlight, our work introduces a robust system capable of navigating and painting outdoors with unprecedented accuracy. Key to our approach is a novel navigation system that combines an infrared (IR) motion capture camera and LiDAR technology, enabling precise location tracking tailored specifically for large-scale artistic applications. We employ a unique control architecture that uses different regulation in tangential and normal directions relative to the planned path, enabling precise trajectory tracking and stable line rendering. We also present algorithms for trajectory planning and path optimization, allowing for complex curve drawing and area filling. The system includes a custom-designed paint spraying mechanism, specifically engineered to function effectively amidst the turbulent airflow generated by the drone’s propellers, which also protects the drone’s critical components from paint-related damage, ensuring longevity and consistent performance. Experimental results demonstrate the system’s robustness and precision in varied conditions, showcasing its potential for autonomous large-scale art creation and expanding the functional applications of robotics in creative fields.},
  archive   = {C_IROS},
  author    = {Andrei A. Korigodskii and Oleg D. Kalachev and Artem E. Vasiunik and Matvei V. Urvantsev and Georgii E. Bondar},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802405},
  month     = {10},
  pages     = {5964-5969},
  title     = {Flying robotics art: ROS-based drone draws the record-breaking mural},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ProSIP: Probabilistic surface interaction primitives for
learning of robotic cleaning of edges. <em>IROS</em>, 5956–5963. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning from demonstration (LfD) has emerged as a promising approach enabling robots to acquire complex tasks directly from human demonstrations. However, tasks involving surface interactions on freeform 3D surfaces present unique challenges in modeling and execution, especially when geometric variations exist between demonstrations and robot execution. This paper proposes a novel framework called probabilistic surface interaction primitives (ProSIP), which systematically incorporates the surface path and the local surface features into the learning procedure. An instrumented tool allows seamless recording and execution of human demonstrations. By design, ProSIPs are independent of time, invariant to rigid-body displacements, and apply to any robotic platform with a Cartesian controller. The framework is employed for an edge-cleaning task of bathroom sinks. The generalization capability to various object geometries and significantly distorted objects is demonstrated. Simulations and an experimental setup with a 9-degrees-of-freedom robotic platform confirm the performance.},
  archive   = {C_IROS},
  author    = {Christoph Unger and Christian Hartl-Nesic and Minh Nhat Vu and Andreas Kugi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802255},
  month     = {10},
  pages     = {5956-5963},
  title     = {ProSIP: Probabilistic surface interaction primitives for learning of robotic cleaning of edges},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward perpetual occlusion-aware observation of comb states
in living honeybee colonies. <em>IROS</em>, 5948–5955. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Honeybees are one of the most important pollinators in the ecosystem. Unfortunately, the dynamics of living honeybee colonies are not well understood due to their complexity and difficulty of observation. In our project “RoboRoyale”, we build and operate a robot to be a part of a bio-hybrid system, which currently observes the honeybee queen in the colony and physically tracks it with a camera. Apart from tracking and observing the queen, the system needs to monitor the state of the honeybee comb which is most of the time occluded by workerbees. This introduces a necessary tradeoff between tracking the queen and visiting the rest of the hive to create a daily map. We aim to collect the necessary data more effectively. We evaluate several mapping methods that consider the previous observations and forecasted densities of bees occluding the view. To predict the presence of bees, we use previously established maps of dynamics developed for autonomy in human-populated environments. Using data from the last observational season, we show significant improvement of the informed comb mapping methods over our current system. This will allow us to use our resources more effectively in the upcoming season.},
  archive   = {C_IROS},
  author    = {Jan Blaha and Tomáš Vintr and Jan Mikula and Jiří Janota and Tomáš Rouĉek and Jiří Ulrich and Fatemeh Rekabi-Bana and Laurenz Alexander Fedotoff and Martin Stefanec and Thomas Schmickl and Farshad Arvin and Miroslav Kulich and Tomáš Krajník},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801380},
  month     = {10},
  pages     = {5948-5955},
  title     = {Toward perpetual occlusion-aware observation of comb states in living honeybee colonies},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SCANet: Correcting LEGO assembly errors with self-correct
assembly network. <em>IROS</em>, 5940–5947. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous assembly in robotics and 3D vision presents significant challenges, particularly in ensuring assembly correctness. Presently, predominant methods such as MEPNet focus on assembling components based on manually provided images. However, these approaches often fall short in achieving satisfactory results for tasks requiring long-term planning. Concurrently, we observe that integrating a self-correction module can partially alleviate such issues. Motivated by this concern, we introduce the Single-Step Assembly Error Correction Task, which involves identifying and rectifying misassembled components. To support research in this area, we present the LEGO Error Correction Assembly Dataset (LEGO-ECA), comprising manual images for assembly steps and instances of assembly failures. Additionally, we propose the Self-Correct Assembly Network (SCANet), a novel method to address this task. SCANet treats assembled components as queries, determining their correctness in manual images and providing corrections when necessary. Finally, we utilize SCANet to correct the assembly results of MEPNet. Experimental results demonstrate that SCANet can identify and correct MEPNet&#39;s misassembled results, significantly improving the correctness of assembly. Our code and dataset are available at https://github.com/Yaser-wyx/SCANet.},
  archive   = {C_IROS},
  author    = {Yuxuan Wan and Kaichen Zhou and Jinhong Chen and Hao Dong},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801934},
  month     = {10},
  pages     = {5940-5947},
  title     = {SCANet: Correcting LEGO assembly errors with self-correct assembly network},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Barely-visible surface crack detection for wind turbine
sustainability. <em>IROS</em>, 5933–5939. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The production of wind energy is a crucial part of sustainable development and reducing the reliance on fossil fuels. Maintaining the integrity of wind turbines to produce this energy is a costly and time-consuming task requiring repeated inspection and maintenance. While autonomous drones have proven to make this process more efficient, the algorithms for detecting anomalies to prevent catastrophic damage to turbine blades have fallen behind due to some dangerous defects, such as hairline cracks, being barely-visible. Existing datasets and literature are lacking and tend towards detecting obvious and visible defects in addition to not being geographically diverse. In this paper we introduce a novel and diverse dataset of barelyvisible hairline cracks collected from numerous wind turbine inspections. To prove the efficacy of our dataset, we detail our end-to-end deployed turbine crack detection pipeline from the image acquisition stage to the use of predictions in providing automated maintenance recommendations to extend the life and efficiency of wind turbines.},
  archive   = {C_IROS},
  author    = {Sourav Agrawal and Isaac Corley and Conor Wallace and Clovis Vaughn and Jonathan Lwowski},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802755},
  month     = {10},
  pages     = {5933-5939},
  title     = {Barely-visible surface crack detection for wind turbine sustainability},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A scalable platform for robot learning and physical skill
data collection. <em>IROS</em>, 5925–5932. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The intersection of robotics and artificial intelligence led to a profound paradigm shift in Robot Learning. Robots have the capacity to replicate human actions and also dynamically adapt, innovate, and excel across a spectrum of tasks. However, the heterogeneity in the deployment of robot platforms and software frameworks poses considerable challenges in terms of systematic testing and comparative analyses. Additionally, the data scarcity of especially force controlled robot manipulation is still restraining the development of advanced foundation models. A reference platform with default software stack can help to increase comparability, reducing development time and collect a large amount of tactile robot manipulation data. To address on this problem, we developed a Parallel and Distributed Robot AI (PD.RAI) framework, comprising a scalable ensemble of Robot Learning Units (RLUs), a global database, and the Robot Cluster Intelligence (RoCI). Each RLU is endowed with robot arms, cameras, and local computational units to autonomously engage in planning, control, and local machine learning of tactile manipulation skills. The RoCI system oversees the learning process and schedules the RLUs tasks. To show the functionality of the system, two black-box optimization algorithms are compared within the robot skill learning domain. An experiment with 24 different optimization tasks is conducted in parallel. The algorithms are incorporated into the same existing default modules acting as a reference environment. This allows for a realistic comparison without sacrificing diversity of possible configurations and testing environments.},
  archive   = {C_IROS},
  author    = {Samuel Schneider and Yansong Wu and Lars Johannsmeier and Fan Wu and Sami Haddadin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801516},
  month     = {10},
  pages     = {5925-5932},
  title     = {A scalable platform for robot learning and physical skill data collection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MQE: Unleashing the power of interaction with multi-agent
quadruped environment. <em>IROS</em>, 5918–5924. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The advent of deep reinforcement learning (DRL) has significantly advanced the field of robotics, particularly in the control and coordination of quadruped robots. However, the complexity of real-world tasks often necessitates the deployment of multi-robot systems capable of sophisticated interaction and collaboration. To address this need, we introduce the Multi-agent Quadruped Environment (MQE), a novel platform designed to facilitate the development and evaluation of multi-agent reinforcement learning (MARL) algorithms in realistic and dynamic scenarios. MQE emphasizes complex interactions between robots and objects, hierarchical policy structures, and challenging evaluation scenarios that reflect real-world applications. We present a series of collaborative and competitive tasks within MQE, ranging from simple coordination to complex adversarial interactions, and benchmark state-of-the-art MARL algorithms. Our findings indicate that hierarchical reinforcement learning can simplify task learning, but also highlight the need for advanced algorithms capable of handling the intricate dynamics of multi-agent interactions. MQE serves as a stepping stone towards bridging the gap between simulation and practical deployment, offering a rich environment for future research in multi-agent systems and robot learning. For open-sourced code and more details of MQE, please refer to https://ziyanx02.github.io/multiagent-quadruped-environment/.},
  archive   = {C_IROS},
  author    = {Ziyan Xiong and Bo Chen and Shiyu Huang and Wei-Wei Tu and Zhaofeng He and Yang Gao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801682},
  month     = {10},
  pages     = {5918-5924},
  title     = {MQE: Unleashing the power of interaction with multi-agent quadruped environment},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pre-training on synthetic driving data for trajectory
prediction. <em>IROS</em>, 5910–5917. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accumulating substantial volumes of real-world driving data proves pivotal in the realm of trajectory forecasting for autonomous driving. Given the heavy reliance of current trajectory forecasting models on data-driven methodologies, we aim to tackle the challenge of learning general trajectory forecasting representations under limited data availability. We propose a pipeline-level solution to mitigate the issue of data scarcity in trajectory forecasting. The solution is composed of two parts: firstly, we adopt HD map augmentation and trajectory synthesis for generating driving data, and then we learn representations by pre-training on them. Specifically, we apply vector transformations to reshape the maps, and then employ a rule-based model to generate trajectories on both original and augmented scenes; thus enlarging the driving data without collecting additional real ones. To foster the learning of general representations within this augmented dataset, we comprehensively explore the different pre-training strategies, including extending the concept of a Masked AutoEncoder (MAE) for trajectory forecasting. Without bells and whistles, our proposed pipeline-level solution is general, simple, yet effective: we conduct extensive experiments to demonstrate the effectiveness of our data expansion and pre-training strategies, which outperform the baseline prediction model by large margins, e.g. 5.04%, 3.84% and 8.30% in terms of MR6, minADE6 and minFDE6. The pre-training dataset and the codes for pre-training and fine-tuning are released at https://github.com/yhli123/Pretraining_on_Synthetic_Driving_Data_for_Trajectory_Prediction.},
  archive   = {C_IROS},
  author    = {Yiheng Li and Seth Z. Zhao and Chenfeng Xu and Chen Tang and Chenran Li and Mingyu Ding and Masayoshi Tomizuka and Wei Zhan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802492},
  month     = {10},
  pages     = {5910-5917},
  title     = {Pre-training on synthetic driving data for trajectory prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing nighttime UAV tracking with light distribution
suppression. <em>IROS</em>, 5902–5909. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual object tracking has boosted extensive intelligent applications for unmanned aerial vehicles (UAVs). However, the state-of-the-art (SOTA) enhancers for nighttime UAV tracking always neglect the uneven light distribution in low-light images, inevitably leading to excessive enhancement in scenarios with complex illumination. To address these issues, this work proposes a novel enhancer, i.e., LDEnhancer, enhancing nighttime UAV tracking with light distribution suppression. Specifically, a novel image content refinement module is developed to decompose the light distribution information and image content information in the feature space, allowing for the targeted enhancement of the image content information. Then this work designs a new light distribution generation module to capture light distribution effectively. The features with light distribution information and image content information are fed into the different parameter estimation modules, respectively, for the parameter map prediction. Finally, leveraging two parameter maps, an innovative interweave iteration adjustment is proposed for the collaborative pixel-wise adjustment of low-light images. Additionally, a challenging nighttime UAV tracking dataset with uneven light distribution, namely NAT2024-2, is constructed to provide a comprehensive evaluation, which contains 40 challenging sequences with over 74K frames in total. Experimental results on the authoritative UAV benchmarks and the proposed NAT2024-2 demonstrate that LDEnhancer outperforms other SOTA low-light enhancers for nighttime UAV tracking. Furthermore, real-world tests on a typical UAV platform with an NVIDIA Orin NX confirm the practicality and efficiency of LDEnhancer. The code is available at https: //github.com/vision4robotics/LDEnhancer.},
  archive   = {C_IROS},
  author    = {Liangliang Yao and Changhong Fu and Yiheng Wang and Haobo Zuo and Kunhan Lu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802568},
  month     = {10},
  pages     = {5902-5909},
  title     = {Enhancing nighttime UAV tracking with light distribution suppression},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HS3-bench: A benchmark and strong baseline for hyperspectral
semantic segmentation in driving scenarios. <em>IROS</em>, 5895–5901.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Semantic segmentation is an essential step for many vision applications in order to understand a scene and the objects within. Recent progress in hyperspectral imaging technology enables the application in driving scenarios and the hope is that the devices perceptive abilities provide an advantage over RGB-cameras. Even though some datasets exist, there is no standard benchmark available to systematically measure progress on this task and evaluate the benefit of hyperspectral data. In this paper, we work towards closing this gap by providing the HyperSpectral Semantic Segmentation benchmark (HS3-Bench). It combines annotated hyperspectral images from three driving scenario datasets and provides standardized metrics, implementations, and evaluation protocols. We use the benchmark to derive two strong baseline models that surpass the previous state-of-the-art performances with and without pre-training on the individual datasets. Further, our results indicate that the existing learning-based methods benefit more from leveraging additional RGB training data than from leveraging the additional hyperspectral channels. This poses important questions for future research on hyperspectral imaging for semantic segmentation in driving scenarios. Code to run the benchmark and the strong baseline approaches are available under https://github.com/nickstheisen/hyperseg.},
  archive   = {C_IROS},
  author    = {Nick Theisen and Robin Bartsch and Dietrich Paulus and Peer Neubert},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801768},
  month     = {10},
  pages     = {5895-5901},
  title     = {HS3-bench: A benchmark and strong baseline for hyperspectral semantic segmentation in driving scenarios},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Subtle-diff: A dataset for precise recognition of subtle
differences among visually similar objects. <em>IROS</em>, 5888–5894.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual inspection robots used in factories and outdoor environments require the ability to accurately recognize visual differences between similar objects and further verbalize the recognition results to present the differences to humans. Despite the application of Large Language Models (LLMs) and multimodal LLMs across various domains, our research highlights their insufficiency in verbalizing nuanced differences across images. To address this, we leveraged LLMs and image generation AI to develop a dataset aimed at assessing difference recognition capabilities. We introduced two novel tasks using this dataset: selecting images based on their visual differences and a conditional difference captioning task, and evaluated existing Vision-Language Models (VLMs) on these tasks. Our findings reveal that advanced models like GPT-4V can describe subtle differences with comparative expressions, yet they fall short of matching human performance across all attributes. This discrepancy between model and human recognition, especially in identifying easily discernible differences, suggests that most current models lack the ability to directly compare image pairs for difference detection. Consequently, we propose a new model that incorporates an image-text similarity approach in the difference recognition task, showing superior performance over existing models, including GPT-4V. Our dataset and findings will contribute to advancements in differencing objects and improve robotic applications in visual inspection and object picking. The dataset is available at DICTA challenge page.},
  archive   = {C_IROS},
  author    = {Fumiya Matsuzawa and Yue Qiu and Yanjun Sun and Kenji Iwata and Hirokatsu Kataoka and Yutaka Satoh},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801725},
  month     = {10},
  pages     = {5888-5894},
  title     = {Subtle-diff: A dataset for precise recognition of subtle differences among visually similar objects},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deformable objects perception is just a few clicks away –
dense annotations from sparse inputs. <em>IROS</em>, 5880–5887. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deformable Objects (DOs), e.g. clothes, garments, cables, wires, and ropes, are pervasive in our everyday environment. Despite their importance and widespread presence, many limitations exist when deploying robotic systems to interact with DOs. One source of challenges arises from their complex perception. Deep learning algorithms can address these issues; however, extensive training data is usually required. This paper introduces a method for efficiently labeling DOs in images at the pixel level, starting from sparse annotations of key points. The method allows for the generation of a real-world dataset of DO images for segmentation purposes with minimal human effort. The approach comprises three main steps. First, a set of images is collected by a camera-equipped robotic arm. Second, a user performs sparse annotation via key points on just one image from the collected set. Third, the initial sparse annotations are converted into dense labels ready for segmentation tasks by leveraging a foundation model in zero-shot settings. Validation of the method on three different sets of DOs, comprising cloth and rope-like objects, showcases its practicality and efficiency. Consequently, the proposed method lays the groundwork for easy DO labeling and the seamless integration of deep learning perception of DOs into robotic agents.},
  archive   = {C_IROS},
  author    = {Alessio Caporali and Kevin Galassi and Matteo Pantano and Gianluca Palli},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802495},
  month     = {10},
  pages     = {5880-5887},
  title     = {Deformable objects perception is just a few clicks away – dense annotations from sparse inputs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Skin the sheep not only once: Reusing various depth datasets
to drive the learning of optical flow. <em>IROS</em>, 5873–5879. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Optical flow estimation is crucial for various applications in vision and robotics. As the difficulty of collecting ground truth optical flow in real-world scenarios, most of the existing methods of learning optical flow still adopt synthetic dataset for supervised training or utilize photometric consistency across temporally adjacent video frames to drive the unsupervised learning, where the former typically has issues of generalizability while the latter usually performs worse than the supervised ones. To tackle such challenges, we propose to leverage the geometric connection between optical flow estimation and stereo matching (based on the similarity upon finding pixel correspondences across images) to unify various real-world depth estimation datasets for generating supervised training data upon optical flow. Specifically, we turn the monocular depth datasets into stereo ones via synthesizing virtual disparity, thus leading to the flows along the horizontal direction; moreover, we introduce virtual camera motion into stereo data to produce additional flows along the vertical direction. Furthermore, we propose applying geometric augmentations on one image of an optical flow pair, encouraging the optical flow estimator to learn from more challenging cases. Lastly, as the optical flow maps under different geometric augmentations actually exhibit distinct characteristics, an auxiliary classifier which trains to identify the type of augmentation from the appearance of the flow map is utilized to further enhance the learning of the optical flow estimator. Our proposed method is general and is not tied to any particular flow estimator, where extensive experiments based on various datasets and optical flow estimation models verify its efficacy and superiority.},
  archive   = {C_IROS},
  author    = {Sheng-Chi Huang and Wei-Chen Chiu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801757},
  month     = {10},
  pages     = {5873-5879},
  title     = {Skin the sheep not only once: Reusing various depth datasets to drive the learning of optical flow},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nerve block target localization and needle guidance for
autonomous robotic ultrasound guided regional anesthesia. <em>IROS</em>,
5867–5872. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual servoing for the development of autonomous robotic systems capable of administering UltraSound (US) guided regional anesthesia requires real-time segmentation of nerves, needle tip localization and needle trajectory extrapolation. First, we recruited 227 patients to build a large dataset of 41,000 anesthesiologist annotated images from US videos of brachial plexus nerves and developed models to localize nerves in the US images. Generalizability of the best suited model was tested on the datasets constructed from separate US scanners. Using these nerve segmentation predictions, we define automated anesthesia needle targets by fitting an ellipse to the nerve contours. Next, we developed an image analysis tool to guide the needle toward their targets. For the segmentation of the needle, a natural RGB pre-trained neural network was first fine-tuned on a large US dataset for domain transfer and then adapted for the needle using a small dataset. The segmented needle’s trajectory angle is calculated using Radon transformation and the trajectory is extrapolated from the needle tip. The intersection of the extrapolated trajectory with the needle target guides the needle navigation for drug delivery. The needle trajectory’s average error was within acceptable range of 5 mm as per experienced anesthesiologists. The entire dataset has been released publicly for further study by the research community at https://github.com/Regional-US/},
  archive   = {C_IROS},
  author    = {Abhishek Tyagi and Abhay Tyagi and Manpreet Kaur and Richa Aggarwal and Kapil D. Soni and Jayanthi Sivaswamy and Anjan Trikha},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801467},
  month     = {10},
  pages     = {5867-5872},
  title     = {Nerve block target localization and needle guidance for autonomous robotic ultrasound guided regional anesthesia},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FEDORA: A flying event dataset fOr reactive behAvior.
<em>IROS</em>, 5859–5866. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability of resource-constrained biological systems such as fruitflies to perform complex and high-speed maneuvers in cluttered environments has been one of the prime sources of inspiration for developing vision-based autonomous systems. To emulate this capability, the perception pipeline of such systems must integrate information cues from tasks including optical flow and depth estimation, object detection and tracking, and segmentation, among others. However, the conventional approach of employing slow, synchronous inputs from standard frame-based cameras constrains these perception capabilities, particularly during high-speed maneuvers. Recently, event-based sensors have emerged as low latency and low energy alternatives to standard frame-based cameras for capturing high-speed motion, effectively speeding up perception and hence navigation. For coherence, all the perception tasks must be trained on the same input data. However, present-day datasets are curated mainly for a single or a handful of tasks and are limited in the rate of the provided ground truths. To address these limitations, we present Flying Event Dataset fOr Reactive behAviour (FEDORA) - a fully synthetic dataset for perception tasks, with raw data from frame-based cameras, event-based cameras, and Inertial Measurement Units (IMU), along with ground truths for depth, pose, and optical flow at a rate much higher than existing datasets.},
  archive   = {C_IROS},
  author    = {Amogh Joshi and Wachirawit Ponghiran and Adarsh Kosta and Manish Nagaraj and Kaushik Roy},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801807},
  month     = {10},
  pages     = {5859-5866},
  title     = {FEDORA: A flying event dataset fOr reactive behAvior},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SpectralWaste dataset: Multimodal data for waste sorting
automation. <em>IROS</em>, 5852–5858. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The increase in non-biodegradable waste is a worldwide concern. Recycling facilities play a crucial role, but their automation is hindered by the complex characteristics of waste recycling lines like clutter or object deformation. In addition, the lack of publicly available labeled data for these environments makes developing robust perception systems challenging. Our work explores the benefits of multimodal perception for object segmentation in real waste management scenarios. First, we present SpectralWaste, the first dataset collected from an operational plastic waste sorting facility that provides synchronized hyperspectral and conventional RGB images. This dataset contains labels for several categories of objects that commonly appear in sorting plants and need to be detected and separated from the main trash flow for several reasons, such as security in the management line or reuse. Additionally, we propose a pipeline employing different object segmentation architectures and evaluate the alternatives on our dataset, conducting an extensive analysis for both multimodal and unimodal alternatives. Our evaluation pays special attention to efficiency and suitability for real-time processing and demonstrates how hyperspectral imaging can bring a boost to RGB-only perception in these realistic industrial settings without much computational overhead.},
  archive   = {C_IROS},
  author    = {Sara Casao and Fernando Peña and Alberto Sabater and Rosa Castillón and Darío Suárez and Eduardo Montijano and Ana C. Murillo},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801797},
  month     = {10},
  pages     = {5852-5858},
  title     = {SpectralWaste dataset: Multimodal data for waste sorting automation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VRSO: Visual-centric reconstruction for static object
annotation. <em>IROS</em>, 5844–5851. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As a part of the perception results of intelligent driving systems, static object detection (SOD) in 3D space provides crucial cues for driving environment understanding. With the rapid deployment of deep neural networks for SOD tasks, the demand for high-quality training samples soars. The traditional, also reliable, way is manual labelling over the dense LiDAR point clouds and reference images. Though most public driving datasets adopt this strategy to provide SOD ground truth (GT), it is still expensive and time-consuming in practice. This paper introduces VRSO, a visual-centric approach for static object annotation. Experiments on the Waymo Open Dataset show that the mean reprojection error from VRSO annotation is only 2.6 pixels, around four times lower than the Waymo Open Dataset labels (10.6 pixels). VRSO is distinguished in low cost, high efficiency, and high quality: (1) It recovers static objects in 3D space with only camera images as input, and (2) manual annotation is barely involved since GT for SOD tasks is generated based on an automatic reconstruction and annotation pipeline.},
  archive   = {C_IROS},
  author    = {Chenyao Yu and Yingfeng Cai and Jiaxin Zhang and Hui Kong and Wei Sui and Cong Yang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801983},
  month     = {10},
  pages     = {5844-5851},
  title     = {VRSO: Visual-centric reconstruction for static object annotation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). UMAD: University of macau anomaly detection benchmark
dataset. <em>IROS</em>, 5836–5843. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Anomaly detection is critical in surveillance systems and patrol robots by identifying anomalous regions in images for early warning. Depending on whether reference data are utilized, anomaly detection can be categorized into anomaly detection with reference and anomaly detection without reference. Currently, anomaly detection without reference, which is closely related to out-of-distribution (OoD) object detection, struggles with learning anomalous patterns due to the difficulty of collecting sufficiently large and diverse anomaly datasets with the inherent rarity and novelty of anomalies. Alternatively, anomaly detection with reference employs the scheme of change detection to identify anomalies by comparing semantic changes between a reference image and a query one. However, there are very few ADr works due to the scarcity of public datasets in this domain. In this paper, we aim to address this gap by introducing the UMAD Benchmark Dataset. To our best knowledge, this is the first benchmark dataset designed specifically for anomaly detection with reference in robotic patrolling scenarios, e.g., where an autonomous robot is employed to detect anomalous objects by comparing a reference and a query video sequences. The reference sequences can be taken by the robot along a specified route when there are no anomalous objects in the scene. The query sequences are captured online by the robot when it is patrolling in the same scene following the same route. Our benchmark dataset is elaborated such that each query image can find a corresponding reference based on accurate robot localization along the same route in the pre-built 3D map, with which the reference and query images can be geometrically aligned using adaptive warping. Besides the proposed benchmark dataset, we evaluate the baseline models of ADr on this dataset. We hope this benchmark dataset will facilitate the advancement of ADr methods in the future. Our UMAD benchmark dataset will be publicly accessible at https://github.com/IMRL/UMAD.},
  archive   = {C_IROS},
  author    = {Dong Li and Lineng Chen and Cheng-Zhong Xu and Hui Kong},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802194},
  month     = {10},
  pages     = {5836-5843},
  title     = {UMAD: University of macau anomaly detection benchmark dataset},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring 3D human pose estimation and forecasting from the
robot’s perspective: The HARPER dataset. <em>IROS</em>, 5828–5835. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce HARPER, a novel dataset for 3D body pose estimation and forecasting in dyadic interactions between users and Spot, the quadruped robot manufactured by Boston Dynamics. The key-novelty of HARPER is its focus on the robot’s perspective, i.e., on the data captured by the robot’s sensors. This makes 3D body pose analysis challenging, as being close to the ground results in only partial captures of humans. The scenario underlying HARPER includes 15 actions, of which 10 involve physical contact between the robot and users. The corpus contains recordings not only from Spot’s built-in stereo cameras but also from a 6-camera OptiTrack system, with all recordings synchronized. This setup leads to ground-truth skeletal representations with a precision of less than a millimeter. Additionally, the corpus includes reproducible benchmarks for 3D Human Pose Estimation, Human Pose Forecasting, and Collision Prediction, all based on publicly available baseline approaches. This enables future HARPER users to rigorously compare their results with those provided in this work.},
  archive   = {C_IROS},
  author    = {Andrea Avogaro and Andrea Toaiari and Federico Cunico and Xiangmin Xu and Haralambos Dafas and Alessandro Vinciarelli and Emma Li and Marco Cristani},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802238},
  month     = {10},
  pages     = {5828-5835},
  title     = {Exploring 3D human pose estimation and forecasting from the robot’s perspective: The HARPER dataset},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HabiCrowd: A high performance simulator for crowd-aware
visual navigation. <em>IROS</em>, 5821–5827. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual navigation, a foundational aspect of Embodied AI (E-AI) and robotics has been extensively studied in the past few years. While many 3D simulators have been introduced for the visual navigation tasks, scarcely works have combined human dynamics, creating the gap between simulation and real-world applications. Furthermore, current 3D simulators incorporating human dynamics have several limitations, particularly in terms of computational efficiency, which is a promise of modern simulators. To overcome these issues, we introduce HabiCrowd, the new standard benchmark for crowd-aware visual navigation that includes a crowd dynamics model with diverse human settings into photorealistic environments. Empirical evaluations demonstrate that our proposed human dynamics model achieves state-of-the-art performance in collision avoidance while exhibiting superior computational efficiency compared to its counterparts. We leverage HabiCrowd to conduct several comprehensive studies on crowd-aware visual navigation tasks and human-robot interactions. The source code and data can be found at https://habicrowd.github.io/.},
  archive   = {C_IROS},
  author    = {An Vuong and Toan Nguyen and Minh Nhat Vu and Baoru Huang and H.T.T Binh and Thieu Vo and Anh Nguyen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801823},
  month     = {10},
  pages     = {5821-5827},
  title     = {HabiCrowd: A high performance simulator for crowd-aware visual navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robot generating data for learning generalizable visual
robotic manipulation. <em>IROS</em>, 5813–5820. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It has been a popular trend in AI to pretrain foundation models on massive data. However, collecting sufficient offline training trajectories for robot learning is particularly expensive since valid control actions are required. Therefore, most existing robotic datasets are collected from human experts. We tackle such a data collection issue with a new framework called &quot;robot self-teaching&quot;, which asks the robot to self-generate effective training data instead of relying on human demonstrators. Our key idea is to train a separate data-generation policy operating on the state space to automatically generate meaningful actions and trajectories with ever-growing complexities. Then, these generated data can be further used to train a visual policy with strong compositional generalization capabilities. We validate our framework in two visual manipulation testbeds, including a multi-object stacking domain and a popular RL benchmark &quot;Franka kitchen&quot;. Experiments show that the final visual policy trained on self-generated data can accomplish novel testing goals that require long-horizon robot executions. Project website https://sites.google.com/view/robot-self-teaching.},
  archive   = {C_IROS},
  author    = {Yunfei Li and Ying Yuan and Jingzhi Cui and Haoran Huan and Wei Fu and Jiaxuan Gao and Zekai Xu and Yi Wu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801742},
  month     = {10},
  pages     = {5813-5820},
  title     = {Robot generating data for learning generalizable visual robotic manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Frontier-based exploration for multi-robot rendezvous in
communication-restricted unknown environments. <em>IROS</em>, 5807–5812.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-robot rendezvous and exploration are fundamental challenges in the domain of mobile robotic systems. This paper addresses multi-robot rendezvous within an initially unknown environment where communication is only possible after the rendezvous. Traditionally, exploration has been focused on rapidly mapping the environment, often leading to suboptimal rendezvous performance in later stages. We adapt a standard frontier-based exploration technique to integrate exploration and rendezvous into a unified strategy, with a mechanism that allows robots to re-visit previously explored regions thus enhancing rendezvous opportunities. We validate our approach in 3D realistic simulations using ROS, showcasing its effectiveness in achieving faster rendezvous times compared to exploration strategies.},
  archive   = {C_IROS},
  author    = {Mauro Tellaroli and Matteo Luperto and Michele Antonazzi and Nicola Basilico},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801321},
  month     = {10},
  pages     = {5807-5812},
  title     = {Frontier-based exploration for multi-robot rendezvous in communication-restricted unknown environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive modeling and scheduling approach for
allocating distributed multi-robot software to the edge/cloud.
<em>IROS</em>, 5799–5806. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Offloading software modules to the edge/cloud can enhance a robot’s capabilities by leveraging massive computing power. However, determining which software module should be offloaded and scheduled to which robot/edge/cloud node is a challenging task, particularly for robot fleets with diverse tasks. In this paper, we tackle the software scheduling problem and introduce a taxonomy to categorize software modules and classify their applicability and requirements for offloading. Additionally, by using prior measurements, we model the compute cluster and formalize software scheduling as a multi-objective optimization problem which we tackle with a genetic algorithm. To evaluate our approach with a challenging setup, we build a mobile manipulation task using open-source frameworks and libraries in the Robot Operating System (ROS2) community in simulation as well as a mildly simplified real-world variant. Our evaluation shows significant improvements compared to the built-in scheduler of Kubernetes (K8s) regarding robotic specific metrics such as the rate of missed cycle time in both simulated and real-world experiments.},
  archive   = {C_IROS},
  author    = {Yongzhou Zhang and Florian Mirus and Frederik Pasch and Kay-Ulrich Scholl and Christian Wurll and Björn Hein},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802443},
  month     = {10},
  pages     = {5799-5806},
  title     = {A comprehensive modeling and scheduling approach for allocating distributed multi-robot software to the Edge/Cloud},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatic design of robot swarms that perform composite
missions: An approach based on inverse reinforcement learning.
<em>IROS</em>, 5791–5798. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate the automatic design of robot swarms that perform composite missions—that is, missions specified as the composition of consecutive sub-missions. Automatic design through performance optimization has become a viable and appealing approach to designing robot swarms. First, a user defines a mission by specifying a performance measure: a function indicating to what extent the swarm has attained its goal. An optimization process then generates suitable control software for the robots by maximizing the performance measure. The definition of a performance measure is a challenging task that requires expert input, which hinders the automatic nature of the approach. Recently, inverse reinforcement learning was introduced to minimize the need for human intervention in the automatic design of robot swarms. However, this method was only applied to single-objective missions. In this paper, we extend the method to address composite missions, by formulating and solving the design problem as a multi-objective optimization problem. We conduct simulations with a swarm of twenty e-puck robots that perform twelve composite missions. We compare the performance of the swarm when the robots operate with control software produced manually or using inverse reinforcement learning.},
  archive   = {C_IROS},
  author    = {Jeanne Szpirer and David Garzón Ramos and Mauro Birattari},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801506},
  month     = {10},
  pages     = {5791-5798},
  title     = {Automatic design of robot swarms that perform composite missions: An approach based on inverse reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Risk-aware non-myopic motion planner for large-scale robotic
swarm using CVaR constraints. <em>IROS</em>, 5784–5790. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Swarm robotics has garnered significant attention due to its ability to accomplish elaborate and synchronized tasks. Existing methodologies for motion planning of swarm robotic systems mainly encounter difficulties in scalability and safety guarantee. To address these limitations, we propose a Risk-aware swarm mOtion planner using conditional ValuE-at-Risk (ROVER) that systematically navigates large-scale swarms through cluttered environments while ensuring safety. ROVER formulates a finite-time model predictive control (FTMPC) problem predicated upon the macroscopic state of the robot swarm represented by a Gaussian Mixture Model (GMM) and integrates conditional value-at-risk (CVaR) to ensure collision avoidance. The key component of ROVER is imposing a CVaR constraint on the distribution of the Signed Distance Function between the swarm GMM and obstacles in the FTMPC to enforce collision avoidance. Utilizing the analytical expression of CVaR of a GMM derived in this work, we develop a computationally efficient solution to solve the non-linear constrained FTMPC through sequential linear programming. Simulations and comparisons with representative benchmark approaches demonstrate the effectiveness of ROVER in flexibility, scalability, and safety guarantee.},
  archive   = {C_IROS},
  author    = {Xuru Yang and Yunze Hu and Han Gao and Kang Ding and Zhaoyang Li and Pingping Zhu and Ying Sun and Chang Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802191},
  month     = {10},
  pages     = {5784-5790},
  title     = {Risk-aware non-myopic motion planner for large-scale robotic swarm using CVaR constraints},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decentralized acceleration-based bird-inspired flocking.
<em>IROS</em>, 5777–5783. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the following, we analyze and discuss the implementation of a novel approach for distributed flocking behavior applied to a group of Uncrewed Aerial Vehicle (UAV)s, also referred to as drones. Inspired by natural flocking phenomena observed in birds, which demonstrate coordinated movement in response to internal and external stimuli, we tackle the problem of robust and dynamic aerial motion for robots and design a control law based on a novel physical model. In contrast to previous works that rely on velocity or position-based references, this approach leverages an acceleration-based law to describe the collective dynamics of many interacting particles. As observed in the following, a third-order control possesses several advantages compared to first or second-order control, such as smoother transitions, better force balancing, and more responsive and dynamic behaviors. These advantages are thoroughly analyzed in the following, thanks to physics-based realistic simulations and field experiments with medium-sized UAVs in an unstructured outdoor environment.},
  archive   = {C_IROS},
  author    = {Luca Iacone and Erwin Lejeune and Tiziano Manoni and Sabato Manfredi and Dario Albani},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802737},
  month     = {10},
  pages     = {5777-5783},
  title     = {Decentralized acceleration-based bird-inspired flocking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrating online learning and connectivity maintenance for
communication-aware multi-robot coordination. <em>IROS</em>, 5770–5776.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a novel data-driven control strategy for maintaining connectivity in networked multi-robot systems. Existing approaches often rely on a predetermined communication model specifying whether pairwise robots can communicate given their relative distance to guide the connectivity-aware control design, which may not capture real-world communication conditions. To relax that assumption, we present the concept of Data-driven Connectivity Barrier Certificates, which utilize Control Barrier Functions (CBF) and Gaussian Processes (GP) to characterize the admissible control space for pairwise robots based on communication performance observed online. This allows robots to maintain a satisfying level of pairwise communication quality (measured by the received signal strength) while in motion. Then we propose a Data-driven Connectivity Maintenance (DCM) algorithm that combines (1) online learning of the communication signal strength and (2) a bi-level optimization-based control framework for the robot team to enforce global connectivity of the realistic multi-robot communication graph and minimally deviate from their task-related motions. We provide theoretical proofs to justify the properties of our algorithm and demonstrate its effectiveness through simulations with up to 20 robots.},
  archive   = {C_IROS},
  author    = {Yupeng Yang and Yiwei Lyu and Yanze Zhang and Ian Gao and Wenhao Luo},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802189},
  month     = {10},
  pages     = {5770-5776},
  title     = {Integrating online learning and connectivity maintenance for communication-aware multi-robot coordination},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Solving multi-robot task allocation and planning in
trans-media scenarios. <em>IROS</em>, 5764–5769. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trans-media robots, capable of operating across diverse environments, add significant complexity for multi-robot task allocation and planning problems. This paper introduces a novel approach to plan missions for such multi-robot systems, that addresses the associated specific complexities and constraints. It streamlines the overall mission planning process by decomposing it into tractable sub-problems, and addresses the issues of coalition formation, path planning, and task scheduling. It provides mission plans in very little computation time and allows to tackle large missions intractable by global planners, with negligible loss in plan optimality.},
  archive   = {C_IROS},
  author    = {Virgile De La Rochefoucauld and Simon Lacroix and Photchara Ratsamee and Haruo Takemura},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801394},
  month     = {10},
  pages     = {5764-5769},
  title     = {Solving multi-robot task allocation and planning in trans-media scenarios},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MULAN-WC: Multi-robot localization uncertainty-aware active
NeRF with wireless coordination. <em>IROS</em>, 5756–5763. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents MULAN-WC, a novel multi-robot 3D reconstruction framework that leverages wireless signal-based coordination between robots and Neural Radiance Fields (NeRF). Our approach addresses key challenges in multi-robot 3D reconstruction, including inter-robot pose estimation, localization uncertainty quantification, and active best-next-view selection. We introduce a method for using wireless Angle-of-Arrival (AoA) and ranging measurements to estimate relative poses between robots, as well as quantifying and incorporating the uncertainty embedded in the wireless localization of these pose estimates into the NeRF training loss to mitigate the impact of inaccurate camera poses. Furthermore, we propose an active view selection approach that accounts for robot pose uncertainty when determining the best-next-views to improve the 3D reconstruction, enabling faster convergence through intelligent view selection. Extensive experiments on both synthetic and real-world datasets demonstrate the effectiveness of our framework in theory and in practice. Leveraging wireless coordination and localization uncertainty-aware training, MULAN-WC can achieve high-quality 3D reconstruction that is close to applying the ground truth camera poses. Furthermore, the quantification of the information gain from a novel view enables consistent rendering quality improvement with incrementally captured images by commanding the robot to the novel view position. Our hardware experiments showcase the practicality of deploying MULAN-WC to real robotic systems.},
  archive   = {C_IROS},
  author    = {Weiying Wang and Victor Cai and Stephanie Gil},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801839},
  month     = {10},
  pages     = {5756-5763},
  title     = {MULAN-WC: Multi-robot localization uncertainty-aware active NeRF with wireless coordination},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Team coordination on graphs: Problem, analysis, and
algorithms. <em>IROS</em>, 5748–5755. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Team Coordination on Graphs with Risky Edges (TCGRE) is a recently emerged problem, in which a robot team collectively reduces graph traversal cost through support from one robot to another when the latter traverses a risky edge. Resembling the traditional Multi-Agent Path Finding (MAPF) problem, both classical and learning-based methods have been proposed to solve TCGRE, however, they lacked either computational efficiency or optimality assurance. In this paper, we reformulate TCGRE as a constrained optimization problem and perform a rigorous mathematical analysis. Our theoretical analysis shows the NP-hardness of TCGRE by reduction from the Maximum 3D Matching problem and that efficient decomposition is a key to tackle this combinatorial optimization problem. Furthermore, we design three classes of algorithms to solve TCGRE, i.e., Joint State Graph (JSG) based, coordination based, and receding-horizon sub-team based solutions. Each of these proposed algorithms enjoys different provable optimality and efficiency characteristics that are demonstrated in our extensive experiments.},
  archive   = {C_IROS},
  author    = {Yanlin Zhou and Manshi Limbu and Gregory J. Stein and Xuan Wang and Daigo Shishika and Xuesu Xiao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802095},
  month     = {10},
  pages     = {5748-5755},
  title     = {Team coordination on graphs: Problem, analysis, and algorithms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed model predictive covariance steering.
<em>IROS</em>, 5740–5747. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes Distributed Model Predictive Covariance Steering (DiMPCS) for multi-agent control under stochastic uncertainty. The scope of our approach is to blend covariance steering theory, distributed optimization and model predictive control (MPC) into a single framework that is safe, scalable and decentralized. Initially, we pose a problem formulation that uses the Wasserstein distance to steer the state distributions of a multi-agent system to desired targets, and probabilistic constraints to ensure safety. We then transform this problem into a finite-dimensional optimization one by utilizing a disturbance feedback policy parametrization for covariance steering and a tractable approximation of the safety constraints. To solve the latter problem, we derive a decentralized consensus-based algorithm using the Alternating Direction Method of Multipliers. This method is then extended to a receding horizon form, which yields the proposed DiMPCS algorithm. Simulation experiments on a variety of multi-robot tasks with up to hundreds of robots demonstrate the effectiveness of DiMPCS. The superior scalability and performance of the proposed method is also highlighted through a comparison against related stochastic MPC approaches. Finally, hardware results on a multi-robot platform also verify the applicability of DiMPCS on real systems.},
  archive   = {C_IROS},
  author    = {Augustinos D. Saravanos and Isin M. Balci and Efstathios Bakolas and Evangelos A. Theodorou},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802104},
  month     = {10},
  pages     = {5740-5747},
  title     = {Distributed model predictive covariance steering},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph neural network-based multi-agent reinforcement
learning for resilient distributed coordination of multi-robot systems.
<em>IROS</em>, 5732–5739. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing multi-agent coordination techniques are often fragile and vulnerable to anomalies such as agent attrition and communication disturbances, which are quite common in the real-world deployment of systems like field robotics. To better prepare these systems for the real world, we present a graph neural network (GNN)-based multi-agent reinforcement learning (MARL) method for resilient distributed coordination of a multi-robot system. Our method, Multi-Agent Graph Embedding-based Coordination (MAGEC), is trained using multi-agent proximal policy optimization (PPO) and enables distributed coordination around global objectives under agent attrition, partial observability, and limited or disturbed communications. We use a multi-robot patrolling scenario to demonstrate our MAGEC method in a ROS 2-based simulator and then compare its performance with prior coordination approaches. Results demonstrate that MAGEC outperforms existing methods in several experiments involving agent attrition and communication disturbance, and provides competitive results in scenarios without such anomalies.},
  archive   = {C_IROS},
  author    = {Anthony Goeckner and Yueyuan Sui and Nicolas Martinet and Xinliang Li and Qi Zhu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802510},
  month     = {10},
  pages     = {5732-5739},
  title     = {Graph neural network-based multi-agent reinforcement learning for resilient distributed coordination of multi-robot systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MAP-NBV: Multi-agent prediction-guided next-best-view
planning for active 3D object reconstruction. <em>IROS</em>, 5724–5731.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Next-Best View (NBV) planning is a long-standing problem of determining where to obtain the next best view of an object from, by a robot that is viewing the object. There are a number of methods for choosing NBV based on the observed part of the object. In this paper, we investigate how predicting the unobserved part helps with the efficiency of reconstructing the object. We present, Multi-Agent Prediction-Guided NBV (MAP-NBV), a decentralized coordination algorithm for active 3D reconstruction with multi-agent systems. Prediction-based approaches have shown great improvement in active perception tasks by learning the cues about structures in the environment from data. However, these methods primarily focus on single-agent systems. We design a decentralized next-best-view approach that utilizes geometric measures over the predictions and jointly optimizes the information gain and control effort for efficient collaborative 3D reconstruction of the object. Our method achieves 19% improvement over the non-predictive multi-agent approach in simulations using AirSim and ShapeNet. We make our code publicly available through our project website: http://raaslab.org/projects/MAPNBV/.},
  archive   = {C_IROS},
  author    = {Harnaik Dhami and Vishnu Dutt Sharma and Pratap Tokekar},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802735},
  month     = {10},
  pages     = {5724-5731},
  title     = {MAP-NBV: Multi-agent prediction-guided next-best-view planning for active 3D object reconstruction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Behavior tree based decentralized multi-agent coordination
for balanced servicing of time varying task queues. <em>IROS</em>,
5718–5723. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this article, we present a reactive multi-agent coordination architecture for the management of material flows between production/pickup stages and delivery/drop-off stages, in scenarios such as underground mines and automated factory floors. The pickup and delivery stages are modelled as variable task queues, with no a priori information about the inflow into the production queues. The proposed solution coordinates the movement of a group of mobile agents operating between the two stages in a reactive and scalable manner, so that the material is transported from multiple production queues to multiple delivery queues in a balanced/equalized manner. In such a scenario, centralized planners suffer from low reactivity and poor scaling, as the number of agents and number of queues increases. To overcome this problem, we propose a decentralized approach comprising of two separate auction-based task distribution systems for the production and delivery stages, along with behavior-tree based management of agent autonomy and task bidding. Each auction system tracks the length of production/delivery queues and solves the optimal task assignment, based on the bids submitted by the agents. The agents participate in one of the two auction systems at any given time, based on the status of the behavior tree executing the two-stage tasks. We analytically show that the proposed decentralized auctioning approach along with agent autonomy and bidding managed by behavior trees, offers better scalability and reactiveness compared to the centralized approach. The proposed methodology is experimentally validated in a lab environment, in three illustrative material flow management scenarios, using TurtleBot3 robots as agents.},
  archive   = {C_IROS},
  author    = {Niklas Dahlquist and Akshit Saradagi and George Nikolakopoulos},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801900},
  month     = {10},
  pages     = {5718-5723},
  title     = {Behavior tree based decentralized multi-agent coordination for balanced servicing of time varying task queues},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Autonomous localization of multiple ionizing radiation
sources using miniature single-layer compton cameras onboard a group of
micro aerial vehicles. <em>IROS</em>, 5710–5717. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A novel method for autonomous localization of multiple sources of gamma radiation using a group of Micro Aerial Vehicles (MAVs) is presented in this paper. The method utilizes an extremely lightweight (44 g) Compton camera MiniPIX TPX3. The compact size of the detector allows for deployment onboard safe and agile small-scale Unmanned Aerial Vehicles (UAVs). The proposed radiation mapping approach fuses measurements from multiple distributed Compton camera sensors to accurately estimate the positions of multiple radioactive sources in real time. Unlike commonly used intensity-based detectors, the Compton camera reconstructs the set of possible directions towards a radiation source from just a single ionizing particle. Therefore, the proposed approach can localize radiation sources without having to estimate the gradient of a radiation field or contour lines, which require longer measurements. The instant estimation is able to fully exploit the potential of highly mobile MAVs. The radiation mapping method is combined with an active search strategy, which coordinates the future actions of the MAVs in order to improve the quality of the estimate of the sources’ positions, as well as to explore the area of interest faster. The proposed solution is evaluated in simulation and real-world experiments with multiple Cesium-137 radiation sources.},
  archive   = {C_IROS},
  author    = {Michal Werner and Tomáš Báča and Petr Štibinger and Daniela Doubravová and Jaroslav Šolc and Jan Rusňák and Martin Saska},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802808},
  month     = {10},
  pages     = {5710-5717},
  title     = {Autonomous localization of multiple ionizing radiation sources using miniature single-layer compton cameras onboard a group of micro aerial vehicles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable networked feature selection with randomized
algorithm for robot navigation. <em>IROS</em>, 5704–5709. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the problem of sparse selection of visual features for localizing a team of robots navigating in an unknown environment, where robots can exchange relative position measurements with neighbors. We select a set of the most informative features by anticipating their importance in robots localization by simulating trajectories of robots over a prediction horizon. Through theoretical proofs, we establish a crucial connection between graph Laplacian and the importance of features. We leverage a scalable randomized algorithm for sparse sums of positive semidefinite matrices to efficiently select a set of the most informative features.},
  archive   = {C_IROS},
  author    = {Vivek Pandey and Arash Amini and Guangyi Liu and Ufuk Topcu and Qiyu Sun and Kostas Daniilidis and Nader Motee},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802446},
  month     = {10},
  pages     = {5704-5709},
  title     = {Scalable networked feature selection with randomized algorithm for robot navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decentralized collaborative localization and map update with
buildings. <em>IROS</em>, 5696–5703. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In urban environments where GNSS performance is degraded, localization can be performed using stable and geo-referenced map features detected by on-board sensors. Prior maps are prone to errors which have a direct impact on localization accuracy. By exchanging observed features and sharing their maps, vehicles can simultaneously improve their localization and update the map. This paper deals with indirect collaboration, where vehicles do not observe each other directly. The features are obtained from building facades using 3D lidar sensors. The paper emphasizes real-time decentralized collaboration with direct communication between vehicles, without the need for a central server. The collaboration takes place when vehicles perceive the same geo-referenced facades. Vehicle poses and maps are collaboratively updated using a Schmidt Kalman filter that carefully manages the cross-covariance terms. To maintain consistent estimates, the Kullback-Leibler Average is used. We also present a lidar data processing pipeline to obtain reliable observations from building facades. Real tests carried out with experimental vehicles on the university campus are reported. The results show that indirect collaboration makes a significant contribution to localization and map update when compared to a standalone method.},
  archive   = {C_IROS},
  author    = {Maxime Escourrou and Joelle Al Hage and Philippe Bonnifait},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801783},
  month     = {10},
  pages     = {5696-5703},
  title     = {Decentralized collaborative localization and map update with buildings},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MERSYS: A collaborative estimation and dense mapping system
for multi-agent generic SLAM. <em>IROS</em>, 5688–5695. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent collaborative Simultaneous Localization and Mapping (SLAM) is an effective way for large-scale mapping. However, this approach, which relies on Visual-Inertial Odometry(VIO) as input, suffers from limitations such as susceptibility to environmental influences and the difficulty in accurately constructing dense 3D maps. To address these challenges, this paper presents Multi-Estimation Robust SLAM System (MERSYS), a novel framework for three-dimensional dense mapping based on the fusion of Lidar-Inertial Odometry(LIO) and VIO. Benefiting from lower communication’s costs and dense information acquisition capability, the proposed framework aims to achieve compatibility in processing both LIO and VIO inputs, establish joint loop closure detection to enable multi-map fusion, and then create a comprehensive global 3D dense point cloud map. Furthermore, an efficient communication strategy has been proposed to enable bidirectional transmission of dense and voluminous data. Experimental evaluations conducted on the publicly available HILTI SLAM 2021 dataset [10] as well as a real world dataset. Experimental results show that MERSYS achieves better results than state-of-the-art methods. The source code is available on the GitHub1.},
  archive   = {C_IROS},
  author    = {Qianhua Lai and Enhao Zhao and Shicai Fan and Jianxiao Zou},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801603},
  month     = {10},
  pages     = {5688-5695},
  title     = {MERSYS: A collaborative estimation and dense mapping system for multi-agent generic SLAM},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing interaction space: Enlarging the capture volume
for multiple portable motion capture devices. <em>IROS</em>, 5680–5687.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Markerless motion capture devices such as the Leap Motion Controller (LMC) have been extensively used for tracking hand, wrist, and forearm positions as an alternative to Marker-based Motion Capture (MMC). However, previous studies have highlighted the subpar performance of LMC in reliably recording hand kinematics. In this study, we employ four LMC devices to optimize their collective tracking volume, aiming to enhance the accuracy and precision of hand kinematics. Through Monte Carlo simulation, we determine an optimized layout for the four LMC devices and subsequently conduct reliability and validity experiments encompassing 1560 trials across ten subjects. The combined tracking volume is validated against an MMC system, particularly for kinematic movements involving wrist, index, and thumb flexion. Utilizing calculation resources in one computer, our result of the optimized configuration has a better visibility rate with a value of 0.05 ± 0.55 compared to the initial configuration with -0.07 ± 0.40. Multiple Leap Motion Controllers (LMCs) have proven to increase the interaction space of capture volume but are still unable to give agreeable measurements from dynamic movement.},
  archive   = {C_IROS},
  author    = {Muhammad Hilman Fatoni and Christopher Herneth and Junnan Li and Fajar Budiman and Amartya Ganguly and Sami Haddadin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801852},
  month     = {10},
  pages     = {5680-5687},
  title     = {Optimizing interaction space: Enlarging the capture volume for multiple portable motion capture devices},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint pedestrian trajectory prediction through posterior
sampling. <em>IROS</em>, 5672–5679. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Joint pedestrian trajectory prediction has long grappled with the inherent unpredictability of human behaviors. Recent works employing conditional diffusion models in trajectory prediction have exhibited notable success. Nevertheless, the heavy dependence on accurate historical data results in their vulnerability to noise disturbances and data incompleteness. To improve the robustness and reliability, we introduce the Guided Full Trajectory Diffuser (GFTD), a novel diffusion-based framework that translates prediction as the inverse problem of spatial-temporal inpainting and models the full joint trajectory distribution which includes both history and the future. By learning from the full trajectory and leveraging flexible posterior sampling methods, GFTD can produce accurate predictions while improving the robustness that can generalize to scenarios with noise perturbation or incomplete historical data. Moreover, the pre-trained model enables controllable generation without an additional training budget. Through rigorous experimental evaluation, GFTD exhibits superior performance in joint trajectory prediction with different data quality and in controllable generation tasks. See more results at https://sites.google.com/andrew.cmu.edu/posterior-sampling-prediction.},
  archive   = {C_IROS},
  author    = {Haotian Lin and Yixiao Wang and Mingxiao Huo and Chensheng Peng and Zhiyuan Liu and Masayoshi Tomizuka},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802382},
  month     = {10},
  pages     = {5672-5679},
  title     = {Joint pedestrian trajectory prediction through posterior sampling},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GSLoc: Visual localization with 3D gaussian splatting.
<em>IROS</em>, 5664–5671. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present GSLoc: a new visual localization method that performs dense camera alignment using 3D Gaussian Splatting as a map representation of the scene. GSLoc backpropagates pose gradients over the rendering pipeline to align the rendered and target images, while it adopts a coarse-to-fine strategy by utilizing blurring kernels to mitigate the non-convexity of the problem and improve the convergence. The results show that our approach succeeds at visual localization in challenging conditions of relatively small overlap between initial and target frames inside textureless environments when state-of-the-art neural sparse methods provide inferior results. Using the byproduct of realistic rendering from the 3DGS map representation, we show how to enhance localization results by mixing a set of observed and virtual reference keyframes when solving the image retrieval problem. We evaluate our method both on synthetic and real-world data, discussing its advantages and application potential.},
  archive   = {C_IROS},
  author    = {Kazii Botashev and Vladislav Pyatov and Gonzalo Ferrer and Stamatios Lefkimmiatis},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801919},
  month     = {10},
  pages     = {5664-5671},
  title     = {GSLoc: Visual localization with 3D gaussian splatting},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BEVLoc: Cross-view localization and matching via
birds-eye-view synthesis. <em>IROS</em>, 5656–5663. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ground to aerial matching is a crucial and challenging task in outdoor robotics, particularly when GPS is absent or unreliable. Structures like buildings or large dense forests create interference, requiring GNSS replacements for global positioning estimates. The true difficulty lies in reconciling the perspective difference between the ground and air images for acceptable localization.Taking inspiration from the autonomous driving community, we propose a novel framework for synthesizing a birds-eye-view (BEV) scene representation to match and localize against an aerial map in off-road environments. We leverage contrastive learning with domain specific hard negative mining to train a network to learn similar representations between the synthesized BEV and the aerial map.During inference, BEVLoc guides the identification of the most probable locations within the aerial map through a coarse-to-fine matching strategy. Our results demonstrate promising initial outcomes in extremely difficult forest environments with limited semantic diversity. We analyze our model’s performance for coarse and fine matching, assessing both the raw matching capability of our model and its performance as a GNSS replacement.Our work delves into off-road map localization while establishing a foundational baseline for future developments in localization. Our code is available at: https://github.com/rpl-cmu/bevloc},
  archive   = {C_IROS},
  author    = {Christopher Klammer and Michael Kaess},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801643},
  month     = {10},
  pages     = {5656-5663},
  title     = {BEVLoc: Cross-view localization and matching via birds-eye-view synthesis},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LF2SLAM: Learning-based features for visual SLAM.
<em>IROS</em>, 5648–5655. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous robot navigation relies on the robot’s ability to understand its environment for localization, typically using a Visual Simultaneous Localization And Mapping (SLAM) algorithm that processes image sequences. While state-of-the-art methods have shown remarkable performance, they still have limitations. Geometric VO algorithms that leverage hand-crafted feature extractors require careful hyper-parameter tuning. Conversely, end-to-end data-driven VO algorithms suffer from limited generalization capabilities and require large datasets for their proper optimizations. Recently, promising results have been shown by hybrid approaches that integrate robust data-driven feature extraction with the geometric estimation pipeline. In this work, we follow these intuitions and propose a hybrid VO method, namely Learned Features For SLAM (LF2SLAM), that combines a deep neural network for feature extraction with a standard VO pipeline. The network is trained in a data-driven framework that includes a pose estimation component to learn feature extractors that are tailored for VO tasks. A novel loss function modification is introduced, using a binary mask that considers only the informative features. The experimental evaluation performed shows that our approach has remarkable generalization capabilities in scenarios that differ from those used for training. Furthermore, LF2SLAM exhibits robustness in more challenging scenarios, i.e., characterized by the presence of poor lighting and low amount of texture, with respect to the state-of-the-art ORB-SLAM3 algorithm.},
  archive   = {C_IROS},
  author    = {Marco Legittimo and Francesco Crocetti and Mario Luca Fravolini and Giuseppe Mollica and Gabriele Costante},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801935},
  month     = {10},
  pages     = {5648-5655},
  title     = {LF2SLAM: Learning-based features for visual SLAM},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LiDAR-visual-inertial tightly-coupled odometry with adaptive
learnable fusion weights. <em>IROS</em>, 5641–5647. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we address the sensitivity of the 3D LiDAR-based localization to environmental structural ambiguity. Although existing approaches employ additional sensors, such as cameras and inertial measurement units, to account for such ambiguities, multi-sensor localization is still an open problem. Limitations are from the need to tune fusion parameters to compensate for limited ambiguity detection manually. Therefore, we propose a feature-based localization method that learns the fusion parameters using ground truth and thus supports autonomous mobile robotic systems in new locations. The method combines planar surface LiDAR features with close and far camera features, and its further advantage is an online adjustment of the feature weights based on the measured environment ambiguity. The evaluation has been performed on the existing M2DGR dataset and custom dataset with geometrical ambiguities. The proposed method is competitive to or outperforms the existing LiDAR-based methods F-LOAM and LIO-SAM and the Visual-Inertial localization method VINS-Mono. Based on the reported results, the proposed method is a vital combination of LiDAR-based and visual features.},
  archive   = {C_IROS},
  author    = {Vsevolod Hulchuk and Jan Bayer and Jan Faigl},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802180},
  month     = {10},
  pages     = {5641-5647},
  title     = {LiDAR-visual-inertial tightly-coupled odometry with adaptive learnable fusion weights},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Renderable street view map-based localization: Leveraging 3D
gaussian splatting for street-level positioning. <em>IROS</em>,
5635–5640. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce a new method that first utilizes 3D Gaussian splatting in street-level localization problem. Robust localization with street-level real-world images such as street view is a major issue for autonomous vehicle, augmented reality (AR) navigation, and outdoor mobile robots. The objective is to determine the position and orientation of a query image that matches a street view database composed of RGB images. However, given the limited information available in the street view images, accurately determining the location solely based on this data presents a significant challenge. To address this challenge, we propose a novel method called renderable street view map-based localization (RSM-Loc). This approach enhances the localization process by augmenting 2D street view images into a renderable 3D map using 3D Gaussian splatting, to resolve street-level localization problems. Upon receiving a query RGB image without geometry information, the proposed method renders 2D images from a pre-made renderable map and compares image pose similarities between the rendered images and the query image. Through iterations of this process, the proposed method eventually estimates the pose of the given query image. The experimental results demonstrate that RSM-Loc outperforms the baselines with neural-field-based localization. Additionally, we conduct deep analysis on the proposed method to show that our method can serve as a new concept for the street-level localization problem.},
  archive   = {C_IROS},
  author    = {Howoong Jun and Hyeonwoo Yu and Songhwai Oh},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802730},
  month     = {10},
  pages     = {5635-5640},
  title     = {Renderable street view map-based localization: Leveraging 3D gaussian splatting for street-level positioning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modular meshed ultra-wideband aided inertial navigation with
robust anchor calibration. <em>IROS</em>, 5627–5634. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a generic filter-based state estimation framework that supports two state-decoupling strategies based on cross-covariance factorization. These strategies reduce the computational complexity and inherently support true modularity – a perquisite for handling and processing meshed range measurements among a time-varying set of devices. In order to utilize these measurements in the estimation framework, positions of newly detected stationary devices (anchors) and the pairwise biases between the ranging devices are required. In this work an autonomous calibration procedure for new anchors is presented, that utilizes range measurements from multiple tags as well as already known anchors. To improve the robustness, an outlier rejection method is introduced. After the calibration is performed, the sensor fusion framework obtains initial beliefs of the anchor positions and dictionaries of pairwise biases, in order to fuse range measurements obtained from new anchors tightly-coupled. The effectiveness of the filter and calibration framework has been validated through evaluations on a recorded dataset and real-world experiments.},
  archive   = {C_IROS},
  author    = {Roland Jung and Luca Santoro and Davide Brunelli and Daniele Fontanelli and Stephan Weiss},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802297},
  month     = {10},
  pages     = {5627-5634},
  title     = {Modular meshed ultra-wideband aided inertial navigation with robust anchor calibration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3D localization of objects buried within granular material
using a distributed 3-axis tactile sensor. <em>IROS</em>, 5621–5626. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While visual sensing is often the predominant modality for a robot to localize objects in the environment, tactile and force sensing become crucial when objects are occluded, poorly visible, or buried. However, existing works on locating buried objects rely solely on force measurements at a single contact point on the robot end-effector, making 3D localization very challenging. This paper presents an alternative approach using a tactile sensor that measures both normal and shear forces (i.e. 3-axis) on distributed points; three Long Short-Term Memory (LSTM) models are trained with real-world data to perform real-time 3D localization (i.e. distance, direction and depth) of an object buried within a granular material. Our experimental results suggest that measuring both normal and shear forces (instead of just normal) on distributed contact points (instead of only one point) is essential for the accurate 3D localization of buried objects.},
  archive   = {C_IROS},
  author    = {Zhengqi Chen and Elisabetta Versace and Lorenzo Jamone},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802276},
  month     = {10},
  pages     = {5621-5626},
  title     = {3D localization of objects buried within granular material using a distributed 3-axis tactile sensor},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SOS-match: Segmentation for open-set robust correspondence
search and robot localization in unstructured environments.
<em>IROS</em>, 5613–5620. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present SOS-Match, a novel framework for detecting and matching objects in unstructured environments. Our system consists of 1) a front-end mapping pipeline using a zero-shot segmentation model to extract object masks from images and track them across frames and 2) a frame alignment pipeline that uses the geometric consistency of object relationships to efficiently localize across a variety of conditions. We evaluate SOS-Match on the Båtvik seasonal dataset which includes drone flights collected over a coastal plot of southern Finland during different seasons and lighting conditions. Results show that our approach is more robust to changes in lighting and appearance than classical image feature-based approaches or global descriptor methods, and it provides more viewpoint invariance than learning-based feature detection and description approaches. SOS-Match localizes within a reference map up to 46x faster than other feature-based approaches and has a map size less than 0.5% the size of the most compact other maps. SOS-Match is a promising new approach for landmark detection and correspondence search in unstructured environments that is robust to changes in lighting and appearance and is more computationally efficient than other approaches, suggesting that the geometric arrangement of segments is a valuable localization cue in unstructured environments. We release our datasets at https://acl.mit.edu/SOS-Match/.},
  archive   = {C_IROS},
  author    = {Annika Thomas and Jouko Kinnari and Parker C. Lusk and Kota Kondo and Jonathan P. How},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801471},
  month     = {10},
  pages     = {5613-5620},
  title     = {SOS-match: Segmentation for open-set robust correspondence search and robot localization in unstructured environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ReLoc-aligner: Orientation-aware scene descriptor for
re-localization within a 3D point cloud map. <em>IROS</em>, 5605–5612.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a new orientation-aware scene descriptor ReLoc-Aligner for re-localization of a 3D point cloud. Re-localization within a 3D point cloud map is crucial for conducting Simultaneous Localization and Mapping (SLAM). Existing re-localization or place recognition methods of 3D LiDAR sensor data aim to estimate the current position of the sensor robustly to orientation changes. However, they do not determine the current orientation of the sensor within a 3D point cloud map, which limits their applications to re-localization or loop closing in SLAM. On the other hand, existing methods capable of orientation estimation tend to be slower than them. Our scene descriptor has a property of orientation awareness that enables us to extract the orientation difference between two scans directly from the descriptor. This is useful for the registration of point clouds from a good initial estimate, which leads to better re-localization of a scan. We propose a training method for the new descriptor. In addition, we develop fast querying and re-localization methods using the descriptors. Intensive experiments demonstrate that the proposed method is superior to the existing state-of-the-art methods in both place recognition and orientation estimation.},
  archive   = {C_IROS},
  author    = {SungJoon Cho and Jun-Sik Kim},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802124},
  month     = {10},
  pages     = {5605-5612},
  title     = {ReLoc-aligner: Orientation-aware scene descriptor for re-localization within a 3D point cloud map},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). U-BEV: Height-aware bird’s-eye-view segmentation and neural
map-based relocalization. <em>IROS</em>, 5597–5604. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Efficient relocalization is essential for intelligent vehicles when GPS reception is insufficient or sensor-based localization fails. Recent advances in Bird’s-Eye-View (BEV) segmentation allow for accurate estimation of local scene appearance and in turn, can benefit the relocalization of the vehicle. However, one downside of BEV methods is the heavy computation required to leverage the geometric constraints. This paper presents U-BEV, a U-Net inspired architecture that extends the current state-of-the-art by allowing the BEV to reason about the scene on multiple height layers before flattening the BEV features. We show that this extension boosts the performance of the U-BEV by up to 4.11 IoU. Additionally, we combine the encoded neural BEV with a differentiable template matcher to perform relocalization on neural SD-map data. The model is fully end-to-end trainable and outperforms transformer-based BEV methods of similar computational complexity by 1.7 to 2.8 mIoU and BEV-based relocalization by over 26% Recall Accuracy on the nuScenes dataset.},
  archive   = {C_IROS},
  author    = {Andrea Boscolo Camiletto and Alfredo Bochicchio and Alexander Liniger and Dengxin Dai and Abel Gawel},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802787},
  month     = {10},
  pages     = {5597-5604},
  title     = {U-BEV: Height-aware bird’s-eye-view segmentation and neural map-based relocalization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Geolocation on cartographic maps with multi-modal fusion.
<em>IROS</em>, 5589–5596. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We explore the geolocation problem, aiming to localize ground-view images on cartographic maps, without the need of any GPS priors. This task mimics the human wayfinding ability and offers high scalability and robustness by using the compact and semantic representations of maps. Current methods often rely on 2D maps to encode dense contextual information for ground-to-map matching. In this paper, we lift ground-to-map matching to a 2.5D space, where heights of structures (e.g. buildings) provide richer geometric information to guide the matching process. We propose a new approach to learning representative embeddings from multi-modal data. Specifically, we establish a projection relationship between 2D and 2.5D space. The projection is further used to combine multi-modal features from the 2D and 2.5D maps using an effective pixel-to-point fusion method. By encoding crucial geometric cues, our method learns discriminative location embeddings for matching panoramic images and maps. Additionally, we construct the first large-scale multi-modal geolocation dataset to validate our method and facilitate future research. Both single-image based and route based geolocation experiments are conducted to test our method. Extensive experiments demonstrate that the proposed method achieves significantly higher geolocation accuracy and faster convergence than previous 2D map-based approaches.},
  archive   = {C_IROS},
  author    = {Mengjie Zhou and Liu Liu and Yiran Zhong and Andrew Calway},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801404},
  month     = {10},
  pages     = {5589-5596},
  title     = {Geolocation on cartographic maps with multi-modal fusion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging neural radiance field in descriptor synthesis for
keypoints scene coordinate regression. <em>IROS</em>, 5581–5588. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Classical structural-based visual localization methods offer high accuracy but face trade-offs in terms of storage, speed, and privacy. A recent innovation, keypoint scene coordinate regression (KSCR) named D2S addresses these issues by leveraging graph attention networks to enhance keypoint relationships and predict their 3D coordinates using a simple multilayer perceptron (MLP). Camera pose is then determined via PnP+RANSAC, using established 2D-3D correspondences. While KSCR achieves competitive results, rivaling state-of-the-art image-retrieval methods like HLoc across multiple benchmarks, its performance is hindered when data samples are limited due to the deep learning model’s reliance on extensive data. This paper proposes a solution to this challenge by introducing a pipeline for keypoint descriptor synthesis using Neural Radiance Field (NeRF). By generating novel poses and feeding them into a trained NeRF model to create new views, our approach enhances the KSCR’s generalization capabilities in data-scarce environments. The proposed system could significantly improve localization accuracy by up to 50% and cost only a fraction of time for data synthesis. Furthermore, its modular design allows for the integration of multiple NeRFs, offering a versatile and efficient solution for visual localization. The implementation is publicly available at: https://github.com/ais-lab/DescriptorSynthesis4Feat2Map.},
  archive   = {C_IROS},
  author    = {Huy-Hoang Bui and Bach-Thuan Bui and Dinh-Tuan Tran and Joo-Ho Lee},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801953},
  month     = {10},
  pages     = {5581-5588},
  title     = {Leveraging neural radiance field in descriptor synthesis for keypoints scene coordinate regression},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Indoor position estimation using NLoS reflected path with
wireless distance sensors. <em>IROS</em>, 5573–5580. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Indoor robot localization is important for the realization of autonomous service robots. Various studies have been conducted on &quot;indoor GPS&quot; measurements using wireless distance sensors such as ultrasonic beacons. However, when these beacons encounter non-line-of-sight (NLoS) conditions due to obstacles, accurate distance measurements become challenging because of multipath and other effects. In this study, we propose a method for simultaneously estimating a robot’s position and distance to reflective surfaces in an environment using wireless distance sensors. The proposed method can estimate not only the robot’s position but also the reflection of the beacon signal. First, the wheel odometry of the robot is assumed to be the initial value, and the measured distance from the beacon to the robot is used as a factor to construct the factor graph. Second, the distance to the reflective surface of the beacon signal, which is parallel to the robot’s movement plane, was estimated from the robot position sequence using the GMM and used as a noise model in the factor graph. Finally, the method is evaluated by acquiring data in a real environment with obstacles. Compared with a method that does not consider reflection paths, this method demonstrated improved accuracy and effectiveness.},
  archive   = {C_IROS},
  author    = {Tomoya Itsuka and Ryo Kurazume},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802063},
  month     = {10},
  pages     = {5573-5580},
  title     = {Indoor position estimation using NLoS reflected path with wireless distance sensors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tightly coupled passive UWB localization for low-density
anchor networks. <em>IROS</em>, 5567–5572. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study investigates the effectiveness of a passive tightly coupled ultra-wideband (UWB) based inertial navigation system for indoor positioning of mobile platforms. Unlike conventional methods that rely on time difference of arrival (TDOA) or two-way ranging (TWR) measurements, the proposed approach utilizes local reception timestamps directly. An error state Kalman filter with right quaternion error definition is used in the state estimation process. Evaluation is performed first in a Matlab simulation environment and then, using a dataset acquired by flying a quadcopter while monitored by a motion capture system. Timestamp measurements were acquired using custom firmware flashed onto Decawave DWM 1000-DEV hardware. Our findings demonstrate that the proposed system outperforms traditional TDOA methods, providing accurate measurements even in the presence of communication interruptions, with as few as one anchor.},
  archive   = {C_IROS},
  author    = {Nushen M. Senevirathna and Oscar De Silva and George K. I. Mann and Raymond G. Gosine},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802419},
  month     = {10},
  pages     = {5567-5572},
  title     = {Tightly coupled passive UWB localization for low-density anchor networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). StereoNavNet: Learning to navigate using stereo cameras with
auxiliary occupancy voxels. <em>IROS</em>, 5559–5566. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual navigation has received significant attention recently. Most of the prior works focus on predicting navigation actions based on semantic features extracted from visual encoders. However, these approaches often rely on large datasets and exhibit limited generalizability. In contrast, our approach draws inspiration from traditional navigation planners that operate on geometric representations, such as occupancy maps. We propose StereoNavNet (SNN), a novel visual navigation approach employing a modular learning framework comprising perception and policy modules. Within the perception module, we estimate an auxiliary 3D voxel occupancy grid from stereo RGB images and extract geometric features from it. These features, along with user-defined goals, are utilized by the policy module to predict navigation actions. Through extensive empirical evaluation, we demonstrate that SNN outperforms baseline approaches in terms of success rates, success weighted by path length, and navigation error. Furthermore, SNN exhibits better generalizability, characterized by maintaining leading performance when navigating across previously unseen environments.},
  archive   = {C_IROS},
  author    = {Hongyu Li and Taşkın Padır and Huaizu Jiang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802385},
  month     = {10},
  pages     = {5559-5566},
  title     = {StereoNavNet: Learning to navigate using stereo cameras with auxiliary occupancy voxels},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-observability learning for vehicle routing problems.
<em>IROS</em>, 5551–5558. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study seeks towards a better understanding of multi-vehicle routing problems (VRPs) under restricted observability. Unlike most prior research that assumes full knowledge of tasks and vehicles, this paper addresses VRPs where each vehicle’s observation is confined to the k-nearest neighbourhood. Vehicles make decisions based on localized policies in a decentralized manner. We theoretically demonstrate that for the imitation policy, the upper bound of the optimality gap diminishes as the neighbourhood range expands. Subsequently, we employed a multi-agent cross-observability policy optimization (MACOPO) algorithm to solve the VRPs with restricted observability. The algorithm optimizes a cross-entropy term by leveraging a fully observable expert to guide the training. Empirical results supported both the theoretical findings and the effectiveness of the multi-agent learning algorithm.},
  archive   = {C_IROS},
  author    = {Ruifan Liu and Hyo-Sang Shin and Antonios Tsourdos},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801785},
  month     = {10},
  pages     = {5551-5558},
  title     = {Cross-observability learning for vehicle routing problems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HM3D-OVON: A dataset and benchmark for open-vocabulary
object goal navigation. <em>IROS</em>, 5543–5550. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present the Habitat-Matterport 3D Open Vocabulary Object Goal Navigation dataset (HM3D-OVON), a large-scale benchmark that broadens the scope and semantic range of prior Object Goal Navigation (ObjectNav) benchmarks. Leveraging the HM3DSem dataset, HM3D-OVON incorporates over 15k annotated instances of household objects across 379 distinct categories, derived from photo-realistic 3D scans of real-world environments. In contrast to earlier ObjectNav datasets, which limit goal objects to a predefined set of 6-21 categories, HM3D-OVON facilitates the training and evaluation of models with an open-set of goals defined through free-form language at test-time. Through this open-vocabulary formulation, HM3D-OVON encourages progress towards learning visuo-semantic navigation behaviors that are capable of searching for any object specified by text in an open-vocabulary manner. Additionally, we systematically evaluate and compare several different types of approaches on HM3D-OVON. We find that HM3D-OVON can be used to train an open-vocabulary ObjectNav agent that achieves both higher performance and is more robust to localization and actuation noise than the state-of-the-art ObjectNav approach. We hope that our benchmark and baseline results will drive interest in developing embodied agents that can navigate real-world spaces to find household objects specified through free-form language, taking a step towards more flexible and human-like semantic visual navigation. Code and videos available at: naoki.io/ovon.},
  archive   = {C_IROS},
  author    = {Naoki Yokoyama and Ram Ramrakhya and Abhishek Das and Dhruv Batra and Sehoon Ha},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802709},
  month     = {10},
  pages     = {5543-5550},
  title     = {HM3D-OVON: A dataset and benchmark for open-vocabulary object goal navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Camera pose estimation from bounding boxes. <em>IROS</em>,
5535–5542. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual localization is an important part of many interesting applications, including robotics. The dominant localization strategy is to estimate the camera pose from 2D-3D matches between 2D pixel positions and 3D points. Yet, such approaches can be quite memory intensive and can lead to privacy risks. An interesting alternative to point-based matches is to use higher-level primitives for pose estimation. Consequently, this work investigates using correspondences between 2D and 3D bounding boxes for camera pose estimation. The resulting scene representation is compact and poses fewer privacy risks. In this setting, there are typically orders of magnitude fewer matches available compared to classical feature-based methods. In addition, the available correspondences are significantly more noisy. We investigate multiple strategies based on converting bounding box correspondences to point correspondences and propose a novel and simple 2-point camera absolute pose solver (DP2P) that exploits the fact that the depths of the objects can be approximated from the sizes of their bounding boxes.},
  archive   = {C_IROS},
  author    = {Vaclav Vavra and Torsten Sattler and Zuzana Kukelova},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801546},
  month     = {10},
  pages     = {5535-5542},
  title     = {Camera pose estimation from bounding boxes},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Embodiment randomization for cross embodiment navigation.
<em>IROS</em>, 5527–5534. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present Embodiment Randomization, a simple, inexpensive, and intuitive technique for training robust behavior policies that can be transferred to multiple robot embodiments. While prior works require real-world data from multiple robots, or complex algorithmic adjustments to address the challenge of embodiment generalization, our approach leverages the power of simulation and large-scale reinforcement learning and can be easily integrated within existing policy learning methods. We show that policies trained with embodiment randomization implicitly perform system identification, enabling them to adapt to new embodiments during deployment. Our approach not only shows significant improvements in adapting to novel robot configurations, but also in generalizing from simulation to reality and contending with real-world perturbations, highlighting the potential of embodiment randomization in creating versatile and adaptable robotic navigation policies.},
  archive   = {C_IROS},
  author    = {Pranav Putta and Gunjan Aggarwal and Roozbeh Mottaghi and Dhruv Batra and Naoki Yokoyama and Joanne Truong and Arjun Majumdar},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802792},
  month     = {10},
  pages     = {5527-5534},
  title     = {Embodiment randomization for cross embodiment navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Context-aware GAN-based image retrieval for coarse
localization of autonomous robots. <em>IROS</em>, 5521–5526. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Effective localization is crucial for the reliable operation of autonomous delivery robots. This paper introduces ConLocGAN, a novel context-aware GAN, addressing challenges in Lidar-based localization. Our approach employs a two-step process, integrating image retrieval with Lidar-based localization. ConLocGAN extracts robust global descriptors for coarse pose estimator, which acts as a precursor for Lidar-based pose refinement. The discriminator in ConLocGAN identifies differences in images of the same scene under diverse conditions at the feature level. This information is then utilized to enhance localization-specific feature extraction by the generator in a self-supervised setting. Additionally, we present a simple data collection pipeline that is seamlessly integrated into routine robot operations. Using heatmaps for visualization, we demonstrate that our network learns robust descriptors by prioritizing static components of the scene while effectively disregarding environmental changes such as illumination and weather, as well as dynamic objects like people and vehicles. We further validate our method on the challenging CMU seasons dataset, where it outperforms state-of-the-art retrieval-based methods in coarse pose estimation.},
  archive   = {C_IROS},
  author    = {Ruphan Swaminathan and Pradyot Korupolu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802206},
  month     = {10},
  pages     = {5521-5526},
  title     = {Context-aware GAN-based image retrieval for coarse localization of autonomous robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TRAVERSE: Traffic-responsive autonomous vehicle experience
&amp; rare-event simulation for enhanced safety. <em>IROS</em>,
5513–5520. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Data for training learning-enabled self-driving cars in the physical world are typically collected in a safe, normal environment. Such data distribution often engenders a strong bias towards safe driving, making self-driving cars unprepared when encountering adversarial scenarios like unexpected accidents. Due to a dearth of such adverse data that is unrealistic for drivers to collect, autonomous vehicles can perform poorly when experiencing such rare events. This work addresses much-needed research by having participants drive a VR vehicle simulator going through simulated traffic with various types of accidental scenarios. It aims to understand human responses and behaviors in simulated accidents, contributing to our understanding of driving dynamics and safety. The simulation framework adopts a robust traffic simulation and is rendered using the Unity Game Engine. Furthermore, the simulation framework is built with portable, light-weight immersive driving simulator hardware, lowering the resource barrier for studies in autonomous driving research.},
  archive   = {C_IROS},
  author    = {Sandeep Thalapanane and Sandip Sharan Senthil Kumar and Guru Nandhan Appiya Dilipkumar Peethambari and Sourang SriHari and Laura Zheng and Julio Poveda and Ming C. Lin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802298},
  month     = {10},
  pages     = {5513-5520},
  title     = {TRAVERSE: Traffic-responsive autonomous vehicle experience &amp; rare-event simulation for enhanced safety},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Preventing catastrophic forgetting in continuous online
learning for autonomous driving. <em>IROS</em>, 5505–5512. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous vehicles require online learning capabilities to enable long-term, unattended operation. However, long-term online learning is accompanied by the problem of forgetting previously learned knowledge. This paper introduces an online learning framework that includes a catastrophic forgetting prevention mechanism, named Long-Short-Term Online Learning (LSTOL). The framework consists of a set of shortterm learners and a long-term controller, where the former is based on the concept of ensemble learning and aims to achieve rapid learning iterations, while the latter contains a simple yet efficient probabilistic decision-making mechanism combined with four control primitives to achieve effective knowledge maintenance. A novel feature of the proposed LSTOL is that it avoids forgetting while learning autonomously. In addition, LSTOL makes no assumptions about the model type of short-term learners and the continuity of the data. The effectiveness of the proposed framework is demonstrated through experiments across well-known datasets in autonomous driving, including KITTI and Waymo. The source code for the method implementation is publicly available at https://github.com/epan-utbm/lstol.},
  archive   = {C_IROS},
  author    = {Rui Yang and Tao Yang and Zhi Yan and Tomas Krajnik and Yassine Ruichek},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801619},
  month     = {10},
  pages     = {5505-5512},
  title     = {Preventing catastrophic forgetting in continuous online learning for autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MPP: Multiscale path planning for UGV navigation in
semi-structured environments. <em>IROS</em>, 5497–5504. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous navigation of unmanned ground vehicles (UGVs) in structured road and indoor environments has made significant progress in recent years. However, navigation in outdoor semi-structured environments remains a challenge. This paper presents the multiscale path planning (MPP) method for UGV navigation in semi-structured environments. MPP leverages global, mid-layer and local planners to obtain global path and handle local obstacles of different sizes. First, the global planner provides guidance based on road connection relationships, selecting optimal connections by evaluating the distance between road nodes. Next, the mid-layer planner perceives large-scale obstacles and constructs the costmap, generating a mid-layer path that offers a general direction for the UGV. Finally, a local trajectory planning algorithm, namely terrain-considering timed elastic band (TC-TEB), is used to obtain local trajectory. This algorithm incorporates terrain-velocity constraints into the TEB algorithm to ensure the vehicle’s vertical stability. We demonstrate the safety and effectiveness of MPP through experiments in both simulated and real-world environments.},
  archive   = {C_IROS},
  author    = {Rui Cao and Zhiqiang Yang and Ran Song and Ziyu Meng and Ruifeng Wang and Wei Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801877},
  month     = {10},
  pages     = {5497-5504},
  title     = {MPP: Multiscale path planning for UGV navigation in semi-structured environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A two-stage reinforcement learning approach for robot
navigation in long-range indoor dense crowd environments. <em>IROS</em>,
5489–5496. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safe and efficient mobility is vital for mobile robots navigating long-range indoor crowd environments, such as supermarkets, restaurants, and railway stations. Traditional path planning methods are challenged because of the high dynamics of pedestrians and constrained feasible regions. Existing long-range deep reinforcement learning (DRL) path planning methods often exhibit low success rates and driving speeds in long-range navigation tasks under crowded conditions. To overcome these issues, we propose a new two-stage DRL method, known as TSDRL, where the long-range navigation task is divided into subgoal generation (SG) and planning refinement (PR) stages. In the SG stage, the agent is trained to learn a decision-making policy to generate subgoals at each decision time to avoid dense crowds. In the PR stage, the agent learns a safer and more efficient planning policy based on each subgoal generated in the SG stage to improve the robot’s movement safety and speed. Simulated experiments show that our method outperforms traditional and long-range DRL path planning methods in terms of safety, efficiency, generalization, and robustness. Furthermore, we evaluate our approach using the Turtlebot2 platform in a real-world setting, demonstrating that the robot can navigate safely and efficiently while avoiding dense crowds.},
  archive   = {C_IROS},
  author    = {Xing Hui Jing and Xin Xiong and Fu Hao Li and Tao Zhang and Long Zeng},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801711},
  month     = {10},
  pages     = {5489-5496},
  title     = {A two-stage reinforcement learning approach for robot navigation in long-range indoor dense crowd environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual forecasting as a mid-level representation for
avoidance. <em>IROS</em>, 5481–5488. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The challenge of navigation in environments with dynamic objects continues to be a central issue in the study of autonomous agents. While predictive methods hold promise, their reliance on precise state information makes them less practical for real-world implementation. This study presents visual forecasting as an innovative alternative. By introducing intuitive visual cues, this approach projects the future trajectories of dynamic objects to improve agent perception and enable anticipatory actions. Our research explores two distinct strategies for conveying predictive information through visual forecasting: (1) sequences of bounding boxes, and (2) augmented paths. To validate the proposed visual forecasting strategies, we initiate evaluations in simulated environments using the Unity engine and then extend these evaluations to real-world scenarios to assess both practicality and effectiveness. The results confirm the viability of visual forecasting as a promising solution for navigation and obstacle avoidance in dynamic environments.},
  archive   = {C_IROS},
  author    = {Hsuan-Kung Yang and Tsung-Chih Chiang and Ting-Ru Liu and Chun-Wei Huang and Jou-Min Liu and Chun-Yi Lee},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801799},
  month     = {10},
  pages     = {5481-5488},
  title     = {Visual forecasting as a mid-level representation for avoidance},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic reconfiguration integrated nested a*: A path planner
for reconfigurable robot to improve performance in confined spaces.
<em>IROS</em>, 5473–5480. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Path planning is crucial in numerous robotic applications. Reconfigurable robots possess the capability to alter their shape, enabling access to confined spaces, a task challenging for fixed-shape robots. However, existing path planners for reconfigurable robots are typically designed with predefined motion patterns for reconfiguration, lacking adaptation to space availability and executing reconfiguration only when the robot is static. This reliance on predefined patterns limits the potential of reconfigurable robots to navigate through confined spaces. This paper proposes a novel path-planning approach based on dynamic reconfiguration to address this limitation. The proposed method employs two nested A* algorithms modified to handle reconfiguration and efficient search, termed Dynamic Reconfiguration integrated Nested A* (DRiNA*). Experimental results demonstrate the proposed method’s ability to find feasible paths for robot navigation using dynamic reconfigurations in confined spaces, surpassing the capabilities of existing path planners. The scalability of the proposed method to reconfigurable robots with varying numbers of blocks is also confirmed. Additionally, DRiNA* significantly reduces energy consumption compared to existing path planners.},
  archive   = {C_IROS},
  author    = {W. K. R. Sachinthana and S. M. Bhagya P. Samarakoon and M. A. Viraj J. Muthugala and Mohan Rajesh Elara},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802331},
  month     = {10},
  pages     = {5473-5480},
  title     = {Dynamic reconfiguration integrated nested a*: A path planner for reconfigurable robot to improve performance in confined spaces},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LDP: A local diffusion planner for efficient robot
navigation and collision avoidance. <em>IROS</em>, 5466–5472. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The conditional diffusion model has been demonstrated as an efficient tool for learning robot policies, owing to its advancement to accurately model the conditional distribution of policies. The intricate nature of real-world scenarios, characterized by dynamic obstacles and maze-like structures, underscores the complexity of robot local navigation decision-making as a conditional distribution problem. Nevertheless, leveraging the diffusion model for robot local navigation is not trivial and encounters several under-explored challenges: (1) Data Urgency The complex conditional distribution in local navigation needs training data to include diverse policy in diverse real-world scenarios; (2) Myopic Observation Due to the diversity of the perception scenarios, diffusion decisions based on the local perspective of robots may prove suboptimal for completing the entire task, as they often lack foresight. In certain scenarios requiring detours, the robot may become trapped. To address these issues, our approach begins with an exploration of a diverse data generation mechanism that encompasses multiple agents exhibiting distinct preferences through target selection informed by integrated global-local insights. Then, based on this diverse training data, a diffusion agent is obtained, capable of excellent collision avoidance in diverse scenarios. Subsequently, we augment our Local Diffusion Planner, also known as LDP by incorporating global observations in a lightweight manner. This enhancement broadens the observational scope of LDP, effectively mitigating the risk of becoming ensnared in local optima and promoting more robust navigational decisions. Our experimental results demonstrated that the LDP outperforms other baseline algorithms in navigation performance, exhibiting enhanced robustness across diverse scenarios with different policy preferences and superior generalization capabilities for unseen scenarios. Moreover, we highlighted the competitive advantage of the LDP within real-world settings.},
  archive   = {C_IROS},
  author    = {Wenhao Yu and Jie Peng and Huanyu Yang and Junrui Zhang and Yifan Duan and Jianmin Ji and Yanyong Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802009},
  month     = {10},
  pages     = {5466-5472},
  title     = {LDP: A local diffusion planner for efficient robot navigation and collision avoidance},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generating force vectors from projective truncated signed
distance fields for collision avoidance and haptic feedback.
<em>IROS</em>, 5460–5465. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Signed Distance Fields are a common surface representation method widely used for both 3D mapping and obstacle avoidance. While the former traditionally uses projective Truncated Signed Distance Fields (TSDF), the latter often requires a complete Euclidean Signed Distance Field (ESDF) representation of the environment. In this paper, we propose a unified system by combining both methods to generate force vectors to nearby obstacles from a TSDF-based 3D reconstruction. We introduce a new merging scheme to better capture the geometry of the object, with no post-processing requirements, and a way to increase the effective range of the system. Validation experiments demonstrate the accuracy of the force vector calculation by comparing it against an ideal simulated environment. The flexibility of the system is demonstrated by implementing a haptic feedback teleoperation setup, which is validated through a user study in a teleoperation task. Through this, it is shown that the proposed method provides a statistically significant improvement to the task. Finally, a brief description on future improvements to the system is presented.},
  archive   = {C_IROS},
  author    = {Seongjin Bien and Abdeldjallil Naceri and Luis Figueredo and Sami Haddadin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801739},
  month     = {10},
  pages     = {5460-5465},
  title     = {Generating force vectors from projective truncated signed distance fields for collision avoidance and haptic feedback},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Collision-free robot navigation in crowded environments
using learning based convex model predictive control. <em>IROS</em>,
5452–5459. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Navigating robots safely and efficiently in crowded and complex environments remains a significant challenge. However, due to the dynamic and intricate nature of these settings, planning efficient and collision-free paths for robots to track is particularly difficult. In this paper, we uniquely bridge the robot’s perception, decision-making and control processes by utilizing the convex obstacle-free region computed from 2D LiDAR data. The overall pipeline is threefold: (1) We proposes a robot navigation framework that utilizes deep reinforcement learning (DRL), conceptualizing the observation as the convex obstacle-free region, a departure from general reliance on raw sensor inputs. (2) We design the action space, derived from the intersection of the robot’s kinematic limits and the convex region, to enable efficient sampling of inherently collision-free reference points. These actions assists in guiding the robot to move towards the goal and interact with other obstacles during navigation. (3) We employ model predictive control (MPC) to track the trajectory formed by the reference points while satisfying constraints imposed by the convex obstacle-free region and the robot’s kinodynamic limits. The effectiveness of proposed improvements has been validated through two sets of ablation studies and a comparative experiment against the Timed Elastic Band (TEB), demonstrating improved navigation performance in crowded and complex environments.},
  archive   = {C_IROS},
  author    = {Zhuanglei Wen and Mingze Dong and Xiai Chen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801893},
  month     = {10},
  pages     = {5452-5459},
  title     = {Collision-free robot navigation in crowded environments using learning based convex model predictive control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient incremental penetration depth estimation between
convex geometries. <em>IROS</em>, 5444–5451. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Penetration depth (PD) is essential for robotics due to its extensive applications in dynamic simulation, motion planning, haptic rendering, etc. The Expanding Polytope Algorithm (EPA) is the de facto standard for this problem, which estimates PD by expanding an inner polyhedral approximation of an implicit set. In this paper, we propose a novel optimization-based algorithm that incrementally estimates minimum penetration depth and its direction. One major advantage of our method is the capability to be warm-started by leveraging the spatial and temporal coherence. This coherence emerges naturally in many robotic applications (e.g., the temporal coherence between adjacent simulation time knots). As a result, our algorithm achieves substantial speedup — we demonstrate it is 5-30x faster than EPA on several benchmarks. Moreover, our approach is built upon the same implicit geometry representation as EPA, which enables easy integration into existing software stacks. The code and supplemental document are available on: https://github.com/weigao95/mind-fcl.},
  archive   = {C_IROS},
  author    = {Wei Gao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801965},
  month     = {10},
  pages     = {5444-5451},
  title     = {Efficient incremental penetration depth estimation between convex geometries},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Terrain-attentive learning for efficient 6-DoF kinodynamic
modeling on vertically challenging terrain. <em>IROS</em>, 5438–5443.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Wheeled robots have recently demonstrated superior mechanical capability to traverse vertically challenging terrain (e.g., extremely rugged boulders comparable in size to the vehicles themselves). Negotiating such terrain introduces significant variations of vehicle pose in all six Degrees-of-Freedom (DoFs), leading to imbalanced contact forces, varying momentum, and chassis deformation due to non-rigid tires and suspensions. To autonomously navigate on vertically challenging terrain, all these factors need to be efficiently reasoned within limited onboard computation and strict real-time constraints. In this paper, we propose a 6-DoF kinodynamics learning approach that is attentive only to the specific underlying terrain critical to the current vehicle-terrain interaction, so that it can be efficiently queried in real-time motion planners onboard small robots. Physical experiment results show our Terrain-Attentive Learning (TAL) demonstrates on average 51.1% reduction in model prediction error among all 6 DoFs compared to a stateof-the-art model for vertically challenging terrain. 1},
  archive   = {C_IROS},
  author    = {Aniket Datar and Chenhui Pan and Mohammad Nazeri and Anuj Pokhrel and Xuesu Xiao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801650},
  month     = {10},
  pages     = {5438-5443},
  title     = {Terrain-attentive learning for efficient 6-DoF kinodynamic modeling on vertically challenging terrain},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3D global path planning for walking robots on sparse
volumetric maps. <em>IROS</em>, 5430–5437. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The use of mobile robots has become increasingly common in multiple areas of daily life. To increase their autonomy for performing various tasks, efficient navigation skills are essential. The most crucial component of such navigation is the ability to calculate a global path between two points. The global path planning problem for mobile robots is typically limited to two-dimensional environments, in which the environment is projected onto a planar surface. While this approach works well in structured environments like industrial settings, it may not be suitable for all applications of mobile robots. With modern walking robots, capable of navigating complex terrain, more advanced path planning approaches are necessary. This work proposes a path-planning approach that utilizes the entire three-dimensional space, allowing for navigation in even the most challenging terrain. The central idea is to extend a traditional A* path planner to work directly on a fast volumetric map structure to generate optimal paths through the environment. Multiple optimizations and adjustments are introduced to improve the algorithm’s performance. By applying morphology operators to sparse maps, sensor inaccuracies during the map construction are mitigated. Additionally, adjustments are made to handle the added complexity introduced by the extra search space dimension and to comply with the limitations of autonomous walking robots. This is paired with an efficient caching strategy to enhance the overall path-planning speed. The capability of the path planning approach is evaluated using both artificial and real-world maps. The results demonstrate that this approach shows great potential for enabling mobile ground robots to autonomously navigate even the most demanding terrains utilizing the entire three-dimensional space.},
  archive   = {C_IROS},
  author    = {Marvin Grosse Besselmann and Ramona Häuselmann and Samuel Mauch and Lennart Puck and Tristan Schnell and Arne Rönnau and Rüdiger Dillmann},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802767},
  month     = {10},
  pages     = {5430-5437},
  title     = {3D global path planning for walking robots on sparse volumetric maps},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Energy-efficient trajectory planning with media transition
for a hybrid unmanned aerial-underwater vehicle. <em>IROS</em>,
5424–5429. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vehicles capable of operating in more than one environment have been developed to solve real problems. Among them, the hybrid unmanned aerial-underwater vehicle (HUAUV) is receiving attention from the robotics community, mainly with a quadrotor-like configuration. However, this vehicle presents high energy consumption because of the larger mass required compared to the only aerial vehicle, limiting its autonomy. This work addresses the trajectory planning problem for a HUAUV. The method is based on Rapidly-exploring Random Trees (RRTs), a highly customizable planning technique. In addition, we propose two new heuristics to increase the energy efficiency of the hybrid vehicle. The first consists of biasing the tree expansion towards the environment with the lowest navigation cost, while the second one assigns estimated costs to nodes in the tree and chooses the least expensive trajectories. These techniques are evaluated in physically realistic simulation experiments performed in 135 scenarios. A comparative analysis of their performances is presented relative to the state of the art. We show that using efficient heuristics can significantly contribute to reducing energy consumption and even increase the average velocity in the missions performed by these vehicles.},
  archive   = {C_IROS},
  author    = {Pedro M. Pinheiro and Armando Alves Neto and Douglas G. Macharet and Paulo L. J. Drews},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802863},
  month     = {10},
  pages     = {5424-5429},
  title     = {Energy-efficient trajectory planning with media transition for a hybrid unmanned aerial-underwater vehicle},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Safety-first tracker: A trajectory planning framework for
omnidirectional robot tracking. <em>IROS</em>, 5416–5423. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a Safety-First Tracker (SF-Tracker) designed for omnidirectional autonomous tracking robots. The position and orientation of omnidirectional robots are decoupled for stepwise planning to ensure trajectory safety and maintain target visibility. SF-Tracker puts the trajectory safety in the first place. First, a collision-free and occlusion-free reference path is efficiently initialized by constructing a directed weighted graph. By building upon this path, safe trajectory optimization is implemented to ensure safe movement. Finally, an orientation planner is developed to achieve target visibility based on the safe trajectory. Extensive experimental evaluations in simulated environments and the real world demonstrate that the SF-Tracker outperforms state-of-the-art methods in terms trajectory safety and target visibility. Ablation experiments further demonstrate the significance of each step of the SF-Tracker. The source code and demonstration video can be found at https://github.com/Yue-0/SF-Tracker.},
  archive   = {C_IROS},
  author    = {Yue Lin and Yang Liu and Pingping Zhang and Xin Chen and Dong Wang and Huchuan Lu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802592},
  month     = {10},
  pages     = {5416-5423},
  title     = {Safety-first tracker: A trajectory planning framework for omnidirectional robot tracking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Speeding up path planning via reinforcement learning in MCTS
for automated parking. <em>IROS</em>, 5410–5415. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we address a method that integrates reinforcement learning into the Monte Carlo tree search to boost online path planning under fully observable environments for automated parking tasks. Sampling-based planning methods under high-dimensional space can be computationally expensive and time-consuming. State evaluation methods are useful by leveraging the prior knowledge into the search steps, making the process faster in a real-time system. Given the fact that automated parking tasks are often executed under complex environments, a solid but lightweight heuristic guidance is challenging to compose in a traditional analytical way. To overcome this limitation, we propose a reinforcement learning pipeline with a Monte Carlo tree search under the path planning framework. By iteratively learning the value of a state and the best action among samples from its previous cycle’s outcomes, we are able to model a value estimator and a policy generator for given states. By doing that, we build up a balancing mechanism between exploration and exploitation, speeding up the path planning process while maintaining its quality without using human expert driver data.},
  archive   = {C_IROS},
  author    = {Xinlong Zheng and Xiaozhou Zhang and Donghao Xu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802417},
  month     = {10},
  pages     = {5410-5415},
  title     = {Speeding up path planning via reinforcement learning in MCTS for automated parking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interactive-FAR: Interactive, fast and adaptable routing for
navigation among movable obstacles in complex unknown environments.
<em>IROS</em>, 5402–5409. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a real-time algorithm for navigating complex unknown environments cluttered with movable obstacles. Our algorithm achieves fast, adaptable routing by actively attempting to manipulate obstacles during path planning and adjusting the global plan from sensor feedback. The main contributions include an improved dynamic Directed Visibility Graph (DV-graph) for rapid global path searching, a real-time interaction planning method that adapts online from new sensory perceptions, and a comprehensive framework designed for interactive navigation in complex unknown or partially known environments. Our algorithm is capable of replanning the global path in several milliseconds. It can also attempt to move obstacles, update their affordances, and adapt strategies accordingly. Extensive experiments validate that our algorithm reduces the travel time by 33%, achieves up to 49% higher path efficiency, and runs faster than traditional methods by orders of magnitude in complex environments. It has been demonstrated to be the most efficient solution in terms of speed and efficiency for interactive navigation in environments of such complexity. We also open-source our code in the docker demo1 to facilitate future research.},
  archive   = {C_IROS},
  author    = {Botao He and Guofei Chen and Wenshan Wang and Ji Zhang and Cornelia Fermuller and Yiannis Aloimonos},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802169},
  month     = {10},
  pages     = {5402-5409},
  title     = {Interactive-FAR: Interactive, fast and adaptable routing for navigation among movable obstacles in complex unknown environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generating continuous paths on learned constraint manifolds
using policy search. <em>IROS</em>, 5396–5401. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many robotic manipulation tasks are constrained due to kinematic limitations placed on the object being manipulated. This increases the complexity of manipulation tasks that operate in high dimensions, leading to increased risk that sampling based planners are unable to find optimal solutions. Whilst trajectory optimisation methods provide guaranteed optimal solutions when implementing constraints, they only provide locally optimal solutions in sequential decision-making and struggle to provide globally optimal paths. These constraints can be incorporated into the probabilistic latent spaces by using demonstrations that satisfy the constraint function. However whenever constraints change or a manipulator must perform different tasks the network must be retrained to accommodate the new constraints. In this paper, we provide an approach that allows the training of a single learned manifold that can be augmented to determine the constraint manifold for the manipulation task. Using this manifold, the geodesic between two points can be computed using policy search to solve the cost function associated with the geodesic curve length ${\mathcal{L}_\gamma }$. We provide comparisons in terms of path length against popular path planning algorithms with different kinematic constraints, demonstrating our method’s ability to find optimal shortest paths on constraint manifolds.},
  archive   = {C_IROS},
  author    = {Ethan Canzini and Simon Pope and Ashutosh Tiwari},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802531},
  month     = {10},
  pages     = {5396-5401},
  title     = {Generating continuous paths on learned constraint manifolds using policy search},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Can vehicle motion planning generalize to realistic
long-tail scenarios? <em>IROS</em>, 5388–5395. (<a
href="https://doi.org/10.1109/IROS58592.2024.10803052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-world autonomous driving systems must make safe decisions in the face of rare and diverse traffic scenarios. Current state-of-the-art planners are mostly evaluated on real-world datasets like nuScenes (open-loop) or nuPlan (closed-loop). In particular nuPlan seems to be an expressive evaluation method since it is based on real-world data and closed-loop, yet it mostly covers basic driving scenarios. This makes it difficult to judge a planner’s capabilities to generalize to rarely-seen situations. Therefore, we propose a novel closed-loop benchmark interPlan containing several edge cases and challenging driving scenarios. We assess existing state-of-the-art planners on our benchmark and show that neither rule-based nor learning-based planners can safely navigate the interPlan scenarios.A recently evolving direction is the usage of foundation models like large language models (LLM) to handle generalization. We evaluate an LLM-only planner and introduce a novel hybrid planner that combines an LLM-based behavior planner with a rule-based motion planner that achieves state-of-the-art performance on our benchmark.},
  archive   = {C_IROS},
  author    = {Marcel Hallgarten and Julian Zapata and Martin Stoll and Katrin Renz and Andreas Zell},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10803052},
  month     = {10},
  pages     = {5388-5395},
  title     = {Can vehicle motion planning generalize to realistic long-tail scenarios?},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RT-RRT: Reverse tree guided real-time path
planning/replanning in unpredictable dynamic environments.
<em>IROS</em>, 5380–5387. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Path planning in unpredictable dynamic environments remains a challenging problem due to the unpredictable appearance, disappearance, and movement of dynamic obstacles during navigation. To address this problem, we propose a reverse tree guided rapid exploration random tree (RTRRT) algorithm that can efficiently perform navigation tasks in dynamic environments. The method first constructs a reverse tree rooted as goal state to search for an initial path. If a collision occurs on the path, The RT-RRT constructs a forward tree rooted as the current robot state in the same configuration space, until it connects with the reverse tree to find a new path. Furthermore, The RT-RRT improves the tree construction method and designs a path optimization strategy to reduce the path cost. The method is validated in different scenarios and has excellent navigation capabilities in unpredictable dynamic environments. In the same scenarios, the RT-RRT algorithm improves the success rate by 16.7%, reduces the path length by 20.54% and reduces the travel time by 10X compared to the RRTX algorithm with the same number of samples.},
  archive   = {C_IROS},
  author    = {Bo Cui and Rongxin Cui and Weisheng Yan and Yongkang Wang and Shi Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802722},
  month     = {10},
  pages     = {5380-5387},
  title     = {RT-RRT: Reverse tree guided real-time path Planning/Replanning in unpredictable dynamic environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LF-3PM: A LiDAR-based framework for perception-aware
planning with perturbation-induced metric. <em>IROS</em>, 5372–5379. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Just as humans can become disoriented in featureless deserts or thick fogs, not all environments are conducive to the Localization Accuracy and Stability (LAS) of autonomous robots. This paper introduces an efficient framework designed to enhance LiDAR-based LAS through strategic trajectory generation, known as Perception-aware Planning. Unlike vision-based frameworks, the LiDAR-based requires different considerations due to unique sensor attributes. Our approach focuses on two main aspects: firstly, assessing the impact of LiDAR observations on LAS. We introduce a perturbation-induced metric to provide a comprehensive and reliable evaluation of LiDAR observations. Secondly, we aim to improve motion planning efficiency. By creating a Static Observation Loss Map (SOLM) as an intermediary, we logically separate the time-intensive evaluation and motion planning phases, significantly boosting the planning process. In the experimental section, we demonstrate the effectiveness of the proposed metrics across various scenes and the feature of trajectories guided by different metrics. Ultimately, our framework is tested in a real-world scenario, enabling the robot to actively choose topologies and orientations preferable for localization. The source code is accessible at https://github.com/ZJU-FAST-Lab/LF-3PM.},
  archive   = {C_IROS},
  author    = {Kaixin Chai and Long Xu and Qianhao Wang and Chao Xu and Peng Yin and Fei Gao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802556},
  month     = {10},
  pages     = {5372-5379},
  title     = {LF-3PM: A LiDAR-based framework for perception-aware planning with perturbation-induced metric},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning social cost functions for human-aware path
planning. <em>IROS</em>, 5364–5371. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Achieving social acceptance is one of the main goals of Social Robotic Navigation. Despite this topic has received increasing interest in recent years, most of the research has focused on driving the robotic agent along obstacle-free trajectories, planning around estimates of future human motion to respect personal distances and optimize navigation. However, social interactions in everyday life are also dictated by norms that do not strictly depend on movement, such as when standing at the end of a queue rather than cutting it. In this paper, we propose a novel method to recognize common social scenarios and modify a traditional planner’s cost function to adapt to them. This solution enables the robot to carry out different social navigation behaviors that would not arise otherwise, maintaining the robustness of traditional navigation. Our approach allows the robot to learn different social norms with a single learned model, rather than having different modules for each task. As a proof of concept, we consider the tasks of queuing and respect interaction spaces of groups of people talking to one another, but the method can be extended to other human activities that do not involve motion.},
  archive   = {C_IROS},
  author    = {Andrea Eirale and Matteo Leonetti and Marcello Chiaberge},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802768},
  month     = {10},
  pages     = {5364-5371},
  title     = {Learning social cost functions for human-aware path planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Multi-fov-constrained trajectory planning for multirotor
safe landing. <em>IROS</em>, 5356–5363. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, multirotors have become more and more widely used, such as in aerial photography and delivery. Ensuring a safe landing in emergencies is the most basic requirement, and it is important to make full use of all the sensors of the multirotor. To improve the safety of UAV landing in unknown unstructured scenes, this paper proposes a multi-FOV-constrained trajectory planning algorithm. Due to the discontinuity of multi-FOV constraints and the nonlinearity of UAV dynamics, the entire trajectory planning problem is a nonlinear optimization problem with non-convex constraints. To address this problem, our algorithm contains two stages, a multi-fov-constrained path search algorithm and a safe landing trajectory optimization algorithm. The multi-fov-constrained path search algorithm is used to generate a safe initial path that satisfies the FOV constraint. Then, the safe landing trajectory optimization algorithm generates a safe trajectory, which considers FOV constraints, dynamics, smoothness, and obstacle avoidance. We conducted simulation experiments and real-world experiments to verify the robustness and effectiveness of our algorithm.},
  archive   = {C_IROS},
  author    = {Dong Wang and Jingping Wang and Suqin He and Jinxin Huang and Bangyan Zhang and Yinian Mao and Guoquan Huang and Chao Xu and Fei Gao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802806},
  month     = {10},
  pages     = {5356-5363},
  title     = {Multi-fov-constrained trajectory planning for multirotor safe landing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flexible and topological consistent local replanning for
multirotors. <em>IROS</em>, 5348–5355. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many situations such as city delivery and wild inspection, quadrotors are often required to follow a predefined reference trajectory. However, these reference trajectories cannot be perfectly safe, resulting in conflicts between tracking the reference precisely, flying safely, and finishing the mission timely. This paper proposes to solve the above problem, by introducing a replanning framework that first generates a topological consistent collision-free initial path and then flexibly optimizes the rejoin point and trajectory duration to generate a smooth and safe local rejoining trajectory. To avoid local trajectory switching in different directions during high-frequency replanning, we propose a topology-preserving path search algorithm based on kinodynamic RRT*. To satisfy dynamic constraints, avoid delays, and achieve a smooth rejoin of the reference trajectory, we propose an optimization-based approach to refine the initial trajectory. The simulation results confirm that our proposed topological consistency and flexible optimization methods can reduce the risk of local trajectory and decrease obstacle avoidance delay for tracking reference trajectory. We also conduct real-world experiments in challenging environments and verify the effectiveness of our method.},
  archive   = {C_IROS},
  author    = {Dong Wang and Hongkai Ye and Neng Pan and Jinxin Huang and Bangyan Zhang and Yinian Mao and Guoquan Huang and Chao Xu and Fei Gao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801554},
  month     = {10},
  pages     = {5348-5355},
  title     = {Flexible and topological consistent local replanning for multirotors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DTG: Diffusion-based trajectory generation for mapless
global navigation. <em>IROS</em>, 5340–5347. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel end-to-end diffusion-based trajectory generation method, DTG, for mapless global navigation in challenging outdoor scenarios with occlusions and unstructured off-road features like grass, buildings, bushes, etc. Given a distant goal, our approach computes a trajectory that satisfies the following goals: (1) minimize the travel distance to the goal; (2) maximize the traversability by choosing paths that do not lie in undesirable areas. Specifically, we present a novel Conditional RNN(CRNN) for diffusion models to efficiently generate trajectories. Furthermore, we propose an adaptive training method that ensures that the diffusion model generates more traversable trajectories. We evaluate our methods in various outdoor scenes and compare the performance with other global navigation algorithms on a Husky robot. In practice, we observe at least a 15% improvement in traveling distance and around a 7% improvement in traversability. Video and Code: https://github.com/jingGM/DTG.git.},
  archive   = {C_IROS},
  author    = {Jing Liang and Amirreza Payandeh and Daeun Song and Xuesu Xiao and Dinesh Manocha},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802055},
  month     = {10},
  pages     = {5340-5347},
  title     = {DTG: Diffusion-based trajectory generation for mapless global navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An actor-critic reinforcement learning scheme for reactive
3D optimal motion planning based on fluid dynamics. <em>IROS</em>,
5332–5339. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work proposes a novel and provably correct method for three-dimensional optimal motion planning in complex environments. Our approach models the 3D motion planning problem by solving streamlines of the potential fluid flow, filling a gap in traditional motion planning techniques by guaranteeing a closed-loop, smooth and natural-looking navigation solution. Special emphasis is given to an inherent challenge of artificial potential field (APF) methods, namely establishing proofs of safety and stability over the entire optimization process. A model-based actor-critic reinforcement learning algorithm is introduced to approximate the optimal solution to the Hamilton-Jacobi-Bellman equation and update the controller parameters in a deterministic manner. Through a series of ROS-Gazebo software-in-the-loop simulations the proposed methodology demonstrates robustness and outperforms widely used methods such as the RRT∗, highlighting its contribution to the field of 3D optimal motion planning.},
  archive   = {C_IROS},
  author    = {Marios Malliaropoulos and Panagiotis Rousseas and Charalampos P. Bechlioulis and Kostas J. Kyriakopoulos},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802111},
  month     = {10},
  pages     = {5332-5339},
  title     = {An actor-critic reinforcement learning scheme for reactive 3D optimal motion planning based on fluid dynamics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Alternative connection radius for asymptotic optimality in
RRT*. <em>IROS</em>, 5327–5331. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Connection radius in asymptotically optimal motion planning algorithms is of interest to both understand the theoretical properties of these algorithms, as well as to ensure practical performance by estimating lower bounds. The smaller the connection radius, the sparser the data structures constructed using them, which makes the associated algorithms computationally more efficient. The original radii for both roadmap and tree variants were reported to be asymptotically shrinking functions of n. A recent amendment to the original arguments for trees demonstrated that the radius has to be larger for tree-based variants (RRT*). A practical problem in the newly proposed radius is the persistence of hard-to-estimate or large-valued parameters (like optimal path cost) within the connection radius function. In this short paper, a new perspective is presented of approaching the proof of asymptotic optimality of RRT* from a minimal variant of RRT* that only includes tree additions within connection neighborhoods. The work provides an alternative connection radius that gets rid of unwieldy parameters, presents insights that holds promise in studying the problem and using the result.},
  archive   = {C_IROS},
  author    = {Rahul Shome},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801710},
  month     = {10},
  pages     = {5327-5331},
  title     = {Alternative connection radius for asymptotic optimality in RRT*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical large scale multirobot path (re)planning.
<em>IROS</em>, 5319–5326. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider a large-scale multi-robot path planning problem in a cluttered environment. Our approach achieves real-time replanning by dividing the workspace into cells and utilizing a hierarchical planner. Specifically, we propose novel multi-commodity flow-based high-level planners that route robots through cells with reduced congestion, along with an anytime low-level planner that computes collision-free paths for robots within each cell in parallel. A highlight of our method is a significant improvement in computation time. Specifically, we show empirical results of a 500-times speedup in computation time compared to the baseline multi-agent pathfinding approach on the environments we study. We account for the robot’s embodiment and support non-stop execution with continuous replanning. We demonstrate the real-time performance of our algorithm with up to 142 robots in simulation, and a representative 32 physical Crazyflie nanoquadrotor experiment.},
  archive   = {C_IROS},
  author    = {Lishuo Pan and Kevin Hsu and Nora Ayanian},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801655},
  month     = {10},
  pages     = {5319-5326},
  title     = {Hierarchical large scale multirobot path (Re)Planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OPENGRASP-LITE version 1.0: A tactile artificial hand with a
compliant linkage mechanism. <em>IROS</em>, 5311–5318. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advancements in artificial hand development have primarily concentrated on enhancing adaptive grasping, dexterity, as well as the integration of biomimetic skin. However, few designs have successfully combined lightweight, cost-effective solutions, and tactile sensing along with adaptive grasping in a human-sized prototype. We propose, an open-source, highly integrated artificial hand. It leverages a compliant linkage mechanism for versatile grasping capabilities, featuring six degrees of actuation and MEMS-based tactile sensors on every fingertip.},
  archive   = {C_IROS},
  author    = {Sonja Groß and Michael Ratzel and Edgar Welte and Diego Hidalgo-Carvajal and Lingyun Chen and Edmundo Pozo Fortunić and Amartya Ganguly and Abdalla Swikir and Sami Haddadin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801426},
  month     = {10},
  pages     = {5311-5318},
  title     = {OPENGRASP-LITE version 1.0: A tactile artificial hand with a compliant linkage mechanism},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rotograb: Combining biomimetic hands with industrial
grippers using a rotating thumb. <em>IROS</em>, 5303–5310. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The development of robotic grippers and hands for automation aims to emulate human dexterity without sacrificing the efficiency of industrial grippers. This study introduces Rotograb, a tendon-actuated robotic hand featuring a novel rotating thumb. The aim is to combine the dexterity of human hands with the efficiency of industrial grippers. The rotating thumb enlarges the workspace and allows in-hand manipulation. A novel joint design minimizes movement interference and simplifies kinematics, using a cutout for tendon routing. We integrate teleoperation, using a depth camera for real-time tracking and autonomous manipulation powered by reinforcement learning with proximal policy optimization. Experimental evaluations demonstrate that Rotograb’s rotating thumb greatly improves both operational versatility and workspace. It can handle various grasping and manipulation tasks with objects from the YCB dataset, with particularly good results when rotating objects within its grasp. Rotograb represents a notable step towards bridging the capability gap between human hands and industrial grippers. The tendon-routing and thumb-rotating mechanisms allow for a new level of control and dexterity. Integrating teleoperation and autonomous learning underscores Rotograb’s adaptability and sophistication, promising substantial advancements in both robotics research and practical applications.},
  archive   = {C_IROS},
  author    = {Arnaud Bersier and Matteo Leonforte and Alessio Vanetta and Sarah Lia Andrea Wotke and Andrea Nappi and Yifan Zhou and Sebastiano Oliani and Alexander M. Kübler and Robert K. Katzschmann},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802788},
  month     = {10},
  pages     = {5303-5310},
  title     = {Rotograb: Combining biomimetic hands with industrial grippers using a rotating thumb},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Under-actuated robotic gripper with multiple grasping modes
inspired by human finger. <em>IROS</em>, 5297–5302. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Under-actuated robot grippers, as a pervasive tool of robots, have become a considerable research focus. Despite their simplicity of mechanical design and control strategy, they suffer from poor versatility and weak adaptability, making widespread applications limited. To better address relevant research gaps, we present a novel 3-finger linkage-based gripper that realizes retractable and reconfigurable multi-mode grasps driven by a single motor. Firstly, inspired by the changes occurred in the contact surface with a human finger moving, we artfully design a slider-slide rail mechanism as the phalanx to achieve retraction of each finger, allowing for better performance in the enveloping grasping mode. Secondly, a reconfigurable structure is constructed to broaden the grasping range of objects’ dimensions for the proposed gripper. By adjusting the configuration and gesture of each finger, the gripper can achieve five grasping modes. Thirdly, the proposed gripper is solely actuated by a single motor, yet it can be capable of grasping and reconfiguring simultaneously. Finally, various experiments on grasps of slender, thin, and large-volume objects are implemented to evaluate the performance of the proposed gripper in practical scenarios, which demonstrates the excellent grasping capabilities of the gripper.},
  archive   = {C_IROS},
  author    = {Jihao Li and Tingbo Liao and Hassen Nigatu and Haotian Guo and Guodong Lu and Huixu Dong},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802532},
  month     = {10},
  pages     = {5297-5302},
  title     = {Under-actuated robotic gripper with multiple grasping modes inspired by human finger},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TRX-hand5: An anthropomorphic hand with integrated tactile
feedback for grasping and manipulation in human environments.
<em>IROS</em>, 5289–5296. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Objects of daily life are designed to suit the human hand. Without major modifications to these objects and our environments, robots will need end-effectors with human hand-like configuration and dexterity to efficiently operate on them. Tight integration of tactile and proprioceptive sensors are also critical to ensure robust execution of manipulation policies without sacrificing range-of-motion. Reliability is also key, and a mechanically robust, easy to repair end-effector is important to minimize downtime. To meet these challenges, we designed a 13 degree-of-freedom anthropomorphic hand with over 1000 tactile sensing elements, named TRX-Hand5. Also embedded within are positional encoders and cable tension sensors to provide proprioceptive perception. TRX-Hand5 has a novel biomimetic topology with six small posture motors in the palm to replicate the function of intrinsic hand muscles and five large power motors in the forearm to play the role of forearm flexor muscles. The whole hand weighs 2.6 kg with its dimensions comparable to those of an adult male’s hand and is capable of actuating its fingertips at over 200°/s while exerting up to 22 N of force. The system can be disassembled in modules for easy maintenance.},
  archive   = {C_IROS},
  author    = {Sicheng Yang and Wang Wei Lee and Zhong Zhang and Youda Xiong and Jiaming Liang and Peng Lu and Yonghui Zhu and Tianliang Liu and Jingchen Li and Rui Wang and Xiong Li and Yu Zheng},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801666},
  month     = {10},
  pages     = {5289-5296},
  title     = {TRX-hand5: An anthropomorphic hand with integrated tactile feedback for grasping and manipulation in human environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Task-oriented dexterous hand pose synthesis using
differentiable grasp wrench boundary estimator. <em>IROS</em>,
5281–5288. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work tackles the problem of task-oriented dexterous hand pose synthesis, which involves generating a static hand pose capable of applying a task-specific set of wrenches to manipulate objects. Unlike previous approaches that focus solely on force-closure grasps, which are unsuitable for non-prehensile manipulation tasks (e.g., turning a knob or pressing a button), we introduce a unified framework covering force-closure grasps, non-force-closure grasps, and a variety of non-prehensile poses. Our key idea is a novel optimization objective quantifying the disparity between the Task Wrench Space (TWS, the desired wrenches predefined as a task prior) and the Grasp Wrench Space (GWS, the achievable wrenches computed from the current hand pose). By minimizing this objective, gradient-based optimization algorithms can synthe-size task-oriented hand poses without additional human demonstrations. Our specific contributions include 1) a fast, accurate, and differentiable technique for estimating the GWS boundary; 2) a task-oriented objective function based on the disparity between the estimated GWS boundary and the provided TWS boundary; and 3) an efficient implementation of the synthesis pipeline that leverages CUDA accelerations and supports large-scale parallelization. Experimental results on 10 diverse tasks demonstrate a 72.6% success rate in simulation. Furthermore, real-world validation for 4 tasks confirms the effectiveness of synthesized poses for manipulation. Notably, despite being primarily tailored for task-oriented hand pose synthesis, our pipeline can generate force-closure grasps 50 times faster than DexGraspNet while maintaining comparable grasp quality. Project page: https://pku-epic.github.io/TaskDexGrasp/.},
  archive   = {C_IROS},
  author    = {Jiayi Chen and Yuxing Chen and Jialiang Zhang and He Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802652},
  month     = {10},
  pages     = {5281-5288},
  title     = {Task-oriented dexterous hand pose synthesis using differentiable grasp wrench boundary estimator},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robotic valve turning: Axial misalignment estimation from
reaction torques. <em>IROS</em>, 5275–5280. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we present a simplified quasi-static model of a two-point contact gripper turning a circular valve to predict the reaction torques produced at the base of the valve as a function of the axis misalignment. Specifically, we learned that geometric features such as (i) the misalignment vector being tangent to the reaction torques, (ii) length of the misalignment vector being directly proportional to the magnitude of the reaction torques and (iii) small axial misalignments resulting in well defined ‘double-loops’ in the 2D reaction torques space are indicative of axis misalignment.},
  archive   = {C_IROS},
  author    = {Gautami Golani and Sri Harsha Turlapati and Lin Yang and Mohammad Zaidi Bin Ariffin and Domenico Campolo},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801385},
  month     = {10},
  pages     = {5275-5280},
  title     = {Robotic valve turning: Axial misalignment estimation from reaction torques},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stable object placing using curl and diff features of
vision-based tactile sensors. <em>IROS</em>, 5267–5274. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ensuring stable object placement is crucial to prevent objects from toppling over, breaking, or causing spills. When an object makes initial contact to a surface, and some force is exerted, the moment of rotation caused by the instability of the object’s placing can cause the object to rotate in a certain direction (henceforth referred to as direction of corrective rotation). Existing methods often employ a Force/Torque (F/T) sensor to estimate the direction of corrective rotation by detecting the moment of rotation as a torque. However, its effectiveness may be hampered by sensor noise and the tension of the external wiring of robot cables. To address these issues, we propose a method for stable object placing using GelSights, vision-based tactile sensors, as an alternative to F/T sensors. Our method estimates the direction of corrective rotation of objects using the displacement of the black dot pattern on the elastomeric surface of GelSight. We calculate the Curl from vector analysis, indicative of the rotational field magnitude and direction of the displacement of the black dots pattern. Simultaneously, we calculate the difference (Diff) of displacement between the left and right fingers’ GelSight’s black dots. Then, the robot can manipulate the objects’ pose using Curl and Diff features, facilitating stable placing. Across experiments, handling 18 differently characterized objects, our method achieves precise placing accuracy (less than 1-degree error) in nearly 100% of cases.},
  archive   = {C_IROS},
  author    = {Kuniyuki Takahashi and Shimpei Masuda and Tadahiro Taniguchi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801674},
  month     = {10},
  pages     = {5267-5274},
  title     = {Stable object placing using curl and diff features of vision-based tactile sensors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contact-implicit model predictive control for dexterous
in-hand manipulation: A long-horizon and robust approach. <em>IROS</em>,
5260–5266. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dexterous in-hand manipulation is an essential skill of production and life. However, the highly stiff and mutable nature of contacts limits real-time contact detection and inference, degrading the performance of model-based methods. Inspired by recent advances in contact-rich locomotion and manipulation, this paper proposes a novel model-based approach to control dexterous in-hand manipulation and overcome the current limitations. The proposed approach has an attractive feature, which allows the robot to robustly perform long-horizon in-hand manipulation without predefined contact sequences or separate planning procedures. Specifically, we design a high-level contact-implicit model predictive controller to generate real-time contact plans executed by the low-level tracking controller. Compared to other model-based methods, such a long-horizon feature enables replanning and robust execution of contact-rich motions to achieve large displacements in-hand manipulation more efficiently; Compared to existing learning-based methods, the proposed approach achieves dexterity and also generalizes to different objects without any pre-training. Detailed simulations and ablation studies demonstrate the efficiency and effectiveness of our method. It runs at 20Hz on the 23-degree-of-freedom, long-horizon, in-hand object rotation task.},
  archive   = {C_IROS},
  author    = {Yongpeng Jiang and Mingrui Yu and Xinghao Zhu and Masayoshi Tomizuka and Xiang Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801751},
  month     = {10},
  pages     = {5260-5266},
  title     = {Contact-implicit model predictive control for dexterous in-hand manipulation: A long-horizon and robust approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Precise well-plate placing utilizing contact during sliding
with tactile-based pose estimation for laboratory automation.
<em>IROS</em>, 5252–5259. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Micro well-plates are an apparatus commonly used in chemical and biological experiments that are a few centimeters thick and contain wells or divets. In this paper, we aim to solve the task of placing the well-plate onto a well-plate holder (referred to as holder). This task is challenging due to the holder’s raised grooves being a few millimeters in height, with a clearance of less than 1 mm between the well-plate and holder, thus requiring precise control during placing. Our placing task has the following challenges: 1) The holder’s detected pose is uncertain; 2) the required accuracy is at the millimeter to sub-millimeter level due to the raised groove’s shallow height and small clearance; 3) the holder is not fixed to a desk and is susceptible to movement from external forces. To address these challenges, we developed methods including a) using tactile sensors for accurate pose estimation of the grasped well-plate to handle issue (1); b) sliding the well-plate onto the target holder while maintaining contact with the holder’s groove and estimating its orientation for accurate alignment. This allows for high precision control (addressing issue (2)) and prevents displacement of the holder during placement (addressing issue (3)). We demonstrate a high success rate for the well-plate placing task, even under noisy observation of the holder’s pose. 4},
  archive   = {C_IROS},
  author    = {Sameer Pai and Kuniyuki Takahashi and Shimpei Masuda and Naoki Fukaya and Koki Yamane and Avinash Ummadisingu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801608},
  month     = {10},
  pages     = {5252-5259},
  title     = {Precise well-plate placing utilizing contact during sliding with tactile-based pose estimation for laboratory automation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PROSPECT: Precision robot spectroscopy exploration and
characterization tool. <em>IROS</em>, 5244–5251. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Near Infrared (NIR) spectroscopy is widely used in industrial quality control and automation to test the purity and grade of items. In this research, we propose a novel sensorized end effector and acquisition strategy to capture spectral signatures from objects and register them with a 3D point cloud. Our methodology first takes a 3D scan of an object generated by a time-of-flight depth camera and decomposes the object into a series of planned viewpoints covering the surface. We generate motion plans for a robot manipulator and end-effector to visit these viewpoints while maintaining a fixed distance and surface normal. This process is enabled by the spherical motion of the end-effector and ensures maximal spectral signal quality. By continuously acquiring surface reflectance values as the end-effector scans the target object, the autonomous system develops a four-dimensional model of the target object: position in an R3 coordinate frame, and a reflectance vector denoting the associated spectral signature. We demonstrate this system in building spectral-spatial object profiles of increasingly complex geometries. We show the proposed system and spectral acquisition planning produce more consistent spectral signals than naïve point scanning strategies. Our work represents a significant step towards high-resolution spectral-spatial sensor fusion for automated quality assessment.},
  archive   = {C_IROS},
  author    = {Nathaniel Hanson and Gary Lvov and Vedant Rautela and Samuel Hibbard and Ethan Holand and Charles DiMarzio and Taşkın Padır},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802210},
  month     = {10},
  pages     = {5244-5251},
  title     = {PROSPECT: Precision robot spectroscopy exploration and characterization tool},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). In-hand singulation and scooping manipulation with a 5 DOF
tactile gripper. <em>IROS</em>, 5238–5243. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Manipulation tasks often require a high degree of dexterity, typically necessitating grippers with multiple degrees of freedom (DoF). While a robotic hand equipped with multiple fingers can execute precise and intricate manipulation tasks, the inherent redundancy stemming from its extensive DoF often adds unnecessary complexity. In this paper, we introduce the design of a tactile sensor-equipped gripper with two fingers and five DoF. We present a novel design integrating a GelSight tactile sensor, enhancing sensing capabilities and enabling finer control during specific manipulation tasks. To evaluate the gripper’s performance, we conduct experiments involving two challenging tasks: 1) retrieving, singularizing, and classification of various objects embedded in granular media, and 2) executing scooping manipulations of credit cards in confined environments to achieve precise insertion. Our results demonstrate the efficiency of the proposed approach, with a high success rate for singulation and classification tasks, particularly for spherical objects at high as 94.3%, and a 100% success rate for scooping and inserting credit cards.},
  archive   = {C_IROS},
  author    = {Yuhao Zhou and Pokuang Zhou and Shaoxiong Wang and Yu She},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801940},
  month     = {10},
  pages     = {5238-5243},
  title     = {In-hand singulation and scooping manipulation with a 5 DOF tactile gripper},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Haptic contour following with the smart suction cup.
<em>IROS</em>, 5232–5237. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Smart Suction Cup is a tactile sensing and gripping system designed to enhance pick-and-place operations in industrial settings. While previous research has primarily focused on utilizing this technology for haptic search in cases of initial grasp failure, this study introduces a novel application: following contours. This function is already established as an important function for object recognition and grasp planning – substantiated by numerous works using other tactile sensors. Here, we explore contour following for a flow-based tactile sensor because it is not susceptible to visual occlusions nor tactile sensor wear. Experimental validation demonstrates the Smart Suction Cup’s ability to track edges at different speeds and navigate various planar contours, showcasing rapid and robust tracking of edges. Notably, the Smart Suction Cup can reliably operate at a speed of 3 cm/s. This is one step towards the adoption of the Smart Suction Cup for real-world applications.},
  archive   = {C_IROS},
  author    = {Sebastian D. Lee and Jungpyo Lee and Hannah S. Stuart},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802260},
  month     = {10},
  pages     = {5232-5237},
  title     = {Haptic contour following with the smart suction cup},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Development of a modular robotic finger for gripping various
shaped objects. <em>IROS</em>, 5226–5231. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the introduction of the Fourth Industrial Revolution and the spread of smart factories, the demand for small-quantity batch production systems is rapidly increasing. As a result, the implementation of robotic gripper systems that can handle various objects is required. Until now, grippers have to be replaced or newly developed each time depending on the object to be gripped. In addition, conventional gripper systems require a picking system based on a sophisticated gripping plan to handle products with complex shapes. This requires the integration of vision and various sensor systems, which in turn increases the cost of the system and makes it challenging to apply it to real industrial sites. To solve this problem, we developed a robotic finger by applying the paired crossed flexure hinge (p-CFH) developed in our previous research. The p-CFH-based robotic finger is driven by an underactuated wire-driven method that can be controlled by a single motor and has compliance and shape adaptive features. It also has the advantage of being modularized, easy to install and replace, and easy to maintain. The proposed finger module has a tip force of about 0.58 kg and its impact absorption capacity has been experimentally verified. In addition, gripping experiments were conducted on a total of four objects with different characteristics, and successful gripping was confirmed.},
  archive   = {C_IROS},
  author    = {Jisu Kim and Jinman Cho and Yeon Kang and Changwha Lee and Dongwon Yun},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801957},
  month     = {10},
  pages     = {5226-5231},
  title     = {Development of a modular robotic finger for gripping various shaped objects},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A soft robotic finger inspired by biological perception
models for tactile sensing. <em>IROS</em>, 5219–5225. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tactile sensing is pivotal for enabling effective human-robot interaction, especially in unstructured environments. This work introduces an innovative bioinspired soft robotic finger endowed with shape-adaptive and multi-modal tactile perception capabilities, drawing inspiration from diverse biological tactile sensing modalities. Through an advanced Fin Ray structure, the soft finger features tactile whiskers on its fingertips, facilitating perception of obstacle orientation, fingertip pressure, surface roughness, and grasping ball size. Leveraging distributed optical fiber sensing technology, we develop a sophisticated multi-point, multi-modal tactile perception neural network tailored for the soft finger. Meticulous integration via advanced 3D printing and silicone coating techniques seamlessly embeds optical fiber sensors within the soft robotic finger, creating an intelligent perception-capable bioinspired mechanical system. Experimental validation confirms the soft robotic finger’s sensitive and precise force perception and curvature recognition abilities, achieving accuracies of up to 100%. In summary, our bioinspired robotic finger holds significant promise for applications in intelligent sensing, non-destructive grasping, and fruit classification within unstructured environments, thus advancing the field of robotics and human-robot interaction.},
  archive   = {C_IROS},
  author    = {Baijin Mao and Qiangjing Yuan and Yuyaocen Xiang and Kunyu Zhou and Weichen Wang and Yaozhen Chen and Hongwei Hao and Juntian Qu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802272},
  month     = {10},
  pages     = {5219-5225},
  title     = {A soft robotic finger inspired by biological perception models for tactile sensing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simulation-assisted learning for efficient bin-packing of
deformable packages in a bimanual robotic cell. <em>IROS</em>,
5211–5218. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bin-packing is an important problem in the robotic warehouse domain. Traditionally, this problem has been studied only for rigid packages (e.g., boxes or rigid objects). In this work, we tackle the problem of bin-packing with deformable packages that have become a popular choice for fulfillment needs. We present a system that incorporates a dual robot arm bimanual setup, uniquely combining suction and sweeping motions to stably and reliably pack deformable packages in a bin. Additionally, we propose a comprehensive action prediction framework to optimize for bin-packing efficiency by predicting optimal actions for both robots involved. Our methodology leverages a two-pronged learning strategy, where initially, we train a model in a self-supervised manner to predict a scoring metric indicative of bin-packing efficiency and then leverage an online optimization scheme to compute optimal actions in real time. The model is pre-trained in simulation in MuJoCo and fine-tuned on small-scale data from a real-world laboratory setting. Our packing score prediction model predicts bin-packing score ∈ [0, 1] with an MSE of 0.003. Real-world experiments validate our method’s adaptability to novel scenarios and its effectiveness in packing operations. Project Website: https://sites.google.com/usc.edu/bimanual-binpacking/},
  archive   = {C_IROS},
  author    = {Omey M. Manyar and Hantao Ye and Meghana Sagare and Siddharth Mayya and Fan Wang and Satyandra K. Gupta},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802246},
  month     = {10},
  pages     = {5211-5218},
  title     = {Simulation-assisted learning for efficient bin-packing of deformable packages in a bimanual robotic cell},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Streamlining object pushing: Behavior tree-based
coordination of control and planning. <em>IROS</em>, 5203–5210. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Efficiently navigating and manipulating objects in complex environments is a fundamental challenge in robotics. This paper presents a novel approach to streamline object-pushing tasks by integrating Behavior Trees (BT) to coordinate a control and planning framework. The proposed system optimizes the execution of tasks involving the pushing of objects while ensuring adaptability to varying scenarios.Our approach employs BTs to encapsulate high-level task specifications and decision-making processes, facilitating a flexible and intuitive representation of robot behavior. By seamlessly integrating BT technology with a coordinated control and planning system, we enable the robot to make real-time decisions and adapt to dynamic environments.We present experimental results demonstrating the effectiveness of our approach, highlighting its ability to improve task execution efficiency and adaptability.},
  archive   = {C_IROS},
  author    = {Filippo Bertoncelli and Lorenzo Sabattini},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802817},
  month     = {10},
  pages     = {5203-5210},
  title     = {Streamlining object pushing: Behavior tree-based coordination of control and planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning temporally composable task segmentations with
language. <em>IROS</em>, 5195–5202. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we present an approach to identify sub-tasks within a demonstrated robot trajectory with the supervision provided by language instructions. Learning longer horizon tasks is challenging with techniques such as reinforcement learning and behavior cloning. Previous approaches have split these long tasks into shorter tasks that are easier to learn by using statistical change point detection methods. However, classical changepoint detection methods function only with low dimensional robot trajectory data and not with high dimensional inputs such as vision. Our goal in this work is to split longer horizon tasks, represented by trajectories into shorter horizon tasks that can be learned using conventional behavior cloning approaches using guidance from language. In our approach we use techniques from the video moment retrieval problem on robot trajectory data to demonstrate a high-dimensional generalizable change-point detection approach. Our proposed moment retrieval-based approach shows a more than 30% improvement in mean average precision (mAP) for identifying trajectory sub-tasks with language guidance compared to that without language. We perform ablations to understand the effects of domain randomization, sample complexity, views, and sim-to-real transfer of our method. In our data ablation we find that just with a 100 labelled trajectories we can achieve a 61.41 mAP, demonstrating the sample efficiency of using such an approach. Further, behavior cloning models trained on our segmented trajectories outperform a single model trained on the whole trajectory by up to 20%.},
  archive   = {C_IROS},
  author    = {Divyanshu Raj and Omkar Patil and Weiwei Gu and Chitta Baral and Nakul Gopalan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802712},
  month     = {10},
  pages     = {5195-5202},
  title     = {Learning temporally composable task segmentations with language},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Constrained bootstrapped learning for few-shot robot skill
adaptation. <em>IROS</em>, 5189–5194. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a robot skill-learning method that facilitates fast adaption to new tasks online. Our method is based on a hybrid learning from demonstration and reinforcement learning approach, which seeds learning with a compact and structured skill model, leading to efficient and stable behaviours. To facilitate fast skill adaption, we propose a bootstrapped learning framework that learns a policy for adapting a skill model across a wide range of initial conditions in simulation. This policy is then used to bootstrap a refinement process that quickly adapts the learnt skill model to new initial conditions in a few learning iterations. Our refined skill model is designed to be deployable on hardware and can correct for discrepancies between the simulation and the real world. Furthermore, we propose a novel method for constraining policy exploration to promising trajectories, which is crucial for enabling manipulation in complex environments. We evaluate our framework in simulation and hardware in multiple environments with varying task complexity. We showcase that compared to the state-of-the-art, which achieves an average success rate of only 56.6% across three different tasks of varying difficulty, our algorithm significantly outperforms it with an average success rate of 90%.},
  archive   = {C_IROS},
  author    = {A K M Nadimul Haque and Fouad Sukkar and Lukas Tanz and Marc G. Carmichael and Teresa Vidal-Calleja},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802705},
  month     = {10},
  pages     = {5189-5194},
  title     = {Constrained bootstrapped learning for few-shot robot skill adaptation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Is a simulation better than teleoperation for acquiring
human manipulation skill data? <em>IROS</em>, 5182–5188. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study explores the feasibility of using simulations as a better interface to collect human object manipulation skills for learning from demonstrations (LfD). Recently, numerous researchers have started introducing teleoperation systems to acquire human manipulation skills. However, capturing the subtle, force-involved interaction skills of humans in teleoperation is still challenging due to its inherent dynamic delays and feedback transparency. This research evaluates the effectiveness of demonstration data obtained through simulation versus teleoperation. To evaluate the efficacy of this approach, tasks such as plane cutting, tight peg-in-hole, and deformable pipe plugging were performed to assess the quality of demonstrations acquired. The experimental results highlight the effectiveness of demonstration through simulation in capturing the operator’s force-involved interaction skills. Simulation creates an environment similar to performing tasks with bare hands by minimising dynamic delays due to the exclusion of physical robots and effectively rendering high stiffness. As a result, the demonstration through simulation method has proven effective in extracting interaction data and capturing physical task performance skills.},
  archive   = {C_IROS},
  author    = {Donghyeon Kim and Seong-Su Park and Kwang-Hyun Lee and Dongheui Lee and Jee-Hwan Ryu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801865},
  month     = {10},
  pages     = {5182-5188},
  title     = {Is a simulation better than teleoperation for acquiring human manipulation skill data?},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incremental learning of robotic manipulation tasks through
virtual reality demonstrations. <em>IROS</em>, 5176–5181. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose an incremental, modular, and extensible method for learning robotic manipulation tasks using a limited number of demonstrations provided in Virtual Reality, while assuming minimal prior information about the objects to be manipulated. The developed framework enables an incremental training process in which the operator first demonstrates specialized tasks to the robotic system, and subsequently more complex tasks, exploiting the skills learned during the previous phases. We illustrate and discuss the method at work considering picking tasks performed by manipulators equipped with multi-fingered sensorized hands. The experimental evaluation highlights the feasibility and advantage of the proposed method, particularly in terms of modularity, low number of demonstrations, and reliability of the trained system.},
  archive   = {C_IROS},
  author    = {Giuseppe Rauso and Riccardo Caccavale and Alberto Finzi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802774},
  month     = {10},
  pages     = {5176-5181},
  title     = {Incremental learning of robotic manipulation tasks through virtual reality demonstrations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Diffusion-PbD: Generalizable robot programming by
demonstration with diffusion features. <em>IROS</em>, 5168–5175. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Programming by Demonstration (PbD) is an intuitive technique for programming robot manipulation skills by demonstrating the desired behavior. However, most existing approaches either require extensive demonstrations or fail to generalize beyond their initial demonstration conditions. We introduce Diffusion-PbD, a novel approach to PbD that enables users to synthesize generalizable robot manipulation skills from a single demonstration by utilizing the representations captured by pre-trained visual foundation models. At demonstration time, hand and object detection priors are used to extract waypoints from the human demonstrations anchored to reference points in the scene. At execution time, features from pre-trained diffusion models are leveraged to identify corresponding reference points in new observations. We validate this approach through a series of real-world robot experiments, showing that Diffusion-PbD is applicable to a wide range of manipulation tasks and has strong ability to generalize to unseen objects, camera viewpoints, and scenes. Code and supplementary videos can be found at https://diffusion-pbd.github.io},
  archive   = {C_IROS},
  author    = {Michael Murray and Entong Su and Maya Cakmak},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802625},
  month     = {10},
  pages     = {5168-5175},
  title     = {Diffusion-PbD: Generalizable robot programming by demonstration with diffusion features},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning symbolic and subsymbolic temporal task constraints
from bimanual human demonstrations. <em>IROS</em>, 5160–5167. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning task models of bimanual manipulation from human demonstration and their execution on a robot should take temporal constraints between actions into account. This includes constraints on (i) the symbolic level such as precedence relations or temporal overlap in the execution, and (ii) the subsymbolic level such as the duration of different actions, or their starting and end points in time. Such temporal constraints are crucial for temporal planning, reasoning, and the exact timing for the execution of bimanual actions on a bimanual robot. In our previous work, we addressed the learning of temporal task constraints on the symbolic level and demonstrated how a robot can leverage this knowledge to respond to failures during execution. In this work, we propose a novel model-driven approach for the combined learning of symbolic and subsymbolic temporal task constraints from multiple bimanual human demonstrations. Our main contributions are a subsymbolic foundation of a temporal task model that describes temporal nexuses of actions in the task based on distributions of temporal differences between semantic action keypoints, as well as a method based on fuzzy logic to derive symbolic temporal task constraints from this representation. This complements our previous work on learning comprehensive temporal task models by integrating symbolic and subsymbolic information based on a subsymbolic foundation, while still maintaining the symbolic expressiveness of our previous approach. We compare our proposed approach with our previous pure-symbolic approach and show that we can reproduce and even outperform it. Additionally, we show how the subsymbolic temporal task constraints can synchronize otherwise unimanual movement primitives for bimanual behavior on a humanoid robot.},
  archive   = {C_IROS},
  author    = {Christian Dreher and Tamim Asfour},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802525},
  month     = {10},
  pages     = {5160-5167},
  title     = {Learning symbolic and subsymbolic temporal task constraints from bimanual human demonstrations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SE(3) linear parameter varying dynamical systems for
globally asymptotically stable end-effector control. <em>IROS</em>,
5152–5159. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Linear Parameter Varying Dynamical Systems (LPV-DS) encode trajectories into an autonomous first-order DS that enables reactive responses to perturbations, while ensuring globally asymptotic stability at the target. However, the current LPV-DS framework is established on Euclidean data only and has not been applicable to broader robotic applications requiring pose control. In this paper we present an extension to the current LPV-DS framework, named Quaternion-DS, which efficiently learns a DS-based motion policy for orientation. Leveraging techniques from differential geometry and Riemannian statistics, our approach properly handles the non-Euclidean orientation data in quaternion space, enabling the integration with positional control, namely SE(3) LPV-DS, so that the synergistic behaviour within the full SE(3) pose is preserved. Through simulation and real robot experiments, we validate our method, demonstrating its ability to efficiently and accurately reproduce the original SE(3) trajectory while exhibiting strong robustness to perturbations in task space.},
  archive   = {C_IROS},
  author    = {Sunan Sun and Nadia Figueroa},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801844},
  month     = {10},
  pages     = {5152-5159},
  title     = {SE(3) linear parameter varying dynamical systems for globally asymptotically stable end-effector control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Riemannian flow matching policy for robot motion learning.
<em>IROS</em>, 5144–5151. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce Riemannian Flow Matching Policies (RFMP), a novel model for learning and synthesizing robot visuomotor policies. RFMP leverages the efficient training and inference capabilities of flow matching methods. By design, RFMP inherits the strengths of flow matching: the ability to encode high-dimensional multimodal distributions, commonly encountered in robotic tasks, and a very simple and fast inference process. We demonstrate the applicability of RFMP to both state-based and vision-conditioned robot motion policies. Notably, as the robot state resides on a Riemannian manifold, RFMP inherently incorporates geometric awareness, which is crucial for realistic robotic tasks. To evaluate RFMP, we conduct two proof-of-concept experiments, comparing its performance against Diffusion Policies. Although both approaches successfully learn the considered tasks, our results show that RFMP provides smoother action trajectories with significantly lower inference times.},
  archive   = {C_IROS},
  author    = {Max Braun and Noémie Jaquier and Leonel Rozo and Tamim Asfour},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801521},
  month     = {10},
  pages     = {5144-5151},
  title     = {Riemannian flow matching policy for robot motion learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PP-TIL: Personalized planning for autonomous driving with
instance-based transfer imitation learning. <em>IROS</em>, 5136–5143.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Personalized motion planning holds significant importance within urban automated driving, catering to the unique requirements of individual users. Nevertheless, prior endeavors have frequently encountered difficulties in simultaneously addressing two crucial aspects: personalized planning within intricate urban settings and enhancing planning performance through data utilization. The challenge arises from the expensive and limited nature of user data, coupled with the scene state space tending towards infinity. These factors contribute to overfitting and poor generalization problems during model training. Henceforth, we propose an instance-based transfer imitation learning approach. This method facilitates knowledge transfer from extensive expert domain data to the user domain, presenting a resolution to these issues. We initially train a pre-trained model using large-scale expert data. Subsequently, during the fine-tuning phase, we feed the batch data, which comprises expert and user data. Employing the inverse reinforcement learning technique, we extract the style feature distribution from user demonstrations, constructing the regularization term for the approximation of user style. In our experiments, we conducted extensive evaluations of the proposed method. Compared to the baseline methods, our approach mitigates the overfitting issue caused by sparse user data. Furthermore, we discovered that integrating the driving model with a differentiable nonlinear optimizer as a safety protection layer for end-to-end personalized fine-tuning results in superior planning performance. The code will be available at https://github.com/LinFunster/PP-TIL.},
  archive   = {C_IROS},
  author    = {Fangze Lin and Ying He and Fei Yu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802818},
  month     = {10},
  pages     = {5136-5143},
  title     = {PP-TIL: Personalized planning for autonomous driving with instance-based transfer imitation learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Knowledge-based programming by demonstration using semantic
action models for industrial assembly. <em>IROS</em>, 5128–5135. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce a knowledge-based Programming by Demonstration (kb-PbD) paradigm to facilitate robot programming in small and medium-sized enterprises (SMEs). PbD in production scenarios requires the recognition of product-specific actions but faces challenges in the lack of suitable and comprehensive datasets, due to the large variety of involved hand actions across different production scenarios. To address this issue, we utilize standardized grasp types as the fundamental feature to recognize basic hand movements, where a Long Short-Term Memory (LSTM) network is employed to recognize grasp types from hand landmarks. The product-specific actions, aggregated from the basic hand movements, are formally modeled in a semantic description language based on the Web Ontology Language (OWL). Description Logic (DL) is used to define the actions with their characteristic properties, which enables the efficient classification of new action instances by an OWL reasoner.The semantic models of hand actions, robot tasks, and work-cell resources are interconnected and stored in a Knowledge Base (KB), which enables the efficient pair-wise translation between hand actions and robot tasks. For the reproduction of human assembly processes, actions are converted to robot tasks via skill descriptions, while reusing the action parameters of involved objects to ensure product integrity. We showcase and evaluate our method in an industrial production setting for control cabinet assembly. Demonstration video available at: https://kb-pbd.github.io/.},
  archive   = {C_IROS},
  author    = {Junsheng Ding and Haifan Zhang and Weihang Li and Liangwei Zhou and Alexander Perzylo},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802511},
  month     = {10},
  pages     = {5128-5135},
  title     = {Knowledge-based programming by demonstration using semantic action models for industrial assembly},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Beyond success: Quantifying demonstration quality in
learning from demonstration. <em>IROS</em>, 5120–5127. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning from Demonstration (LfD) empowers novice users to teach robots daily life tasks without writing sophisticated code, thereby promoting the democratization of robotics. However, novice users often provide sub-optimal demonstrations, which can potentially impact the robot’s ability to efficiently learn and execute the tasks. Prior research has assessed the quality of demonstrations by evaluating the robot’s task performance; however, the approach remains insufficient to qualify individual demonstrations, leaving the reason for classifying demonstrations as high- or low-quality unknown. Therefore, this simulation-based study aims to quantify the quality of individual demonstration at each step by incorporating motion-related quality features such as manipulability and joint-space jerk. To assess the efficacy of these features, we initially evaluated the given demonstrations—taking into account each quality feature—to rank them from high- to low-quality. Subsequently, we investigated the impact of demonstration’s quality on task performance and the quality of task execution. In this pursuit, we trained a series of LfD models for distinct manipulation tasks: cube lifting and pick-and-place of soda can. Our results illustrate a strong correlation between ranked demonstrations and the quality of task execution. Interestingly, we observed that the quality features have a significant impact on task performance, particularly when the provided demonstrations exhibit diversity in terms of quality. Overall, this analysis enables quantifying the quality of individual demonstrations based on motion-related quality features, thus improving learning from demonstration.},
  archive   = {C_IROS},
  author    = {Muhammad Bilal and Nir Lipovetzky and Denny Oetomo and Wafa Johal},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802187},
  month     = {10},
  pages     = {5120-5127},
  title     = {Beyond success: Quantifying demonstration quality in learning from demonstration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient tactile sensing-based learning from limited
real-world demonstrations for dual-arm fine pinch-grasp skills.
<em>IROS</em>, 5112–5119. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Imitation learning for robot dexterous manipulation, especially with a real robot setup, typically requires a large number of demonstrations. In this paper, we present a data-efficient learning from demonstration framework which exploits the use of rich tactile sensing data and achieves fine bimanual pinch grasping. Specifically, we employ a convolutional autoencoder network that can effectively extract and encode high-dimensional tactile information. Further, we develop a framework that achieves efficient multi-sensor fusion for imitation learning, allowing the robot to learn contact-aware sensorimotor skills from demonstrations. The ablation studies on encoded tactile features highlighted the effectiveness of incorporating rich contact information, which enabled dexterous bimanual grasping with active contact searching. Extensive experiments demonstrated the robustness of the fine pinch grasp policy directly learned from few-shot demonstration, including grasping of the same object with different initial poses, generalizing to ten unseen new objects, robust and firm grasping against external pushes, as well as contact-aware and reactive re-grasping in case of dropping objects under very large perturbations. Furthermore, the saliency map analysis method is used to describe weight distribution across various modalities during pinch grasping, confirming the effectiveness of our framework at leveraging multimodal information. The video is available online at: https://youtu.be/BlzxGgiKfck.},
  archive   = {C_IROS},
  author    = {Xiaofeng Mao and Yucheng Xu and Ruoshi Wen and Mohammadreza Kasaei and Wanming Yu and Efi Psomopoulou and Nathan F. Lepora and Zhibin Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802651},
  month     = {10},
  pages     = {5112-5119},
  title     = {Efficient tactile sensing-based learning from limited real-world demonstrations for dual-arm fine pinch-grasp skills},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DexSkills: Skill segmentation using haptic data for learning
autonomous long-horizon robotic manipulation tasks. <em>IROS</em>,
5104–5111. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Effective execution of long-horizon tasks with dexterous robotic hands remains a significant challenge in real-world problems. While learning from human demonstrations has shown encouraging results, they require extensive data collection for training. Hence, decomposing long-horizon tasks into reusable primitive skills is a more efficient approach. To achieve so, we developed DexSkills, a novel supervised learning framework that addresses long-horizon dexterous manipulation tasks using primitive skills. DexSkills is trained to recognize and replicate a select set of skills using human demonstration data, which can then segment a demonstrated long-horizon dexterous manipulation task into a sequence of primitive skills to achieve one-shot execution by the robot directly. Significantly, DexSkills operates solely on proprioceptive and tactile data, i.e., haptic data. Our real-world robotic experiments show that DexSkills can accurately segment skills, thereby enabling autonomous robot execution of a diverse range of tasks.},
  archive   = {C_IROS},
  author    = {Xiaofeng Mao and Gabriele Giudici and Claudio Coppola and Kaspar Althoefer and Ildar Farkhatdinov and Zhibin Li and Lorenzo Jamone},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802807},
  month     = {10},
  pages     = {5104-5111},
  title     = {DexSkills: Skill segmentation using haptic data for learning autonomous long-horizon robotic manipulation tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). JUICER: Data-efficient imitation learning for robotic
assembly. <em>IROS</em>, 5096–5103. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While learning from demonstrations is powerful for acquiring visuomotor policies, high-performance imitation without large demonstration datasets remains challenging for tasks requiring precise, long-horizon manipulation. This paper proposes a pipeline for improving imitation learning performance with a small human demonstration budget. We apply our approach to assembly tasks that require precisely grasping, reorienting, and inserting multiple parts over long horizons and multiple task phases. Our pipeline combines expressive policy architectures and various techniques for dataset expansion and simulation-based data augmentation. These help expand dataset support and supervise the model with locally corrective actions near bottleneck regions requiring high precision. We demonstrate our pipeline on four furniture assembly tasks in simulation, enabling a manipulator to assemble up to five parts over nearly 2500 time steps directly from RGB images, outperforming imitation and data augmentation baselines. Project website: https://imitation-juicer.github.io/.},
  archive   = {C_IROS},
  author    = {Lars Ankile and Anthony Simeonov and Idan Shenfeld and Pulkit Agrawal},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802498},
  month     = {10},
  pages     = {5096-5103},
  title     = {JUICER: Data-efficient imitation learning for robotic assembly},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reward-driven automated curriculum learning for
interaction-aware self-driving at unsignalized intersections.
<em>IROS</em>, 5088–5095. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we present a reward-driven automated curriculum reinforcement learning approach for interaction-aware self-driving at unsignalized intersections, taking into account the uncertainties associated with surrounding vehicles (SVs). These uncertainties encompass the uncertainty of SVs’ driving intention and also the quantity of SVs. To deal with this problem, the curriculum set is specifically designed to accommodate a progressively increasing number of SVs. By implementing an automated curriculum selection mechanism, the importance weights are rationally allocated across various curricula, thereby facilitating improved sample efficiency and training outcomes. Furthermore, the reward function is meticulously designed to guide the agent towards effective policy exploration. Thus the proposed framework could proactively address the above uncertainties at unsignalized intersections by employing the automated curriculum learning technique that progressively increases task difficulty, and this ensures safe self-driving through effective interaction with SVs. Comparative experiments are conducted in Highway_Env, and the results indicate that our approach achieves the highest task success rate, attains strong robustness to initialization parameters of the curriculum selection module, and exhibits superior adaptability to diverse situational configurations at unsignalized intersections. Furthermore, the effectiveness of the proposed method is validated using the high-fidelity CARLA simulator.},
  archive   = {C_IROS},
  author    = {Zengqi Peng and Xiao Zhou and Lei Zheng and Yubin Wang and Jun Ma},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801582},
  month     = {10},
  pages     = {5088-5095},
  title     = {Reward-driven automated curriculum learning for interaction-aware self-driving at unsignalized intersections},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WasteGAN: Data augmentation for robotic waste sorting
through generative adversarial networks. <em>IROS</em>, 5080–5087. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic waste sorting poses significant challenges in both perception and manipulation, given the extreme variability of objects that should be recognized on a cluttered conveyor belt. While deep learning has proven effective in solving complex tasks, the necessity for extensive data collection and labeling limits its applicability in real-world scenarios like waste sorting. To tackle this issue, we introduce a data augmentation method based on a novel GAN architecture called wasteGAN. The proposed method allows to increase the performance of semantic segmentation models, starting from a very limited bunch of labeled examples, such as few as 100. The key innovations of wasteGAN include a novel loss function, a novel activation function, and a larger generator block. Overall, such innovations helps the network to learn from limited number of examples and synthesize data that better mirrors real-world distributions. We then leverage the higher-quality segmentation masks predicted from models trained on the wasteGAN synthetic data to compute semantic-aware grasp poses, enabling a robotic arm to effectively recognizing contaminants and separating waste in a real-world scenario. Through comprehensive evaluation encompassing dataset-based assessments and real-world experiments, our methodology demonstrated promising potential for robotic waste sorting, yielding performance gains of up to 5.8% in picking contaminants. The project page is available at https://github.com/bach05/wasteGAN.git.},
  archive   = {C_IROS},
  author    = {Alberto Bacchin and Leonardo Barcellona and Matteo Terreran and Stefano Ghidoni and Emanuele Menegatti and Takuya Kiyokawa},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802403},
  month     = {10},
  pages     = {5080-5087},
  title     = {WasteGAN: Data augmentation for robotic waste sorting through generative adversarial networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Progressive representation learning for real-time UAV
tracking. <em>IROS</em>, 5072–5079. (<a
href="https://doi.org/10.1109/IROS58592.2024.10803050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual object tracking has significantly promoted autonomous applications for unmanned aerial vehicles (UAVs). However, learning robust object representations for UAV tracking is especially challenging in complex dynamic environments, when confronted with aspect ratio change and occlusion. These challenges severely alter the original information of the object. To handle the above issues, this work proposes a novel progressive representation learning framework for UAV tracking, i.e., PRL-Track. Specifically, PRL-Track is divided into coarse representation learning and fine representation learning. For coarse representation learning, two innovative regulators, which rely on appearance and semantic information, are designed to mitigate appearance interference and capture semantic information. Furthermore, for fine representation learning, a new hierarchical modeling generator is developed to intertwine coarse object representations. Exhaustive experiments demonstrate that the proposed PRL-Track delivers exceptional performance on three authoritative UAV tracking benchmarks. Real-world tests indicate that the proposed PRL-Track realizes superior tracking performance with 42.6 frames per second on the typical UAV platform equipped with an edge smart camera. The code, model, and demo videos are available at https://github.com/vision4robotics/PRL-Track.},
  archive   = {C_IROS},
  author    = {Changhong Fu and Xiang Lei and Haobo Zuo and Liangliang Yao and Guangze Zheng and Jia Pan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10803050},
  month     = {10},
  pages     = {5072-5079},
  title     = {Progressive representation learning for real-time UAV tracking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). MPGNet: Learning move-push-grasping synergy for
target-oriented grasping in occluded scenes. <em>IROS</em>, 5064–5071.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper focuses on target-oriented grasping in occluded scenes, where the target object is specified by a binary mask and the goal is to grasp the target object with as few robotic manipulations as possible. Most existing methods rely on a push-grasping synergy to complete this task. To deliver a more powerful target-oriented grasping pipeline, we present MPGNet, a three-branch network for learning a synergy between moving, pushing, and grasping actions. We also propose a multi-stage training strategy to train the MPGNet which contains three policy networks corresponding to the three actions. The effectiveness of our method is demonstrated via both simulated and real-world experiments. Video of the real-world experiments is at https://youtu.be/S_QKZqkh0w8.},
  archive   = {C_IROS},
  author    = {Dayou Li and Chenkun Zhao and Shuo Yang and Ran Song and Xiaolei Li and Wei Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802784},
  month     = {10},
  pages     = {5064-5071},
  title     = {MPGNet: Learning move-push-grasping synergy for target-oriented grasping in occluded scenes},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Domain randomization-free sim-to-real: An
attention-augmented memory approach for robotic tasks. <em>IROS</em>,
5056–5063. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The sim-to-real gap, a long-standing challenge in the field of robotics, has garnered significant attention. Essentially, it is important to learn robust representation models that can be seamlessly applied in both simulation and real world. Traditional approaches like domain randomization have demonstrated success in zero-short setting, by creating representations that are resilient and adaptable through the augmentation of diversity within simulations. However, they suffer from the need for extensive training across a range of parameter variances, and dependency on heuristic approaches. In this work, we present a novel reinforcement learning architecture named Soft Attention-Augmented Actor-Critic (Soft3AC) for sim-to-real robotic tasks without the need for heuristic domain randomization. Our approach achieves the learning of semantically task-relevant feature representations that exhibit resilience against appearance gaps. This is realized by employing an architectural design that separates current perceptions from historical perceptions in memory, fostering abstract spatial-temporal understanding. Simultaneously, the introduction of an attention mechanism enables a more contextual processing. We validated our method through conducting a valve rotation task with a robotic hand, under both sim-to-sim and sim-to-real conditions. The results indicate that our model adeptly bridges the appearance gap observed in sim-to-sim and sim-to-real transfers. Our method demonstrated its ability to be deployed directly into the real world in a domain randomization free zero-shot manner.},
  archive   = {C_IROS},
  author    = {Jia Qu and Shun Otsubo and Tomoya Yamanokuchi and Takamitsu Matsubara and Shotaro Miwa},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801944},
  month     = {10},
  pages     = {5056-5063},
  title     = {Domain randomization-free sim-to-real: An attention-augmented memory approach for robotic tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). NeuFlow: Real-time, high-accuracy optical flow estimation
on robots using edge devices. <em>IROS</em>, 5048–5055. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-time high-accuracy optical flow estimation is a crucial component in various applications, including localization and mapping in robotics, object tracking, and activity recognition in computer vision. While recent learning-based optical flow methods have achieved high accuracy, they often come with heavy computation costs. In this paper, we propose a highly efficient optical flow architecture, called NeuFlow, that addresses both high accuracy and computational cost concerns. The architecture follows a global-to-local scheme. Given the features of the input images extracted at different spatial resolutions, global matching is employed to estimate an initial optical flow on the 1/16 resolution, capturing large displacement, which is then refined on the 1/8 resolution with lightweight CNN layers for better accuracy. We evaluate our approach on Jetson Orin Nano and RTX 2080 to demonstrate efficiency improvements across different computing platforms. We achieve a notable 10-80 speedup compared to several state-of-the-art methods, while maintaining comparable accuracy. Our approach achieves around 30 FPS on edge computing platforms, which represents a significant breakthrough in deploying complex computer vision tasks such as SLAM on small robots like drones. The full training and evaluation code is available at https://github.com/neufieldrobotics/NeuFlow.},
  archive   = {C_IROS},
  author    = {Zhiyong Zhang and Huaizu Jiang and Hanumant Singh},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802353},
  month     = {10},
  pages     = {5048-5055},
  title     = {NeuFlow: Real-time, high-accuracy optical flow estimation on robots using edge devices},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast spatial reasoning of implicit 3D maps through explicit
near-far sampling range prediction. <em>IROS</em>, 5040–5047. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D mapping is critical for many robotics applications, such as autonomous navigation and object manipulation. Recently, deep implicit mapping approaches have received much attention for their compactness and ability to represent fine-grained details. However, without explicit guidance, such implicit representations are often cumbersome for searching the full range on the rays to find the object surfaces. As a result, several approaches, including hierarchical sampling, occupancy grids, and zero-level set baking, have been proposed to improve sampling where costly forward passes of the neural network should be performed. However, hierarchical sampling is still suboptimal in that it requires uniform coarse samples. Discrete occupancy grids of Instant NGP and zero-level sets of various baking methods are less suitable for large and noisy real scenes.In this paper, we present a novel framework for adaptively predicting the near-far range for sampling the query positions of the deep implicit map. For this purpose, the truncated signed distance grid for the map is pre-constructed and used to provide hints for near-far prediction during rendering. In addition, our recovery algorithm automatically detects failed near-far predictions and recovers only those rays by directly using the implicit map. We conduct extensive experiments on a synthetic dataset, a public real dataset, and a real dataset captured by our multi-camera robot system. The experimental results show that our algorithm achieves the same rendering quality with surprisingly fewer samples compared to the existing methods, which means that the robot can reason about the image and depth properties of the scene much faster. Finally, a thorough analysis of the sample distribution along the rays is provided to give a better understanding of our method’s strong efficiency, adaptability, and robustness. https://chaerinmin.github.io/TSDF-sampling/},
  archive   = {C_IROS},
  author    = {Chaerin Min and Sehyun Cha and Changhee Won and Jongwoo Lim},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802100},
  month     = {10},
  pages     = {5040-5047},
  title     = {Fast spatial reasoning of implicit 3D maps through explicit near-far sampling range prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Rethinking 3D geometric object features for enhancing
skeleton-based action recognition. <em>IROS</em>, 5032–5039. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human action recognition is crucial for intelligent robots, especially in the realm of human-robot collaboration research. Recent advancements in human pose estimation algorithms have shifted the focus of action recognition towards skeleton-based models, which exhibit robustness to changes in background and illumination. However, many state-of-the-art action recognition models rely on 2D skeleton data, neglecting object features. This limitation becomes obvious in complex scenarios where human interactions with objects are crucial, potentially compromising the reliability of assistive robots in understanding human behavior in their environment. To address this issue, we propose a method that effectively integrates 3D geometric object features into skeleton data using graph convolutional neural networks (GCNs). In addition to analyzing the effectiveness of information from different dimensions such as object center position, category, translation, and rotation, we explore various adjacency matrix designs for graph networks. Our model performance is evaluated on two challenging datasets: IKEA ASM and Bimanual Actions. The results demonstrate a significant improvement in action recognition by integrating object features into skeleton-based models. Specifically, on the IKEA-ASM dataset, our approach achieves a frame-wise Top-1 score improvement of 10.8% and an average F1@k improvement of 13.3%, while on the Bimanual Actions dataset, it achieves a frame-wise Top-1 score improvement of 11.4% and an average F1@k improvement of 5.3%, with negligible increases in model complexity.},
  archive   = {C_IROS},
  author    = {Yuankai Wu and Chi Wang and Driton Salihu and Constantin Patsch and Marsil Zakour and Eckehard Steinbach},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802032},
  month     = {10},
  pages     = {5032-5039},
  title     = {Rethinking 3D geometric object features for enhancing skeleton-based action recognition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assessing monocular depth estimation networks for UAS
deployment in rainforest environments. <em>IROS</em>, 5024–5031. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The primary objective of this study was to utilize state-of-the-art deep learning-based monocular depth estimation models to assist UAS pilots in rainforest canopy data collection and navigation. Monocular depth estimation models provide a complementary technique to other depth measurement and estimation techniques to extend the range and improve measurements. Several state-of-the-art models were evaluated using a novel dataset composed of data from a simulated rainforest environment. In the evaluation, MiDaS outperformed the other models, and a segmentation pipeline was designed using this model to identify the highest areas of the canopies. The segmentation pipeline was evaluated using 1080p and 360p input videos from the simulated rainforest dataset. It was able to achieve an IoU of 0.848 and 0.826 and an F1 score of 0.915 and 0.902 at each resolution, respectively. We incorporated the proposed depth-estimation-based segmentation pipeline into an example application and deployed it on an edge system. Experimental results display the capabilities of a UAS using the segmentation pipeline for rainforest data collection.},
  archive   = {C_IROS},
  author    = {Srisai Anirudh Tangellapalli and Harman Singh Sangha and Joshua Peschel and Brittany A. Duncan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802125},
  month     = {10},
  pages     = {5024-5031},
  title     = {Assessing monocular depth estimation networks for UAS deployment in rainforest environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SSL-RGB2IR: Semi-supervised RGB-to-IR image-to-image
translation for enhancing visual task training in semantic segmentation
and object detection. <em>IROS</em>, 5017–5023. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The scarcity of annotated infrared (IR) image datasets limits deep learning networks from achieving performances comparable to those achieved with RGB data. To address this, we introduce a novel semi-supervised RGB-to-IR Image-to-Image Translation model (SSL-RGB2IR) that generates synthetic IR data from RGB images. Our model effectively preserves the IR characteristics in the generated images from both synthetic and real-world data. Compared to existing image-to-image translation techniques, training models on this generated IR data significantly improves performance in downstream tasks like segmentation and detection. Notably, in sim-to-real transfer, the segmentation model trained on SSL-RGB2IR generated IR images outperforms baselines and other Image-to-Image (I2I) models. Furthermore, for real-world applications utilizing EO/IR fusion images, this approach solves the well-known challenge of co-registering EO and IR images, which often have inherent misalignment’s due to differing sensor characteristics. Our code is available at https://github.com/prahlad-anand/ssl-rgb2ir https://github.com/prahlad-anand/ssl-rgb2ir.},
  archive   = {C_IROS},
  author    = {Aniruddh Sikdar and Qiranul Saadiyean and Prahlad Anand and Suresh Sundaram},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802815},
  month     = {10},
  pages     = {5017-5023},
  title     = {SSL-RGB2IR: Semi-supervised RGB-to-IR image-to-image translation for enhancing visual task training in semantic segmentation and object detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Few-shot transparent instance segmentation for bin picking.
<em>IROS</em>, 5009–5016. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we consider the problem of segmenting multiple instances of a transparent object from RGB or gray scale camera images in a robotic bin picking setting. Prior methods for solving this task are usually built on the Mask-RCNN framework, but they require large annotated datasets for fine-tuning. Instead, we consider the task in a few-shot setting and present TrInSeg, a data-efficient and robust instance segmentation method for transparent objects based on Mask-RCNN. Our key innovations in TrInSeg are twofold: i) a novel method, dubbed TransMixup, for producing new training images using synthetic transparent object instances created by spatially transforming annotated examples; and ii) a method for scoring the consistency between the predicted segments and rotations of an ideal object template. In our new scoring method, the spatial transformations are produced by an auxiliary neural network, and the scores are then used to filter inconsistent instance predictions. To demonstrate the effectiveness of our method, we present experiments on a new few-shot dataset consisting of seven categories of non-opaque (transparent and translucent) objects, each category varying in the size, shape, and degree of transparency of the objects. Our results show that TrInSeg achieves state-of-the-art performance, improving fine-tuned Mask-RCNN by more than 14% in mIoU, while requiring very few annotated training samples.},
  archive   = {C_IROS},
  author    = {Anoop Cherian and Siddarth Jain and Tim K. Marks},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802859},
  month     = {10},
  pages     = {5009-5016},
  title     = {Few-shot transparent instance segmentation for bin picking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust multi-camera BEV perception: An image-perceptive
approach to counter imprecise camera calibration. <em>IROS</em>,
5002–5008. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, Bird’s Eye View (BEV) detection methodologies that utilize surround-view cameras have seen significant advancements in autonomous driving systems. Traditional methods, however, are constrained by their reliance on specific camera parameters, which poses challenges in generalizing across different vehicle-mounted cameras with varying poses and under adverse conditions. To address these challenges, we propose a robust BEV representation network that integrates Dual-Space Positional Encoding (DSPE) and image perception. This network is designed to enhance resilience to calibration errors and pose fluctuations, resulting in reliable detection performance on the Nuscenes dataset, even with imprecise extrinsic inputs. Our approach demonstrates competitive accuracy when compared to other methods that do not rely on temporal data, highlighting the effectiveness of our DSPE strategy in improving the robustness and accuracy of BEV detection in dynamic and challenging environments.},
  archive   = {C_IROS},
  author    = {Rundong Sun and Mengyin Fu and Hao Liang and Chunhui Zhu and Zhipeng Dong and Yi Yang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802840},
  month     = {10},
  pages     = {5002-5008},
  title     = {Robust multi-camera BEV perception: An image-perceptive approach to counter imprecise camera calibration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). D2SR: Decentralized detection, de-synchronization, and
recovery of LiDAR interference. <em>IROS</em>, 4994–5001. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the challenge of multi-LiDAR interference, an issue of growing importance as LiDAR sensors are embedded in a growing set of pervasive devices. We introduce a novel approach named D2SR, enabling decentralized interference detection, mitigation, and recovery without explicit coordination among nearby LiDAR devices. D2SR comprises three stages: (a) Detection, which identifies interfered frames, (b) Mitigation, which performs time-shifting of a LiDAR’s active period to reduce interference, and (c) Recovery, which corrects or reconstructs the depth values in interfered regions of a depth frame. Key contributions include a lightweight interference detection algorithm achieving an F1-score of 92%, a simple yet effective decentralized de-synchronization mechanism, and a lightweight depth recovery pipeline that preserves high throughput processing on edge devices. Evaluation on Nvidia Jetson devices demonstrates D2SR’s efficacy: under static settings, D2SR accurately detects interference in 93% of cases (recall=82%) and reduces the depth estimation error by 27% (RMSE= 38.7 cm, compared to RMSE= 60.6 cm for a baseline without D2SR). Furthermore, D2SR is able to reduce the fraction of interfered frames by 75.1% and reduce the depth estimation error (for interfered frames) by 24.9% even for a moving robot scenario.},
  archive   = {C_IROS},
  author    = {Darshana Rathnayake and Hemanth Sabbella and Meera Radhakrishnan and Archan Misra},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801437},
  month     = {10},
  pages     = {4994-5001},
  title     = {D2SR: Decentralized detection, de-synchronization, and recovery of LiDAR interference},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Single-shot 6DoF pose and 3D size estimation for robotic
strawberry harvesting. <em>IROS</em>, 4988–4993. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this study, we introduce a deep-learning approach for determining both the 6DoF pose and 3D size of strawberries, aiming to significantly augment robotic harvesting efficiency. Our model was trained on a synthetic strawberry dataset, which is automatically generated within the Ignition Gazebo simulator, with a specific focus on the inherent symmetry exhibited by strawberries. By leveraging domain randomization techniques, the model demonstrated exceptional performance, achieving an 84.77% average precision (AP) of 3D Intersection over Union (IoU) scores on the simulated dataset. Empirical evaluations, conducted by testing our model on real-world datasets, underscored the model’s viability for real-world strawberry harvesting scenarios, even though its training was based on synthetic data. The model also exhibited robust occlusion handling abilities, maintaining accurate detection capabilities even when strawberries were obscured by other strawberries or foliage. Additionally, the model showcased remarkably swift inference speeds, reaching up to 60 frames per second (FPS).},
  archive   = {C_IROS},
  author    = {Lun Li and Hamidreza Kasaei},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802107},
  month     = {10},
  pages     = {4988-4993},
  title     = {Single-shot 6DoF pose and 3D size estimation for robotic strawberry harvesting},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploiting local features and range images for small data
real-time point cloud semantic segmentation. <em>IROS</em>, 4980–4987.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Semantic segmentation of point clouds is an essential task for understanding the environment in autonomous driving and robotics. Recent range-based works achieve real-time efficiency, while point- and voxel-based methods produce better results but are affected by high computational complexity. Moreover, highly complex deep learning models are often not suited to efficiently learn from small datasets. Their generalization capabilities can easily be driven by the abundance of data rather than the architecture design. In this paper, we harness the information from the three-dimensional representation to proficiently capture local features, while introducing the range image representation to incorporate additional information and facilitate fast computation. A GPU-based KDTree allows for rapid building, querying, and enhancing projection with straightforward operations. Extensive experiments on SemanticKITTI and nuScenes datasets demonstrate the benefits of our modification in a &quot;small data&quot; setup, in which only one sequence of the dataset is used to train the models, but also in the conventional setup, where all sequences except one are used for training. We show that a reduced version of our model not only demonstrates strong competitiveness against full-scale state-of-the-art models but also operates in real-time, making it a viable choice for real-world case applications. The code of our method is available at https://github.com/Bender97/WaffleAndRange.},
  archive   = {C_IROS},
  author    = {Daniel Fusaro and Simone Mosco and Emanuele Menegatti and Alberto Pretto},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801329},
  month     = {10},
  pages     = {4980-4987},
  title     = {Exploiting local features and range images for small data real-time point cloud semantic segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hyperbolic image-and-pointcloud contrastive learning for 3D
classification. <em>IROS</em>, 4973–4979. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D contrastive representation learning has exhibited remarkable efficacy across various downstream tasks. However, existing contrastive learning paradigms based on cosine similarity fail to deeply explore the potential intra-modal hierarchical and cross-modal semantic correlations about multi-modal data in Euclidean space. In response, we seek solutions in hyperbolic space and propose a hyperbolic image-and-pointcloud contrastive learning method (HyperIPC). For the intra-modal branch, we rely on the intrinsic geometric structure to explore the hyperbolic embedding representation of point cloud to capture invariant features. For the cross-modal branch, we leverage images to guide the point cloud in establishing strong semantic hierarchical correlations. Empirical experiments underscore the outstanding classification performance of HyperIPC. Notably, HyperIPC enhances object classification results by 2.8% and few-shot classification outcomes by 5.9% on ScanObjectNN compared to the baseline. Furthermore, ablation studies and confirmatory testing validate the rationality of HyperIPC’s parameter settings and the effectiveness of its submodules.},
  archive   = {C_IROS},
  author    = {Naiwen Hu and Haozhe Cheng and Yifan Xie and Pengcheng Shi and Jihua Zhu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802543},
  month     = {10},
  pages     = {4973-4979},
  title     = {Hyperbolic image-and-pointcloud contrastive learning for 3D classification},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continual domain randomization. <em>IROS</em>, 4965–4972.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Domain Randomization (DR) is commonly used for sim2real transfer of reinforcement learning (RL) policies in robotics. Most DR approaches require a simulator with a fixed set of tunable parameters from the start of the training, from which the parameters are randomized simultaneously to train a robust model for use in the real world. However, the combined randomization of many parameters increases the task difficulty and might result in sub-optimal policies. To address this problem and to provide a more flexible training process, we propose Continual Domain Randomization (CDR) for RL that combines domain randomization with continual learning to enable sequential training in simulation on a subset of randomization parameters at a time. Starting from a model trained in a non-randomized simulation where the task is easier to solve, the model is trained on a sequence of randomizations, and continual learning is employed to remember the effects of previous randomizations. Our robotic reaching and grasping tasks experiments show that the model trained in this fashion learns effectively in simulation and performs robustly on the real robot while matching or outperforming baselines that employ combined randomization or sequential randomization without continual learning. Our code and videos are available at https://continual-dr.github.io/.},
  archive   = {C_IROS},
  author    = {Josip Josifovski and Sayantan Auddy and Mohammadhossein Malmir and Justus Piater and Alois Knoll and Nicolás Navarro-Guerrero},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802060},
  month     = {10},
  pages     = {4965-4972},
  title     = {Continual domain randomization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SwinMTL: A shared architecture for simultaneous depth
estimation and semantic segmentation from monocular camera images.
<em>IROS</em>, 4957–4964. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This research paper presents an innovative multitask learning framework that allows concurrent depth estimation and semantic segmentation using a single camera. The proposed approach is based on a shared encoder-decoder architecture, which integrates various techniques to improve the accuracy of the depth estimation and semantic segmentation task without compromising computational efficiency. Additionally, the paper incorporates an adversarial training component, employing a Wasserstein GAN framework with a critic network, to refine model’s predictions. The framework is thoroughly evaluated on two datasets - the outdoor Cityscapes dataset and the indoor NYU Depth V2 dataset - and it outperforms existing state-of-the-art methods in both segmentation and depth estimation tasks. We also conducted ablation studies to analyze the contributions of different components, including pre-training strategies, the inclusion of critics, the use of logarithmic depth scaling, and advanced image augmentations, to provide a better understanding of the proposed framework. The accompanying source code is accessible at https://github.com/PardisTaghavi/SwinMTL.},
  archive   = {C_IROS},
  author    = {Pardis Taghavi and Reza Langari and Gaurav Pandey},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802239},
  month     = {10},
  pages     = {4957-4964},
  title     = {SwinMTL: A shared architecture for simultaneous depth estimation and semantic segmentation from monocular camera images},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SDTrack: Spatially decoupled tracker for visual tracking.
<em>IROS</em>, 4949–4956. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent models based on encoder-decoder architecture have shown excellent performance in visual object tracking. The encoder models the global spatiotemporal feature correlation between the template and the search regions, while the decoder learns query embeddings to predict the spatial location of the target. However, in previous methods, decoders are query-shared, which may lead to suboptimal results. We observe that different regions in the visual feature map are suitable for performing different tasks. Salient regions in object provide important information for classification task, while the boundaries around it are more beneficial for box localization task. We therefore propose a spatially decoupled tracker called SDTrack. The tracker contains a query selection module that we carefully design to select appropriate queries for both classification and regression tasks. We divide the cross-attention module in the decoder and add the box-to-pixel relative position offset (BoxRPB) term to the cross-attention, so that the attention is more focused on the respective areas of interest while introducing smaller overhead. Finally, we propose an alignment loss to solve the misalignment problem between accurate classification and precise localization, further improving tracking performance. Through extensive experiments, we demonstrate that SDTrack achieves new SOTA performance on multiple benchmarks compared to previous work, while running at real-time speeds.},
  archive   = {C_IROS},
  author    = {Zihao Xia and Xin Bi and Baojie Fan and Zhiquan Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802504},
  month     = {10},
  pages     = {4949-4956},
  title     = {SDTrack: Spatially decoupled tracker for visual tracking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A neurosymbolic approach to adaptive feature extraction in
SLAM. <em>IROS</em>, 4941–4948. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous robots, autonomous vehicles, and humans wearing mixed-reality headsets require accurate and reliable tracking services for safety-critical applications in dynamically changing real-world environments. However, the existing tracking approaches, such as Simultaneous Localization and Mapping (SLAM), do not adapt well to environmental changes and boundary conditions despite extensive manual tuning. On the other hand, while deep learning-based approaches can better adapt to environmental changes, they typically demand substantial data for training and often lack flexibility in adapting to new domains. To solve this problem, we propose leveraging the neurosymbolic program synthesis approach to construct adaptable SLAM pipelines that integrate the domain knowledge from traditional SLAM approaches while leveraging data to learn complex relationships. While the approach can synthesize end-to-end SLAM pipelines, we focus on synthesizing the feature extraction module. We first devise a domain-specific language (DSL) that can encapsulate domain knowledge on the essential attributes for feature extraction and the real-world performance of various feature extractors. Our neurosymbolic architecture then undertakes adaptive feature extraction, optimizing parameters via learning while employing symbolic reasoning to select the most suitable feature extractor. Our evaluations demonstrate that our approach, neurosymbolic Feature EXtraction (nFEX), yields higher-quality features. It also reduces the pose error observed for the state-of-the-art baseline feature extractors ORB and SIFT by up to 90% and up to 66%, respectively, thereby enhancing the system’s efficiency and adaptability to novel environments.},
  archive   = {C_IROS},
  author    = {Yasra Chandio and Momin A. Khan and Khotso Selialia and Luis Garcia and Joseph DeGol and Fatima M. Anwar},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802379},
  month     = {10},
  pages     = {4941-4948},
  title     = {A neurosymbolic approach to adaptive feature extraction in SLAM},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A robotic-centric paradigm for 3D human tracking under
complex environments using multi-modal adaptation. <em>IROS</em>,
4934–4940. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The goal of this paper is to strike a feasible tracking paradigm that can make 3D human trackers applicable on robot platforms and enable more high-level tasks. Till now, two fundamental problems haven’t been adequately addressed. One is the computational cost lightweight enough for robotic deployment, and the other is the easily-influenced accuracy varied greatly in complex real environments. In this paper, a robotic-centric tracking paradigm called MATNet is proposed that directly matches the LiDAR point clouds and RGB videos through end-to-end learning. To improve the low accuracy of human tracking against disturbance, a coarse-to-fine Transformer along with target-ware augmentation is proposed by fusing RGB videos and point clouds through a pyramid encoding and decoding strategy. To better meet the real-time requirement of actual robot deployment, we introduce the parameter-efficient adaptation tuning that greatly shortens the model’s training time. Furthermore, we also propose a five-step Anti-shake Refinement strategy and have added human prior values to overcome the strong shaking on the robot plat-form. Extensive experiments confirm that MATNet significantly outperforms the previous state-of-the-art on both open-source datasets and large-scale robotic datasets.},
  archive   = {C_IROS},
  author    = {Shuo Xin and Zhen Zhang and Liang Liu and Xiaojun Hou and Deye Zhu and Mengmeng Wang and Yong Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802166},
  month     = {10},
  pages     = {4934-4940},
  title     = {A robotic-centric paradigm for 3D human tracking under complex environments using multi-modal adaptation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CR3DT: Camera-RADAR fusion for 3D detection and tracking.
<em>IROS</em>, 4926–4933. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To enable self-driving vehicles accurate detection and tracking of surrounding objects is essential. While Light Detection and Ranging (LiDAR) sensors have set the benchmark for high-performance systems, the appeal of camera-only solutions lies in their cost-effectiveness. Notably, despite the prevalent use of Radio Detection and Ranging (RADAR) sensors in automotive systems, their potential in 3D detection and tracking has been largely disregarded due to data sparsity and measurement noise. As a recent development, the combination of RADARs and cameras is emerging as a promising solution. This paper presents Camera-RADAR 3D Detection and Tracking (CR3DT), a camera-RADAR fusion model for 3D object detection, and Multi-Object Tracking (MOT). Building upon the foundations of the State-of-the-Art (SotA) camera-only BEVDet architecture, CR3DT demonstrates substantial improvements in both detection and tracking capabilities, by incorporating the spatial and velocity information of the RADAR sensor. Experimental results demonstrate an absolute improvement in detection performance of 5.3% in mean Average Precision (mAP) and a 14.9% increase in Average Multi-Object Tracking Accuracy (AMOTA) on the nuScenes dataset when leveraging both modalities. CR3DT bridges the gap between high-performance and cost-effective perception systems in autonomous driving, by capitalizing on the ubiquitous presence of RADAR in automotive applications. The code is available at: https://github.com/ETH-PBL/CR3DT.},
  archive   = {C_IROS},
  author    = {Nicolas Baumann and Michael Baumgartner and Edoardo Ghignone and Jonas Kühne and Tobias Fischer and Yung-Hsu Yang and Marc Pollefeys and Michele Magno},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801848},
  month     = {10},
  pages     = {4926-4933},
  title     = {CR3DT: Camera-RADAR fusion for 3D detection and tracking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FusionTrack: An online 3D multi-object tracking framework
based on camera-LiDAR fusion. <em>IROS</em>, 4920–4925. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D multi-object tracking is an important component of the perception module in autonomous driving systems. Due to the limitations of a single sensor, tracking methods based on either LiDAR or cameras always have certain deficiencies. Fusion-based tracking methods have received increasing attention. However, existing fusion-based tracking methods often underutilize image information, ignore the respective effects of appearance information and 2D detection results, and lack further analysis on the simultaneous use of both. This paper proposes a novel camera-LiDAR fusion tracking framework that primarily relies on the motion model using 3D objects. It fully leverages the appearance information and 2D detection results simultaneously from images and introduces three modules to reduce the number of false positive samples, false negative samples and ID switches, respectively. Besides, the entire tracking process does not require global processing and achieves online tracking. The proposed method achieves competitive results on the KITTI tracking dataset with 78.50% HOTA. Compared with EagerMOT using the same 3D and 2D detectors, the HOTA metric improved by 4.11%. Code is available on https://github.com/zengwz/FusionTrack.},
  archive   = {C_IROS},
  author    = {Weizhen Zeng and Jiaqi Fan and Xuelin Tian and Hongqing Chu and Bingzhao Gao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802805},
  month     = {10},
  pages     = {4920-4925},
  title     = {FusionTrack: An online 3D multi-object tracking framework based on camera-LiDAR fusion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CLAT: Convolutional local attention tracker for real-time
UAV target tracking system with feedback information. <em>IROS</em>,
4912–4919. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-time UAV vision target tracking systems encounter the intricate challenges of striking a trade-off for tracking speed and performance, and the robustness of the following control. In existing tracking systems, the global attention mechanism enhances tracking performance, but it introduces higher computational complexity, impacting target tracking speed; the local attention mechanism can reduce computational complexity but often exhibits limitations in modeling the receptive field. In this paper, we propose a new framework named Convolutional Local Attention Tracker (CLAT) to address these challenges. Firstly, we design a hierarchical convolutional local attention structure as the feature extractor for CLAT. This leverages convolutional projection before local window partitioning, facilitating connections between non-overlapping windows and expanding the receptive field. Secondly, we introduce a streamlined feature fusion network comprising the unshared-weights convolutional layer and a global attention network. The whole design can balance speed and accuracy. Furthermore, to enhance servo control robustness, we have redesigned the upper-level controller by integrating all bounding box information. To capture feedback spatiotemporal information in CLAT, a dynamic template update is implemented by incorporating an IOU head into the predictor. Extensive experiments on visual tracking benchmarks and in the real world demonstrate that CLAT achieves competitive performance. Moreover, we have developed a comprehensive tracking system demonstration capable of precisely tracking targets across various categories. The tracker code will be released on https://github.com/xiaolousun/refine-pytracking.git.},
  archive   = {C_IROS},
  author    = {XiaoLou Sun and ZhiBin Quan and Wei Wang and WuFei Si and ChunYan Wang and YunTian Li and Yuan Wu and Meng Shen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801760},
  month     = {10},
  pages     = {4912-4919},
  title     = {CLAT: Convolutional local attention tracker for real-time UAV target tracking system with feedback information},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). QTrack: Embracing quality clues for robust 3D multi-object
tracking. <em>IROS</em>, 4904–4911. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D Multi-Object Tracking (MOT) has achieved tremendous achievement thanks to the rapid development of 3D object detection and 2D MOT. Recent advanced works generally employ a series of object attributes, e.g., position, size, velocity, and appearance, to provide the clues for the association in 3D MOT. However, these cues may not be reliable due to some visual noise, such as occlusion and blur, leading to tracking performance bottlenecks. To reveal the dilemma, we conduct extensive empirical analysis to expose the key bottleneck of each clue and how they correlate with each other. The analysis results motivate us to efficiently absorb the merits among all cues and adaptively produce an optimal tracking manner. Specifically, we present Location and Velocity Quality Learning, which efficiently guides the network to estimate the quality of predicted object attributes. Based on these quality estimations, we propose a quality-aware object association (QOA) strategy to leverage the quality score as an important reference factor for achieving robust association. Despite its simplicity, extensive experiments indicate that the proposed strategy significantly boosts tracking performance by 2.2% AMOTA and our method outperforms all existing state-of-the-art works on nuScenes by a large margin. Moreover, QTrack achieves 51.1%, 54.8% and 56.6% AMOTA tracking performance on the nuScenes test sets with BEVDepth, VideoBEV, and StreamPETR models respectively, which significantly reduces the performance gap between the pure camera and LiDAR-based trackers.},
  archive   = {C_IROS},
  author    = {Jinrong Yang and En Yu and Zeming Li and Xiaoping Li and Wenbing Tao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801741},
  month     = {10},
  pages     = {4904-4911},
  title     = {QTrack: Embracing quality clues for robust 3D multi-object tracking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GroupTrack: Multi-object tracking by using group motion
patterns. <em>IROS</em>, 4896–4903. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The main challenge of Multi-Object Tracking (MOT) lies in maintaining a distinctive identity for each target in dense crowds or occluded scenarios. Although the existing methods have achieved significantly progress by using robust object detectors or complex association strategies, they cannot effectively solve long-term tracking due to individually motion or appearance modeling for each single target. In this paper, we propose a novel 2D MOT tracker GroupTrack, to learn reliable motion state for each target using group motion patterns. Specifically, for each tracklet, we first choose its neighboring ones to form a group of motion patterns, which can provide informative clues for the motion estimation of the current tracklet. Then, we apply the group motion patterns to perform tracklet prediction and data association. By integrating prior from neighboring motion patterns into the data association process, GroupTrack provides a new paradigm for target motion modeling in extremely crowded and occluded scenarios. Through extensive experiments on the public MOT17 and MOT20 datasets, we demonstrate the effectiveness of our approach in challenging scenarios and show state-of-the-art performance at various MOT metrics.},
  archive   = {C_IROS},
  author    = {Xinglong Xu and Weihong Ren and Gan Sun and Haoyu Ji and Yu Gao and Honghai Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802541},
  month     = {10},
  pages     = {4896-4903},
  title     = {GroupTrack: Multi-object tracking by using group motion patterns},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-target tracking with occlusion resistance for mobile
robots in dynamic environments*. <em>IROS</em>, 4888–4895. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the context of tracking multiple targets on a novel mobile robot, it is essential to obtain the three-dimensional coordinates of specified targets based on tracking boxes. Most existing multi-target tracking algorithms neglect the inherent constraints of the novel mobile robot, such as insufficient computational power, dynamically complex working environments, and irregularly occluded targets. To address these limitations, we propose a robust tracking algorithm with occlusion resistance (hereinafter referred to as ROTrack). ROTrack compensates for the predictions of Kalman filter (KF) by incorporating Inertial Measurement Unit (IMU) information, enabling the tracker to achieve more accurate tracking in dynamic environments. Additionally, MobileSAM is employed to handle occlusion issues and obtain the correct three-dimensional coordinates of the targets. At the same time, a depth-triggered segmentation strategy is proposed to reduce computational resource consumption. The effect of ROTrack is demonstrated through alignment between IMU signals and Camera Motion Compensation (CMC) data in BoT-SORT. Real-world tracking tests validate the robustness and real-time capability of ROTrack.},
  archive   = {C_IROS},
  author    = {Zhongyan Liu and Biao Lu and Xinghai Xing and Dun Mao and Yongchun Fang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802834},
  month     = {10},
  pages     = {4888-4895},
  title     = {Multi-target tracking with occlusion resistance for mobile robots in dynamic environments*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A unified framework of hybrid vision-force control with
nullspace compliance for redundant robots. <em>IROS</em>, 4882–4887. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to handle contact makes robots qualified for many complicated tasks, such as welding, hammering, and wiping. Robot cameras facilitate position planning and control without the geometric knowledge of contact surfaces since they can project contact surfaces onto a 2-dimensional image plane. However, existing hybrid vision-force control (HVFC) methods still rely on this knowledge to project the force on the constraint subspace and do not adequately leverage the redundant degrees of freedom (DoFs) for redundant robots with contact tasks. This paper proposes an enhanced HVFC solution for redundant robots equipped with an eye-to-hand camera to unify HVFC in the Cartesian space and impedance control in the joint nullspace into one closed-loop dynamics with rigorous stability guarantees. Any geometric knowledge of contact surfaces is not required by projecting the force into the redundant space of the visual task rather than the surface’s normal space. Experiments on a seven- DoF collaborative robot have verified that the proposed method is qualified for simultaneous contact tasks in the Cartesian space and compliant interaction in the joint nullspace.},
  archive   = {C_IROS},
  author    = {Zhiwen Li and Weibing Li and Yanjie Chen and Yongping Pan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801808},
  month     = {10},
  pages     = {4882-4887},
  title     = {A unified framework of hybrid vision-force control with nullspace compliance for redundant robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust partitioned visual servoing for aerial manipulation
utilizing controllable-space image planning and adaptive image
representation. <em>IROS</em>, 4874–4881. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the pursuit of object retrieval using an aerial manipulator, developing robust visual servoing techniques in the presence of projection and motion model uncertainties is paramount. This paper proposes a novel approach to conducting image-space planning within the controllable-space of the aerial manipulator. Our new strategy resolves the inherent challenge of adhering to a piecewise linear camera trajectory which is infeasible for an aerial manipulator due to the platform’s underactuation and presence of secondary tasks for visual servoing. Through this approach, we introduce center of gravity alignment and camera orientation potential fields without relying on specific degrees of freedom from the arm. Moreover, we introduce a new approach that utilizes an image-resolution scaling technique involving an adaptive virtual camera focal length, leading to a numerically well-conditioned image Jacobian. Our proposed framework maintains robustness to the uncertainty in the intrinsic parameters of the camera. We substantiate the efficacy of our methodology through experiments conducted in a realistic physics-based simulation environment.},
  archive   = {C_IROS},
  author    = {Mohammad Soltanshah and Abolfazl Eskandarpour and Mehran Mehrandezh and Kamal Gupta},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802700},
  month     = {10},
  pages     = {4874-4881},
  title     = {Robust partitioned visual servoing for aerial manipulation utilizing controllable-space image planning and adaptive image representation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automating trophectoderm cells aspiration and separation in
embryo biopsy at the blastocyst stage: A vision-based control approach.
<em>IROS</em>, 4867–4873. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reproductive medicine has recently witnessed significant advancements, particularly in vitro fertilization (IVF). One crucial aspect of IVF involves the extraction of cellular material and its analysis to maximize the chance of successful implantation. This work highlights the development and application of the automated system for Trophectoderm cell (TE) extraction and separation, addressing the need for precision, efficiency, and reduced manual intervention. The presented automated system is equipped with a computer vision algorithm, microliter pump, vacuum system, and micromanipulation tools to consistently and accurately biopsy TE cells. An experimental setup is developed to verify the behavior of the proposed method, in which a holding micropipette is connected to a vacuum system and holds the embryo stationary. Three steps are performed to complete the process and are controlled by a computer vision algorithm. The coordinates of the Zona Pellucida (ZP) perforation (perforated in a previous step) are used as a feedback signal to a simple proportional controller to control the biopsy pipette motion. The computer vision monitors the amount of TE cells aspirated inside the biopsy pipette and controls the microliter pump. The aspirated TE cells were separated away using a laser cutting system. Experimental results demonstrate that the system can relocate the biopsy pipette, TE cell extraction, and separation.},
  archive   = {C_IROS},
  author    = {Ihab Abu Ajamieh and Mohammad Al Saaideh and Mohammad Al Janaideh and James K. Mills},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801504},
  month     = {10},
  pages     = {4867-4873},
  title     = {Automating trophectoderm cells aspiration and separation in embryo biopsy at the blastocyst stage: A vision-based control approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-spectral visual servoing. <em>IROS</em>, 4861–4866.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel approach for Visual Servoing (VS) using a multispectral camera, where the number of data are more than three times that of a standard color camera. To meet real-time feasibility, the multispectral data captured by the camera are processed using dimensionality reduction techniques. Instead of relying on traditional approaches that select a subset of bands, the proposed method unlocks the full potential of a multispectral camera by pinpointing individual pixels that hold the richest information across all bands. While sacrificing spectral resolution for enhanced spatial resolution - crucial for precise robotic control in forested environments - this fusion process offers a powerful tool for robust and real-time VS in natural settings. Validated through simulations and real-world experiments, the proposed approach demonstrates its efficacy by leveraging the full spectral information of the camera while preserving spatial details.},
  archive   = {C_IROS},
  author    = {Enrico Fiasché and Ezio Malis and Philippe Martinet},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802387},
  month     = {10},
  pages     = {4861-4866},
  title     = {Multi-spectral visual servoing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Visual servo control of a conceptual magnetically anchored
and guided flexible endoscope. <em>IROS</em>, 4854–4860. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a conceptual magnetically anchored and guided flexible endoscope for minimally invasive surgery (MIS). Leveraging both the magnetic coupling between the external and internal permanent magnets and the bending of a flexible joint, the endoscope offers improved maneuver-ability and adaptability within confined surgical spaces. The visual servo control allows the endoscope to autonomously track surgical instruments during procedures, thereby reducing the risk of human error and operator fatigue. First, the design and working principles of the endoscope are introduced. Subsequently, the kinematic modeling of the endoscope is derived, and the control scheme is developed based on a quadratic programming (QP) framework by taking into account both magnetically anchoring constraints and physical constraints, where the joint velocities can be resolved given the desired task velocities in a one-step way. Simulative validations are conducted to verify the effectiveness of the visual servo control for the presented endoscope tracking a static/dynamic target with physical constraints considered.},
  archive   = {C_IROS},
  author    = {Weibing Li and Yang Yang and Yongping Pan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801620},
  month     = {10},
  pages     = {4854-4860},
  title     = {Visual servo control of a conceptual magnetically anchored and guided flexible endoscope},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A mathematical characterization of the convergence domain
for direct visual servoing. <em>IROS</em>, 4846–4853. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Direct Visual Servoing (DVS) is a technique that controls the robot motion by using the pixel intensities captured by a camera. DVS demonstrates high accuracy at convergence, prompting the development of various methods aimed at expanding its convergence domain.In this paper, we propose a mathematical characterization of the DVS convergence domain with closed-form expressions for the controlled degrees of freedom. From these expressions, we concluded that the extent of the convergence domain is related to the presence of isotropic or defocus blur, a phenomenon that had only been observed previously as a trend in empirical experiments.},
  archive   = {C_IROS},
  author    = {Meriem Belinda Naamani and Guillaume Caron and Mitsuharu Morisawa and El Mustapha Mouaddib},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801898},
  month     = {10},
  pages     = {4846-4853},
  title     = {A mathematical characterization of the convergence domain for direct visual servoing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic-range focal sweep: Seamless continuous autofocus
based on high-speed vision for magnified object tracking. <em>IROS</em>,
4838–4845. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an innovative continuous autofocus (C-AF) approach based on high-speed vision. It consistently provides focused images with stable and sufficiently high frame rates, aiming to improve the ability to track small, fast-moving objects in a highly magnified scene. To achieve this, we propose the concept of a dynamic-range focal sweep enabled by a high-speed camera and a focus-tunable liquid lens with high adjustment capability. The focal sweep consistently covers a small range around the object’s focus position, guided by previous depth results obtained through the depth-from-focus (DFF) technique. We conducted verification experiments to thoroughly analyze the capability of the proposed C-AF approach. By integrating a 2-axis Galvano mirror, we built a high-speed C-AF active vision system with rapid focus and pan-tilt adjustments. The comprehensive experiment results highlight the advanced capabilities of our seamless C-AF in tracking magnified objects.},
  archive   = {C_IROS},
  author    = {Tianyi Zhang and Kohei Shimasaki and Idaku Ishii and Akio Namiki},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802751},
  month     = {10},
  pages     = {4838-4845},
  title     = {Dynamic-range focal sweep: Seamless continuous autofocus based on high-speed vision for magnified object tracking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transformer-based relationship inference model for household
object organization by integrating graph topology and ontology.
<em>IROS</em>, 4831–4837. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In domestic environments, the conventional organization of objects by service robots often relies on the inherent properties of each object, such as placing fragile bowls in enclosed cupboards. However, this approach tends to overlook the importance of the orderly arrangement of objects, neglecting the specific placement order of bowls within the cabinet. In practice, effective object organization necessitates consideration of both individual properties and the relationships defined by these properties. In this paper, we have constructed a specialized dataset encompassing the ontological properties of household objects along with their relationships. Furthermore, we have introduced a graph-based model to explicitly represent these relationships and proposed a novel feature extraction technique that integrates the Graph Attention Network (GAT) with the BERT model to predict the relationships among objects. Subsequently, we utilized the Transformer framework to train a model, enabling it to infer relationships between objects. Experimental validation demonstrates the effectiveness of our approach in accurately predicting relationships between household objects, thus facilitating their orderly organization. Our approach significantly augments the object organization capabilities for service robots by accurately predicting the relationships among household objects. Our code is available at: https://github.com/Li-XD-Pro/Household-Object-Organization},
  archive   = {C_IROS},
  author    = {Xiaodong Li and Guohui Tian and Yongcheng Cui and Yu Gu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802781},
  month     = {10},
  pages     = {4831-4837},
  title     = {Transformer-based relationship inference model for household object organization by integrating graph topology and ontology},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Crowd-aware robot navigation with switching between
learning-based and rule-based methods using normalizing flows.
<em>IROS</em>, 4823–4830. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile robot navigation in crowded environments with pedestrians is a crucial challenge in realizing service robots that can assist people in their daily lives. Navigation methods for mobile robots in environments employing deep reinforcement learning have been extensively studied. However, addressing such unexpected situations is a significant challenge. This study presents an approach that discerns whether a situation has been supposed to utilize a normalizing flow and dynamically switches between learning- and rule-based methods. Specifically, the proposed method achieves a higher success rate than employing only a learning-based approach and reaches the destination faster than employing only a rule-based approach in unexpected situations. Experiments are conducted to validate the performance enhancement achieved with the proposed switching method in both simulated and real-world settings.},
  archive   = {C_IROS},
  author    = {Kohei Matsumoto and Yuki Hyodo and Ryo Kurazume},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802676},
  month     = {10},
  pages     = {4823-4830},
  title     = {Crowd-aware robot navigation with switching between learning-based and rule-based methods using normalizing flows},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive social force window planner with reinforcement
learning. <em>IROS</em>, 4816–4822. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human-aware navigation is a complex task for mobile robots, requiring an autonomous navigation system capable of achieving efficient path planning together with socially compliant behaviors. Social planners usually add costs or constraints to the objective function, leading to intricate tuning processes or tailoring the solution to the specific social scenario. Machine Learning can enhance planners’ versatility and help them learn complex social behaviors from data. This work proposes an adaptive social planner, using a Deep Reinforcement Learning agent to dynamically adjust the weighting parameters of the cost function used to evaluate trajectories. The resulting planner combines the robustness of the classic Dynamic Window Approach, integrated with a social cost based on the Social Force Model, and the flexibility of learning methods to boost the overall performance on social navigation tasks. Our extensive experimentation on different environments demonstrates the general advantage of the proposed method over static cost planners.},
  archive   = {C_IROS},
  author    = {Mauro Martini and Noé Pérez-Higueras and Andrea Ostuni and Marcello Chiaberge and Fernando Caballero and Luis Merino},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802383},
  month     = {10},
  pages     = {4816-4822},
  title     = {Adaptive social force window planner with reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SocialNav-FTI: Field-theory-inspired social-aware navigation
framework based on human behavior and social norms. <em>IROS</em>,
4808–4815. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Social navigation is a key consideration for integrating robots into human environments. Concurrently, it imposes heightened requisites: tasks must not only be executed succesfully without collisions, but also adhere to principles encompassing comprehensibility, courtesy, social compliance, comprehension, foresight, and scenario compliance. In this paper, we present the incorporation of social norms as a guiding framework for robot navigation within social contexts. We adopt field theory to provide a formal elucidation of the social norms, using Physical-Informed Neural Network (PINN) to predict pedestrian movement under the influence of social norms, respectively, and using Reinforcement Learning (RL) for navigation. We use supervised learning to train the pedestrian velocity field prediction model and reinforcement learning to train the navigation policy. We conduct three parts of experiments: (1) analyzing the spatiotemporal characteristics of the velocity field in the walking pedestrians dataset; (2) evaluating the accuracy of the vector field prediction in the pedestrian dataset; (3) using Gazebo simulation and the PEDSIM library to evaluate the improvement of navigation performance under constraints of social norms. Experiments have confirmed that the pedestrian motion data set indeed satisfies the Gaussian divergence theorem and can be described by the concept of field. The performance of navigation strategies incorporating social rules has been improved to a certain extent.},
  archive   = {C_IROS},
  author    = {Siyi Lu and Ping Zhong and Shuqi Ye and Bolei Chen and Yu Sheng and Run Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802269},
  month     = {10},
  pages     = {4808-4815},
  title     = {SocialNav-FTI: Field-theory-inspired social-aware navigation framework based on human behavior and social norms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Redefining data pairing for motion retargeting leveraging a
human body prior. <em>IROS</em>, 4800–4807. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose MR.HuBo (Motion Retargeting leveraging a HUman BOdy prior), a cost-effective and convenient method to collect high-quality upper body paired 〈robot, human〉 pose data, which is essential for data-driven motion retargeting methods. Unlike existing approaches which collect 〈robot, human〉 pose data by converting human MoCap poses into robot poses, our method goes in reverse. We first sample diverse random robot poses, and then convert them into human poses. However, since random robot poses can result in extreme and infeasible human poses, we propose an additional technique to sort out extreme poses by exploiting a human body prior trained from a large amount of human pose data. Our data collection method can be used for any humanoid robots, if one designs or optimizes the system’s hyperparameters which include a size scale factor and the joint angle ranges for sampling. In addition to this data collection method, we also present a two-stage motion retargeting neural network that can be trained via supervised learning on a large amount of paired data. Compared to other learning-based methods trained via unsupervised learning, we found that our deep neural network trained with ample high-quality paired data achieved notable performance. Our experiments also show that our data filtering method yields better retargeting results than training the model with raw and noisy data. Our code and video results are available on https://sites.google.com/view/mr-hubo/.},
  archive   = {C_IROS},
  author    = {Xiyana Figuera and Soogeun Park and Hyemin Ahn},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801789},
  month     = {10},
  pages     = {4800-4807},
  title     = {Redefining data pairing for motion retargeting leveraging a human body prior},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Social navigation in crowded environments with model
predictive control and deep learning-based human trajectory prediction.
<em>IROS</em>, 4793–4799. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Navigating a robot among a crowd has received increasing attention from researchers over the last few decades, resulting in the emergence of numerous approaches aimed at addressing the problem of social navigation to date. Our proposed approach couples agent motion prediction and planning to avoid the freezing robot problem while simultaneously capturing multi-agent social interactions by utilizing a state-of-the-art trajectory prediction model i.e., social long short-term memory model (Social-LSTM). Leveraging the output of Social-LSTM for the prediction of future trajectories of pedestrians at each time-step given the robot’s possible future actions, our framework computes the optimal control action using Model Predictive Control (MPC) for the robot to navigate among pedestrians. We demonstrate the effectiveness of our proposed approach in multiple scenarios of simulated social navigation and compare it against several state-of-the-art reinforcement learning-based methods.},
  archive   = {C_IROS},
  author    = {Viet-Anh Le and Behdad Chalaki and Vaishnav Tadiparthi and Hossein Nourkhiz Mahjoub and Jovin D’Sa and Ehsan Moradi-Pari},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802371},
  month     = {10},
  pages     = {4793-4799},
  title     = {Social navigation in crowded environments with model predictive control and deep learning-based human trajectory prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Socially integrated navigation: A social acting robot with
deep reinforcement learning. <em>IROS</em>, 4785–4792. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile robots are being used on a large scale in various crowded situations and become part of our society. The socially acceptable navigation behavior of a mobile robot with individual human consideration is an essential requirement for scalable applications and human acceptance. Deep Reinforcement Learning (DRL) approaches are recently used to learn a robot’s navigation policy and to model the complex interactions between robots and humans. We propose to divide existing DRL-based navigation approaches based on the robot’s exhibited social behavior and distinguish between social collision avoidance with a lack of social behavior and socially aware approaches with explicit predefined social behavior. In addition, we propose a novel socially integrated navigation approach where the robot’s social behavior is adaptive and emerges from the interaction with humans. The formulation of our approach is derived from a sociological definition, which states that social acting is oriented toward the acting of others. The DRL policy is trained in an environment where other agents interact socially integrated and reward the robot’s behavior individually. The simulation results indicate that the proposed socially integrated navigation approach outperforms a socially aware approach in terms of ego navigation performance while significantly reducing the negative impact on all agents within the environment.},
  archive   = {C_IROS},
  author    = {Daniel Flögel and Lars Fischer and Thomas Rudolf and Tobias Schürmann and Sören Hohmann},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801998},
  month     = {10},
  pages     = {4785-4792},
  title     = {Socially integrated navigation: A social acting robot with deep reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The subtle line between personalization and user
manipulation in a european regulatory perspective. A proposal for a
technology-assessment methodology for artificial intelligence systems *.
<em>IROS</em>, 4777–4784. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Much of HRI research focuses on personalizing robots in order to ease societal acceptance, and favour their uptake. Companion robots are indeed conceived as a potential solution to numerous societal concerns, among which aging population, and individuals’ isolation. In such a perspective, personalization is indeed key, for it ensures individuals feel comfortable using robots in their daily lives and environments. This also depends upon the so called cultural competences the machine possesses. In fact, how humans behave largely depends upon their heritage, and overall understanding of the environment. An identical reaction, posture or expression might, indeed, be perceived very differently according to the culture of the person exposed to it.},
  archive   = {C_IROS},
  author    = {Andrea Bertolini},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801918},
  month     = {10},
  pages     = {4777-4784},
  title     = {The subtle line between personalization and user manipulation in a european regulatory perspective. a proposal for a technology-assessment methodology for artificial intelligence systems *},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contextual emotion recognition using large vision language
models. <em>IROS</em>, 4769–4776. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {How does the person in the bounding box feel?&quot; Achieving human-level recognition of the apparent emotion of a person in real world situations remains an unsolved task in computer vision. Facial expressions are not enough: body pose, contextual knowledge, and commonsense reasoning all contribute to how humans perform this emotional theory of mind task. In this paper, we examine two major approaches enabled by recent large vision language models: 1) image captioning followed by a language-only LLM, and 2) vision language models, under zero-shot and fine-tuned setups. We evaluate the methods on the Emotions in Context (EMOTIC) dataset and demonstrate that a vision language model, fine-tuned even on a small dataset, can significantly outperform traditional baselines. The results of this work aim to help robots and agents perform emotionally sensitive decision-making and interaction in the future.},
  archive   = {C_IROS},
  author    = {Yasaman Etesam and Özge Nilay Yalçın and Chuxuan Zhang and Angelica Lim},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802538},
  month     = {10},
  pages     = {4769-4776},
  title     = {Contextual emotion recognition using large vision language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pilot study for a robot-assisted timed up and go
assessment*. <em>IROS</em>, 4763–4768. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Falls and fall risk management are challenges that are increasing in an aging society, exacerbated by the decreasing availability of care professionals to provide suitable fall management plans. Technology may provide a solution to this, with robotics and vision systems receiving increased attention. A pilot study was conducted using a vision system mounted on a Turtlebot 4, MoveNet, and different machine learning algorithms to assess a Timed Up and Go (TUG) test. The system was evaluated on the performance of a previously trained action classifier and by comparing times for the different phases of the TUG test from the output of the model with the output from the QTUG test acquired by IMU sensors worn by the participants. The results showed the system could determine if the person was sitting, in transition, or standing with high accuracy (97.09%) with higher levels of consistency for participants between tests than the QTUG. This demonstrates that the system is not only advantageous requiring minimal user input but also can match the performance of wearable sensors that are considered the &quot;gold standard&quot; for TUG tests.},
  archive   = {C_IROS},
  author    = {Matthew Story and Khaoula Ait-Belaid and Nicola Camp and Roberto Vagnetti and Daniele Magistro and Massimiliano Zecca and Alessandro Di Nuovo},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801568},
  month     = {10},
  pages     = {4763-4768},
  title     = {Pilot study for a robot-assisted timed up and go assessment*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Combining ontological knowledge and large language model for
user-friendly service robots. <em>IROS</em>, 4755–4762. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Lifestyle support through robotics is an increasingly promising field, with expectations for robots to take over or assist with chores like floor cleaning, table setting and clearing, and fetching items. The growth of AI, particularly foundation models, such as large language models (LLMs) and visual language models (VLMs), is significantly shaping this sector. LLMs, by facilitating natural interactions and providing vast general knowledge, are proving invaluable for robotic tasks. This paper focuses on the benefits of LLMs for &quot;bring-me&quot; tasks, where robots fetch specific items for users, often based on ambiguous instructions. Our previous efforts utilized an ontology extended to handle environmental data to resolve such ambiguities, but faced limitations when unresolvable ambiguities required user intervention for clarity. Here, we enhance our approach by integrating LLMs for providing additional commonsense knowledge, pairing it with ontological data to mitigate the issue of hallucinations and reduce the need for user queries, thus improving system usability. We present a system that merges these knowledge bases and assess its efficacy on &quot;bring-me&quot; tasks, aiming to provide a more seamless and efficient robotic assistance experience.},
  archive   = {C_IROS},
  author    = {Haru Nakajima and Jun Miura},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802273},
  month     = {10},
  pages     = {4755-4762},
  title     = {Combining ontological knowledge and large language model for user-friendly service robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). React to this! How humans challenge interactive agents using
nonverbal behaviors. <em>IROS</em>, 4747–4754. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {How do people use their faces and bodies to test the interactive abilities of a robot? Making lively, believable agents is often seen as a goal for robots and virtual agents but believability can easily break down. In this Wizard-of-Oz (WoZ) study, we observed 1169 nonverbal interactions between 20 participants and 6 types of agents. We collected the nonverbal behaviors participants used to challenge the characters physically, emotionally, and socially. The participants interacted freely with humanoid and non-humanoid forms: a robot, a human, a penguin, a pufferfish, a banana, and a toilet. We present a human behavior codebook of 188 unique nonverbal behaviors used by humans to test the virtual characters. The insights and design strategies drawn from video observations aim to help build more interaction-aware and believable robots, especially when humans push them to their limits.},
  archive   = {C_IROS},
  author    = {Chuxuan Zhang and Bermet Burkanova and Lawrence H. Kim and Lauren Yip and Ugo Cupcic and Stéphane Lallée and Angelica Lim},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801673},
  month     = {10},
  pages     = {4747-4754},
  title     = {React to this! how humans challenge interactive agents using nonverbal behaviors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PICaSo: A collaborative robotics system for inpainting on
physical canvas using marker and eraser. <em>IROS</em>, 4739–4746. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotics collaborative drawing involves the inter-action between humans and robots to create of visual art using a variety of tools and materials, serving various functions such as communication, narration, and emotional representation. A creative technique within the human natural drawing process is known as inpainting, which involves reconstructing or editing elements in a drawing. This paper introduces PICaSo (Physical Inpainting on Canvas Solution), a robotic drawing system that enables multiple users to collaboratively create artwork on a canvas by integrating the inpainting process. PICaSo utilizes a fine-tuned text-to-image model to interpret natural language prompts into artistic renderings on canvas. Users guide the process by simple descriptive text and specifying desired drawing placement, empowering the robotic arm to autonomously translate these instructions into physical artworks. Our system’s innovation lies in its effective translation of digital inpainting processes into physical actions. By leveraging our erasing capability that enables selective removal of specific parts on the canvas without impacting neighboring areas, facilitating the creation of sequential drawings. This paper comprehensively outlines the capabilities of the proposed system, explores potential applications across various domains, and addresses technical challenges encountered during its development. Project website: shadynasrat.github.io/PICaSo},
  archive   = {C_IROS},
  author    = {Shady Nasrat and Jae-Bong Yi and Minseong Jo and Seung-joon Yi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801850},
  month     = {10},
  pages     = {4739-4746},
  title     = {PICaSo: A collaborative robotics system for inpainting on physical canvas using marker and eraser},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reducing cognitive load in teleoperating swarms of robots
through a data-driven shared control approach. <em>IROS</em>, 4731–4738.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-robot systems have gained increasing interest across various fields such as medicine, environmental monitoring, and more. Despite the evident advantages, the coordination of the swarm arises significant challenges for human operators, particularly concerning the cognitive burden needed for efficiently controlling the robots. In this study, we present a novel approach for enabling a human operator to effectively control the motion of multiple robots. Leveraging a shared control data-driven approach, we enable a single user to control the 9 degrees of freedom related to the pose and shape of a swarm. Our methodology was evaluated through an experimental campaign conducted in simulated 3D environments featuring a narrow cylindrical path, which could represent, e.g., blood vessels, industrial pipes. Subjective measures of cognitive load were assessed using a post-experiment questionnaire, comparing different levels of autonomy of the system. Results show substantial reductions in operator cognitive load when compared to conventional teleoperation techniques, accompanied by enhancements in task performance, including reduced completion times and fewer instances of contact with obstacles. This research underscores the efficacy of our approach in enhancing human-robot interaction and improving operational efficiency in multi-robot systems.},
  archive   = {C_IROS},
  author    = {Enrico Turco and Chiara Castellani and Valerio Bo and Claudio Pacchierotti and Domenico Prattichizzo and Tommaso Lisini Baldi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802645},
  month     = {10},
  pages     = {4731-4738},
  title     = {Reducing cognitive load in teleoperating swarms of robots through a data-driven shared control approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Retargeting human facial expression to human-like robotic
face through neural network surrogate-based optimization. <em>IROS</em>,
4724–4730. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Facial mimicry is crucial for human-like robots in human-robot interaction. The challenge is that the high diversity of facial expressions proposes difficulties in programming a robotic face to mimic human facial expressions using traditional methods. In this paper, we present a data-driven method to retarget human facial expressions to robotic faces without human effort. Our data collection is fully automatic, where only a robotic face and Apple ARKit are involved to sample actuator commands and record the resulting facial blendshape values. We trained a neural network that predicts blendshape values from commands, which is then used as a surrogate model to optimize command values to resemble given facial expressions. Experiments show that the proposed method has achieved lower error in terms of facial blendshape values than baselines. Moreover, the response time can be reduced to 0.2 seconds via TCP/IP through WiFi, offering great potential for real-time application. Our method is a novel framework for retargeting facial expressions to robotic faces, which can be incorporated into various human-robot interaction systems.},
  archive   = {C_IROS},
  author    = {Bowen Wu and Chaoran Liu and Carlos T. Ishi and Takashi Minato and Hiroshi Ishiguro},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801759},
  month     = {10},
  pages     = {4724-4730},
  title     = {Retargeting human facial expression to human-like robotic face through neural network surrogate-based optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Skeleton-based human action recognition with noisy labels.
<em>IROS</em>, 4716–4723. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Understanding human actions from body poses is critical for assistive robots sharing space with humans in order to make informed and safe decisions about the next interaction. However, precise temporal localization and annotation of activity sequences is time-consuming and the resulting labels are often noisy. If not effectively addressed, label noise negatively affects the model’s training, resulting in lower recognition quality. Despite its importance, addressing label noise for skeleton-based action recognition has been overlooked so far. In this study, we bridge this gap by implementing a framework that augments well-established skeleton-based human action recognition methods with label-denoising strategies from various research areas to serve as the initial benchmark. Observations reveal that these baselines yield only marginal performance when dealing with sparse skeleton data. Consequently, we introduce a novel methodology, NoiseEraSAR, which integrates global sample selection, co-teaching, and Cross-Modal Mixture-of-Experts (CM-MOE) strategies, aimed at mitigating the adverse impacts of label noise. Our proposed approach demonstrates better performance on the established benchmark, setting new state-of-the-art standards. The source code for this study will be made accessible at https://github.com/xuyizdby/NoiseEraSAR.},
  archive   = {C_IROS},
  author    = {Yi Xu and Kunyu Peng and Di Wen and Ruiping Liu and Junwei Zheng and Yufan Chen and Jiaming Zhang and Alina Roitberg and Kailun Yang and Rainer Stiefelhagen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801681},
  month     = {10},
  pages     = {4716-4723},
  title     = {Skeleton-based human action recognition with noisy labels},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Automatic field of view adjustment of an RCM
constraint-free continuum laparoscopic robot. <em>IROS</em>, 4708–4715.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automatic laparoscopic field of view (FOV) adjustment can effectively assist surgeons in minimally invasive surgery (MIS). However, existing work based on rod-shaped laparoscopes is inevitably constrained by the remote center of motion (RCM) during the process of FOV adjustment. The RCM limits laparoscopic movement and makes modeling and control more complex. This paper proposes a novel tendon-driven continuum laparoscope that is not affected by the RCM constraint. Furthermore, an automatic FOV adjustment method is designed for the proposed laparoscope robot, which considers the surgical instrument position and size in the image, as well as eye-hand consistency. Two simulation platforms are developed using MATLAB and Webots to intuitively study and optimize the proposed adjustment method. The convergence time of surgical tool tracking with a complex 3D trajectory is only 1s, the average tracking error after stabilization is about 9.97 pixels, and the maximum eye-hand error is only 0.04°. A first-generation prototype is built to verify the tracking performance of the proposed tendon-driven continuum laparoscope. The experimental results show that the proposed system can perform real-time laparoscopic FOV adjustment without being constrained by the RCM.},
  archive   = {C_IROS},
  author    = {Jing Zhang and Baichuan Wang and Zhijie Pan and Weiqi Li and Mengtang Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802132},
  month     = {10},
  pages     = {4708-4715},
  title     = {Automatic field of view adjustment of an RCM constraint-free continuum laparoscopic robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cartesian impedance control generalized to one-parameter
splines. <em>IROS</em>, 4701–4707. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic-assisted upper limb rehabilitation has gained significant attention in recent years due to its potential to enhance the recovery process for individuals with motor impairments resulting from neurological conditions and injuries. The main rehabilitation treatments rely on the repetitive execution of a movement of the upper-limb, guided by a therapist to prevent incorrect movements and to provide the necessary support. Many of the exercises performed by therapists can be modeled as a movement in SE(3) space (position and orientation). This movement itself is one-dimensional, as it can be modeled by a one-dimensional curve. To solve a similar problem, some approaches have been proposed in human-robot interaction (HRI) following virtual guides, but are either limited to specific types of curves (e.g. without orientation) or rely on linear control methods with non-intuitive parameters. To address these limitations and enable the use of these methods in physical rehabilitation, this paper extends Cartesian impedance control to splines, which we term path impedance control. It capitalizes on the intrinsic path geometry of end-effector robotic rehabilitation systems. The primary objective of this control algorithm is to emulate the sensation of maneuvering a physical object along a wire, akin to conventional exercise machines; and, in conjunction, provide an intuitive parametrization of rehabilitation exercises. We build on existing virtual guide control strategies using non-linear control and Lie Groups to generalize the control law to any one-parameter SE(3) curve.},
  archive   = {C_IROS},
  author    = {Ignacio Montesino and Juan G. Victores and Carlos Balaguer and Alberto Jardon},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801939},
  month     = {10},
  pages     = {4701-4707},
  title     = {Cartesian impedance control generalized to one-parameter splines},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Meta-learning for fast adaptation in intent inferral on a
robotic hand orthosis for stroke. <em>IROS</em>, 4693–4700. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose MetaEMG, a meta-learning approach for fast adaptation in intent inferral on a robotic hand orthosis for stroke. One key challenge in machine learning for assistive and rehabilitative robotics with disabled-bodied subjects is the difficulty of collecting labeled training data. Muscle tone and spasticity often vary significantly among stroke subjects, and hand function can even change across different use sessions of the device for the same subject. We investigate the use of metalearning to mitigate the burden of data collection needed to adapt high-capacity neural networks to a new session or subject. Our experiments on real clinical data collected from five stroke subjects show that MetaEMG can improve the intent inferral accuracy with a small session- or subject-specific dataset and very few fine-tuning epochs. To the best of our knowledge, we are the first to formulate intent inferral on stroke subjects as a meta-learning problem and demonstrate fast adaptation to a new session or subject for controlling a robotic hand orthosis with EMG signals.},
  archive   = {C_IROS},
  author    = {Pedro Leandro La Rotta and Jingxi Xu and Ava Chen and Lauren Winterbottom and Wenxi Chen and Dawn Nilsen and Joel Stein and Matei Ciocarlie},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801596},
  month     = {10},
  pages     = {4693-4700},
  title     = {Meta-learning for fast adaptation in intent inferral on a robotic hand orthosis for stroke},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating gait symmetry with a smart robotic walker: A
novel approach to mobility assessment. <em>IROS</em>, 4686–4692. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Gait asymmetry, a consequence of various neurological or physical conditions such as aging and stroke, detrimentally impacts bipedal locomotion, causing biomechanical alterations, increasing the risk of falls and reducing quality of life. Addressing this critical issue, this paper introduces a novel diagnostic method for gait symmetry analysis through the use of an assistive robotic Smart Walker equipped with an innovative asymmetry detection scheme. This method analyzes sensor measurements capturing the interaction torque between user and walker. By applying a seasonal-trend decomposition tool, we isolate gait-specific patterns within these data, allowing for the estimation of stride durations and calculation of a symmetry index. Through experiments involving 5 experimenters, we demonstrate the Smart Walker’s capability in detecting and quantifying gait asymmetry by achieving an accuracy of 84.9% in identifying asymmetric cases in a controlled testing environment. Further analysis explores the classification of these asymmetries based on their underlying causes, providing valuable insights for gait assessment. The results underscore the potential of the device as a precise, ready-to-use monitoring tool for personalized rehabilitation, facilitating targeted interventions for enhanced patient outcomes.},
  archive   = {C_IROS},
  author    = {Mahdi Chalaki and Abed Soleymani and Xingyu Li and Vivian Mushahwar and Mahdi Tavakoli},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801709},
  month     = {10},
  pages     = {4686-4692},
  title     = {Evaluating gait symmetry with a smart robotic walker: A novel approach to mobility assessment},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discover2Walk: A cable-driven robotic platform to promote
gait in pediatric population. <em>IROS</em>, 4678–4685. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advancements in gait rehabilitation have led to the development of innovative approaches that complement traditional therapeutic methods. These include intense, task-specific exercise strategies, non-invasive treatments, surgical interventions, and advanced robotic technologies. While robotic systems for adult gait rehabilitation are well-established, there is a notable scarcity of such devices for pediatric patients, especially toddlers, due to their unique developmental and biomechanical needs. This work introduces Discover2Walk (D2W), a novel robotic platform designed specifically for pediatric gait rehabilitation in small children. The D2W platform features a multi-module, cable-driven architecture guided by an omnidirectional traction module, addressing the limitations of current rehabilitation devices for younger populations. The platform&#39;s modular design consists of three actuated modules—pelvic, ankle, and traction—synchronized by a personalised gait pattern generator. This configuration allows for simultaneous control and monitoring of pelvic and ankle motion using partial body weight support and Assistance As Needed (AAN) approaches. Preliminary evaluations were conducted with pediatric patients with Cerebral Palsy, involving two ambulatory six-year-olds and one non-ambulatory four-year-old, over a series of 10 gait rehabilitation sessions. Data analysis from the ambulatory children showed a decrease in the robotic effort required to assist limb movements along healthy trajectories throughout the sessions, accompanied by an increase in walking speeds. Further work will include expanding the patient cohort to include a broader range of ages, sizes and GMFCS levels to validate the system&#39;s effectiveness across a wider spectrum of pediatric gait disabilities and validating the traction effectiveness.},
  archive   = {C_IROS},
  author    = {Pablo Romero-Sorozabal and Gabriel Delgado-Oleas and Annemarie F Laudanski and Álvaro Gutiérrez and Eduardo Rocon},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802838},
  month     = {10},
  pages     = {4678-4685},
  title     = {Discover2Walk: A cable-driven robotic platform to promote gait in pediatric population},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A parallel-actuated robot with two end-effector
degrees-of-freedom: Application as a novel wearable head-neck traction
brace. <em>IROS</em>, 4672–4677. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper describes a parallel-actuated robotic mechanism designed to provide two degrees-of-freedom (DOF) to the end-effector relative to a fixed base. In a potential application as a head-neck traction brace, these two independent DOFs are the vertical translation of the head with respect to shoulders and a specified orientation of the head in lateral bending. Motivated by recommended clinical methods to apply traction forces on the head, it is designed to provide vertical traction force on the head while tilted in a specific orientation. The design has four chains starting from a base stationed at the shoulders, each chain having 5 DOFs. Each chain imposes a single constraint on the motion of the end-effector. Together, four chains would apply four constraints, allowing only two DOFs of motion to the end-effector. Two out of four component chains are actively driven by linear actuators. Our kinematic studies show that the achievable workspace of this mechanism with a specific stroke length of actuators of ± 50 mm results in 175-222 mm of vertical translation and up to ± 9 ◦ of lateral bending. The lateral bending is coupled to the flexion/extension angle of the end-effector. A physical prototype was constructed to investigate the functional realization of the design in hardware. Overall, the physical prototype validated the motion of the theoretical model despite potential errors in the fabrication, making the design a candidate for potential head-neck traction application.},
  archive   = {C_IROS},
  author    = {Jingzong Zhou and Priya Kulkarni and Sunil Agrawal},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802646},
  month     = {10},
  pages     = {4672-4677},
  title     = {A parallel-actuated robot with two end-effector degrees-of-freedom: Application as a novel wearable head-neck traction brace},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Creating discomfort maps via hand-held human feedback
interface for robotic shoulder physiotherapy. <em>IROS</em>, 4664–4671.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we propose a method of capturing the patient’s discomfort during robotic shoulder physiotherapy, creating &quot;discomfort maps&quot;. These maps depict the personalized distribution of discomfort that each patient perceived across their shoulder range of motion, facilitating both robotic devices and human therapists to account for patient-specific characteristics during the therapeutic process. Our system enables a patient to communicate and map discomfort in space and time during movement via a handheld push-button device, while interacting with a robotic physical therapy device capable of moving the patient and estimating their pose. We validated our method through human factors experiments simulating shoulder physiotherapy sessions with 10 healthy participants. To avoid the risk of injury to the participants and to allow for ground truth map information, we emulate perceived discomfort via an auditory signal. Our experimental apparatus enabled participants to reconstruct synthetic discomfort maps, demonstrating the feasibility of automatically capturing and storing patient discomfort during robotic physiotherapy.},
  archive   = {C_IROS},
  author    = {Jevon Ravenberg and Italo Belli and J. Micah Prendergast and Ajay Seth and Luka Peternel},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801858},
  month     = {10},
  pages     = {4664-4671},
  title     = {Creating discomfort maps via hand-held human feedback interface for robotic shoulder physiotherapy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A series variable-stiffness joint for robot-assisted
resistance training. <em>IROS</em>, 4658–4663. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As the safe and comfortable human-robot interaction draw more attention, variable stiffness joints are adopted in the rehabilitation robots for the compliant experiences. For patients with joint injury, resistance training has demonstrated its efficacy in concurrently enhancing muscle strength in the rehabilitation process. Here, we introduce a compact passive joint with variable stiffness and damping generation capability for robot-assisted resistance training. The abilities of serial variable stiffness and passive angle tracking were demonstrated by a prototype. The variable stiffness mechanism was mainly formed by 3D-printing, which only weighs 0.1kg with a wide range of stiffness (0.004-11.176Nm/rad). Moreover, the torque-angular displacement characteristics show excellent linearity at high joint stiffness. Finally, the feasibility of being integrated into the wearable rehabilitation robot has been evaluated.},
  archive   = {C_IROS},
  author    = {Xingyu Hu and Yuebing Li and Haoyang Wu and Wuxiang Zhang and Yanggang Feng},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801599},
  month     = {10},
  pages     = {4658-4663},
  title     = {A series variable-stiffness joint for robot-assisted resistance training},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modular robot wear for walking assistance according to
physical functionality. <em>IROS</em>, 4651–4657. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As people age, walking can become difficult. This difficulty in walking can lead to further physical decline and eventually result in the need for caregiving. To prevent this, wearable robots supporting walking rehabilitation have been developed. In this study, we aim to develop a modular robot wear that can be customized according to the user’s needs. The modular robot wear consists of motor modules, sensor modules, and a processor. Motor modules can be attached to any part of the body that requires assistance, enabling support not only in the sagittal plane but also in the frontal plane. By calculating the relative movement of the center of gravity based on the information from the acceleration sensors in the sensor modules, commands to the motor modules are generated. This allows for assistance tailored to the user’s walking pattern. Through verification experiments of the robot wear’s operation, we confirmed its ability to provide support according to changes in walking patterns. However, while the robot wear can induce changes in walking patterns, there are challenges regarding the output it provides to the wearer.},
  archive   = {C_IROS},
  author    = {Kunihiro Ogata and Toshiki Futawatari and Masahiro Fujimoto and Yumeko Imamura and Yoshio Matsumoto},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801867},
  month     = {10},
  pages     = {4651-4657},
  title     = {Modular robot wear for walking assistance according to physical functionality},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design improvements to the float upper-limb exoskeleton
better mimics the glenohumeral complex kinematics. <em>IROS</em>,
4643–4650. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The shoulder glenohumeral complex stands out as one of the most complex structures within the human body. Designing a system that can effectively interface with it poses a significant challenge for researchers. In this study, we propose a methodology based on evaluating various metrics to assess the performance of new kinematic solutions for mimicking the glenohumeral complex. The proposed method is demonstrated on an existing design (Float) of an upper-limb exoskeleton. The results show a successful expansion of the reachable workspace and enhancement of the shoulder internal-external rotation. The improvements ensure the necessary range-of-motion for the patient’s natural use of the exoskeleton. Specifically, the existing Eulerian wrist architecture is replaced with a 3-degree-of-freedom RPY wrist to better resemble the glenohumeral shoulder joint complex. This study also explores the trade-offs between these enhancements and the desired system manipulability.},
  archive   = {C_IROS},
  author    = {Giulia Bodo and Federico Tessari and Gianluca Capitta and Luca De Guglielmo and Stefano Buccelli and Matteo Laffranchi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802633},
  month     = {10},
  pages     = {4643-4650},
  title     = {Design improvements to the float upper-limb exoskeleton better mimics the glenohumeral complex kinematics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-selecting semi-supervised transformer-attention
convolutional network for four class EEG-based motor imagery decoding.
<em>IROS</em>, 4636–4642. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Brain-computer interfaces (BCI) serve as an important tool in areas such as neurorehabilitation and constructing prostheses. Electroencephalogram (EEG) motor imagery (MI) signal is a common method used to communicate between the human brain and the computer interface. However, differentiating between multiple motor imagery signals may be challenging due to the presence of high noise-to-signal ratio and small dataset sizes. In this study, we propose a variational autoencoder and transformer-attention based convolutional neural network (SSTACNet) for multi-class EEG-based motor imagery classification. The SSTACNet model leverages upon variational autoencoders’ ability to measure the contrastive distance between two sets of inputs to perform data self-selection. The model further utilizes multi-head self-attention as well as spatial and temporal convolutional filters to achieve superior extraction of signal features. The model additionally utilizes the variational autoencoder’s ability to augment the dataset with feature-informed pseudo-data, achieving stronger classification results. The proposed model outperforms the current state-of-the-art techniques in the BCI Competition IV-2a dataset with an accuracy of 85.52% and 70.56% for the subject-dependent and subject-independent modes, respectively. Codes may be found at: https://github.com/NgHanWei/SSTACNet},
  archive   = {C_IROS},
  author    = {Han Wei Ng and Cuntai Guan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801654},
  month     = {10},
  pages     = {4636-4642},
  title     = {Self-selecting semi-supervised transformer-attention convolutional network for four class EEG-based motor imagery decoding},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). A wearable mechanical pressure-electrophysiological bimodal
sensing system for rehabilitation electromechanical device.
<em>IROS</em>, 4630–4635. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the aging of society, there has been an increase in the number of elderly individuals with limb movement disorders. Active rehabilitation training using limb rehabilitation electromechanical devices that incorporate multimodal sensing and monitoring functions can significantly contribute to the recovery of limb motor functions. This report introduces a wearable mechanical pressure-electrophysiological monitoring bimodal sensing system specifically designed for human limb rehabilitation devices. By utilizing just four electrodes (SE, CE/DE, GND, REF), this system enables simultaneous and co-located measurement of surface electromyographic (sEMG), pressure, and mechanomyography (MMG) signals. These signals can be utilized to analyze muscle tension, stiffness, and tremor information. At last, this sensing system was used to assess muscle contraction force and localized muscle fatigue. The time and frequency domain characteristics of physiological signals during exercise were thoroughly investigated. The wearable mechanical pressure-electrophysiological bimodal sensing system can provide valuable data references for rehabilitation robots or human limb rehabilitation device, which is of great significance in the diagnosis of muscular diseases and rehabilitation treatment.},
  archive   = {C_IROS},
  author    = {Peng Wang and Jixiao Liu and Dianpeng Qi and Shijie Guo},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801697},
  month     = {10},
  pages     = {4630-4635},
  title     = {A wearable mechanical pressure-electrophysiological bimodal sensing system for rehabilitation electromechanical device},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). X-ray-guided magnetic fields for wireless control of
untethered magnetic robots in cerebral vascular phantoms. <em>IROS</em>,
4624–4629. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper explores the application of X-ray-guided magnetic fields for the wireless control of untethered magnetic robots (UMRs) within cerebral vascular phantoms. With a focus on addressing challenges associated with strokes and brain aneurysms, the study aims to enhance neurosurgical procedures by improving precision and maneuverability. Experimental findings showcase the feasibility and effectiveness of this innovative approach in navigating UMRs, characterized by a screw-shaped body and a ferromagnetic core, through complex vascular structures. Cone-beam computed tomography is employed to determine the tomography and provide various reference trajectories for the UMR inside the cerebral vascular phantom. Our motion control experiments show that the X-ray-guided magnetic fields enable the UMR to move along any intended path with an average success rate of 89%, allowing the UMR to move between the left and right common carotid artery to the left and right internal and external carotid artery.},
  archive   = {C_IROS},
  author    = {Leendert-Jan W. Ligtenberg and Marcus C. J. de Boer and Iris Mulder and Roger Lomme and Dorothee Wasserberg and Emily A. M. Klein Rot and Doron Ben Ami and Udi Sadeh and H. Remco Liefers and Oded Shoseyov and Pascal Jonkheijm and Michiel Warlé and Islam S. M. Khalil},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802534},
  month     = {10},
  pages     = {4624-4629},
  title     = {X-ray-guided magnetic fields for wireless control of untethered magnetic robots in cerebral vascular phantoms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A robotic mediation device for skill assessment and training
during colonoscopy. <em>IROS</em>, 4616–4623. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Colonoscopy demands multi-finger coordinated motion to achieve safe navigation. As a result, training for colonoscopists is challenging and skill assessment currently relies on subjective scoring by expert proctors. There is a need to provide tools for skill assessment and aid with training new interventionalists. This paper presents a new concept of an in-hand robotic mediation device that can be used for both skill assessment and training. The robotic device can be used to infer the kinematic motion as well as the power input of a user - both of which are proposed to be used for skill assessment and subtask skill classification. Preliminary results collected expert and novice users performing colonoscopy navigation are used to demonstrate this device as a skill assessment tool. A machine-learning model (classification and regression trees) is used for subtask classification of skill and evaluating the most important classification features. A user study demonstrates the effectiveness of this in hand haptic training and assessment tool. We believe that, in the future, this device will enable accelerated skill assessment and training and possible semi-automation of difficult maneuvers.},
  archive   = {C_IROS},
  author    = {Olivia Richards and Elan Ahronovich and Neel Shihora and Ahmet Yildiz and Jumana Atoum and Jie Ying Wu and Keith L. Obstein and Nabil Simaan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801301},
  month     = {10},
  pages     = {4616-4623},
  title     = {A robotic mediation device for skill assessment and training during colonoscopy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Procedural generation of tunnel networks for unsupervised
training and testing in underground applications. <em>IROS</em>,
4608–4615. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Developing a robotic application requires thorough testing of the complete system to ensure its reliability. However, depending on the target environment, real-life testing can be difficult to carry out, which favors simulations. Also, some techniques like those based on machine learning, may require large varieties of sensor data, which can be gathered in simulation with ease, whereas doing the same in real environments can pose a great challenge.This work presents a flexible approach to the procedural generation of tunnel networks suitable for underground robotics simulations. The method starts with a graph representation of an underground environment, and applies a custom meshing strategy to generate tunnels that follow the graph structure. This mesh can then be imported into the desired simulation software. The ease of use of this method allows for the testing of robotic applications in an arbitrary number of different environments in completely automated workflows.},
  archive   = {C_IROS},
  author    = {Lorenzo Cano and Danilo Tardioli and Alejandro R. Mosteo},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801552},
  month     = {10},
  pages     = {4608-4615},
  title     = {Procedural generation of tunnel networks for unsupervised training and testing in underground applications},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hardware-based time synchronization for a multi-sensor
system. <em>IROS</em>, 4600–4607. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate time synchronization is crucial for multisensor fusion, which is widely used in mobile robotics, autonomous driving, and virtual reality. Despite many advancements, precise multi-sensor synchronization is still challenging due to the sensors’ internal characteristics, data filtering, disjointed clock reference, and transmission delay caused by operation system scheduling. This paper proposes a novel hardware-based synchronization solution to achieve synchronization in microsecond-level precision. By introducing a Sensor Adaptor board that provides a unified clock reference, the proposed hardware architecture enables high-precision synchronization across multiple sensors. Furthermore, we develop a method for Visual-Inertial time synchronization that actively controls the exposure duration using an ambient light sensor. By managing the IMU clock signal and exposure trigger, we align the camera’s sampling moment with the authentic IMU sampling time and significantly reduce the time discrepancy in the Visual-Inertial system. Experiments are conducted to evaluate the efficiency of the proposed method and system, including comparisons with previous work. The results indicate that our method can achieve precise time synchronization and be successfully implemented in multi-sensor systems.},
  archive   = {C_IROS},
  author    = {Yueqi Wang and Tangyou Liu and Licheng Feng and Jinze Wang and Yang Yang and Jianjun Bao and Binghao Li and Liao Wu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802693},
  month     = {10},
  pages     = {4600-4607},
  title     = {Hardware-based time synchronization for a multi-sensor system},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing kubernetes deployment of robotic applications
with HEFT-based container orchestration. <em>IROS</em>, 4594–4599. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study addresses the challenge of deploying robotic software with Quality of Service (QoS) constraints in Edge-Cloud computing clusters. The paper introduces HEFT4K, an event-driven scheduling method tailored for Kubernetes-managed systems based on the Heterogeneous Early Finish Time (HEFT) algorithm. This algorithm reduces software execution time (makespan) and facilitates re-mapping in case of node failures, involving only essential containers to maintain uninterrupted robot functionality. Experimental results, conducted on a real-world robot and synthetic benchmarks, show a 75% speedup in makespan compared to the standard Kubernetes scheduler, enhancing the efficiency of QoS-focused scheduling for robotic applications in distributed systems.},
  archive   = {C_IROS},
  author    = {Francesco Lumpp and Franco Fummi and Nicola Bombieri},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802072},
  month     = {10},
  pages     = {4594-4599},
  title     = {Optimizing kubernetes deployment of robotic applications with HEFT-based container orchestration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NeRF-enabled analysis-through-synthesis for ISAR imaging of
small everyday objects with sparse and noisy UWB radar data.
<em>IROS</em>, 4586–4593. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inverse Synthetic Aperture Radar (ISAR) imaging presents a formidable challenge when it comes to small everyday objects due to their limited Radar Cross-Section (RCS) and the inherent resolution constraints of radar systems. Existing ISAR reconstruction methods including backprojection (BP) often require complex setups and controlled environments, rendering them impractical for many real-world noisy scenarios. In this paper, we propose a novel Analysis-through-Synthesis (ATS) framework enabled by Neural Radiance Fields (NeRF) for high-resolution coherent ISAR imaging of small objects using sparse and noisy Ultra-Wideband (UWB) radar data with an inexpensive and portable setup. Our end-to-end framework integrates ultra-wideband radar wave propagation, reflection characteristics, and scene priors, enabling efficient 2D scene reconstruction without the need for costly anechoic chambers or complex measurement test beds. With qualitative and quantitative comparisons, we demonstrate that the proposed method outperforms traditional techniques and generates ISAR images of complex scenes with multiple targets and complex structures in Non-Line-of-Sight (NLOS) and noisy scenarios, particularly with limited number of views and sparse UWB radar scans. This work represents a significant step towards practical, cost-effective ISAR imaging of small everyday objects, with broad implications for robotics and mobile sensing applications.},
  archive   = {C_IROS},
  author    = {Md Farhan Tasnim Oshim and Albert Reed and Suren Jayasuriya and Tauhidur Rahman},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801921},
  month     = {10},
  pages     = {4586-4593},
  title     = {NeRF-enabled analysis-through-synthesis for ISAR imaging of small everyday objects with sparse and noisy UWB radar data},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ROS-lite2: Autonomous-driving software platform for
clustered many-core processor. <em>IROS</em>, 4578–4585. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In intelligent robotics, which spans from assistive devices to automation, the development of autonomous systems merges computational and physical capabilities, such as in autonomous wheelchairs. Many-core processor, essential for real-time operations, pose challenges to software adaptability. This paper introduces ROS-lite2, which is a software platform for autonomous vehicles, utilizing a ROS 2 framework based on many-core processor to boost flexibility and simplify deployment. Our method facilitates complex function integration and intuitive operation with less hardware knowledge. Experiments with an autonomous wheelchair demonstrate the platform’s effectiveness in improving autonomy and reducing development effort, advancing robotic assistance.},
  archive   = {C_IROS},
  author    = {Yuta Tajima and Shuhei Tsunoda and Takuya Azumi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802569},
  month     = {10},
  pages     = {4578-4585},
  title     = {ROS-lite2: Autonomous-driving software platform for clustered many-core processor},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robot design optimization with rotational and prismatic
joints using black-box multi-objective optimization. <em>IROS</em>,
4571–4577. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots generally have a structure that combines rotational joints and links in a serial fashion. On the other hand, various joint mechanisms are being utilized in practice, such as prismatic joints, closed links, and wire-driven systems. Previous research have focused on individual mechanisms, proposing methods to design robots capable of achieving given tasks by optimizing the length of links and the arrangement of the joints. In this study, we propose a method for the design optimization of robots that combine different types of joints, specifically rotational and prismatic joints. The objective is to automatically generate a robot that minimizes the number of joints and link lengths while accomplishing a desired task, by utilizing a black-box multi-objective optimization approach. This enables the simultaneous observation of a diverse range of body designs through the obtained Pareto solutions. Our findings confirm the emergence of practical and known combinations of rotational and prismatic joints, as well as the discovery of novel joint combinations.},
  archive   = {C_IROS},
  author    = {Kento Kawaharazuka and Kei Okada and Masayuki Inaba},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802642},
  month     = {10},
  pages     = {4571-4577},
  title     = {Robot design optimization with rotational and prismatic joints using black-box multi-objective optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Task-driven computational framework for simultaneously
optimizing design and mounted pose of modular reconfigurable
manipulators. <em>IROS</em>, 4563–4570. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modular reconfigurable manipulators enable quick adaptation and versatility to address different application environments and tailor to the specific requirements of the tasks. Task performance significantly depends on the manipulator’s mounted pose and morphology design, therefore posing the need of methodologies for selecting suitable modular robot configurations and mounted pose that can address the specific task requirements and required performance. Morphological changes in modular robots can be derived through a discrete optimization process that involves the selective addition or removal of modules. In contrast, the adjustment of the mounted pose operates within a continuous space, allowing for smooth and precise alterations in both orientation and position. This work introduces a computational framework that simultaneously optimizes the pose and morphology mounted on modular manipulators. The core of the work is that we design a mapping function that implicitly captures the morphological state of manipulators in the continuous space. This transformation function unifies the optimization of mounted pose and morphology within a continuous space. Furthermore, our optimization framework incorporates a array of performance metrics, such as minimum joint effort and maximum manipulability, and considerations for trajectory execution error and physical and collision constraints. To highlight our method’s benefits, we compare it with previous methods that framed such problems as a combinatorial optimization problem and demonstrate its practicality in selecting the modular robot configuration for executing a drilling task with the CONCERT modular robotic platform.},
  archive   = {C_IROS},
  author    = {Maolin Lei and Edoardo Romiti and Arturo Laurenzi and Nikos G. Tsagarakis},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802089},
  month     = {10},
  pages     = {4563-4570},
  title     = {Task-driven computational framework for simultaneously optimizing design and mounted pose of modular reconfigurable manipulators},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flight structure optimization of modular reconfigurable
UAVs. <em>IROS</em>, 4556–4562. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a Genetic Algorithm (GA) designed to reconfigure a large group of modular Unmanned Aerial Vehicles (UAVs), each with different weights and inertia parameters, into an over-actuated flight structure with improved dynamic properties. Previous research efforts either utilized expert knowledge to design flight structures for a specific task or relied on enumeration-based algorithms that required extensive computation to find an optimal one. However, both approaches encounter challenges in accommodating the heterogeneity among modules. Our GA addresses these challenges by incorporating the complexities of over-actuation and dynamic properties into its formulation. Additionally, we employ a tree representation and a vector representation to describe flight structures, facilitating efficient crossover operations and fitness evaluations within the GA framework, respectively. Using cubic modular quadcopters capable of functioning as omnidirectional thrust generators, we validate that the proposed approach can (i) adeptly identify suboptimal configurations ensuring both over-actuation and trajectory tracking accuracy and (ii) significantly reduce computational costs compared to traditional enumeration-based methods.},
  archive   = {C_IROS},
  author    = {Yao Su and Ziyuan Jiao and Zeyu Zhang and Jingwen Zhang and Hang Li and Meng Wang and Hangxin Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801469},
  month     = {10},
  pages     = {4556-4562},
  title     = {Flight structure optimization of modular reconfigurable UAVs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Satellite-model-free deep learning based pose estimation of
non-cooperative satellite and tracking using navigation filter.
<em>IROS</em>, 4548–4555. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One core component of Active Debris Removal (ADR) and On-Orbit Servicing (OOS) missions in space is to estimate and track the relative pose of a non-cooperative satellite in close proximity. Conventionally, Image Processing methods have been popular in pose estimation by employing manual feature extraction techniques. But the performance of such methods plateaus in the challenging illumination conditions and sensor capability constraints in space, because of which Deep Learning (DL)-based approaches have gained traction. This paper aims to provide an improvement over the existing state-of-the-art direct pose estimation methods from a monocular camera, without relying on any 3D model of the target satellite. The main contribution of this work is to develop a general purpose satellite-invariant pose estimation architecture with improved accuracy and implement an adaptive navigation filter over it to track the pose continuously over a stream of images. The pose estimation module includes a modified DenseNet architecture. In order to test the generalization capability, the proposed pose estimation module is tested on the SPEED, SPEED+, SHIRT and URSO datasets and compared with other existing methods. The advantage of the proposed method is that the same model architecture is able to give accurate pose estimation results for different satellite datasets. To perform continuous tracking of the relative pose, an adaptive EKF (Extended Kalman Filter) is implemented on the initial pose estimates. For performance evaluation of the navigation filter, the accuracy goals required for the relative navigation of Hubble Space Telescope SM4 mission are considered while testing on the SHIRT dataset.},
  archive   = {C_IROS},
  author    = {Shubham Shukla and Raunak Srivastava and Rolif Lima and Titas Bera},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801805},
  month     = {10},
  pages     = {4548-4555},
  title     = {Satellite-model-free deep learning based pose estimation of non-cooperative satellite and tracking using navigation filter},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stability of tethered ground robots on extreme terrains.
<em>IROS</em>, 4542–4547. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the absence of a tether attachment mechanism that can provide infinitely large tension to the tethered robots moving on extreme planetary terrains, there is a limit on how much tension can be realistically generated or supported by the tether. In this paper, we consider a team of two robots tethered together moving on extreme terrains. The traction on the wheels of the robot and the friction between the tether and the tether attachment surfaces/objects (e.g., rocks) is the only way to support the tether tension. Given a path for the robots to navigate, we provide a systematic algorithm to check if the robots will be stable along the given path while considering the maximum constraints on the tension generated or supported by the tether. The results are validated via simulation experiments.},
  archive   = {C_IROS},
  author    = {Rahul Kumar and Vishnu S. Chipade and Sze Zheng Yong},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801748},
  month     = {10},
  pages     = {4542-4547},
  title     = {Stability of tethered ground robots on extreme terrains},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Test-time certifiable self-supervision to bridge the
Sim2Real gap in event-based satellite pose estimation. <em>IROS</em>,
4534–4541. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep learning plays a critical role in vision-based satellite pose estimation. However, the scarcity of real data from the space environment means that deep models need to be trained using synthetic data, which raises the Sim2Real domain gap problem. A major cause of the Sim2Real gap are novel lighting conditions encountered during test time. Event sensors have been shown to provide some robustness against lighting variations in vision-based pose estimation. However, challenging lighting conditions due to strong directional light can still cause undesirable effects in the output of commercial off-the-shelf event sensors, such as noisy/spurious events and inhomogeneous event densities on the object. Such effects are non-trivial to simulate in software, thus leading to Sim2Real gap in the event domain. To close the Sim2Real gap in event-based satellite pose estimation, the paper proposes a test-time self-supervision scheme with a certifier module. Self-supervision is enabled by an optimisation routine that aligns a dense point cloud of the predicted satellite pose with the event data to attempt to rectify the inaccurately estimated pose. The certifier attempts to verify the corrected pose, and only certified test-time inputs are backpropagated via implicit differentiation to refine the predicted landmarks, thus improving the pose estimates and closing the Sim2Real gap. Results show that the our method outperforms established test-time adaptation schemes.},
  archive   = {C_IROS},
  author    = {Mohsi Jawaid and Rajat Talak and Yasir Latif and Luca Carlone and Tat-Jun Chin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802404},
  month     = {10},
  pages     = {4534-4541},
  title     = {Test-time certifiable self-supervision to bridge the Sim2Real gap in event-based satellite pose estimation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Development of a peristaltic flexible transfer system for
transporting feces under microgravity: Construction and validation of
transport models. <em>IROS</em>, 4527–4533. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this study, we propose a peristaltic flexible transfer system for transferring feces in a free piping route. Currently, human feces is incinerated and disposed in space, such as on the International Space Station. As feces contain a large amount of reusable organic matter and water, the ability to recycle feces will improve the performance of manned space technology. However, existing space toilets are not designed to reuse feces, and the transportation of feces from the toilet bowl to the collection area is a technical challenge. A method for transporting intermittent supplies such as feces with less energy and water consumption is required. In a previous study, a peristaltic transfer system was developed based on the peristalsis of the intestinal tract of living organisms. In this method, multiple pump units driven by low air pressure generate peristaltic motion, which enables horizontal, vertical, and curved transfer of simulated feces. However, because the frame of each unit is rigid, the transport path cannot be changed flexibly, and the design must be adapted to the installation location. Therefore, the frame must be flexible.We propose a peristaltic flexible transfer system for transferring feces in a free piping route. First, we construct a simple model of content transfer using the peristaltic flexible transfer system and calculate the transfer rate based on data obtained from basic characteristic experiments of a single unit. Then, we conduct an actual content transfer experiment using the transfer system and compare the results with the simplified model results.},
  archive   = {C_IROS},
  author    = {M. Kawano and S. Uzawa and C. Yamazaki and T. Nakamura},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802587},
  month     = {10},
  pages     = {4527-4533},
  title     = {Development of a peristaltic flexible transfer system for transporting feces under microgravity: Construction and validation of transport models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GNC design and orbital performance evaluation of ISS onboard
autonomous free-flying robot int-ball2. <em>IROS</em>, 4519–4526. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The International Space Station (ISS) crew has to complete various tasks in a limited time. The Int-Ball project is one of the activities aimed at the efficient utilization of crew time. Int-Ball2 is an autonomous mobile robot that conducts imaging operations in place of crew members by remote control from the ground. This paper first introduces the design of the guidance, navigation and control (GNC) and propulsion systems of Int-Ball2, offering autonomous flight inside the ISS. This robot has unique features compared to other free-flying robots in space, including its small size for use in crowded environments and its high thrust despite its size. In addition, the navigation system uses a visual SLAM algorithm, which does not rely on external markers. The results of orbital performance verification tests for 6-DOF translational and rotational motion are also presented in this paper. Our analysis indicated that the proposed GNC algorithm provided sufficient performance to conduct the required photography. Finally, the results of ground tests simulating the microgravity environment are compared with the results of orbital experiments to evaluate the differences in the robot’s mobility performance in these tests. The results confirmed that the ground verification method was valid for achieving stable on-orbit mobility.},
  archive   = {C_IROS},
  author    = {Taisei Nishishita and Keisuke Watanabe and Daichi Hirano and Shinji Mitani},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802183},
  month     = {10},
  pages     = {4519-4526},
  title     = {GNC design and orbital performance evaluation of ISS onboard autonomous free-flying robot int-ball2},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GDM-net: Gas distribution mapping with a mobile robot using
deep reinforcement learning and gaussian process regression.
<em>IROS</em>, 4511–4518. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In a gas distribution mapping (GDM) task, the objective of a mobile robot is to map the gas concentrations of an airborne chemical over a region of interest using onboard sensing. Given the limited battery budget available to the robot, covering the entire area to measure gas concentrations at every location might be infeasible. Assuming that the robot only has a budget for b meters of travel, in the rest of the locations, gas concentrations can be inferred using a supervised machine learning technique, namely the Gaussian Process (GP). In this paper, we propose a novel technique that combines deep reinforcement learning and GP regression to find an effective policy for GDM. We have implemented the proposed technique in Python within a 16×16 4-connected plane. We have used six types of Gaussian plumes to validate our presented approach. Compared to two popular baselines, our approach outperforms greedy and random exploration by 62% and 151% in terms of earned rewards, while outperforming them by 47% and 345%, respectively, in terms of the precision of gas distribution modeling in all test cases without obstacles. Our approach also improves the coverage of the exploration while consequently reducing the uncertainty in the prediction.},
  archive   = {C_IROS},
  author    = {Iliya Kulbaka and Ayan Dutta and O. Patrick Kreidl and Ladislau Bölöni and Swapnoneel Roy},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802502},
  month     = {10},
  pages     = {4511-4518},
  title     = {GDM-net: Gas distribution mapping with a mobile robot using deep reinforcement learning and gaussian process regression},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cooperative modular manipulation with numerous cable-driven
robots for assistive construction and gap crossing. <em>IROS</em>,
4503–4510. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soldiers in the field often need to cross negative obstacles, such as rivers or canyons, to reach goals or safety. Military gap crossing involves on-site temporary bridges construction. However, this procedure is conducted with dangerous, time and labor intensive operations, and specialized machinery. We envision a scalable robotic solution inspired by advancements in force-controlled and Cable-Driven Parallel Robots (CDPRs); this solution can address the challenges inherent in this transportation problem, achieving fast, efficient, and safe deployment and field operations. We introduce the embodied vision in Co3MaNDR, a solution to the military gap crossing problem, a distributed robot consisting of several modules simultaneously pulling on a central payload, controlling the cables’ tensions to achieve complex objectives, such as precise trajectory tracking or force amplification. Hardware experiments demonstrate teleoperation of a payload, trajectory following, and the sensing and amplification of operators’ applied physical forces during slow operations. An operator was shown to manipulate a 27.2 kg (60 lb) payload with an average force utilization of 14.5% of its weight. Results indicate that the system can be scaled up to heavier payloads without compromising performance or introducing superfluous complexity. This research lays a foundation to expand CDPR technology to uncoordinated and unstable mobile platforms in unknown environments.},
  archive   = {C_IROS},
  author    = {Kevin Murphy and Joao C.V. Soares and Justin K. Yim and Dustin Nottage and Ahmet Soylemezoglu and Joao Ramos},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801616},
  month     = {10},
  pages     = {4503-4510},
  title     = {Cooperative modular manipulation with numerous cable-driven robots for assistive construction and gap crossing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LTL-d*: Incrementally optimal replanning for feasible and
infeasible tasks in linear temporal logic specifications. <em>IROS</em>,
4495–4502. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an incremental replanning algorithm, dubbed LTL-D*, for temporal-logic-based task planning in a dynamically changing environment. Unexpected changes in the environment may lead to failures in satisfying a task specification in the form of a Linear Temporal Logic (LTL). In this study, the considered failures are categorized into two classes: (i) the desired LTL specification can be satisfied via replanning, and (ii) the desired LTL specification is infeasible to meet strictly and can only be satisfied in a &quot;relaxed&quot; fashion. To address these failures, the proposed algorithm finds an optimal replanning solution that minimally violates desired task specifications. In particular, our approach leverages the D* Lite algorithm and employs a distance metric within the synthesized automaton to quantify the degree of the task violation and then replan incrementally. This ensures plan optimality and reduces planning time, especially when frequent replanning is required. Our approach is implemented in a robot navigation simulation to demonstrate a significant improvement in the computational efficiency for replanning by two orders of magnitude.},
  archive   = {C_IROS},
  author    = {Jiming Ren and Haris Miller and Karen M. Feigh and Samuel Coogan and Ye Zhao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802218},
  month     = {10},
  pages     = {4495-4502},
  title     = {LTL-d*: Incrementally optimal replanning for feasible and infeasible tasks in linear temporal logic specifications},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploratory motion guided tactile learning for
shape-consistent robotic insertion. <em>IROS</em>, 4487–4494. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Intelligent robots are expected to do manipulation tasks relying on real-time sensing feedback. Especially, tactile sensing plays a more and more important role in precise manipulation tasks. For example, a 1 mm error while inserting a USB stick, which is hard to perceive visually, will result in a failed insertion or even break the USB stick. In this paper, to estimate and compensate residual position uncertainties during robotic insertion tasks, an exploration motion is introduced to acquire environment information by tactile sensing and a state-of-the-art transformer-based neural network is proposed to estimate the error distance from long-duration tactile sensing data. Our system is trained on over 2000 insertion trials with basic geometry shaped 3D printed objects. Without any prior knowledge, we achieve an 85% insertion success rate with average 5 attempts on 4 unseen daily objects relying only on tactile feedback acquired from our proposed exploratory motion. It is noteworthy that our designed exploration motion can provide insightful information about extrinsic contact information and our proposed learning model exceeds previous baselines in extracting useful information regarding the contact interaction between the grasped object and the environment.},
  archive   = {C_IROS},
  author    = {Gang Yan and Jinsong He and Satoshi Funabashi and Alexander Schmitz and Shigeki Sugano},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801550},
  month     = {10},
  pages     = {4487-4494},
  title     = {Exploratory motion guided tactile learning for shape-consistent robotic insertion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SPDAGG-TransNet: Integrating symmetric positive definite
networks with transformers for UAV-human action recognition*.
<em>IROS</em>, 4479–4486. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The advent of unmanned aerial vehicles (UAVs) has initiated a revolutionary era in human action recognition, profoundly influencing various domains. This transition underscores the critical necessity for comprehensive benchmarks crucial for formulating and evaluating UAV-centric models tailored to human behavior analysis.This paper presents an novel approach called SPDAGG-TransNet network for UAV-human action recognition, lever-aging the resilience of skeletal-based features amidst these obstacles. Our approach revolves around a deep neural network adept at capturing the intricate spatial and temporal dimensions of human actions, leading to the development of Semi-Positive Definite (SPD) matrix representations. These representations are then transformed using a transformer encoder before being classified using a Multilayer Perceptron (MLP). To assess the effectiveness of our approach, we conduct thorough evaluations using publicly available datasets such as the UAV-Human Action Recognition and UAV-Gesture datasets. Our findings underscore the state-of-the-art performance achieved by our method, highlighting its potential to significantly advance UAV-based human action recognition.},
  archive   = {C_IROS},
  author    = {Mohamed Sanim Akremi and Najett Neji and Hedi Tabia},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802016},
  month     = {10},
  pages     = {4479-4486},
  title     = {SPDAGG-TransNet: Integrating symmetric positive definite networks with transformers for UAV-human action recognition*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BaRiFlex: A robotic gripper with versatility and collision
robustness for robot learning. <em>IROS</em>, 4106–4113. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a new approach to robot hand design specifically suited to enable robot learning methods and daily tasks in human environments. We introduce BaRiFlex, an innovative gripper design that alleviates the issues caused by unexpected contact and collisions during robot learning, offering collision mitigation, grasping versatility, task versatility, and simplicity to the learning processes. This achievement is enabled by the incorporation of low-inertia actuators, providing high Back-drivability, and the strategic combination of Rigid and Flexible materials which enhances versatility and the gripper’s resilience against unpredicted collisions. Furthermore, the integration of flexible Fin-Ray and rigid linkages allows the gripper to execute compliant grasping and precise pinching. We conducted rigorous performance tests to characterize the novel gripper’s compliance, durability, grasping and task versatility, and precision. We also integrated the BaRiFlex with a 7 Degree of Freedom (DoF) Franka Emika’s Panda robotic arm to evaluate its capacity to support a trial-and-error (reinforcement learning) training procedure. The results of our experimental study are then compared to those obtained using the original rigid Franka Hand and a reference Fin-Ray soft gripper, demonstrating the superior capabilities and advantages of our developed gripper system. More information and videos at https://robin-lab.cs.utexas.edu/bariflex},
  archive   = {C_IROS},
  author    = {Gu-Cheol Jeong and Arpit Bahety and Gabriel Pedraza and Ashish D. Deshpande and Roberto Martín-Martín},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802024},
  month     = {10},
  pages     = {4106-4113},
  title     = {BaRiFlex: A robotic gripper with versatility and collision robustness for robot learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Development of a compact robust passive transformable
omni-ball for enhanced step-climbing and vibration reduction.
<em>IROS</em>, 4098–4105. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces the Passive Transformable Omni-Ball (PTOB), an advanced omnidirectional wheel engineered to enhance step-climbing performance, incorporate built-in actuators, diminish vibrations, and fortify structural integrity. By modifying the omni-ball’s structure from two to three segments, we have achieved improved in-wheel actuation and a reduction in vibrational feedback. Additionally, we have implemented a sliding mechanism in the follower wheels to boost the wheel’s step-climbing abilities. A prototype with a 127 mm diameter PTOB was constructed, which confirmed its functionality for omnidirectional movement and internal actuation. Compared to a traditional omni-wheel, the PTOB demonstrated a comparable level of vibration while offering superior capabilities. Extensive testing in varied settings showed that the PTOB can adeptly handle step obstacles up to 45 mm, equivalent to 35 % of the wheel’s diameter, in both the forward and lateral directions. The PTOB showcased robust construction and proved to be versatile in navigating through environments with diverse obstacles.},
  archive   = {C_IROS},
  author    = {Kazuo Hongo and Takashi Kito and Yasuhisa Kamikawa and Masaya Kinoshita and Yasunori Kawanami},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801929},
  month     = {10},
  pages     = {4098-4105},
  title     = {Development of a compact robust passive transformable omni-ball for enhanced step-climbing and vibration reduction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multistable soft actuator for physical human-robot
interaction. <em>IROS</em>, 4090–4097. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collaboration with robots through physical contact offers a more intuitive, natural, and engaging operational experience, showcasing vast potential in the field of human-robot interaction. However, current physical interaction devices, such as collaborative robots and haptic feedback mechanisms, are limited by their singular modes of motion and feedback, hindering enhancements in interaction experiences. Herein, we present a multistable soft actuator capable of driving multimodal shape changes and passively conforming to user touch. This actuator can memorize and maintains any deformation with zero power consumption. Its structural mechanical properties can be dynamically adjusted to produce rich haptic feedback for the user, including changes in shape, elasticity, stiffness, and even sensations of rupture and weightlessness. Structurally, the mechanism consists of a network of pneumatic bistable units in series and parallel configurations, which can switch states under air pressure or external force, achieving extension, contraction, and omnidirectional bending. The input of air pressure can either impede or assist deformation, altering structural stiffness and resulting in varied loading curves. With its high safety in physical interactions, robust operability, and rich mechanical tactile feedback, the multistable soft actuator promises new design directions for physical human-robot interaction devices.},
  archive   = {C_IROS},
  author    = {Juncai Long and Jituo Li and Xiaojie Diao and Chengdi Zhou and Guodong Lu and Yixiong Feng},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801795},
  month     = {10},
  pages     = {4090-4097},
  title     = {Multistable soft actuator for physical human-robot interaction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel vitreoretinal surgical robot system to maximize the
internal reachable workspace and minimize the external link motion*.
<em>IROS</em>, 4084–4089. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel robotic system designed for efficient minimally invasive eye surgery. The proposed prototype integrates a concentric tube mechanism(CTM) and a belt-driven remote center of motion(RCM) mechanism, aiming to maximize the internal reachable workspace while minimizing external robot movements. The integrated system provides several advantages, including preventing collisions between surgical tools and the lens, minimizing sclera stress, and having efficient robot motion inside and outside the eyeball. It provides sufficient link motions with roll and pitch angles of ±32° and ±85° respectively at the RCM point, allowing access to 89% of the retina. The experiment evaluates the system&#39;s performance, with the RCM point accuracy at 0.718mm, CTM position accuracy at 207 μm, and a repeatability error of 246 μm. To reduce hysteresis errors at the RCM point caused by the belt, a lever-based belt tensioner is used for initial calibration while an optical tracking system tracks each joint’s movement. Targeting experiments highlight that the wider workspace was achieved by the CTM+RCM system compared to the traditional RCM mechanism with a straight tool. The results showed the system&#39;s compactness, efficiency, and dexterity, confirming its feasibility and potential for the proposed eye surgery robot.},
  archive   = {C_IROS},
  author    = {Gowoon Jeong and Seong Yeong Ko},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801874},
  month     = {10},
  pages     = {4084-4089},
  title     = {A novel vitreoretinal surgical robot system to maximize the internal reachable workspace and minimize the external link motion*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Beyond feasibility: Efficiently planning robotic assembly
sequences that minimize assembly path lengths. <em>IROS</em>, 4076–4083.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Advancements in Industry 4.0 demand sophisticated solutions for automatic robotic assembly sequence planning (RASP), capable of handling the diversity and complexity of modern manufacturing tasks. One approach to RASP is Assembly-by-Disassembly (AbD). It first searches for a disassembly sequence that is then inverted to obtain an assembly sequence. One of the challenges of AbD, however, is the exponential number of potential assembly sequences for any given assembly. To mitigate this challenge, we propose to transfer knowledge obtained during previous planning attempts. Specifically, we present an approach that combines Monte Carlo Tree Search (MCTS) with deep Q-learning to optimize the total length of robotic assembly paths. We use a graph-based representation of disassembly states in combination with a graph neural network to learn the Q-function. We further discuss a principled approach to generate 3D assemblies out of aluminium profiles that a single robot manipulator can assemble. With this approach, we generated two datasets consisting of 14 assemblies with 21 removable parts and 7 assemblies with 30 removable parts. Using leave-one-out cross-validation, we were able to demonstrate how our approach outperformed an unmodified MCTS. Moreover, we successfully transferred knowledge between datasets.},
  archive   = {C_IROS},
  author    = {Alexander Cebulla and Tamim Asfour and Torsten Kröger},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801475},
  month     = {10},
  pages     = {4076-4083},
  title     = {Beyond feasibility: Efficiently planning robotic assembly sequences that minimize assembly path lengths},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Harnessing with twisting: Single-arm deformable linear
object manipulation for industrial harnessing task. <em>IROS</em>,
4069–4075. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Wire-harnessing tasks pose great challenges to be automated by the robot due to the complex dynamics and unpredictable behavior of the deformable wire. Traditional methods, often reliant on dual-robot arms or tactile sensing, face limitations in adaptability, cost, and scalability. This paper introduces a novel single-robot wire-harnessing pipeline that leverages a robot’s twisting motion to generate necessary wire tension for precise insertion into clamps, using only one robot arm with an integrated force/torque (F/T) sensor. Benefiting from this design, the single robot arm can efficiently apply tension for wire routing and insertion into clamps in a narrow space. Our approach is structured around four principal components: a Model Predictive Control (MPC) based on the Koopman operator for tension tracking and wire following, a motion planner for sequencing harnessing waypoints, a suite of insertion primitives for clamp engagement, and a fix-point switching mechanism for wire constraint updating. Evaluated on an industrial-level wire harnessing task, our method demonstrated superior performance and reliability over conventional approaches, efficiently handling both single and multiple wire configurations with high success rates.},
  archive   = {C_IROS},
  author    = {Xiang Zhang and Hsien-Chung Lin and Yu Zhao and Masayoshi Tomizuka},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802801},
  month     = {10},
  pages     = {4069-4075},
  title     = {Harnessing with twisting: Single-arm deformable linear object manipulation for industrial harnessing task},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). A robust and efficient robotic packing pipeline with
dissipativity- based adaptive impedance-force control. <em>IROS</em>,
4061–4068. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For humans, dense bin packing heavily relies on force perception. However, current robotic packing studies only focus on the visual input or adopt auxiliary push-to-place actions to eliminate gaps, suffering from high time expenditure and poor robustness. To address such limitations, we first introduce a novel external force estimation method based on the generalized momentum observer, which can avoid the influence of joint acceleration noises and achieve real-time high-precision monitoring. Second, to obtain compliant interaction and fine robustness, an adaptive variable impedance policy is developed to track dynamic motion and desired force, and compensate for uncertainties. Meanwhile, we perform dissipativity analysis and a virtual energy supply function is augmented to the system for optimization, providing a solid foundation for stability. Third, we propose an efficient packing methodology with three sub- tasks by considering the distinct interaction and constraint states in different areas. Our packing strategies eliminate the need for subsequent auxiliary actions and are proven to enhance efficiency. We perform quantitative evaluations to verify our external force estimation method, conduct comparison studies with current packing methods, and investigate the contribution of our dissipativity-based adaptive controller. The superior results not only prove the robustness and efficiency of our pipeline, but also pave the way for practical applications of packing.},
  archive   = {C_IROS},
  author    = {Zhenning Zhou and Lei Zhou and Shengxin Sun and Marcelo H Ang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802392},
  month     = {10},
  pages     = {4061-4068},
  title     = {A robust and efficient robotic packing pipeline with dissipativity- based adaptive impedance-force control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Peristaltic soft robot for long-distance pipe inspection
with an endoskeletal structure for propulsion and traction
amplification. <em>IROS</em>, 4053–4060. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study proposed a peristaltic motion-type inspection robot equipped with a &quot;linear antagonistic mechanism using artificial muscles with an endoskeletal structure&quot; to amplify propulsion and traction. We sought to develop an in-pipe inspection robot for long, narrow, and complex pipes requiring large propulsion, traction, and flexibility. In a previous study, we proposed a linear antagonistic mechanism allowing the inspection robot to generate both high propulsion and traction along with flexibility in narrow pipes. The proposed mechanism consisted of two extension actuators and a gripping actuator sandwiched between these extension actuators. The large extension force by the extension actuators is distributed to both propulsion and traction. However, owing to the piston-shaped configuration of the extension actuators, the generated force decreased in a manner dependent on the cross-sectional area within narrow pipelines. Therefore, the in-pipe inspection robot took time to move in long-distance, small-diameter pipes with multiple bends. This paper describes a &quot;linear antagonistic mechanism using artificial muscles with an endoskeletal structure&quot; that amplifies propulsion and traction by inserting a tension spring (skeleton) inside the contraction actuators (artificial muscles) and utilizing the action force generated by the actuator and transmitted by the tension spring. In this study, the developed robot with an endoskeleton exhibited maximum propulsion of 60.2 N, surpassing its non-endoskeleton counterpart by a factor of 1.61. Furthermore, the robot equipped with the endoskeleton passed through an elbow pipe 1.29 times faster than that without the endoskeleton, reducing the time from 741 to 576 s. The function value that compares the propulsion and traction considering the effects of the applied pressure and pipe diameter required for long-distance inspection was more than 1.13 times that of the previous study. In addition, the non-dimensionalized traction was 1.55 times greater than that of any other pipe inspection robot, and the propulsion was large enough to pass through a bending pipe. This result indicates the feasibility of the developed robot for inspecting long, narrow, and complex pipes.},
  archive   = {C_IROS},
  author    = {R. Okuma and Y. Naruse and F. Ito and T. Nakamura},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801920},
  month     = {10},
  pages     = {4053-4060},
  title     = {Peristaltic soft robot for long-distance pipe inspection with an endoskeletal structure for propulsion and traction amplification},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel variable stiffness suspension system for improved
stability and control of tactile mobile manipulators. <em>IROS</em>,
3682–3689. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile manipulators (MM) have proven valuable in assisting humans in industrial settings. However, their strict separation from humans in controlled environments limits their effectiveness. Efforts have been made to bridge this gap for physical human-robot interaction (pHRI), leading to the development of collaborative mobile manipulators (CMM). Nonetheless, unpredictable environments continue to present challenges. This paper introduces an innovative suspension design for mobile bases (MBs) to enhance the safety and autonomy of CMMs. We propose an electromechanical approach leveraging variable stiffness and combining passive springs with adaptive transmission mechanisms. Through simulation, physical prototype development, and experimental validation, we demonstrate the effectiveness of our approach in stabilizing the MB against external disturbances. Our findings provide valuable insights for the development of CMMs in dynamic environments.},
  archive   = {C_IROS},
  author    = {Sebastian Kuhn and Mehmet C. Yildirim and Edmundo Pozo Fortunić and Kübra Karacan and Abdalla Swikir and Sami Haddadin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802223},
  month     = {10},
  pages     = {3682-3689},
  title     = {A novel variable stiffness suspension system for improved stability and control of tactile mobile manipulators},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MAkEable: Memory-centered and affordance-based task
execution framework for transferable mobile manipulation skills.
<em>IROS</em>, 3674–3681. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To perform versatile mobile manipulation tasks in human-centered environments, the ability to efficiently transfer learned skills, knowledge, and experiences from one robot to another or across different environments is critical. In this paper, we present MAkEable, a versatile uni- and multi-manual mobile manipulation framework that facilitates the transfer of capabilities and knowledge across different tasks, environments, and robots. Our framework integrates an affordance-based task description into the memory-centric cognitive architecture of the ARMAR humanoid robot family, which supports the sharing of experiences and demonstrations for transferring mobile manipulation skills. By representing mobile manipulation actions through affordances, i. e., interaction possibilities of the robot with its environment, we provide a unifying framework for the autonomous uni- and multi-manual manipulation of known and unknown objects in various environments. We demonstrate MAkEable’s applicability in real-world experiments for multiple robots, tasks, and environments. This includes grasping known and unknown objects, object placing, bimanual object grasping, memory-enabled skill transfer in a drawer opening scenario across two different humanoid robots, and a pouring task learned from human demonstration. Code is available through our project page1.},
  archive   = {C_IROS},
  author    = {Christoph Pohl and Fabian Reister and Fabian Peller-Konrad and Tamim Asfour},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801986},
  month     = {10},
  pages     = {3674-3681},
  title     = {MAkEable: Memory-centered and affordance-based task execution framework for transferable mobile manipulation skills},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BaSeNet: A learning-based mobile manipulator base pose
sequence planning for pickup tasks. <em>IROS</em>, 3666–3673. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many applications, a mobile manipulator robot is required to grasp a set of objects distributed in space. This may not be feasible from a single base pose and the robot must plan the sequence of base poses for grasping all objects, minimizing the total navigation and grasping time. This is a Combinatorial Optimization problem that can be solved using exact methods, which provide optimal solutions but are computationally expensive, or approximate methods, which offer computationally efficient but sub-optimal solutions. Recent studies have shown that learning-based methods can solve Combinatorial Optimization problems, providing near-optimal and computationally efficient solutions.In this work, we present BaSeNet - a learning-based approach to plan the sequence of base poses for the robot to grasp all the objects in the scene. We propose a Reinforcement Learning based solution that learns the base poses for grasping individual objects and the sequence in which the objects should be grasped to minimize the total navigation and grasping costs using Layered Learning. As the problem has a varying number of states and actions, we represent states and actions as a graph and use Graph Neural Networks for learning. We show that the proposed method can produce comparable solutions to exact and approximate methods with significantly less computation time. The code and Reinforcement Learning environments will be made available on the project webpage*.},
  archive   = {C_IROS},
  author    = {Lakshadeep Naik and Sinan Kalkan and Sune L. Sørensen and Mikkel B. Kjærgaard and Norbert Krüger},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802615},
  month     = {10},
  pages     = {3666-3673},
  title     = {BaSeNet: A learning-based mobile manipulator base pose sequence planning for pickup tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Harmonic mobile manipulation. <em>IROS</em>, 3658–3665. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advancements in robotics have enabled robots to navigate complex scenes or manipulate diverse objects independently. However, robots are still impotent in many household tasks requiring coordinated behaviors such as opening doors. The factorization of navigation and manipulation, while effective for some tasks, fails in scenarios requiring coordinated actions. To address this challenge, we introduce, HarmonicMM, an end-to-end learning method that optimizes both navigation and manipulation, showing notable improvement over existing techniques in everyday tasks. This approach is validated in simulated and real-world environments and adapts to novel unseen settings without additional tuning. Our contributions include a new benchmark for mobile manipulation and the successful deployment with only RGB visual observation in a real unseen apartment, demonstrating the potential for practical indoor robot deployment in daily life.},
  archive   = {C_IROS},
  author    = {Ruihan Yang and Yejin Kim and Rose Hendrix and Aniruddha Kembhavi and Xiaolong Wang and Kiana Ehsani},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802201},
  month     = {10},
  pages     = {3658-3665},
  title     = {Harmonic mobile manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Embodied AI with two arms: Zero-shot learning, safety and
modularity. <em>IROS</em>, 3651–3657. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an embodied AI system which receives open-ended natural language instructions from a human, and controls two arms to collaboratively accomplish potentially long-horizon tasks over a large workspace. Our system is modular: it deploys state of the art Large Language Models for task planning, Vision-Language models for semantic perception, and Point Cloud transformers for grasping. With semantic and physical safety in mind, these modules are interfaced with a real-time trajectory optimizer and a compliant tracking controller to enable human-robot proximity. We demonstrate performance for the following tasks: bi-arm sorting, bottle opening, and trash disposal tasks. These are done zero-shot where the models used have not been trained with any real world data from this bi-arm robot, scenes or workspace. Composing both learning- and non-learning-based components in a modular fashion with interpretable inputs and outputs allows the user to easily debug points of failures and fragilities. One may also in-place swap modules to improve the robustness of the overall platform, for instance with imitation-learned policies.},
  archive   = {C_IROS},
  author    = {Jake Varley and Sumeet Singh and Deepali Jain and Krzysztof Choromanski and Andy Zeng and Somnath Basu Roy Chowdhury and Avinava Dubey and Vikas Sindhwani},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802181},
  month     = {10},
  pages     = {3651-3657},
  title     = {Embodied AI with two arms: Zero-shot learning, safety and modularity},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learned regions of attraction for safe motion primitive
transitions. <em>IROS</em>, 3643–3650. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Estimating regions of attraction (ROAs) of dynamical systems is critical for understanding the operational bounds within which a system will converge to a desired state. In this paper, we introduce a neural network-based approach to approximating ROAs that leverages labeled data generated by offline sampling and simulation of initial conditions, with labels determined by flow membership in an &quot;explicit region of attraction.&quot; This framework is designed to estimate ROAs with a level of precision suitable for integration into a motion primitive transition framework as conditions to switch between candidate primitive behaviors. To account for gaps between the simulated environment and the real world, online learning is employed; this refines the offline-learned model of the ROA based on observed discrepancies between predicted and actual system behaviors. We validate this methodology on a quadrupedal robot, demonstrating that our ROA estimates can effectively model regions of attraction for a high-dimensional system. We show this for multiple primitive behaviors and in environments different from the training data. The outcomes highlight the usefulness of our method in estimating regions of attraction and informing transition conditions between primitive behaviors.},
  archive   = {C_IROS},
  author    = {Wyatt Ubellacker and Aaron D. Ames},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801671},
  month     = {10},
  pages     = {3643-3650},
  title     = {Learned regions of attraction for safe motion primitive transitions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Jointly learning cost and constraints from demonstrations
for safe trajectory generation. <em>IROS</em>, 3635–3642. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning from Demonstration (LfD) allows robots to mimic human actions. However, these methods do not model constraints crucial to ensure safety of the learned skill. Moreover, even when explicitly modelling constraints, they rely on the assumption of a known cost function, which limits their practical usability for task with unknown cost. In this work we propose a two-step optimization process that allow to estimate cost and constraints by decoupling the learning of cost functions from the identification of unknown constraints within the demonstrated trajectories. Initially, we identify the cost function by isolating the effect of constraints on parts of the demonstrations. Subsequently, a constraint leaning method is used to identify the unknown constraints. Our approach is validated both on simulated trajectories and a real robotic manipulation task. Our experiments show the impact that incorrect cost estimation has on the learned constraints and illustrate how the proposed method is able to infer unknown constraints, such as obstacles, from demonstrated trajectories without any initial knowledge of the cost.},
  archive   = {C_IROS},
  author    = {Shivam Chaubey and Francesco Verdoja and Ville Kyrki},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802533},
  month     = {10},
  pages     = {3635-3642},
  title     = {Jointly learning cost and constraints from demonstrations for safe trajectory generation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automating ROS2 security policies extraction through static
analysis. <em>IROS</em>, 3627–3634. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cybersecurity in mission-critical robotic applications is a necessity to scale deployments securely. ROS2 builds upon DDS-Security specs in ROS Client Library (RCL) to implement its security features. Utilizing SROS2, developers have access to a set of utilities to help set up security in a way RCL can use. Through SROS2, security deployment is eased for developers. However, while access control is handled by DDS and consequently based on the SROS2-generated permission artifacts, the necessary authorization policies are manually generated by developers. This requires an entire system exercise to be sampled via live extraction and, per each node, list all the necessary Topics, Services, and Actions, which is a daunting and laborious process. Developers first have to generate tests. Then, they obtain a ’snapshot’ of the system for each test. Later, these snapshots must be collected and grouped into a policy by a minimum set of rules. All this procedure is quite error-prone. This paper introduces LiSA4ROS2, a tool for automatically extract the ROS2 computational graph via static analysis to derive a minimal correct configuration for ROS2 security policies. Our approach relies on the abstract interpretation theory to statically overapproximate all possible executions to extract a minimal and complete configuration per node. We evaluate our approach with minimal examples covering all the main communication patterns in ROS2 tutorials and all publicly available real-world ROS2 Python systems extracted from GitHub. The results of the minimal examples show that LiSA4ROS2 precisely supports all the main communication patterns. The extensive evaluation underlines that our prototype implementation of the analysis in LiSA4ROS2 is already able to precisely analyze 66% of existing repositories, automatically producing detailed computational graphs and access policies. All the results of the analysis, as well as a Docker artifact to reproduce them, are publicly available.},
  archive   = {C_IROS},
  author    = {Giacomo Zanatta and Gianluca Caiazza and Pietro Ferrara and Luca Negrini and Ruffin White},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802507},
  month     = {10},
  pages     = {3627-3634},
  title     = {Automating ROS2 security policies extraction through static analysis},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and modeling of a thin-walled multi-segment continuum
robotic bronchoscope. <em>IROS</em>, 3620–3626. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cable-driven continuum robots in bronchoscopic procedures hold immense potential to revolutionize the diagnosis and treatment of lung cancer. However, robotic bronchoscopes in current studies are typically large in size and inflexible. Therefore, this article introduces a novel cable-driven continuum robot bronchoscopy system that achieves modular design between the actuation and operation ends. A continuum structure with a dual-segment notched flexible skeleton, featuring a wall thickness of 0.45 mm, has been designed to perform bending movements exceeding 190°. This enhances flexibility and increases the spatial capacity of the working channels. A kinematic model was developed, integrating the actuation force and the mechanical characteristics of the driving cables for error compensation, estimating the correlation between the displacement of the driving cables and the position of the continuum robot’s end-effector. The verification showed that the root mean square error (RMSE) of the end-effector position is 2.57 mm, which accounts for 4.8% of the continuum’s length. A prototype of the robotic bronchoscopy system was created, and its performance and potential applications in bronchoscopic intervention surgeries were validated through vivo pig intervention experiments.},
  archive   = {C_IROS},
  author    = {Gui-Bin Bian and Ming-Yang Zhang and Qiang Ye and Han Ren and Yu-Peng Zhai and Ruichen Ma and Zhen Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801862},
  month     = {10},
  pages     = {3620-3626},
  title     = {Design and modeling of a thin-walled multi-segment continuum robotic bronchoscope},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Safe imitation learning of nonlinear model predictive
control for flexible robots. <em>IROS</em>, 3613–3619. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Flexible robots may overcome some of the industry’s major challenges, such as enabling intrinsically safe human-robot collaboration and achieving a higher payload-to-mass ratio. However, controlling flexible robots is complicated due to their complex dynamics, which include oscillatory behavior and a high-dimensional state space. Nonlinear model predictive control (NMPC) offers an effective means to control such robots, but its significant computational demand often limits its application in real-time scenarios. To enable fast control of flexible robots, we propose a framework for a safe approximation of NMPC using imitation learning and a predictive safety filter. Our framework significantly reduces computation time while incurring a slight loss in performance. Compared to NMPC, our framework shows more than an eightfold improvement in computation time when controlling a three-dimensional flexible robot arm in simulation, all while guaranteeing safety constraints. Notably, our approach out-performs state-of-the-art reinforcement learning methods. The development of fast and safe approximate NMPC holds the potential to accelerate the adoption of flexible robots in industry. The project code is available at: tinyurl.com/anmpc4fr},
  archive   = {C_IROS},
  author    = {Shamil Mamedov and Rudolf Reiter and Seyed Mahdi B. Azad and Ruan Viljoen and Joschka Boedecker and Moritz Diehl and Jan Swevers},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801854},
  month     = {10},
  pages     = {3613-3619},
  title     = {Safe imitation learning of nonlinear model predictive control for flexible robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). DIABLO: A 6-DoF wheeled bipedal robot composed entirely of
direct-drive joints. <em>IROS</em>, 3605–3612. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Wheeled bipedal robots offer the advantages of both wheeled and legged robots, combining the ability to traverse a wide range of terrains and environments with high efficiency. However, the conventional approach in existing wheeled bipedal robots involves motor-driven joints with high-ratio gearboxes. While this approach provides specific benefits, it also presents several challenges, including increased mechanical complexity, efficiency losses, noise, vibrations, and higher maintenance and lubrication requirements.Addressing the aforementioned concerns, we developed a direct-drive wheeled bipedal robot called DIABLO, which eliminates the use of gearboxes entirely. Our robotic system is simplified as a second-order inverted pendulum, and we have designed an LQR-based balance controller to ensure stability. Additionally, we implemented comprehensive motion controller, including yaw, split-angle, height, and roll controllers. Through experiments in both simulations and real-world prototypes, we have demonstrated that our platform achieves satisfactory performance.},
  archive   = {C_IROS},
  author    = {Dingchuan Liu and Fangfang Yang and Xuanhong Liao and Ximin Lyu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801943},
  month     = {10},
  pages     = {3605-3612},
  title     = {DIABLO: A 6-DoF wheeled bipedal robot composed entirely of direct-drive joints},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and control of a novel six-degree-of-freedom hybrid
robotic arm. <em>IROS</em>, 3597–3604. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic arms are key components in fruit-harvesting robots. In agricultural settings, conventional serial or parallel robotic arms often fall short in meeting the demands for a large workspace, rapid movement, enhanced capability of obstacle avoidance and affordability. This study proposes LingXtend, a novel hybrid six-degree-of-freedom (DoF) robotic arm that combines the advantages of parallel and serial mechanisms. Inspired by yoga, we designed two sliders capable of moving independently along a single rail, acting as two feet. These sliders are interconnected with linkages and a meshed-gear set, allowing the parallel mechanism to lower itself and perform a split to pass under obstacles. This unique feature allows the arm to avoid obstacles such as pipes, tables and beams typically found in greenhouses. Integrated with serially mounted joints, the patented hybrid arm is able to maintain the end’s pose even when it moves with a mobile platform, facilitating fruit picking with the optimal pose in dynamic conditions. Moreover, the hybrid arm’s workspace is substantially larger, being almost three times the volume of UR3 serial arms and fourteen times that of the ABB IRB parallel arms. Experiments show that the repeatability errors are 0.017 mm, 0.03 mm and 0.109 mm for the two sliders and the arm’s end, respectively, providing sufficient precision for agricultural robots.},
  archive   = {C_IROS},
  author    = {Yang Chen and Zhonghua Miao and Yuanyue Ge and Sen Lin and Liping Chen and Ya Xiong},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802044},
  month     = {10},
  pages     = {3597-3604},
  title     = {Design and control of a novel six-degree-of-freedom hybrid robotic arm},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhanced omni-ball: Spherical omnidirectional wheel
achieving passive rollers with high load capacity and smoothness through
an offset rotational axis. <em>IROS</em>, 3589–3596. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces an innovation of the Spherical Omnidirectional Wheel, designed to achieve omnidirectional driving motion. In previous models, the supporting shaft was placed at the center of the mechanism. However, achieving both smoothness and high load-capacity in such designs proved challenging. The mechanism proposed in this study features an offset design, enabling outer support for the wheel. A prototype was developed and its basic motion was experimentally validated.},
  archive   = {C_IROS},
  author    = {Kenjiro Tadakuma and Seiji Sakiyama and Eri Takane and Riichiro Tadakuma and Satoshi Tadokoro},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802231},
  month     = {10},
  pages     = {3589-3596},
  title     = {Enhanced omni-ball: Spherical omnidirectional wheel achieving passive rollers with high load capacity and smoothness through an offset rotational axis},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Versatile variable-stiffness scooping end-effector:
Tilting-scooping-transfer mechanism for objects with various properties.
<em>IROS</em>, 3581–3588. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To address the setup and changeover time issues in high-mix, low-volume production systems, we developed an end-effector capable of uniformly scooping, holding, and transporting a wide variety of objects, and demonstrated this system with prototype. Our experiments showed that the prototype was successful in scooping up and transporting a wide variety of objects and could be applied to high-mix low-volume production systems. In addition, the load testing of the spatula and modeling of objects that can be tilted backwards provided insight into further improvements of the scooping performance of this mechanism.},
  archive   = {C_IROS},
  author    = {Yuta Takahashi and Kenjiro Tadakuma and Kazuki Abe and Masahiro Watanabe and Shoya Shimizu and Satoshi Tadokoro},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801873},
  month     = {10},
  pages     = {3581-3588},
  title     = {Versatile variable-stiffness scooping end-effector: Tilting-scooping-transfer mechanism for objects with various properties},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SNU-avatar haptic glove: Novel modularized haptic glove via
trigonometric series elastic actuators. <em>IROS</em>, 3573–3580. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The avatar robot is a robot capable of realistic remote operation. In remote operation, the controllability of the glove is crucial. This glove can manipulate the hand interacting directly with the environment at the remote site. The glove must be able to accurately estimate the hand posture and provide haptic feedback to convey information about the remote environment and enhance operability. Throughout the process, user discomfort should be minimized. To achieve this goal, the research proposes providing force feedback to the fingers using Trigonometric Series Elastic Actuators. Haptic gloves are attached to the Middle Phalanx to facilitate the easy installation of additional add-ons, ensuring users feel securely fixed when attached. Additionally, by proposing an algorithm to estimate the fingertip position without directly attaching it to the fingertip, the haptic glove estimates hand posture and delivers appropriate force as needed. Finally, the system, including the haptic glove, participated in the ANA Avatar XPRIZE competition. The avatar system performed eight missions, which included not only remote manipulation of objects but also social interactions, demonstrating its effectiveness.},
  archive   = {C_IROS},
  author    = {Eunho Sung and Seungbin You and Seongkyeong Moon and Juhyun Kim and Jaeheung Park},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802590},
  month     = {10},
  pages     = {3573-3580},
  title     = {SNU-avatar haptic glove: Novel modularized haptic glove via trigonometric series elastic actuators},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trans-rotor: An active omnidirectional aerial-ground vehicle
with differential gear joint transformation mechanism. <em>IROS</em>,
3565–3572. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Aerial-ground vehicles have shown great potential in various fields due to their superior mobility and outstanding endurance. However, most of morphing aerial-ground vehicles consider little about controllability and traversability in ground mode. We present a novel aerial-ground vehicle called TransRotor. By proposing a differential gear joint, we equip TransRotor with omnidirectional mobility in both air and ground mode. Besides, using a four-wheel-steering model in ground mode provides better traversability and ground flexibility. Moreover, we design mid-mode transformation for Trans-Rotor, which provides smooth and rapid mode switching. In this work, we firstly propose a novel design of an aerial-ground vehicle. Then, we propose a decoupled controller considering the four-wheel-steer model to achieve autonomous navigation of the vehicle. Comprehensive experiments and a benchmark comparison are carried out to validate the outstanding performance of the proposed system, where the system shows ground flexibility and saves energy up to more than 95%.},
  archive   = {C_IROS},
  author    = {Xuankang Wu and Haoxiang Sun and Tong Xiao and Yanzhang Pan and Zheng Fang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801804},
  month     = {10},
  pages     = {3565-3572},
  title     = {Trans-rotor: An active omnidirectional aerial-ground vehicle with differential gear joint transformation mechanism},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parametric synthesis of compliant joints for impact-robust
shaftless leg mechanisms. <em>IROS</em>, 3559–3564. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper describes a novel parametric optimization procedure for three flexure cross hinges (TFCH) integrated into multi-link leg mechanisms with closed-loop kinematics. Despite advantages such as compliance, no need for joint lubrication, light weight and cost-efficiency, such shaftless mechanisms have not been widely used, especially in the field of dynamic locomotion, also because their design is challenging and barely studied. Using a morphological computation approach, we have optimized the TFCH geometry to achieve the desired joint stiffness using frequency analysis, ensuring safe and stable hopping under external perturbations. We combined rigid body dynamics with lumped stiffness model and finite element modeling using the SPACAR toolbox to simulate various designs within our optimization pipeline. To illustrate the efficiency of the resulting designs, we built a prototype and conducted a series of full-scale experiments with ramp jumps whose trajectories were recorded by a motion capture system. The experiments showed that TFCH can be effectively integrated into leg mechanisms, providing benefits such as impact robustness, energy recuperation, and the ability to work in extreme conditions.},
  archive   = {C_IROS},
  author    = {Egor A. Rakshin and Dmitriy V. Ogureckiy and Ivan I. Borisov and Sergey A. Kolyubin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801668},
  month     = {10},
  pages     = {3559-3564},
  title     = {Parametric synthesis of compliant joints for impact-robust shaftless leg mechanisms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Development of a mobile reconfigurable mecanum robot with a
locking device of rollers. <em>IROS</em>, 3553–3558. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents the design and analysis of an omnidirectional reconfigurable wheeled robot capable of switching between omnidirectional and conventional wheeled mode. We have developed a new pneumatic locking mechanism of rollers for the mecanum wheel. In the mecanum mode, the robot can perform holonomic movements, and in the wheeled platform mode, it can overcome inclined surfaces and perform more energy-efficient movements. In addition, the locking device allows the robot to brake faster compared to other mecanum robots. Unlike other works describing the reconfigurable structure of the mecanum wheel, this work offers a new design characterized by the simplicity of the mechanism and does not require the location of active reconfiguration elements inside the wheel itself. The paper describes the design concept and presents the mechanism for locking rollers. The study evaluates the use of the developed robot in various scenarios, including movement on an inclined surface, sudden braking on a plane and an inclined surface, and also analyzes the energy efficiency of the resulting solution for some operating scenarios. The experiments carried out confirm that this mobile platform, when switching mode, is able to move on surfaces with a large angle of inclination and perform more effective deceleration on both flat and inclined surfaces.},
  archive   = {C_IROS},
  author    = {Dmitrii N. Zakharov and Andrei M. Iaremenko and Denis M. Kurovskii and Artem M. Kurovskii and Oleg I. Borisov and Botao Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801549},
  month     = {10},
  pages     = {3553-3558},
  title     = {Development of a mobile reconfigurable mecanum robot with a locking device of rollers},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Torque ripple reduction in quasi-direct drive motors
through angle-based repetitive learning observer and model predictive
torque controller. <em>IROS</em>, 3546–3552. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Torque ripple reduction in quasi-direct drive (QDD) motors is crucial in their robotic applications for dynamic locomotion and dexterous manipulation. In this paper, we present a novel approach for reducing torque ripples of QDD motors, which integrates an angle-based repetitive learning observer (ARLO) and a model predictive control-based field-oriented controller (MPC-FOC). The proposed method successfully improves the torque loop control bandwidth and surpasses conventional proportional-integral (PI) controllers owing to the integrated physical constraints inside MPC. Additionally, the ARLO portion is able to mitigate ripple caused by the inherent cogging torque in brushless motors and also the periodic friction torque from the planetary gearboxes in QDD systems. The effectiveness of the proposed method is demonstrated through both simulation of a single QDD motor and experiments on a two-degree-of-freedom robotic leg, where the performance improvement can be 72.7% in speed tracking and 58.5% in trajectory tracking. The proposed method shows great potential in facilitating smooth motion and precise force control in future robotic applications.},
  archive   = {C_IROS},
  author    = {Hefei Zhang and Xiaohu Zhang and Jinyu Cheng and Jiangtao Hu and Chao Ji and Yu Wang and Yutong Jiang and Zhen Han and Wei Gao and Shiwu Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801721},
  month     = {10},
  pages     = {3546-3552},
  title     = {Torque ripple reduction in quasi-direct drive motors through angle-based repetitive learning observer and model predictive torque controller},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Novel multiport output twisted string actuator with
self-differential mechanism: Hand glove application. <em>IROS</em>,
3540–3545. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The differential mechanism can reduce the number of actuators and efficiently distribute force or power. We proposed a novel multiport output twisted string actuator (MO-TSA) with self-differential mechanism that employs a single actuator to achieve multiport outputs. The differential MO-TSA is adaptively controlled in accordance with the force differences at each output port, thus replacing the traditional differential gears and whiffletree mechanisms. Inspired by the hand muscles, we designed one hand glove using the MO-TSA, aiming to enhance the range of achievable grasp configurations. The hand glove is capable of performing various grasps with a single actuator, resulting in a lighter and simpler hand design and revolutionizing the field of twisted string actuators (TSAs) by offering a streamlined solution for achieving versatile actuation.},
  archive   = {C_IROS},
  author    = {Dunwen Wei and Chengguang Cui and Haitao Yu and Tao Gao and Chao Li and Sajjad Hussain and Fanny Ficuciello},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802340},
  month     = {10},
  pages     = {3540-3545},
  title     = {Novel multiport output twisted string actuator with self-differential mechanism: Hand glove application},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Energy minimization using custom-designed magnetic-spring
actuators. <em>IROS</em>, 3534–3539. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study introduces an innovative actuator that resembles a motor with a non-uniform permanent magnetic field. We have developed a prototype of the actuator by combining a standard motor, characterized by a uniform magnetic field, with a custom rotary magnetic spring exhibiting a non-uniform magnetic field. We have also presented a systematic computational approach to customize the magnetic field to minimize the energy consumption of the actuator when used for a user-defined oscillatory task. Experiments demonstrate that this optimized actuator significantly lowers energy consumption in a typical oscillatory task, such as pick-and-place or oscillatory limb motion during locomotion, compared to conventional motors. Our findings imply that incorporating task-optimized non-uniform permanent magnetic fields into conventional motors and direct-drive actuators could enhance the energy efficiency of robotic systems.},
  archive   = {C_IROS},
  author    = {Yue Yang Fu and Ali U. Kilic and David J. Braun},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802861},
  month     = {10},
  pages     = {3534-3539},
  title     = {Energy minimization using custom-designed magnetic-spring actuators},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Static modeling of the stiffness and contact forces of
rolling element eccentric drives for use in robotic drive systems.
<em>IROS</em>, 3526–3533. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Rolling element eccentric drives promise to be an easy-to-manufacture and performant gear system for robotic actuators. They share characteristics with other eccentric drives, such as strain wave and cycloidal drives, but use rolling elements instead of an eccentric gear. They offer reduced manufacturing complexity and costs by using readily available standard parts. Little research into rolling element eccentric drives is available, and their characteristics are still underexplored. This work uses a contact-based model to investigate the previously unknown stiffness of rolling element eccentric drives. Such calculation methods are well established for structurally similar components, such as cycloidal drives and roller bearings, and provide a high-level and computationally efficient model. Good stiffness models are critical for accurately predicting robotic actuator behavior and enabling better control of robotic systems. Additionally, the proposed model is used to calculate the contact forces under load occurring in rolling element eccentric drives. Contact forces are critical to calculating a drive’s load capacity, lifetime, and efficiency and serve as the foundation for further research. The mathematical description of the proposed model is derived, and the stiffness of a representative rolling element eccentric drive is calculated. Different manufacturing techniques, characterized by tolerance levels and material choices, are compared. Irrespective of manufacturing precision, similar stiffness curves result for drives made of steel, but higher contact forces result from less precise manufacturing. The stiffness of drives made from 3D printed plastic is considerably lower than that of drives made from steel. Additionally, the stiffness of rolling element eccentric drives is compared to similar eccentric drives, and a comparable twist-over-torque curve is shown.},
  archive   = {C_IROS},
  author    = {Simon Fritsch and Stefan Landler and Michael Otto and Birgit Vogel-Heuser and Markus Zimmermann and Karsten Stahl},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802779},
  month     = {10},
  pages     = {3526-3533},
  title     = {Static modeling of the stiffness and contact forces of rolling element eccentric drives for use in robotic drive systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design of a variable wheel-propeller integrated mechanism
for amphibious robots. <em>IROS</em>, 3519–3525. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In order to address the high complexity and low efficiency of amphibious propulsion systems, this paper proposes a novel variable wheel-propeller integrated mechanism for amphibious robots. By adjusting the blade pitch angle, it enables multiple motion modes, including rapid and stable movement on flat ground, obstacle crossing, and omnidirectional movement on water surface. This study establishes a kinematic model for the propeller blades and conducts multi-objective optimization of the structural parameters by considering both the land obstacle-crossing performance and underwater propulsion performance. Based on the optimized structural parameters, a virtual simulation prototype is constructed. Simulation results indicate that when water surface movement, with a driving torque of 3N.m, robot achieves a maximum linear velocity of 1.25m/s and a maximum angular self-rotation velocity of 3.5rad/s. Moreover, varying the blade pitch angle can alter the thrust direction, enabling omnidirectional mobility on water surface. During land movement, with a rotation speed of 60rpm, the highest obstacle-crossing height is 184mm. This wheel-propeller integrated mechanism exhibits robust comprehensive motion performance and environmental adaptability, with convenient motion modes switching.},
  archive   = {C_IROS},
  author    = {Liang Lu and Xiangquan Gao and Ming Xiang and Zefeng Yan and Bin Han},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801376},
  month     = {10},
  pages     = {3519-3525},
  title     = {Design of a variable wheel-propeller integrated mechanism for amphibious robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active learning for forward/inverse kinematics of
redundantly-driven flexible tensegrity manipulator. <em>IROS</em>,
3512–3518. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In flexible redundantly-driven multi-DOF systems, like living beings, the representation of redundant kinematics including the diversity of solutions, is crucial for leveraging its distinctive characteristics. This paper proposes an active learning framework for forward and inverse modeling of complex kinematics that improves expressions of control space, task space, and null space. It consists of a Variational Auto Encoder (VAE)-type network that internally holds expressions of control space, task space, and null space, and an algorithm for selecting new data using the cross-entropy method. The validity of the proposed system was verified using a tensegrity manipulator driven by 40 pneumatic cylinders. As a result, it was confirmed that active learning contributed to achieving the entire range of motion covered and a well-organized representation of the null space.},
  archive   = {C_IROS},
  author    = {Yuhei Yoshimitsu and Takayuki Osa and Heni Ben Amor and Shuhei Ikemoto},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802310},
  month     = {10},
  pages     = {3512-3518},
  title     = {Active learning for Forward/Inverse kinematics of redundantly-driven flexible tensegrity manipulator},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-robot navigation among movable obstacles: Implicit
coordination to deal with conflicts and deadlocks. <em>IROS</em>,
3505–3511. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {How to coordinate multiple robots moving in modifiable cluttered environments? In this paper, we introduce the multi-robot version of the NAMO problem (Navigation Among Movable Obstacles). In MR-NAMO, robots must not only plan for the possibility of displacing obstacles as needed to facilitate their navigation, but also solve conflicts that may arise when trying to simultaneously access a location or obstacle. After identifying all different types of conflicts, we define and compare variants of an implicit coordination strategy allowing the use of existing NAMO algorithms [1] in a Multi-Robot context. We also show how our previously introduced social occupation cost model [2] can improve the efficiency of multirobot plans with better obstacle placement choices, and how it can be applied in a novel way to find relevant robot placement choices to solve deadlock situations.},
  archive   = {C_IROS},
  author    = {Benoit Renault and Jacques Saraydaryan and David Brown and Olivier Simonin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802092},
  month     = {10},
  pages     = {3505-3511},
  title     = {Multi-robot navigation among movable obstacles: Implicit coordination to deal with conflicts and deadlocks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tree-based reconfiguration of metamorphic robots.
<em>IROS</em>, 3498–3504. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Metamorphic robots have gained the attention of many researchers due to their ability to change shape and adapt to various tasks. In order to utilize the versatility of metamorphic systems, we need to be able to find a shape-shifting (reconfiguration) plan efficiently; however, finding these plans is challenging due to the high degree of freedom of modular systems. Reconfiguration algorithms proposed so far either scale poorly with a growing number of modules, impose specific restrictions on modules, or produce plans that are unrealistic outside of zero-gravity environments. This paper presents a new approach to the reconfiguration problem of chain-type metamorphic robots. Our algorithm relies on forming tentacles and using them to transport modules, which allows us to search through a reduced state space by computing many smaller planning instances. As a result, we obtain a heuristic solution that is more scalable than optimal planners, while producing realistic plans that impose no specific module requirements.},
  archive   = {C_IROS},
  author    = {Patrick Ondika and Jan Mrázek and Jiří Barnat},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801396},
  month     = {10},
  pages     = {3498-3504},
  title     = {Tree-based reconfiguration of metamorphic robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Communication-constrained multi-robot exploration with
intermittent rendezvous. <em>IROS</em>, 3490–3497. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Communication constraints can significantly impact robots’ ability to share information, coordinate their movements, and synchronize their actions, thus limiting coordination in Multi-Robot Exploration (MRE) applications. In this work, we address these challenges by modeling the MRE application as a DEC-POMDP and designing a joint policy that follows a rendezvous plan. This policy allows robots to explore unknown environments while intermittently sharing maps opportunistically or at rendezvous locations without being constrained by joint path optimizations. To generate the rendezvous plan, robots represent the MRE task as an instance of the Job Shop Scheduling Problem (JSSP) and minimize JSSP metrics. They aim to reduce waiting times and increase connectivity, which correlates to the DEC-POMDP rewards and time to complete the task. Our simulation results suggest that our method is more efficient than using relays or maintaining intermittent communication with a base station, being a suitable approach for Multi-Robot Exploration. We developed a proof-of-concept using the Robot Operating System (ROS) that is available at: https://github.com/multirobotplayground/Noetic-Multi-Robot-Sandbox.},
  archive   = {C_IROS},
  author    = {Alysson Ribeiro Da Silva and Luiz Chaimowicz and Thales C. Silva and M. Ani Hsieh},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802343},
  month     = {10},
  pages     = {3490-3497},
  title     = {Communication-constrained multi-robot exploration with intermittent rendezvous},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust and safe task-driven planning and navigation for
heterogeneous multi-robot teams with uncertain dynamics. <em>IROS</em>,
3482–3489. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Task and motion planning (TAMP) can enhance intelligent multi-robot coordination. TAMP becomes signifi-cantly more complicated in obstacle-cluttered environments and in the presence of robot dynamic uncertainties. We propose a control framework that solves the motion-planning problem for multi-robot teams with uncertain dynamics, addressing a key component of the TAMP pipeline. The principal part of the proposed algorithm constitutes a decentralized feedback control policy for tracking of reference paths taken by the robots while avoiding collision and adapting in real time to the underlying dynamic uncertainties. The proposed framework further leverages sampling-based motion planners to free the robots from local-minimum configurations. Extensive experimental results in complex, realistic environments illustrate the superior efficiency of the proposed approach, in terms of planning time and number of encountered local minima, with respect to state-of-the-art baseline methods.},
  archive   = {C_IROS},
  author    = {Tianyang Pan and Christos K. Verginis and Lydia E. Kavraki},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802695},
  month     = {10},
  pages     = {3482-3489},
  title     = {Robust and safe task-driven planning and navigation for heterogeneous multi-robot teams with uncertain dynamics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Opinion-based strategy for distributed multi-robot task
allocation in swarms of robots. <em>IROS</em>, 3476–3481. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Opinions of individuals in large groups evolve through interactions with neighbors and the environment, which can be modeled with opinion dynamics. In this paper, we propose a distributed opinion-based strategy for large-scale multi-robot task allocation utilizing the convergence behaviors of opinion dynamics. The strategy relies on the specialized opinion dynamics on the unit sphere for robot task selection. We investigate the convergence behaviors of opinion dynamics in the context of regions of attraction. Simulation results with a swarm of 200 homogeneous robots validate the effectiveness of our proposed strategy.},
  archive   = {C_IROS},
  author    = {Ziqiao Zhang and Shengkang Chen and Scott Mayberry and Fumin Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801579},
  month     = {10},
  pages     = {3476-3481},
  title     = {Opinion-based strategy for distributed multi-robot task allocation in swarms of robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). D-MARL: A dynamic communication-based action space
enhancement for multi agent reinforcement learning exploration of large
scale unknown environments. <em>IROS</em>, 3470–3475. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this article, we propose a novel communication-based action space enhancement for the D-MARL exploration algorithm to improve the efficiency of mapping an unknown environment, represented by an occupancy grid map. In general, communication between autonomous systems is crucial when exploring large and unstructured environments. In such real-world scenarios, data transmission is limited and relies heavily on inter-agent proximity and the attributes of the autonomous platforms. In the proposed approach, each agent’s policy is optimized by utilizing the heterogeneous-agent proximal policy optimization algorithm to autonomously choose whether to communicate or explore the environment. To accomplish this, multiple novel reward functions are formulated by integrating inter-agent communication and exploration. The investigated approach aims to increase efficiency and robustness in the mapping process, minimize exploration overlap, and prevent agent collisions. The D-MARL policies trained on different reward functions have been compared to understand the effect of different reward terms on the collaborative attitude of the homogeneous agents. Finally, multiple simulation results are provided to prove the efficacy of the proposed scheme.},
  archive   = {C_IROS},
  author    = {Gabriele Calzolari and Vidya Sumathy and Christoforos Kanellakis and George Nikolakopoulos},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801319},
  month     = {10},
  pages     = {3470-3475},
  title     = {D-MARL: A dynamic communication-based action space enhancement for multi agent reinforcement learning exploration of large scale unknown environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning to imitate spatial organization in multi-robot
systems. <em>IROS</em>, 3463–3469. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Understanding collective behavior and how it evolves is important to ensure that robot swarms can be trusted in a shared environment. One way to understand the behavior of the swarm is through collective behavior reconstruction using prior demonstrations. Existing approaches often require access to the swarm controller which may not be available. We reconstruct collective behaviors in distinct swarm scenarios involving shared environments without using swarm controller information. We achieve this by transforming prior demonstrations into features that describe multi-agent interactions before behavior reconstruction with multi-agent generative adversarial imitation learning (MA-GAIL). We show that our approach outperforms existing algorithms in spatial organization, and can be used to observe and reconstruct a swarm’s behavior for further analysis and testing, which might be impractical or undesirable on the original robot swarm.},
  archive   = {C_IROS},
  author    = {Ayomide O. Agunloye and Sarvapali D. Ramchurn and Mohammad D. Soorati},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801662},
  month     = {10},
  pages     = {3463-3469},
  title     = {Learning to imitate spatial organization in multi-robot systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CGA: Corridor generating algorithm for multi-agent
environments. <em>IROS</em>, 3455–3462. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we consider path planning for a team of mobile agents where one agent must reach a given target as soon as possible and the others must accommodate to avoid collisions. We call this practical problem the Single-Agent Corridor Generating (SACG) problem and explore several algorithms for solving it. We propose two baseline algorithms based on existing Multi-Agent Path Finding (MAPF) algorithms and outline their limitations. Then, we present the Corridor Generating Algorithm (CGA), a fast and complete algorithm for solving SACG. CGA performs well compared to the baseline approaches. In addition, we show how CGA can be generalized to address the lifelong version of MAPF, where new goals appear over time.},
  archive   = {C_IROS},
  author    = {Arseniy Pertzovsky and Roni Stern and Roie Zivan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802519},
  month     = {10},
  pages     = {3455-3462},
  title     = {CGA: Corridor generating algorithm for multi-agent environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design of a multi-robot coordination system based on
functional expressions using large language models. <em>IROS</em>,
3447–3454. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A system is expected to facilitate coordination among multiple construction machines or robots, enabling them to adaptively perform various tasks in disaster sites and unknown environments. Prior research has generally adopted a model-based approach to designing cooperative behavior. However, it is difficult to adapt to environments and scenarios that cannot be predicted by the model. In recent years, it has been reported that a robot equipped with foundation models can adapt to unknown (open) environments and unpredictable situations. However, there has been little discussion on foundation models for multiple robot systems; a flow that cooperatively handles unexpected events does not exist. In this paper, we propose the system flow that enables multiple robots to adaptively coordinate to unforeseen scenarios based on the functional expressions of each other and environment understanding utilizing GPT-4 and GPT-4V. Through experimentation, we verify that the proposed flow is able to adapt to an unforeseen environment, particularly path obstruction via robot experiments. Furthermore, we examine the validity of the proposed flow by varying the robots’ functional expressions and sensor information for the environment.},
  archive   = {C_IROS},
  author    = {Yuki Kato and Takahiro Yoshida and Yuichiro Sueoka and Koichi Osuka and Ryosuke Yajima and Keiji Nagatani and Hajime Asama},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802571},
  month     = {10},
  pages     = {3447-3454},
  title     = {Design of a multi-robot coordination system based on functional expressions using large language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Coalition formation game approach for task allocation in
heterogeneous multi-robot systems under resource constraints.
<em>IROS</em>, 3439–3446. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies a case of the multi-robot task allocation (MRTA) problem, where each unmanned aerial vehicle (UAV) is endowed with multiple but limited resources. Completing each task necessitates UAVs to combine different resources through coalition formation, which will incur various costs including flight cost, execution cost, and cooperation cost. To minimize the total cost while maximizing both task completion rate and resource utilization rate, we model the MRTA problem of the UAVs as a leader-follower coalition formation game. In this game, leader UAVs coordinate follower UAVs to fulfill task resource requisites. Meanwhile, follower UAVs select suitable coalitions to join based on the altruistic preference. Theoretical analysis confirms the existence of a Nash stable partition in the coalition formation game. To achieve this stable partition, we propose a coalition formation algorithm. Simulation experiments validate that the proposed algorithm outperforms existing methods for the MRTA problem under resource constraints in terms of both task completion rate and resource utilization rate.},
  archive   = {C_IROS},
  author    = {Liwang Zhang and Dong Liang and Minglong Li and Wenjing Yang and Shaowu Yang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801429},
  month     = {10},
  pages     = {3439-3446},
  title     = {Coalition formation game approach for task allocation in heterogeneous multi-robot systems under resource constraints},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-robot path planning with boolean specification tasks
under motion uncertainties. <em>IROS</em>, 3433–3438. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies the path planning problem of multi-robot systems under motion uncertainties with high-level tasks that are expressed as Boolean specifications. The specification imposes logical constraints on robot trajectories and final states. First, a global Markov decision process model of the multi-robot system is constructed to provide its current state. In order to tackle the state explosion problem, at each stage, we construct a local Markov decision process for every individual agent in sequence to compute the local optimal movement strategy and update the global Markov decision process accordingly (i.e., compute locally and update globally). Next, we propose a heuristic reward function design method that provides different rewards for visiting different task points by introducing the estimated distance to complete the global task. Finally, a series of numerical experiments are conducted to demonstrate the computational efficiency and scalability of our developed approach.},
  archive   = {C_IROS},
  author    = {Zhe Zhang and Zhou He and Ning Ran and Michel Reniers},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801558},
  month     = {10},
  pages     = {3433-3438},
  title     = {Multi-robot path planning with boolean specification tasks under motion uncertainties},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Collaborative object manipulation on the water surface by a
UAV-USV team using tethers. <em>IROS</em>, 3425–3432. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces an innovative methodology for object manipulation on the surface of water through the collaboration of an Unmanned Aerial Vehicle (UAV) and an Unmanned Surface Vehicle (USV) connected to the object by tethers. We propose a novel mathematical model of a robotic system that combines the UAV, USV, and the tethered floating object. A novel Model Predictive Control (MPC) framework is designed for using this model to achieve precise control and guidance for this collaborative robotic system. Extensive simulations in the realistic robotic simulator Gazebo demonstrate the system’s readiness for real-world deployment, highlighting its versatility and effectiveness. Our multi-robot system overcomes the state-of-the-art single-robot approach, exhibiting smaller control errors during the tracking of the floating object’s reference. Additionally, our multi-robot system demonstrates a shorter recovery time from a disturbance compared to the single-robot approach.},
  archive   = {C_IROS},
  author    = {Filip Novák and Tomáš Báča and Martin Saska},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802469},
  month     = {10},
  pages     = {3425-3432},
  title     = {Collaborative object manipulation on the water surface by a UAV-USV team using tethers},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decentralized communication-maintained coordination for
multi-robot exploration: Achieving connectivity and adaptability.
<em>IROS</em>, 3417–3424. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The realm of multi-robot autonomous exploration tasks underscores the critical role of communication in coordinating group activities. This paper introduces an innovative decentralized multi-robot exploration algorithm, meticulously crafted to ensure unbroken communication within robotic groups, a crucial element for effective coordination. The motivation for our work is two-fold: Firstly, seamless communication is vital for coordinating multi-robot autonomous exploration tasks. Secondly, in applications such as disaster rescue operations or military maneuvers, there are numerous scenarios where spatial congregation of multiple robots is imperative for joint task accomplishment. Our approach addresses these challenges through a stringent communication constraint, ensuring that each robot remains in constant communicative contact with the rest of the group. This is realized by employing a decentralized policy that integrates Graph Neural Network (GNN) layers with self-attention mechanism. Such policy network design allows adaptation to different numbers of robots and varied environments. After an initial imitation learning phase, the policy is refined through learning from experiences generated via a tree-search-based lookahead technique. Our experimental analysis validates that the algorithm not only maintains consistent communication links among all group members but also improve the exploration efficiency under the communication constraints. These results highlight the potential of our method in enhancing the effectiveness of robotic group explorations while ensuring robust communication connection.},
  archive   = {C_IROS},
  author    = {Wei Tang and Chao Li and Jun Wu and Qiuguo Zhu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802832},
  month     = {10},
  pages     = {3417-3424},
  title     = {Decentralized communication-maintained coordination for multi-robot exploration: Achieving connectivity and adaptability},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robustness study of optimal geometries for cooperative
multi-robot localization. <em>IROS</em>, 3409–3416. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work focuses on localizing a single target robot with multi-robot formations in 2D space. The cooperative robots employ inter-robot range measurements to assess the target position. In the presence of noisy measurements, the choice of formation geometries significantly impacts the accuracy of the target robot’s pose estimation. While an infinite number of geometries exists to optimize localization accuracy, the current practice is to choose the final formation geometry based on convenience criteria such as simplicity or proximity to the initial position of the robots. The former leads to the selection of regular polygon-shaped formations, while the latter results in behaviour-based formations. Different from existing works, we conduct a complete robustness study of formation geometries in the presence of deviations from the desired formation and range measurement errors. In 2D scenarios, we establish necessary and sufficient conditions for formation geometries to be robust against robot positioning errors. This result substantiates the extensive use of regular polygon formations. However, our analysis reveals the lack of robustness of the commonly used square formation geometry, which stands as an exception. Simulation results illustrate the advantages of these robust geometries in enhancing target localization accuracy.},
  archive   = {C_IROS},
  author    = {Mathilde Theunissen and Isabelle Fantoni and Ezio Malis and Philippe Martinet},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802509},
  month     = {10},
  pages     = {3409-3416},
  title     = {Robustness study of optimal geometries for cooperative multi-robot localization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep ad-hoc sub-team partition learning for multi-agent air
combat cooperation. <em>IROS</em>, 3403–3408. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the future, unmanned autonomous air combat will encounter large-scale confrontation scenarios, where agents must consider complex time-varying relationships among aircraft when making decisions. Previous works have already introduced Multi-Agent Reinforcement Learning (MARL) into air combat and succeeded in surpassing the human expert level. However, they mainly focus on small-scale air combat with low relationship complexity, e.g., 1-vs-1 or 2-vs-2. As more agents join the confrontation, existing algorithms tend to suffer significant performance degradation due to the increase in problem dimensions. In view of this, this paper proposes Deep Ad-hoc Sub-Team Partition Learning(DASPL) to address large-scale air combat problems. DASPL models multi-agent air combat as a graph to handle the complex relations and introduces an automatic partitioning mechanism to generate dynamic sub-teams, which converts the existing large-scale multi-agent air combat cooperation problem into multiple small-scale equivalence problems. Additionally, DASPL incorporates an efficient message passing method among the participating sub-teams.},
  archive   = {C_IROS},
  author    = {Songyuan Fan and Haiyin Piao and Yi Hu and Feng Jiang and Roushu Yang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801614},
  month     = {10},
  pages     = {3403-3408},
  title     = {Deep ad-hoc sub-team partition learning for multi-agent air combat cooperation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Anchor-oriented localized voronoi partitioning for
GPS-denied multi-robot coverage. <em>IROS</em>, 3395–3402. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-robot coverage is crucial in numerous applications, including environmental monitoring, search and rescue operations, and precision agriculture. In modern applications, a multi-robot team must collaboratively explore unknown spatial fields in GPS-denied and extreme environments where global localization is unavailable. Coverage algorithms typically assume that the robot positions and the coverage environment are defined in a global reference frame. However, coordinating robot motion and ensuring coverage of the shared convex workspace without global localization is challenging. This paper proposes a novel anchor-oriented coverage (AOC) approach to generate dynamic localized Voronoi partitions based around a common anchor position. We further propose a consensus-based coordination algorithm that achieves agreement on the coverage workspace around the anchor in the robots’ relative frames of reference. Through extensive simulations and real-world experiments, we demonstrate that the proposed anchor-oriented approach using localized Voronoi partitioning performs as well as the state-of-the-art coverage controller using GPS.},
  archive   = {C_IROS},
  author    = {Aiman Munir and Ehsan Latif and Ramviyas Parasuraman},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802222},
  month     = {10},
  pages     = {3395-3402},
  title     = {Anchor-oriented localized voronoi partitioning for GPS-denied multi-robot coverage},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HGP-RL: Distributed hierarchical gaussian processes for
wi-fi-based relative localization in multi-robot systems. <em>IROS</em>,
3387–3394. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Relative localization is crucial for multi-robot systems to perform cooperative tasks, especially in GPS-denied environments. Current techniques for multi-robot relative localization rely on expensive or short-range sensors such as cameras and LIDARs. As a result, these algorithms face challenges such as high computational complexity (e.g., map merging), dependencies on well-structured environments, etc. To remedy this gap, we propose a new distributed approach to perform relative localization (RL) using a common Access Point (AP). To achieve this efficiently, we propose a novel Hierarchical Gaussian Processes (HGP) mapping of the Radio Signal Strength Indicator (RSSI) values from a Wi-Fi AP to which the robots are connected. We termed this approach as HGP-RL (Hierarchical Gaussian Process for Relative Localization). Each robot performs hierarchical inference using the HGP map to locate the AP in its reference frame, and the robots obtain relative locations of the neighboring robots leveraging AP-oriented algebraic transformations. The approach readily applies to resource-constrained devices and relies only on the ubiquitously-available WiFi RSSI measurement. We extensively validate the performance of the proposed HGP-RL in Robotarium simulations against several state-of-the-art methods. The results indicate superior performance of HGP-RL regarding localization accuracy, computation, and communication over-heads. Finally, we showcase the utility of HGP-RL through a multi-robot cooperative experiment to achieve a rendezvous task in a team of three mobile robots.},
  archive   = {C_IROS},
  author    = {Ehsan Latif and Ramviyas Parasuraman},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802653},
  month     = {10},
  pages     = {3387-3394},
  title     = {HGP-RL: Distributed hierarchical gaussian processes for wi-fi-based relative localization in multi-robot systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LiDAR-based HD map localization using semantic generalized
ICP with road marking detection. <em>IROS</em>, 3379–3386. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In GPS-denied scenarios, a robust environmental perception and localization system becomes crucial for autonomous driving. In this paper, a LiDAR-based online localization system is developed, incorporating road marking detection and registration on a high-definition (HD) map. Within our system, a road marking detection approach is proposed with realtime performance, in which an adaptive segmentation technique is first introduced to isolate high-reflectance points correlated with road markings, enhancing real-time efficiency. Then, a spatio-temporal probabilistic local map is formed by aggregating historical LiDAR scans, providing a dense point cloud. Finally, a LiDAR bird’s-eye view (LiBEV) image is generated, and an instance segmentation network is applied to accurately label the road markings. For road marking registration, a semantic generalized iterative closest point (SG-ICP) algorithm is designed. Linear road markings are modeled as 1-manifolds embedded in 2D space, mitigating the influence of constraints along the linear direction, addressing the under-constrained problem and achieving a lower localization errors on HD maps than ICP. Extensive experiments are conducted in real-world scenarios, demonstrating the effectiveness and robustness of our system.},
  archive   = {C_IROS},
  author    = {Yansong Gong and Xinglian Zhang and Jingyi Feng and Xiao He and Dan Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801497},
  month     = {10},
  pages     = {3379-3386},
  title     = {LiDAR-based HD map localization using semantic generalized ICP with road marking detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Three-dimensional vehicle dynamics state estimation for
high-speed race cars under varying signal quality. <em>IROS</em>,
3371–3378. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work aims to present a three-dimensional vehicle dynamics state estimation under varying signal quality. Few researchers have investigated the impact of three-dimensional road geometries on the state estimation and, thus, neglect road inclination and banking. Especially considering high velocities and accelerations, the literature does not address these effects. Therefore, we compare two- and three-dimensional state estimation schemes to outline the impact of road geometries. We use an Extended Kalman Filter with a point-mass motion model and extend it by an additional formulation of reference angles. Furthermore, virtual velocity measurements significantly improve the estimation of road angles and the vehicle’s side slip angle. We highlight the importance of steady estimations for vehicle motion control algorithms and demonstrate the challenges of degraded signal quality and Global Navigation Satellite System dropouts. The proposed adaptive covariance facilitates a smooth estimation and enables stable controller behavior. The developed state estimation has been deployed on a high-speed autonomous race car at various racetracks. Our findings indicate that our approach outperforms state-of-the-art vehicle dynamics state estimators and an industry-grade Inertial Navigation System. Further studies are needed to investigate the performance under varying track conditions and on other vehicle types.},
  archive   = {C_IROS},
  author    = {Sven Goblirsch and Marcel Weinmann and Johannes Betz},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802776},
  month     = {10},
  pages     = {3371-3378},
  title     = {Three-dimensional vehicle dynamics state estimation for high-speed race cars under varying signal quality},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tightly-coupled factor graph formulation for radar-inertial
odometry. <em>IROS</em>, 3364–3370. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a Radar-Inertial Odometry (RIO) method based on the nonlinear optimization of factor graphs in a sliding window fashion. Our method makes use of a light-weight, low-power, inexpensive and commonly available hardware enabling easy deployment on small Unmanned Aerial Vehicles (UAV)s. We keep the state estimation problem bounded by employing partial marginalization of the oldest states, rendering the method real-time capable. We compare the implemented approach to the state-of-the-art multi-state Extended Kalman Filter (EKF)-based method in a one-to-one fashion. That is, we implemented in a single custom C++ RIO framework both estimation back-ends with all other parts shared and thus identical for a fair direct comparison. In the real-world flight experiments, we compare the two methods and show that both perform similarly in terms of accuracy when the linearization point is not far from the true state. Upon wrong initialization, the factor graph approach heavily outperforms the EKF approach. We also acknowledge that the influence of undetected outliers can overwhelm the inherent benefits of the nonlinear optimization approach leading to the insight that the estimator front-end has an important (and often underestimated) role in the overall performance. The open source code and datasets can be found here: https://github.com/aau-cns/aaucns_rio.},
  archive   = {C_IROS},
  author    = {Jan Michalczyk and Julius Quell and Florian Steidle and Marcus G. Müller and Stephan Weiss},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801945},
  month     = {10},
  pages     = {3364-3370},
  title     = {Tightly-coupled factor graph formulation for radar-inertial odometry},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing visual place recognition via fast and slow
adaptive biasing in event cameras. <em>IROS</em>, 3356–3363. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Event cameras are increasingly popular in robotics due to beneficial features such as low latency, energy efficiency, and high dynamic range. Nevertheless, their downstream task performance is greatly influenced by the optimization of bias parameters. These parameters, for instance, regulate the necessary change in light intensity to trigger an event, which in turn depends on factors such as the environment lighting and camera motion. This paper introduces feedback control algorithms that automatically tune the bias parameters through two interacting methods: 1) An immediate, on-the-fly fast adaptation of the refractory period, which sets the minimum interval between consecutive events, and 2) if the event rate exceeds the specified bounds even after changing the refractory period repeatedly, the controller adapts the pixel bandwidth and event thresholds, which stabilizes after a short period of noise events across all pixels (slow adaptation). Our evaluation focuses on the visual place recognition task, where incoming query images are compared to a given reference database. We conducted comprehensive evaluations of our algorithms’ adaptive feedback control in real-time. To do so, we collected the QCR-Fast-and-Slow dataset that contains DAVIS346 event camera streams from 366 repeated traversals of a Scout Mini robot navigating through a 100 meter long indoor lab setting (totaling over 35km distance traveled) in varying brightness conditions with ground truth location information. Our proposed feedback controllers result in superior performance when compared to the standard bias settings and prior feedback control methods. Our findings also detail the impact of bias adjustments on task performance and feature ablation studies on the fast and slow adaptation mechanisms.},
  archive   = {C_IROS},
  author    = {Gokul B. Nair and Michael Milford and Tobias Fischer},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802384},
  month     = {10},
  pages     = {3356-3363},
  title     = {Enhancing visual place recognition via fast and slow adaptive biasing in event cameras},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). JointLoc: A real-time visual localization framework for
planetary UAVs based on joint relative and absolute pose estimation.
<em>IROS</em>, 3348–3355. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unmanned aerial vehicles (UAVs) visual localization in planetary aims to estimate the absolute pose of the UAV in the world coordinate system through satellite maps and images captured by on-board cameras. However, since planetary scenes often lack significant landmarks and there are modal differences between satellite maps and UAV images, the accuracy and real-time performance of UAV positioning will be reduced. In order to accurately determine the position of the UAV in a planetary scene in the absence of the global navigation satellite system (GNSS), this paper proposes JointLoc, which estimates the real-time UAV position in the world coordinate system by adaptively fusing the absolute 2-degree-of-freedom (2-DoF) pose and the relative 6-degree-of-freedom (6-DoF) pose. Extensive comparative experiments were conducted on a proposed planetary UAV image cross-modal localization dataset, which contains three types of typical Martian topography generated via a simulation engine as well as real Martian UAV images from the Ingenuity helicopter. JointLoc achieved a root-mean-square error of 0.237m in the trajectories of up to 1,000m, compared to 0.594m and 0.557m for ORB-SLAM2 and ORB-SLAM3 respectively. The source code will be available at https://github.com/LuoXubo/JointLoc.},
  archive   = {C_IROS},
  author    = {Xubo Luo and Xue Wan and Yixing Gao and Yaolin Tian and Wei Zhang and Leizheng Shu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802040},
  month     = {10},
  pages     = {3348-3355},
  title     = {JointLoc: A real-time visual localization framework for planetary UAVs based on joint relative and absolute pose estimation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamically modulating visual place recognition sequence
length for minimum acceptable performance scenarios. <em>IROS</em>,
3340–3347. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile robots and autonomous vehicles are often required to function in environments where critical position estimates from sensors such as GPS become uncertain or unreliable. Single image visual place recognition (VPR) provides an alternative for localization but often requires techniques such as sequence matching to improve robustness, which incurs additional computation and latency costs. Even then, the sequence length required to localize at an acceptable performance level varies widely; and simply setting overly long fixed sequence lengths creates unnecessary latency, computational overhead, and can even degrade performance. In these scenarios it is often more desirable to meet or exceed a set target performance at minimal expense. In this paper we present an approach which uses a calibration set of data to fit a model that modulates sequence length for VPR as needed to exceed a target localization performance. We make use of a coarse position prior, which could be provided by any other localization system, and capture the variation in appearance across this region. We use the correlation between appearance variation and sequence length to curate VPR features and fit a Multi-Layer Perceptron (MLP) for selecting the optimal length. We demonstrate that this method is effective at modulating sequence length to maximize the number of sections in a dataset which meet or exceed a target performance whilst minimizing the median length used. We show applicability across several datasets and reveal key phenomena like generalization capabilities, the benefits of curating features and the utility of non-state-of-the-art feature extractors with nuanced properties.},
  archive   = {C_IROS},
  author    = {Connor Malone and Ankit Vora and Thierry Peynot and Michael Milford},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802570},
  month     = {10},
  pages     = {3340-3347},
  title     = {Dynamically modulating visual place recognition sequence length for minimum acceptable performance scenarios},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-model fusion of LiDAR-inertial odometry via
localization and mapping. <em>IROS</em>, 3334–3339. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents a comprehensive LiDAR-inertial odometry framework featuring robust smoothing and mapping capabilities, effectively correcting LiDAR feature point skewness using an inertial measurement unit (IMU). While the Extended Kalman Filter (EKF) is a common choice for nonlinear motion estimation, its complexity grows when handling maneuvering targets. To overcome this challenge, a new framework that incorporates the Iterated Interactive Multiple Models of Kalman Filter (IMMKF) is given, providing a solution for reliable navigation in dynamic motion and noisy conditions. To ensure map consistency, an ikd-tree that facilitates continuous updates and adaptive rebalance is employed, preserving the map’s integrity. To guarantee the robustness of our approach, it undergoes extensive testing across diverse scales of indoor and outdoor environments. This testing scenario simulates absolute GPS denial. In terms of estimated motion, the new algorithm demonstrates superior accuracy compared to existing approaches. The implementation is openly accessible on GitHub4 for further exploration.},
  archive   = {C_IROS},
  author    = {An Duy Nguyen and Chuong Phuoc Le and Pratik Walunj and Anton Netchaev and Thanh Nho Do and Hung Manh La},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802004},
  month     = {10},
  pages     = {3334-3339},
  title     = {A multi-model fusion of LiDAR-inertial odometry via localization and mapping},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ModaLink: Unifying modalities for efficient
image-to-PointCloud place recognition. <em>IROS</em>, 3326–3333. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Place recognition is an important task for robots and autonomous cars to localize themselves and close loops in pre-built maps. While single-modal sensor-based methods have shown satisfactory performance, cross-modal place recognition that retrieving images from a point-cloud database remains a challenging problem. Current cross-modal methods transform images into 3D points using depth estimation for modality conversion, which are usually computationally intensive and need expensive labeled data for depth supervision. In this work, we introduce a fast and lightweight framework to encode images and point clouds into place-distinctive descriptors. We propose an effective Field of View (FoV) transformation module to convert point clouds into an analogous modality as images. This module eliminates the necessity for depth estimation and helps subsequent modules achieve real-time performance. We further design a non-negative factorization-based encoder to extract mutually consistent semantic features between point clouds and images. This encoder yields more distinctive global descriptors for retrieval. Experimental results on the KITTI dataset show that our proposed methods achieve state-of-the-art performance while running in real time. Additional evaluation on the HAOMO dataset covering a 17 km trajectory further shows the practical generalization capabilities. We have released the implementation of our methods as open source at: https://github.com/haomo-ai/ModaLink.git.},
  archive   = {C_IROS},
  author    = {Weidong Xie and Lun Luo and Nanfei Ye and Yi Ren and Shaoyi Du and Minhang Wang and Jintao Xu and Rui Ai and Weihao Gu and Xieyuanli Chen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801556},
  month     = {10},
  pages     = {3326-3333},
  title     = {ModaLink: Unifying modalities for efficient image-to-PointCloud place recognition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explicit interaction for fusion-based place recognition.
<em>IROS</em>, 3318–3325. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fusion-based place recognition is an emerging technique jointly utilizing multi-modal perception data, to recognize previously visited places in GPS-denied scenarios for robots and autonomous vehicles. Recent fusion-based place recognition methods combine multi-modal features in implicit manners. While achieving remarkable results, they do not explicitly consider what the individual modality affords in the fusion system. Therefore, the benefit of multi-modal feature fusion may not be fully explored. In this paper, we propose a novel fusion-based network, dubbed EINet, to achieve explicit interaction of the two modalities. EINet uses LiDAR ranges to supervise more robust vision features for long time spans, and simultaneously uses camera RGB data to improve the discrimination of LiDAR point clouds. In addition, we develop a new benchmark for the place recognition task based on the nuScenes dataset. To establish this benchmark for future research with comprehensive comparisons, we introduce both supervised and self-supervised training schemes alongside evaluation protocols. We conduct extensive experiments on the proposed benchmark, and the experimental results show that our EINet exhibits better recognition performance as well as solid generalization ability compared to the state-of-the-art fusion-based place recognition approaches. Our open-source code and benchmark are released at: https://github.com/BIT-XJY/EINet.},
  archive   = {C_IROS},
  author    = {Jingyi Xu and Junyi Ma and Qi Wu and Zijie Zhou and Yue Wang and Xieyuanli Chen and Wenxian Yu and Ling Pei},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802665},
  month     = {10},
  pages     = {3318-3325},
  title     = {Explicit interaction for fusion-based place recognition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-accuracy 2-d AoA estimation using lightweight UWB
arrays. <em>IROS</em>, 3312–3317. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ultra-wide band (UWB) systems are gaining popularity for multi-robot localization benefiting from their high-accuracy ranging capabilities. However, current UWB systems fall short in determining orientations and realizing pair-wise localization for neglecting bearing information. Given the importance of bearing capabilities, especially when vision-based methods fail, this paper proposes a high-accuracy 2-D bearing estimation method using stereo UWB arrays. We propose a novel phase error calibration method that effectively mitigates various phase imperfections. This array is designed with antenna spacing larger than half the wavelength to diminish antenna coupling and enhance bearing accuracy. As regards the phase ambiguity issue arising from large antenna spacing, a distributed range-assisted phase ambiguity determination method is developed. Our bearing estimation method exhibits low complexity and is well-suited for the deployment on mobile robots with limited computational resources. The performance of the proposed method is validated on the practical platforms under dynamic scenarios, yielding root mean squared errors (RMSEs) less than 4° and 3° for azimuth and elevation angle estimation, respectively.},
  archive   = {C_IROS},
  author    = {Yi Li and Hanying Zhao and Yiman Liu and Tianyu Wang and Jincheng Yu and Yuan Shen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801988},
  month     = {10},
  pages     = {3312-3317},
  title     = {High-accuracy 2-D AoA estimation using lightweight UWB arrays},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Augmenting vision with radar for all-weather
geo-localization without a prior HD map. <em>IROS</em>, 3305–3311. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate and robust geo-localization in all-weather conditions is essential for enabling autonomous vehicles and delivery robots to offer uninterrupted mobility services in the real world. In this paper, we propose the first camera and radar fusion based geo-localisation method that is robust to all-weather conditions. The core of the proposed method is to leverage the rich semantics information in images and sensing consistency in radars across all-weather. Our proposed method surpasses the state of the art camera-based and LiDAR-camera based methods in inclement weather conditions, shown by extensive comparative experiments. Notably, our approach requires only an open accessible map, eliminating the need for high-definition maps and offering a cost-effective solution for geo-localizing or globally localizing autonomous vehicles in any weather condition. Our code and trained model will be released publicly.},
  archive   = {C_IROS},
  author    = {Can Dong and Ziyang Hong and Siru Li and Liang Hu and Huijun Gao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802744},
  month     = {10},
  pages     = {3305-3311},
  title     = {Augmenting vision with radar for all-weather geo-localization without a prior HD map},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal robot formations: Balancing range-based
observability and user-defined configurations. <em>IROS</em>, 3297–3304.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a set of customizable and novel cost functions that enable the user to easily specify desirable robot formations, such as a &quot;high-coverage&quot; infrastructure-inspection formation, while maintaining high relative pose estimation accuracy. The overall cost function balances the need for the robots to be close together for good ranging-based relative localization accuracy and the need for the robots to achieve specific tasks, such as minimizing the time taken to inspect a given area. The formations found by minimizing the aggregated cost function are evaluated in a coverage path planning task in simulation and experiment, where the robots localize themselves and unknown landmarks using a simultaneous localization and mapping algorithm based on the extended Kalman filter. Compared to an optimal formation that maximizes ranging-based relative localization accuracy, these formations significantly reduce the time to cover a given area with minimal impact on relative pose estimation accuracy.},
  archive   = {C_IROS},
  author    = {Syed Shabbir Ahmed and Mohammed Ayman Shalaby and Jerome Le Ny and James Richard Forbes},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801342},
  month     = {10},
  pages     = {3297-3304},
  title     = {Optimal robot formations: Balancing range-based observability and user-defined configurations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neighborhood consensus guided matching based place
recognition with spatial-channel embedding. <em>IROS</em>, 3291–3296.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As a crucial part of mobile robotics and autonomous driving, Visual Place Recognition (VPR) is usually addressed by recognizing its similar reference images from a pre-obtained database. However, VPR always suffers from environmental changes, such as weather, illumination, perceptual-aliasing and so on. To address this, we firstly introduce a robust and discriminative global descriptor aggregation technique that normalizes the spatial and channel dimensions of features. A Spatial-Channel Embedding (SCE) module is proposed to learn the spatial and scale information of features which make global features more discriminative. Meanwhile, the traditional re-ranking methods (e.g. RANSAC) for geometric consistency verification are time-consuming. Here we propose a Neighborhood Consensus Guided Matching (NCGM) module, which uses Neighborhood Consensus to filter the features from patch-level matching to achieve more accurate matching while reduces the time consumption. Through extensive experiments on multiple benchmarks, we demonstrate that our method outperforms several state-of-the-art methods while maintaining lower time consumption and storage requirements.},
  archive   = {C_IROS},
  author    = {Kunmo Li and Yunzhou Zhang and Jian Ning and Xinge Zhao and Guiyuan Wang and Wei Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801802},
  month     = {10},
  pages     = {3291-3296},
  title     = {Neighborhood consensus guided matching based place recognition with spatial-channel embedding},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Resource-aware collaborative monte carlo localization with
distribution compression. <em>IROS</em>, 3283–3290. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Global localization is essential in enabling robot autonomy, and collaborative localization is key for multi-robot systems, allowing for more efficient planning and execution of tasks. In this paper, we address the task of collaborative global localization under computational and communication constraints. We propose a method which reduces the amount of information exchanged and the computational cost. We also analyze, implement and open-source seminal approaches, which we believe to be a valuable contribution to the community. We exploit techniques for distribution compression in near-linear time, with error guarantees. We evaluate our approach and the implemented baselines on multiple challenging scenarios, simulated and real-world. Our approach can run online on an onboard computer. We release an open-source C++/ROS2 implementation of our approach, as well as the baselines.1},
  archive   = {C_IROS},
  author    = {Nicky Zimmerman and Alessandro Giusti and Jérôme Guzzi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801896},
  month     = {10},
  pages     = {3283-3290},
  title     = {Resource-aware collaborative monte carlo localization with distribution compression},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SGNet: Salient geometric network for point cloud
registration. <em>IROS</em>, 3276–3282. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Point Cloud Registration (PCR) is a critical and challenging task in computer vision and robotics. One of the primary difficulties in PCR is identifying salient and meaningful points that exhibit consistent semantic and geometric properties across different scans. Previous methods have encountered challenges with ambiguous matching due to the similarity among patch blocks throughout the entire point cloud and the lack of consideration for efficient global geometric consistency. To address these issues, we propose a new framework that includes several novel techniques. Firstly, we introduce a semantic-aware geometric encoder that combines object-level and patch-level semantic information. This encoder significantly improves registration recall by reducing ambiguity in patch-level superpoint matching. Additionally, we incorporate a prior knowledge approach that utilizes an intrinsic shape signature to identify salient points. This enables us to extract the most salient super points and meaningful dense points in the scene. Secondly, we introduce an innovative transformer that encodes High-Order (HO) geometric features. These features are crucial for identifying salient points within initial overlap regions while considering global high-order geometric consistency. We introduce an anchor node selection strategy to optimize this high-order transformer further. By encoding inter-frame triangle or polyhedron consistency features based on these anchor nodes, we can effectively learn high-order geometric features of salient super points. These high-order features are then propagated to dense points and utilized by a Sinkhorn matching module to identify critical correspondences for successful registration. The experiments conducted on the 3DMatch/3DLoMatch and KITTI datasets demonstrate the effectiveness of our method.},
  archive   = {C_IROS},
  author    = {Qianliang Wu and Yaqing Ding and Lei Luo and Haobo Jiang and Shuo Gu and Chuanwei Zhou and Jin Xie and Jian Yang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802262},
  month     = {10},
  pages     = {3276-3282},
  title     = {SGNet: Salient geometric network for point cloud registration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CBGL: Fast monte carlo passive global localisation of 2D
LIDAR sensor. <em>IROS</em>, 3268–3275. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Navigation of a mobile robot is conditioned on the knowledge of its pose. In observer-based localisation configurations its initial pose may not be knowable in advance, leading to the need of its estimation. Solutions to the problem of global localisation are either robust against noise and environment arbitrariness but require motion and time, which may (need to) be economised on, or require minimal estimation time but assume environmental structure, may be sensitive to noise, and demand preprocessing and tuning. This article proposes a method that retains the strengths and avoids the weaknesses of the two approaches. The method leverages properties of the Cumulative Absolute Error per Ray (CAER) metric with respect to the errors of pose hypotheses of a 2D LIDAR sensor, and utilises scan–to–map-scan matching for fine(r) pose estimations. A large number of tests, in real and simulated conditions, involving disparate environments and sensor properties, illustrate that the proposed method outperforms state-of-the-art methods of both classes of solutions in terms of pose discovery rate and execution time. The source code is available for download.},
  archive   = {C_IROS},
  author    = {Alexandros Filotheou},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802235},
  month     = {10},
  pages     = {3268-3275},
  title     = {CBGL: Fast monte carlo passive global localisation of 2D LIDAR sensor},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning sampling distribution and safety filter for
autonomous driving with VQ-VAE and differentiable optimization.
<em>IROS</em>, 3260–3267. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sampling trajectories from a distribution followed by ranking them based on a specified cost function is a common approach in autonomous driving. Typically, the sampling distribution is hand-crafted (e.g a Gaussian, or a grid). Recently, there have been efforts towards learning the sampling distribution through generative models such as Conditional Variational Autoencoder (CVAE). However, these approaches fail to capture the multi-modality of the driving behaviour due to the Gaussian latent prior of the CVAE. Thus, in this paper, we re-imagine the distribution learning through vector quantized variational autoencoder (VQ-VAE), whose discrete latent-space is well equipped to capture multi-modal sampling distribution. The VQ-VAE is trained with demonstration data of optimal trajectories. We further propose a differentiable optimization based safety filter to minimally correct the VQ-VAE sampled trajectories to ensure collision avoidance. We use backpropagation through the optimization layers in a self-supervised learning set-up to learn good initialization and optimal parameters of the safety filter. We perform extensive comparisons with state-of-the-art CVAE-based baseline in dense and aggressive traffic scenarios and show a reduction of up to 12 times in collision-rate while being competitive in driving speeds.},
  archive   = {C_IROS},
  author    = {Simon Idoko and Basant Sharma and Arun Kumar Singh},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801610},
  month     = {10},
  pages     = {3260-3267},
  title     = {Learning sampling distribution and safety filter for autonomous driving with VQ-VAE and differentiable optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Look before you leap: Socially acceptable high-speed ground
robot navigation in crowded hallways. <em>IROS</em>, 3254–3259. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To operate safely and efficiently, autonomous warehouse/delivery robots must be able to accomplish tasks while navigating in dynamic environments and handling the large uncertainties associated with the motions/behaviors of other robots and/or humans. A key scenario in such environments is the hallway problem, where robots must operate in the same narrow corridor as human traffic going in one or both directions. Traditionally, robot planners have tended to focus on socially acceptable behavior in the hallway scenario at the expense of performance. This paper proposes a planner that aims to address the consequent &quot;robot freezing problem&quot; in hallways by allowing for &quot;peek-and-pass&quot; maneuvers. We then go on to demonstrate in simulation how this planner improves robot time to goal without violating social norms. Finally, we show initial hardware demonstrations of this planner in the real world, along with a novel STAR (Socially Trained Agile Robot) platform designed with human comfort in mind.},
  archive   = {C_IROS},
  author    = {Lakshay Sharma and Nicolaniello Buono and Ashton Flather and Xiaoyi Cai and Jonathan P. How},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801967},
  month     = {10},
  pages     = {3254-3259},
  title     = {Look before you leap: Socially acceptable high-speed ground robot navigation in crowded hallways},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Skill q-network: Learning adaptive skill ensemble for
mapless navigation in unknown environments. <em>IROS</em>, 3246–3253.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper focuses on the acquisition of mapless navigation skills within unknown environments. We introduce the Skill Q-Network (SQN), a novel reinforcement learning method featuring an adaptive skill ensemble mechanism. Unlike existing methods, our model concurrently learns a high-level skill decision process alongside multiple low-level navigation skills, all without the need for prior knowledge. Leveraging a tailored reward function for mapless navigation, the SQN is capable of learning adaptive maneuvers that incorporate both exploration and goal-directed skills, enabling effective navigation in new environments. Our experiments demonstrate that our SQN can effectively navigate complex environments, exhibiting a 40% higher performance compared to baseline models. Without explicit guidance, SQN discovers how to combine low-level skill policies, showcasing both goal-directed navigations to reach destinations and exploration maneuvers to escape from local minimum regions in challenging scenarios. Remarkably, our adaptive skill ensemble method enables zero-shot transfer to out-of-distribution domains, characterized by unseen observations from non-convex obstacles or uneven, subterranean-like environments. The project page is available at https://sites.google.com/view/skill-q-net.},
  archive   = {C_IROS},
  author    = {Hyunki Seong and David Hyunchul Shim},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801531},
  month     = {10},
  pages     = {3246-3253},
  title     = {Skill Q-network: Learning adaptive skill ensemble for mapless navigation in unknown environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neuro-explorer: Efficient and scalable exploration planning
via learned frontier regions. <em>IROS</em>, 3240–3245. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an efficient and scalable learning-based autonomous exploration system for mobile robots navi-gating unknown indoor environments. Our system incorporates three network models trained to identify the frontier region (FR), to evaluate the detected FR regions based on their proximity to the robot (A*-Net), and to measure the coverage reward at the FR regions (Viz-Net). Our method employs an active window of the map that moves along with the robot, offering scalable exploration capabilities while maintaining a high rate of exploration coverage owing to the two exploratory measures utilized by A*-Net (proximity) and Viz-Net (coverage). Consequently, Our system completes over 99% coverage in a large-scale benchmarking world, scaling up to 135m × +80m. In contrast, other state-of-the-art approaches completed only less than 40% of the same world with a 30% slower exploration speed than ours.},
  archive   = {C_IROS},
  author    = {Kyung Min Han and Young J. Kim},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802704},
  month     = {10},
  pages     = {3240-3245},
  title     = {Neuro-explorer: Efficient and scalable exploration planning via learned frontier regions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Magnetic field aided vehicle localization with acceleration
correction. <em>IROS</em>, 3234–3239. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel approach for vehicle localization by leveraging the ambient magnetic field within a given environment. Our approach involves introducing a global mathematical function for magnetic field mapping, combined with Euclidean distance-based matching technique for accurately estimating vehicle position in suburban settings. The mathematical function based map structure ensures efficiency and scalability of the magnetic field map, while the batch processing based localization provides continuity in pose estimation. Additionally, we establish a bias estimation pipeline for an onboard accelerometer by utilizing the updated poses obtained through magnetic field matching. Our work aims to showcase the potential utility of magnetic fields as supplementary aids to existing localization methods, particularly beneficial in scenarios where Global Positioning System (GPS) signal is restricted or where cost-effective navigation systems are required.},
  archive   = {C_IROS},
  author    = {Mrunmayee Deshpande and Manoranjan Majji and J. Humberto Ramos},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802833},
  month     = {10},
  pages     = {3234-3239},
  title     = {Magnetic field aided vehicle localization with acceleration correction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning autonomous driving from aerial imagery.
<em>IROS</em>, 3226–3233. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we consider the problem of learning end to end perception to control for ground vehicles solely from aerial imagery. Photogrammetric simulators allow the synthesis of novel views through the transformation of pre-generated assets into novel views. However, they have a large setup cost, require careful collection of data and often human effort to create usable simulators. We use a Neural Radiance Field (NeRF) as an intermediate representation to synthesize novel views from the point of view of a ground vehicle. These novel viewpoints can then be used for several downstream autonomous navigation applications. In this work, we demonstrate the utility of novel view synthesis though the application of training a policy for end to end learning from images and depth data. In a traditional real to sim to real framework, the collected data would be transformed into a visual simulator which could then be used to generate novel views. In contrast, using a NeRF allows a compact representation and the ability to optimize over the parameters of the visual simulator as more data is gathered in the environment. We demonstrate the efficacy of our method in a custom built mini-city environment through the deployment of imitation policies on robotic cars. We additionally consider the task of place localization and demonstrate that our method is able to relocalize the car in the real world.},
  archive   = {C_IROS},
  author    = {Varun Murali and Rosman Guy and Karaman Sertac and Daniela Rus},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801752},
  month     = {10},
  pages     = {3226-3233},
  title     = {Learning autonomous driving from aerial imagery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An observability constrained downward-facing
optical-flow-aided visual-inertial odometry. <em>IROS</em>, 3218–3225.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual-Inertial Odometry (VIO) has been widely used by autonomous drones as an onboard navigation method. However, it suffers from drifts especially in scenarios where the environments have few texture features such as an empty room with solid color walls. Optical flow sensors are another type of onboard sensor used by drones that face downward and measure the velocity by detecting changes in pixels between consecutive images, which don’t introduce accumulative error. In this work, we present an efficient tight-coupled estimator to improve the accuracy of VIO by fusing the measurements of a downward-facing optical flow sensor into the VIO framework consistently. We further analyze the observability of the estimators and prove that there are four unobservable directions in the ideal case and then we utilize OC-EKF to maintain the consistency of the estimator. Furthermore, we extend an adaptive weighting algorithm to the proposed method, which can better adapt to the scenes where feature tracking is less accurate. Finally, both simulation and real-world experiments demonstrate the feasibility of the proposed method.},
  archive   = {C_IROS},
  author    = {Dandi Liu and Jiahao Mei and Jin Zhou and Shuo Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802729},
  month     = {10},
  pages     = {3218-3225},
  title     = {An observability constrained downward-facing optical-flow-aided visual-inertial odometry},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rain-reaper: Unmasking LiDAR-based detector vulnerabilities
in rain. <em>IROS</em>, 3212–3217. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {LIDAR-based 3D object detection aims to enhance the situational awareness of autonomous vehicles. Despite recent advancements in this technology, there has been evidence that the susceptibility of 3D object detectors to signal spoofing is high, leading to the erroneous detection of &quot;ghost objects&quot; or the failure to detect genuine ones. While prior work has investigated the design of these new attacks and new defenses, the effect of weather conditions, which is a hot topic in autonomous vehicle research, on both attacks and defenses has never been studied. Inspired by this observation, in this paper, we present a novel genetic algorithm-based attack, entitled Rain-Reaper, that leverages on the effect of rain and identifies critical detection points used by 3D detectors. We show that adverse weather conditions not only diminish detection distance and accuracy but also expose the limitations of existing defenses. We have found that the unique characteristics of wet roads lead to underperforming defenses, thus, leading to a false sense of confidence in them. The effectiveness and efficiency of the attack and the robustness of the defenses have been evaluated with both simulated and real data. Our Rain-Reaper demonstrates a high attack success rate while successfully evading existing defenses with an adversarial point budget of up to 8.8 times smaller than previously demonstrated state-of-the-art attacks.},
  archive   = {C_IROS},
  author    = {Richard Capraru and Emil C. Lupu and Soteris Demetriou and Jian-Gang Wang and Boon Hee Soong},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801716},
  month     = {10},
  pages     = {3212-3217},
  title     = {Rain-reaper: Unmasking LiDAR-based detector vulnerabilities in rain},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual perception system for autonomous driving.
<em>IROS</em>, 3204–3211. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The recent surge in interest in autonomous driving is fueled by its rapidly developing capacity to enhance safety, efficiency, and convenience. A key component of autonomous driving technology lies in its perceptual systems, where advancements have led to more precise algorithms applicable to autonomous driving, such as vision-based Simultaneous Localization and Mapping (SLAM), object detection, and tracking algorithms. This work introduces a visual-based perception system for autonomous driving that integrates trajectory tracking and prediction of moving objects to prevent collisions while addressing the localization and mapping needs of autonomous driving. The system leverages motion cues from pedestrians to monitor and forecast their movements while simultaneously mapping the environment. This integrated approach resolves camera localization and tracks other moving objects in the scene, ultimately generating a sparse map to facilitate vehicle navigation. The performance, efficiency, and resilience of this approach are demonstrated through comprehensive evaluations of both simulated and real-world datasets.},
  archive   = {C_IROS},
  author    = {Qi Zhang and Siyuan Gou and Wenbin Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802028},
  month     = {10},
  pages     = {3204-3211},
  title     = {Visual perception system for autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Switching sampling space of model predictive path-integral
controller to balance efficiency and safety in 4WIDS vehicle navigation.
<em>IROS</em>, 3196–3203. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Four-wheel independent drive and steering vehicle (4WIDS Vehicle, Swerve Drive Robot) has the ability to move in any direction by its eight degrees of freedom (DoF) control inputs. Although the high maneuverability enables efficient navigation in narrow spaces, obtaining the optimal command is challenging due to the high dimension of the solution space. This paper presents a navigation architecture using the Model Predictive Path Integral (MPPI) control algorithm to avoid collisions with obstacles of any shape and reach a goal point. The key idea to make the problem easier is to explore the optimal control input in a reasonably reduced dimension that is adequate for navigation. Through evaluation in simulation, we found that the selecting sampling space of MPPI greatly affects navigation performance. In addition, our proposed controller which switches multiple sampling spaces according to the real-time situation can achieve balanced behavior between efficiency and safety.Source code is available at https://github.com/MizuhoAOKI/mppi_swerve_drive_ros.},
  archive   = {C_IROS},
  author    = {Mizuho Aoki and Kohei Honda and Hiroyuki Okuda and Tatsuya Suzuki},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802359},
  month     = {10},
  pages     = {3196-3203},
  title     = {Switching sampling space of model predictive path-integral controller to balance efficiency and safety in 4WIDS vehicle navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Domain adaptation in visual reinforcement learning via
self-expert imitation with purifying latent feature. <em>IROS</em>,
3189–3195. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generalizing visual reinforcement learning is fundamental to robot visual navigation, involving the acquisition of a policy from interactions with source environments to facilitate adaptation to analogous, yet unfamiliar target environments. Recent advancements capitalize on data augmentation techniques, self-supervised learning methods, and the generative adversarial network framework to train policy neural networks with enhanced generalizability. However, current methods, upon extracting domain-general latent features, further utilize these features to train the reinforcement learning policy, resulting in a decline in the performance of the learned policy guiding the agent to accomplish tasks. To tackle these challenges, a framework of self-expert imitation with purifying latent features was devised, empowering the policy to achieve robust and stable zero-shot generalization performance in visually similar domains previously unseen, without diminishing the performance of guiding the agent to accomplish tasks. The extraction method of domain-general latent features is proposed to enhance their quality based on the variational autoencoder. Extensive experiments have shown that our policy, compared with state-of-the-art counterparts, does not diminish the performance of the policy guiding the agent to accomplish tasks after generalization.},
  archive   = {C_IROS},
  author    = {Lin Chen and Jianan Huang and Zhen Zhou and Yaonan Wang and Yang Mo and Zhiqiang Miao and Kai Zeng and Mingtao Feng and Danwei Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801440},
  month     = {10},
  pages     = {3189-3195},
  title     = {Domain adaptation in visual reinforcement learning via self-expert imitation with purifying latent feature},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time hazard prediction in connected autonomous
vehicles: A digital twin approach. <em>IROS</em>, 3182–3188. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The growing interest in connected autonomous vehicles (CAVs) has intensified the focus on technologies and algorithms that enhance behavior, comfort, and safety. Among these, the concept of Digital Twins (DT) represents an emerging field of research that is now beginning to be applied to autonomous systems. Traditional Advanced Driver-Assistance Systems (ADAS) can prevent real-time collisions using sensor data. However, we propose that employing a DT can enable the accounting for complex, simulated decisions before they occur in reality. This paper introduces an initial model of a Digital Twin, founded on an internal simulator aligned with vehicle control architecture, for real-time hazard prediction and effective decision-making. Our DT synchronizes with the vehicle’s state to simulate various hazardous scenarios in advance, allowing for preemptive actions. To support our hypothesis, we introduce an algorithm for the early detection of potential collisions between CAVs and pedestrians through the unsupervised simulation of diverse traffic scenarios. This solution integrates the CORTEX cognitive architecture with CARLA for internal simulation, leveraging probabilistic models to select optimal scenarios. Employing data from external pedestrian cameras, a particle filter predicts the most probable pedestrian trajectories via DT simulations, thereby informing safe maneuvers. Although the algorithm itself is established, the novelty of our approach lies in incorporating a simulator within the digital twin. This simulator, informed by real-time data on the vehicle’s and environment’s state, facilitates appropriate responses to unpredictable behaviors. We have conducted extensive tests with an actual autonomous electric vehicle on a university campus to validate the system’s predictive and adaptive functions.},
  archive   = {C_IROS},
  author    = {Sergio Barroso and Noé Zapata and Gerardo Pérez and Pablo Bustos and Pedro Núñez},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802333},
  month     = {10},
  pages     = {3182-3188},
  title     = {Real-time hazard prediction in connected autonomous vehicles: A digital twin approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time path generation and alignment control for
autonomous curb following. <em>IROS</em>, 3175–3181. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Curb following is a key technology for autonomous road sweeping vehicles. Currently, existing implementations primarily involve pre-recording waypoints during human driving and subsequently retracing them autonomously. Moreover, existing research related to this topic predominately focuses on curb detection for driver assistance, yet the resultant curb detection outcomes remain underutilized in the development of autonomous curb following systems. To fill this gap, this paper proposes a real-time path generation and alignment control approach to facilitate autonomous curb following. Firstly, a segmented path generation algorithm is introduced that progressively generates reference path segments while ensuring the overall continuity of the reference path. Secondly, a parameterized alignment control algorithm is developed to accurately navigate the vehicle along the planned reference path with proved stability. Real public road experiments have been conducted to validate the proposed approach. The experimental results demonstrate the efficacy of the proposed methodologies across various curb following scenarios, including common concave, convex, and straight-concave curbs, thereby showcasing the practical viability of our methods in real-world applications.},
  archive   = {C_IROS},
  author    = {Yuanzhe Wang and Yunxiang Dai and Danwei Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801832},
  month     = {10},
  pages     = {3175-3181},
  title     = {Real-time path generation and alignment control for autonomous curb following},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reward-field guided motion planner for navigation with
limited sensing range. <em>IROS</em>, 3167–3174. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we focus on improving planning efficiency for ground vehicles in navigation and exploration tasks where the environment is unknown or partially known, leading to frequent updates of the navigational goal as new sensory information is acquired. Asymptotically optimal motion planners like RRT* or FMT* can be used to plan the sequence of actions the robot can follow to achieve its current goal. Frequent replanning of the whole action sequence becomes computationally demanding when actions are not executed precisely because of limited information about the foreground terrain. The decoupled approach can decrease the computational burden with separated path planning and path following; however, it might lead to suboptimal solutions. Therefore, we propose a novel approach based on generating a reusable reward function that guides a fast sampling-based motion planner. The proposed method provides improved results in navigation scenarios compared to the former approaches, and it led to about 7% faster autonomous exploration than the decoupled approach. The present results support the suitability of the proposed method in navigation tasks with continuously updated navigation goals.},
  archive   = {C_IROS},
  author    = {Jan Bayer and Jan Faigl},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801843},
  month     = {10},
  pages     = {3167-3174},
  title     = {Reward-field guided motion planner for navigation with limited sensing range},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Perception for connected autonomous vehicles under adverse
weather conditions. <em>IROS</em>, 3161–3166. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous Vehicles (AVs) have recently attracted considerable attention due to their potential to significantly reduce road accidents and improve people’s lives. However, they rely solely on the data collected by their mounted sensors to make predictions, which can lead to inaccurate results if a sensor becomes occluded or damaged. This issue can be addressed by employing Vehicle-to-Vehicle communication, which allows a Connected Autonomous Vehicle (CAV) to interact with other CAVs within its field of view and exchange information about their surrounding objects. Existing research on cooperative perception has primarily focused on clear weather scenarios, with limited exploration into adverse weather conditions. This paper demonstrates the necessity of Vehicle-to-Vehicle communication by showcasing its benefits in maintaining high accuracy under adverse weather conditions. A collaborative perception system is introduced and its performance in foggy weather scenarios is assessed to further improve adverse weather perception. The pipeline of the network combines state-of-the-art methods for accurate object detection. Specifically, with PointPillars as the backbone, the Spatial-wise Adaptive Feature Fusion method is used to aggregate information from different vehicles. The model is trained on the large-scale dataset OPV2V and evaluated on modified data to simulate fog. The experiments show that cooperative perception can maintain high detection accuracy even in challenging weather conditions. Finally, a comparative analysis of LiDAR detectors for cooperative perception in bad weather conditions is presented.},
  archive   = {C_IROS},
  author    = {Dimitra Tsakmakopoulou and Konstantinos Moustakas},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801295},
  month     = {10},
  pages     = {3161-3166},
  title     = {Perception for connected autonomous vehicles under adverse weather conditions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DriVLMe: Enhancing LLM-based autonomous driving agents with
embodied and social experiences. <em>IROS</em>, 3153–3160. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advancements in foundation models (FMs) have unlocked new prospects in autonomous driving, yet the experimental settings of these studies are preliminary, oversimplified, and fail to capture the complexity of real-world driving scenarios in human environments. It remains under-explored whether FM agents can handle long-horizon navigation tasks with free-from dialogue and deal with unexpected situations caused by environmental dynamics or task changes. To explore the capabilities and boundaries of FMs faced with the challenges above, we introduce DriVLMe, a video-language-model-based agent to facilitate natural and effective communication between humans and autonomous vehicles that perceive the environment and navigate. We develop DriVLMe from both embodied experiences in a simulated environment and social experiences from real human dialogue. While DriVLMe demonstrates competitive performance in both open-loop benchmarks and closed-loop human studies, we reveal several limitations and challenges, including unacceptable inference time, imbalanced training data, limited visual understanding, challenges with multi-turn interactions, simplified language generation from robotic experiences, and difficulties in handling on-the-fly unexpected situations like environmental dynamics and task changes. Nevertheless, DriVLMe offers a promising new direction for autonomous driving agents that need to navigate not just complex environments but also complex social interactions.},
  archive   = {C_IROS},
  author    = {Yidong Huang and Jacob Sansom and Ziqiao Ma and Felix Gervits and Joyce Chai},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802555},
  month     = {10},
  pages     = {3153-3160},
  title     = {DriVLMe: Enhancing LLM-based autonomous driving agents with embodied and social experiences},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flexible informed trees (FIT*): Adaptive batch-size approach
in informed sampling-based path planning. <em>IROS</em>, 3146–3152. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In path planning, anytime almost-surely asymptotically optimal planners dominate the benchmark of sampling-based planners. A notable example is Batch Informed Trees (BIT*), where planners iteratively determine paths to batches of vertices within the exploration area. However, utilizing a consistent batch size is inefficient for initial pathfinding and optimal performance, it relies on effective task allocation. This paper introduces Flexible Informed Trees (FIT*), a sampling-based planner that integrates an adaptive batch-size method to enhance the initial path convergence rate. FIT* employs a flexible approach in adjusting batch sizes dynamically based on the inherent dimension of the configuration spaces and the hypervolume of the n-dimensional hyperellipsoid. By applying dense and sparse sampling strategy, FIT* improves convergence rate while finding successful solutions faster with lower initial solution cost. This method enhances the planner’s ability to handle confined, narrow spaces in the initial finding phase and increases batch vertices sampling frequency in the optimization phase. FIT* outperforms existing single-query, sampling-based planners on the tested problems in R2 to R8, and was demonstrated on a real-world mobile manipulation task.},
  archive   = {C_IROS},
  author    = {Liding Zhang and Zhenshan Bing and Kejia Chen and Lingyun Chen and Kuanqi Cai and Yu Zhang and Fan Wu and Peter Krumbholz and Zhilin Yuan and Sami Haddadin and Alois Knoll},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802466},
  month     = {10},
  pages     = {3146-3152},
  title     = {Flexible informed trees (FIT*): Adaptive batch-size approach in informed sampling-based path planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sampling-based motion planning for optimal probability of
collision under environment uncertainty. <em>IROS</em>, 3138–3145. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion planning is a fundamental capability in robotics applications. Real-world scenarios can introduce uncertainty to the motion planning problem. In this work we study environment uncertainty in general high-dimensional problems wherein the choice of appropriate metrics and formulations are shown to have significant effect on the probability of collision of the solution path. Several practically motivated cost functions have been proposed in literature to model and solve the problem but are shown in this work to suffer from higher probabilities of collision. The current work presents a theoretically sound formulation that was first mentioned in previous work on minimum constraint removal. In this work, approximating the optimal problem is shown to be better in achieving lower probability of collision. To demonstrate the formulation in a sampling-based setting, a mixed integer linear program seeded by greedy search over a roadmap with sampled environments is used to report paths with low probability of collision. Compared against minimizing the sum and minimizing max probability cost functions on a seven degree-of-freedom robotic arm in uncertain environments, we show clear benefits and promise towards motion planning for optimal probability of collision.},
  archive   = {C_IROS},
  author    = {Hao Lu and Hanna Kurniawati and Rahul Shome},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801890},
  month     = {10},
  pages     = {3138-3145},
  title     = {Sampling-based motion planning for optimal probability of collision under environment uncertainty},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust precision landing of a quadrotor with online temporal
scaling adaptation of dynamic movement primitives. <em>IROS</em>,
3130–3137. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we address the challenges of robust precision landing maneuvers for a quadrotor on both stationary and moving ground targets in the presence of disturbances that can cause the quadrotor to deviate from its desired trajectory, leading to maneuver failure. To overcome this, we propose a novel online adaptive trajectory planning approach based on the online temporal scaling adaptation of dynamic movement primitives (DMPs). This adaptation enables the desired trajectory to be dynamically adjusted in response to tracking errors and the goal’s state. Consequently, our proposed approach enhances accuracy, precision, and safety during landing maneuvers. The effectiveness of the approach is evaluated through comprehensive experiments conducted in both physical simulations and real-world environments, covering various disturbance scenarios.},
  archive   = {C_IROS},
  author    = {Kongkiat Rothomphiwat and Prakarn Jaroonsorn and Pakpoom Kriengkomol and Poramate Manoonpong},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801311},
  month     = {10},
  pages     = {3130-3137},
  title     = {Robust precision landing of a quadrotor with online temporal scaling adaptation of dynamic movement primitives},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient path planning for modular reconfigurable robots.
<em>IROS</em>, 3123–3129. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Industrial robots are essential for modern production but often struggle to adapt to new tasks. Modular (reconfigurable) robots can overcome this challenge by eliminating the need to replace the whole robot. However, finding the optimal assembly for a task remains difficult because a valid path has to be computed for each generated assembly – consuming a significant fraction of the computation time. Similar to online path planning, where previous approaches adapt known paths to a changing environment, we show that transferring paths from previously considered module assemblies accelerates path planning for the next assemblies. On average, our method reduces the planning time for single-goal tasks by 50%. The usefulness of our method is evaluated by integrating it in a genetic algorithm (GA) for optimizing assemblies and evaluating it on our benchmark suite CoBRA. Within the optimization loop for modular robots, the time used to check a single assembly is shortened by up to 50%.},
  archive   = {C_IROS},
  author    = {Matthias Mayer and Zihao Li and Matthias Althoff},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801534},
  month     = {10},
  pages     = {3123-3129},
  title     = {Efficient path planning for modular reconfigurable robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Energy-optimized planning in non-uniform wind fields with
fixed-wing aerial vehicles. <em>IROS</em>, 3116–3122. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fixed-wing small uncrewed aerial vehicles (sUAVs) possess the capability to remain airborne for extended durations and traverse vast distances. However, their operation is susceptible to wind conditions, particularly in regions of complex terrain where high wind speeds may push the aircraft beyond its operational limits, potentially raising safety concerns. Moreover, wind impacts the energy required to follow a path, especially in locations where the wind direction and speed are not favorable. Incorporating wind information into mission planning is essential to ensure both safety and energy efficiency. In this paper, we propose a sampling-based planner using the kinematic Dubins aircraft paths with respect to the ground, to plan energy-efficient paths in non-uniform wind fields. We study the characteristics of the planner with synthetic and real-world wind data and compare its performance against baseline cost and path formulations. We demonstrate that the energy-optimized planner effectively utilizes updrafts to minimize energy consumption, albeit at the expense of increased travel time. The ground-relative path formulation facilitates the generation of safe trajectories onboard sUAVs within reasonable computational timeframes.},
  archive   = {C_IROS},
  author    = {Yufei Duan and Florian Achermann and Jaeyoung Lim and Roland Siegwart},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801294},
  month     = {10},
  pages     = {3116-3122},
  title     = {Energy-optimized planning in non-uniform wind fields with fixed-wing aerial vehicles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sequential convex programming for time-optimal quadrotor
waypoint flight. <em>IROS</em>, 3108–3115. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Agile flight is significant for target tracking, search and rescue, and delivery applications. To achieve agile flight, we can exploit the actuator’s potential by utilizing the full dynamics of the quadrotor. However, the 6-degrees-of-freedom dynamics render the optimization problem non-convex, and thus computationally intractable. To tackle this issue, we convert the original non-convex optimal control problem (OCP) into a convex subproblem and use the sequential convex programming (SCP) algorithm to iteratively solve the subproblems. Moreover, the state-triggered constraints are proposed to simultaneously optimize the time allocation of the waypoint and the trajectory itself. The numerical and physical experiment results1 show that the SCP algorithm can significantly reduce the computing time while ensuring a satisfactory solution.},
  archive   = {C_IROS},
  author    = {Zhipeng Shen and Guanzhong Zhou and Hailong Huang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802749},
  month     = {10},
  pages     = {3108-3115},
  title     = {Sequential convex programming for time-optimal quadrotor waypoint flight},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An optimization-based planner with b-spline parameterized
continuous-time reference signals. <em>IROS</em>, 3100–3107. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For the cascaded planning and control modules implemented for robot navigation, the frequency gap between the planner and controller has received limited attention. In this study, we introduce a novel B-spline parameterized optimization-based planner (BSPOP) designed to address the frequency gap challenge with limited onboard computational power in robots. The proposed planner generates continuous-time control inputs for low-level controllers running at arbitrary frequencies to track. Furthermore, when considering the convex control action sets, BSPOP uses the convex hull property to automatically constrain the continuous-time control inputs within the convex set. Consequently, compared with the discrete-time optimization-based planners, BSPOP reduces the number of decision variables and inequality constraints, which improves computational efficiency as a byproduct. Simulation results demonstrate that our approach can achieve a comparable planning performance to the high-frequency baseline optimization-based planners while demanding less computational power. Both simulation and experiment results show that the proposed method performs better in planning compared with baseline planners in the same frequency.},
  archive   = {C_IROS},
  author    = {Chuyuan Tao and Sheng Cheng and Fanxin Wang and Yang Zhao and Naira Hovakimyan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802083},
  month     = {10},
  pages     = {3100-3107},
  title     = {An optimization-based planner with B-spline parameterized continuous-time reference signals},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A safe and efficient timed-elastic-band planner for
unstructured environments. <em>IROS</em>, 3092–3099. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In unstructured environments with complex obstacles and obscure road boundaries, the local planner faces more severe challenges in terms of safety and real-time performance. In order to fulfill these emerging requirements, we propose a novel Timed-Elastic-Band approach for unstructured environments, abbreviated as TEB-U. This approach incorporates a free space extraction optimization module for 2D occupancy grid maps, which efficiently transforms irregular free space boundaries into polygons and restrains robots within the boundaries. Moreover, a dynamic global point adjustment module is designed to adaptively correct the trajectory points obtained from the global planner, thereby enabling robots to travel along the centerline of free space and providing a better initial trajectory for subsequent modules. To reduce the computational cost, we replace the obstacle constraint of TEB with the boundary constraint in hyper-graph optimization. We evaluate our planner in three distinct scenarios, and the results show that TEB-U improves the average success rate by 21% and reduces the planning time by 23% compared to TEB in unstructured road, which demonstrates its safety and efficiency.},
  archive   = {C_IROS},
  author    = {Haoyu Xi and Wei Li and Fangzhou Zhao and Liang Chen and Yu Hu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802811},
  month     = {10},
  pages     = {3092-3099},
  title     = {A safe and efficient timed-elastic-band planner for unstructured environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A generic trajectory planning method for constrained
all-wheel-steering robots. <em>IROS</em>, 3084–3091. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a generic trajectory planning method for wheeled robots with fixed steering axes while the steering angle of each wheel is constrained. In the existing literatures, All-Wheel-Steering (AWS) robots, incorporating modes such as rotation-free translation maneuvers, in-situ rotational maneuvers, and proportional steering, exhibit inefficient performance due to time-consuming mode switches. This inefficiency arises from wheel rotation constraints and inter-wheel cooperation requirements. The direct application of a holonomic moving strategy can lead to significant slip angles or even structural failure. Additionally, the limited steering range of AWS wheeled robots exacerbates non-linearity characteristics, thereby complicating control processes. To address these challenges, we developed a novel planning method termed Constrained AWS (C-AWS), which integrates second-order discrete search with predictive control techniques. Experimental results demonstrate that our method adeptly generates feasible and smooth trajectories for C-AWS while adhering to steering angle constraints. Code and video can be found at https://github.com/Rex-sys-hk/AWSPlanning.},
  archive   = {C_IROS},
  author    = {Ren Xin and Hongji Liu and Yingbing Chen and Jie Cheng and Sheng Wang and Jun Ma and Ming Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801878},
  month     = {10},
  pages     = {3084-3091},
  title     = {A generic trajectory planning method for constrained all-wheel-steering robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing safety via deep reinforcement learning in
trajectory planning for agile flights in unknown environments.
<em>IROS</em>, 3076–3083. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unmanned aerial vehicles (UAVs), known for their agile flight capabilities, require safe trajectory planning to achieve high-speed flights. This is necessary to swiftly evade obstacles and adapt trajectories under hard real-time constraints. These adjustments are essential to generate viable paths that prevent collisions while maintaining high speeds with minimal tracking errors. This paper addresses the challenge of enhancing the safety of agile trajectory planning. The proposed method combines a supervised learning approach, as teacher policy, with deep reinforcement learning (DRL), as student policy. Initially, we train the teacher policy using a path planning algorithm that prioritizes safety while minimizing jerk and flight time. Then, we use this policy to guide the learning of the student policy in various unknown environments. Testing in simulation demonstrates noteworthy advancements, including an 80% reduction in tracking error, a 31% decrease in flight time, a 19% increase in high-speed duration, and a success rate improvement from 50% to 100%, as compared to baseline methods.},
  archive   = {C_IROS},
  author    = {Lidia Rocha and Jorge Bidinotto and Fredrik Heintz and Mattias Tiger and Kelen Vivaldini},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801910},
  month     = {10},
  pages     = {3076-3083},
  title     = {Enhancing safety via deep reinforcement learning in trajectory planning for agile flights in unknown environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning-informed long-horizon navigation under uncertainty
for vehicles with dynamics. <em>IROS</em>, 3069–3075. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel approach to learning-augmented, long-horizon navigation under uncertainty in large-scale environments in which considering the robot dynamics is essential for informing good behavior. Our approach tightly integrates sampling-based motion planning, which computes dynamically feasible routes to the goal through different unexplored boundaries, and a high-level planner that leverages predictions about unseen space to select a route that best makes progress toward the unseen goal. Owing to its ability to understand the impacts of the robot’s dynamics on how it should attempt to reach the goal, our approach achieves both higher reliability and improved navigation performance compared to competitive learning-informed and non-learned baselines in simulated office-building-like environments.},
  archive   = {C_IROS},
  author    = {Abhish Khanal and Hoang-Dung Bui and Erion Plaku and Gregory J. Stein},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801880},
  month     = {10},
  pages     = {3069-3075},
  title     = {Learning-informed long-horizon navigation under uncertainty for vehicles with dynamics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Local path planning among pushable objects based on
reinforcement learning. <em>IROS</em>, 3062–3068. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce a method to tackle the problem of robot local path planning among pushable objects –an open problem in robotics. In particular, we simultaneously train multiple agents in a physics-based simulation environment, utilizing an Advantage Actor-Critic algorithm coupled with a deep neural network. The developed online policy enables these agents to push obstacles in ways that are not limited to axial alignments, adapt to unforeseen changes in obstacle dynamics instantaneously, and effectively tackle local path planning in confined areas. We tested the method in various simulated environments to prove the adaptation effectiveness to various unseen scenarios in unfamiliar settings. Moreover, we have successfully applied this policy on an actual quadruped robot, confirming its capability to handle the unpredictability and noise associated with real-world sensors and the inherent uncertainties in unexplored object-pushing tasks.},
  archive   = {C_IROS},
  author    = {Linghong Yao and Valerio Modugno and Andromachi Maria Delfaki and Yuanchang Liu and Danail Stoyanov and Dimitrios Kanoulas},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802257},
  month     = {10},
  pages     = {3062-3068},
  title     = {Local path planning among pushable objects based on reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Planning for long-term monitoring missions in time-varying
environments. <em>IROS</em>, 3055–3061. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent years have seen autonomous robots deployed in long-term missions across an ever-increasing breadth of domains. We consider robots deployed over a sequence of finite-horizon missions in the same environment, with the objective of maximising the value from observations of some unknown spatiotemporal process. This work is motivated by applications such as ecological monitoring, in which a robot might be repeatedly deployed in the field over weeks or months with the task of modelling processes of scientific interest. We formalise the problem of long-term monitoring over multiple finite-horizon missions as a Markov decision process with a partially unknown state, and present an online planning approach to address it. Our approach uses a spatiotemporal Gaussian process to model the environment and make predictions about unvisited states, integrating this with a belief-based Monte Carlo tree search algorithm which decides where the robot should go next. We demonstrate the strengths of our framework empirically through a series of experiments using synthetic data as well as real acoustic data from monitoring of bioactivity in coral reefs.},
  archive   = {C_IROS},
  author    = {Alex Stephens and Bruno Lacerda and Nick Hawes},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802248},
  month     = {10},
  pages     = {3055-3061},
  title     = {Planning for long-term monitoring missions in time-varying environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural trajectory model: Implicit neural trajectory
representation for trajectories generation. <em>IROS</em>, 3049–3054.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The multi-agent trajectory planning problem is a difficult problem in robotics due to its computational complexity and real-world environment complexity with uncertainty, non-linearity, and real-time requirements. Many existing solutions are either search-based or optimization-based approaches with simplified assumptions of environment, limited planning speed, and limited scalability in the number of agents. In this work, we first attempt to reformulate single-agent and multi-agent trajectory planning problems as query problems over an implicit neural representation of trajectories. We formulate such implicit representations as Neural Trajectory Models (NTM) which can be queried to generate nearly optimal trajectory in complex environments. We conduct experiments in simulation environments and demonstrate that NTM achieve (1) sub-millisecond planning time using GPUs, (2) almost avoiding all collisions, and (3) generating almost shortest paths. We also demonstrate that the same NTM framework can also be used for refining low-quality and conflicting multi-agent trajectories into nearly optimal solutions efficiently. (Open source code is available at https://github.com/laser2099/neural-trajectory-model)},
  archive   = {C_IROS},
  author    = {Zihan Yu and Yuqing Tang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802789},
  month     = {10},
  pages     = {3049-3054},
  title     = {Neural trajectory model: Implicit neural trajectory representation for trajectories generation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Time-optimal path parameterization for cooperative multi-arm
robotic systems with third-order constraints. <em>IROS</em>, 3043–3048.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a time-optimal path parameterization (TOPP) method for cooperative multi-arm robotic systems (MARS) manipulating heavy objects with third-order constraints that include jerk, torque rate and wrench rate limits. The method is based on a problem reformulation as a sequential linear program and provides a unified planning approach that is faster than previous convex optimization techniques. The equivalence to a reachability-based TOPP is shown and simulation results for a cooperative MARS consisting of two 7 degree of freedom (DOF) robots and a tightly grasped object with 6 DOFs are provided.},
  archive   = {C_IROS},
  author    = {Maximilian Dio and Knut Graichen and Andreas Völz},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802342},
  month     = {10},
  pages     = {3043-3048},
  title     = {Time-optimal path parameterization for cooperative multi-arm robotic systems with third-order constraints},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active information gathering for long-horizon navigation
under uncertainty by learning the value of information. <em>IROS</em>,
3036–3042. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the task of long-horizon navigation in partially mapped environments for which active gathering of information about faraway unseen space is essential for good behavior. We present a novel planning strategy that, at training time, affords tractable computation of the value of information associated with revealing potentially informative regions of unseen space, data used to train a graph neural network to predict the goodness of temporally-extended exploratory actions. Our learning-augmented model-based planning approach predicts the expected value of information of revealing unseen space and is capable of using these predictions to actively seek information and so improve long-horizon navigation. Across two simulated office-like environments, our planner outperforms competitive learned and non-learned baseline navigation strategies, achieving improvements of up to 63.76% and 36.68%, demonstrating its capacity to actively seek performance-critical information.},
  archive   = {C_IROS},
  author    = {Raihan Islam Arnob and Gregory J. Stein},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801868},
  month     = {10},
  pages     = {3036-3042},
  title     = {Active information gathering for long-horizon navigation under uncertainty by learning the value of information},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel geometrical structure robot hand for linear-parallel
pinching and coupled self-adaptive hybrid grasping. <em>IROS</em>,
3030–3035. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current robot hand grippers capable of self-adaptive or coupled grasping often cannot perform linear-parallel pinching at the physical end of the gripper, which is widely used in industrial applications. For this reason, this paper introduces a gripper with hybrid grasping modes— the LPCSA hand. It can achieve three grasping modes: linear-parallel pinching, coupled, and self-adaptive grasping. The design cleverly couples two kinds of Chebyshev linear mechanisms to enable flat movement at the end of the finger. It also utilizes the deformability of the parallelogram to achieve self-adaptive grasping. Furthermore, the gripper uses an idle stroke and a special component to facilitate the switch between the three modes. The linear-parallel pinching function is suitable for pinching objects of different sizes on the desktop. The self-adaptive grasping mode can adapt to objects of various shapes and sizes. The coupled grasping mode enables fast grasping of irregular objects. This paper also analyzes the kinematics and dynamics of the LPCSA hand. Combined with experiments, it demonstrates that the LPCSA hand has a wide range of grasping space and stable performance.},
  archive   = {C_IROS},
  author    = {Shi Chen and Bihao Zhang and Kehan Feng and Yizhou Wang and Jiayun Li and Wenzeng Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802602},
  month     = {10},
  pages     = {3030-3035},
  title     = {A novel geometrical structure robot hand for linear-parallel pinching and coupled self-adaptive hybrid grasping},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Task-oriented design method for monolithic flexible hands
with wire drive systems. <em>IROS</em>, 3022–3029. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper discusses a novel task-oriented design method for wire-driven flexible hands. For a monolithic hand fabricated using 3D printing, an analytical design method is proposed to enable it to perform the given tasks. First, the wiring-synergy equation, which relates the parameters of the hand mechanism, the wire tension, and the generated posture is derived based on an analytical model of a hand with wire drive systems. Next, the posture-synergy equation is derived, using principal component analysis for multiple desired postures given to perform a task. Based on the isomorphism of the mathematical structure in the two synergy equations, a method for designing a hand is developed. By quantitatively evaluating the posture reproducibility with respect to the number of wire drive systems, this method can analytically determine the mechanism parameters and wire tension for the desired postures. Subsequently, the proposed method is validated through case studies. Finally, a hand for an in-hand manipulation task is developed, and the feasibility of the proposed method is validated experimentally. The method potentially contributes to expediting the design procedure, increasing the accuracy of the posture reproduction, and reducing the number of actuators.},
  archive   = {C_IROS},
  author    = {Rina Kusuhara and Mitsuru Higashimori},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802550},
  month     = {10},
  pages     = {3022-3029},
  title     = {Task-oriented design method for monolithic flexible hands with wire drive systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing object grasping efficiency with deep learning and
post-processing for multi-finger robotic hands. <em>IROS</em>,
3014–3021. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper builds upon the well-established ML-based grasping technique, known as the Grasp-Rectangle (GR) method. The original GR method made two simplifying assumptions: it was designed exclusively for two-finger grippers, and it assumed that the gripper would approach objects solely from a top-down perspective on a horizontal surface. We have extended the GR method, for a multi-finger hand beyond these assumptions to (1) enable grasping from top and side views and (2) engage multiple points of contact, enhancing the algorithm’s overall performance. Our approach leverages geometric cues extracted from object images to calculate the optimal grasp pose and contact points, thereby enhancing grasp reliability. Extensive testing was conducted using a 7DOF robotic arm equipped with a 7-DOF 3-finger gripper. We achieved an accuracy of 98.6% on the Cornell Grasping Dataset with a processing time of 120 milliseconds. Furthermore, when assessing object grasping from both top and side perspectives, our algorithm delivered successful grasps at rates of 95% and 96%, respectively. These findings are rooted in a comprehensive series of tests performed across a diverse array of objects.},
  archive   = {C_IROS},
  author    = {Pouya Samandi and Kamal Gupta and Mehran Mehrandezh},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801496},
  month     = {10},
  pages     = {3014-3021},
  title     = {Enhancing object grasping efficiency with deep learning and post-processing for multi-finger robotic hands},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GripFlexer: Development of hybrid gripper with a novel shape
that can perform in narrow spaces. <em>IROS</em>, 3008–3013. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, the role of robots across industries has become increasingly diverse, and they are now required to perform complex missions beyond simple repetitive tasks. However, robots used in confined spaces that humans cannot reach or in disaster field missions have challenges in performing various tasks due to their small size. In this study, we developed a compact hybrid gripper that fuses a multi-finger gripper and a jamming gripper to perform various tasks in a confined environment. Such a hybrid gripper can have both the strengths of a multi finger gripper that can perform various tasks and a jamming gripper that can effectively handle irregular small objects. In this study, we developed a hybrid gripper &quot;GripFlexer&quot; based on theoretical analysis and confirmed its performance through experiments by taking the task of turning a circular doorknob, which is one of the most difficult tasks in disaster sites, as the final target task. We also confirmed that the two grippers of GripFlexer can interact by showing performance improvement effects when two grippers are operated simultaneously.},
  archive   = {C_IROS},
  author    = {Donghyun Kim and Sunghyun Choi and Bongsub Song and Jinhyeok Song and Jingon Yoon and Dongwon Yun},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802638},
  month     = {10},
  pages     = {3008-3013},
  title     = {GripFlexer: Development of hybrid gripper with a novel shape that can perform in narrow spaces},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 6-DoF grasp detection in clutter with enhanced receptive
field and graspable balance sampling. <em>IROS</em>, 3000–3007. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {6-DoF grasp detection of small-scale grasps is crucial for robots to perform specific tasks. This paper focuses on enhancing the recognition capability of small-scale grasping, aiming to improve the overall accuracy of grasping prediction results and the generalization ability of the network. We propose an enhanced receptive field method that includes a multi-radii cylinder grouping module and a passive attention module. This method enhances the receptive field area within the graspable space and strengthens the learning of graspable features. Additionally, we design a graspable balance sampling module based on a 3D segmentation network, which enables the network to focus on features of small objects, thereby improving the recognition capability of small-scale grasping. Our network achieves state-of-the-art performance on the GraspNet-1Billion dataset, with an overall improvement of approximately 10% in average precision@k (AP). Furthermore, we deployed our grasp detection model on pybullet grasping platform and in real-world scenarios, which validates the effectiveness of our method.},
  archive   = {C_IROS},
  author    = {Hanwen Wang and Ying Zhang and Yunlong Wang and Jian Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802025},
  month     = {10},
  pages     = {3000-3007},
  title     = {6-DoF grasp detection in clutter with enhanced receptive field and graspable balance sampling},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward an analytic theory of intrinsic robustness for
dexterous grasping. <em>IROS</em>, 2992–2999. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Conventional approaches to grasp planning re- quire perfect knowledge of an object’s pose and geometry. Uncertainties in these quantities induce uncertainties in the quality of planned grasps, which can lead to failure. Classically, grasp robustness refers to the ability to resist external disturbances after grasping an object. In contrast, this work studies robustness to intrinsic sources of uncertainty like object pose or geometry affecting grasp planning before execution. To do so, we develop a novel analytic theory of grasping that reasons about this intrinsic robustness by characterizing the effect of friction cone uncertainty on a grasp’s force closure status. We apply this result in two ways. First, we analyze the theoretical guarantees on intrinsic robustness of two grasp metrics in the literature, the classical Ferrari-Canny metric and more recent min-weight metric. We validate these results with hardware trials that compare grasps synthesized with and without robustness guarantees, showing a clear improvement in success rates. Second, we use our theory to develop a novel analytic notion of probabilistic force closure, which we show can generate unique, uncertainty-aware grasps in simulation.},
  archive   = {C_IROS},
  author    = {Albert H. Li and Preston Culbertson and Aaron D. Ames},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802185},
  month     = {10},
  pages     = {2992-2999},
  title     = {Toward an analytic theory of intrinsic robustness for dexterous grasping},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Speeding up 6-DoF grasp sampling with quality-diversity.
<em>IROS</em>, 2985–2991. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in AI have led to significant results in robotic learning, including natural language-conditioned planning and efficient optimization of controllers using generative models. However, the interaction data remains the bottleneck for generalization. Getting data for grasping is a critical challenge, as this skill is required to complete many manipulation tasks. Quality-Diversity (QD) algorithms optimize a set of solutions to get diverse, high-performing solutions to a given problem. This paper investigates how QD can be combined with priors to speed up the generation of diverse grasps poses in simulation compared to standard 6-DoF grasp sampling schemes. Experiments conducted on 4 grippers with 2-to-5 fingers on standard objects show that QD outperforms commonly used methods by a large margin. Further experiments show that QD optimization automatically finds some efficient priors that are usually hard coded. The deployment of generated grasps on a 2-finger gripper and an Allegro hand shows that the diversity produced maintains sim-to-real transferability. We believe these results to be a significant step toward the generation of large datasets that can lead to robust and generalizing robotic grasping policies.},
  archive   = {C_IROS},
  author    = {Johann Huber and François Hélénon and Mathilde Kappel and Elie Chelly and Mahdi Khoramshahi and Faïz Ben Amar and Stéphane Doncieux},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801391},
  month     = {10},
  pages     = {2985-2991},
  title     = {Speeding up 6-DoF grasp sampling with quality-diversity},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MultiGripperGrasp: A dataset for robotic grasping from
parallel jaw grippers to dexterous hands. <em>IROS</em>, 2978–2984. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a large-scale dataset named MultiGripperGrasp for robotic grasping. Our dataset contains 30.4M grasps from 11 grippers for 345 objects. These grippers range from two-finger grippers to five-finger grippers, including a human hand. All grasps in the dataset are verified in the robot simulator Isaac Sim to classify them as successful and unsuccessful grasps. Additionally, the object fall-off time for each grasp is recorded as a grasp quality measurement. Furthermore, the grippers in our dataset are aligned according to the orientation and position of their palms, allowing us to transfer grasps from one gripper to another. The grasp transfer significantly increases the number of successful grasps for each gripper in the dataset. Our dataset is useful to study generalized grasp planning and grasp transfer across different grippers. 1},
  archive   = {C_IROS},
  author    = {Luis Felipe Casas and Ninad Khargonkar and Balakrishnan Prabhakaran and Yu Xiang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801708},
  month     = {10},
  pages     = {2978-2984},
  title     = {MultiGripperGrasp: A dataset for robotic grasping from parallel jaw grippers to dexterous hands},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-fingered end-effector grasp reflex modeling for
one-shot tactile servoing in tool manipulation tasks. <em>IROS</em>,
2971–2977. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous tool manipulation tasks are challenging for robots because they must reason over the tool’s object affordances, how to grasp the tool so it may be used, how the tool will interact with other objects in the environment, and how to perform the complex tool affordances to complete the manipulation task. Focusing on tool grasping presents further challenges, specifically generalization to novel tools and modeling the problem in an explainable way suitable for safety-critical task domains, such as robots operating autonomously to perform repair tasks in NASA lunar habitats. In this work, we focus on grasping tools in an explainable way that can be generalized to novel tools. We present a logistic regression based grasp reflex model, which maps continuous end-effector sensor data to a set of discrete symbolic states. An adjustment policy uses these symbolic states to compute the appropriate gradient to change the end-effector pose and increase the probability of a secure tool grasp. Once the tool grasp is sufficiently secure, the robot proceeds with the rest of the manipulation task. We test our grasp reflex model on 6 novel tools, and find that the model achieves one-shot generalization by successfully using tactile servoing to secure grasps from one example of a secure grasp state. The robot’s ability to learn to grasp tools in an explainable way that achieves one-shot generalization to novel tools demonstrates the power of our grasp reflex model in allowing robots to achieve autonomous tool manipulation tasks.},
  archive   = {C_IROS},
  author    = {Emily Sheetz and Misha Savchenko and Emma Zemler and Abbas Presswala and Andrew Crouch and Shaun Azimi and Benjamin Kuipers},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801477},
  month     = {10},
  pages     = {2971-2977},
  title     = {Multi-fingered end-effector grasp reflex modeling for one-shot tactile servoing in tool manipulation tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gravity-aware grasp generation with implicit grasp mode
selection for underactuated hands. <em>IROS</em>, 2964–2970. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning-based grasp detectors typically assume a precision grasp, where each finger only has one contact point, and estimate the grasp probability. In this work, we propose a data generation and learning pipeline that can leverage power grasping, which has more contact points with an enveloping configuration and is robust against both positioning error and force disturbance. To train a grasp detector to prioritize power grasping while still keeping precision grasping as the secondary choice, we propose to train the network against the magnitude of disturbance in the gravity direction a grasp can resist (gravity-rejection score) rather than the binary classification of success. We also provide an efficient data generation pipeline for a dataset with gravity-rejection score annotation. Evaluation in both simulation and real-robot clarifies the significant improvement in our approach, especially when the objects are heavy.},
  archive   = {C_IROS},
  author    = {Tianyi Ko and Takuya Ikeda and Thomas Stewart and Robert Lee and Koichi Nishiwaki},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801600},
  month     = {10},
  pages     = {2964-2970},
  title     = {Gravity-aware grasp generation with implicit grasp mode selection for underactuated hands},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Development of a bendable and extendable soft gripper driven
by differential worm gear mechanism. <em>IROS</em>, 2958–2963. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A gripper mechanism using flexible double-rack finger actuated using differential worm gear mechanism that can change its finger length according to the object being grasped was developed. Thermoplastic polyurethane (TPU) based fingers were soft, impact resistant, highly compliant, and were able to conform to the contour of the objects when grasping. The fingers can extend, contract and bend in 2D space when driven by the differential worm gear mechanism. A normal worm and an inner worm gear was used to actuate the flexible double-rack fingers. The relative motions between the two worm gears resulted in different motions of the finger. Multiple fingered configuration of the gripper can be driven from the same mechanism actuated using two actuators. A mathematical model was developed to describe the kinematics of the finger for positioning experiments. Experiments were also conducted to measure the fingertip force for different lengths of the finger. Grasping experiments of two and three-finger gripper were performed to test the grasping performance of the gripper for objects of different size, shape and weight. The gripper successfully grasped objects of different sizes by adjusting the finger length and conforming to the shape of the objects.},
  archive   = {C_IROS},
  author    = {Moses Gladson Selvamuthu and Riichiro Tadakuma},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802270},
  month     = {10},
  pages     = {2958-2963},
  title     = {Development of a bendable and extendable soft gripper driven by differential worm gear mechanism},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multimodal failure prediction for vision-based manipulation
tasks with camera faults. <em>IROS</em>, 2951–2957. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Due to the increasing behavioral and structural complexity of robots, it is challenging to predict the execution outcome after error detection. Anomaly detection methods can help detect errors and prevent potential failures. However, not every fault leads to a failure due to the system’s fault tolerance or unintended error masking. In practical applications, a robotic system should have a potential failure evaluation module to estimate the probability of failures when receiving an error alert. Subsequently, a decision-making mechanism should help to take the next action, e.g., terminate, degrade performance, or continue the execution of the task. This paper proposes a multimodal method for failure prediction for vision-based manipulation systems that suffer from potential camera faults. We inject faults into images (e.g., noise and blur) and observe manipulation failure scenarios (e.g., pick failure, place failure, and collision) that can occur during the task. Through extensive fault injection experiments, we created a FAULT-to-FAILURE dataset containing 4000 real-world manipulation samples. The dataset is subsequently used to train the failure predictor. Our approach processes the combination of RGB images, masked images, and planned paths to effectively evaluate whether a certain faulty image could potentially lead to a manipulation failure. Results demonstrate that the proposed method outperforms state-of-the-art models in terms of overall performance, requires fewer sensors, and achieves faster inference speeds. The analytical software prototype and dataset are available at Github: MultimodalFailurePrediction.},
  archive   = {C_IROS},
  author    = {Yuliang Ma and Jingyi Liu and Ilshat Mamaev and Andrey Morozov},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802274},
  month     = {10},
  pages     = {2951-2957},
  title     = {Multimodal failure prediction for vision-based manipulation tasks with camera faults},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring how non-prehensile manipulation expands capability
in robots experiencing multi-joint failure. <em>IROS</em>, 2943–2950.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work explores non-prehensile manipulation (NPM) and whole-body interaction as strategies for enabling robotic manipulators to conduct manipulation tasks despite experiencing locked multi-joint (LMJ) failures. LMJs are critical system faults where two or more joints become inoperable; they impose constraints on the robot’s configuration and control spaces, consequently limiting the capability and reach of a prehensile-only approach. This approach involves three components: i) modeling the failure-constrained workspace of the robot, ii) generating a kinodynamic map of NPM actions within this workspace, and iii) a manipulation action planner that uses a sim-in-the-loop approach to select the best actions to take from the kinodynamic map. The experimental evaluation shows that our approach can increase the failure-constrained reachable area in LMJ cases by 79%. Further, it demonstrates the ability to complete real-world manipulation with up to 88.9% success when the end-effector is unusable and up to 100% success when it is usable.},
  archive   = {C_IROS},
  author    = {Gilberto Briscoe-Martinez and Anuj Pasricha and Ava Abderezaei and Santosh Chaganti and Sarath Chandra Vajrala and Sri Kanth Popuri and Alessandro Roncone},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801883},
  month     = {10},
  pages     = {2943-2950},
  title     = {Exploring how non-prehensile manipulation expands capability in robots experiencing multi-joint failure},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Insert-one: One-shot robust visual-force servoing for novel
object insertion with 6-DoF tracking. <em>IROS</em>, 2935–2942. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advancements in autonomous robotic assembly have shown promising results, especially in addressing the precision insertion challenge. However, achieving adaptability across diverse object categories and tasks often necessitates a learning phase that requires costly real-world data collection. Moreover, previous research often assumes either the rigid attachment of the inserted object to the robot’s end-effector or relies on precise calibration within structured environments. We propose a one-shot method for high-precision contact-rich manipulation assembly tasks, enabling a robot to perform insertions of new objects from randomly presented orientations using just a single demonstration image. Our method incorporates a hybrid framework that blends 6-DoF visual tracking-based iterative control and impedance control, facilitating high-precision tasks with real-time visual feedback. Importantly, our approach requires no pre-training and demonstrates resilience against uncertainties arising from camera pose calibration errors and disturbances in the object in-hand pose. We validate the effectiveness of the proposed framework through extensive experiments in real-world scenarios, encompassing various high-precision assembly tasks.},
  archive   = {C_IROS},
  author    = {Haonan Chang and Abdeslam Boularias and Siddarth Jain},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801884},
  month     = {10},
  pages     = {2935-2942},
  title     = {Insert-one: One-shot robust visual-force servoing for novel object insertion with 6-DoF tracking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Beyond the cascade: Juggling vanilla siteswap patterns.
<em>IROS</em>, 2928–2934. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Being widespread in human motor behavior, dynamic movements demonstrate higher efficiency and greater capacity to address a broader range of skill domains compared to their quasi-static counterparts. Among the frequently studied dynamic manipulation problems, robotic juggling tasks stand out due to their inherent ability to scale their difficulty levels to arbitrary extents, making them an excellent subject for investigation. In this study, we explore juggling patterns with mixed throw heights, following the vanilla siteswap juggling notation, which jugglers widely adopted to describe toss juggling patterns. This requires extending our previous analysis of the simpler cascade juggling task by a throw-height sequence planner and further constraints on the end effector trajectory. These are not necessary for cascade patterns but are vital to achieving patterns with mixed throw heights. Using a simulated environment, we demonstrate successful juggling of most common 3-9 ball siteswap patterns up to 9 ball height, transitions between these patterns, and random sequences covering all possible vanilla siteswap patterns with throws between 2 and 9 ball height. https://kai-ploeger.com/beyond-cascades},
  archive   = {C_IROS},
  author    = {Mario Gomez Andreu and Kai Ploeger and Jan Peters},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801588},
  month     = {10},
  pages     = {2928-2934},
  title     = {Beyond the cascade: Juggling vanilla siteswap patterns},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Seg2Grasp: A robust modular suction grasping in bin picking.
<em>IROS</em>, 2921–2927. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current bin picking methods that rely heavily on end-to-end learning often falter when confronted with unfamiliar or complex objects in unstructured environments. To overcome these limitations, we introduce Seg2Grasp, a modular pipeline designed for robust suction grasping in dynamic and cluttered bin scenarios. Seg2Grasp is built on a three-step process: Segmentation, Grasping, and Classification. The Segmentation module employs a Transformer-based model to generate class-agnostic object masks from RGB-D images, ensuring accurate detection across various conditions. The Grasping module uses surface normals and mask proposals to determine the optimal suction points, enhancing grasp success. Finally, the Classification module leverages fine-tuned open-vocabulary Mask-CLIP for precise object identification, enabling versatile handling of diverse objects. Real-world robotic experiments demonstrate that Seg2Grasp outperforms existing methods in success rates and adaptability, establishing it as a powerful tool for automated bin picking in industrial settings.},
  archive   = {C_IROS},
  author    = {Hye-Jung Yoon and Juno Kim and Yesol Park and Jun-Ki Lee and Byoung-Tak Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801644},
  month     = {10},
  pages     = {2921-2927},
  title     = {Seg2Grasp: A robust modular suction grasping in bin picking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RTTF: Rapid tactile transfer framework for contact-rich
manipulation tasks. <em>IROS</em>, 2913–2920. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An increasing number of robotic manipulation tasks now use optical tactile sensors to provide tactile feedback, making tactile servo control a crucial aspect of robotic operations. This paper presents a rapid tactile transfer framework (RTTF) that achieves optical-tactile image sim2real transfer and robust tactile servo control using limited paired data. The sim2real aspect of RTTF employs a semi-supervised approach, beginning with pretraining the latent space representations of tactile images and subsequently mapping different tactile image domains to a shared latent space within a simulated tactile image domain. This latent space, combined with the proprioceptive information of the robotic arm, is then integrated into a privileged learning framework for policy training, which results in a deployable tactile control policy. Our results demonstrate the robustness of the proposed framework in achieving task objectives across different tactile sensors with varying physical parameters. Furthermore, manipulators equipped with tactile sensors, allow for rapid training and deployment for diverse contact-rich tasks, including object pushing and surface following.},
  archive   = {C_IROS},
  author    = {Qiwei Wu and Xuanbin Peng and Jiayu Zhou and Zhuoran Sun and Xiaogang Xiong and Yunjiang Lou},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801764},
  month     = {10},
  pages     = {2913-2920},
  title     = {RTTF: Rapid tactile transfer framework for contact-rich manipulation tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ViSaRL: Visual reinforcement learning guided by human
saliency. <em>IROS</em>, 2907–2912. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Training robots to perform complex control tasks from high-dimensional pixel input using reinforcement learning (RL) is sample-inefficient, because image observations are comprised primarily of task-irrelevant information. By contrast, humans are able to visually attend to task-relevant objects and areas. Based on this insight, we introduce Visual Saliency-Guided Reinforcement Learning (ViSaRL). Using ViSaRL to learn visual representations significantly improves the success rate, sample efficiency, and generalization of an RL agent on diverse tasks including DeepMind Control benchmark, robot manipulation in simulation and on a real robot. We present approaches for incorporating saliency into both CNN and Transformer-based encoders. We show that visual representations learned using ViSaRL are robust to various sources of visual perturbations including perceptual noise and scene variations. ViSaRL nearly doubles success rate on the real-robot tasks compared to the baseline which does not use saliency.},
  archive   = {C_IROS},
  author    = {Anthony Liang and Jesse Thomason and Erdem Bıyık},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801388},
  month     = {10},
  pages     = {2907-2912},
  title     = {ViSaRL: Visual reinforcement learning guided by human saliency},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Imitation learning for sim-to-real adaptation of robotic
cutting policies based on residual gaussian process disturbance force
model. <em>IROS</em>, 2899–2906. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic cutting, a crucial task in applications such as disassembly and decommissioning, faces challenges due to uncertainties in real-world environments. This paper presents a novel approach to enhance sim-to-real transfer of robotic cutting policies, leveraging a hybrid method integrating Gaussian process (GP) regression to model disturbance forces encountered during cutting tasks. By learning from a limited number of real-world trials, our method captures residual process dynamics, enabling effective adaptation to diverse materials without the need for fine-tuning on physical robots. Key to our approach is the utilisation of imitation learning, where expert actions in the uncorrected simulation are paired with GP-corrected observations. This pairing aligns action distributions between simulated and real-world domains, facilitating robust policy transfer. We illustrate the efficacy of our method through real world cutting trials in autonomously adapting to diverse material properties; our method surpasses re-training, while providing similar benefits to fine-tuning in real-world cutting scenarios. Notably, policies transferred using our approach exhibit enhanced resilience to noise and disturbances, while maintaining fidelity to expert behaviours from the source domain.},
  archive   = {C_IROS},
  author    = {Jamie Hathaway and Rustam Stolkin and Alireza Rastegarpanah},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802660},
  month     = {10},
  pages     = {2899-2906},
  title     = {Imitation learning for sim-to-real adaptation of robotic cutting policies based on residual gaussian process disturbance force model},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Safe CoR: A dual-expert approach to integrating imitation
learning and safe reinforcement learning using constraint rewards.
<em>IROS</em>, 2893–2898. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the realm of autonomous agents, ensuring safety and reliability in complex and dynamic environments remains a paramount challenge. Safe reinforcement learning addresses these concerns by introducing safety constraints, but still faces challenges in navigating intricate environments such as complex driving situations. To overcome these challenges, we present the safe constraint reward (Safe CoR) framework, a novel method that utilizes two types of expert demonstrations—reward expert demonstrations focusing on performance optimization and safe expert demonstrations prioritizing safety. By exploiting a constraint reward (CoR), our framework guides the agent to balance performance goals of reward sum with safety constraints. We test the proposed framework in diverse environments, including the safety gym, metadrive, and the real-world Jackal platform. Our proposed framework improves algorithm performance by 39% and reduces constraint violations by 88% on the real-world Jackal platform, highlighting its effectiveness. Through this innovative approach, we expect significant advancements in real-world performance, leading to transformative effects in the realm of safe and reliable autonomous agents.},
  archive   = {C_IROS},
  author    = {Hyeokjin Kwon and Gunmin Lee and Junseo Lee and Songhwai Oh},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801577},
  month     = {10},
  pages     = {2893-2898},
  title     = {Safe CoR: A dual-expert approach to integrating imitation learning and safe reinforcement learning using constraint rewards},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust imitation learning for mobile manipulator focusing on
task-related viewpoints and regions. <em>IROS</em>, 2885–2892. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study how to generalize the visuomotor policy of a mobile manipulator from the perspective of visual observations. The mobile manipulator is prone to occlusion owing to its own body when only a single viewpoint is employed and a significant domain shift when deployed in diverse situations. However, to the best of the authors’ knowledge, no study has been able to solve occlusion and domain shift simultaneously and propose a robust policy. In this paper, we propose a robust imitation learning method for mobile manipulators that focuses on task-related viewpoints and their spatial regions when observing multiple viewpoints. The multiple viewpoint policy includes attention mechanism, which is learned with an augmented dataset, and brings optimal viewpoints and robust visual embedding against occlusion and domain shift. Comparison of our results for different tasks and environments with those of previous studies revealed that our proposed method improves the success rate by up to 29.3 points. We also conduct ablation studies using our proposed method. Learning task-related viewpoints from the multiple viewpoints dataset increases robustness to occlusion than using a uniquely defined viewpoint. Focusing on task-related regions contributes to up to a 33.3-point improvement in the success rate against domain shift.},
  archive   = {C_IROS},
  author    = {Yutaro Ishida and Yuki Noguchi and Takayuki Kanai and Kazuhiro Shintani and Hiroshi Bito},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802396},
  month     = {10},
  pages     = {2885-2892},
  title     = {Robust imitation learning for mobile manipulator focusing on task-related viewpoints and regions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TinyLidarNet: 2D LiDAR-based end-to-end deep learning model
for F1TENTH autonomous racing. <em>IROS</em>, 2878–2884. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Prior research has demonstrated the effectiveness of end-to-end deep learning for robotic navigation, where the control signals are directly derived from raw sensory data. However, the majority of existing end-to-end navigation solutions are predominantly camera-based. In this paper, we introduce TinyLidarNet, a lightweight 2D LiDAR-based end-to-end deep learning model for autonomous racing. An F1TENTH vehicle using TinyLidarNet won 3rd place in the 12th F1TENTH Autonomous Grand Prix competition, demonstrating its competitive performance. We systematically analyze its performance on untrained tracks and computing requirements for real-time processing. We find that TinyLidarNet’s 1D Convolutional Neural Network (CNN) based architecture significantly outperforms widely used Multi-Layer Perceptron (MLP) based architecture. In addition, we show that it can be processed in real-time on low-end micro-controller units (MCUs).},
  archive   = {C_IROS},
  author    = {Mohammed Misbah Zarrar and Qitao Weng and Bakhbyergyen Yerjan and Ahmet Soyyigit and Heechul Yun},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801430},
  month     = {10},
  pages     = {2878-2884},
  title     = {TinyLidarNet: 2D LiDAR-based end-to-end deep learning model for F1TENTH autonomous racing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RISE: 3D perception makes real-world robot imitation simple
and effective. <em>IROS</em>, 2870–2877. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Precise robot manipulations require rich spatial information in imitation learning. Image-based policies model object positions from fixed cameras, which are sensitive to camera view changes. Policies utilizing 3D point clouds usually predict keyframes rather than continuous actions, posing difficulty in dynamic and contact-rich scenarios. To utilize 3D perception efficiently, we present RISE, an end-to-end baseline for real-world imitation learning, which predicts continuous actions directly from single-view point clouds. It compresses the point cloud to tokens with a sparse 3D encoder. After adding sparse positional encoding, the tokens are featurized using a transformer. Finally, the features are decoded into robot actions by a diffusion head. Trained with 50 demonstrations for each real-world task, RISE surpasses currently representative 2D and 3D policies by a large margin, showcasing significant advantages in both accuracy and efficiency. Experiments also demonstrate that RISE is more general and robust to environmental change compared with previous baselines. Project website: rise-policy.github.io.},
  archive   = {C_IROS},
  author    = {Chenxi Wang and Hongjie Fang and Hao-Shu Fang and Cewu Lu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801678},
  month     = {10},
  pages     = {2870-2877},
  title     = {RISE: 3D perception makes real-world robot imitation simple and effective},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self supervised detection of incorrect human demonstrations:
A path toward safe imitation learning by robots in the wild.
<em>IROS</em>, 2862–2869. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A major appeal of learning from demonstrations or imitation learning (IL) in robotics is that it learns a policy directly from lay users. However, Lay users may inadvertently provide erroneous demonstrations that lead to learning of policies that are inaccurate and hence, unsafe for humans and/or robot. This paper makes two contributions in the endeavour of recognizing human errors in demonstrations and thereby helping to learn a safe IL policy. First, we created a dataset – Layman V1.0 – with 15 lay users who provided a total of 1200 demonstrations for three simulated tasks – Lift, Can and Square in the simulated Robosuite environment – and two real robot tasks with a Sawyer robot, using a custom designed Android app for tele-operation. Second, we propose a framework named Behavior Cloning for Error Detection (BED) to autonomously detect and discard erroneous demonstrations from a demonstration pool. Our method uses a Behavior Cloning method as self-supervised technique and assigns binary weight to each demonstration based on its inconsistencies with the rest of the demonstrations. We show the effectiveness of this framework in detecting incorrect demonstrations in the Layman V1.0 dataset. We further show that state-of-the-art (SOTA) policy learners learns a better policy when bad demonstrations, identified through the proposed framework, are removed from the training pool. Dataset and Codes are available in https://github.com/AssistiveRoboticsUNH/bed},
  archive   = {C_IROS},
  author    = {Noushad Sojib and Momotaz Begum},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802106},
  month     = {10},
  pages     = {2862-2869},
  title     = {Self supervised detection of incorrect human demonstrations: A path toward safe imitation learning by robots in the wild},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ARCADE: Scalable demonstration collection and generation via
augmented reality for imitation learning. <em>IROS</em>, 2855–2861. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot Imitation Learning (IL) is a crucial technique in robot learning, where agents learn by mimicking human demonstrations. However, IL encounters scalability challenges stemming from both non-user-friendly demonstration collection methods and the extensive time required to amass a sufficient number of demonstrations for effective training. In response, we introduce the Augmented Reality for Collection and generAtion of DEmonstrations (ARCADE) framework, designed to scale up demonstration collection for robot manipulation tasks. Our framework combines two key capabilities: 1) it leverages AR to make demonstration collection as simple as users performing daily tasks using their hands, and 2) it enables the automatic generation of additional synthetic demonstrations from a single human-derived demonstration, significantly reducing user effort and time. We assess ARCADE’s performance on a real Fetch robot across three robotics tasks: 3-Waypoints-Reach, Push, and Pick-And-Place. Using our framework, we were able to rapidly train a policy using vanilla Behavioral Cloning (BC), a classic IL algorithm, which excelled across these three tasks. We also deploy ARCADE on a real household task, Pouring-Water, achieving an 80% success rate.},
  archive   = {C_IROS},
  author    = {Yue Yang and Bryce Ikeda and Gedas Bertasius and Daniel Szafir},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801810},
  month     = {10},
  pages     = {2855-2861},
  title     = {ARCADE: Scalable demonstration collection and generation via augmented reality for imitation learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning generalizable tool-use skills through trajectory
generation. <em>IROS</em>, 2847–2854. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous systems that efficiently utilize tools can assist humans in completing many common tasks such as cooking and cleaning. However, current systems fall short of matching human-level of intelligence in terms of adapting to novel tools. Prior works based on affordance often make strong assumptions about the environments and cannot scale to more complex, contact-rich tasks. In this work, we tackle this challenge and explore how agents can learn to use previously unseen tools to manipulate deformable objects. We propose to learn a generative model of the tool-use trajectories as a sequence of tool point clouds, which generalizes to different tool shapes. Given any novel tool, we first generate a tool-use trajectory and then optimize the sequence of tool poses to align with the generated trajectory. We train a single model on four different challenging deformable object manipulation tasks, using demonstration data from only one tool per task. The model generalizes to various novel tools, significantly outperforming baselines. We further test our trained policy in the real world with unseen tools, where it achieves the performance comparable to human. Additional materials can be found on our project website.1},
  archive   = {C_IROS},
  author    = {Carl Qi and Yilin Wu and Lifan Yu and Haoyue Liu and Bowen Jiang and Xingyu Lin and David Held},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801653},
  month     = {10},
  pages     = {2847-2854},
  title     = {Learning generalizable tool-use skills through trajectory generation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IntervenGen: Interventional data generation for robust and
data-efficient robot imitation learning. <em>IROS</em>, 2840–2846. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Imitation learning is a promising paradigm for training robot control policies, but these policies can suffer from distribution shift, where the conditions at evaluation time differ from those in the training data. A popular approach for increasing policy robustness to distribution shift is interactive imitation learning (i.e., DAgger and variants), where a human operator provides corrective interventions during policy rollouts. However, collecting a sufficient amount of interventions to cover the distribution of policy mistakes can be burdensome for human operators. We propose IntervenGen (I-Gen), a novel data augmentation system for robot control that autonomously produces a large set of corrective interventions with rich coverage of the state space from a small number of human interventions. We apply I-Gen to 4 simulated environments and 1 physical environment with object pose estimation error and show that it can increase policy robustness by up to 39× with only 10 human interventions. Videos and more results are available at https://sites.google.com/view/intervengen2024.},
  archive   = {C_IROS},
  author    = {Ryan Hoque and Ajay Mandlekar and Caelan Garrett and Ken Goldberg and Dieter Fox},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801523},
  month     = {10},
  pages     = {2840-2846},
  title     = {IntervenGen: Interventional data generation for robust and data-efficient robot imitation learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning multi-reference frame skills from demonstration
with task-parameterized gaussian processes. <em>IROS</em>, 2832–2839.
(<a href="https://doi.org/10.1109/IROS58592.2024.10803060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A central challenge in Learning from Demonstration is to generate representations that are adaptable and can generalize to unseen situations. This work proposes to learn such a representation without using task-specific heuristics within the context of multi-reference frame skill learning by superimposing local skills in the global frame. Local policies are first learned by fitting the relative skills with respect to each frame using Gaussian Processes (GPs). Then, another GP, which determines the relevance of each frame for every time step, is trained in a self-supervised manner from a different batch of demonstrations. The uncertainty quantification capability of GPs is exploited to stabilize the local policies and to train the frame relevance in a fully Bayesian way. We validate the method through a dataset of multi-frame tasks generated in simulation and on real-world experiments with a robotic manipulation pick-and-place re-shelving task.We evaluate the performance of our method with two metrics: how close the generated trajectories get to each of the task goals and the deviation between these trajectories and test expert trajectories. According to both of these metrics, the proposed method consistently outperforms the state-of-the-art baseline, Task-Parameterised Gaussian Mixture Model (TPGMM).},
  archive   = {C_IROS},
  author    = {Mariano Ramírez Montero and Giovanni Franzese and Jens Kober and Cosimo Della Santina},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10803060},
  month     = {10},
  pages     = {2832-2839},
  title     = {Learning multi-reference frame skills from demonstration with task-parameterized gaussian processes},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Driving from vision through differentiable optimal control.
<em>IROS</em>, 2824–2831. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes DriViDOC: a framework for Driving from Vision through Differentiable Optimal Control, and its application to learn autonomous driving controllers from human demonstrations. DriViDOC combines the automatic inference of relevant features from camera frames with the properties of nonlinear model predictive control (NMPC), such as constraint satisfaction. Our approach leverages the differentiability of parametric NMPC, allowing for end-to-end learning of the driving model from images to control. The model is trained on an offline dataset comprising various human demonstrations collected on a motion-base driving simulator. During online testing, the model demonstrates successful imitation of different driving styles, and the interpreted NMPC parameters provide insights into the achievement of specific driving behaviors. Our experimental results show that DriViDOC outperforms other methods involving NMPC and neural networks, exhibiting an average improvement of 20% in imitation scores.},
  archive   = {C_IROS},
  author    = {Flavia Sofia Acerbo and Jan Swevers and Tinne Tuytelaars and Tong Duy Son},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802306},
  month     = {10},
  pages     = {2824-2831},
  title     = {Driving from vision through differentiable optimal control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient trajectory forecasting and generation with
conditional flow matching. <em>IROS</em>, 2816–2823. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trajectory prediction and generation are crucial for autonomous robots in dynamic environments. While prior research has typically focused on either prediction or generation, our approach unifies these tasks to provide a versatile framework and achieve state-of-the-art performance. While diffusion models excel in trajectory generation, their iterative sampling process is computationally intensive, hindering robotic systems’ dynamic capabilities. We introduce Trajectory Conditional Flow Matching (T-CFM), a novel approach using flow matching techniques to learn a solver time-varying vector field for efficient, fast trajectory generation. T-CFM demonstrates effectiveness in adversarial tracking, real-world aircraft trajectory forecasting, and long-horizon planning, outperforming state-of-the-art baselines with 35% higher predictive accuracy and 142% improved planning performance. Crucially, T-CFM achieves up to 100× speed-up compared to diffusion models without sacrificing accuracy, enabling real-time decision making in robotics. Codebase: https://github.com/CORE-Robotics-Lab/TCFM},
  archive   = {C_IROS},
  author    = {Sean Ye and Matthew C. Gombolay},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802208},
  month     = {10},
  pages     = {2816-2823},
  title     = {Efficient trajectory forecasting and generation with conditional flow matching},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DecAP: Decaying action priors for accelerated imitation
learning of torque-based legged locomotion policies. <em>IROS</em>,
2809–2815. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Optimal Control for legged robots has gone through a paradigm shift from position-based to torque-based control, owing to the latter’s compliant and robust nature. In parallel to this shift, the community has also turned to Deep Reinforcement Learning (DRL) as a promising approach to directly learn locomotion policies for complex real-life tasks. However, most end-to-end DRL approaches still operate in position space, mainly because learning in torque space is often sample-inefficient and does not consistently converge to natural gaits. To address these challenges, we propose a two-stage framework. In the first stage, we generate our own imitation data by training a position-based policy, eliminating the need for expert knowledge to design optimal controllers. The second stage incorporates decaying action priors, a novel method to enhance the exploration of torque-based policies aided by imitation rewards. We show that our approach consistently outperforms imitation learning alone and is robust to scaling these rewards from 0.1x to 10x. We further validate the benefits of torque control by comparing the robustness of a position-based policy to a position-assisted torque-based policy on a quadruped (Unitree Go1) without any domain randomization in the form of external disturbances during training.3},
  archive   = {C_IROS},
  author    = {Shivam Sood and Ge Sun and Peizhuo Li and Guillaume Sartoretti},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802000},
  month     = {10},
  pages     = {2809-2815},
  title     = {DecAP: Decaying action priors for accelerated imitation learning of torque-based legged locomotion policies},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian optimization for sample-efficient policy
improvement in robotic manipulation. <em>IROS</em>, 2801–2808. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sample efficient learning of manipulation skills poses a major challenge in robotics. While recent approaches demonstrate impressive advances in the type of task that can be addressed and the sensing modalities that can be incorporated, they still require large amounts of training data. Especially with regard to learning actions on robots in the real world, this poses a major problem due to the high costs associated with both demonstrations and real-world robot interactions. To address this challenge, we introduce BOpt-GMM, a hybrid approach that combines imitation learning with own experience collection. We first learn a skill model as a dynamical system encoded in a Gaussian Mixture Model from a few demonstrations. We then improve this model with Bayesian optimization building on a small number of autonomous skill executions in a sparse reward setting. We demonstrate the sample efficiency of our approach on multiple complex manipulation skills in both simulations and real-world experiments. Furthermore, we make the code and pre-trained models publicly available at http://bopt-gmm.cs.uni-freiburg.de.},
  archive   = {C_IROS},
  author    = {Adrian Röfer and Iman Nematollahi and Tim Welschehold and Wolfram Burgard and Abhinav Valada},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802400},
  month     = {10},
  pages     = {2801-2808},
  title     = {Bayesian optimization for sample-efficient policy improvement in robotic manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conditional variational autoencoders for probabilistic pose
regression. <em>IROS</em>, 2794–2800. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots rely on visual relocalization to estimate their pose from camera images when they lose track. One of the challenges in visual relocalization is repetitive structures in the operation environment of the robot. This calls for probabilistic methods that support multiple hypotheses for robot’s pose. We propose such a probabilistic method to predict the posterior distribution of camera poses given an observed image. Our proposed training strategy results in a generative model of camera poses given an image, which can be used to draw samples from the pose posterior distribution. Our method is streamlined and well-founded in theory and outperforms existing methods on localization in presence of ambiguities.},
  archive   = {C_IROS},
  author    = {Fereidoon Zangeneh and Leonard Bruns and Amit Dekel and Alessandro Pieropan and Patric Jensfelt},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802091},
  month     = {10},
  pages     = {2794-2800},
  title     = {Conditional variational autoencoders for probabilistic pose regression},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SSAP: A shape-sensitive adversarial patch for comprehensive
disruption of monocular depth estimation in autonomous navigation
applications. <em>IROS</em>, 2786–2793. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Monocular depth estimation (MDE) has advanced significantly, primarily through the integration of convolutional neural networks (CNNs) and more recently, Transformers. However, concerns about their susceptibility to adversarial attacks have emerged, especially in safety-critical domains like autonomous driving and robotic navigation. Existing approaches for assessing CNN-based depth prediction methods have fallen short in inducing comprehensive disruptions to the vision system, often limited to specific local areas. In this paper, we introduce SSAP (Shape-Sensitive Adversarial Patch), a novel approach designed to comprehensively disrupt monocular depth estimation (MDE) in autonomous navigation applications. Our patch is crafted to selectively undermine MDE in two distinct ways: by distorting estimated distances or by creating the illusion of an object disappearing from the system’s perspective. Notably, our patch is shape-sensitive, meaning it considers the specific shape and scale of the target object, thereby extending its influence beyond immediate proximity. Furthermore, our patch is trained to effectively address different scales and distances from the camera. Experimental results demonstrate that our approach induces a mean depth estimation error surpassing 0.5, impacting up to 99% of the targeted region for CNN-based MDE models. Additionally, we investigate the vulnerability of Transformer-based MDE models to patch-based attacks, revealing that SSAP yields a significant error of 0.59 and exerts substantial influence over 99% of the target region on these models.},
  archive   = {C_IROS},
  author    = {Amira Guesmi and Muhammad Abdullah Hanif and Ihsen Alouani and Bassem Ouni and Muhammad Shafique},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802252},
  month     = {10},
  pages     = {2786-2793},
  title     = {SSAP: A shape-sensitive adversarial patch for comprehensive disruption of monocular depth estimation in autonomous navigation applications},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MARVIS: Motion &amp; geometry aware real and virtual image
segmentation. <em>IROS</em>, 2778–2785. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tasks such as autonomous navigation, 3D reconstruction, and object recognition near the water surfaces are crucial in marine robotics applications. However, challenges arise due to dynamic disturbances, e.g., light reflections and refraction from the random air-water interface, irregular liquid flow, and similar factors, which can lead to potential failures in perception and navigation systems. Traditional computer vision algorithms struggle to differentiate between real and virtual image regions, significantly complicating tasks. A virtual image region is an apparent representation formed by the redirection of light rays, typically through reflection or refraction, creating the illusion of an object’s presence without its actual physical location. This work proposes a novel approach for segmentation on real and virtual image regions, exploiting synthetic images combined with domain-invariant information, a Motion Entropy Kernel, and Epipolar Geometric Consistency. Our segmentation network does not need to be re-trained if the domain changes. We show this by deploying the same segmentation network in two different domains: simulation and the real world. By creating realistic synthetic images that mimic the complexities of the water surface, we provide fine-grained training data for our network (MARVIS) to discern between real and virtual images effectively. By motion &amp; geometry-aware design choices and through comprehensive experimental analysis, we achieve state-of-the-art real-virtual image segmentation performance in unseen real world domain, achieving an IoU over 78% and a F1-Score over 86% while ensuring a small computational footprint. MARVIS offers over 43 FPS (8 FPS) inference rates on a single GPU (CPU core). Our code and dataset are available here https://github.com/jiayi-wu-umd/MARVIS.},
  archive   = {C_IROS},
  author    = {Jiayi Wu and Xiaomin Lin and Shahriar Negahdaripour and Cornelia Fermüller and Yiannis Aloimonos},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801473},
  month     = {10},
  pages     = {2778-2785},
  title     = {MARVIS: Motion &amp; geometry aware real and virtual image segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring few-beam LiDAR assistance in self-supervised
multi-frame depth estimation. <em>IROS</em>, 2770–2777. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-supervised multi-frame depth estimation methods only require unlabeled monocular videos for training. However, most existing methods face challenges, including accuracy degradation caused by moving objects in dynamic scenes and scale ambiguity due to the absence of real-world references. In this field, the emergence of low-cost LiDAR sensors highlights the potential to improve the robustness of multi-frame depth estimation by exploiting accurate sparse measurements at the correct scale. Moreover, the LiDAR ranging points often intersect moving objects, providing more precise depth cues for them. This paper explores the impact of few-beam LiDAR data on self-supervised multi-frame depth estimation, proposing a method that fuses multi-frame matching and sparse depth features. It significantly enhances depth estimation robustness, particularly in scenarios involving moving objects and textureless backgrounds. We demonstrate the effectiveness of our approach through comprehensive experiments, showcasing its potential to address the limitations of existing methods and paving the way for more robust and reliable depth estimation based on this paradigm.},
  archive   = {C_IROS},
  author    = {Rizhao Fan and Matteo Poggi and Stefano Mattoccia},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801997},
  month     = {10},
  pages     = {2770-2777},
  title     = {Exploring few-beam LiDAR assistance in self-supervised multi-frame depth estimation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning to estimate the pose of a peer robot in a camera
image by predicting the states of its LEDs. <em>IROS</em>, 2763–2769.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problem of training a fully convolutional network to estimate the relative 6D pose of a robot given a camera image, when the robot is equipped with independent controllable LEDs placed in different parts of its body. The training data is composed by few (or zero) images labeled with a ground truth relative pose and many images labeled only with the true state (ON or OFF) of each of the peer LEDs. The former data is expensive to acquire, requiring external infrastructure for tracking the two robots; the latter is cheap as it can be acquired by two unsupervised robots moving randomly and toggling their LEDs while sharing the true LED states via radio. Training with the latter dataset on estimating the LEDs’ state of the peer robot (pretext task) promotes learning the relative localization task (end task). Experiments on real-world data acquired by two autonomous wheeled robots show that a model trained only on the pretext task successfully learns to localize a peer robot on the image plane; fine-tuning such model on the end task with few labeled images yields statistically significant improvements in 6D relative pose estimation with respect to baselines that do not use pretext-task pre-training, and alternative approaches. Estimating the state of multiple independent LEDs promotes learning to estimate relative heading. The approach works even when a large fraction of training images do not include the peer robot and generalizes well to unseen environments.},
  archive   = {C_IROS},
  author    = {Nicholas Carlotti and Mirko Nava and Alessandro Giusti},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801959},
  month     = {10},
  pages     = {2763-2769},
  title     = {Learning to estimate the pose of a peer robot in a camera image by predicting the states of its LEDs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MaskingDepth: Masked consistency regularization for
semi-supervised monocular depth estimation. <em>IROS</em>, 2755–2762.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose MaskingDepth, a semi-supervised learning framework for monocular depth estimation. MaskingDepth is designed to enforce consistency between the depths obtained from strongly-augmented images and the pseudo-depths derived from weakly-augmented images, which enables mitigating the reliance on large ground-truth depth quantities. In this framework, we leverage uncertainty estimation to only retain high-confident depth predictions from the weakly-augmented branch as pseudo-depths. We also present a novel data augmentation, dubbed K-way disjoint masking, that takes advantage of a naïve token masking strategy as an augmentation, while avoiding its scale ambiguity problem between depths from weakly-and strongly-augmented branches and risk of missing small-scale objects. Experiments on KITTI and NYU-Depth-v2 datasets demonstrate the effectiveness of each component, its robustness to the use of fewer depth-annotated images, and superior performance compared to other state-of-the-art semi-supervised learning methods for monocular depth estimation.},
  archive   = {C_IROS},
  author    = {Jongbeom Baek and Gyeongnyeon Kim and Seonghoon Park and Honggyu An and Matteo Poggi and Seungryong Kim},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801719},
  month     = {10},
  pages     = {2755-2762},
  title     = {MaskingDepth: Masked consistency regularization for semi-supervised monocular depth estimation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SD-net: Symmetric-aware keypoint prediction and domain
adaptation for 6D pose estimation in bin-picking scenarios.
<em>IROS</em>, 2747–2754. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite the success of 6D pose estimation in bin-picking scenarios, existing methods still struggle to produce accurate prediction results for symmetry objects in real-world scenarios. The primary bottlenecks include 1) the ambiguity in keypoints caused by object symmetries; and 2) the domain gap between real and synthetic data. To circumvent these problems, we propose a novel 6D pose estimation network with symmetric-aware keypoint prediction and self-training domain adaptation (SD-Net). SD-Net builds on point-wise keypoint regression and deep hough voting to perform reliable keypoint detection under clutter and occlusion. Specifically, at the keypoint prediction stage, we propose a robust 3D keypoint selection strategy considering the symmetry class of objects and equivalent keypoints, which facilitate locating 3D keypoints even in highly occluded scenes. Additionally, we build an effective filtering algorithm on predicted keypoints to dynamically eliminate multiple ambiguity and outlier key-point candidates. At the domain adaptation stage, we propose the self-training framework using a student-teacher training scheme. To carefully distinguish reliable predictions, we harness tailored heuristics for 3D geometry pseudo labelling based on semi-chamfer distance. On the public Siléane dataset, SD-Net achieves state-of-the-art results, obtaining an average precision of 96%. Testing learning and generalization abilities on public Parametric datasets, SD-Net is 8% higher than the state-of-the-art method.},
  archive   = {C_IROS},
  author    = {Ding-Tao Huang and En-Te Lin and Lipeng Chen and Li-Fu Liu and Long Zeng},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802595},
  month     = {10},
  pages     = {2747-2754},
  title     = {SD-net: Symmetric-aware keypoint prediction and domain adaptation for 6D pose estimation in bin-picking scenarios},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VANP: Learning where to see for navigation with
self-supervised vision-action pre-training. <em>IROS</em>, 2741–2746.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans excel at efficiently navigating through crowds without collision by focusing on specific visual regions relevant to navigation. However, most robotic visual navigation methods rely on deep learning models pre-trained on vision tasks, which prioritize salient objects—not necessarily relevant to navigation and potentially misleading. Alternative approaches train specialized navigation models from scratch, requiring significant computation. On the other hand, self-supervised learning has revolutionized computer vision and natural language processing, but its application to robotic navigation remains underexplored due to the difficulty of defining effective self-supervision signals. Motivated by these observations, in this work, we propose a Self-Supervised Vision-Action Model for Visual Navigation Pre-Training (VANP). Instead of detecting salient objects that are beneficial for tasks such as classification or detection, VANP learns to focus only on specific visual regions that are relevant to the navigation task. To achieve this, VANP uses a history of visual observations, future actions, and a goal image for self-supervision, and embeds them using two small Transformer Encoders. Then, VANP maximizes the information between the embeddings by using a mutual information maximization objective function. We demonstrate that most VANP-extracted features match with human navigation intuition. VANP achieves comparable performance as models learned end-to-end with half the training time and models trained on a large-scale, fully supervised dataset, i.e., ImageNet, with only 0.08% data. 1},
  archive   = {C_IROS},
  author    = {Mohammad Nazeri and Junzhe Wang and Amirreza Payandeh and Xuesu Xiao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802451},
  month     = {10},
  pages     = {2741-2746},
  title     = {VANP: Learning where to see for navigation with self-supervised vision-action pre-training},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CaFNet: A confidence-driven framework for radar camera depth
estimation. <em>IROS</em>, 2734–2740. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Depth estimation is critical in autonomous driving for interpreting 3D scenes accurately. Recently, radar-camera depth estimation has become of sufficient interest due to the robustness and low-cost properties of radar. Thus, this paper introduces a two-stage, end-to-end trainable Confidence-aware Fusion Net (CaFNet) for dense depth estimation, combining RGB imagery with sparse and noisy radar point cloud data. The first stage addresses radar-specific challenges, such as ambiguous elevation and noisy measurements, by predicting a radar confidence map and a preliminary coarse depth map. A novel approach is presented for generating the ground truth for the confidence map, which involves associating each radar point with its corresponding object to identify potential projection surfaces. These maps, together with the initial radar input, are processed by a second encoder. For the final depth estimation, we innovate a confidence-aware gated fusion mechanism to integrate radar and image features effectively, thereby enhancing the reliability of the depth map by filtering out radar noise. Our methodology, evaluated on the nuScenes dataset, demonstrates superior performance, improving upon the current leading model by 3.2% in Mean Absolute Error (MAE) and 2.7% in Root Mean Square Error (RMSE). Code: https://github.com/harborsarah/CaFNet},
  archive   = {C_IROS},
  author    = {Huawei Sun and Hao Feng and Julius Ott and Lorenzo Servadei and Robert Wille},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801594},
  month     = {10},
  pages     = {2734-2740},
  title     = {CaFNet: A confidence-driven framework for radar camera depth estimation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Latent disentanglement for low light image enhancement.
<em>IROS</em>, 2728–2733. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many learning-based low light image enhancement (LLIE) algorithms are based on the Retinex theory. However, the Retinex-based decomposition models introduce corruptions which limit their enhancement performance. In this paper, we propose a Latent Disentangle-based Enhancement Network (LDE-Net) for low light vision tasks. The latent disentanglement module disentangles the input image in latent space such that no corruption remains in the disentangled Content and Illumination components. For LLIE task, we design a Content-Aware Embedding (CAE) module that utilizes Content features to direct the enhancement of the Illumination component. For downstream tasks (e.g. nighttime UAV tracking and low light object detection), we develop an effective light-weight enhancer based on the latent disentanglement framework. Comprehensive quantitative and qualitative experiments demonstrate that our LDE-Net significantly outperforms state-of-the-art methods on various LLIE benchmarks. In addition, the great results obtained by applying our framework on the downstream tasks also demonstrate the usefulness of our latent disentanglement design.},
  archive   = {C_IROS},
  author    = {Zhihao Zheng and Mooi Choo Chuah},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802761},
  month     = {10},
  pages     = {2728-2733},
  title     = {Latent disentanglement for low light image enhancement},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards dynamic and small objects refinement for
unsupervised domain adaptative nighttime semantic segmentation.
<em>IROS</em>, 2720–2727. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Nighttime semantic segmentation plays a crucial role in practical applications, such as autonomous driving, where it frequently encounters difficulties caused by inadequate illumination conditions and the absence of well-annotated datasets. Moreover, semantic segmentation models trained on daytime datasets often face difficulties in generalizing effectively to nighttime conditions. Unsupervised domain adaptation (UDA) has shown the potential to address the challenges and achieved remarkable results for nighttime semantic segmentation. However, existing methods still face limitations in 1) their reliance on style transfer or relighting models, which struggle to generalize to complex nighttime environments, and 2) their ignorance of dynamic and small objects like vehicles and poles, which are difficult to be directly learned from other domains. This paper proposes a novel UDA method that refines both label and feature levels for dynamic and small objects for nighttime semantic segmentation. First, we propose a dynamic and small object refinement module to complement the knowledge of dynamic and small objects from the source domain to target the nighttime domain. These dynamic and small objects are normally context-inconsistent in under-exposed conditions. Then, we design a feature prototype alignment module to reduce the domain gap by deploying contrastive learning between features and prototypes of the same class from different domains, while re-weighting the categories of dynamic and small objects. Extensive experiments on three benchmark datasets demonstrate that our method outperforms prior arts by a large margin for nighttime segmentation. Project page: https://rorisis.github.io/DSRNSS/.},
  archive   = {C_IROS},
  author    = {Jingyi Pan and Sihang Li and Yucheng Chen and Jinjing Zhu and Lin Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801389},
  month     = {10},
  pages     = {2720-2727},
  title     = {Towards dynamic and small objects refinement for unsupervised domain adaptative nighttime semantic segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RAM-NAS: Resource-aware multiobjective neural architecture
search method for robot vision tasks. <em>IROS</em>, 2712–2719. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Neural architecture search (NAS) has shown great promise in automatically designing lightweight models. However, conventional approaches are insufficient in training the supernet and pay little attention to actual robot hardware resources. To meet such challenges, we propose RAM-NAS, a resource-aware multi-objective NAS method that focuses on improving the supernet pretrain and resource-awareness on robot hardware devices. We introduce the concept of subnets mutual distillation, which refers to mutually distilling all subnets sampled by the sandwich rule. Additionally, we utilize the Decoupled Knowledge Distillation (DKD) loss to enhance logits distillation performance. To expedite the search process with consideration for hardware resources, we used data from three types of robotic edge hardware to train Latency Surrogate predictors. These predictors facilitated the estimation of hardware inference latency during the search phase, enabling a unified multi-objective evolutionary search to balance model accuracy and latency trade-offs. Our discovered model family, RAM-NAS models, can achieve top-1 accuracy ranging from 76.7% to 81.4% on ImageNet. In addition, the resource-aware multi-objective NAS we employ significantly reduces the model’s inference latency on edge hardware for robots. We conducted experiments on downstream tasks to verify the scalability of our methods. The inference time for detection and segmentation is reduced on all three hardware types compared to MobileNetv3-based methods. Our work fills the gap in NAS for robot hardware resource-aware.},
  archive   = {C_IROS},
  author    = {Shouren Mao and Minghao Qin and Wei Dong and Huajian Liu and Yongzhuo Gao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802271},
  month     = {10},
  pages     = {2712-2719},
  title     = {RAM-NAS: Resource-aware multiobjective neural architecture search method for robot vision tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Just flip: Flipped observation generation and optimization
for neural radiance fields to cover unobserved view. <em>IROS</em>,
2704–2711. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the advent of Neural Radiance Field (NeRF), representing 3D scenes through multiple observations has shown significant improvements. Since this cutting-edge technique can obtain high-resolution renderings by interpolating dense 3D environments, various approaches have been proposed to apply NeRF for the spatial understanding of robot perception. However, previous works are challenging to represent unobserved scenes or views on the unexplored robot trajectory, as these works do not take into account 3D reconstruction without observation information. To overcome this problem, we propose a method to generate flipped observation in order to cover absent observation for unexplored robot trajectory. Our approach involves a data augmentation technique for 3D reconstruction using NeRF, by flipping observed images and estimating the 6DOF poses of the flipped cameras. Furthermore, to ensure the NeRF model operates robustly in general scenarios, we also propose a training method that adjusts the flipped pose and considers the uncertainty in flipped images accordingly. Our technique does not utilize an additional network, making it simple and fast, thus ensuring its suitability for robotic applications where real-time performance is crucial.},
  archive   = {C_IROS},
  author    = {Sibaek Lee and Kyeongsu Kang and Hyeonwoo Yu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802266},
  month     = {10},
  pages     = {2704-2711},
  title     = {Just flip: Flipped observation generation and optimization for neural radiance fields to cover unobserved view},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Best of both worlds: Hybrid SNN-ANN architecture for
event-based optical flow estimation. <em>IROS</em>, 2696–2703. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the field of robotics, event-based cameras are emerging as a promising low-power alternative to traditional frame-based cameras for capturing high-speed motion and high dynamic range scenes. This is due to their sparse and asynchronous event outputs. Spiking Neural Networks (SNNs) with their asynchronous event-driven compute, show great potential for extracting the spatio-temporal features from these event streams. In contrast, the standard Analog Neural Networks (ANNs1) fail to process event data effectively. However, training SNNs is difficult due to additional trainable parameters (thresholds and leaks), vanishing spikes at deeper layers, and a non-differentiable binary activation function. Furthermore, an additional data structure, &quot;membrane potential&quot;, responsible for keeping track of temporal information, must be fetched and updated at every timestep in SNNs. To overcome these challenges, we propose a novel SNN-ANN hybrid architecture that combines the strengths of both. Specifically, we leverage the asynchronous compute capabilities of SNN layers to effectively extract the input temporal information. Concurrently, the ANN layers facilitate training and efficient hardware deployment on traditional machine learning hardware such as GPUs. We provide extensive experimental analysis for assigning each layer to be spiking or analog, leading to a network configuration optimized for performance and ease of training. We evaluate our hybrid architecture for optical flow estimation on DSEC-flow and Multi-Vehicle Stereo Event-Camera (MVSEC) datasets. On the DSEC-flow dataset, the hybrid SNN-ANN architecture achieves a 40% reduction in average endpoint error (AEE) with 22% lower energy consumption compared to Full-SNN, and 48% lower AEE compared to Full-ANN, while maintaining comparable energy usage.},
  archive   = {C_IROS},
  author    = {Shubham Negi and Deepika Sharma and Adarsh Kumar Kosta and Kaushik Roy},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802844},
  month     = {10},
  pages     = {2696-2703},
  title     = {Best of both worlds: Hybrid SNN-ANN architecture for event-based optical flow estimation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A deep signed directional distance function for shape
representation. <em>IROS</em>, 2689–2695. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Predicting accurate observations efficiently from novel views is a key requirement for several robotics applications. Existing shape and surface representations, however, either require expensive ray-tracing operations, e.g., in the case of meshes or signed distance functions (SDFs), or offer only a coarse view, e.g., in the case of quadrics or point clouds. We develop a new representation that captures viewing direction and enables fast novel view synthesis. Our first contribution is a signed directional distance function (SDDF) that extends the SDF definition by measuring distance in a desired viewing direction rather than to the nearest point. As a result, SDDF removes post-processing steps for view synthesis required by SDF, such as surface extraction via marching cubes or rendering via sphere tracing, and allows ray-tracing through a single function call. SDDF also encodes by construction the property that distance decreases linearly along the viewing direction. We show that this enables dimensionality reduction in the function representation and guarantees the prediction accuracy independent of the distance to the surface. Recent advances demonstrate impressive performance of deep neural networks for shape learning, including IGR for SDF, Occupancy Networks for occupancy, AtlasNet for meshes, and NeRF for density. Our second contribution, DeepSDDF, is a deep neural network model for SDDF shape learning. Similar to IGR, we show that DeepSDDF can model whole object categories and interpolate or complete shapes from partial views.},
  archive   = {C_IROS},
  author    = {Ehsan Zobeidi and Nikolay Atanasov},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801713},
  month     = {10},
  pages     = {2689-2695},
  title     = {A deep signed directional distance function for shape representation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A non-invasive device for skin cancer diagnosis: First
clinical evidence with spectroscopic data enhanced by machine learning
algorithms. <em>IROS</em>, 2683–2688. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Skin cancer represents a significant global health concern, with melanoma alone accounting for thousands of deaths annually. Early diagnosis is crucial for improving survival rates and reducing healthcare costs. While traditional diagnostic approaches involve visual inspection followed by biopsy, emerging technologies offer less invasive options with improved precision. In this study, a novel non-invasive device was designed, developed, and validated to employ near-infrared reflectance spectroscopy for skin lesion analysis. Furthermore, this work presents a machine learning approach aimed at classifying different types of skin lesions, as well as a new sequential approach to distinguish benign from malignant lesions based on spectral data and exploring the impact of anamnestic features. The device was used in two independent hospitals in Italy to collect data from 69 patients in total, including various types of skin lesions, all of whom followed the standard protocol for screening and diagnosis intervention. The implemented model achieved a recall of 93.8% and an accuracy of 75% for melanoma and benign classification, and a recall of 100% and an accuracy of 98.6% in distinguishing non-melanoma cancer from benign lesions, demonstrating promising results for skin cancer diagnosis utilizing spectral and anamnestic data. In summary, this study contributes to the development of allied non-invasive diagnostic tools and underscores the potential of machine learning in dermatology using spectroscopic data.},
  archive   = {C_IROS},
  author    = {V. Mainardi and L. Carletti and D. Tsiakmakis and M. Dal Canto and T. Melillo and S. Noferi and G. Bagnoni and P. Rubegni and G. Ciuti},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802772},
  month     = {10},
  pages     = {2683-2688},
  title     = {A non-invasive device for skin cancer diagnosis: First clinical evidence with spectroscopic data enhanced by machine learning algorithms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). R2SNet: Scalable domain adaptation for object detection in
cloud–based robotic ecosystems via proposal refinement. <em>IROS</em>,
2676–2682. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a novel approach for scalable domain adaptation in cloud robotics scenarios where robots rely on third–party AI inference services powered by large pre– trained deep neural networks. Our method is based on a downstream proposal–refinement stage running locally on the robots, exploiting a new lightweight DNN architecture, R2SNet. This architecture aims to mitigate performance degradation from domain shifts by adapting the object detection process to the target environment, focusing on relabeling, rescoring, and suppression of bounding–box proposals. Our method allows for local execution on robots, addressing the scalability challenges of domain adaptation without incurring significant computational costs. Real–world results on mobile service robots performing door detection show the effectiveness of the proposed method in achieving scalable domain adaptation.},
  archive   = {C_IROS},
  author    = {Michele Antonazzi and Matteo Luperto and N. Alberto Borghese and Nicola Basilico},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802847},
  month     = {10},
  pages     = {2676-2682},
  title     = {R2SNet: Scalable domain adaptation for object detection in Cloud–Based robotic ecosystems via proposal refinement},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MDHA: Multi-scale deformable transformer with hybrid anchors
for multi-view 3D object detection. <em>IROS</em>, 2668–2675. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-view 3D object detection is a crucial component of autonomous driving systems. Contemporary query-based methods primarily depend either on dataset-specific initialization of 3D anchors, introducing bias, or utilize dense attention mechanisms, which are computationally inefficient and unscalable. To overcome these issues, we present MDHA, a novel sparse query-based framework, which constructs adaptive 3D output proposals using hybrid anchors from multi-view, multi-scale image input. Fixed 2D anchors are combined with depth predictions to form 2.5D anchors, which are projected to obtain 3D proposals. To ensure high efficiency, our proposed Anchor Encoder performs sparse refinement and selects the top-k anchors and features. Moreover, while existing multi-view attention mechanisms rely on projecting reference points to multiple images, our novel Circular Deformable Attention mechanism only projects to a single image but allows reference points to seamlessly attend to adjacent images, improving efficiency without compromising on performance. On the nuScenes val set, it achieves 46.4% mAP and 55.0% NDS with a ResNet101 backbone. MDHA significantly outperforms the baseline where anchor proposals are modelled as learnable embeddings. Code is available at https://github.com/NaomiEX/MDHA.},
  archive   = {C_IROS},
  author    = {Michelle Adeline and Junn Yong Loo and Vishnu Monn Baskaran},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802056},
  month     = {10},
  pages     = {2668-2675},
  title     = {MDHA: Multi-scale deformable transformer with hybrid anchors for multi-view 3D object detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scale disparity of instances in interactive point cloud
segmentation. <em>IROS</em>, 2660–2667. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Interactive point cloud segmentation has become a pivotal task for understanding 3D scenes, enabling users to guide segmentation models with simple interactions such as clicks, therefore significantly reducing the effort required to tailor models to diverse scenarios and new categories. However, in the realm of interactive segmentation, the meaning of instance diverges from that in instance segmentation, because users might desire to segment instances of both thing and stuff categories that vary greatly in scale. Existing methods have focused on thing categories, neglecting the segmentation of stuff categories and the difficulties arising from scale disparity. To bridge this gap, we propose ClickFormer, an innovative interactive point cloud segmentation model that accurately segments instances of both thing and stuff categories. We propose a query augmentation module to augment click queries by a global query sampling strategy, thus maintaining consistent performance across different instance scales. Additionally, we employ global attention in the query-voxel transformer to mitigate the risk of generating false positives, along with several other network structure improvements to further enhance the model’s segmentation performance. Experiments demonstrate that ClickFormer outperforms existing interactive point cloud segmentation methods across both indoor and outdoor datasets, providing more accurate segmentation results with fewer user clicks in an open-world setting. Project page: https://sites.google.com/view/clickformer/},
  archive   = {C_IROS},
  author    = {Chenrui Han and Xuan Yu and Yuxuan Xie and Yili Liu and Sitong Mao and Shunbo Zhou and Rong Xiong and Yue Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802361},
  month     = {10},
  pages     = {2660-2667},
  title     = {Scale disparity of instances in interactive point cloud segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Non-repetitive: A promising LiDAR scanning pattern.
<em>IROS</em>, 2653–2659. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {LiDAR is an essential sensor for intelligent vehicles. Recently, LiDARs used in vehicles produced by different companies have significant differences in their scanning patterns. Some vehicles use mechanical and solid-state (repetitive) LiDARs, while others use prism-based (non-repetitive) LiDARs. The scanning pattern of a LiDAR has a profound impact on its scanning performance. To investigate the influence of LiDAR scanning patterns, we created the &quot;Repetitive-or-not&quot; dataset, which is collected simultaneously by LiDARs with both repetitive and non-repetitive scanning patterns in the CARLA simulation environment. Using this dataset, we conducted a comprehensive statistical analysis of the scanning ability of repetitive and non-repetitive LiDARs. Furthermore, we looked into the effects of these two LiDAR scanning patterns on the performance of various 3D object detection algorithms. Finally, we explored the domain gap in the point cloud data produced by repetitive and non-repetitive LiDARs. Through an in-depth investigation of the &quot;Repetitive-or-not&quot; dataset, we have discovered that non-repetitive LiDAR shows great promise. This conclusion is primarily supported by its superior object scanning capabilities.},
  archive   = {C_IROS},
  author    = {Angchen Xie and Yeqiang Qian and Weihao Yan and Chunxiang Wang and Ming Yang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802663},
  month     = {10},
  pages     = {2653-2659},
  title     = {Non-repetitive: A promising LiDAR scanning pattern},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised 3D part decomposition via leveraged gaussian
splatting. <em>IROS</em>, 2647–2652. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel unsupervised method for motion-based 3D part decomposition of articulated objects using a single monocular video of a dynamic scene. In contrast to existing unsupervised methods relying on optical flow or tracking techniques, our approach addresses this problem without additional information by leveraging Gaussian splatting techniques. We generate a series of Gaussians from a monocular video and analyze their relationships to decompose the dynamic scene into motion-based parts. To decompose dynamic scenes consisting of articulated objects, we design an articulated deformation field suitable for the movement of articulated objects. And to effectively understand the relationships of Gaussians of different shapes, we propose a 3D reconstruction loss using 3D occupied voxel maps generated from the Gaussians. Experimental results demonstrate that our method outperforms existing approaches in terms of 3D part decomposition for articulated objects. More demos and code are available at https://choonsik93.github.io/artnerf/.},
  archive   = {C_IROS},
  author    = {Jae Goo Choy and Geonho Cha and Hogun Kee and Songhwai Oh},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802165},
  month     = {10},
  pages     = {2647-2652},
  title     = {Unsupervised 3D part decomposition via leveraged gaussian splatting},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Embodied uncertainty-aware object segmentation.
<em>IROS</em>, 2639–2646. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce uncertainty-aware object instance segmentation (UncOS) and demonstrate its usefulness for embodied interactive segmentation. To deal with uncertainty in robot perception, we propose a method for generating a hypothesis distribution of object segmentation. We obtain a set of region-factored segmentation hypotheses together with confidence estimates by making multiple queries of large pre-trained models. This process can produce segmentation results that achieve state-of-the-art performance on unseen object segmentation problems. The output can also serve as input to a belief-driven process for selecting robot actions to perturb the scene to reduce ambiguity. We demonstrate the effectiveness of this method in real-robot experiments. Website: https://sites.google.com/view/embodied-uncertain-seg.},
  archive   = {C_IROS},
  author    = {Xiaolin Fang and Leslie Pack Kaelbling and Tomás Lozano-Pérez},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801562},
  month     = {10},
  pages     = {2639-2646},
  title     = {Embodied uncertainty-aware object segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BAM: Box abstraction monitors for real-time OoD detection in
object detection. <em>IROS</em>, 2632–2638. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Out-of-distribution (OoD) detection techniques for deep neural networks (DNNs) become crucial thanks to their filtering of abnormal inputs, especially when DNNs are used in safety-critical applications and interact with an open and dynamic environment. Nevertheless, integrating OoD detection into state-of-the-art (SOTA) object detection DNNs poses significant challenges, partly due to the complexity introduced by the SOTA OoD construction methods, which require the modification of DNN architecture and the introduction of complex loss functions. This paper proposes a simple, yet surprisingly effective, method that requires neither retraining nor architectural change in object detection DNN, called Box Abstraction-based Monitors (BAM). The novelty of BAM stems from using a finite union of convex box abstractions to capture the learned features of objects for in-distribution (ID) data, and an important observation that features from OoD data are more likely to fall outside of these boxes. The union of convex regions within the feature space allows the formation of non-convex and interpretable decision boundaries, overcoming the limitations of VOS-like detectors without sacrificing real-time performance. Experiments integrating BAM into Faster R-CNN-based object detection DNNs demonstrate a considerably improved performance against SOTA OoD detection techniques, with a reduction in the false detection rate of over 10% in most cases.},
  archive   = {C_IROS},
  author    = {Changshun Wu and Weicheng He and Chih-Hong Cheng and Xiaowei Huang and Saddek Bensalem},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801584},
  month     = {10},
  pages     = {2632-2638},
  title     = {BAM: Box abstraction monitors for real-time OoD detection in object detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). CTS: Sim-to-real unsupervised domain adaptation on 3D
detection. <em>IROS</em>, 2624–2631. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simulation data can be accurately labeled and have been expected to improve the performance of data-driven algorithms, including object detection. However, due to the various domain inconsistencies from simulation to reality (sim-to-real), cross-domain object detection algorithms usually suffer from dramatic performance drops. While numerous unsupervised domain adaptation (UDA) methods have been developed to address cross-domain tasks between real-world datasets, progress in sim-to-real remains limited. This paper presents a novel Complex-to-Simple (CTS) framework to transfer models from labeled simulation (source) to unlabeled reality (target) domains. Based on a two-stage detector, the novelty of this work is threefold: 1) developing fixed-size anchor heads and RoI augmentation to address size bias and feature diversity between two domains, thereby improving the quality of pseudo-label; 2) developing a novel corner-format representation of aleatoric uncertainty (AU) for the bounding box, to uniformly quantify pseudo-label quality; 3) developing a noise-aware mean teacher domain adaptation method based on AU, as well as object-level and frame-level sampling strategies, to migrate the impact of noisy labels. Experimental results demonstrate that our proposed approach significantly enhances the sim-to-real domain adaptation capability of 3D object detection models, outperforming state-of-the-art cross-domain algorithms, which are usually developed for real-to-real UDA tasks.},
  archive   = {C_IROS},
  author    = {Meiying Zhang and Weiyuan Peng and Guangyao Ding and Chenyang Lei and Chunlin Ji and Qi Hao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801462},
  month     = {10},
  pages     = {2624-2631},
  title     = {CTS: Sim-to-real unsupervised domain adaptation on 3D detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MOSFormer: A transformer-based multi-modal fusion network
for moving object segmentation. <em>IROS</em>, 2618–2623. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D moving object segmentation (MOS) is vital for autonomous systems, providing essential information for downstream tasks like mapping and localization. However, current MOS methods face challenges due to the limitation of existing datasets, which are sparse in moving objects and limited in scene diversity. Meanwhile, the prevalent methods are projection-based, struggling with the challenge of blurred boundaries. To tackle the dataset issue, we introduce a nuScenes-based MOS dataset, which provides richer scenes and more dynamic instances. To alleviate the boundary blur-ring issue and further improve accuracy and generalizability, we propose a dual-branch multimodal fusion MOS network, MOSFormer. The Transformer structure is incorporated to extract spatio-temporal information better, while image semantic information is utilized to refine the boundaries of moving objects. Finally, experiments on two datasets show that our method achieves state-of-the-art performance, and a mapping experiment with our method confirms its effectiveness in downstream tasks such as mapping and localization.},
  archive   = {C_IROS},
  author    = {Zike Cheng and Hengwang Zhao and Qiyuan Shen and Weihao Yan and Chunxiang Wang and Ming Yang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802637},
  month     = {10},
  pages     = {2618-2623},
  title     = {MOSFormer: A transformer-based multi-modal fusion network for moving object segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3D object detection via stereo pyramid transformers with
rich semantic feature fusion. <em>IROS</em>, 2610–2617. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Camera-based 3D object detectors, prized for their broader applicability and cost-effectiveness compared to LiDAR sensors, still grapple with the inherently ill-posed nature of depth extraction from images. In this work, we present a novel approach that employs a transformer-based backbone and a fused geometry volume to bolster feature richness and elevate detection accuracy. Firstly, we propose the Stereo Pyramid Transformer backbone to extract features from stereo images, which can capture global information and establish cross-image semantic connections. Then, to tackle the challenge posed by small baseline binocular cameras, we propose to fuse stereo geometry volumes constructed by Stereo Plane Sweeping Volume (SPSV), Monocular Semantic Volume (MSV), and Lifted Volume (LV) to create finely detailed feature volumes. Through extensive experiments on both the KITTI and our datasets, our approach not only surpasses all existing transformer-based stereo 3D detection methods but also marks a significant milestone by achieving comparable performance with the leading CNN-based 3D detectors for the first time.},
  archive   = {C_IROS},
  author    = {Rongqi Gu and Chu Yang and Yaohan Lu and Peigen Liu and Fei Wu and Guang Chen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802137},
  month     = {10},
  pages     = {2610-2617},
  title     = {3D object detection via stereo pyramid transformers with rich semantic feature fusion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SWCF-net: Similarity-weighted convolution and local-global
fusion for efficient large-scale point cloud semantic segmentation.
<em>IROS</em>, 2602–2609. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large-scale point cloud consists of a multitude of individual objects, thereby encompassing rich structural and underlying semantic contextual information, resulting in a challenging problem in efficiently segmenting a point cloud. Most existing researches mainly focus on capturing intricate local features without giving due consideration to global ones, thus failing to leverage semantic context. In this paper, we propose a Similarity-Weighted Convolution and local-global Fusion Network, named SWCF-Net, which takes into account both local and global features. We propose a Similarity-Weighted Convolution (SWConv) to effectively extract local features, where similarity weights are incorporated into the convolution operation to enhance the generalization capabilities. Then, we employ a downsampling operation on the K and V channels within the attention module, thereby reducing the quadratic complexity to linear, enabling Transformer to deal with large-scale point cloud. At last, orthogonal components are extracted in the global features and then aggregated with local features, thereby eliminating redundant information between local and global features and consequently promoting efficiency. We evaluate SWCF-Net on large-scale outdoor datasets SemanticKITTI and Toronto3D. Our experimental results demonstrate the effectiveness of the proposed network. Our method achieves a competitive result with less computational cost, and is able to handle large-scale point clouds efficiently. The code is available at https://github.com/Sylva-Lin/SWCF-Net.},
  archive   = {C_IROS},
  author    = {Zhenchao Lin and Li He and Hongqiang Yang and Xiaoqun Sun and Guojin Zhang and Weinan Chen and Yisheng Guan and Hong Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801684},
  month     = {10},
  pages     = {2602-2609},
  title     = {SWCF-net: Similarity-weighted convolution and local-global fusion for efficient large-scale point cloud semantic segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Proto-CLIP: Vision-language prototypical network for
few-shot learning. <em>IROS</em>, 2594–2601. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel framework for few-shot learning by leveraging large-scale vision-language models such as CLIP [1]. Motivated by unimodal prototypical networks for few-shot learning, we introduce Proto-CLIP which utilizes image prototypes and text prototypes for few-shot learning. Specifically, Proto-CLIP adapts the image and text encoder embeddings from CLIP in a joint fashion using few-shot examples. The embeddings from the two encoders are used to compute the respective prototypes of image classes for classification. During adaptation, we propose aligning the image and text prototypes of the corresponding classes. Such alignment is beneficial for few-shot classification due to the reinforced contributions from both types of prototypes. Proto-CLIP has both training-free and fine-tuned variants. We demonstrate the effectiveness of our method by conducting experiments on benchmark datasets for few-shot learning, as well as in the real world for robot perception1.},
  archive   = {C_IROS},
  author    = {Jishnu Jaykumar P and Kamalesh Palanisamy and Yu-Wei Chao and Xinya Du and Yu Xiang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801660},
  month     = {10},
  pages     = {2594-2601},
  title     = {Proto-CLIP: Vision-language prototypical network for few-shot learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model agnostic defense against adversarial patch attacks on
object detection in unmanned aerial vehicles. <em>IROS</em>, 2586–2593.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Object detection forms a key component in Unmanned Aerial Vehicles (UAVs) for completing high-level tasks that depend on the awareness of objects on the ground from an aerial perspective. In that scenario, adversarial patch attacks on an onboard object detector can severely impair the performance of upstream tasks. This paper proposes a novel model-agnostic defense mechanism against the threat of adversarial patch attacks in the context of UAV-based object detection. We formulate adversarial patch defense as an occlusion removal task. The proposed defense method can neutralize adversarial patches located on objects of interest, without exposure to adversarial patches during training. Our lightweight single-stage defense approach allows us to maintain a model-agnostic nature, that once deployed does not require to be updated in response to changes in the object detection pipeline. The evaluations in digital and physical domains show the feasibility of our method for deployment in UAV object detection pipelines, by significantly decreasing the Attack Success Ratio without incurring significant processing costs. As a result, the proposed defense solution can improve the reliability of object detection for UAVs.},
  archive   = {C_IROS},
  author    = {Saurabh Pathak and Samridha Shrestha and Abdelrahman AlMahmoud},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802588},
  month     = {10},
  pages     = {2586-2593},
  title     = {Model agnostic defense against adversarial patch attacks on object detection in unmanned aerial vehicles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FedRC: A rapid-converged hierarchical federated learning
framework in street scene semantic understanding. <em>IROS</em>,
2578–2585. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Street Scene Semantic Understanding (denoted as TriSU) is a crucial but complex task for world-wide distributed autonomous driving (AD) vehicles (e.g., Tesla). Its inference model faces poor generalization issue due to inter-city domain-shift. Hierarchical Federated Learning (HFL) offers a potential solution for improving TriSU model generalization, but suffers from slow convergence rate because of vehicles’ surrounding heterogeneity across cities. Going beyond existing HFL works that have deficient capabilities in complex tasks, we propose a rapid-converged heterogeneous HFL framework (FedRC) to address the inter-city data heterogeneity and accelerate HFL model convergence rate. In our proposed FedRC framework, both single RGB image and RGB dataset are modelled as Gaussian distributions in HFL aggregation weight design. This approach not only differentiates each RGB sample instead of typically equalizing them, but also considers both data volume and statistical properties rather than simply taking data quantity into consideration. Extensive experiments on the TriSU task using across-city datasets demonstrate that FedRC converges faster than the state-of-the-art benchmark by 38.7%, 37.5%, 35.5%, and 40.6% in terms of mIoU, mPrecision, mRecall, and mF1, respectively. Furthermore, qualitative evaluations in the CARLA simulation environment confirm that the proposed FedRC framework delivers top-tier performance.},
  archive   = {C_IROS},
  author    = {Wei-Bin Kou and Qingfeng Lin and Ming Tang and Shuai Wang and Guangxu Zhu and Yik-Chung Wu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802073},
  month     = {10},
  pages     = {2578-2585},
  title     = {FedRC: A rapid-converged hierarchical federated learning framework in street scene semantic understanding},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continuous rapid learning by human imitation using audio
prompts and one-shot learning. <em>IROS</em>, 2572–2577. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the general field of collaborative robotics, one of the topics of greatest interest to the scientific community is the ability to learn to perform certain actions by imitating humans. If we think about humans, when someone teaches us how to perform a certain action, we often need to be shown just one time how to do it. Likewise, we believe that robotics should follow this line, using models that do not involve the capture of huge data sets or exhaustive training. Furthermore, while general models can typically be pretrained offline, the robot must quickly adapt to new knowledge without requiring an expensive retraining process. In this article we present a flexible neural learning architecture that allows a robot to learn how-to pick-up a given object just by watching how a human does it. Then, the robot will be able to pick up the current object, or other objects previously learned, anywhere in the work field, with a simple audible indication from the user. This is achieved based on continuous incremental learning techniques and generic segmentation networks integrated with Siamese network models according to the recently proposed CP-CVV method. Results are presented for the success rate in grasping a varied set of objects.},
  archive   = {C_IROS},
  author    = {Jaime Duque-Domingo and Miguel García-Gómez and Eduardo Zalama and Jaime Gómez-García-Bermejo},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801547},
  month     = {10},
  pages     = {2572-2577},
  title     = {Continuous rapid learning by human imitation using audio prompts and one-shot learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Swiss DINO: Efficient and versatile vision framework for
on-device personal object search. <em>IROS</em>, 2564–2571. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we address a recent trend in robotic home appliances to include vision systems on personal devices, capable of personalizing the appliances on the fly. In particular, we formulate and address an important technical task of personal object search, which involves localization and identification of personal items of interest on images captured by robotic appliances, with each item referenced only by a few annotated images. The task is crucial for robotic home appliances and mobile systems, which need to process personal visual scenes or to operate with particular personal objects (e.g., for grasping or navigation). In practice, personal object search presents two main technical challenges. First, a robot vision system needs to be able to distinguish between many fine-grained classes, in the presence of occlusions and clutter. Second, the strict resource requirements for the on-device system restrict the usage of most state-of-the-art methods for few-shot learning and often prevent on-device adaptation. In this work, we propose Swiss DINO: a simple yet effective framework for one-shot personal object search based on the recent DINOv2 transformer model, which was shown to have strong zero-shot generalization properties. Swiss DINO handles challenging on-device personalized scene understanding requirements and does not require any adaptation training. We show significant improvement (up to 55%) in segmentation and recognition accuracy compared to the common lightweight solutions, and significant footprint reduction of backbone inference time (up to 100×) and GPU consumption (up to 10×) compared to the heavy transformer-based solutions1.},
  archive   = {C_IROS},
  author    = {Kirill Paramonov and Jia-Xing Zhong and Umberto Michieli and Jijoong Moon and Mete Ozay},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802332},
  month     = {10},
  pages     = {2564-2571},
  title     = {Swiss DINO: Efficient and versatile vision framework for on-device personal object search},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Architectural-scale artistic brush painting with a hybrid
cable robot. <em>IROS</em>, 2556–2563. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot art presents an opportunity to both showcase and advance state-of-the-art robotics through the challenging task of creating art. Creating large-scale artworks in particular engages the public in a way that small-scale works cannot, and the distinct qualities of brush strokes contribute to an organic and human-like quality. Combining the large scale of murals with the strokes of the brush medium presents an especially impactful result, but also introduces unique challenges in maintaining precise, dextrous motion control of the brush across such a large workspace. In this work, we present the first robot to our knowledge that can paint architectural-scale murals with a brush. We create a hybrid robot consisting of a cable-driven parallel robot and 4 degree of freedom (DoF) serial manipulator to paint a 27m by 3.7m mural on windows spanning 2-stories of a building. We discuss our approach to achieving both the scale and accuracy required for brush-painting a mural through a combination of novel mechanical design elements, coordinated planning and control, and on-site calibration algorithms with experimental validations.},
  archive   = {C_IROS},
  author    = {Gerry Chen and Tristan Al-Haddad and Frank Dellaert and Seth Hutchinson},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802489},
  month     = {10},
  pages     = {2556-2563},
  title     = {Architectural-scale artistic brush painting with a hybrid cable robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AEGO: Modeling attention for HRI in ego-sphere neural
networks. <em>IROS</em>, 2549–2555. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite important progress in recent years, social robots are still far away from showing advanced behavior for interaction and adaptation in human environments. Thus, we are interested in studying social cognition in human-robot interaction (HRI), notably in improving communication skills relying on joint attention (JA) and knowledge sharing. Since JA involves low-level cognitive processes in humans, we take into account the implications of Moravec’s Paradox and focus on the aspect of knowledge representation. Inspired by 4E cognition principles, we study egocentric localization through the concept of sensory ego-sphere. We propose a neural network architecture named AEGO to model attention for each agent in interaction and show how to fuse information in a common representation space. From the perspective of dynamic fields theory, AEGO takes into account the dynamics of bottom-up and top-down modulation processes and the effects of neural excitatory and inhibitory synaptic interaction. In this work we evaluate the model in simulation and experiments with the robot Pepper in JA tasks based on proprioception, vision, rudimentary natural language and Hebbian plasticity. Results show that AEGO is convenient for HRI, allowing the human and the robot to share attention and knowledge about objects in scenarios close to everyday situations. AEGO constitutes a novel brain-inspired architecture to model attention that is suitable for multi-agent applications relying on social cognition skills, having the potential to generalize to several robotics platforms and HRI scenarios.},
  archive   = {C_IROS},
  author    = {Hendry Ferreira Chame and Rachid Alami},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802086},
  month     = {10},
  pages     = {2549-2555},
  title     = {AEGO: Modeling attention for HRI in ego-sphere neural networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Context-aware conversation adaptation for human-robot
interaction. <em>IROS</em>, 2542–2548. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing conversational robots are mostly reactive in that the interactions are usually initiated by the users. With the knowledge of the environmental context such as people’s daily activities, robots can be more intelligent and proactive. In this paper, we proposed a context-aware conversation adaptation system (CACAS) for human-robot interaction (HRI). First, a context recognition module and a language processing module are developed to obtain the context information, user intent and slots, which become part of the state. Second, a reinforcement learning algorithm is developed to train an initial policy with a simulated user. User feedback data is collected through HRI using the initial policy. Third, a policy combining the reinforcement learning-based policy with the neural network-based policy is adapted based on the user feedback. We conducted both simulated user tests and real human subject tests to evaluate the proposed system. The results show that CACAS achieved a success rate of 85% in the real human subject test and 87.5% of participants were satisfied with the adaptation results. For the simulation test, CACAS had the highest success rate compared with the baseline methods.},
  archive   = {C_IROS},
  author    = {Zhidong Su and Weihua Sheng},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802500},
  month     = {10},
  pages     = {2542-2548},
  title     = {Context-aware conversation adaptation for human-robot interaction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A service robot in the wild: Analysis of users intentions,
robot behaviors, and their impact on the interaction. <em>IROS</em>,
2536–2541. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider a service robot that offers chocolate treats to people passing in its proximity: it has the capability of predicting in advance a person’s intention to interact, and to actuate an &quot;offering&quot; gesture, subtly extending the tray of chocolates towards a given target. We run the system for more than 5 hours across 3 days and two different crowded public locations; the system implements three possible behaviors that are randomly toggled every few minutes: passive (e.g. never performing the offering gesture); or active, triggered by either a naive distance-based rule, or a smart approach that relies on various behavioral cues of the user. We collect a real-world dataset that includes information on 1777 users with several spontaneous human-robot interactions and study the influence of robot actions on people’s behavior. Our comprehensive analysis suggests that users are more prone to engage with the robot when it proactively starts the interaction. We release the dataset and provide insights to make our work reproducible for the community. Also, we report qualitative observations collected during the acquisition campaign and identify future challenges and research directions in the domain of social human-robot interaction.},
  archive   = {C_IROS},
  author    = {Simone Arreghini and Gabriele Abbate and Alessandro Giusti and Antonio Paolillo},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801875},
  month     = {10},
  pages     = {2536-2541},
  title     = {A service robot in the wild: Analysis of users intentions, robot behaviors, and their impact on the interaction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Belief-aided navigation using bayesian reinforcement
learning for avoiding humans in blind spots. <em>IROS</em>, 2528–2535.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent research on mobile robot navigation has focused on socially aware navigation in crowded environments. However, existing methods do not adequately account for human–robot interactions and demand accurate location information from omnidirectional sensors, rendering them unsuitable for practical applications. In response to this need, this study introduces a novel algorithm, BNBRL+, predicated on the partially observable Markov decision process framework to assess risks in unobservable areas and formulate movement strategies under uncertainty. BNBRL+ consolidates belief algorithms with Bayesian neural networks to probabilistically infer beliefs based on the positional data of humans. It further integrates the interactions between the robot, humans, and inferred beliefs to determine the navigation paths, thereby facilitating socially aware navigation. Through experiments in various risk-laden scenarios, this study validates the effectiveness of BNBRL+ in navigating crowded environments with blind spots. The model’s ability to navigate effectively in spaces with limited visibility and avoid obstacles dynamically can significantly improve the safety and reliability of autonomous vehicles. The complement source code can be accessed here: https://github.com/JinnnK/BNBRLplus.},
  archive   = {C_IROS},
  author    = {Jinyeob Kim and Daewon Kwak and Hyunwoo Rim and Donghan Kim},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802765},
  month     = {10},
  pages     = {2528-2535},
  title     = {Belief-aided navigation using bayesian reinforcement learning for avoiding humans in blind spots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Are large language models aligned with people’s social
intuitions for human–robot interactions? <em>IROS</em>, 2520–2527. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large language models (LLMs) are increasingly used in robotics, especially for high-level action planning. Meanwhile, many robotics applications involve human supervisors or collaborators. Hence, it is crucial for LLMs to generate socially acceptable actions that align with people’s preferences and values. In this work, we test whether LLMs capture people’s intuitions about behavior judgments and communication preferences in human-robot interaction (HRI) scenarios. For evaluation, we reproduce three HRI user studies, comparing the output of LLMs with that of real participants. We find that GPT-4 strongly outperforms other models, generating answers that correlate strongly with users’ answers in two studies — the first study dealing with selecting the most appropriate communicative act for a robot in various situations (rs = 0.82), and the second with judging the desirability, intentionality, and surprisingness of behavior (rs = 0.83). However, for the last study, testing whether people judge the behavior of robots and humans differently, no model achieves strong correlations. Moreover, we show that vision models fail to capture the essence of video stimuli and that LLMs tend to rate different communicative acts and behavior desirability higher than people.},
  archive   = {C_IROS},
  author    = {Lennart Wachowiak and Andrew Coles and Oya Celiktutan and Gerard Canal},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801325},
  month     = {10},
  pages     = {2520-2527},
  title     = {Are large language models aligned with people’s social intuitions for Human–Robot interactions?},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Investigating behavioral and cognitive changes induced by
autonomous delivery robots in incidentally copresent persons*.
<em>IROS</em>, 2514–2519. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous delivery robots (ADRs) encounter incidentally copresent persons (InCoPs) during their delivery journeys. Despite the potential for ADRs&#39; behavior to influence the behavior and cognition of InCoPs, there is limited research on the interaction between ADRs and InCoPs. Therefore, in this study, we conducted a within-participants experiment (N=30) with a 3 (confederate types: humans vs. high anthropomorphism robots vs. low anthropomorphism robots) x 2 (jaywalking status: jaywalking vs. not jaywalking) design to investigate the impact of ADRs on InCoPs’ behavioral and cognitive changes induced by the social influence of ADRs. During the experiment, participants watched a video depicting interactions between ADRs and InCoPs at a crosswalk. Each participant was immersed in the video as an InCoP, instructed to make jaywalking decisions, and subsequently completed questionnaires. Results indicated that, behaviorally, participants displayed similar levels of conformity towards jaywalking behaviors across both the human and robot confederates. Cognitively, there were significant differences in morality based on the confederate types. Additionally, robots that refrained from jaywalking received more positive ratings in terms of morality and intention to use. This study confirms that ADRs have the capacity to induce conformity similar to humans and that the ethical behavior of ADRs can positively influence InCoPs&#39; impressions and intention to use toward ADRs.},
  archive   = {C_IROS},
  author    = {Nayoung Kim and Sonya S. Kwak},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801351},
  month     = {10},
  pages     = {2514-2519},
  title     = {Investigating behavioral and cognitive changes induced by autonomous delivery robots in incidentally copresent persons*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OmniRace: 6D hand pose estimation for intuitive guidance of
racing drone. <em>IROS</em>, 2508–2513. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents the OmniRace approach to controlling a racing drone with 6-degree of freedom (DoF) hand pose estimation and gesture recognition. To our knowledge, this is the first technology enabling low-level control of high-speed drones through gestures. OmniRace employs a gesture interface based on computer vision and a deep neural network to estimate 6-DoF hand pose. The advanced machine learning algorithm robustly interprets human gestures, allowing users to control drone motion intuitively. Real-time control tests validate the system’s effectiveness and its potential to revolutionize drone racing and other applications. Experimental results conducted in simulation environment revealed that OmniRace allows the users to complite the UAV race track significantly (by 25.1%) faster and to decrease the length of the test drone path (from 102.9 to 83.7 m). Users preferred the gesture interface for attractiveness (1.57 UEQ score), hedonic quality (1.56 UEQ score), and lower perceived temporal demand (32.0 score in NASA-TLX), while noting the high efficiency (0.75 UEQ score) and low physical demand (19.0 score in NASA-TLX) of the baseline remote controller. The deep neural network attains an average accuracy of 99.75% when applied to both normalized datasets and raw datasets. OmniRace can potentially change the way humans interact with and navigate racing drones in dynamic and complex environments. The source code is available at https://github.com/SerValera/OmniRace.git.},
  archive   = {C_IROS},
  author    = {Valerii Serpiva and Aleksey Fedoseev and Sausar Karaf and Ali Alridha Abdulkarim and Dzmitry Tsetserukou},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801907},
  month     = {10},
  pages     = {2508-2513},
  title     = {OmniRace: 6D hand pose estimation for intuitive guidance of racing drone},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Empathetic response generation system: Enhancing photo
reminiscence chatbot with emotional context analysis. <em>IROS</em>,
2502–2507. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dementia affects 50 million people worldwide, underscoring the urgent need for effective interventions to enhance their well-being. While reminiscence intervention shows promise, its implementation is hindered by limited human resources, making machine-aided systems a viable automated solution for seamless photo-reminiscence sessions. In this paper, we introduce an empathetic response generation system specifically designed to enhance a question-only photo-reminiscence chatbot, with a focus on improving emotional context understanding and enhancing conversation engagement. We leverage Transformers to encode dialogue history, infer emotional states from user responses, and extract named entities. By combining template-based utterances with a retrieval chatbot, our system generates relevant and empathetic responses to user replies. Our system’s effectiveness is validated through human evaluations using a Likert-like scale to assess engagement levels. The results demonstrate that our approach surpasses both the question-only system and other models from existing works, including retrieval and generated models. This highlights our system’s potential to enhance interactions and engagement, advancing technology-driven interventions for dementia that improve well-being and quality of life.},
  archive   = {C_IROS},
  author    = {Alberto Herrera and Xiaobei Qian and Li-Chen Fu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802527},
  month     = {10},
  pages     = {2502-2507},
  title     = {Empathetic response generation system: Enhancing photo reminiscence chatbot with emotional context analysis},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Where and when should the teleoperated avatar look: Gaze
instruction dataset for enhanced teleoperated avatar communication*.
<em>IROS</em>, 2494–2501. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Effective teleoperated avatar communication requires expressing social behaviors. Gaze behavior is one of the crucial social behaviors and includes reflexive reactions to the avatar’s surroundings and intentional responses to the operator’s speech and actions. Teleoperated avatars must have their gaze behavior controlled according to situational changes in both the avatar’s and operator’s contexts. However, it is not clear how to adjust the avatar’s gaze in response to changes in both situations. In this paper, we collect a dataset of gazing positions that the avatar is instructed to face, taking into account both avatar and operator situations, and annotation labels that represent both situations in detail. We then exploratorily analyze the ratio of gazing positions per situation through dynamic area-of-interest (AOI) analysis. Our analysis provides insights into determining the gaze behavior of teleoperated avatars.},
  archive   = {C_IROS},
  author    = {Kenya Hoshimure and Jun Baba and Junya Nakanishi and Yuichiro Yoshikawa and Hiroshi Ishiguro},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802049},
  month     = {10},
  pages     = {2494-2501},
  title     = {Where and when should the teleoperated avatar look: Gaze instruction dataset for enhanced teleoperated avatar communication*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multimodal coherent explanation generation of robot
failures. <em>IROS</em>, 2487–2493. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The explainability of a robot’s actions is crucial to its acceptance in social spaces. Explaining why a robot fails to complete a given task is particularly important for non-expert users to be aware of the robot’s capabilities and limitations. So far, research on explaining robot failures has only considered generating textual explanations, even though several studies have shown the benefits of multimodal ones. However, a simple combination of multiple modalities may lead to semantic incoherence between the information across different modalities - a problem that is not well-studied. An incoherent multimodal explanation can be difficult to understand, and it may even become inconsistent with what the robot and the human observe and how they perform reasoning with the observations. Such inconsistencies may lead to wrong conclusions about the robot’s capabilities. In this paper, we introduce an approach to generate coherent multimodal explanations by checking the logical coherence of explanations from different modalities, followed by refinements as required. We propose a classification approach for coherence assessment, where we evaluate if an explanation logically follows another. Our experiments suggest that fine-tuning a neural network that was pre-trained to recognize textual entailment, performs well for coherence assessment of multimodal explanations. Code &amp; data: https://pradippramanick.github.io/coherent-explain/.},
  archive   = {C_IROS},
  author    = {Pradip Pramanick and Silvia Rossi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802671},
  month     = {10},
  pages     = {2487-2493},
  title     = {Multimodal coherent explanation generation of robot failures},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PhotoBot: Reference-guided interactive photography via
natural language. <em>IROS</em>, 2479–2486. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce PhotoBot, a framework for fully automated photo acquisition based on an interplay between high-level human language guidance and a robot photographer. We propose to communicate photography suggestions to the user via reference images that are selected from a curated gallery. We leverage a visual language model (VLM) and an object detector to characterize the reference images via textual descriptions and then use a large language model (LLM) to retrieve relevant reference images based on a user’s language query through text-based reasoning. To correspond the reference image and the observed scene, we exploit pretrained features from a vision transformer capable of capturing semantic similarity across marked appearance variations. Using these features, we compute suggested pose adjustments for an RGB-D camera by solving a perspective-n-point (PnP) problem. We demonstrate our approach using a manipulator equipped with a wrist camera. Our user studies show that photos taken by PhotoBot are often more aesthetically pleasing than those taken by users themselves, as measured by human feedback. We also show that PhotoBot can generalize to other reference sources such as paintings.},
  archive   = {C_IROS},
  author    = {Oliver Limoyo and Jimmy Li and Dmitriy Rivkin and Jonathan Kelly and Gregory Dudek},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801790},
  month     = {10},
  pages     = {2479-2486},
  title     = {PhotoBot: Reference-guided interactive photography via natural language},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Good things come in threes: The impact of robot
responsiveness on workload and trust in multi-user human-robot
collaboration. <em>IROS</em>, 2471–2478. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human-robot collaboration has the potential of unlocking new manufacturing paradigms thanks to the introduction of a robotic architecture in a production chain that involves human workers. A possible innovative declination of this is the use of collaborative robots to enable two workers to concurrently act on the same manufacturing target without causing mutual disturbances. By doing so, the efficiency of the process would be preserved while reducing the production times. This work designs a physical collaborative task that involves two users and one collaborative robot. The users act in the scenario in a concurrent way on the same target object, while the robot physically intervenes in the scene as a mediator by adjusting the position and orientation of the object to accommodate both users at the same time. Through this experimental setup, 78 apprentices and teachers of the BAE Systems Academy for Skills and Knowledge Centre were recruited to investigate the users’ perception of the task workload and trust towards the robotic system. Specifically, they performed the same task under two experimental conditions, in which the robot responded to changes in the interaction in a reactive or timed way, respectively. The statistical analysis showed that a timed response of the robot was associated with lower perceived workload and higher predictability of the system.},
  archive   = {C_IROS},
  author    = {Francesco Semeraro and Jon Carberry and James Leadbetter and Angelo Cangelosi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801410},
  month     = {10},
  pages     = {2471-2478},
  title     = {Good things come in threes: The impact of robot responsiveness on workload and trust in multi-user human-robot collaboration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Emotional tandem robots: How different robot behaviors
affect human perception while controlling a mobile robot. <em>IROS</em>,
2465–2470. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In human-robot interaction (HRI), we study how humans interact with robots, but also the effects of robot behavior on human perception and well-being. Especially, the influence on humans by tandem robots with one human-controlled and one autonomous robot or even semi-autonomous multi-robot systems is not yet fully understood. Here, we focus on a leader-follower scenario and study how emotionally expressive motion patterns of a small, mobile follower robot affect the perception of a human operator controlling the leading robot. We examined three distinct emotional behaviors for the follower compared to a neutral condition: angry, happy, and sad. We asked participants to maneuver the leader robot along a set path while experiencing each follower behavior in a randomized order. We identified a significant shift in subjective attention toward the follower with emotionally expressive behaviors compared to the neutral condition. For example, the angry behavior significantly heightened participant stress levels and was considered the least preferred behavior. The happy behavior was the most preferred and associated with increased excitement by the participants. Integrating the proposed behaviors in robots can profoundly influence the human operator’s perceived attention, emotional state, and overall experience. These insights are valuable for future HRI tandem robot designs.},
  archive   = {C_IROS},
  author    = {Julian Kaduk and Friederike Weilbeer and Heiko Hamann},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801974},
  month     = {10},
  pages     = {2465-2470},
  title     = {Emotional tandem robots: How different robot behaviors affect human perception while controlling a mobile robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Understanding robot minds: Leveraging machine teaching for
transparent human-robot collaboration across diverse groups.
<em>IROS</em>, 2457–2464. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we aim to improve transparency and efficacy in human-robot collaboration by developing machine teaching algorithms suitable for groups with varied learning capabilities. While previous approaches focused on tailored approaches for teaching individuals, our method teaches teams with various compositions of diverse learners using team belief representations. We investigate various group teaching strategies, such as focusing on individual beliefs or the group’s collective beliefs, and assess their impact on learning robot policies for different team compositions. Our findings reveal that team belief strategies produce less variation in learning duration and better accommodate diverse teams compared to individual belief strategies, suggesting their suitability in mixed proficiency settings with limited resources. In contrast, individual belief strategies provide a more uniform knowledge level, particularly effective for homogeneously inexperienced groups. Our study indicates that the effectiveness of the teaching strategy is significantly influenced by team composition and learner proficiency, highlighting the importance of real-time assessment of learner proficiency and adapting teaching approaches based on learner proficiency for optimal teaching outcomes.},
  archive   = {C_IROS},
  author    = {Suresh Kumaar Jayaraman and Reid Simmons and Aaron Steinfeld and Henny Admoni},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801370},
  month     = {10},
  pages     = {2457-2464},
  title     = {Understanding robot minds: Leveraging machine teaching for transparent human-robot collaboration across diverse groups},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Autonomous storytelling for social robot with human-centered
reinforcement learning. <em>IROS</em>, 2450–2456. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Social robots are gradually integrating into human’s daily lives. Storytelling by social robots could bring a different experience to users through non-verbal and emotional capabilities compared to text-only one. However, as user needs and preferences over storytelling might change over time during long-term interaction with social robots, it is important for social robots to learn from social interactions with human users in real-time. In this paper, we propose to allow our social robot Haru to learn personalized storytelling styles for different human user’s emotional states via human-centered reinforcement learning using the reward provided and delivered by directly interaction with the user explicitly. Results of our user study show that Haru can learn to adapt its storytelling style for detected human emotional states in a few number of interactions, and was perceived to have a better storytelling performance, experience and impact than a neutral one.},
  archive   = {C_IROS},
  author    = {Lei Zhang and Chuanxiong Zheng and Hui Wang and Randy Gomez and Eric Nichols and Guangliang Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802035},
  month     = {10},
  pages     = {2450-2456},
  title     = {Autonomous storytelling for social robot with human-centered reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Seven benefits of using series elastic actuators in the
design of an affordable, simple controlled, and functional prosthetic
hand. <em>IROS</em>, 2444–2449. (<a
href="https://doi.org/10.1109/IROS58592.2024.10803048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper highlights the benefits of using series elastic actuators (SEA) in designing a cost-efficient, easily controlled, and functional prosthetic hand. The designed 3D-printed hand uses only two motors in an antagonistic configuration, transferring power to the fingers via pulleys, cables, and springs; i.e., the motors are in an SEA configuration with the load/fingers. In the designed underactuated prosthetic hand, the thumb is adjustable for various tasks, and the optimization of pulley diameters ensures synchronized finger movement during hand flexion and extension. Thanks to the SEA configuration of the motors and fingers, simple position control of the motor enables features like hand position control, morphological grasp, force control, impedance control, slippage detection, safe interaction, and efficient grasp. An extensive set of experiments has been conducted to evaluate the designed prosthetic hand’s performance. The experiments confirm the hand’s satisfactory performance while also highlighting the importance of improving the proposed design in different aspects. To attain better position control and morphological grasp, minimizing the cable-body and joint friction is recommended. A higher resolution of the current/torque sensor is needed for the precise force control and slippage detection. Finally, a motor brake system is required to achieve efficient grasping.},
  archive   = {C_IROS},
  author    = {Erfan Koochakzadeh and Alireza Kargar and Parsa Sattari and Diba Ravanshid and Rezvan Nasiri},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10803048},
  month     = {10},
  pages     = {2444-2449},
  title     = {Seven benefits of using series elastic actuators in the design of an affordable, simple controlled, and functional prosthetic hand},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CathFlow: Self-supervised segmentation of catheters in
interventional ultrasound using optical flow and transformers.
<em>IROS</em>, 2436–2443. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In minimally invasive endovascular procedures, contrast-enhanced angiography remains the most robust imaging technique, but exposes patients and surgeons to prolonged radiation. Alternatives such as ultrasound are difficult to interpret, are highly prone to artifacts and noise, and vary in quality, depending on the experience of the interventional radiologist and machine settings. In this work, we seek to address both problems by introducing a self-supervised deep learning architecture to segment catheters in longitudinal ultrasound images, without demanding any labeled data. The network architecture builds upon AiAReSeg, a segmentation transformer built with the Attention in Attention mechanism, and is capable of learning feature changes across time and space. To facilitate training, we used synthetic ultrasound data based on physics-driven catheter insertion simulations, and translated the data into a unique CT-Ultrasound common domain, CACTUSS, to improve the segmentation performance. We generated ground truth segmentation masks by computing the optical flow between adjacent frames using FlowNet2, and performed thresholding to obtain a binary mask estimate. Finally, we validated our model on a test dataset, consisting of unseen synthetic data and images collected from silicon aorta phantoms, thus demonstrating its potential for applications to clinical data in the future.},
  archive   = {C_IROS},
  author    = {Alex Ranne and Liming Kuang and Yordanka Velikova and Nassir Navab and Ferdinando Rodriguez Y Baena},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802113},
  month     = {10},
  pages     = {2436-2443},
  title     = {CathFlow: Self-supervised segmentation of catheters in interventional ultrasound using optical flow and transformers},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Head-mounted hydraulic needle driver for targeted
interventions in neurosurgery. <em>IROS</em>, 2429–2435. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Needle interventions are crucial in neurosurgery, requiring high precision and stability. This paper presents a 5-DoF head-mounted hydraulic needle robot designed for accurate and targeted needle insertion and neuroimaging in the deep brain. The robot is compact and lightweight by utilizing a hydraulic pipe transmission to connect the needle driver and actuator. The syringe pistons serve as the actuator and executor, enabling synchronized motion, minimal hysteresis, and high-accuracy insertion. The hydraulic transmission system exhibits hysteresis of less than 0.8 mm, with bidirectional insertion accuracy of approximately 0.05 mm. The resulting needle driver features a compact structure measuring 48 mm × 25 mm × 9 mm, accompanied by a 70-mm-long needle guide. The needle driver is mainly 3D printed, while the hydraulic transmission ensures full compatibility with magnetic resonance imaging (MRI) by isolating all electromagnetic parts from the executor. This compact and lightweight robot-assisted needle intervention system significantly enhances the safety, accuracy, and effectiveness of deep-brain neuroimaging. The feasibility of precise positioning and insertion is further demonstrated by deploying an optical coherence tomography (OCT) microneedle in a rat brain.},
  archive   = {C_IROS},
  author    = {Zhiwei Fang and Chao Xu and Huxin Gao and Danny Tat-Ming Chan and Wu Yuan and Hongliang Ren},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801339},
  month     = {10},
  pages     = {2429-2435},
  title     = {Head-mounted hydraulic needle driver for targeted interventions in neurosurgery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing surgical precision in autonomous robotic incisions
via physics-based tissue cutting simulation. <em>IROS</em>, 2421–2428.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In soft tissue surgeries, such as tumor resections, achieving precision is of utmost importance. Surgeons conventionally achieve this precision through intraoperative adjustments to the cutting plan, responding to deformations from tool-tissue interactions. This study examines the integration of physics-based tissue cutting simulations into autonomous robotic surgery to preoperatively predict and compensate for such deformations, aiming to improve surgical precision and reduce the necessity for dynamic adjustments during autonomous surgeries. This study adapts a real-to-sim-to-real workflow. Initially, the Autonomous System for Tumor Resection (ASTR) was employed to evaluate its accuracy in performing preoperatively intended incisions along the irregular contours of porcine tongue pseudotumors. Following this, a finite element analysis-based simulation, utilizing the Simulation Open Framework Architecture (SOFA), was developed and tuned to accurately mimic these tissue and incision interactions. Insights gained from this simulation were applied to refine the robot’s path planning, ensuring a closer alignment of actual incisions with the initially intended surgical plan. The efficacy of this approach was validated by comparing surface incision precision on ex vivo porcine tongues, with the average absolute error reducing from 1.73mm to 1.46mm after applying simulation-driven path adjustments (p &lt; 0.001). Additionally, our method not only demonstrated improvements in maintaining the intended cutting shapes and locations, with shape matching scores using Hu moments enhancing from 0.10 to 0.06 and centroid shifts decreasing from 2.09mm to 1.33mm, but it also potentially reduced the likelihood of adverse oncologic outcomes by preventing clinically suggested excessively close margins of 2.2mm. This feasibility study suggests that merging physics-based cutting simulations with autonomous robotic surgery could potentially lead to more accurate incisions.},
  archive   = {C_IROS},
  author    = {Jiawei Ge and Ethan Kilmer and Leila J. Mady and Justin D. Opfermann and Axel Krieger},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802347},
  month     = {10},
  pages     = {2421-2428},
  title     = {Enhancing surgical precision in autonomous robotic incisions via physics-based tissue cutting simulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The design of a sensorized laryngoscope training system for
pediatric intubation. <em>IROS</em>, 2414–2420. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Intubation is essential for ventilating critically ill patients and involves precise maneuvering of a laryngoscope to place an endotracheal tube (ETT). However, training for this procedure is fraught with challenges. Traditional methods, relying on manikins or training with a single sensing modality, fail to adequately convey important interaction information. Such challenge is heightened in pediatric intubation due to anatomical differences that demand greater precision. Furthermore, integrating multiple sensing modalities into a laryngoscope without changing its size presents a significant design challenge, critical for maintaining realistic training scenarios. To overcome these obstacles, we developed a sensorized laryngoscope system equipped with a force-torque sensor, a 9-axis inertial measurement unit (IMU), and tactile sensors. This system, validated in a preliminary user study, provides online feedback on angles, forces, and grip strength through an online feedback GUI. Adopting a learning-by-demonstration approach with both experts and novices, the initial validation confirmed the system’s potential, paving the way for expanded trials with more participants.},
  archive   = {C_IROS},
  author    = {Ningzhe Hou and Liang He and Alessandro Albini and Louis Halamek and Perla Maiolino},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801567},
  month     = {10},
  pages     = {2414-2420},
  title     = {The design of a sensorized laryngoscope training system for pediatric intubation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Portable robot for needle insertion assistance to femoral
artery. <em>IROS</em>, 2407–2413. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Femoral artery access is a common and critical procedure for various cardiovascular interventions. Although it is a time critical operation, accessing the Common Femoral Artery (CFA) typically requires expertise found in specialized medical settings. The necessity for specialized personnel or transport to equipped facilities can lead to delays, potentially exacerbating patient outcomes. To address this challenge, a portable and cost-effective robotic device that autonomously localizes a CFA and precisely positions a needle guide is developed. Through the needle guide, needle can be quickly and accurately inserted into the artery even by non-specialist physicians. Different from the conventional B-mode ultrasound guided procedure, the proposed robotic solution utilizes a Doppler transducer for detecting the arterial location and employs a single M-mode transducer for depth measurement. A series of experiments are designed and conducted to validate the system’s feasibility, achieving high accuracy within 2 mm, rapid processing within 1.5 min, and a 100% success rate, thus proving the system’s efficacy. These results convince us for further refinement of the system and support its evaluation in animal studies.},
  archive   = {C_IROS},
  author    = {Zhuoqi Cheng and Bence Mány and Kasper Balsby Jørgensen and Siheon An and Marcus Leander Jensen and Richard Thulstrup and Habib Frost and Thiusius R. Savarimuthu and Olof Huldt},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802084},
  month     = {10},
  pages     = {2407-2413},
  title     = {Portable robot for needle insertion assistance to femoral artery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel approach for precise tissue tracking in breast
lumpectomy. <em>IROS</em>, 2400–2406. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One of the most common cancers among women is breast cancer which can be treated surgically in the early stages with a lumpectomy technique. In the context of breast lumpectomy procedures, accurately tracking tumours presents a critical challenge worsened by various sources of anatomical deformations, including breathing, tissue cutting, and ultrasound probe pressure. To address this, we explore how a realistic tissue deformation simulator can enhance the precision of locating internal targets by accurately assessing the deformation applied to a preoperative model of the breast, considering the distinct mechanical properties of both the breast tissue and the tumour within it. Our method uses advanced artificial intelligence techniques by combining a generative variation autoencoder (GNN-VAE) and an updating method called ensemble smoother with multiple data assimilation (ES-MDA), creating a dynamic model based exclusively on surface node data to update all nodes within the tissue. By leveraging a realistic tissue deformation simulator, our approach uses breast surface tracking to infer full tissue deformations. This makes the method compatible with various simulation tools and suitable for tissues with complex properties. The results indicate that the trained network has an accuracy of 0.014 cm with training data, and 0.026 cm with the testing portion of data, demonstrating precision in tumour localization and significantly improving upon current methods. This innovation can potentially enhance patient outcomes by making breast cancer surgery safer, less invasive, and more efficient.},
  archive   = {C_IROS},
  author    = {Yeganeh Aliyari and Mehrnoosh Afshar and Ericka Wiebe and Lashan Peiris and Mahdi Tavakoli},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802477},
  month     = {10},
  pages     = {2400-2406},
  title     = {A novel approach for precise tissue tracking in breast lumpectomy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Development of five-finger hand-type robotic forceps for
laparoscopic gastrointestinal surgery. <em>IROS</em>, 2394–2399. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In laparoscopic surgery, large organs are difficult to handle with one forceps, resulting in low surgical efficiency. In this study, to improve surgical efficiency, we developed a five-fingered robotic forceps that can be inserted through small incisions and has multiple degrees of freedom to handle large organs. The results of an experiment simulating sigmoidectomy showed that the proposed robotic forceps could shorten the surgical time. In addition, we confirmed that the physical burden while using the instrument was comparable to that of conventional forceps.},
  archive   = {C_IROS},
  author    = {Hiroyuki Wakamatsu and Ibuki Kobayashi and Yuya Nagase and Ryu Kato and Masaya Mukai},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801672},
  month     = {10},
  pages     = {2394-2399},
  title     = {Development of five-finger hand-type robotic forceps for laparoscopic gastrointestinal surgery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Wirelessly actuated rotation-free magnetic motor.
<em>IROS</em>, 2387–2393. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the challenge of actuating millimetre-sized motors, which are wirelessly driven by external magnetic fields. Traditional approaches, relying on rotating magnetic fields, often inadvertently cause the entire robot – especially if it is small and lightweight – to rotate, instead of a specified shaft in the motor. To overcome this issue, our study introduces a novel mechanism that leverages symmetrically configured magnetic motors to cancel out the torques, thus preventing unwanted rotation of the robot. This is achieved by utilizing a magnetic field along a single axis to induce rotational movement. The design features two millimetre-sized rotating magnets that interact to achieve a 90◦ rotation, complemented by an external magnetic field that accomplishes the remaining 270◦, thus completing a full rotation. Furthermore, we demonstrate that applying a perpendicularly oriented magnetic field can inversely affect the motor’s rotation direction. A proof-of-concept experiment employing this mechanism successfully actuated a gripper in a water tank while it is free-floating, showcasing its potential for enhancing robotic applications at the sub-centimeter scale, where the small net torque of a miniature motor is essential.},
  archive   = {C_IROS},
  author    = {Umur Ulas Harman and Ahmed Hafez and Cameron Duffield and Zihan Zhao and Luke Dixon and Daniela Rus and Shuhei Miyashita},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802494},
  month     = {10},
  pages     = {2387-2393},
  title     = {Wirelessly actuated rotation-free magnetic motor},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards robotised palpation for cancer detection through
online tissue viscoelastic characterisation with a collaborative robotic
arm. <em>IROS</em>, 2380–2386. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a new method for online estimating the penetration of the end-effector and the viscoelastic properties of a soft body, through palpation exams using a collaborative robotic arm. The estimator is based on the dimensionality reduction method that simplifies the nonlinear Hunt-Crossley model. In addition, in our algorithm, the model parameters can be found without a force sensor, leveraging only the robotic arm controller data. An extended Kalman filter is employed to achieve online estimation, which embeds the dynamic contact model. The algorithm is tested with various types of silicone, a material that resembles biological tissues, including samples with hard intrusions to simulate cancerous cells within a softer tissue. The results indicate that this technique can accurately determine the model parameters and estimate the penetration of the end-effector into the soft body. These promising preliminary results demonstrate robots’ potential to be an effective tool for early-stage cancer diagnostics.},
  archive   = {C_IROS},
  author    = {Luca Beber and Edoardo Lamon and Giacomo Moretti and Daniele Fontanelli and Matteo Saveriano and Luigi Palopoli},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802582},
  month     = {10},
  pages     = {2380-2386},
  title     = {Towards robotised palpation for cancer detection through online tissue viscoelastic characterisation with a collaborative robotic arm},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Thermal ablation therapy control with tissue necrosis-driven
temperature feedback enabled by neural state space model with extended
kalman filter. <em>IROS</em>, 2373–2379. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Thermal ablation therapy is a major minimally invasive treatment. One of the challenges is that the targeted region and therapeutic progression are often invisible to clinicians, requiring feedback provided in numerical information or imaging. Several emerging imaging modalities offer visualization of the ablation-induced necrosis formation; however, relying solely on necrosis monitoring can result in tissue overheating and endangering patients. Some of the necrosis monitoring modalities are known for their capabilities in temperature sensing, but the principles on which they are based have several limitations, such as sensitivity to the tissue motion and their environment. In this study, we propose a necrosis progression-based temperature estimation technique as an added safety feature for avoiding overheating. This model-based method does not require additional sensing hardware. It is designed to work as an independent estimator or a complimentary estimation component with other thermometers for improved robustness. For this objective, the Neural State Space model is used to approximate the ablation therapy, whose theoretical models involve nonlinear partial differential equations. Then, the Extended Kalman Filter is designed based on the model. The simulation study shows the estimation module robustly estimates the tissue temperature under several types of noise. The maximum estimation error observed before terminating ablation was around 1 °C, and the desired safety feature was successfully demonstrated. The estimator is expected to be used in a variety of necrosis monitoring modalities to guarantee more precise and safer treatment. More ambitiously, the architecture with the Neural State Space model and Extended Kalman Filter is generalizable to other medical/biological procedures involving nonlinear and patient/environment-specific physics and even to procedures having no reliable theoretical models.},
  archive   = {C_IROS},
  author    = {Ryo Murakami and Satoshi Mori and Haichong K. Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801769},
  month     = {10},
  pages     = {2373-2379},
  title     = {Thermal ablation therapy control with tissue necrosis-driven temperature feedback enabled by neural state space model with extended kalman filter},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Development of a low pressure pouch sensor for force
measurement in colonoscopy procedures. <em>IROS</em>, 2366–2372. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel pneumatic pouch sensor, designed to mount on a colonoscope, that can effectively estimate the contact forces with the environment. The pouch sensor was designed to maximize the sensing range, and it was fabricated using a 2D laser welding technique from our track record. A flow compensation (FC) algorithm was introduced to improve the accuracy of the sensor in the presence of static load. The proposed system can reliably measure external forces up to 9.5 N with high repeatability. The system allows discriminating between different levels of force which are typically associated with increasing patient discomfort in colonoscopy: low (0-4 N), medium (4-6 N), and high (&gt;6 N). This system achieves over 80% accuracy in comparison to the ground truth under steady state conditions (P ≤ 0.05) and maintains over 68% accuracy in dynamic scenarios.},
  archive   = {C_IROS},
  author    = {Korn Borvorntanajanya and Jabed F Ahmed and Mark Runciman and Enrico Franco and Nisha Patel and Ferdinando Rodriguez y Baena},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802184},
  month     = {10},
  pages     = {2366-2372},
  title     = {Development of a low pressure pouch sensor for force measurement in colonoscopy procedures},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robot-enabled machine learning-based diagnosis of gastric
cancer polyps using partial surface tactile imaging. <em>IROS</em>,
2360–2365. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, to collectively address the existing limitations on endoscopic diagnosis of Advanced Gastric Cancer (AGC) Tumors, for the first time, we propose (i) utilization and evaluation of our recently developed Vision-based Tactile Sensor (VTS), and (ii) a complementary Machine Learning (ML) algorithm for classifying tumors using their textural features. Leveraging a seven DoF robotic manipulator and unique custom-designed and additively-manufactured realistic AGC tumor phantoms, we demonstrated the advantages of automated data collection using the VTS addressing the problem of data scarcity and biases encountered in traditional ML-based approaches. Our synthetic-data-trained ML model was successfully evaluated and compared with traditional ML models utilizing various statistical metrics even under mixed morphological characteristics and partial sensor contact.},
  archive   = {C_IROS},
  author    = {Siddhartha Kapuria and Jeff Bonyun and Yash Kulkarni and Naruhiko Ikoma and Sandeep Chinchali and Farshid Alambeigi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802585},
  month     = {10},
  pages     = {2360-2365},
  title     = {Robot-enabled machine learning-based diagnosis of gastric cancer polyps using partial surface tactile imaging},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3D ultrasound image acquisition and diagnostic analysis of
the common carotid artery with a portable robotic device. <em>IROS</em>,
2353–2359. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ultrasound (US) imaging of the carotid artery (CA) is a non-invasive diagnostic tool widely used in the medical field to assess the condition of the carotid artery, thereby predicting the risk of cardiovascular and cerebrovascular diseases. However, implementing this method in primary healthcare can be challenging due to the requirement for professionally trained sonographers. With the adoption of US robotic devices, the probe pose can be acquired while scanning, offering the possibility for 3D reconstruction and providing analyses that are not dependent on operator experience. This article introduces a method to semi-automatically acquire serialized US images of the common carotid artery (CCA). The method involves a specially designed robotic device built with a 6-RSU parallel mechanism, which is controlled according to robot pose, force sensor data and synchronous US images. To validate the images acquired, a method is proposed to segment the intima-media of CCA and calculate the intima-media thickness (IMT), which is a key indicator for cerebrovascular events prediction. After that, we propose an algorithm to reconstruct CCA into 3D voxel data with patient movement and cardiac cycle compensated, and a longitudinal view US image of CCA can be resliced from the voxel. The methods are tested on human subjects and the results indicate that the system and workflow can provide both quantitative and qualitative information of CCA for further diagnosis.},
  archive   = {C_IROS},
  author    = {Longyue Tan and Zhaokun Deng and Mingrui Hao and Pengcheng Zhang and Xilong Hou and Chen Chen and Xiaolin Gu and Xiao-Hu Zhou and Zeng-Guang Hou and Shuangyi Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802713},
  month     = {10},
  pages     = {2353-2359},
  title     = {3D ultrasound image acquisition and diagnostic analysis of the common carotid artery with a portable robotic device},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dung beetle optimizer-based high-precision localization for
magnetic-controlled capsule robot. <em>IROS</em>, 2347–2352. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As a medical microrobot, magnetic-controlled capsule robots (MCRs) are pivotal in internal diagnostics and therapeutic interventions. Achieving high-precision localization of MCRs is essential for the successful execution of medical procedures. This paper introduces a novel Dung Beetle Optimizer (DBO)-based localization method for MCR, demonstrating high localization accuracy and flexibility in static magnetic field environments and under the control of existing magnetic control systems. With the aid of an FPGA-based parallel measurement system, it can effectively eliminate measurement distortion. The average position and orientation errors could achieve 0.53 mm and 0.60° when performing 600 iterations per computation, and further increasing the number of iterations reduces the errors, which is superior to existing methods. Experimental validations underscore the method’s robust performance and compatibility with existing magnetic control systems.},
  archive   = {C_IROS},
  author    = {Zijin Zeng and Fengwu Wang and Chan Li and Menglu Tan and Shengyuan Wang and Lin Feng},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802352},
  month     = {10},
  pages     = {2347-2352},
  title     = {Dung beetle optimizer-based high-precision localization for magnetic-controlled capsule robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A feasibility study of a soft, low-cost, 6-axis load cell
for haptics. <em>IROS</em>, 2339–2346. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Haptic devices have shown to be valuable in supplementing surgical training, especially when providing haptic feedback based on user performance metrics such as wrench applied by the user on the tool. However, current 6-axis force/torque sensors are prohibitively expensive. This paper presents the design and calibration of a low-cost, six-axis force/torque sensor specially designed for laparoscopic haptic training applications. The proposed design uses Hall-effect sensors to measure the change in the position of magnets embedded in a silicone layer that results from an applied wrench to the device. Preliminary experimental validation demonstrates that these sensors can achieve an accuracy of 0.45 N and 0.014 Nm, and a theoretical XY range of ±50N, Z range of ±20N, and torque range of ±0.2Nm. This study indicates that the proposed low-cost 6-axis force/torque sensor can accurately measure user force and provide useful feedback during laparoscopic training on a haptic device.},
  archive   = {C_IROS},
  author    = {Madison Veliky and Garrison L.H. Johnston and Ahmet Yildiz and Nabil Simaan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802018},
  month     = {10},
  pages     = {2339-2346},
  title     = {A feasibility study of a soft, low-cost, 6-axis load cell for haptics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scheduling of robotic cellular manufacturing systems with
timed petri nets and reinforcement learning. <em>IROS</em>, 2333–2338.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a new Petri-net-based Q-learning scheduling method to schedule robotic cellular manufacturing (RCM) systems efficiently. First, we use generalized and place-timed Petri nets to model RCM systems. Then, we design a reinforcement learning method with a sparse Q-table to evaluate state-transition pairs of the net’s reachability graph. It uses the negative transition firing time as a reward for an action selection and adopts a large penalty for any encountered deadlock. In addition, it balances the state space exploration and the experience exploitation by using a dynamic ϵ-greedy policy to update the state values with an accumulative reward. Three different dynamic ϵ-greedy policies are designed for different application scenarios. Some benchmark RCM systems are tested with the proposed method and several popular PN-based online dispatching rules, such as FIFO and SRPT. Simulation results demonstrate that our method schedules RCM systems as quickly as the online dispatching rules while outperforming them in terms of schedule makespan. For readers’ reference, our source code and test data are available at https://github.com/PNOptimizer/PNQL.},
  archive   = {C_IROS},
  author    = {ZhuTao Yao and Bo Huang and JianYong Lv and XiaoYu Lu and MeiJi Cui and ShaoHua Yu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802059},
  month     = {10},
  pages     = {2333-2338},
  title     = {Scheduling of robotic cellular manufacturing systems with timed petri nets and reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lang2LTL-2: Grounding spatiotemporal navigation commands
using large language and vision-language models. <em>IROS</em>,
2325–2332. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Grounding spatiotemporal navigation commands to structured task specifications enables autonomous robots to understand a broad range of natural language and solve long-horizon tasks with safety guarantees. Prior works mostly focus on grounding spatial or temporally extended language for robots. We propose Lang2LTL-2, a modular system that leverages pretrained large language and vision-language models and multimodal semantic information to ground spatiotemporal navigation commands in novel city-scaled environments without retraining. Lang2LTL-2 achieves 93.53% language grounding accuracy on a dataset of 21,780 semantically diverse natural language commands in unseen environments. We run an ablation study to validate the need for different modalities. We also show that a physical robot equipped with the same system without modification can execute 50 semantically diverse natural language commands in both indoor and outdoor environments.},
  archive   = {C_IROS},
  author    = {Jason Xinyu Liu and Ankit Shah and George Konidaris and Stefanie Tellex and David Paulius},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802696},
  month     = {10},
  pages     = {2325-2332},
  title     = {Lang2LTL-2: Grounding spatiotemporal navigation commands using large language and vision-language models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robotic measurement for electrical property of polymers by
force-sensing robot toward materials lab-automation. <em>IROS</em>,
2319–2324. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the background of research on materials laboratory automation, this study aims to construct an automation system for measuring dielectric property, which is an electrical property of materials. The automation system is composed of the combination of a manipulation by a force-sensing robot and a control system for the measurement instrument. As challenges for the automation system, we worked on stabilizing a polymer film placement during insertion into the measurement instrument, implementing a communication control system between different platforms, and constructing a polymer film transfer environment. In the measurement experiment using the automation system, it was confirmed that the dielectric properties could be measured as well as that of a human.},
  archive   = {C_IROS},
  author    = {Yuki Asano and Kei Okada and Junichiro Shiomi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802186},
  month     = {10},
  pages     = {2319-2324},
  title     = {Robotic measurement for electrical property of polymers by force-sensing robot toward materials lab-automation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stick roller: Precise in-hand stick rolling with a
sample-efficient tactile model. <em>IROS</em>, 2312–2318. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In-hand manipulation is challenging in robotics due to the intricate contact dynamics and high degrees of control freedom. Precise manipulation with high accuracy often requires tactile perception, which adds further complexity to the system. Despite the challenges in perception and control, the rolling stick problem is an essential and practical motion primitive with many demanding industrial applications. This work aims to learn the high-resolution tactile dynamics of the rolling stick. Specifically, we try manipulating a small stick using the Allegro hand equipped with the Digit vision-based tactile sensor. The learning framework includes an action filtering module, tactile perception module, and learning with uncertainty module, all designed to operate in low data regimes. With only 2.3% amount of data and 5.7% model complexity of previous similar work, our learned contact dynamics model achieves better grasp stability, sub-millimeter precision, and promising zero-shot generalizability across novel objects. The proposed framework demonstrates the potential for precise in-hand manipulation with tactile feedback on real hardware. The project source code is available at: https://github.com/duyipai/Allegro_Digit. A video presentation is available here.},
  archive   = {C_IROS},
  author    = {Yipai Du and Pokuang Zhou and Michael Yu Wang and Wenzhao Lian and Yu She},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802003},
  month     = {10},
  pages     = {2312-2318},
  title     = {Stick roller: Precise in-hand stick rolling with a sample-efficient tactile model},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficiently obtaining reachset conformance for the formal
analysis of robotic contact tasks. <em>IROS</em>, 2305–2311. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Formal verification of robotic tasks requires a simple yet conformant model of the used robot. We present the first work on generating reachset conformant models for robotic contact tasks considering hybrid (mixed continuous and discrete) dynamics. Reachset conformance requires that the set of reachable outputs of the abstract model encloses all previous measurements to transfer safety properties. Aiming for industrial applications, we describe the system using a simple hybrid automaton with linear dynamics. We inject non-determinism into the continuous dynamics and the discrete transitions, and we optimally identify all model parameters together with the non-determinism required to capture the recorded behaviors. Using two 3-DOF robots, we show that our approach can effectively generate models to capture uncertainties in system behavior and substantially reduce the required testing effort in industrial applications.},
  archive   = {C_IROS},
  author    = {Chencheng Tang and Matthias Althoff},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802263},
  month     = {10},
  pages     = {2305-2311},
  title     = {Efficiently obtaining reachset conformance for the formal analysis of robotic contact tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Soft task planning with hierarchical temporal logic
specifications. <em>IROS</em>, 2299–2304. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This works exploits soft constraints in linear temporal logic task planning to enhance the agent’s capability in handling potentially conflicting or even infeasible tasks. Different from most existing works that focus on sticking to the original plan and trying to find a relaxed plan if the workspace does not permit, we augment the soft constraints to represent possible candidate sub-tasks that can be selected to fulfill the global task. Specifically, a hierarchical temporal logic specification is developed to represent LTL tasks with soft constraints and preferences. The hierarchical structure consists of an outer and inner layer, where the outer layer uses co-safe LTL to specify the task-level specifications and the inner layer specifies the low-level task-related atomic propositions via soft constraints. To cope with the hierarchical temporal logic specification, a hierarchical iterative search (HIS) algorithm is developed, which incrementally searches feasible atomic propositions and automaton states, and returns a task plan with minimum cost. Rigorous analysis shows that HIS based planning is feasible (i.e., the generated plan is applicable and satisfactory with respect to the task specification) and optimal (i.e, with minimum cost). Extensive simulation demonstrates the effectiveness of the proposed soft task planning approach.},
  archive   = {C_IROS},
  author    = {Ziyang Chen and Zhangli Zhou and Lin Li and Zhen Kan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801732},
  month     = {10},
  pages     = {2299-2304},
  title     = {Soft task planning with hierarchical temporal logic specifications},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). One problem, one solution: Unifying robot design and cell
layout optimization. <em>IROS</em>, 2292–2298. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The task-specific optimization of robotic systems has since the inception of the field been divided into the optimization of the robot and the optimization of the layout of its workstations. In this letter, we argue that these two problems are interdependent and should be treated as such. To this end, we present a unified problem formulation that enables for the simultaneous optimization of both the robot kinematics and the workstation layout. We demonstrate the effectiveness of our approach by jointly optimizing a robotic milling system. To compare our approach to the state of the art, we optimize the robot’s kinematics and layout separately. The results show that our approach outperforms the state of the art and that simultaneous optimization leads to up to eight times better solutions.},
  archive   = {C_IROS},
  author    = {Jan Baumgärtner and Alexander Puchta and Jürgen Fleischer},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801417},
  month     = {10},
  pages     = {2292-2298},
  title     = {One problem, one solution: Unifying robot design and cell layout optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data-driven modeling of cable slab dynamics via neural
networks. <em>IROS</em>, 2286–2291. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A novel method for analyzing the dynamics and bend geometry of a cable slab via trained neural networks is introduced. Neural networks are trained from real-time visual feedback capture via a high-speed camera during cyclic motion to track the positions of multiple markers affixed to the cable slab through image processing techniques. Experimental parameters are systematically varied to ensure a diverse range of training patterns. Consequently, two distinct data-driven neural network models are developed: a coupled model and a decoupled model. These models accurately predict the two-dimensional positions of the markers, even during non-cyclic motion profiles. Subsequently, the marker positions are utilized as waypoints to generate a cubic spline curve with time-varying coefficients, approximating the spatiotemporal solution of the cable slab dynamics. Notably, this spline can be segmented into smaller sections tailored to specific research objectives. Experimental results validate the effectiveness of the proposed methodology.},
  archive   = {C_IROS},
  author    = {Yazan M. Al-Rawashdeh and Mohammad Al Saaideh and Michael Pumphrey and Natheer Alatawneh and Mohammad Al Janaideh},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801972},
  month     = {10},
  pages     = {2286-2291},
  title     = {Data-driven modeling of cable slab dynamics via neural networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IDF-MFL: Infrastructure-free and drift-free magnetic field
localization for mobile robot. <em>IROS</em>, 2278–2285. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, infrastructure-based localization methods have achieved significant progress thanks to their reliable and drift-free localization capability. However, the preinstalled infrastructures suffer from inflexibilities and high maintenance costs. This poses an interesting problem of how to develop a drift-free localization system without using the preinstalled infrastructures. In this paper, an infrastructure-free and drift-free localization system is proposed using the ambient magnetic field (MF) information, namely IDF-MFL. IDF-MFL is infrastructure-free thanks to the high distinctiveness of the ambient MF information produced by inherent ferromagnetic objects in the environment, such as steel and reinforced concrete structures of buildings, and underground pipelines. The MF-based localization problem is defined as a stochastic optimization problem with the consideration of the non-Gaussian heavy-tailed noise introduced by MF measurement outliers (caused by dynamic ferromagnetic objects), and an outlier-robust state estimation algorithm is derived to find the optimal distribution of robot state that makes the expectation of MF matching cost achieves its lower bound. The proposed method is evaluated in multiple scenarios 1, including experiments on high-fidelity simulation, and real-world environments. The results demonstrate that the proposed method can achieve high-accuracy, reliable, and real-time localization without any pre-installed infrastructures.},
  archive   = {C_IROS},
  author    = {Hongming Shen and Zhenyu Wu and Wei Wang and Qiyang Lyu and Huiqin Zhou and Danwei Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802339},
  month     = {10},
  pages     = {2278-2285},
  title     = {IDF-MFL: Infrastructure-free and drift-free magnetic field localization for mobile robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modernising delivery: A low-energy tethered package system
using fixed-wing drones. <em>IROS</em>, 2271–2277. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fixed-wing Uncrewed Aerial Vehicles (UAVs) can be used for remote package delivery missions by connecting a package to a UAV via a long tether. With a circular flight path at a calculated loiter radius, tether length and orbiting velocity, a package can be lowered to the ground in a quasi-stationary manner. This method achieves this with significantly less energy than hybrid hovering aircraft, which are currently used. UAV operating limitations pose a challenge for this method as ensuring the package stabilises such that it can be safely deployed without damage or posing a risk to people or property is challenging in most environmental conditions. To improve tether and package stabilisation, we introduce a novel Mid-Tether Drag Device (MTDD) which enables compliant delivery missions within regulatory frameworks. We present a mathematical model of the delivery with a UAV and MTDD. We verify the accuracy of our model with real-world flight tests in low wind conditions without the MTDD, which have not been previously conducted at this scale in literature. Further validation is presented with flight tests at a flight range using a UAV instrumented with a package deployment system with both UAV and package tracking data acquisition. Our work enhances the abilities of UAVs to conduct aerial package delivery.},
  archive   = {C_IROS},
  author    = {Samuel Ord and Matthew Marino and Timothy Wiley},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802785},
  month     = {10},
  pages     = {2271-2277},
  title     = {Modernising delivery: A low-energy tethered package system using fixed-wing drones},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Image to patterning: Density-specified patterning of
micro-structured surfaces with a mobile robot. <em>IROS</em>, 2264–2270.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Micro-structured surfaces possess useful properties such as friction modification, anti-fouling, and hydrophobicity. However, manufacturing these surfaces in an affordable, scalable, and efficient manner remains challenging. Standard coverage methods for surface patterning require precise placement of micro-scale features over meter-scale surfaces with expensive tooling for support. In this work, we address the scalability challenge in surface patterning by designing a mobile robot with a credit-card-sized footprint to generate micro-scale divots using a modulated tool tip. We provide a control architecture with a target feature density to specify surface coverage, eliminating the dependence on individual indentation locations. Our robot produces high-fidelity surface patterns and achieves automatic coverage of a surface from sophisticated target images. We validate an exemplary application of such micro-structured surfaces by controlling the friction coefficients at different locations according to the density of indentations. These results show the potential for compact robots to perform scalable manufacturing of functional surfaces, switching the focus from precision machines to small-footprint devices tasked with matching only the density of features.},
  archive   = {C_IROS},
  author    = {Annalisa T. Taylor and Malachi Landis and Yaoke Wang and Todd D. Murphey and Ping Guo},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802317},
  month     = {10},
  pages     = {2264-2270},
  title     = {Image to patterning: Density-specified patterning of micro-structured surfaces with a mobile robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluation of the design of a tool for the automated
assembly of preconfigured wires*. <em>IROS</em>, 2258–2263. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The assembly of control cabinets is highly affected by manual processes. For reasons such as the shortage of skilled workers, there is a considerable need to automate the production steps. The automated wiring of prefabricated wires, which is a mayor process step in the manufacturing, is not automated yet. Besides different sensor technologies, a reliable tool for assembly must be developed. This article discusses the challenges and criteria for the development of such a tool. The most important functions of the tool include the handling of wires with different lengths, cross-sections, and colors as well as the consideration of close mounting positions. Based on a morphological box, a tool concept is derived and validated via tests on a demonstrator.},
  archive   = {C_IROS},
  author    = {Stefanie Bartelt and Bernd Kuhlenkötter},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802773},
  month     = {10},
  pages     = {2258-2263},
  title     = {Evaluation of the design of a tool for the automated assembly of preconfigured wires*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Time-optimal TCP and robot base placement for pick-and-place
tasks in highly constrained environments. <em>IROS</em>, 2251–2257. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work proposes a highly parallelized optimization scheme to simultaneously optimize the robot base and tool center point (TCP) placement within a robotic work cell for a sequence of pick-and-place tasks. The placement is optimized for minimum cycle time by considering the scenario holistically, including point-to-point trajectory planning while respecting the kinodynamic constraints of the robot, collision avoidance in highly constrained environments, redundancy in grasp configurations and inverse kinematic solutions, and the cyclic constraint of the process. The proposed algorithm is applied to optimize the robot base and TCP placements in a spatially constrained packaging scenario, demonstrating a cycle time reduction of 41% compared to state-of-the-art approaches. The results are validated experimentally using a KUKA LBR iiwa with 7 degrees of freedom, where the TCP placement is realized using topology optimization and 3D printing.},
  archive   = {C_IROS},
  author    = {Alexander Wachter and Andreas Kugi and Christian Hartl-Nesic},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801373},
  month     = {10},
  pages     = {2251-2257},
  title     = {Time-optimal TCP and robot base placement for pick-and-place tasks in highly constrained environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visuo-tactile exploration of unknown rigid 3D curvatures by
vision-augmented unified force-impedance control. <em>IROS</em>,
2243–2250. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite recent advancements in torque-controlled tactile robots, integrating them into manufacturing settings remains challenging, particularly in complex environments. Simplifying robotic skill programming for non-experts is crucial for increasing robot deployment in manufacturing. This work proposes an innovative approach, Vision-Augmented Unified Force-Impedance Control (VA-UFIC), aimed at intuitive visuo-tactile exploration of unknown 3D curvatures. VA-UFIC stands out by seamlessly integrating vision and tactile data, enabling the exploration of diverse contact shapes in three dimensions, including point contacts, flat contacts with concave and convex curvatures, and scenarios involving contact loss. A pivotal component of our method is a robust online contact alignment monitoring system that considers tactile error, local surface curvature, and orientation, facilitating adaptive adjustments of robot stiffness and force regulation during exploration. We introduce virtual energy tanks within the control framework to ensure safety and stability, effectively addressing inherent safety concerns in visuo-tactile exploration. Evaluation using a Franka Emika research robot demonstrates the efficacy of VA-UFIC in exploring unknown 3D curvatures while adhering to arbitrarily defined force-motion policies. By seamlessly integrating vision and tactile sensing, VA-UFIC offers a promising avenue for intuitive exploration of complex environments, with potential applications spanning manufacturing, inspection, and beyond.},
  archive   = {C_IROS},
  author    = {Kübra Karacan and Anran Zhang and Hamid Sadeghian and Fan Wu and Sami Haddadin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801386},
  month     = {10},
  pages     = {2243-2250},
  title     = {Visuo-tactile exploration of unknown rigid 3D curvatures by vision-augmented unified force-impedance control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The control strategy for vehicle transfer robots in RO/RO
terminal environments. <em>IROS</em>, 2237–2242. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the labor-intensive Roll-On/Roll-Off (RO/RO) terminal environment, research on vehicle transport robots with mobility, stability, and reliability is receiving increasing attention. This paper presents a novel control framework for a Straddle-Type Dual-Body vehicle transfer robot. Initially, fine segmentation and processing of point clouds from different areas of the robot are performed, switching perception strategies for different areas based on event triggers. For target pose estimation, a traversal-based point cloud matrix fitting algorithm is designed. Additionally, for loading and unloading operations, a docking controller based on real-time target detection is developed to ensure minimal lateral and angular errors during target docking. Finally, the proposed control framework is validated through operations of the vehicle transfer robot in outdoor RO/RO terminal yards. Experimental results indicate that the average docking error remains within 3cm, with a 6.5% reduction in docking time under the same conditions. The docking precision and stability performance of the vehicle transfer robot surpass traditional methods, demonstrating satisfactory performance.},
  archive   = {C_IROS},
  author    = {Zhi Liu and Yongkang Xu and Lin Zhang and Shoukun Wang and Junzheng Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801881},
  month     = {10},
  pages     = {2237-2242},
  title     = {The control strategy for vehicle transfer robots in RO/RO terminal environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design, prototype, and performance assessment of an
autonomous manipulation system for mars sample recovery helicopter.
<em>IROS</em>, 2229–2236. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents the design, prototype, and testing of a 150 g (current best estimate) manipulation system that enables Mars Sample Recovery Helicopter (SRH) concept to autonomously pickup, stow, and drop-off Returnable Sample Tube and Glove Assemblies (RGAs) on the surface of Mars next to the Sample Retrieval Lander (SRL). It consists of a 3 DOF planar Robotic Arm (RA), a novel 2 DOF Gripper with compliant fingers, and a Stow Mechanism. Within the planned Mars Sample Return (MSR) campaign, two SRHs would operate in parallel to retrieve and transfer total of 10 RGAs (146g each) to the SRL, as the backup to the Perseverance Rover. Once SRH arrives at the target pickup location, the RA places the Gripper precisely over the RGA. The gripper grabs and picks up RGAs using a linkage based non-back-drivable mechanism and its compliant fingers. Subsequently, the RA is secured into the stow features, following dislodging rocks and pebbles, by going through a specific sequence of joint trajectories. This ensures the RA and RGA are stable and secure during transit to the SRL while all Manipulation System actuators are powered off. The whole sequence of manipulation is performed autonomously using feedback of a pair of stereo-cameras and absolute encoders. Experimental evaluation of the Manipulation System performance has proved its robustness and consistency in successful RGA pickup, stow, and drop-off.},
  archive   = {C_IROS},
  author    = {Arash Kalantari and Alex Brinkman and Kalind Carpenter and Matthew Gildner and Justin Jenkins and David Newill-Smith and Jeffrey Seiden and Allen Umali and Ryan Mccormick},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802158},
  month     = {10},
  pages     = {2229-2236},
  title     = {Design, prototype, and performance assessment of an autonomous manipulation system for mars sample recovery helicopter},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust backstepping controller with adaptive sliding mode
observer for a tilt-augmented quadrotor with uncertainty using SO(3).
<em>IROS</em>, 2223–2228. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The conventional quadrotor is incapable of controlling position and orientation independently. To mitigate this deficiency, we use a tilt-augmented quadrotor for greater mobility in a constrained environment. When the rotors tilt in a tilt-augmented quadrotor, it leads to changes in moment- of-inertia. This changes in the moment-of-inertia and external disturbances will introduce uncertainty terms into the model. In this paper, we design an adaptive super twisting sliding mode observer which guarantees finite time estimation of uncertain terms with unknown maximum bound. With the help of this observer, a backstepping controller using SO(3) is developed to establish exponential convergence to the desired trajectory. The exponential convergence of the backstepping controller and finite time convergence of the observer are shown using the Lyapunov approach. Hardware experiments are performed to compare the performance of both the existing controller and our proposed controller and corresponding videos are at https://www.youtube.com/watch?v=brTd5UYvciM.},
  archive   = {C_IROS},
  author    = {Sathyanarayanan Seshasayanan and Soumya Ranjan Sahoo},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801541},
  month     = {10},
  pages     = {2223-2228},
  title     = {Robust backstepping controller with adaptive sliding mode observer for a tilt-augmented quadrotor with uncertainty using SO(3)},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EyeSight hand: Design of a fully-actuated dexterous robot
hand with integrated vision-based tactile sensors and compliant
actuation. <em>IROS</em>, 1853–1860. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we introduce the EyeSight Hand, a 7 degrees of freedom (DoF) humanoid hand featuring integrated vision-based tactile sensors tailored for enhanced whole-hand manipulation. Additionally, we introduce an actuation scheme centered around quasi-direct drive actuation to achieve human-like strength and speed while ensuring robustness for large-scale data collection. We evaluate the EyeSight Hand on three challenging tasks: bottle opening, plasticine cutting, and plate pick and place, which require a blend of complex manipulation, tool use, and precise force application. Imitation learning models trained on these tasks, with a vision dropout strategy, showcase the benefits of tactile feedback in enhancing task success rates. Our results reveal that the integration of tactile sensing dramatically improves task performance, underscoring the critical role of tactile information in dexterous manipulation.},
  archive   = {C_IROS},
  author    = {Branden Romero and Hao-Shu Fang and Pulkit Agrawal and Edward Adelson},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802778},
  month     = {10},
  pages     = {1853-1860},
  title     = {EyeSight hand: Design of a fully-actuated dexterous robot hand with integrated vision-based tactile sensors and compliant actuation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IMU based pose reconstruction and closed-loop control for
soft robotic arms. <em>IROS</em>, 1847–1852. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft continuum manipulators are celebrated for their versatility and physical robustness to external forces and perturbations. However, this feature comes at a cost. The many degrees of freedom and compliance pose challenges for accurate pose reconstruction, both in terms of distributed sensing and pose reconstruction algorithms. Moreover, soft arms are inherently susceptible to deformation from external forces or loads, meaning that closed-loop control is essential for robust task performance. In this article, we propose the integration of multiple Inertial Measurement Units (IMUs) of a soft robot arm, Helix, for reconstruction of pose under internal and external forces. Furthermore, we integrate this dynamic pose reconstruction for kinematic-based closed-loop control strategies. By serially integrating sensing in the body of the Helix soft manipulator, we provide the system with high-frequency pose reconstruction and demonstrate improvements in end effector position with comparison to open-loop performance.},
  archive   = {C_IROS},
  author    = {Guanran Pei and Francesco Stella and Omar Meebed and Zhenshan Bing and Cosimo Della Santina and Josie Hughes},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802377},
  month     = {10},
  pages     = {1847-1852},
  title     = {IMU based pose reconstruction and closed-loop control for soft robotic arms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Differentiable collision-free parametric corridors.
<em>IROS</em>, 1839–1846. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a method to compute differentiable collision-free parametric corridors. In contrast to existing solutions that decompose the obstacle-free space into multiple convex sets, the continuous corridors computed by our method are smooth and differentiable, making them compatible with existing numerical techniques for learning and optimization. To achieve this, we represent the collision-free corridors as a path-parametric off-centered ellipse with a polynomial basis. We show that the problem of maximizing the volume of such corridors is convex, and can be efficiently solved. To assess the effectiveness of the proposed method, we examine its performance in a synthetic case study and subsequently evaluate its applicability in a real-world scenario from the KITTI dataset.},
  archive   = {C_IROS},
  author    = {Jon Arrizabalaga and Zachary Manchester and Markus Ryll},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802782},
  month     = {10},
  pages     = {1839-1846},
  title     = {Differentiable collision-free parametric corridors},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A convex formulation of frictional contact for the material
point method and rigid bodies. <em>IROS</em>, 1831–1838. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce a novel convex formulation that seamlessly integrates the Material Point Method (MPM) with articulated rigid body dynamics in frictional contact scenarios. We extend the linear corotational hyperelastic model into the realm of elastoplasticity and include an efficient return mapping algorithm. This approach is particularly effective for MPM simulations involving significant deformation and topology changes, while preserving the convexity of the optimization problem. Our method ensures global convergence, enabling the use of large simulation time steps without compromising robustness. We have validated our approach through rigorous testing and performance evaluations, highlighting its superior capabilities in managing complex simulations relevant to robotics. Compared to previous MPM-based robotic simulators, our method significantly improves the stability of contact resolution — a critical factor in robot manipulation tasks. We make our method available in the open-source robotics toolkit, Drake. The supplemental video is available here.},
  archive   = {C_IROS},
  author    = {Zeshun Zong and Chenfanfu Jiang and Xuchen Han},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801598},
  month     = {10},
  pages     = {1831-1838},
  title     = {A convex formulation of frictional contact for the material point method and rigid bodies},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ground-density clustering for approximate agricultural field
segmentation. <em>IROS</em>, 1824–1830. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Instance and semantic segmentation form the backbone of robotic perception and are crucial to many tasks. While most research in the area focuses on improving segmentation quality metrics, there are plenty of applications where approximate methods are adequate as long as they are fast, especially in applications with large amounts of data like precision agriculture. In order to apply the recent successes of machine learning and computer vision on a large scale using robotics, efficient and general algorithms must be designed to intelligently split point clouds into small, yet actionable, portions that can then be processed by more complex algorithms. In this paper, we capitalize on a similarity between the current state-of-the-art for roughly segmenting corn plants and a commonly used density-based clustering algorithm, Quickshift. Exploiting this similarity we propose a novel algorithm, Ground-Density Quickshift++, with the goal of producing a general and scalable field segmentation algorithm that segments individual plants and their stems. This algorithm produces quantitatively better results than the current state-of-the-art on both plant separation and stem segmentation while being less sensitive to input parameters and maintaining the same algorithmic time complexity. When incorporated into field-scale phenotyping systems, the proposed algorithm should work as a drop-in replacement that can greatly improve the accuracy of results while ensuring that performance and scalability remain undiminished.},
  archive   = {C_IROS},
  author    = {Henry J. Nelson and Nikolaos Papanikolopoulos},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802415},
  month     = {10},
  pages     = {1824-1830},
  title     = {Ground-density clustering for approximate agricultural field segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sim2real cattle joint estimation in 3D point clouds.
<em>IROS</em>, 1818–1823. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Understanding the well-being of cattle is crucial in various agricultural contexts. Cattle’s body shape and joint articulation carry significant information about their welfare, yet acquiring comprehensive datasets for 3D body pose estimation presents a formidable challenge. This study delves into the construction of such a dataset specifically tailored for cattle. Leveraging the expertise of digital artists, we use a single animated 3D model to represent diverse cattle postures. To address the disparity between virtual and real-world data, we augment the 3D model’s shape to encompass a range of potential body appearances, thereby narrowing the &quot;sim2real&quot; gap. We use these annotated models to train a deep-learning framework capable of estimating internal joints solely based on external surface curvature. Our contribution is specifically the use of geodesic distance over the surface manifold, coupled with multilateration to extract joints in a semantic keypoint detection encoder-decoder architecture. We demonstrate the robustness of joint extraction by comparing the link lengths extracted on real cattle mobbing and walking within a race. Furthermore, inspired by the established allometric relationship between bone length and the overall height of mammals, we utilise the estimated joints to predict hip height within a real cattle dataset, extending the utility of our approach to offer insights into improving cattle monitoring practices.},
  archive   = {C_IROS},
  author    = {Mohammad Okour and Raphael Falque and Alen Alempijevic},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801591},
  month     = {10},
  pages     = {1818-1823},
  title     = {Sim2real cattle joint estimation in 3D point clouds},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vinymap: A vineyard inspection and 3D reconstruction
framework for agricultural robots. <em>IROS</em>, 1812–1817. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Efficient and thorough vineyard inspection is crucial for optimizing yield and preventing disease from spreading. Manual approaches are labor-intensive and prone to human error, motivating the development of automated solutions. Precision viticulture benefits greatly from access to photo-realistic 3D vineyard maps and from capturing intricate visual details necessary for accurate canopy and grape health assessment. Generating such maps efficiently proves challenging, particularly when employing cost-effective equipment. This paper presents a novel vineyard inspection and 3D reconstruction framework implemented on a Robotic Platform (RP) equipped with three stereo cameras. The framework’s performance was evaluated on an experimental synthetic vineyard developed at NTUA. This testing setup allowed experimentation under diverse lighting conditions, ensuring the system’s robustness under realistic scenarios. Unlike existing solutions, which often focus on specific aspects of the inspection, our framework offers a top-down approach, encompassing autonomous navigation, high-fidelity 3D reconstruction, and canopy growth assessment. The developed software is available at the Control Systems Laboratory’s (CSL) bitbucket repository [1].},
  archive   = {C_IROS},
  author    = {Ioannis Zarras and Athanasios Mastrogeorgiou and Konstaninos Machairas and Konstantinos Koutsoukis and Evangelos Papadopoulos},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801409},
  month     = {10},
  pages     = {1812-1817},
  title     = {Vinymap: A vineyard inspection and 3D reconstruction framework for agricultural robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BonnBeetClouds3D: A dataset towards point cloud-based
organ-level phenotyping of sugar beet plants under real field
conditions. <em>IROS</em>, 1804–1811. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Agricultural production is facing challenges in the next decades induced by climate change and the need for more sustainability by reducing its impact on the environment. Advances in field management through robotic intervention, monitoring of crops by autonomous unmanned aerial vehicles (UAVs) supporting breeding of novel and more resilient crop varieties can help to address these challenges. The analysis of plant traits is called phenotyping and is an essential activity in plant breeding; it however involves a great amount of manual labor. With this paper, we provide means to better tackle the problems of instance segmentation to support robotic intervention and automatic fine-grained, organ-level geometric analysis needed for precision phenotyping. As the availability of real-world data in this domain is relatively scarce, we provide a novel dataset that was acquired using UAVs capturing high-resolution images of real breeding trials containing 48 plant varieties and therefore covering a relevant morphological and appearance spectrum. This enables the development of approaches for instance segmentation and autonomous phenotyping that generalize well to different plant varieties. Based on overlapping high-resolution images taken from multiple viewing angles, we provide photogrammetric dense point clouds and provide detailed and accurate point-wise labels for plants, leaves, and salient points as the tip and the base in 3D. Additionally, we include measurements of phenotypic traits performed by experts from the German Federal Plant Variety Office on the real plants, allowing the evaluation of new approaches not only on segmentation and keypoint detection but also directly on actual traits. The provided labeled point clouds enable finegrained plant analysis and support further progress in the development of automatic phenotyping approaches, but also enable further research in surface reconstruction, point cloud completion, and semantic interpretation of point clouds.},
  archive   = {C_IROS},
  author    = {Elias Marks and Jonas Bömer and Federico Magistri and Anurag Sag and Jens Behley and Cyrill Stachniss},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802820},
  month     = {10},
  pages     = {1804-1811},
  title     = {BonnBeetClouds3D: A dataset towards point cloud-based organ-level phenotyping of sugar beet plants under real field conditions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multimodal evolutionary encoder for continuous
vision-language navigation. <em>IROS</em>, 1443–1450. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Can multimodal encoder evolve when facing increasingly tough circumstances? Our work investigates this possibility in the context of continuous vision-language navigation (continuous VLN), which aims to navigate robots under linguistic supervision and visual feedback. We propose a multimodal evolutionary encoder (MEE) comprising a unified multimodal encoder architecture and an evolutionary pre-training strategy. The unified multimodal encoder unifies rich modalities, including depth and sub-instruction, to enhance the solid understanding of environments and tasks. It also effectively utilizes monocular observation, reducing the reliance on panoramic vision. The evolutionary pre-training strategy exposes the encoder to increasingly unfamiliar data domains and difficult objectives. The multi-stage adaption helps the encoder establish robust intra- and inter-modality connections and improve its generalization to unfamiliar environments. To achieve such evolution, we collect a large-scale multi-stage dataset with specialized objectives, addressing the absence of suitable continuous VLN pre-training. Evaluation on VLN-CE demonstrates the superiority of MEE over other direct action-predicting methods. Furthermore, we deploy MEE in real scenes using self-developed service robots, showcasing its effectiveness and potential for real-world applications. Our code and dataset are available at https://github.com/RavenKiller/MEE.},
  archive   = {C_IROS},
  author    = {Zongtao He and Liuyi Wang and Lu Chen and Shu Li and Qingqing Yan and Chengju Liu and Qijun Chen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802484},
  month     = {10},
  pages     = {1443-1450},
  title     = {Multimodal evolutionary encoder for continuous vision-language navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BEVCar: Camera-radar fusion for BEV map and object
segmentation. <em>IROS</em>, 1435–1442. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Semantic scene segmentation from a bird’s-eye-view (BEV) perspective plays a crucial role in facilitating planning and decision-making for mobile robots. Although recent vision-only methods have demonstrated notable advancements in performance, they often struggle under adverse illumination conditions such as rain or nighttime. While active sensors offer a solution to this challenge, the prohibitively high cost of LiDARs remains a limiting factor. Fusing camera data with automotive radars poses a more inexpensive alternative but has received less attention in prior research. In this work, we aim to advance this promising avenue by introducing BEVCar, a novel approach for joint BEV object and map segmentation. The core novelty of our approach lies in first learning a point-based encoding of raw radar data, which is then leveraged to efficiently initialize the lifting of image features into the BEV space. We perform extensive experiments on the nuScenes dataset and demonstrate that BEVCar outperforms the current state of the art. Moreover, we show that incorporating radar information significantly enhances robustness in challenging environmental conditions and improves segmentation performance for distant objects. To foster future research, we provide the weather split of the nuScenes dataset used in our experiments, along with our code and trained models at http://bevcar.cs.uni-freiburg.de.},
  archive   = {C_IROS},
  author    = {Jonas Schramm and Niclas Vödisch and Kürsat Petek and B Ravi Kiran and Senthil Yogamani and Wolfram Burgard and Abhinav Valada},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802147},
  month     = {10},
  pages     = {1435-1442},
  title     = {BEVCar: Camera-radar fusion for BEV map and object segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spike-based high energy efficiency and accuracy tracker for
robot. <em>IROS</em>, 1428–1434. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Spiking Neural Networks (SNNs) have gained attention for their apparent energy efficiency and significant biological interpretability, although they also face significant challenges such as prolonged latency and suboptimal tracking accuracy. Recent studies have explored the application of SNNs in object tracking tasks. Dynamic visual sensors (DVS) have become a popular way to implement SNN-based object tracking due to their asynchronous and spiking characteristics similar to SNNs. However, challenges such as the high cost of DVS cameras and the lack of object surface texture information hinder the utility and performance of DVS trackers. In contrast, RGB information has inherent advantages, including low acquisition cost and comprehensive object surface texture representation. However, RGB information is prone to excessive image blurring in low-light conditions or in fast-motion scenes. To address these challenges, we propose the “Motion Feature Extractor” and the &quot;RGB-DVS Fusion Module&quot;. The “Motion Feature Extractor” can replace the DVS camera at a very low cost, and the &quot;RGB-DVS Fusion Module&quot; can deeply fuse the feature information of the two to make up for their respective deficiencies. In addition, we adopt a conversion method to obtain a lossless SNN version of the model. Through experiments, our model achieves a 13.6% improvement in the expected average overlap (EAO) index using only 1.47% of the energy consumption of SiamRPN (VOT2016 dataset). In addition, we deployed the model to a robot and then conducted tracking experiments, which confirmed that the model can operate on the robot losslessly with satisfactory results.},
  archive   = {C_IROS},
  author    = {Jinye Qu and Zeyu Gao and Yi Li and Yanfeng Lu and Hong Qiao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801326},
  month     = {10},
  pages     = {1428-1434},
  title     = {Spike-based high energy efficiency and accuracy tracker for robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evidential semantic mapping in off-road environments with
uncertainty-aware bayesian kernel inference. <em>IROS</em>, 1420–1427.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic mapping with Bayesian Kernel Inference (BKI) has shown promise in creating semantic maps by effectively leveraging local spatial information. However, existing semantic mapping methods face challenges in constructing reliable maps in unstructured outdoor scenarios due to unreliable semantic predictions. To address this issue, we propose an evidential semantic mapping, which can enhance reliability in perceptually challenging off-road environments. We integrate Evidential Deep Learning into the semantic segmentation network to obtain the uncertainty estimate of semantic prediction. Subsequently, this semantic uncertainty is incorporated into an uncertainty-aware BKI, tailored to prioritize more confident semantic predictions when accumulating semantic information. By adaptively handling semantic uncertainties, the proposed framework constructs robust representations of the surroundings even in previously unseen environments. Comprehensive experiments across various off-road datasets demonstrate that our framework enhances accuracy and robustness, consistently outperforming existing methods in scenes with high perceptual uncertainties.},
  archive   = {C_IROS},
  author    = {Junyoung Kim and Junwon Seo and Jihong Min},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802766},
  month     = {10},
  pages     = {1420-1427},
  title     = {Evidential semantic mapping in off-road environments with uncertainty-aware bayesian kernel inference},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DiMSam: Diffusion models as samplers for task and motion
planning under partial observability. <em>IROS</em>, 1412–1419. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generative models such as diffusion models, excel at capturing high-dimensional distributions with diverse input modalities, e.g. robot trajectories, but are less effective at multistep constraint reasoning. Task and Motion Planning (TAMP) approaches are suited for planning multi-step autonomous robot manipulation. However, it can be difficult to apply them to domains where the environment and its dynamics are not fully known. We propose to overcome these limitations by composing diffusion models using a TAMP system. We use the learned components for constraints and samplers that are difficult to engineer in the planning model, and use a TAMP solver to search for the task plan with constraint-satisfying action parameter values. To tractably make predictions for unseen objects in the environment, we define the learned samplers and TAMP operators on learned latent embedding of changing object states. We evaluate our approach in a simulated articulated object manipulation domain and show how the combination of classical TAMP, generative modeling, and latent embedding enables multi-step constraint-based reasoning. We also apply the learned sampler in the real world. Website: https://sites.google.com/view/dimsam-tamp.},
  archive   = {C_IROS},
  author    = {Xiaolin Fang and Caelan Reed Garrett and Clemens Eppner and Tomás Lozano-Pérez and Leslie Pack Kaelbling and Dieter Fox},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802746},
  month     = {10},
  pages     = {1412-1419},
  title     = {DiMSam: Diffusion models as samplers for task and motion planning under partial observability},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Millipede-inspired multi-legged magnetic soft robots for
targeted locomotion in tortuous environments. <em>IROS</em>, 1406–1411.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Miniature robots capable of untethered operation hold great promise for performing diagnostic and therapeutic procedures in hard-to-reach regions within the human body. Nonetheless, navigating these complex and diverse physiological environments remains a significant challenge. To effectively navigate the tortuous pathways inside the human body, it is essential to equip miniature robots with flexible body structures that can adapt to complex geometries and develop efficient actuation strategies for deformed robots. In this study, we present a miniature soft robot featuring a zigzag body structure, imparting the robot with remarkable deformation capabilities that enable it to adapt to confined and tortuous spaces. This robot is equipped with arrays of magnetic legs, enabling robust locomotion propelled by traveling metachronal waves. We demonstrate that the robot can crawl on both flat surfaces and slopes. Leveraging its in-plane flexibility and discrete actuation system, this robot can navigate through intricate environments with precise control using magnetic fields. Our work provides valuable insights into the development of crawling robots with enhanced agility and adaptability, creating opportunities for their future use in a wide range of biomedical applications.},
  archive   = {C_IROS},
  author    = {Yibin Wang and Yiting Xiong and Kaiwen Fang and Jiangfan Yu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802069},
  month     = {10},
  pages     = {1406-1411},
  title     = {Millipede-inspired multi-legged magnetic soft robots for targeted locomotion in tortuous environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the modularity of elementary dynamic actions.
<em>IROS</em>, 1398–1405. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, a kinematically modular approach to robot control is presented. The method involves structures called Elementary Dynamic Actions and a network model combining these elements. With this control framework, a rich repertoire of movements can be generated by combination of basic modules. The problems of solving inverse kinematics, managing kinematic singularity and kinematic redundancy are avoided. The modular approach is robust against contact and physical interaction, which makes it particularly effective for contact-rich manipulation. Each kinematic module can be learned by Imitation Learning, thereby resulting in a modular learning strategy for robot control. The theoretical foundations and their implementation on a real robot are presented. Using a KUKA LBR iiwa robot, three tasks were considered: (1) generating a sequence of discrete movements, (2) generating a combination of discrete and rhythmic movements, and (3) a drawing and erasing task. The results obtained show that this modular approach has the potential to simplify the generation of a diverse range of robot actions.},
  archive   = {C_IROS},
  author    = {Moses C. Nah and Johannes Lachner and Federico Tessari and Neville Hogan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801502},
  month     = {10},
  pages     = {1398-1405},
  title     = {On the modularity of elementary dynamic actions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FogROS2-FT: Fault tolerant cloud robotics. <em>IROS</em>,
1390–1397. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cloud robotics enables robots to offload complex computational tasks to cloud servers for performance and ease of management. However, cloud compute can be costly, cloud services can suffer occasional downtime, and connectivity between the robot and cloud can be prone to variations in network Quality-of-Service (QoS). We present FogROS2-FT (Fault Tolerant) to mitigate these issues by introducing a multi-cloud extension that automatically replicates independent stateless robotic services, routes requests to these replicas, and directs the first response back. With replication, robots can still benefit from cloud computations even when a cloud service provider is down or there is low QoS. Additionally, many cloud computing providers offer low-cost &quot;spot&quot; computing instances that may shutdown unpredictably. Normally, these low-cost instances would be inappropriate for cloud robotics, but the fault tolerance nature of FogROS2-FT allows them to be used reliably. We demonstrate FogROS2-FT fault tolerance capabilities in 3 cloud-robotics scenarios in simulation (visual object detection, semantic segmentation, motion planning) and 1 physical robot experiment (scan-pick-and-place). Running on the same hardware specification, FogROS2-FT achieves motion planning with up to 2.2x cost reduction and up to a 5.53x reduction on 99 Percentile (P99) long-tail latency. FogROS2-FT reduces the P99 long-tail latency of object detection and semantic segmentation by 2.0x and 2.1x, respectively, under network slowdown and resource contention. Videos and code are available at https://sites.google.com/view/fogros2-ft.},
  archive   = {C_IROS},
  author    = {Kaiyuan Chen and Kush Hari and Trinity Chung and Michael Wang and Nan Tian and Christian Juette and Jeffrey Ichnowski and Liu Ren and John Kubiatowicz and Ion Stoica and Ken Goldberg},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802613},
  month     = {10},
  pages     = {1390-1397},
  title     = {FogROS2-FT: Fault tolerant cloud robotics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Abstraction of the body ability of the transformer robot
system for the transportation and installation of heavy objects in land
and underwater environments. <em>IROS</em>, 1382–1389. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To give the single robot system the ability to realize many behaviors and to realize tasks with shifting environments and objectives, it is necessary to abstract the robot’s body ability to the extent that they can be detected by sensors in the body so that we can plan as the problem of state transition.In this paper, to abstract the transformer robot system that performs heavy lifting and environmental attachment tasks in an aquatic and terrestrial environment, we extend the graphical representation of the robot’s body to manage joint capability and the body adaptability for the environment. To abstract the body ability, we divide the body into elements and define Connection between them at three different granularities. And using Connection, we propose the Connection Modification Feature(CMF) as the representation for changing body ability. To implement the Connection Modification Feature, we perform the abstract description and extract Connection to construct Body Ability Graph, a graph for the robot to manage its body ability. We show that it is possible to plan to manipulate and use its own Connection Modification Feature through multiple experiments by defining Normal Action that does not change body abilities and Body Ability Modifying Action that manipulate body abilities.},
  archive   = {C_IROS},
  author    = {Tasuku Makabe and Kei Okada and Masayuki Inaba},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801309},
  month     = {10},
  pages     = {1382-1389},
  title     = {Abstraction of the body ability of the transformer robot system for the transportation and installation of heavy objects in land and underwater environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). External interaction estimation of 6-PSS parallel robots
with embodied mechanical intelligence. <em>IROS</em>, 1376–1381. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traditional interaction perception of parallel robots relies on a six-dimensional force sensor for contact sensing at their distal end. However, the sensor body occupies the space of moving platform and also increases the load on the robot actuations. To enable both minimization and embodied intelligence, this paper proposes an external interaction estimation method with embodied mechanical intelligence by embedding two single-axis force sensors in each leg of 6-PSS parallel robot. The method uses a backward propagation neural network optimized by sparrow search algorithm, and it can simultaneously estimate the external force and its position using information from multiple single-axis force sensors and the encoder of driving motor. The experimental platform is established to collect the data and train the network. The result shows that the force estimation mean error is 2.4% and the position estimation error is 2.9%. A demonstration with a virtual display interface showing the reconstructed parallel robot pose, and the interaction force and its pose using the proposed estimation method, indicates the effectiveness of the proposed interaction method with embodied mechanical intelligence for 6-PSS parallel robot},
  archive   = {C_IROS},
  author    = {Jingyuan Xia and Zecai Lin and Xiaojie Ai and Guangjun Yu and Anzhu Gao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801416},
  month     = {10},
  pages     = {1376-1381},
  title     = {External interaction estimation of 6-PSS parallel robots with embodied mechanical intelligence},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Development of a novel redundant parallel mechanism with
enlarged workspace and enhanced dexterity for fracture reduction
surgery. <em>IROS</em>, 1370–1375. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The limited workspace and complex singularity issues are predominant factors impeding the clinical applicability of fracture reduction parallel robots. To address these challenges, this paper proposes a novel redundant parallel mechanism (NRPM) for robotic-assisted fracture reduction with an enlarged workspace and enhanced dexterity capabilities based on the traditional Stewart parallel mechanism (SPM). With six redundant degrees-of-freedom (DOFs) added to the novel mechanism, the kinematics of NRPM needs to be thoroughly analyzed. Furthermore, the calculation of its workspace and determination of its dexterity are deduced. Both the analytical simulation and real experiment results demonstrated the effectiveness and superior performance of the proposed NRPM compared to SPM.},
  archive   = {C_IROS},
  author    = {Quan Yuan and Xu Liang and Tingting Su and Weibang Bai},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801501},
  month     = {10},
  pages     = {1370-1375},
  title     = {Development of a novel redundant parallel mechanism with enlarged workspace and enhanced dexterity for fracture reduction surgery},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analysis of lockable passive prismatic and revolute joints.
<em>IROS</em>, 1363–1369. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper analyzes the stresses, positional errors, and friction in lockable passive prismatic and revolute joints. The joints’ locking mechanisms that use solenoids to trigger the locking action have a self-alignment capability. The stress analysis evaluates the strength and material deformation of the joints’ components. The positional error analysis relates the clearances and contact deformations in the joints’ assembly with the positional errors of the joints. The friction analysis investigates how the friction during the locking motion interacts with the joint load, the pushing force, and the locking acceleration. The stress analysis was performed analytically for simplified cases and by finite element analysis for cases involving complex geometries and nonlinear contact. The positional error and friction analyses were performed analytically by deriving the kinematic and dynamic equations. Discussions based on the analyses provide a deeper understanding of the behavior of lockable joints that applies not only to the specific joints discussed in this paper but also to other lockable joints working with similar principles.},
  archive   = {C_IROS},
  author    = {Abdur Rosyid and Bashar El-Khasawneh},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801950},
  month     = {10},
  pages     = {1363-1369},
  title     = {Analysis of lockable passive prismatic and revolute joints},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On a magnetically driven array system with autonomous motion
and object delivery for biomedical microrobots. <em>IROS</em>,
1357–1362. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The application of microrobots in the biomedical field has attracted great interest, among which drug transportation is one of the application scenarios. Traditional studies used the global magnetic field to control single microrobot, therefore it is impossible to control multiple microrobots. To address this problem, this paper develops a local magnetic field generation system to realize the independent control of multiple microrobots. The proposed multi-microrobot motion system integrates perception, planning, and actuation, enabling autonomous multi-task drug delivery. In our system, we first develop a printed circuit board (PCB) array magnetic driven microrobot system based on a micro coil array, then the Yolov8 framework is employed for the target/environment recognition, accurately identifying microrobots and magnetic fluids, while the Rapidly-exploring Random Trees (RRT) algorithm is used for path planning. We have conducted experiments on obstacle avoidance, droplet transport, and drug fusion. The results clearly demonstrate the significant potential of magnetic fielddriven microcoil array devices in transportation and drug fusion engineering.},
  archive   = {C_IROS},
  author    = {Yueyue Liu and Zhe Hou and Qigao Fan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801712},
  month     = {10},
  pages     = {1357-1362},
  title     = {On a magnetically driven array system with autonomous motion and object delivery for biomedical microrobots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new 10-mg SMA-based fast bimorph actuator for
microrobotics. <em>IROS</em>, 1349–1356. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a new millimeter-scale bimorph actuator for microrobotic applications, driven by feedforward controlled shape-memory alloy (SMA) wires. The device weighs 10 mg, measures 14 mm in length, and occupies a volume of 4.8 mm3, which makes it the lightest and smallest fully functional SMA-based bimorph actuator for microrobotics developed to date. The experimentally measured operational bandwidth is on the order of 20 Hz, and the unimorph and bimorph maximum low-frequency displacement outputs are on the order of 3.5 and 7 mm, respectively. To test and demonstrate the functionality and suitability of the actuator for microrobotics, we developed the Fish-&amp;-Ribbon–Inspired Small Swimming Harmonic roBot (FRISSHBot). Loosely inspired by carangiformes, the FRISSHBot leverages fluid-structure interaction (FSI) phenomena to propel itself forward, weighs 30 mg, measures 34 mm in length, operates at frequencies of up to 4 Hz, and swims at speeds of up to 3.06 mm • s–1 (0.09 Bl • s–1). This robot is the lightest and smallest swimmer with onboard actuation developed to date.},
  archive   = {C_IROS},
  author    = {Conor K. Trygstad and Elijah K. Blankenship and Néstor O. Pérez-Arancibia},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802033},
  month     = {10},
  pages     = {1349-1356},
  title     = {A new 10-mg SMA-based fast bimorph actuator for microrobotics},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NanoNeRF: Robot-assisted nanoscale 360° reconstruction with
neural radiance field under scanning electron microscope. <em>IROS</em>,
1343–1348. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The pursuit of 3D reconstruction from 2D images for nanomanipulation under scanning electron microscopy stands as a critical research endeavor. Previous methods either necessitates additional lighting which is difficult in standard SEM devices or relies on feature matching with low resolution and precision, further constraining reconstruction performance. In this paper, we propose a novel robot-assisted nanoscale 360° reconstruction approach, which simplifies SEM setups and maximizes the utilization of robot motion and feedback. By harnessing a nanorobotic system, we capture 360°multi-view images automatically with precise mapping information and camera postures. Sequentially, neural radiance field reconstruct the pixel-wise structure and synthesizing images from diverse perspectives. Experimental results using two real datasets demonstrates our approach’s efficacy, achieving PSNR of 28.1 and SSIM of 0.93 for nanotube reconstruction, and PSNR of 32.8 and SSIM of 0.98 for AFM cantilever reconstruction. These results validate the reliability and robustness of our proposed robot-assisted reconstruction method.},
  archive   = {C_IROS},
  author    = {Xiang Fu and Yifan Xu and Shudong Wang and Haojian Lu and Jiaqi Li and Youfu Li and Hu Su and Song Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802598},
  month     = {10},
  pages     = {1343-1348},
  title     = {NanoNeRF: Robot-assisted nanoscale 360° reconstruction with neural radiance field under scanning electron microscope},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Navigated locomotion and controllable splitting of a
microswarm in a complex environment. <em>IROS</em>, 1337–1342. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reconfigurable microswarms have received extensive attention recently. In this work, we propose a control strategy for a ribbon-like swarm to perform navigated locomotion with a stable pattern, and perform controllable splitting into double subswarms to reach two targets simultaneously. Two different behaviors of the ribbon-like swarm are firstly investigated, i.e., locomotion with a stable pattern, and controllable splitting. The two behaviors of the ribbon-like swarm are realized based on different aspect ratio of the swarm. Subsequently, we propose a morphology controller to keep the aspect ratio of the swarm within a desired range. The morphology controller consists of a feedforward controller and a PD controller. The feedforward controller containing a fitted model, and a fuzzy logic controller for online compensation of the model error. The control strategy combining the morphology planning, morphology controller, path planning, and motion controller is developed. Using the proposed control strategy, the ribbon-like swarm can be navigated to follow a desired path with a stable pattern while avoiding obstacles, and finally perform controllable splitting into double subswarms to reach two predefined targets simultaneously.},
  archive   = {C_IROS},
  author    = {Yuezhen Liu and Guangjun Zeng and Xingzhou Du and Kaiwen Fang and Jiangfan Yu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801336},
  month     = {10},
  pages     = {1337-1342},
  title     = {Navigated locomotion and controllable splitting of a microswarm in a complex environment},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing robustness in manipulability assessment: The
pseudo-ellipsoid approach. <em>IROS</em>, 1329–1336. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Manipulability analysis is a methodology employed to assess the capacity of an articulated system, at a specific configuration, to produce motion or exert force in diverse directions. The conventional method entails generating a virtual ellipsoid using the system’s configuration and model. Yet, this approach poses challenges when applied to systems such as the human body, where direct access to such information is limited, necessitating reliance on estimations. Any inaccuracies in these estimations can distort the ellipsoid’s configuration, potentially compromising the accuracy of the manipulability assessment. To address this issue, this article extends the standard approach by introducing the concept of the manipulability pseudo-ellipsoid. Through a series of theoretical analyses, simulations, and experiments, the article demonstrates that the proposed method exhibits reduced sensitivity to noise in sensory information, consequently enhancing the robustness of the approach.},
  archive   = {C_IROS},
  author    = {Erfan Shahriari and Kim Kristin Peper and Matej Hoffmann and Sami Haddadin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801492},
  month     = {10},
  pages     = {1329-1336},
  title     = {Enhancing robustness in manipulability assessment: The pseudo-ellipsoid approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ICR-based kinematics for wheeled skid-steer vehicles on firm
slopes*. <em>IROS</em>, 1322–1328. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a new kinematic model for wheeled skid-steer vehicles moving with low inertia on firm slopes that is based on the variation of the Instantaneous Center of Rotation (ICR) of its lateral treads. To this end, the ICR-based model for horizontal surfaces, where constant tread ICRs were considered, has been extended with two additional parameters to account for the sliding down phenomenon on inclined terrains that the former is unable to predict. The current pitch and roll angles of the vehicle together with the speeds of the treads are employed to calculate the changing positions of tread ICRs during turnings. This kinematic approach shows promising results when applied to the heavy robotic rover J8 on a slanted surface of smooth concrete.},
  archive   = {C_IROS},
  author    = {Jorge L. Martínez and Jesús Morales and Manuel Sánchez and Alfonso García-Cerezo},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801776},
  month     = {10},
  pages     = {1322-1328},
  title     = {ICR-based kinematics for wheeled skid-steer vehicles on firm slopes*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards electricity-free pneumatic miniature rotation
actuator for optical coherence tomography endoscopy. <em>IROS</em>,
1316–1321. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Miniature rotation actuators have been extensively developed and utilized in optical coherence tomography (OCT) endoscopy, enabling distortion-free OCT imaging in complex and tortuous environments. However, the use of electrical-driven rotation actuators raises safety concerns. Although magnetic-driven rotation actuators have been reported in OCT endoscopy, their use can potentially interfere with other medical devices in clinical settings. Here, we propose a pneumatic miniature rotation actuator that eliminates the electricity and magnetism concerns in circumferential imaging for OCT endoscopy. The rotor of the actuator is designed as a windmill, enabling it to convert air energy into rotation energy. In addition, to maintain the stable rotation, both a sliding bearing with two supporting points and a glass spindle with a half-ball end surface are developed. The rotation speed of our pneumatic actuator can be controlled from 66 to 97 revolutions per second by adjusting the airflow rate from 3.25 to 4.00 liters per minute. By OCT imaging of the human fingers, we demonstrate the feasibility of the pneumatic actuator in electricity-free distal scanning OCT endoscopy. Our pneumatic rotation actuator has wide-ranging potential in various fiber-imaging modalities, including not only OCT but also ultrasound imaging that requires similar rotation capabilities.},
  archive   = {C_IROS},
  author    = {Tinghua Zhang and Sishen Yuan and Chao Xu and Peng Liu and Hongliang Ren and Wu Yuan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802388},
  month     = {10},
  pages     = {1316-1321},
  title     = {Towards electricity-free pneumatic miniature rotation actuator for optical coherence tomography endoscopy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design and implementation of a novel wheel-based cable
inspection robot. <em>IROS</em>, 1310–1315. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Regular maintenance and inspection of cables are essential for cable-stayed bridges and suspension bridges, as cables serve as the core components. In order to enable automated detection of cables, this paper proposes a novel wheeled cable inspection robot. The robot utilizes a bilateral wheel structure and is composed of four independent suspension mechanisms. By collaborating with a lifting mechanism, the robot achieves functions such as adhesion, climbing, and obstacle overcoming. The robot is powered by a lithium polymer battery and operated via wireless control by ground personnel. This paper provides a detailed exposition on the structure design and control system of robots, and conducts a mechanical analysis of the suspension mechanism of robots. The maximum obstacle-negotiation height of the robot is calculated, and a mechanical model for cable climbing is established. During prototype testing, the robot demonstrated a mass of 6.7kg, a maximum payload capacity of 6kg, a maximum obstacle height of 10mm, and a fastest climbing speed of 14m/min. These specifications meet the requirements of practical inspections.},
  archive   = {C_IROS},
  author    = {Hou Mengqi and Jie Li and Fengyu Xu and Lezhi Hu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802131},
  month     = {10},
  pages     = {1310-1315},
  title     = {Design and implementation of a novel wheel-based cable inspection robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Formalization of temporal and spatial constraints of
bimanual manipulation categories. <em>IROS</em>, 1302–1309. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Executing bimanual manipulation tasks on humanoid robots introduces additional challenges due to inherent spatial and temporal coordination between both hands. In our previous work, we proposed the Bimanual Manipulation Taxonomy, which defines categories of bimanual manipulation strategies based on the coordination and physical interaction between both hands, the role of each hand in the task, and the symmetry of arm movements during task execution. In this work, we build upon this taxonomy and provide a formalization of temporal and spatial constraints associated with each category of the taxonomy. This formalization uses Petri nets to represent temporal constraints and differentiates between relative and global targets. We incorporate these constraints in a category-specific controller to enable reactive adaptation of the behavior according to the respective coordination constraints. We evaluated our approach in simulation and in real-world experiments on the humanoid robot ARMAR-6. The results demonstrate that category-specific constraints can be enforced when needed while maintaining flexibility to accommodate additional constraints.},
  archive   = {C_IROS},
  author    = {Franziska Krebs and Tamim Asfour},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801861},
  month     = {10},
  pages     = {1302-1309},
  title     = {Formalization of temporal and spatial constraints of bimanual manipulation categories},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CubiX: Portable wire-driven parallel robot connecting to and
utilizing the environment. <em>IROS</em>, 1296–1301. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A wire-driven parallel robot is a type of robotic system where multiple wires are used to control the movement of a end-effector. The wires are attached to the end-effector and anchored to fixed points on external structures. This configuration allows for the separation of actuators and end-effectors, enabling lightweight and simplified movable parts in the robot. However, its range of motion remains confined within the space formed by the wires, limiting the wire-driven capability to only within the pre-designed operational range. Here, in this study, we develop a wire-driven robot, CubiX, capable of connecting to and utilizing the environment. CubiX connects itself to the environment using up to 8 wires and drives itself by winding these wires. By integrating actuators for winding the wires into CubiX, a portable wire-driven parallel robot is realized without limitations on its workspace. Consequently, the robot can form parallel wire-driven structures by connecting wires to the environment at any operational location.},
  archive   = {C_IROS},
  author    = {Shintaro Inoue and Kento Kawaharazuka and Temma Suzuki and Sota Yuzaki and Kei Okada and Masayuki Inaba},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802299},
  month     = {10},
  pages     = {1296-1301},
  title     = {CubiX: Portable wire-driven parallel robot connecting to and utilizing the environment},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kinematic modeling of twisted string actuator based on
invertible neural networks. <em>IROS</em>, 1290–1295. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Twisted String Actuators (TSAs) exhibit several advantages, including lightweight, compact, and having a high power-to-weight ratio. However, current research on kinematic models of TSAs is limited to deriving the relationship between motor input and output through idealized geometric calculations. Therefore, the accuracy of these models does not meet the requirements for practical applications. Previous studies on the kinematic modeling of TSA have not considered the impact of material plastic deformation and stroke times on TSA kinematics. Accumulation of plastic deformation over multiple strokes leads to changes in the output displacement of TSA, significantly affecting the accuracy of the kinematic model. This study aims to address the limitations of previous research by investigating the use of Invertible Neural Networks (INNs) in kinematic modeling of TSAs, taking into account material plastic deformation and stroke times. Through a series of TSA experiments, a kinematic model of TSA was established using an INN that considers stroke times. The INN model proves to be superior in both forward and inverse kinematic modeling by effectively compensating for the effects of plastic deformation during TSA operation. The experimental results demonstrate that the kinematic model established by the proposed INN is more aligned with the actual conditions when compared to traditional kinematic models. This insight can aid in predicting the lifespan of TSA in the future.},
  archive   = {C_IROS},
  author    = {Zekun Liu and Dunwen Wei and Tao Gao and Jumin Gong},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802799},
  month     = {10},
  pages     = {1290-1295},
  title     = {Kinematic modeling of twisted string actuator based on invertible neural networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MANIP: A modular architecture for integrating interactive
perception for robot manipulation. <em>IROS</em>, 1283–1289. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a modular systems architecture, MANIP, that can facilitate the design and development of robot manipulation systems by systematically combining learned subpolicies with well-established procedural algorithmic primitives such as Inverse Kinematics, Kalman Filters, RANSAC outlier rejection, PID modules, etc. (aka &quot;Good Old Fashioned Engineering (GOFE)&quot;). The MANIP architecture grew from our lab’s experience developing robot systems for folding clothes, routing cables, and untangling knots. To address failure modes, MANIP can facilitate inclusion of &quot;interactive perception&quot; subpolicies that execute robot actions to modify system state to bring the system into alignment with the training distribution and / or to disambiguate system state when system state confidence is low. We demonstrate how MANIP can be applied with 3 case studies and then describe a detailed case study in cable tracing with experiments that suggest MANIP can improve performance by up to 88%. Code and details are available at: https://berkeleyautomation.github.io/MANIP/},
  archive   = {C_IROS},
  author    = {Justin Yu and Tara Sadjadpour and Abby O’Neill and Mehdi Khfifi and Lawrence Yunliang Chen and Richard Cheng and Muhammad Zubair Irshad and Ashwin Balakrishna and Thomas Kollar and Ken Goldberg},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801746},
  month     = {10},
  pages     = {1283-1289},
  title     = {MANIP: A modular architecture for integrating interactive perception for robot manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Risk-averse planning and plan assessment for marine robots*.
<em>IROS</em>, 1277–1282. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous Underwater Vehicles (AUVs) need to operate for days without human intervention and thus must be able to do efficient and reliable task planning. Unfortunately, efficient task planning requires deliberately abstract domain models (for scalability reasons), which in practice leads to plans that might be unreliable or under performing in practice. An optimal abstract plan may turn out suboptimal or unreliable during physical execution. To overcome this, we introduce a method that first generates a selection of diverse high-level plans and then assesses them in a low-level simulation to select the optimal and most reliable candidate. We evaluate the method using a realistic underwater robot simulation, estimating the risk metrics for different scenarios, demonstrating feasibility and effectiveness of the approach.},
  archive   = {C_IROS},
  author    = {Mahya Mohammadi Kashani and Tobias John and Jeremy P. Coffelt and Einar Broch Johnsen and Andrzej Wąsowski},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802496},
  month     = {10},
  pages     = {1277-1282},
  title     = {Risk-averse planning and plan assessment for marine robots*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interpretation of legged locomotion in underwater robots
based on rimless wheel model. <em>IROS</em>, 1271–1276. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inspired by the fascinating underwater locomotion of cephalopods such as octopuses, this research explores the possibility of using legged robots in underwater environments. Using a rimless wheel model, we investigated their navigation and adaptation capabilities in a dynamic fluid environment. Through sophisticated numerical simulations, we reproduce legged robot behaviours such as walking underwater and jumping over uneven seabed terrain. Our research aims to provide valuable insights for the development of versatile underwater robotic systems suitable for various applications in ocean exploration and surveying.},
  archive   = {C_IROS},
  author    = {Yuetong He and Fumihiko Asano},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801686},
  month     = {10},
  pages     = {1271-1276},
  title     = {Interpretation of legged locomotion in underwater robots based on rimless wheel model},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OAS-GPUCB: On-the-way adaptive sampling using GPUCB for
bathymetry mapping. <em>IROS</em>, 1265–1270. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bathymetry mapping of static water bodies like lakes is essential for sustainable ecosystem development strategies. However, bathymetry mapping using (i) traditional manual sampling approaches using single-beam echosounder (SBE) has large mapping errors and (ii) performing multi-beam survey is very expensive. Alternatively, performing lawn-mower SBE surveys is time consuming due to limited field-of-view. In order to address the above issues, in this paper, we present an on-the-way sampling approach with Gaussian Process Upper Confidence Bound (GPUCB) algorithm called as OAS-GPUCB that can adaptively sample the lake to minimize the bathymetry error while reducing the distance travelled to achieve a given mapping accuracy. We validate the proposed approach using simulations on actual lake bathymetry maps and also carry out real-world experiments using an Autonomous Surface Vehicle(ASV) with SBE. Further, we compare OAS-GPUCB to lawn-mower, GPUCB, and GPUCB with fixed radius approaches. The results consistently show that the proposed approach can achieve less than 10% bathymetry error while achieving distance reduction of more than 55%compared to the lawn-mower approach, and more than 90% less distance travelled compared to GPUCB and GPUCB with fixed radius approaches. The results shows the general applicability of OAS-GPUCB for bathymetry mapping of water bodies without any prior information maps.},
  archive   = {C_IROS},
  author    = {Rajat Agrawal and Karthik Nambiar and Bhawana Chhaglani and Mandar Chitre and P. B. Sujit},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801947},
  month     = {10},
  pages     = {1265-1270},
  title     = {OAS-GPUCB: On-the-way adaptive sampling using GPUCB for bathymetry mapping},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A deep reinforcement learning framework and methodology for
reducing the sim-to-real gap in ASV navigation. <em>IROS</em>,
1258–1264. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite the increasing adoption of Deep Reinforcement Learning (DRL) for Autonomous Surface Vehicles (ASVs), there still remain challenges limiting real-world deployment. In this paper, we first integrate buoyancy and hydrodynamics models into a modern Reinforcement Learning framework to reduce training time. Next, we show how system identification coupled with domain randomization improves the RL agent performance and narrows the sim-to-real gap. Real-world experiments for the task of capturing floating waste show that our approach lowers energy consumption by 13.1% while reducing task completion time by 7.4%. These findings, supported by sharing our open-source implementation, hold the potential to impact the efficiency and versatility of ASVs, contributing to environmental conservation efforts.},
  archive   = {C_IROS},
  author    = {Luis F. W. Batista and Junghwan Ro and Antoine Richard and Pete Schroepfer and Seth Hutchinson and Cedric Pradalier},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802067},
  month     = {10},
  pages     = {1258-1264},
  title     = {A deep reinforcement learning framework and methodology for reducing the sim-to-real gap in ASV navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). This is the way: Mitigating the roll of an autonomous
uncrewed surface vessel in wavy conditions using model predictive
control *. <em>IROS</em>, 1252–1257. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Though larger vessels may be well-equipped to deal with wavy conditions, smaller vessels are often more susceptible to disturbances. This paper explores the development of a nonlinear model predictive control (NMPC) system for Uncrewed Surface Vessels (USVs) in wavy conditions to minimize average roll. The NMPC is based on a prediction method that uses information about the vessel’s dynamics and an assumed wave model. This method is able to mitigate the roll of an under-actuated USV in a variety of conditions by adjusting the weights of the cost function. The results show a reduction of 39 % of average roll with a tuned controller in conditions with 1.75-metre sinusoidal waves. A general and intuitive tuning strategy is established. This preliminary work is a proof of concept which sets the stage for the leveraging of wave prediction methodologies to perform planning and control in real time for USVs in real-world scenarios and field trials.},
  archive   = {C_IROS},
  author    = {Daniel Jenkins and Joshua A. Marshall},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801593},
  month     = {10},
  pages     = {1252-1257},
  title     = {This is the way: Mitigating the roll of an autonomous uncrewed surface vessel in wavy conditions using model predictive control *},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prediction of acoustic communication performance for AUVs
using gaussian process classification. <em>IROS</em>, 1244–1251. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cooperating autonomous underwater vehicles (AUVs) often rely on acoustic communication to coordinate their actions effectively. However, the reliability of underwater acoustic communication decreases as the communication range between vehicles increases. Consequently, teams of cooperating AUVs typically make conservative assumptions about the maximum range at which they can communicate reliably. To address this limitation, we propose a novel approach that involves learning a map representing the probability of successful communication based on the locations of the transmitting and receiving vehicles. This probabilistic communication map accounts for factors such as the range between vehicles, environmental noise, and multi-path effects at a given location. In pursuit of this goal, we investigate the application of Gaussian process binary classification to generate the desired communication map. We specialize existing results to this specific binary classification problem and explore methods to incorporate uncertainty in vehicle location into the mapping process. Furthermore, we compare the prediction performance of the probability communication map generated using binary classification with that of a signal-to-noise ratio (SNR) communication map generated using Gaussian process regression. Our approach is experimentally validated using communication and navigation data collected during trials with a pair of Virginia Tech 690 AUVs.},
  archive   = {C_IROS},
  author    = {Yifei Gao and Harun Yetkin and McMahon James and Daniel J. Stilwell},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802108},
  month     = {10},
  pages     = {1244-1251},
  title     = {Prediction of acoustic communication performance for AUVs using gaussian process classification},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SAVOR: Sonar-aided visual odometry and reconstruction for
autonomous underwater vehicles*. <em>IROS</em>, 1236–1243. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual odometry (VO) relies on sequential camera images to estimate robot motion. For underwater robots, this is often complicated by turbidity, light attenuation, and environments containing scarce or repetitive features. Even ideal imagery suffers from the issue of scale ambiguity common to all monocular VO implementations. To address these issues, we supplement a camera with a multibeam echosounder. This acoustic, time-of-flight sensor comes with its own challenges, including relatively slow and sparse measurements that can be further degraded by backscatter from suspended particulate matter as well as interfering sounds from nearby marine traffic. We propose a method for fusing only data from these two inspection sensors into a hybrid VO solution that does not rely on IMU, DVL, or any other positioning sensor. We demonstrate this method on real data collected by an autonomous underwater vehicle performing end-to-end pipeline inspection in the open ocean, where multiple passes through the same scene (i.e., the “loop closure” common to SLAM algorithms) is often time and cost prohibitive. We also show how this approach can be extended for the creation of dense point clouds that provide a colored reconstruction of the surveyed scene.},
  archive   = {C_IROS},
  author    = {Jeremy Paul Coffelt and Peter Kampmann and Bilal Wehbe},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801846},
  month     = {10},
  pages     = {1236-1243},
  title     = {SAVOR: Sonar-aided visual odometry and reconstruction for autonomous underwater vehicles*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrated 3DOF trajectory tracking control for
under-actuated marine surface vehicles by trajectory linearization.
<em>IROS</em>, 1228–1235. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An integrated 3DOF trajectory tracking control algorithm with lateral drift correction for under-actuated Marine Surface Vehicles is presented using the Multi-Nested Loop Trajectory Linearization Control architecture. The sideslip angle is used as a virtual control effector for generating a lateral hydrodynamic force to correct the lateral drift due to a skid-turn, which is intrinsic to MSVs. The nominal sideslip angle is determined based on the kinematics and dynamics of the MSV along a nominal trajectory. Simulation results for a sub-scale vessel with significant vessel parameter perturbations are presented to demonstrate effectiveness of the proposed algorithm.},
  archive   = {C_IROS},
  author    = {Miguel Sempertegui and J. Jim Zhu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802669},
  month     = {10},
  pages     = {1228-1235},
  title     = {Integrated 3DOF trajectory tracking control for under-actuated marine surface vehicles by trajectory linearization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive control barrier functions for near-structure ROV
operations. <em>IROS</em>, 1222–1227. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a novel control design focused on enhancing the operational safety and efficiency of inspection, maintenance, and repair (IMR) operations conducted by autonomous remotely operated vehicles. Specifically, we propose using a safeguarding controller based on an adaptive control barrier function (CBF) incorporating safety properties previously identified in the literature. This approach permits temporary safe set violations, using an integrative penalty term to strengthen safety measures when necessary. A nominal nonlinear controller inside the safety bounds is proposed to ensure good reference tracking. The proposed controller is demonstrated in a simulation case study where an ROV (remotely operated vehicle) is set to clean an offshore monopile. The simulation includes unknown time-varying and step-like disturbances caused by water waves, the tether, ocean currents, and the high-pressure water jet. The proposed control law can react to the disturbances, and the ROV never leaves the defined safe set.},
  archive   = {C_IROS},
  author    = {Malte von Benzon and Mathias Marley and Fredrik Sørensen and Jesper Liniger and Simon Pedersen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801882},
  month     = {10},
  pages     = {1222-1227},
  title     = {Adaptive control barrier functions for near-structure ROV operations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time horizon locking on unmanned surface vehicles.
<em>IROS</em>, 1214–1221. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The expanding use of automated vision, assistance systems, and augmented reality applications in marine settings calls for reliable and accurate horizon detection and locking. Traditional methods utilizing Inertial Measurement Units (IMU) or feature-based computer vision techniques often yield inconsistent results, particularly when unmanned surface vehicles or boats are subject to high-speed movement or choppy waters. Addressing this, our work introduces a computer vision (CV)-based solution for real-time horizon locking. Employing real-time semantic segmentation, we accurately differentiate between sky, land or water in the frame, enabling computational locking of the horizon’s position. This stable visual reference significantly improves the performance and reliability of on-board systems for autonomous navigation, augmented reality overlays, and multi-object tracking. Supported by a dataset collected under various marine conditions, our method has proven to achieve high accuracy with low computational latency, making it a promising avenue for wide-scale implementation on automated and semi-automated systems.},
  archive   = {C_IROS},
  author    = {Benjamin Kiefer and Andreas Zell},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801836},
  month     = {10},
  pages     = {1214-1221},
  title     = {Real-time horizon locking on unmanned surface vehicles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient feature mapping using a collaborative team of
AUVs. <em>IROS</em>, 1206–1213. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present the results of experiments performed using a team of small autonomous underwater vehicles (AUVs) to determine the location of an isobath. The primary contributions of this work are (1) the development of a novel objective function for level set estimation that utilizes a rigorous assessment of uncertainty, and (2) a description of the practical challenges and corresponding solutions needed to implement our approach in the field using a team of AUVs. We combine path planning techniques and an approach to decentralization from prior work that yields theoretical performance guarantees. Experimentation with a team of AUVs provides empirical evidence that the desirable performance guarantees can be preserved in practice even in the presence of limitations that commonly arise in underwater robotics, including slow and intermittent acoustic communications and limited computational resources.},
  archive   = {C_IROS},
  author    = {Benjamin Biggs and Daniel J. Stilwell and Harun Yetkin and James McMahon},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802456},
  month     = {10},
  pages     = {1206-1213},
  title     = {Efficient feature mapping using a collaborative team of AUVs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards a factor graph-based method using angular rates for
full magnetometer calibration and gyroscope bias estimation.
<em>IROS</em>, 1199–1205. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {MEMS Attitude Heading Reference Systems are widely employed to determine a system’s attitude, but sensor measurement biases limit their accuracy. This paper introduces a novel factor graph-based method called MAgnetometer and GYroscope Calibration (MAGYC). MAGYC leverages three-axis angular rate measurements from an angular rate gyroscope to enhance calibration for batch and online applications. Our approach imposes less restrictive conditions for instrument movements required for calibration, eliminates the need for knowledge of the local magnetic field or instrument attitude, and facilitates integration into factor graph algorithms within Smoothing and Mapping frameworks. We evaluate the proposed methods through numerical simulations and in-field experimental assessments using a sensor installed on an underwater vehicle. Ultimately, our proposed methods reduced the underwater vehicle’s heading error standard deviation from 6.21° to 0.57° for a standard seafloor mapping survey.},
  archive   = {C_IROS},
  author    = {Sebastián Rodríguez-Martínez and Giancarlo Troni},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801438},
  month     = {10},
  pages     = {1199-1205},
  title     = {Towards a factor graph-based method using angular rates for full magnetometer calibration and gyroscope bias estimation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TURTLMap: Real-time localization and dense mapping of
low-texture underwater environments with a low-cost unmanned underwater
vehicle. <em>IROS</em>, 1191–1198. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Significant work has been done on advancing localization and mapping in underwater environments. Still, state-of-the-art methods are challenged by low-texture environments, which is common for underwater settings. This makes it difficult to use existing methods in diverse, real-world scenes. In this paper, we present TURTLMap, a novel solution that focuses on textureless underwater environments through a real-time localization and mapping method. We show that this method is low-cost, and capable of tracking the robot accurately, while constructing a dense map of a low-textured environment in real-time. We evaluate the proposed method using real-world data collected in an indoor water tank with a motion capture system and ground truth reference map. Qualitative and quantitative results validate the proposed system achieves accurate and robust localization and precise dense mapping, even when subject to wave conditions. The project page for TURTLMap is https://umfieldrobotics.github.io/TURTLMap.},
  archive   = {C_IROS},
  author    = {Jingyu Song and Onur Bagoren and Razan Andigani and Advaith Sethuraman and Katherine A. Skinner},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801692},
  month     = {10},
  pages     = {1191-1198},
  title     = {TURTLMap: Real-time localization and dense mapping of low-texture underwater environments with a low-cost unmanned underwater vehicle},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IMU-based monitoring of buoy-ballast system through cable
dynamics simulation. <em>IROS</em>, 1185–1190. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study is twofold. First, a comprehensive simulation framework of cable dynamics is introduced. This framework considers variable length cables and allows to incorporate elements such as buoys, ballast or Inertial Measurement Unit (IMU) sensors. The accuracy of this framework is assessed through experimental data. Second, a novel and improved solution for the instrumentation of a V-shaped buoy-ballast system using IMU sensors is investigated. This latter, designed for a neutrally buoyant tether between a Remotely Operated Vehicle (ROV) and an Unmanned Surface Vehicle (USV), is meant to improve operation safety. The discussed IMU-based solution provides wider information on the interaction between the ROV and the cable, including its 3D orientation and curvature amplitude, which could be used for both the control of the USV trajectory and its onboard winch.},
  archive   = {C_IROS},
  author    = {Charly Peraud and Martin Filliung and Cédric Anthierens and Claire Dune and Nicolas Boizot and Vincent Hugel},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802858},
  month     = {10},
  pages     = {1185-1190},
  title     = {IMU-based monitoring of buoy-ballast system through cable dynamics simulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Development of contextual collision risk framework for
operational envelope of autonomous navigation system. <em>IROS</em>,
1177–1184. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces the ‘Contextual Collision Risk (CCR)’ method, a novel collision risk assessment approach for autonomous navigation systems. As global shipping traffic increases, marine traffic becomes more complex and busier, requiring advanced risk assessment methods for autonomous vessels. The CCR establishes the Operational Envelope (OE) of autonomous navigation systems by integrating the complexities of real-time marine traffic and the specific maneuvering capabilities of own ship. The central aspect of CCR involves setting up Reachable Velocities (RV), which includes the dynamics of own ship. Additionally, the Velocity Obstacle (VO) algorithm is implemented to identify potential collision risks from other vessels. By integrating RV and VO analyses, CCR provides a framework that effectively quantifies collision risk in congested maritime environments. To demonstrate the effects of CCR, we employ data from transoceanic voyages across various scenarios. Furthermore, simulations of collision avoidance maneuvers are conducted, focusing on high-risk situations such as near-misses and potential collisions. In particular, the analysis of transoceanic voyage data shows that CCR holds promise for enhancing navigation safety in complex maritime environments.},
  archive   = {C_IROS},
  author    = {Inbeom Kim and Kwangsung Ko and Jinmo Park},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801615},
  month     = {10},
  pages     = {1177-1184},
  title     = {Development of contextual collision risk framework for operational envelope of autonomous navigation system},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Opti-acoustic semantic SLAM with unknown objects in
underwater environments. <em>IROS</em>, 1169–1176. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite recent advances in semantic Simultaneous Localization and Mapping (SLAM) for terrestrial and aerial applications, underwater semantic SLAM remains an open and largely unaddressed research problem due to the unique sensing modalities and the object classes found underwater. This paper presents an object-based semantic SLAM method for underwater environments that can identify, localize, classify, and map a wide variety of marine objects without a priori knowledge of the object classes present in the scene. The method performs unsupervised object segmentation and object-level feature aggregation, and then uses opti-acoustic sensor fusion for object localization. Probabilistic data association is used to determine observation to landmark correspondences. Given such correspondences, the method then jointly optimizes landmark and vehicle position estimates. Indoor and outdoor underwater datasets with a wide variety of objects and challenging acoustic and lighting conditions are collected for evaluation and made publicly available. Quantitative and qualitative results show the proposed method achieves reduced trajectory error compared to baseline methods, and is able to obtain comparable map accuracy to a baseline closed-set method that requires hand-labeled data of all objects in the scene.},
  archive   = {C_IROS},
  author    = {Kurran Singh and Jungseok Hong and Nicholas R. Rypkema and John J. Leonard},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802819},
  month     = {10},
  pages     = {1169-1176},
  title     = {Opti-acoustic semantic SLAM with unknown objects in underwater environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decentralized linear convoying for underactuated surface
craft with partial state coupling. <em>IROS</em>, 1161–1168. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work introduces a novel decentralized algorithm and control law for stable linear convoying using a layered control approach. This algorithm was implemented in MOOS-IvP, using an abstraction layer that models a virtual system decoupled from the system’s actual dynamics. This makes the algorithm platform-agnostic and able to be combined with other behaviors such as collision avoidance and operating region behaviors. A trajectory defined by a lead agent is discretized and embedded with the leader’s dynamics and propagated to all following agents. We first demonstrate that this approach when paired with a simple PD controller prevents accumulated errors and improves trajectory tracking for follower agents. Thereafter we demonstrate how virtually coupling a subset of agent states improves the overall cohesiveness of the convoy. Improvements are demonstrated in both simulations and field trials using five autonomous surface vehicles.},
  archive   = {C_IROS},
  author    = {Raymond Turrisi and Michael Benjamin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802418},
  month     = {10},
  pages     = {1161-1168},
  title     = {Decentralized linear convoying for underactuated surface craft with partial state coupling},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Text2Map: From navigational instructions to graph-based
indoor map representations using LLMs. <em>IROS</em>, 1153–1160. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In spatial navigation, the shift from manual cartography to digital map representations has revolutionized how we interact with and comprehend outdoor and indoor environments. While digital mapping has substantially advanced outdoor navigation with robust techniques like satellite imagery and sophisticated data labeling, the full potential of indoor digital mapping remains untapped. Accurate indoor mapping promises to enhance the operational efficiency of mobile robots, improving their ability to interact with human environments, and bolstering emergency response capabilities. However, its realization is impeded by the complexity of current methods and the need for heavy manual labor, expert knowledge, and specialized equipment. To address these challenges, we introduce Text2Map – a novel methodology that harnesses natural language navigational instructions, the power of off-the-shelf Large Language Models (LLMs), and Few-shot Learning, to create graph-based digital maps of indoor spaces. This approach simplifies the mapping process for widespread use, leveraging crowd-sourceable ubiquitous navigation instructions as a data source without requiring specialized map data formats or hardware. Our paper presents the Text2Map system architecture, details the creation of the first dedicated dataset, and evaluates the system’s efficacy, highlighting the substantial potential and scalability of our approach. Text2Map achieves a Graph-Edit-Distance (GED) ranging from 0.5X to 2X the total number of regions in a building and an Edge Similarity score between 0.87 and 0.9. These results highlight the precision, robustness, and effectiveness of our methodology. Our work paves the way for a more accessible and streamlined approach to indoor digital mapping, setting the stage for broader adoption in human and mobile robot navigation applications.},
  archive   = {C_IROS},
  author    = {Ammar Karkour and Khaled A. Harras and Eduardo Feo-Flushing},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802370},
  month     = {10},
  pages     = {1153-1160},
  title     = {Text2Map: From navigational instructions to graph-based indoor map representations using LLMs},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OpenOcc: Open vocabulary 3D scene reconstruction via
occupancy representation. <em>IROS</em>, 1145–1152. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D reconstruction has been widely used in autonomous navigation fields of mobile robotics. However, the former research can only provide the basic geometry structure without the capability of open-world scene understanding, limiting advanced tasks like human interaction and visual navigation. Moreover, traditional 3D scene understanding approaches rely on expensive labeled 3D datasets to train a model for a single task with supervision. Thus, geometric reconstruction with zero-shot scene understanding i.e. Open vocabulary 3D Understanding and Reconstruction, is crucial for the future development of mobile robots. In this paper, we propose OpenOcc, a novel framework unifying the 3D scene reconstruction and open vocabulary understanding with neural radiance fields. We model the geometric structure of the scene with occupancy representation and distill the pre-trained open vocabulary model into a 3D language field via volume rendering for zero-shot inference. Furthermore, a novel semantic-aware confidence propagation (SCP) method has been proposed to relieve the issue of language field representation degeneracy caused by inconsistent measurements in distilled features. Experimental results show that our approach achieves competitive performance in 3D scene understanding tasks, especially for small and long-tail objects.},
  archive   = {C_IROS},
  author    = {Haochen Jiang and Yueming Xu and Yihan Zeng and Hang Xu and Wei Zhang and Jianfeng Feng and Li Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801860},
  month     = {10},
  pages     = {1145-1152},
  title     = {OpenOcc: Open vocabulary 3D scene reconstruction via occupancy representation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FRAGG-map: Frustum accelerated GPU-based grid map.
<em>IROS</em>, 1138–1144. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In robotics, occupancy grids serve as required repositories of information about the environment in numerous applications. One such critical application is Simultaneous Localization and Mapping (SLAM), where robots dynamically scan and explore their surroundings while in motion. In the context of extended-duration missions, it becomes imperative to confront the complexities linked to the expansion of occupancy grids as well as handling loop closure detection. These challenges primarily revolve around two key aspects: enabling the seamless expansion of the map on multiple occasions, thus avoiding the need to map smaller regions in numerous separate missions, and ensuring real-time updates to the map to sustain the robot’s knowledge base and enhance its responsiveness. To address these challenges, we introduce an innovative map called Frustum Accelerated GPU-Based Grid Map (FRAGG-Map). This map adopts a highly parallelizable 3D grid structure and leverages the power of CUDA kernels to facilitate efficient insertion of point-clouds and enables real-time updates of the map. FRAGG-Map identifies the portions of the map that require updates and utilises the GPU to update them, significantly enhancing computational performance. Our results show that FRAGG-Map can run 31 times faster than OctoMap, significantly outperforming state-of-the-art methods.},
  archive   = {C_IROS},
  author    = {Michele Grimaldi and Narcis Palomeras and Ignacio Carlucho and Yvan R. Petillot and Pere Ridao Rodriguez},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801590},
  month     = {10},
  pages     = {1138-1144},
  title     = {FRAGG-map: Frustum accelerated GPU-based grid map},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ν-DBA: Neural implicit dense bundle adjustment enables
image-only driving scene reconstruction. <em>IROS</em>, 1130–1137. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The joint optimization of the sensor trajectory and 3D map is a crucial characteristic of bundle adjustment (BA), essential for autonomous driving. This paper presents ν-DBA, a novel framework implementing geometric dense bundle adjustment (DBA) using 3D neural implicit surfaces for map parametrization, which optimizes both the map surface and trajectory poses using geometric error guided by dense optical flow prediction. Additionally, we fine-tune the optical flow model with per-scene self-supervision to further improve the quality of the dense mapping. Our experimental results on multiple driving scene datasets demonstrate that our method achieves superior trajectory optimization and dense reconstruction accuracy. We also investigate the influences of photometric error and different neural geometric priors on the performance of surface reconstruction and novel view synthesis. Our method stands as a significant step towards leveraging neural implicit representations in dense bundle adjustment for more accurate trajectories and detailed environmental mapping.},
  archive   = {C_IROS},
  author    = {Yunxuan Mao and Bingqi Shen and Yifei Yang and Kai Wang and Rong Xiong and Yiyi Liao and Yue Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801847},
  month     = {10},
  pages     = {1130-1137},
  title     = {ν-DBA: Neural implicit dense bundle adjustment enables image-only driving scene reconstruction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PSS-BA: LiDAR bundle adjustment with progressive spatial
smoothing. <em>IROS</em>, 1124–1129. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate and consistent construction of point clouds from LiDAR scanning data is fundamental for 3D modeling applications. Current solutions, such as multiview point cloud registration and LiDAR bundle adjustment, predominantly depend on the local plane assumption, which may be inadequate in complex environments lacking of planar geometries or substantial initial pose errors. To mitigate this problem, this paper presents a LiDAR bundle adjustment with progressive spatial smoothing, which is suitable for complex environments and exhibits improved convergence capabilities. The proposed method consists of a spatial smoothing module and a pose adjustment module, which combines the benefits of local consistency and global accuracy. With the spatial smoothing module, we can obtain robust and rich surface constraints employing smoothing kernels across various scales. Then the pose adjustment module corrects all poses utilizing the novel surface constraints. Ultimately, the proposed method simultaneously achieves fine poses and parametric surfaces that can be directly employed for high-quality point cloud reconstruction. The effectiveness and robustness of our proposed approach have been validated on both simulation and real-world datasets. The experimental results demonstrate that the proposed method outperforms the existing methods and achieves better accuracy in complex environments with low planar structures.},
  archive   = {C_IROS},
  author    = {Jianping Li and Thien-Minh Nguyen and Shenghai Yuan and Lihua Xie},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802614},
  month     = {10},
  pages     = {1124-1129},
  title     = {PSS-BA: LiDAR bundle adjustment with progressive spatial smoothing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). A novel framework for structure descriptors-guided
hand-drawn floor plan reconstruction. <em>IROS</em>, 1116–1123. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the absence of a pre-built indoor map, robot navigation suffers from the limitations of sensors and environments, resulting in decreased efficiency in performing ad-hoc tasks. Given that blueprints are difficult to obtain, an intuitive method is to provide robots with prior knowledge via hand-drawn floor plans. However, due to the inability of robots to directly comprehend hand-drawn styles, the applicability of this method is limited. In this paper, we present a novel framework for hand-drawn floor plan reconstruction that can recognize abstract hand-drawn elements and standardize the reconstruction of hand-drawn floor plans, thereby providing robots with valuable global map information. Specifically, we design a new series of structure descriptors as reconstruction components and employ a deep learning-based model for recognition. Then the standardized results are obtained through the proposed floor plan reconstruction algorithm. To verify the effectiveness of the framework, we conduct experiments on electronic and paper hand-drawn floor plans. Compared with other state-of-the-art methods, our proposed method achieves superior reconstruction results. This work expands the application scenarios for indoor robots, enabling them to quickly comprehend the semantics of complex scenes, thereby enhancing the competitiveness in downstream tasks.},
  archive   = {C_IROS},
  author    = {Zhentong Zhang and Juan Liu and Xinde Li and ChuanFei Hu and Fir Dunkin and Shaokun Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802609},
  month     = {10},
  pages     = {1116-1123},
  title     = {A novel framework for structure descriptors-guided hand-drawn floor plan reconstruction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RMap: Millimeter-wave radar mapping through volumetric
upsampling. <em>IROS</em>, 1108–1115. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Millimeter Wave Radar is being adopted as a viable alternative to lidar and radar in adverse visually degraded conditions, such as in the presence of fog and dust. However, this sensor modality suffers from severe sparsity and noise under nominal conditions, which makes it difficult to use in precise applications such as mapping. This work presents a novel solution to generate accurate 3D maps from sparse radar point clouds. RMap uses a generative transformer architecture which upsamples, denoises, and fills the incomplete radar maps to resemble lidar maps. We test this method on the ColoRadar dataset to demonstrate its efficacy.},
  archive   = {C_IROS},
  author    = {Ajay Narasimha Mopidevi and Kyle Harlow and Christoffer Heckman},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801827},
  month     = {10},
  pages     = {1108-1115},
  title     = {RMap: Millimeter-wave radar mapping through volumetric upsampling},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DHP-mapping: A dense panoptic mapping system with
hierarchical world representation and label optimization techniques.
<em>IROS</em>, 1101–1107. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Maps provide robots with crucial environmental knowledge, thereby enabling them to perform interactive tasks effectively. Easily accessing accurate abstract-to-detailed geometric and semantic concepts from maps is crucial for robots to make informed and efficient decisions. To comprehensively model the environment and effectively manage the map data structure, we propose DHP-Mapping, a dense mapping system that utilizes multiple Truncated Signed Distance Field (TSDF) submaps and panoptic labels to hierarchically model the environment. The output map is able to maintain both voxel- and submap-level metric and semantic information. Two modules are presented to enhance the mapping efficiency and label consistency: (1) an inter-submaps label fusion strategy to eliminate duplicate points across submaps and (2) a conditional random field (CRF) based approach to enhance panoptic labels. We conducted experiments with two public datasets including indoor and outdoor scenarios. Our system performs comparably to state-of-the-art (SOTA) methods across geometry and label accuracy evaluation metrics. The experiment results highlight the effectiveness and scalability of our system, as it is capable of constructing precise geometry and maintaining consistent panoptic labels. Our code is publicly available at https://github.com/hutslib/DHP-Mapping.},
  archive   = {C_IROS},
  author    = {Tianshuai Hu and Jianhao Jiao and Yucheng Xu and Hongji Liu and Sheng Wang and Ming Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802278},
  month     = {10},
  pages     = {1101-1107},
  title     = {DHP-mapping: A dense panoptic mapping system with hierarchical world representation and label optimization techniques},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Teaching robots where to go and how to act with human
sketches via spatial diagrammatic instructions. <em>IROS</em>,
1094–1100. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces Spatial Diagrammatic Instructions (SDIs), an approach for human operators to specify objectives and constraints that are related to spatial regions in the working environment. Human operators are enabled to sketch out regions directly on camera images that correspond to the objectives and constraints. These sketches are projected to 3D spatial coordinates, and continuous Spatial Instruction Maps (SIMs) are learned upon them. These maps can then be integrated into optimization problems for tasks of robots. In particular, we demonstrate how Spatial Diagrammatic Instructions can be applied to solve the Base Placement Problem of mobile manipulators, which concerns the best place to put the manipulator to facilitate a certain task. Human operators can specify, via sketch, spatial regions of interest for a manipulation task and permissible regions for the mobile manipulator to be at. Then, an optimization problem that maximizes the manipulator’s reachability, or coverage, over the designated regions of interest while remaining in the permissible regions is solved. We provide extensive empirical evaluations, and show that our formulation of Spatial Instruction Maps provides accurate representations of user-specified diagrammatic instructions. Furthermore, we demonstrate that our diagrammatic approach to the Mobile Base Placement Problem enables higher quality solutions and faster runtime.},
  archive   = {C_IROS},
  author    = {Qilin Sun and Weiming Zhi and Tianyi Zhang and Matthew Johnson-Roberson},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801371},
  month     = {10},
  pages     = {1094-1100},
  title     = {Teaching robots where to go and how to act with human sketches via spatial diagrammatic instructions},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing online road network perception and reasoning with
standard definition maps. <em>IROS</em>, 1086–1093. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous driving for urban and highway driving applications often requires High Definition (HD) maps to generate a navigation plan. Nevertheless, various challenges arise when generating and maintaining HD maps at scale. While recent online mapping methods have started to emerge, their performance especially for longer ranges is limited by heavy occlusion in dynamic environments. With these considerations in mind, our work focuses on leveraging lightweight and scalable priors–Standard Definition (SD) maps–in the development of online vectorized HD map representations. We first examine the integration of prototypical rasterized SD map representations into various online mapping architectures. Furthermore, to identify lightweight strategies, we extend the OpenLane-V2 dataset with OpenStreetMaps and evaluate the benefits of graphical SD map representations. A key finding from designing SD map integration components is that SD map encoders are model agnostic and can be quickly adapted to new architectures that utilize bird’s eye view (BEV) encoders. Our results show that making use of SD maps as priors for the online mapping task can significantly speed up convergence and boost the performance of the online centerline perception task by 30% (mAP). Furthermore, we show that the introduction of the SD maps leads to a reduction of the number of parameters in the perception and reasoning task by leveraging SD map graphs while improving the overall performance. Project Page: https://henryzhangzhy.github.io/sdhdmap/.},
  archive   = {C_IROS},
  author    = {Hengyuan Zhang and David Paz and Yuliang Guo and Arun Das and Xinyu Huang and Karsten Haug and Henrik I. Christensen and Liu Ren},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801423},
  month     = {10},
  pages     = {1086-1093},
  title     = {Enhancing online road network perception and reasoning with standard definition maps},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). V-PRISM: Probabilistic mapping of unknown tabletop scenes.
<em>IROS</em>, 1078–1085. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to construct concise scene representations from sensor input is central to the field of robotics. This paper addresses the problem of robustly creating a 3D representation of a tabletop scene from a segmented RGBD image. These representations are then critical for a range of downstream manipulation tasks. Many previous attempts to tackle this problem do not capture accurate uncertainty, which is required to subsequently produce safe motion plans. In this paper, we cast the representation of 3D tabletop scenes as a multi-class classification problem. To tackle this, we introduce V-PRISM, a framework and method for robustly creating probabilistic 3D segmentation maps of tabletop scenes. Our maps contain both occupancy estimates, segmentation information, and principled uncertainty measures. We evaluate the robustness of our method in (1) procedurally generated scenes using open-source object datasets, and (2) real-world tabletop data collected from a depth camera. Our experiments show that our approach outperforms alternative continuous reconstruction approaches that do not explicitly reason about objects in a multi-class formulation.},
  archive   = {C_IROS},
  author    = {Herbert Wright and Weiming Zhi and Matthew Johnson-Roberson and Tucker Hermans},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802358},
  month     = {10},
  pages     = {1078-1085},
  title     = {V-PRISM: Probabilistic mapping of unknown tabletop scenes},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the 3D trochoidal motion model of LiDAR sensors placed
off-centered inside spherical mobile mapping systems. <em>IROS</em>,
1070–1077. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the motion model of a sensor rigidly mounted inside a ball. Due to the rigid placement inside the ball, the geometry of the sensor trajectory resembles a 3D curate trochoid. A new calibration method for spherical systems estimates the extrinsic parameters of the sensor with respect to the balls center of rotation. We deploy the calibration and motion model on our spherical mobile mapping platform to estimate the trajectory of a LiDAR sensor and compare it to trajectories of state-of-the-art LiDAR-Inertial odometry (LIO) methods. The motion model, which is solely based on IMU measurements, produces comparable results to the LIO methods, sometimes even outperforming them in positional accuracy. Although the LIO methods provide better rotational accuracy due to the utilization of LiDAR data, they struggle to reproduce the trochoidal nature of the trajectory and only provide pose estimations at the LiDAR frequency, whereas the motion model produces a more consistent trochoidal trajectory at the much higher IMU frequency. The results demonstrate the difficulty that current LIO methods have on spherical systems and indicate that our motion model is suitable for overcoming these issues.},
  archive   = {C_IROS},
  author    = {Fabian Arzberger and Andreas Nüchter},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801675},
  month     = {10},
  pages     = {1070-1077},
  title     = {On the 3D trochoidal motion model of LiDAR sensors placed off-centered inside spherical mobile mapping systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural semantic map-learning for autonomous vehicles.
<em>IROS</em>, 1062–1069. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous vehicles demand detailed maps to maneuver reliably through traffic, which need to be kept up-to-date to ensure a safe operation. A promising way to adapt the maps to the ever-changing road-network is to use crowd-sourced data from a fleet of vehicles. In this work, we present a mapping system that fuses local submaps gathered from a fleet of vehicles at a central instance to produce a coherent map of the road environment including drivable area, lane markings, poles, obstacles and more as a 3D mesh. Each vehicle contributes locally reconstructed submaps as lightweight meshes, making our method applicable to a wide range of reconstruction methods and sensor modalities. Our method jointly aligns and merges the noisy and incomplete local submaps using a scene-specific Neural Signed Distance Field, which is supervised using the submap meshes to predict a fused environment representation. We leverage memory-efficient sparse feature-grids to scale to large areas and introduce a confidence score to model uncertainty in scene reconstruction. Our approach is evaluated on two datasets with different local mapping methods, showing improved pose alignment and reconstruction over existing methods. Additionally, we demonstrate the benefit of multi-session mapping and examine the required amount of data to enable high-fidelity map learning for autonomous vehicles.},
  archive   = {C_IROS},
  author    = {Markus Herb and Nassir Navab and Federico Tombari},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802211},
  month     = {10},
  pages     = {1062-1069},
  title     = {Neural semantic map-learning for autonomous vehicles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). CSR: A lightweight crowdsourced road structure
reconstruction system for autonomous driving. <em>IROS</em>, 1054–1061.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Highly accurate and robust vectorized reconstruction of road structures is crucial for autonomous vehicles. Traditional LiDAR-based methods require multiple processes and are often expensive, time-consuming, labor-intensive, and cumbersome. In this paper, we propose a lightweight crowdsourced road structure reconstruction system (termed CSR) that relies solely on online perceived semantic elements. Ambiguities and perceptual errors of semantic features and Global Navigation Satellite System (GNSS) global pose errors constitute the predominant challenge in achieving alignment across multi-trip data. To this end, a robust two-phased coarse-to-fine multi-trip alignment method is performed considering local geometric consistency, global topology consistency, intra-trip temporal consistency, and inter-trip consistency. Further, we introduce an incremental pose graph optimization framework with adaptive weight tuning ability to integrate pre-built road structures, currently perceived multi-trip semantic features, odometry, and GNSS, enabling accurate and robust incremental road structure reconstruction. CSR is highly automated, efficient, and scalable for large-scale autonomous driving scenarios, significantly expediting road structure production. We quantitatively and qualitatively validate the reconstruction performance of CSR in real-world scenes. CSR achieves centimeter-level accuracy commensurate with established LiDAR-based methods, concurrently boosting efficiency and reducing resource expenditure.},
  archive   = {C_IROS},
  author    = {Huayou Wang and Qingyao Liu and Jiazheng Wu and Kun Liu and Chao Ding and Xianpeng Lang and Changliang Xue},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801299},
  month     = {10},
  pages     = {1054-1061},
  title     = {CSR: A lightweight crowdsourced road structure reconstruction system for autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Thermal-NeRF: Neural radiance fields from an infrared
camera. <em>IROS</em>, 1046–1053. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, Neural Radiance Fields (NeRFs) have demonstrated significant potential in encoding highly-detailed 3D geometry and environmental appearance, positioning themselves as a promising alternative to traditional explicit representation for 3D scene reconstruction. However, the predominant reliance on RGB imaging presupposes ideal lighting conditions—a premise frequently unmet in robotic applications plagued by poor lighting or visual obstructions. This limitation overlooks the capabilities of infrared (IR) cameras, which excel in low-light detection and present a robust alternative under such adverse scenarios. To tackle these issues, we introduce Thermal-NeRF, the first method that estimates a volumetric scene representation in the form of a NeRF solely from IR imaging. By leveraging a thermal mapping and structural thermal constraint derived from the thermal characteristics of IR imaging, our method showcases unparalleled proficiency in recovering NeRFs in visually degraded scenes where RGB-based methods fall short. We conduct extensive experiments to demonstrate that Thermal-NeRF can achieve superior quality compared to existing methods. Furthermore, we contribute a dataset for IR-based NeRF applications, paving the way for future research in IR NeRF reconstruction, see https://github.com/Cerf-Volant425/Thermal-NeRF.},
  archive   = {C_IROS},
  author    = {Tianxiang Ye and Qi Wu and Junyuan Deng and Guoqing Liu and Liu Liu and Songpengcheng Xia and Liang Pang and Wenxian Yu and Ling Pei},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802480},
  month     = {10},
  pages     = {1046-1053},
  title     = {Thermal-NeRF: Neural radiance fields from an infrared camera},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Geometry-aided underwater 3D mapping using side-scan sonar.
<em>IROS</em>, 1038–1045. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, the interest in underwater exploration with Autonomous Underwater Vehicles (AUVs) equipped with side-scan sonars (SSS) has grown considerably. However, state-of-the-art SSS Simultaneous Localization and Mapping (SLAM) systems encounter challenges in data association across large viewpoint changes. Additionally, these systems assume that the seabed is a flat surface, leading to significant mapping error in uneven underwater terrains. To address these challenges, we propose a framework that leverages the side-scan sonar geometry to facilitate data association and improve mapping accuracy. The framework begins with a preprocessing module that extracts feature points and provides initial estimates of the elevation angles of the landmarks. Then, a non-consecutive data association module applies epipolar line search to establish correspondences between the current and historical frames. Finally, the mapping module uses side-scan sonar bundle adjustment to recover the positions of the landmarks. The proposed method is evaluated using an underwater terraced fields dataset. Our method achieves over 90% matching rate and reduces the average mapping error from 3.799 to 0.134.},
  archive   = {C_IROS},
  author    = {Yiqiao Yang and Chenglin Pang and Chengdong Wu and Zheng Fang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801520},
  month     = {10},
  pages     = {1038-1045},
  title     = {Geometry-aided underwater 3D mapping using side-scan sonar},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Active vehicle re-localization based on non-repetitive
LiDAR with gimbal motion strategy. <em>IROS</em>, 1032–1037. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The installation of a multi-layer 3D LiDAR atop the vehicle is a widely adopted hardware configuration for map-matching-based localization in intelligent driving. By offering a comprehensive 360° horizontal Field of View (FoV), this setup aims to achieve precise matching outcomes through the imposition of substantial geometric constraints against dynamic interferences and structural degradation. However, several factors limit its environmental adaptability, such as sparse point cloud density at distances, insufficient maximum sensing range, and notably, the restricted beam elevation angle, limiting the perception of the environment beyond obstacles. The rapid advancement of non-repetitive scanning LiDARs shows promise in mitigating such limitations. Nevertheless, their narrow FoV remains a challenge to overcome. In this study, we propose a solution by mounting such one single LiDAR on a two-axis rotating gimbal, enabling the vehicle to surpass the ranges and vertical FoV limitations of traditional setups actively. The corresponding gimbal motion strategy has been designed to automatically focus on the environment component with the most robust geometric constraints. Experimental results validate that the proposed method achieves superior robustness under high dynamic interference while delivering sufficient performance under standard conditions.},
  archive   = {C_IROS},
  author    = {Xin’Ao Wu and Chenxi Yang and Yiyang Guo and Hanyang Zhuang and Chunxiang Wang and Ming Yang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802042},
  month     = {10},
  pages     = {1032-1037},
  title     = {Active vehicle re-localization based on non-repetitive LiDAR with gimbal motion strategy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adversarial attack on trajectory prediction for autonomous
vehicles with generative adversarial networks. <em>IROS</em>, 1026–1031.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate trajectory prediction is crucial for autonomous vehicles to realize safe driving. Current trajectory prediction approaches generally rely on deep neural networks, which are susceptible to adversarial attacks. To evaluate the adversarial robustness and security of deep-learning-based trajectory prediction models, this paper proposes an adversarial attack method on trajectory prediction using generative adversarial networks (GANs). First, a novel LSTM-based attack trajectory model named Adv-GAN is proposed considering both the temporal and spatial driving features. The networks in Adv-GAN are trained through game learning between the generator and the discriminator to obtain the adversarial trajectories with real driving feature distribution. Furthermore, the generated trajectory is optimized with the vehicle kinematics model for driving feasibility on roads. The derived adversarial attack can lead to considerable deviations in trajectory prediction which affects driving safety for autonomous vehicles. We evaluate the proposed Adv-GAN on three public datasets, and experimental results show the effectiveness with better attack performance compared to a state-of-the-art adversarial attack model.},
  archive   = {C_IROS},
  author    = {Jiping Fan and Zhenpo Wang and Guoqiang Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802234},
  month     = {10},
  pages     = {1026-1031},
  title     = {Adversarial attack on trajectory prediction for autonomous vehicles with generative adversarial networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-uncertainty aware autonomous cooperative planning.
<em>IROS</em>, 1018–1025. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous cooperative planning (ACP) is a promising technique to improve the efficiency and safety of multi-vehicle interactions for future intelligent transportation systems. However, realizing robust ACP is a challenge due to the aggregation of perception, motion, and communication uncertainties. This paper proposes a novel multi-uncertainty aware ACP (MUACP) framework that simultaneously accounts for multiple types of uncertainties via regularized cooperative model predictive control (RC-MPC). The regularizers and constraints for perception, motion, and communication are constructed according to the confidence levels, weather conditions, and outage probabilities, respectively. The effectiveness of the proposed method is evaluated in the Car Learning to Act (CARLA) simulation platform. Results demonstrate that the proposed MUACP efficiently performs cooperative formation in real time and outperforms other benchmark approaches in various scenarios under imperfect knowledge of the environment.},
  archive   = {C_IROS},
  author    = {Shiyao Zhang and He Li and Shengyu Zhang and Shuai Wang and Derrick Wing Kwan Ng and Chengzhong Xu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802006},
  month     = {10},
  pages     = {1018-1025},
  title     = {Multi-uncertainty aware autonomous cooperative planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). HeteroLight: A general and efficient learning approach for
heterogeneous traffic signal control. <em>IROS</em>, 1010–1017. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Efficient and scalable adaptive traffic signal control is crucial in reducing congestion, maximizing through-put, and improving mobility experience in ever-expanding cities. Recent advances in multi-agent reinforcement learning (MARL) with parameter sharing have significantly improved the adaptive optimization of large-scale, complex, and dynamic traffic flows. However, the limited model representation capability due to shared parameters impedes the learning of diverse control strategies for intersections with different flows/topologies, posing significant challenges to achieving effective signal control in complex and varied real-world traffic scenarios. To address these challenges, we present a novel MARL-based general traffic signal control framework, called HeteroLight. Specifically, we first introduce a General Feature Extraction (GFE) module, crafted in a decoder-only fashion, where we employ an attention mechanism to facilitate efficient and flexible extraction of traffic dynamics at intersections with varied topologies. Additionally, we incorporate an Intersection Specifics Extraction (ISE) module, designed to identify key latent vectors that represent the unique intersection’s topology and traffic dynamics through variational inference techniques. By integrating the learned intersection-specific information into policy learning, we enhance the parameter-sharing mechanism, improving the model’s representation diversity among different agents and enabling the learning of a more efficient shared control strategy. Through comprehensive evaluations against other state-of-the-art traffic signal control methods on the real-world Monaco traffic network, our empirical findings reveal that HeteroLight consistently outperforms other methods across various evaluation metrics, highlighting its superiority in optimizing traffic flows in heterogeneous traffic networks.},
  archive   = {C_IROS},
  author    = {Yifeng Zhang and Peizhuo Li and Mingfeng Fan and Guillaume Sartoretti},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801524},
  month     = {10},
  pages     = {1010-1017},
  title     = {HeteroLight: A general and efficient learning approach for heterogeneous traffic signal control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Agent-agnostic centralized training for decentralized
multi-agent cooperative driving. <em>IROS</em>, 1002–1009. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Active traffic management with autonomous vehicles offers the potential for reduced congestion and improved traffic flow. However, developing effective algorithms for real-world scenarios requires overcoming challenges related to infinite-horizon traffic flow and partial observability. To address these issues and further decentralize traffic management, we propose an asymmetric actor-critic model that learns decentralized cooperative driving policies for autonomous vehicles using single-agent reinforcement learning. By employing attention neural networks with masking, our approach efficiently manages real-world traffic dynamics and partial observability, eliminating the need for predefined agents or agent-specific experience buffers in multi-agent reinforcement learning. Extensive evaluations across various traffic scenarios demonstrate our method’s significant potential in improving traffic flow at critical bottleneck points. Moreover, we address the challenges posed by conservative autonomous vehicle driving behaviors that adhere strictly to traffic rules, showing that our cooperative policy effectively alleviates potential slowdowns without compromising safety.},
  archive   = {C_IROS},
  author    = {Shengchao Yan and Lukas König and Wolfram Burgard},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801946},
  month     = {10},
  pages     = {1002-1009},
  title     = {Agent-agnostic centralized training for decentralized multi-agent cooperative driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SmartPathfinder: Pushing the limits of heuristic solutions
for vehicle routing problem with drones using reinforcement learning.
<em>IROS</em>, 994–1001. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Vehicle Routing Problem with Drones (VRPD) seeks to optimize the routing paths for both trucks and drones, where the trucks are responsible for delivering parcels to customer locations, and the drones are dispatched from these trucks for parcel delivery, subsequently being retrieved by the trucks. Given the NP-Hard complexity of VRPD, numerous heuristic approaches have been introduced. However, improving solution quality, the definition of which can vary depending on various heuristic approaches, e.g., the total operation time, remain significant challenges. In this paper, we conduct a comprehensive examination of heuristic methods designed for solving VRPD, distilling and standardizing them into core elements. We then develop a novel reinforcement learning (RL) framework that is seamlessly integrated with the heuristic solution components, establishing a set of universal principles for incorporating the RL framework with heuristic strategies in an aim to improve both the solution quality and computation speed, regardless of how the solution quality is defined. This integration has been applied to a state-of-the-art heuristic solution for VRPD, showcasing the substantial benefits of incorporating the RL framework. Our evaluation results demonstrated that the heuristic solution incorporated with our RL framework not only elevated the quality of solutions but also achieved rapid computation speeds, especially when dealing with extensive customer locations.},
  archive   = {C_IROS},
  author    = {Navid Mohammad Imran and Myounggyu Won},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801949},
  month     = {10},
  pages     = {994-1001},
  title     = {SmartPathfinder: Pushing the limits of heuristic solutions for vehicle routing problem with drones using reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A data-informed analysis of scalable supervision for safety
in autonomous vehicle fleets. <em>IROS</em>, 986–993. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous driving is a highly anticipated approach toward eliminating roadway fatalities. At the same time, the bar for safety is both high and costly to verify. This work considers the role of remotely-located human operators supervising a fleet of autonomous vehicles (AVs) for safety. Such a ‘scalable supervision’ concept was previously proposed to bridge the gap between still-maturing autonomy technology and the pressure to begin commercial offerings of autonomous driving. The present article proposes DISCES, a framework for Data-Informed Safety-Critical Event Simulation, to investigate the practicality of this concept from a dynamic network loading standpoint. With a focus on the safety-critical context of AVs merging into mixed-autonomy traffic, vehicular arrival processes at 1,097 highway merge points are modeled using microscopic traffic reconstruction with historical data from interstates across three California counties. Combined with a queuing theoretic model, these results characterize the dynamic supervision requirements and thereby scalability of the teleoperation approach. Across all scenarios we find reductions in operator requirements greater than 99% as compared to in-vehicle supervisors for the time period analyzed. The work also demonstrates two methods for reducing these empirical supervision requirements: (i) the use of cooperative connected AVs — which are shown to produce an average 3.67 orders-of-magnitude system reliability improvement across the scenarios studied — and (ii) aggregation across larger regions.},
  archive   = {C_IROS},
  author    = {Cameron Hickert and Zhongxia Yan and Cathy Wu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801392},
  month     = {10},
  pages     = {986-993},
  title     = {A data-informed analysis of scalable supervision for safety in autonomous vehicle fleets},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large language models powered context-aware motion
prediction in autonomous driving. <em>IROS</em>, 980–985. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion prediction is among the most fundamental tasks in autonomous driving. Traditional methods of motion forecasting primarily encode vector information of maps and historical trajectory data of traffic participants, lacking a comprehensive understanding of overall traffic semantics, which in turn affects the performance of prediction tasks. In this paper, we utilized Large Language Models (LLMs) to enhance the global traffic context understanding for motion prediction tasks. We first conducted systematic prompt engineering, visualizing complex traffic environments and historical trajectory information of traffic participants into image prompts— Transportation Context Map (TC-Map), accompanied by corresponding text prompts. Through this approach, we obtained rich traffic context information from the LLM. By integrating this information into the motion prediction model, we demonstrate that such context can enhance the accuracy of motion predictions. Furthermore, considering the cost associated with LLMs, we propose a cost-effective deployment strategy: enhancing the accuracy of motion prediction tasks at scale with 0.7% LLM-augmented datasets. Our research offers valuable insights into enhancing the understanding of traffic scenes of LLMs and the motion prediction performance of autonomous driving. The source code is available at https://github.com/AIR-DISCOVER/LLM-Augmented-MTR and https://aistudio.baidu.com/projectdetail/7809548.},
  archive   = {C_IROS},
  author    = {Xiaoji Zheng and Lixiu Wu and Zhijie Yan and Yuanrong Tang and Hao Zhao and Chen Zhong and Bokui Chen and Jiangtao Gong},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802397},
  month     = {10},
  pages     = {980-985},
  title     = {Large language models powered context-aware motion prediction in autonomous driving},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning dynamics models for velocity estimation in
autonomous racing. <em>IROS</em>, 972–979. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Velocity estimation is of great importance in autonomous racing. Still, existing solutions are characterized by limited accuracy, especially in the case of aggressive driving or poor generalization to unseen road conditions. To address these issues, we propose to utilize Unscented Kalman Filter (UKF) with a learned dynamics model that is optimized directly for the state estimation task. Moreover, we propose to aid this model with the online-estimated friction coefficient, which increases the estimation accuracy and enables zero-shot adaptation to the new road conditions. To evaluate the UKF-based velocity estimator with the proposed dynamics model, we introduced a publicly available dataset of aggressive maneuvers performed by an F1TENTH car, with side-slip angles reaching 40°. Using this dataset, we show that learning the dynamics model through UKF leads to improved estimation performance and that the proposed solution outperforms state-of-the-art learning-based state estimators by 17% in the nominal scenario. Moreover, we present unseen zero-shot adaptation abilities of the proposed method to the new road surface thanks to the proposed learning-based tire dynamics model with online friction estimation.},
  archive   = {C_IROS},
  author    = {Jan Węgrzynowski and Grzegorz Czechmanowski and Piotr Kicki and Krzysztof Walas},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802481},
  month     = {10},
  pages     = {972-979},
  title     = {Learning dynamics models for velocity estimation in autonomous racing},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SurrealDriver: Designing LLM-powered generative driver agent
framework based on human drivers’ driving-thinking data. <em>IROS</em>,
966–971. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Leveraging advanced reasoning capabilities and extensive world knowledge of large language models (LLMs) to construct generative agents for solving complex real-world problems is a major trend. However, LLMs inherently lack embodiment as humans, resulting in suboptimal performance in many embodied decision-making tasks. In this paper, we introduce a framework for building human-like generative driving agents using post-driving self-report driving-thinking data from human drivers as both demonstration and feedback. To capture high-quality, natural language data from drivers, we conducted urban driving experiments, recording drivers’ verbalized thoughts under various conditions to serve as chain-of-thought prompts and demonstration examples for the LLM-Agent. The framework’s effectiveness was evaluated through simulations and human assessments. Results indicate that incorporating expert demonstration data significantly reduced collision rates by 81.04% and increased human likeness by 50% compared to a baseline LLM-based agent. Our study provides insights into using natural language-based human demonstration data for embodied tasks. The driving-thinking dataset is available at https://github.com/AIR-DISCOVER/Driving-Thinking-Dataset.},
  archive   = {C_IROS},
  author    = {Ye Jin and Ruoxuan Yang and Zhijie Yi and Xiaoxi Shen and Huiling Peng and Xiaoan Liu and Jingli Qin and Jiayang Li and Jintao Xie and Peizhong Gao and Guyue Zhou and Jiangtao Gong},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802229},
  month     = {10},
  pages     = {966-971},
  title     = {SurrealDriver: Designing LLM-powered generative driver agent framework based on human drivers’ driving-thinking data},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-agent path finding for mixed autonomy traffic
coordination. <em>IROS</em>, 958–965. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the evolving landscape of urban mobility, the prospective integration of Connected and Automated Vehicles (CAVs) with Human-Driven Vehicles (HDVs) presents a complex array of challenges and opportunities for autonomous driving systems. While recent advancements in robotics have yielded Multi-Agent Path Finding (MAPF) algorithms tailored for agent coordination task characterized by simplified kinematics and complete control over agent behaviors, these solutions are inapplicable in mixed-traffic environments where uncontrollable HDVs must coexist and interact with CAVs. Addressing this gap, we propose the Behavior Prediction Kinematic Priority Based Search (BK-PBS), which leverages an offline-trained conditional prediction model to forecast HDV responses to CAV maneuvers, integrating these insights into a Priority Based Search (PBS) where the A* search proceeds over motion primitives to accommodate kinematic constraints. We compare BK-PBS with CAV planning algorithms derived by rule-based car-following models, and reinforcement learning. Through comprehensive simulation on a highway merging scenario across diverse scenarios of CAV penetration rate and traffic density, BK-PBS outperforms these baselines in reducing collision rates and enhancing system-level travel delay. Our work is directly applicable to many scenarios of multi-human multi-robot coordination.},
  archive   = {C_IROS},
  author    = {Han Zheng and Zhongxia Yan and Cathy Wu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802051},
  month     = {10},
  pages     = {958-965},
  title     = {Multi-agent path finding for mixed autonomy traffic coordination},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Realistic rainy weather simulation for LiDARs in CARLA
simulator. <em>IROS</em>, 951–957. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Data augmentation methods to enhance perception performance in adverse weather have recently attracted considerable attention. Most of the LiDAR data augmentation methods post-process the existing dataset by physics-based models or machine-learning methods. However, due to the limited environmental annotations and the fixed vehicle trajectories in existing datasets, it is challenging to edit the scene and expand the diversity of traffic flow and scenario. To this end, we propose a simulator-based physical modeling approach to augment LiDAR data in rainy weather, enhancing the performance of the perception model. We complete the modeling task of the rainy weather effect in the CARLA simulator and establish a data collection pipeline for LiDAR. Furthermore, we pay special attention to the spray generated by vehicles in rainy weather and simulate this phenomenon through the Spray Emitter method we developed. In addition, considering the influence of different weather conditions on point cloud intensity, we develop a prediction network to forecast the intensity of the LiDAR echo. This enables us to complete the rainy weather simulation of 4D point cloud data. In the experiment, we observe that the model augmented by our synthetic dataset improves the performance for 3D object detection in rainy weather. Both code and dataset are available at https://github.com/PJLab-ADG/PCSim#rainypcsim.},
  archive   = {C_IROS},
  author    = {Donglin Yang and Xinyu Cai and Zhenfeng Liu and Wentao Jiang and Bo Zhang and Guohang Yan and Xing Gao and Si Liu and Botian Shi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802036},
  month     = {10},
  pages     = {951-957},
  title     = {Realistic rainy weather simulation for LiDARs in CARLA simulator},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep stochastic kinematic models for probabilistic motion
forecasting in traffic. <em>IROS</em>, 943–950. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In trajectory forecasting tasks for traffic, future output trajectories can be computed by advancing the ego vehicle’s state with predicted actions according to a kinematics model. By unrolling predicted trajectories via time integration and models of kinematic dynamics, predicted trajectories should not only be kinematically feasible but also relate uncertainty from one timestep to the next. While current works in probabilistic prediction do incorporate kinematic priors for mean trajectory prediction, variance is often left as a learnable parameter, despite uncertainty in one time step being inextricably tied to uncertainty in the previous time step. In this paper, we show simple and differentiable analytical approximations describing the relationship between variance at one timestep and that at the next with the kinematic bicycle model. In our results, we find that encoding the relationship between variance across timesteps works especially well in unoptimal settings, such as with small or noisy datasets. We observe up to a 50% performance boost in partial dataset settings and up to an 8% performance boost in large-scale learning compared to previous kinematic prediction methods on SOTA trajectory forecasting architectures out-of-the-box, with no fine-tuning.},
  archive   = {C_IROS},
  author    = {Laura Zheng and Sanghyun Son and Jing Liang and Xijun Wang and Brian Clipp and Ming C. Lin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802164},
  month     = {10},
  pages     = {943-950},
  title     = {Deep stochastic kinematic models for probabilistic motion forecasting in traffic},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Applying neural monte carlo tree search to unsignalized
multi-intersection scheduling for autonomous vehicles. <em>IROS</em>,
935–942. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dynamic scheduling of access to shared resources by autonomous systems is a challenging problem, characterized as being NP-hard. The complexity of this task leads to a combinatorial explosion of possibilities in highly dynamic systems where arriving requests must be continuously scheduled subject to strong safety and time constraints. An example of such a system is an unsignalized intersection, where automated vehicles’ access to potential conflict zones must be dynamically scheduled. In this paper, we apply Neural Monte Carlo Tree Search (NMCTS) to the challenging task of scheduling platoons of vehicles crossing unsignalized intersections. Crucially, we introduce a transformation model that maps successive sequences of potentially conflicting road-space reservation requests from platoons of vehicles into a series of board-game-like problems and use NMCTS to search for solutions representing optimal road-space allocation schedules in the context of past allocations. To optimize search, we incorporate a prioritized re-sampling method with parallel NMCTS (PNMCTS) to improve the quality of training data. To optimize training, a curriculum learning strategy is used to train the agent to schedule progressively more complex boards culminating in overlapping boards that represent busy intersections. In a busy single four-way unsignalized intersection simulation, PNMCTS solved 95% of unseen scenarios, reducing crossing time by 43% in light and 52% in heavy traffic versus first-in, first-out control. In a 3x3 multi-intersection network, the proposed method maintained free-flow in light traffic when all intersections are under control of PNMCTS and outperformed state-of-the-art RL-based traffic-light controllers in average travel time by 74.5% and total throughput by 16% in heavy traffic.},
  archive   = {C_IROS},
  author    = {Yucheng Shi and Wenlong Wang and Xiaowen Tao and Ivana Dusparic and Vinny Cahill},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801887},
  month     = {10},
  pages     = {935-942},
  title     = {Applying neural monte carlo tree search to unsignalized multi-intersection scheduling for autonomous vehicles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving behavior profile discovery for vehicles.
<em>IROS</em>, 927–934. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multiple approaches have already been proposed to mimic real driver behaviors in simulation. This article proposes a new one, based solely on the exploration of undisturbed observation of intersections. From them, the behavior profiles for each macro-maneuver will be discovered. Using the macro-maneuvers already identified in previous works, a comparison method between trajectories with different lengths using an Extended Kalman Filter (EKF) is proposed, which combined with an Expectation-Maximization (EM) inspired method, defines the different clusters that represent the behaviors observed. This is also paired with a Kullback-Liebler divergent (KL) criteria to define when the clusters need to be split or merged. Finally, the behaviors for each macro-maneuver are determined by each cluster discovered, without using any map information about the environment and being dynamically consistent with vehicle motion. By observation it becomes clear that the two main factors for driver’s behavior are their assertiveness and interaction with other road users.},
  archive   = {C_IROS},
  author    = {Nelson de Moura and Fernando Garrido and Fawzi Nashashibi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802505},
  month     = {10},
  pages     = {927-934},
  title     = {Improving behavior profile discovery for vehicles},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NeSyMoF: A neuro-symbolic model for motion forecasting.
<em>IROS</em>, 919–926. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advancements in deep learning have significantly enhanced the development of efficient models for multi-modal path prediction within urban environments, offering approaches to navigate complex environments accurately. Despite their performance, models grounded in deep learning techniques frequently encounter challenges related to interpretability. This limitation not only hampers their practical application but also complicates the process of diagnosing and rectifying errors within these systems, which is a critical factor for ensuring reliability and safety in realworld deployments. In this paper we propose NeSyMoF, a Neuro-Symbolic model for Motion Forecasting, to address this critical gap by combining the predictive power of deep neural networks with the interpretable logic inherent in symbolic reasoning. Data processing in NeSyMoF involves extracting pertinent features from the agent’s environment and channeling them into a neuro-symbolic reasoning module. The neurosymbolic reasoning module generates first-order logic rules that describe and condition the path prediction process, thereby providing clear explanations and intentions behind the forecasts of the model. We evaluate our model with the Argoverse benchmark for path forecasting, as it includes challenging driving situations, necessary to extensively evaluate our model. The results of our evaluation show that NeSyMoF outperforms state-of-the-art interpretable models for single-mode predictions while providing logic-based explanations for its forecasts, that articulate the reasoning behind predictions, making NeSyMoF more adapted for human-centric applications.},
  archive   = {C_IROS},
  author    = {Achref Doula and Huijie Yin and Max Mühlhäuser and Alejandro Sanchez Guinea},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801779},
  month     = {10},
  pages     = {919-926},
  title     = {NeSyMoF: A neuro-symbolic model for motion forecasting},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Asynchronous microphone array calibration using hybrid TDOA
information. <em>IROS</em>, 913–918. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Asynchronous microphone array calibration is a prerequisite for many audition robot applications. A popular solution to the above calibration problem is the batch form of Simultaneous Localisation and Mapping (SLAM), using the time difference of arrival measurements between two microphones (TDOA-M), and the robot (which serves as a moving sound source during calibration) odometry information. In this paper, we introduce a new form of measurement for microphone array calibration, i.e. the time difference of arrival between adjacent sound events (TDOA-S) with respect to the microphone channels. We propose to use TDOA-S and TDOA-M, called hybrid TDOA, together with odometry measurements for bath SLAM-based calibration of asynchronous microphone arrays. Extensive simulation and real-world experiments show that our method is more independent of microphone number, less sensitive to initial values (when using off-the-shelf algorithms such as Gauss-Newton iterations), and has better calibration accuracy and robustness under various TDOA noises. Simulation results also demonstrate that our method has a lower Cramér-Rao lower bound (CRLB) for microphone parameters. To benefit the community, we open-source our code and data at https://github.com/AISLAB-sustech/Hybrid-TDOA-Calib.},
  archive   = {C_IROS},
  author    = {Chengjie Zhang and Jiang Wang and He Kong},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801363},
  month     = {10},
  pages     = {913-918},
  title     = {Asynchronous microphone array calibration using hybrid TDOA information},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hybrid model and learning-based force estimation framework
for surgical robots. <em>IROS</em>, 906–912. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Haptic feedback to the surgeon during robotic surgery would enable safer and more immersive surgeries but estimating tissue interaction forces at the tips of robotically controlled surgical instruments has proven challenging. Few existing surgical robots can measure interaction forces directly and the additional sensor may limit the life of instruments. We present a hybrid model and learning-based framework for force estimation for the Patient Side Manipulators (PSM) of a da Vinci Research Kit (dVRK). The model-based component identifies the dynamic parameters of the robot and estimates free-space joint torque, while the learning-based component compensates for environmental factors, such as the additional torque caused by trocar interaction between the PSM instrument and the patient’s body wall. We evaluate our method in an abdominal phantom and achieve an error in force estimation of under 10% normalized root-mean-squared error. We show that by using a model-based method to perform dynamics identification, we reduce reliance on the training data covering the entire workspace. Although originally developed for the dVRK, the proposed method is a generalizable framework for other compliant surgical robots. The code is available at https://github.com/vu-maple-lab/dvrk_force_estimation.},
  archive   = {C_IROS},
  author    = {Hao Yang and Haoying Zhou and Gregory S. Fischer and Jie Ying Wu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802648},
  month     = {10},
  pages     = {906-912},
  title     = {A hybrid model and learning-based force estimation framework for surgical robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A piecewise-weighted RANSAC method utilizing abandoned
hypothesis model information with a new application on robot
self-calibration. <em>IROS</em>, 898–905. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Industrial robots and collaborative robots are widely employed in industry and are progressively being utilized to assist individuals in their daily routines. To improve their absolute accuracy, self-calibration methods using portable local measurement devices are cost-effective solutions. However, compared with the conventional external calibration methods, self-calibration methods employing two configurations as a calibration sample introduce more non-kinematic errors to the robot. Therefore, noise reduction is significantly necessary in self-calibration. A novel Piecewise-weighted Random Sample Consensus (RANSAC) method is proposed in this paper. Instead of choosing an optimal model with all inliers, the proposed method employs a general weight considering both the sample and hypothesis model qualities to generate a new model with Weighted Least Square (WLS) method. Besides, the proposed method turns the target of finding an uncontaminated set of inliers into the training of the proper weight coefficient for WLS, which not only improves the accuracy but also greatly enhances the speed. The self-calibration experiment on a 6 degree-of-freedom(DOF) robot CR10 shows that the accuracy of the proposed Piecewise-weighted RANSAC method makes a 27.7% accuracy improvement from that employing Least Square method, a 20.0% accuracy improvement from that employing standard RANSAC method, and a 5.5% accuracy improvement from that employing LO-RANSAC method. Besides, the proposed method is also over 10.9 times faster than the standard RANSAC method and 18.6 times faster than the LO-RANSAC method.},
  archive   = {C_IROS},
  author    = {Jianhui He and Yiyang Feng and Guilin Yang and Wenjun Shen and Silu Chen and Tianjiang Zheng and Junjie Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802682},
  month     = {10},
  pages     = {898-905},
  title     = {A piecewise-weighted RANSAC method utilizing abandoned hypothesis model information with a new application on robot self-calibration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). V2I-calib: A novel calibration approach for collaborative
vehicle and infrastructure LiDAR systems. <em>IROS</em>, 892–897. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cooperative LiDAR systems integrating vehicles and road infrastructure, termed V2I calibration, exhibit substantial potential, yet their deployment encounters numerous challenges. A pivotal aspect of ensuring data accuracy and consistency across such systems involves the calibration of LiDAR units across heterogeneous vehicular and infrastructural endpoints. This necessitates the development of calibration methods that are both real-time and robust, particularly those that can ensure robust performance in urban canyon scenarios without relying on initial positioning values. Accordingly, this paper introduces a novel approach to V2I calibration, leveraging spatial association information among perceived objects. Central to this method is the innovative Overall Intersection over Union (oIoU) metric, which quantifies the correlation between targets identified by vehicle and infrastructure systems, thereby facilitating the real-time monitoring of calibration results. Our approach involves identifying common targets within the perception results of vehicle and infrastructure LiDAR systems through the construction of an affinity matrix. These common targets then form the basis for the calculation and optimization of extrinsic parameters. Comparative and ablation studies conducted using the DAIR-V2X dataset substantiate the superiority of our approach. For further insights and resources, our project repository is accessible at https://github.com/MassimoQu/v2i-calib.},
  archive   = {C_IROS},
  author    = {Qianxin Qu and Yijin Xiong and Guipeng Zhang and Xin Wu and Xiaohan Gao and Xin Gao and Hanyu Li and Shichun Guo and Guoying Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802098},
  month     = {10},
  pages     = {892-897},
  title     = {V2I-calib: A novel calibration approach for collaborative vehicle and infrastructure LiDAR systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MEMROC: Multi-eye to mobile RObot calibration.
<em>IROS</em>, 884–891. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents MEMROC (Multi-Eye to Mobile RObot Calibration), a novel motion-based calibration method that simplifies the process of accurately calibrating multiple cameras relative to a mobile robot’s reference frame. MEMROC utilizes a known calibration pattern to facilitate accurate calibration with a lower number of images during the optimization process. Additionally, it leverages robust ground plane detection for comprehensive 6-DoF extrinsic calibration, overcoming a critical limitation of many existing methods that struggle to estimate the complete camera pose. The proposed method addresses the need for frequent recalibration in dynamic environments, where cameras may shift slightly or alter their positions due to daily usage, operational adjustments, or vibrations from mobile robot movements. MEMROC exhibits remarkable robustness to noisy odometry data, requiring minimal calibration input data. This combination makes it highly suitable for daily operations involving mobile robots. A comprehensive set of experiments on both synthetic and real data proves MEMROC’s efficiency, surpassing existing state-of-the-art methods in terms of accuracy, robustness, and ease of use. To facilitate further research, we have made our code publicly available1.},
  archive   = {C_IROS},
  author    = {Davide Allegro and Matteo Terreran and Stefano Ghidoni},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801771},
  month     = {10},
  pages     = {884-891},
  title     = {MEMROC: Multi-eye to mobile RObot calibration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uncertainty-aware deployment of pre-trained
language-conditioned imitation learning policies. <em>IROS</em>,
878–883. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large-scale robotic policies trained on data from diverse tasks and robotic platforms hold great promise for enabling general-purpose robots; however, reliable generalization to new environment conditions remains a major challenge. Toward addressing this challenge, we propose a novel approach for uncertainty-aware deployment of pre-trained language-conditioned imitation learning agents. Specifically, we use temperature scaling to calibrate these models and exploit the calibrated model to make uncertainty-aware decisions by aggregating the local information of candidate actions. We implement our approach in simulation using three such pre-trained models, and showcase its potential to significantly enhance task completion rates. The accompanying code is accessible at the link: https://github.com/BobWu1998/uncertainty_quant_all.git},
  archive   = {C_IROS},
  author    = {Bo Wu and Bruce D. Lee and Kostas Daniilidis and Bernadette Bucher and Nikolai Matni},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802849},
  month     = {10},
  pages     = {878-883},
  title     = {Uncertainty-aware deployment of pre-trained language-conditioned imitation learning policies},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A graph-based self-calibration technique for cable-driven
robots with sagging cable. <em>IROS</em>, 872–877. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The efficient operation of large-scale Cable-Driven Parallel Robots (CDPRs) relies on precise calibration of kinematic parameters and the simplicity of the calibration process. This paper presents a graph-based self-calibration framework that explicitly addresses cable sag effects and facilitates the calibration procedure for large-scale CDPRs by only relying on internal sensors. A unified factor graph is proposed, incorporating a catenary cable model to capture cable sagging. The factor graph iteratively refines kinematic parameters, including anchor point locations and initial cable length, by considering jointly onboard sensor data and the robot’s kineto-static model. The applicability and accuracy of the proposed technique are demonstrated through Finite Element (FE) simulations, on both large and small-scale CDPRs subjected to significant initialization perturbations.},
  archive   = {C_IROS},
  author    = {M. R. Dindarloo and A. S. Mirjalili and S. A. Khalilpour and R. Khorrambakht and Stephan Weiss and H. D. Taghirad},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802627},
  month     = {10},
  pages     = {872-877},
  title     = {A graph-based self-calibration technique for cable-driven robots with sagging cable},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MFCalib: Single-shot and automatic extrinsic calibration for
LiDAR and camera in targetless environments based on multi-feature edge.
<em>IROS</em>, 864–871. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents MFCalib, an innovative extrinsic calibration technique for LiDAR and RGB camera that operates automatically in targetless environments with a single data capture. At the heart of this method is using a rich set of edge information, significantly enhancing calibration accuracy and robustness. Specifically, we extract both depth-continuous and depth-discontinuous edges, along with intensity-discontinuous edges on planes. This comprehensive edge extraction strategy ensures our ability to achieve accurate calibration with just one round of data collection, even in complex and varied settings. Addressing the uncertainty of depth-discontinuous edges, we delve into the physical measurement principles of LiDAR and develop a beam model, effectively mitigating the issue of edge inflation caused by the LiDAR beam. Extensive experiment results demonstrate that MFCalib outperforms the state-of-the-art targetless calibration methods across various scenes, achieving and often surpassing the precision of multi-scene calibrations in a single-shot collection. To support community development, we make our code available open-source on GitHub.},
  archive   = {C_IROS},
  author    = {Tianyong Ye and Wei Xu and Chunran Zheng and Yukang Cui},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802395},
  month     = {10},
  pages     = {864-871},
  title     = {MFCalib: Single-shot and automatic extrinsic calibration for LiDAR and camera in targetless environments based on multi-feature edge},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient extrinsic self-calibration of multiple IMUs using
measurement subset selection. <em>IROS</em>, 856–863. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the problem of choosing a sparse subset of measurements for quick calibration parameter estimation. A standard solution to this is selecting a measurement only if its utility—the difference between posterior (with the measurement) and prior information (without the measurement)—exceeds some threshold. Theoretically, utility, a function of the parameter estimate, should be evaluated at the estimate obtained with all measurements selected so far, hence necessitating a recalibration with each new measurement. However, we hypothesize that utility is insensitive to changes in the parameter estimate for many systems of interest, suggesting that evaluating utility at some initial parameter guess would yield equivalent results in practice. We provide evidence supporting this hypothesis for extrinsic calibration of multiple inertial measurement units (IMUs), showing the reduction in calibration time by two orders of magnitude by forgoing recalibration for each measurement.},
  archive   = {C_IROS},
  author    = {Jongwon Lee and David Hanley and Timothy Bretl},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802604},
  month     = {10},
  pages     = {856-863},
  title     = {Efficient extrinsic self-calibration of multiple IMUs using measurement subset selection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Research of calibration method for fusion of LDS sensor and
ToF low-cost sensor. <em>IROS</em>, 848–855. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a method for calibrating the external parameters of the LDS sensor and ToF depth camera based on three cylinders. This method obtains the scanning data of the side surfaces of the three cylinders at different postures by changing the posture of the robot. For the single-line laser plane scanned by the LDS sensor, three elliptical contours are obtained by intersecting with the side surfaces of the three cylinders respectively. The Random Sample Consensus (RANSAC) algorithm is used to obtain the coordinates of the center points of the three elliptical contours and two random points on each elliptical contour. For the three-dimensional point cloud image of the cylinder scanned by the ToF depth camera, the RANSAC algorithm is used to fit the central axis of the three cylinders. The nonlinear optimization equation is established using the three center points obtained from the three elliptical contours and the distances from the two random points on each elliptical contour to their corresponding central axes. In this paper, we propose to use a fusion method of the Powell algorithm and the BFGS algorithm to solve the nonlinear optimization equations to obtain the transformation matrix between the LDS sensor and the ToF depth camera. Finally, simulation and actual test are carried out based on the proposed method, and the influence of the initial value of the calibration parameter on the calibration result is discussed. The accuracy of the calibration algorithm in this paper is verified through comparative experiments of the calibration algorithm. The results show that the calibration accuracy of the proposed method is better than that of the traditional planar calibration method, and it has the characteristics of simple operation and high calibration accuracy.},
  archive   = {C_IROS},
  author    = {Jiahui Zhu and Guitao Yu and Yang He and Kui Yang and Dongtai Liang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801424},
  month     = {10},
  pages     = {848-855},
  title     = {Research of calibration method for fusion of LDS sensor and ToF low-cost sensor},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Extrinsic calibration of multiple LiDARs for a mobile robot
based on floor plane and object segmentation. <em>IROS</em>, 840–847.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The utilization of mobile robots equipped with multiple light detection and ranging (LiDAR) sensors, capable of perceiving their surroundings, is on the rise due to the miniaturization and cost reduction of LiDAR technology. This paper introduces a target-less extrinsic calibration method for multiple LiDARs with non-overlapping fields of view (FoV). The proposed method leverages accumulated point clouds of the floor plane and objects obtained during robot motion. It enables accurate calibration, even in challenging configurations where LiDARs are directed towards the floor plane, which can introduce biased feature values. Additionally, the method incorporates a noise removal module that takes into account the scanning pattern to address bleeding points, which are significant sources of error in point cloud alignment when using high-density LiDARs. Evaluations conducted through simulation demonstrate that the proposed method achieves higher accuracy in extrinsic calibration with two and four LiDARs compared to conventional methods, regardless of the type of objects. Furthermore, experiments conducted using a real mobile robot validate the effectiveness of our proposed noise removal module in precisely eliminating noise compared to conventional methods. The estimated extrinsic parameters successfully contribute to the creation of consistent 3D maps.},
  archive   = {C_IROS},
  author    = {Shun Niijima and Atsushi Suzuki and Ryoichi Tsuzaki and Masaya Kinoshita},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802291},
  month     = {10},
  pages     = {840-847},
  title     = {Extrinsic calibration of multiple LiDARs for a mobile robot based on floor plane and object segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sensor-agnostic visuo-tactile robot calibration exploiting
assembly-precision model geometries. <em>IROS</em>, 832–839. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual sensor modalities dominate traditional robot calibration, but when environment contacts are relevant, the tactile modality can provide another natural, accurate, and highly relevant modality. Most existing tactile sensing methods for robot calibration are constrained to specific sensor-object pairs, limiting their applicability. This paper pioneers a general approach to exploit contacts in robot calibration, supporting self-touch throughout the entire system kinematics by generalizing touchable surfaces to any accurately represented mesh surface. The approach supports different contact sensors as long as a simple single-contact interface can be provided. Integrated into the Atomic Transformation Optimization Method (ATOM) calibration methodology, our work facilitates seamless integration of both modalities in a single approach. Our results demonstrate comparable performance to single-modality calibration but can trade off accuracy between both modalities, thus increasing overall robustness. Furthermore, we observe that utilizing a touch point at the end of a kinematic chain slightly improves calibration over touching the chain links with an external sensor but find no significant advantage of restricting touch to end-effector contacts when calibrating a dual-arm system with our method.},
  archive   = {C_IROS},
  author    = {Manuel Gomes and Michael Görner and Miguel Oliveira and Jianwei Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801647},
  month     = {10},
  pages     = {832-839},
  title     = {Sensor-agnostic visuo-tactile robot calibration exploiting assembly-precision model geometries},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A direct algorithm for multi-gyroscope infield calibration.
<em>IROS</em>, 824–831. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we address the problem of estimating the rotational extrinsics, as well as the scale factors of two gyroscopes rigidly mounted on the same device. In particular, we formulate the problem as a least-squares minimization and introduce a direct algorithm that computes the estimated quantities without any iterations, hence avoiding local minima and improving efficiency. Furthermore, we show that the rotational extrinsics are observable while the scale factors can be determined up to a global scale for general configurations of the gyroscopes. To this end, we also study special placements of the gyroscopes where a pair, or all, of their axes are parallel and analyze their impact on the scale factors’ observability. Lastly, we evaluate our algorithm in simulation and real-world experiments to assess its performance as a function of key motion and sensor characteristics.},
  archive   = {C_IROS},
  author    = {Tianheng Wang and Stergios I. Roumeliotis},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801933},
  month     = {10},
  pages     = {824-831},
  title     = {A direct algorithm for multi-gyroscope infield calibration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EasyHeC++: Fully automatic hand-eye calibration with
pretrained image models. <em>IROS</em>, 816–823. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hand-eye calibration plays a fundamental role in robotics by directly influencing the efficiency of critical operations such as manipulation and grasping. In this work, we present a novel framework, EasyHeC++, designed for fully automatic hand-eye calibration. In contrast to previous methods that necessitate manual calibration, specialized markers, or the training of arm-specific neural networks, our approach is the first system that enables accurate calibration of any robot arm in a marker-free, training-free, and fully automatic manner. Our approach employs a two-step process. First, we initialize the camera pose using a sampling or feature-matching-based method with the aid of pretrained image models. Subsequently, we perform pose optimization through differentiable rendering. Extensive experiments demonstrate the system’s superior accuracy in both synthetic and real-world datasets across various robot arms and camera settings. Project page: https://ootts.github.io/easyhec_plus/.},
  archive   = {C_IROS},
  author    = {Zhengdong Hong and Kangfu Zheng and Linghao Chen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801359},
  month     = {10},
  pages     = {816-823},
  title     = {EasyHeC++: Fully automatic hand-eye calibration with pretrained image models},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An online automatic calibration method for
infrastructure-based LiDAR-camera via cross-modal object matching.
<em>IROS</em>, 810–815. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In indoor environments where the Global Navigation Satellite System (GNSS) isn’t available, the infrastructure-based LiDAR-camera joint array can provide high-precision localization for mobile robots, such as Autonomous Valet Parking (AVP). The primary challenge in employing the infrastructure-based LiDAR-camera joint array is the extrinsic calibration between the LiDAR and the camera. Moreover, to handle interference deviation caused by vibrations or inadequate mounting stiffness during operation, the calibration’s extrinsic parameters must be automatically updated online, presenting higher demands for infrastructure-based LiDAR-camera extrinsic calibration. This paper proposes an infrastructure LiDAR-camera online automatic calibration method based on prior knowledge of cross-modal target registration. This method requires no manual targets and initial pose guesses and can achieve extrinsic calibration. The object-prior model based on a lightweight object detection algorithm can rapidly detect scenes favorable for extrinsic calibration in sub-images of camera images. This creates favorable conditions for the registration of cross-modal networks and poses optimization of the LiDAR camera. Additionally, because a lightweight algorithm is used, the process does not compromise efficiency or consume excessive computational resources. Experimental results demonstrate that the proposed calibration method is suitable for calibrating infrastructure-based LiDAR-camera, with comparable accuracy and the ability to perform online calibration. Comparative experiments also show that the object-prior model can indeed select better scenes for LiDAR-camera extrinsic calibration, thus improving the accuracy and stability of extrinsic calibration to some extent.},
  archive   = {C_IROS},
  author    = {Tao Wang and Yuesheng He and Hanyang Zhuang and Ming Yang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802407},
  month     = {10},
  pages     = {810-815},
  title     = {An online automatic calibration method for infrastructure-based LiDAR-camera via cross-modal object matching},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online adaptation of learned vehicle dynamics model with
meta-learning approach. <em>IROS</em>, 802–809. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We represent a vehicle dynamics model for autonomous driving near the limits of handling via a multilayer neural network. Online adaptation is desirable in order to address unseen environments. However, the model needs to adapt to new environments without forgetting previously encountered ones. In this study, we apply Continual-MAML to overcome this difficulty. It enables the model to adapt to the previously encountered environments quickly and efficiently by starting updates from optimized initial parameters. We evaluate the impact of online model adaptation with respect to inference performance and impact on control performance of a model predictive path integral (MPPI) controller using the TRIKart platform. The neural network was pre-trained using driving data collected in our test environment, and experiments for online adaptation were executed on multiple different road conditions not contained in the training data. Empirical results show that the model using Continual-MAML outperforms the fixed model and the model using gradient descent in test set loss and online tracking performance of MPPI.},
  archive   = {C_IROS},
  author    = {Yuki Tsuchiya and Thomas Balch and Paul Drews and Guy Rosman},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801427},
  month     = {10},
  pages     = {802-809},
  title     = {Online adaptation of learned vehicle dynamics model with meta-learning approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Virtual model control for compliant reaching under
uncertainties. <em>IROS</em>, 795–801. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Virtual Model Control (VMC) is an approach to design a controller for force-controlled robots in complex uncertain environments. While this method was primarily investigated for legged robot locomotion in the past, it can be more generally applicable to other types of robotic systems. This paper investigates the VMC framework for reaching tasks in a force-controlled robotic arm. We propose six different approaches to designing virtual models in order to achieve reaching tasks in environments with obstacles and uncertainties. A force-controlled 8 degree-of-freedom humanoid robot was used to validate the proposed approach in the real world. We conducted three experiments to test the performance of VMC controllers in terms of predictability, sensitivity to external force, and adaptability against known and unknown obstacles. Experimental analyses show that, even though the proposed approach needs to sacrifice accuracy and trajectory optimality, it enables us to design complex reaching motions under uncertainties, in an intuitive and extendable manner.},
  archive   = {C_IROS},
  author    = {Yi Zhang and Daniel Larby and Fumiya Iida and Fulvio Forni},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801592},
  month     = {10},
  pages     = {795-801},
  title     = {Virtual model control for compliant reaching under uncertainties},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robot guided evacuation with viewpoint constraints.
<em>IROS</em>, 787–794. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a viewpoint-based non-linear Model Predictive Control (MPC) for evacuation guiding robots. Specifically, the proposed MPC algorithm enables evacuation guiding robots to track and guide cooperative human targets in emergency scenarios. Our algorithm accounts for the environment layout as well as distances between the robot and human target and distance to the goal location. A key challenge for evacuation guiding robot is the trade-off between its planned motion for leading the target toward a goal position and staying in the target’s viewpoint while maintaining line-of-sight for guiding. We illustrate the effectiveness of our proposed evacuation guiding algorithm in both simulated and real-world environments with an Unmanned Aerial Vehicle (UAV) guiding a human. Our results suggest that using the contextual information from the environment for motion planning, increases the visibility of the guiding UAV to the human while achieving faster total evacuation time.},
  archive   = {C_IROS},
  author    = {Gong Chen and Malika Meghjani and Marcel Bartholomeus Prasetyo},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802741},
  month     = {10},
  pages     = {787-794},
  title     = {Robot guided evacuation with viewpoint constraints},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attitude control of the hydrobatic intervention AUV
cuttlefish using incremental nonlinear dynamic inversion. <em>IROS</em>,
781–786. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present an attitude control scheme for an autonomous underwater vehicle (AUV), which is based on incremental nonlinear dynamic inversion (INDI). Conventional model-based controllers depend on an exact model of the controlled system, which is difficult to find, especially for marine vehicles subject to highly nonlinear hydrodynamic effects. INDI trades off model accuracy with sensor accuracy by incorporating acceleration feedback and actuator output feedback to linearize a nonlinear system incrementally. Existing research primarily focuses on studying INDI on unmanned aerial vehicles. However, there is barely any research on controlling marine vehicles using INDI. The control task we are performing is a 90 degrees pitch-up maneuver, where the dual-arm intervention AUV Cuttlefish transitions from a horizontal traveling pose to a vertical intervention pose. We compare INDI to a classical model-based control scheme in the maritime test basin at DFKI RIC, Germany, and we find that INDI keeps the AUV much more steady both in the transitioning phase as well as in the station keeping phase.},
  archive   = {C_IROS},
  author    = {Tom Slawik and Shubham Vyas and Leif Christensen and Frank Kirchner},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802674},
  month     = {10},
  pages     = {781-786},
  title     = {Attitude control of the hydrobatic intervention AUV cuttlefish using incremental nonlinear dynamic inversion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Segmented safety docking control for mobile
self-reconfigurable robots. <em>IROS</em>, 773–780. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile self-reconfigurable robots (MSRRs), as a novel multi-robot system with flexible configurations and task adaptability, hold promising applications in unstructured task environments. However, existing autonomous docking strategies are primarily applied in laboratory settings and face numerous challenges and limitations in actual applications, including differences in sensor characteristics, safety threats, and saturation constraints. To address these issues, this paper proposes a segmented secure docking control framework based on global localization and local perception to achieve stable and reliable reconfiguration of MSRRs in practical applications. Specific contributions include the implementation of a dual-layer constraint framework for safeness of units in the long-distance phase against velocity and acceleration nested windups, and the integration of active line-of-sight (LOS) correction and adaptive windup driving mobile units to achieve precise and rapid locking of docked positions within the LOS in the close-range phase. Finally, the validity of the proposed method is verified via physical experiments, offering an innovative approach to deploying MSRRs in complex scenarios.},
  archive   = {C_IROS},
  author    = {Zhi Zheng and Tao Jiang and Senqi Tan and Hao Zhang and Jianchuan Ye},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802209},
  month     = {10},
  pages     = {773-780},
  title     = {Segmented safety docking control for mobile self-reconfigurable robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Development of a spherical wheel-legged composite mobile
robot with multimodal motion capabilities. <em>IROS</em>, 767–772. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a spherical wheel-legged mobile robot, aiming to meet the demands of adaptability to complex terrains and high maneuverability. It consists of a spherical main body and five-bar linkage parallel wheel-legged mechanisms. It can switch between legged and spherical modes by extending and retracting its legs according to the demands of the actual environment, thereby enhancing the overall mobility of the robot. By designing the Linear Quadratic Regulator (LQR) controller, we achieve the impact-resistant leg balancing motion and autonomous pitch adjustment for the robot on inclined surfaces in the legged configuration. For the rolling control in the spherical configuration, a hierarchical sliding mode control method and Proportional-Integral-Derivative controller (PID) are employed to control the rolling and turning of the robot. We verify the robustness of the robot in wheel-legged configuration against disturbances and the stability of its motion in spherical configuration.},
  archive   = {C_IROS},
  author    = {Yuyang Du and Ruihua Ye and Wenfu Xu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801641},
  month     = {10},
  pages     = {767-772},
  title     = {Development of a spherical wheel-legged composite mobile robot with multimodal motion capabilities},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Response improvement of hydraulic robotic joints via a force
servo and inverted pendulum demo. <em>IROS</em>, 761–766. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, a force servo for hydraulic robot joints is designed and applied to an inverted pendulum demo. First, the nonlinear nominal model of the hydraulic cylinder is reviewed, and the nominal integrator is introduced. Second, through a partial fusion of our modeling and control techniques, a force servo is designed based on a modification of the nominal integrator as an intermediate stage. Finally, against the non-short hydraulic pipes (1.9 m), the response is improved by the designed force servo in the presence of rotational motion effects.},
  archive   = {C_IROS},
  author    = {Ryo Arai and Satoru Sakai and Kazuki Ono},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802664},
  month     = {10},
  pages     = {761-766},
  title     = {Response improvement of hydraulic robotic joints via a force servo and inverted pendulum demo},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Current-based impedance control for interacting with mobile
manipulators. <em>IROS</em>, 753–760. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As robots shift from industrial to human-centered spaces, adopting mobile manipulators, which expand workspace capabilities, becomes crucial. In these settings, seamless interaction with humans necessitates compliant control. Two common methods for safe interaction, admittance, and impedance control, require force or torque sensors, often absent in lower-cost or lightweight robots. This paper presents an adaption of impedance control that can be used on current-controlled robots without the use of force or torque sensors and shows its application for compliant control of a mobile manipulator. A calibration method is designed that enables estimation of the actuators’ current/torque ratios and frictions, used by the adapted impedance controller, and that can handle model errors. The calibration method and the performance of the designed controller are experimentally validated using the Kinova GEN3 Lite arm. Results show that the calibration method is consistent and that the designed controller for the arm is compliant while also being able to track targets with five-millimeter precision when no interaction is present. Additionally, this paper presents two operational modes for interacting with the mobile manipulator: one for guiding the robot around the workspace through interacting with the arm and another for executing a tracking task, both maintaining compliance to external forces. These operational modes were tested in real-world experiments, affirming their practical applicability and effectiveness.Code: https://github.com/tud-amr/mobile-manipulator-compliance},
  archive   = {C_IROS},
  author    = {Jelmer de Wolde and Luzia Knoedler and Gianluca Garofalo and Javier Alonso-Mora},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802856},
  month     = {10},
  pages     = {753-760},
  title     = {Current-based impedance control for interacting with mobile manipulators},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved contact stability for admittance control of
industrial robots with inverse model compensation. <em>IROS</em>,
746–752. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Industrial robots have increased payload, repeatability, and reach compared to collaborative robots, however, they have a fixed position controller and low intrinsic admittance. This makes realizing safe contact challenging due to large contact force overshoots in contact transitions and contact instability when the environment and robot dynamics are coupled. To improve safe contact on industrial robots, we propose an admittance controller with inverse model compensation, designed and implemented outside the position controller. By including both the inner loop and outer loop dynamics in its design, the proposed method achieves expanded admittance in terms of increasing both gain and cutoff frequency of the desired admittance. Results from theoretical analyses and experiments on a commercial industrial robot show that the proposed method improves rendering of the desired admittance while maintaining contact stability. We further validate this by conducting actual assembly tasks of plug insertion with fine positioning, switch insertion onto the rail, and colliding the robot end effector with random objects and surfaces, as seen at https://youtu.be/8XfkdHEdWDs.},
  archive   = {C_IROS},
  author    = {Kangwagye Samuel and Kevin Haninger and Sami Haddadin and Sehoon Oh},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802440},
  month     = {10},
  pages     = {746-752},
  title     = {Improved contact stability for admittance control of industrial robots with inverse model compensation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Position control of a low-energy c-core reluctance actuator
in a motion system. <em>IROS</em>, 740–745. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a position control system for a motion stage driven by a low-energy C-core reluctance actuator. The central concept explored here is the utilization of a variable air gap to enable energy-efficient operation of the motion stage. First, we show the design and mathematical model of the reluctance-actuated motion system (RAMS). Then, by analyzing open-loop responses of the RAMS under various conditions including variable air gaps and different excitation voltages, we show that using variable air gap can reduce the required current. Finally, the paper formulates a control approach that combines a feedforward controller to linearize the RAMS’s dynamic behavior and a state feedback controller to achieve tracking performance. Experimental results demonstrate the effectiveness of this control approach in achieving tracking objectives with errors that are less than 2% for constant desired displacement and less than 10% for tracking a sinusoidal reference signal.},
  archive   = {C_IROS},
  author    = {Mohammad Al Saaideh and Yazan M. Al-Rawashdeh and Natheer Alatawneh and Khaled Aljanaideh and Mohammad Al Janaideh},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802109},
  month     = {10},
  pages     = {740-745},
  title     = {Position control of a low-energy C-core reluctance actuator in a motion system},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A proxy-tactile reactive control for robots moving in
clutter. <em>IROS</em>, 733–739. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots performing tasks in challenging environments must be supported by control or planning algorithms that exploit sensor feedback to effectively plan the robot’s actions. In this paper, we propose a reactive control law that simultaneously utilizes proximity and tactile feedback to perform a pick-and-place task in an unknown and cluttered environment. Specifically, the presented solution leverages proximity sensing obtained from distributed Time of Flight (ToF) sensors to avoid collision when this does not interfere with the pick-and-place task. Safety is guaranteed by a higher-priority task using tactile feedback that reduces contact forces when a collision occurs. Additionally, we compare the effectiveness of this control scheme with a collision detection and reaction scheme based solely on tactile sensing. Our results demonstrate that the proposed approach reduces the collisions with the environment and the task execution time of the pick-and-place operation.},
  archive   = {C_IROS},
  author    = {Giammarco Caroleo and Francesco Giovinazzo and Alessandro Albini and Francesco Grella and Giorgio Cannata and Perla Maiolino},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802341},
  month     = {10},
  pages     = {733-739},
  title     = {A proxy-tactile reactive control for robots moving in clutter},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Zero-shot transfer of a tactile-based continuous force
control policy from simulation to robot. <em>IROS</em>, 725–732. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The advent of tactile sensors in robotics has sparked many ideas on how robots can leverage direct contact measurements of their environment interactions to improve manipulation tasks. An important line of research in this regard is grasp force control, which aims to manipulate objects safely by limiting the amount of force exerted on the object. While prior works have either hand-modeled their force controllers, employed model-based approaches, or not shown sim-to-real transfer, we propose a model-free deep reinforcement learning approach trained in simulation and then transferred to the robot without further fine-tuning. We, therefore, present a simulation environment that produces realistic normal forces, which we use to train continuous force control policies. A detailed evaluation shows that the learned policy performs similarly or better than a hand-crafted baseline. Ablation studies prove that the proposed inductive bias and domain randomization facilitate sim-to-real transfer. Code, models, and supplementary videos are available on https://sites.google.com/view/rl-force-ctrl},
  archive   = {C_IROS},
  author    = {Luca Lach and Robert Haschke and Davide Tateo and Jan Peters and Helge Ritter and Júlia Borràs and Carme Torras},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802386},
  month     = {10},
  pages     = {725-732},
  title     = {Zero-shot transfer of a tactile-based continuous force control policy from simulation to robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neuromorphic force-control in an industrial task: Validating
energy and latency benefits. <em>IROS</em>, 717–724. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As robots become smarter and more ubiquitous, optimizing the power consumption of intelligent compute becomes imperative towards ensuring the sustainability of technological advancements. Neuromorphic computing hardware makes use of biologically inspired neural architectures to achieve energy and latency improvements compared to conventional von Neumann computing architecture. Applying these benefits to robots has been demonstrated in several works in the field of neurorobotics, typically on relatively simple control tasks. Here, we introduce an example of neuromorphic computing applied to the real-world industrial task of object insertion. We trained a spiking neural network (SNN) to perform force-torque feedback control using a reinforcement learning approach in simulation. We then ported the SNN to the Intel neuromorphic research chip Loihi interfaced with a KUKA robotic arm. At inference time we show latency competitive with current CPU/GPU architectures, and one order of magnitude less energy usage in comparison to traditional low-energy edge-hardware. We offer this example as a proof of concept implementation of a neuromoprhic controller in real-world robotic setting, highlighting the benefits of neuromorphic hardware for the development of intelligent controllers for robots.},
  archive   = {C_IROS},
  author    = {Camilo Amaya and Evan Eames and Gintautas Palinauskas and Alexander Perzylo and Yulia Sandamirskaya and Axel von Arnim},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802430},
  month     = {10},
  pages     = {717-724},
  title     = {Neuromorphic force-control in an industrial task: Validating energy and latency benefits},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Koopman dynamic modeling for global and unified
representations of rigid body systems making and breaking contact.
<em>IROS</em>, 709–716. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A global modeling methodology based on Koopman operator theory for the dynamics of rigid bodies that make and break contact is presented. Traditionally, robotic systems that contact with their environment are represented as a system comprised of multiple dynamic equations that are switched depending on the contact state. This switching of governing dynamics has been a challenge in both task planning and control. Here, a Koopman lifting linearization approach is presented to subsume multiple dynamics such that no explicit switching is required for examining the dynamic behaviors across diverse contact states. First, it is shown that contact/non-contact transitions are continuous at a microscopic level. This allows for the application of Koopman operator theory to the class of robotic systems that repeat contact/non-contact transitions. Second, an effective method for finding Koopman operator observables for capturing rapid changes to contact forces is presented. The method is applied to the modeling of dynamic peg insertion where a peg collides against and bounces on the chamfer of the hole. Furthermore, the method is applied to the dynamic modeling of a sliding object subject to complex friction and damping properties. Segmented dynamic equations are unified with the Koopman modeling method.},
  archive   = {C_IROS},
  author    = {Cormac O’Neill and H. Harry Asada},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801737},
  month     = {10},
  pages     = {709-716},
  title     = {Koopman dynamic modeling for global and unified representations of rigid body systems making and breaking contact},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning-based adaptive control of quadruped robots for
active stabilization on moving platforms. <em>IROS</em>, 701–708. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A quadruped robot faces balancing challenges on a six-degrees-of-freedom moving platform, like subways, buses, airplanes, and yachts, due to independent platform motions and resultant diverse inertia forces on the robot. To alleviate these challenges, we present the Learning-based Active Stabilization on Moving Platforms (LAS-MP), featuring a self-balancing policy and system state estimators. The policy adaptively adjusts the robot’s posture in response to the platform’s motion. The estimators infer robot and platform states based on proprioceptive sensor data. For a systematic training scheme across various platform motions, we introduce platform trajectory generation and scheduling methods. Our evaluation demonstrates superior balancing performance across multiple metrics compared to three baselines. Furthermore, we conduct a detailed analysis of the LAS-MP, including ablation studies and evaluation of the estimators, to validate the effectiveness of each component.},
  archive   = {C_IROS},
  author    = {Minsung Yoon and Heechan Shin and Jeil Jeong and Sung-Eui Yoon},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802150},
  month     = {10},
  pages     = {701-708},
  title     = {Learning-based adaptive control of quadruped robots for active stabilization on moving platforms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TopoNav: Topological navigation for efficient exploration in
sparse reward environments. <em>IROS</em>, 693–700. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous robots exploring unknown environments face a significant challenge: navigating effectively without prior maps and with limited external feedback. This challenge intensifies in sparse reward environments, where traditional exploration techniques often fail. In this paper, we present TopoNav, a novel topological navigation framework that integrates active mapping, hierarchical reinforcement learning, and intrinsic motivation to enable efficient goal-oriented exploration and navigation in sparse-reward settings. TopoNav dynamically constructs a topological map of the environment, capturing key locations and pathways. A two-level hierarchical policy architecture, comprising a high-level graph traversal policy and low-level motion control policies, enables effective navigation and obstacle avoidance while maintaining focus on the overall goal. Additionally, TopoNav incorporates intrinsic motivation to guide exploration towards relevant regions and frontier nodes in the topological map, addressing the challenges of sparse extrinsic rewards. We evaluate TopoNav both in the simulated and real-world off-road environments using a Clearpath Jackal robot, across three challenging navigation scenarios: goal-reaching, feature-based navigation, and navigation in complex terrains. We observe an increase in exploration coverage by 7-20%, in success rates by 9-19%, and reductions in navigation times by 15-36% across various scenarios, compared to state-of-the-art methods.},
  archive   = {C_IROS},
  author    = {Jumman Hossain and Abu-Zaher Faridee and Nirmalya Roy and Jade Freeman and Timothy Gregory and Theron Trout},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802380},
  month     = {10},
  pages     = {693-700},
  title     = {TopoNav: Topological navigation for efficient exploration in sparse reward environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning when to stop: Efficient active tactile perception
with deep reinforcement learning. <em>IROS</em>, 685–692. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Actively guiding attention is an important mechanism to employ limited processing resources efficiently. The Recurrent Visual Attention Model (RAM) has been successfully applied to process large input images by sequentially attending to smaller image regions with an RL framework. In tactile perception, sequential attention methods are required naturally due to the limited size of the tactile receptive field. The concept of RAM was transferred to the haptic domain by the Haptic Attention Model (HAM) to iteratively generate a fixed number of informative haptic glances for tactile object classification. We extend HAM to a system capable of actively determining when sufficient haptic data is available for reliable classification. To this end, we introduce a hybrid action space, augmenting the continuous glance location with the discrete decision of when to classify. This allows balancing the cost of obtaining new samples against the cost of misclassification, resulting in an optimized number of glances while maintaining reasonable accuracy. We evaluate the efficiency of our approach on a handcrafted dataset, which allows us to compute the most efficient glance locations.},
  archive   = {C_IROS},
  author    = {Christopher Niemann and David Leins and Luca Lach and Robert Haschke},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801966},
  month     = {10},
  pages     = {685-692},
  title     = {Learning when to stop: Efficient active tactile perception with deep reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable multi-agent reinforcement learning for warehouse
logistics with robotic and human co-workers. <em>IROS</em>, 677–684. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider a warehouse in which dozens of mobile robots and human pickers work together to collect and deliver items within the warehouse. The fundamental problem we tackle, called the order-picking problem, is how these worker agents must coordinate their movement and actions in the warehouse to maximise performance in this task. Established industry methods using heuristic approaches require large engineering efforts to optimise for innately variable warehouse configurations. In contrast, multi-agent reinforcement learning (MARL) can be flexibly applied to diverse warehouse configurations (e.g. size, layout, number/types of workers, item replenishment frequency), and different types of order-picking paradigms (e.g. Goods-to-Person and Person-to-Goods), as the agents can learn how to cooperate optimally through experience. We develop hierarchical MARL algorithms in which a manager agent assigns goals to worker agents, and the policies of the manager and workers are co-trained toward maximising a global objective (e.g. pick rate). Our hierarchical algorithms achieve significant gains in sample efficiency over baseline MARL algorithms and overall pick rates over multiple established industry heuristics in a diverse set of warehouse configurations and different order-picking paradigms.},
  archive   = {C_IROS},
  author    = {Aleksandar Krnjaic and Raul D. Steleac and Jonathan D. Thomas and Georgios Papoudakis and Lukas Schäfer and Andrew Wing Keung To and Kuan-Ho Lao and Murat Cubuktepe and Matthew Haley and Peter Börsting and Stefano V. Albrecht},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802813},
  month     = {10},
  pages     = {677-684},
  title     = {Scalable multi-agent reinforcement learning for warehouse logistics with robotic and human co-workers},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BayRnTune: Adaptive bayesian domain randomization via
strategic fine-tuning. <em>IROS</em>, 670–676. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Domain randomization (DR), which entails training a policy with randomized dynamics, has proven to be a simple yet effective algorithm for reducing the gap between simulation and the real world. However, DR often requires careful tuning of randomization parameters. Methods like Bayesian Domain Randomization (Bayesian DR) and Active Domain Randomization (Adaptive DR) address this issue by automating parameter range selection using real-world experience. While effective, these algorithms often require long computation time, as a new policy is trained from scratch every iteration. In this work, we propose Adaptive Bayesian Domain Randomization via Strategic Fine-tuning (BayRnTune), which inherits the spirit of BayRn but aims to significantly accelerate the learning processes by fine-tuning from previously learned policy. This idea leads to a critical question: which previous policy should we use as a prior during fine-tuning? We investigated four different fine-tuning strategies and compared them against baseline algorithms in five simulated environments, ranging from simple benchmark tasks to more complex legged robot environments. Our analysis demonstrates that our method yields better rewards in the same amount of timesteps compared to vanilla domain randomization or Bayesian DR.},
  archive   = {C_IROS},
  author    = {Tianle Huang and Nitish Sontakke and K. Niranjan Kumar and Irfan Essa and Stefanos Nikolaidis and Dennis W. Hong and Sehoon Ha},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802145},
  month     = {10},
  pages     = {670-676},
  title     = {BayRnTune: Adaptive bayesian domain randomization via strategic fine-tuning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model-based policy optimization using symbolic world model.
<em>IROS</em>, 664–669. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The application of learning-based control methods in robotics presents significant challenges. One is that model-free reinforcement learning algorithms use observation data with low sample efficiency. To address this challenge, a prevalent approach is model-based reinforcement learning, which involves employing an environment dynamics model. We suggest approximating transition dynamics with symbolic expressions, which are generated via symbolic regression. Approximation of a mechanical system with a symbolic model has fewer parameters than approximation with neural networks, which can potentially lead to higher accuracy and quality of extrapolation. We use a symbolic dynamics model to generate trajectories in model-based policy optimization to improve the sample efficiency of the learning algorithm. We evaluate our approach across various tasks within simulated environments. Our method demonstrates superior sample efficiency in these tasks compared to model-free and model-based baseline methods.},
  archive   = {C_IROS},
  author    = {Andrey Gorodetskiy and Konstantin Mironov and Aleksandr Panov},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801405},
  month     = {10},
  pages     = {664-669},
  title     = {Model-based policy optimization using symbolic world model},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Task and domain adaptive reinforcement learning for robot
control. <em>IROS</em>, 656–663. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep reinforcement learning (DRL) has shown remarkable success in simulation domains, yet its application in designing robot controllers remains limited, due to its singletask orientation and insufficient adaptability to environmental changes. To overcome these limitations, we present a novel adaptive agent that leverages transfer learning techniques to dynamically adapt policy in response to different tasks and environmental conditions. The approach is validated through the blimp control challenge, where multitasking capabilities and environmental adaptability are essential. The agent is trained using a custom, highly parallelized simulator built on IsaacGym. We perform zero-shot transfer to fly the blimp in the real world to solve various tasks. We share our code at https://github.com/robot-perception-group/adaptive_agent/.},
  archive   = {C_IROS},
  author    = {Yu Tang Liu and Nilaksh Singh and Aamir Ahmad},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801963},
  month     = {10},
  pages     = {656-663},
  title     = {Task and domain adaptive reinforcement learning for robot control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DEAR: Disentangled environment and agent representations for
reinforcement learning without reconstruction. <em>IROS</em>, 650–655.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement Learning (RL) algorithms can learn robotic control tasks from visual observations, but they often require a large amount of data, especially when the visual scene is complex and unstructured. In this paper, we explore how the agent’s knowledge of its shape can improve the sample efficiency of visual RL methods. We propose a novel method, Disentangled Environment and Agent Representations (DEAR), that uses the segmentation mask of the agent as supervision to learn disentangled representations of the environment and the agent through feature separation constraints. Unlike previous approaches, DEAR does not require reconstruction of visual observations. These representations are then used as an auxiliary loss to the RL objective, encouraging the agent to focus on the relevant features of the environment. We evaluate DEAR on two challenging benchmarks: Distracting DeepMind control suite and Franka Kitchen manipulation tasks. Our findings demonstrate that DEAR surpasses state-of-the-art methods in sample efficiency, achieving comparable or superior performance with reduced parameters. Our results indicate that integrating agent knowledge into visual RL methods has the potential to enhance their learning efficiency and robustness.},
  archive   = {C_IROS},
  author    = {Ameya Pore and Riccardo Muradore and Diego Dall’Alba},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801730},
  month     = {10},
  pages     = {650-655},
  title     = {DEAR: Disentangled environment and agent representations for reinforcement learning without reconstruction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical consensus-based multi-agent reinforcement
learning for multi-robot cooperation tasks. <em>IROS</em>, 642–649. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In multi-agent reinforcement learning (MARL), the Centralized Training with Decentralized Execution (CTDE) framework is pivotal but struggles due to a gap: global state guidance in training versus reliance on local observations in execution, lacking global signals. Inspired by human societal consensus mechanisms, we introduce the Hierarchical Consensus-based Multi-Agent Reinforcement Learning (HC-MARL) framework to address this limitation. HC-MARL employs contrastive learning to foster a global consensus among agents, enabling cooperative behavior without direct communication. This approach enables agents to form a global consensus from local observations, using it as an additional piece of information to guide collaborative actions during execution. To cater to the dynamic requirements of various tasks, consensus is divided into multiple layers, encompassing both short-term and long-term considerations. Short-term observations prompt the creation of an immediate, low-layer consensus, while long-term observations contribute to the formation of a strategic, high-layer consensus. This process is further refined through an adaptive attention mechanism that dynamically adjusts the influence of each consensus layer. This mechanism optimizes the balance between immediate reactions and strategic planning, tailoring it to the specific demands of the task at hand. Extensive experiments and real-world applications in multi-robot systems showcase our framework’s superior performance, marking significant advancements over baselines.},
  archive   = {C_IROS},
  author    = {Pu Feng and Junkang Liang and Size Wang and Xin Yu and Xin Ji and Yiting Chen and Kui Zhang and Rongye Shi and Wenjun Wu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802212},
  month     = {10},
  pages     = {642-649},
  title     = {Hierarchical consensus-based multi-agent reinforcement learning for multi-robot cooperation tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deeper introspective SLAM: How to avoid tracking failures
over longer routes? <em>IROS</em>, 635–641. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large scale active exploration has recently revealed limitations of visual SLAM’s tracking ability. Active view planning methods based on reinforcement learning have been proposed to improve visual tracking robustness.In this work, we expose the limitations of deep reinforcement learning-based visual SLAM over longer routes. We demonstrate that additional modalities (depth, scene layout) offer little improvement. Furthermore, reward shaping is not the main reason behind the shortsightedness of the state-of-the-art visual SLAM tracker. We propose a novel video vision transformer-based architecture that improves the farsightedness of the visual tracker, which results in the completion of longer routes with efficient paths.Out of 60 challenging routes, our approach manages to complete 56 routes, which is a three-fold improvement over the state-of-the-art active view mapping (DI-SLAM) baseline. Interestingly, ORB-SLAM3 was unable to complete a single route without tracking failure. Our code is available at https://tinyurl.com/w935spuz.},
  archive   = {C_IROS},
  author    = {Kanwal Naveed and Muhammad Latif Anjum and Wajahat Hussain and Donghwan Lee},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801911},
  month     = {10},
  pages     = {635-641},
  title     = {Deeper introspective SLAM: How to avoid tracking failures over longer routes?},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Benchmarking smoothness and reducing high-frequency
oscillations in continuous control policies. <em>IROS</em>, 627–634. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning (RL) policies are prone to high-frequency oscillations, especially undesirable when deploying to hardware in the real-world. In this paper, we identify, categorize, and compare methods from the literature that aim to mitigate high-frequency oscillations in deep RL. We define two broad classes: loss regularization and architectural methods. At their core, these methods incentivize learning a smooth mapping, such that nearby states in the input space produce nearby actions in the output space. We present benchmarks in terms of policy performance and control smoothness on traditional RL environments from the Gymnasium and a complex manipulation task, as well as three robotics locomotion tasks that include deployment and evaluation with real-world hardware. Finally, we also propose hybrid methods that combine elements from both loss regularization and architectural methods. We find that the best-performing hybrid outperforms other methods, and improves control smoothness by 26.8% over the baseline, with a worst-case performance degradation of just 2.8%.},
  archive   = {C_IROS},
  author    = {Guilherme Christmann and Ying-Sheng Luo and Hanjaya Mandala and Wei-Chao Chen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802057},
  month     = {10},
  pages     = {627-634},
  title     = {Benchmarking smoothness and reducing high-frequency oscillations in continuous control policies},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Meta SAC-lag: Towards deployable safe reinforcement learning
via MetaGradient-based hyperparameter tuning. <em>IROS</em>, 619–626.
(<a href="https://doi.org/10.1109/IROS58592.2024.10802547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safe Reinforcement Learning (Safe RL) is one of the prevalently studied subcategories of trial-and-error-based methods with the intention to be deployed on real-world systems. In safe RL, the goal is to maximize reward performance while minimizing constraints, often achieved by setting bounds on constraint functions and utilizing the Lagrangian method. However, deploying Lagrangian-based safe RL in real-world scenarios is challenging due to the necessity of threshold fine-tuning, as imprecise adjustments may lead to suboptimal policy convergence. To mitigate this challenge, we propose a unified Lagrangian-based model-free architecture called Meta Soft Actor-Critic Lagrangian (Meta SAC-Lag). Meta SAC-Lag uses meta-gradient optimization to automatically update the safety-related hyperparameters. The proposed method is designed to address safe exploration and threshold adjustment with minimal hyperparameter tuning requirement. In our pipeline, the inner parameters are updated through the conventional formulation and the hyperparameters are adjusted using the meta-objectives which are defined based on the updated parameters. Our results show that the agent can reliably adjust the safety performance due to the relatively fast convergence rate of the safety threshold. We evaluate the performance of Meta SAC-Lag in five simulated environments against Lagrangian baselines, and the results demonstrate its capability to create synergy between parameters, yielding better or competitive results. Furthermore, we conduct a real-world experiment involving a robotic arm tasked with pouring coffee into a cup without spillage. Meta SAC-Lag is successfully trained to execute the task, while minimizing effort constraints. The success of Meta SAC-Lag in performing the experiment is intended to be a step toward practical deployment of safe RL algorithms to learn the control process of safety-critical real-world systems without explicit engineering.},
  archive   = {C_IROS},
  author    = {Homayoun Honari and Amir M. Soufi Enayati and Mehran Ghafarian Tamizi and Homayoun Najjaran},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802547},
  month     = {10},
  pages     = {619-626},
  title     = {Meta SAC-lag: Towards deployable safe reinforcement learning via MetaGradient-based hyperparameter tuning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards accurate and robust dynamics and reward modeling for
model-based offline inverse reinforcement learning. <em>IROS</em>,
611–618. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper enhances model-based offline inverse reinforcement learning (IRL) by refining conservative Markov decision process (MDP) frameworks, traditionally employing uncertainty penalties to deter exploitation in uncertain areas. Existing methods, dependent on neural network ensembles to model MDP dynamics and quantify uncertainty through ensemble prediction heuristics, face limitations: they presume Gaussian-distributed state transitions, leading to simplified environmental representations. Additionally, ensemble modeling often results in high variance, indicating potential overfitting and a lack of generalizability. Moreover, the heuristic reliance for uncertainty quantification struggles to fully grasp environmental complexities, offering an incomplete foundation for informed decisions. Maintaining multiple models also demands substantial computational resources. Addressing these shortcomings, we propose leveraging score-based diffusion generative models for dynamic modeling. This method significantly broadens the scope of representable target distributions, surpassing Gaussian constraints. It not only improves the accuracy of transition modeling but also roots uncertainty quantification in diffusion models’ theoretical underpinnings, enabling more precise and dependable reward regularization. We further innovate by incorporating a transition stability regularizer (TSR) into the reward estimation. This novel element embeds stability into the reward learning process, diminishing the influence of transition variability and promoting more consistent policy optimization. Our empirical studies on diverse Mujoco robotic control tasks demonstrate that our diffusion-based methodology not only furnishes more accurate transition estimations but also surpasses conventional ensemble approaches in policy effectiveness. The addition of the TSR marks a distinctive advancement in offline IRL by enhancing the reward and policy learning efficacy. Code: https://github.com/GabrielZH/doc-irl.},
  archive   = {C_IROS},
  author    = {Gengyu Zhang and Yan Yan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802715},
  month     = {10},
  pages     = {611-618},
  title     = {Towards accurate and robust dynamics and reward modeling for model-based offline inverse reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gradient-based regularization for action smoothness in
robotic control with reinforcement learning. <em>IROS</em>, 603–610. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep Reinforcement Learning (DRL) has achieved remarkable success, ranging from complex computer games to real-world applications, showing the potential for intelligent agents capable of learning in dynamic environments. However, its application in real-world scenarios presents challenges, including the jerky problem, in which jerky trajectories not only compromise system safety but also increase power consumption and shorten the service life of robotic and autonomous systems. To address jerky actions, a method called conditioning for action policy smoothness (CAPS) was proposed by adding regularization terms to reduce the action changes. This paper further proposes a novel method, named Gradient-based CAPS (Grad-CAPS), that modifies CAPS by reducing the difference in the gradient of action and then uses displacement normalization to enable the agent to adapt to invariant action scales. Consequently, our method effectively reduces zigzagging action sequences while enhancing policy expressiveness and the adaptability of our method across diverse scenarios and environments. In the experiments, we integrated Grad-CAPS with different reinforcement learning algorithms and evaluated its performance on various robotic-related tasks in DeepMind Control Suite and OpenAI Gym environments. The results demonstrate that Grad-CAPS effectively improves performance while maintaining a comparable level of smoothness compared to CAPS and Vanilla agents.},
  archive   = {C_IROS},
  author    = {I Lee and Hoang-Giang Cao and Cong-Tinh Dao and Yu-Cheng Chen and I-Chen Wu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801464},
  month     = {10},
  pages     = {603-610},
  title     = {Gradient-based regularization for action smoothness in robotic control with reinforcement learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mitigating adversarial perturbations for deep reinforcement
learning via vector quantization. <em>IROS</em>, 595–602. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent studies reveal that well-performing reinforcement learning (RL) agents in training often lack resilience against adversarial perturbations during deployment. This highlights the importance of building a robust agent before deploying it in the real world. Most prior works focus on developing robust training-based procedures to tackle this problem, including enhancing the robustness of the deep neural network component itself or adversarially training the agent on strong attacks. In this work, we instead study an input transformation-based defense for RL. Specifically, we propose using a variant of vector quantization (VQ) as a transformation for input observations, which is then used to reduce the space of adversarial attacks during testing, resulting in the transformed observations being less affected by attacks. Our method is computationally efficient and seamlessly integrates with adversarial training, further enhancing the robustness of RL agents against adversarial attacks. Through extensive experiments in multiple environments, we demonstrate that using VQ as the input transformation effectively defends against adversarial attacks on the agent’s observations.},
  archive   = {C_IROS},
  author    = {Tung M. Luu and Thanh Nguyen and Tee Joshua Tian Jin and Sungwoon Kim and Chang D. Yoo},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802066},
  month     = {10},
  pages     = {595-602},
  title     = {Mitigating adversarial perturbations for deep reinforcement learning via vector quantization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Image-based deep reinforcement learning with intrinsically
motivated stimuli: On the execution of complex robotic tasks.
<em>IROS</em>, 587–594. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement Learning (RL) has been widely used to solve tasks where the environment consistently provides a dense reward value. However, in real-world scenarios, rewards can often be poorly defined or sparse. Auxiliary signals are indispensable for discovering efficient exploration strategies and aiding the learning process. In this work, inspired by intrinsic motivation theory, we postulate that the intrinsic stimuli of novelty and surprise can assist in improving exploration in complex, sparsely rewarded environments. We introduce a novel sample-efficient method able to learn directly from pixels, an image-based extension of TD3 with an autoencoder called NaSA-TD3. The experiments demonstrate that NaSA-TD3 is easy to train and an efficient method for tackling complex continuous-control robotic tasks, both in simulated environments and real-world settings. NaSA-TD3 outperforms existing state-of-the-art RL image-based methods in terms of final performance without requiring pre-trained models or human demonstrations.},
  archive   = {C_IROS},
  author    = {David Valencia and Henry Williams and Yuning Xing and Trevor Gee and Minas Liarokapis and Bruce A. MacDonald},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801857},
  month     = {10},
  pages     = {587-594},
  title     = {Image-based deep reinforcement learning with intrinsically motivated stimuli: On the execution of complex robotic tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bi-CL: A reinforcement learning framework for robots
coordination through bi-level optimization. <em>IROS</em>, 581–586. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In multi-robot systems, achieving coordinated missions remains a significant challenge due to the coupled nature of coordination behaviors and the lack of global information for individual robots. To mitigate these challenges, this paper introduces a novel approach, Bi-level Coordination Learning (Bi-CL), that leverages a bi-level optimization structure within a CTDE paradigm. Our bi-level reformulation decomposes the original problem into a reinforcement learning level with reduced action space, and an imitation learning level that gains demonstrations from a global optimizer. Bi-CL further integrates an alignment penalty mechanism, aiming to minimize the discrepancy between the two levels without degrading their training efficiency. We introduce a running example to conceptualize the problem formulation. Simulation results demonstrate that Bi-CL can learn more efficiently and achieve comparable performance with traditional multi-agent reinforcement learning baselines for multi-robot coordination.},
  archive   = {C_IROS},
  author    = {Zechen Hu and Daigo Shishika and Xuesu Xiao and Xuan Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801728},
  month     = {10},
  pages     = {581-586},
  title     = {Bi-CL: A reinforcement learning framework for robots coordination through bi-level optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ag2Manip: Learning novel manipulation skills with
agent-agnostic visual and action representations. <em>IROS</em>,
573–580. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous robotic systems capable of learning novel manipulation tasks are poised to transform industries from manufacturing to service automation. However, current methods (e.g., VIP and R3M) still face significant hurdles, notably the domain gap among robotic embodiments and the sparsity of successful task executions within specific action spaces, resulting in misaligned and ambiguous task representations. We introduce Ag2Manip (Agent-Agnostic representations for Manipulation), a framework aimed at addressing these challenges through two key innovations: (1) an agent-agnostic visual representation derived from human manipulation videos, with the specifics of embodiments obscured to enhance generalizability; and (2) an agent-agnostic action representation abstracting a robot’s kinematics to a universal agent proxy, emphasizing crucial interactions between end-effector and object. Ag2Manip has been empirically validated across simulated benchmarks, showing a 325% performance increase without relying on domain-specific demonstrations. Ablation studies further underline the essential contributions of the agent-agnostic visual and action representations to this success. Extending our evaluations to the real world, Ag2Manip significantly improves imitation learning success rates from 50% to 77.5%, demonstrating its effectiveness and generalizability across both simulated and real environments.},
  archive   = {C_IROS},
  author    = {Puhao Li and Tengyu Liu and Yuyang Li and Muzhi Han and Haoran Geng and Shu Wang and Yixin Zhu and Song-Chun Zhu and Siyuan Huang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801835},
  month     = {10},
  pages     = {573-580},
  title     = {Ag2Manip: Learning novel manipulation skills with agent-agnostic visual and action representations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kinematics-aware trajectory generation and prediction with
latent stochastic differential modeling. <em>IROS</em>, 565–572. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trajectory generation and trajectory prediction are two critical tasks in autonomous driving, which generate various trajectories for testing during development and predict the trajectories of surrounding vehicles during operation, respectively. In recent years, emerging data-driven deep learning-based methods have shown great promise for these two tasks in learning various traffic scenarios and improving average performance without assuming physical models. However, it remains a challenging problem for these methods to ensure that the generated/predicted trajectories are physically realistic. This challenge arises because learning-based approaches often function as opaque black boxes and do not adhere to physical laws. Conversely, existing model-based methods provide physically feasible results but are constrained by predefined model structures, limiting their capabilities to address complex scenarios. To address the limitations of these two types of approaches, we propose a new method that integrates kinematic knowledge into neural stochastic differential equations (SDE) and designs a variational autoencoder based on this latent kinematics-aware SDE (LK-SDE) to generate vehicle motions. Experimental results demonstrate that our method significantly outperforms both model-based and learning-based baselines in producing physically realistic and precisely controllable vehicle trajectories. Additionally, it performs well in predicting unobservable physical variables in the latent space.},
  archive   = {C_IROS},
  author    = {Ruochen Jiao and Yixuan Wang and Xiangguo Liu and Simon Sinong Zhan and Chao Huang and Qi Zhu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802438},
  month     = {10},
  pages     = {565-572},
  title     = {Kinematics-aware trajectory generation and prediction with latent stochastic differential modeling},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Disentangled acoustic fields for multimodal physical scene
understanding. <em>IROS</em>, 557–564. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of multimodal physical scene understanding, where an embodied agent needs to find fallen objects by inferring object properties, direction, and distance of an impact sound source. Previous works adopt feed-forward neural networks to directly regress the variables from sound, leading to poor generalization and domain adaptation issues. In this paper, we illustrate that learning a disentangled model of acoustic formation, referred to as disentangled acoustic field (DAF), to capture the sound generation and propagation process, enables the embodied agent to construct a spatial uncertainty map over where the objects may have fallen. We demonstrate that our analysis-by-synthesis framework can jointly infer sound properties by explicitly decomposing and factorizing the latent space of the disentangled model. We further show that the spatial uncertainty map can significantly improve the success rate for the localization of fallen objects by proposing multiple plausible exploration locations.},
  archive   = {C_IROS},
  author    = {Jie Yin and Andrew Luo and Yilun Du and Anoop Cherian and Tim K. Marks and Jonathan Le Roux and Chuang Gan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802429},
  month     = {10},
  pages     = {557-564},
  title     = {Disentangled acoustic fields for multimodal physical scene understanding},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforcement learning of dolly-in filming using a
ground-based robot. <em>IROS</em>, 549–556. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Free-roaming dollies enhance filmmaking with dynamic movement, but challenges in automated camera control remain unresolved. Our study advances this field by applying Reinforcement Learning (RL) to automate dolly-in shots using free-roaming ground-based filming robots, overcoming traditional control hurdles. We demonstrate the effectiveness of combined control for precise film tasks by comparing it to independent control strategies. Our robust RL pipeline surpasses traditional Proportional-Derivative controller performance in simulation and proves its efficacy in real-world tests on a modified ROSBot 2.0 platform equipped with a camera turret. This validates our approach’s practicality and sets the stage for further research in complex filming scenarios, contributing significantly to the fusion of technology with cinematic creativity. This work presents a leap forward in the field and opens new avenues for research and development, effectively bridging the gap between technological advancement and creative filmmaking.},
  archive   = {C_IROS},
  author    = {Philip Lorimer and Jack Saunders and Alan Hunter and Wenbin Li},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802717},
  month     = {10},
  pages     = {549-556},
  title     = {Reinforcement learning of dolly-in filming using a ground-based robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Waypoint-based reinforcement learning for robot manipulation
tasks. <em>IROS</em>, 541–548. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot arms should be able to learn new tasks. One framework here is reinforcement learning, where the robot is given a reward function that encodes the task, and the robot autonomously learns actions to maximize its reward. Existing approaches to reinforcement learning often frame this problem as a Markov decision process, and learn a policy (or a hierarchy of policies) to complete the task. These policies reason over hundreds of fine-grained actions that the robot arm needs to take: e.g., moving slightly to the right or rotating the end-effector a few degrees. But the manipulation tasks that we want robots to perform can often be broken down into a small number of high-level motions: e.g., reaching an object or turning a handle. In this paper we therefore propose a waypoint-based approach for model-free reinforcement learning. Instead of learning a low-level policy, the robot now learns a trajectory of waypoints, and then interpolates between those waypoints using existing controllers. Our key novelty is framing this waypoint-based setting as a sequence of multi-armed bandits: each bandit problem corresponds to one waypoint along the robot’s motion. We theoretically show that an ideal solution to this reformulation has lower regret bounds than standard frameworks. We also introduce an approximate posterior sampling solution that builds the robot’s motion one waypoint at a time. Results across benchmark simulations and two real-world experiments suggest that this proposed approach learns new tasks more quickly than state-of-the-art baselines. See our website here: https://collab.me.vt.edu/rl-waypoints/},
  archive   = {C_IROS},
  author    = {Shaunak A. Mehta and Soheil Habibian and Dylan P. Losey},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802681},
  month     = {10},
  pages     = {541-548},
  title     = {Waypoint-based reinforcement learning for robot manipulation tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PolyFit: A peg-in-hole assembly framework for unseen polygon
shapes via sim-to-real adaptation. <em>IROS</em>, 533–540. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The study addresses the foundational and challenging task of peg-in-hole assembly in robotics, where misalignments caused by sensor inaccuracies and mechanical errors often result in insertion failures or jamming. This research introduces PolyFit, representing a paradigm shift by transitioning from a reinforcement learning approach to a supervised learning methodology. PolyFit is a Force/Torque (F/T)-based supervised learning framework designed for 5-DoF peg-in-hole assembly. It utilizes F/T data for accurate extrinsic pose estimation and adjusts the peg pose to rectify misalignments. Extensive training in a simulated environment involves a dataset encompassing a diverse range of peg-hole shapes, extrinsic poses, and their corresponding contact F/T readings. The study proposes a sim-to-real adaptation method for real-world application, using a sim-real paired dataset to enable effective generalization to complex and unseen polygon shapes. Real-world evaluations demonstrate substantial success rates of 96.7% and 91.3%, highlighting the robustness and adaptability of the proposed method. Videos of data generation and experiments are available online at https://sites.google.com/view/polyfit-peginhole.},
  archive   = {C_IROS},
  author    = {Geonhyup Lee and Joosoon Lee and Sangjun Noh and Minhwan Ko and Kangmin Kim and Kyoobin Lee},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802554},
  month     = {10},
  pages     = {533-540},
  title     = {PolyFit: A peg-in-hole assembly framework for unseen polygon shapes via sim-to-real adaptation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). FI-SLAM: Feature fusion and instance reconstruction for
neural implicit SLAM. <em>IROS</em>, 527–532. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advancements in neural implicit fields for Simultaneous Localization and Mapping (SLAM) have provided breakthroughs. However, the benefits of reconstruction results to the perception ability of robot are minimal. Therefore, we propose FI-SLAM, a dense semantic instance SLAM system based on neural implicit representation, which significantly aids robots in better understanding the scene. FI-SLAM employs a coordinate and plane joint encoding method, which reduces the difficulty of feature storage by flattening the feature space. Furthermore, to improve representation efficiency, we use the method of adjacent feature level linear interpolation to describe features. We propose a feature fusion (FF) method to merge the object features with the scene features. The fused feature vector enhances the reconstruction accuracy of the local scene while ensuring the global reconstruction effect. It has improved the global reconstruction effect of the scene and the accuracy of camera tracking. Numerous experiments on synthetic and real-world datasets demonstrate that our method can assure accurate tracking precision, high-fidelity reconstruction results, and complete semantic instance maps. In summary, the algorithm we proposed heavily augments the scene perception capabilities of robot.},
  archive   = {C_IROS},
  author    = {Xingshuo Wang and Yunzhou Zhang and Zhiyao Zhang and Mengting Wang and Zhiteng Li and Xuanhua Chen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802296},
  month     = {10},
  pages     = {527-532},
  title     = {FI-SLAM: Feature fusion and instance reconstruction for neural implicit SLAM},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Event-based few-shot fine-grained human action recognition.
<em>IROS</em>, 519–526. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Few-shot fine-grained human (FGH) action recognition is crucial in the context of human-robot interaction within open-set real-world environments. Existing works mainly focus on features extracted from RGB frames. However, their performances are drastically impacted in challenging scenarios, such as high-dynamic or low lighting conditions. Event cameras can independently and sparsely capture brightness changes in a scene at microsecond resolution and high dynamic range, which offer a promising solution. However, the modality differences between events and RGB frames, and the lack of paired fine-grained data hinder the development of event-based FGH action recognition. Therefore, in this paper, we introduce the first Event Camera Fine-grained Human Action (E-FAction) dataset. This dataset comprises 3304 paired ‘event stream and RGB sequence’, covering 15 coarse action classes and 128 fine-grained actions. Then, we develop a versatile event feature extractor. Considering the spatial sparsity of event stream, we design two modules to mine the temporal motion and semantic features under the guidance of paired RGB frames, facilitating robust weight initialization for the feature extractor in few-shot FGH action recognition. We conduct extensive experiments on both published and our built synthetic and real datasets, and consistently achieve state-of-the-art performance compared to existing baselines. Code and dataset will be available at link.},
  archive   = {C_IROS},
  author    = {Zonglin Yang and Yan Yang and Yuheng Shi and Hao Yang and Ruikun Zhang and Liu Liu and Xinxiao Wu and Liyuan Pan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801824},
  month     = {10},
  pages     = {519-526},
  title     = {Event-based few-shot fine-grained human action recognition},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Loss distillation via gradient matching for point cloud
completion with weighted chamfer distance. <em>IROS</em>, 511–518. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D point clouds enhanced the robot’s ability to perceive the geometrical information of the environments, making it possible for many downstream tasks such as grasp pose detection and scene understanding. The performance of these tasks, though, heavily relies on the quality of data input, as incomplete can lead to poor results and failure cases. Recent training loss functions designed for deep learning-based point cloud completion, such as Chamfer distance (CD) and its variants (e.g. HyperCD [1]), imply a good gradient weighting scheme can significantly boost performance. However, these CD-based loss functions usually require data-related parameter tuning, which can be time-consuming for data-extensive tasks. To address this issue, we aim to find a family of weighted training losses (weighted CD) that requires no parameter tuning. To this end, we propose a search scheme, Loss Distillation via Gradient Matching, to find good candidate loss functions by mimicking the learning behavior in backpropagation between HyperCD and weighted CD. Once this is done, we propose a novel bilevel optimization formula to train the backbone network based on the weighted CD loss. We observe that: (1) with proper weighted functions, the weighted CD can always achieve similar performance to HyperCD, and (2) the Landau weighted CD, namely Landau CD, can outperform HyperCD for point cloud completion and lead to new state-of-the-art results on several benchmark datasets. Our demo code is available at https://github.com/Zhang-VISLab/IROS2024-LossDistillationWeightedCD.},
  archive   = {C_IROS},
  author    = {Fangzhou Lin and Haotian Liu and Haoying Zhou and Songlin Hou and Kazunori D Yamada and Gregory S. Fischer and Yanhua Li and Haichong K. Zhang and Ziming Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801828},
  month     = {10},
  pages     = {511-518},
  title     = {Loss distillation via gradient matching for point cloud completion with weighted chamfer distance},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An LSTM-based model to recognize driving style and predict
acceleration. <em>IROS</em>, 504–510. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To ensure safe cooperative driving in mixed traffic with both manned and unmanned vehicles, it is crucial to understand and model the driving styles of human drivers. This paper explores how to develop accurate recognition of driving style and use that for the prediction of vehicle motion, which enables better performance in cooperative driving. A simulation testbed that consists of a driving simulator and a copilot is first introduced for the purpose of data collection and testing. A Long Short-Term Memory (LSTM)-based network that models human driving styles and predicts driving acceleration is developed. Standalone tests are conducted to examine the model performance in the simulation testbed. Finally, the model is evaluated in a series of merging experiments that involves 5 vehicles.},
  archive   = {C_IROS},
  author    = {Jiaxing Lu and Sanzida Hossain and Weihua Sheng and He Bai},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802611},
  month     = {10},
  pages     = {504-510},
  title     = {An LSTM-based model to recognize driving style and predict acceleration},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time coordinated motion generation: A hierarchical deep
predictive learning model for bimanual tasks. <em>IROS</em>, 496–503.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots that autonomously operate in human living environments require the ability to adapt to unpredictable changes and flexibly handle a variety of tasks. Particularly, coordinated bimanual motions are essential for enabling tasks that are difficult with just one hand, such as grasping bulky objects, transporting heavy loads, and precision work. Traditional methods of generating robot motions typically involve executing pre-programmed motions, making it challenging to adapt to complex and unpredictable environmental changes. To address this issue, our research focuses on generating diverse motions that can flexibly adapt to environmental changes based on Deep Predictive Learning from a small amount of real-world data. Previous Deep Predictive Learning models have generated the motions of a robot’s left and right arms by a single LSTM, making it difficult to operate them independently. Therefore, we propose a new Hierarchical Deep Predictive Learning model specialized for generating coordinated bimanual motions. This model comprises three components: a Left-LSTM, which learns the body and visual information on the robot’s left side, a Right-LSTM that performs a similar function for the right side, and a Union-LSTM which integrates this information at a higher level. To verify the effectiveness of the proposed model, we conducted bimanual grasping experiments with multiple different objects using two different robots. The experimental results showed that independent of hardware, our model demonstrated a higher success rate compared to the traditional approach, indicating its enhanced capability in coordinating bimanual motions.},
  archive   = {C_IROS},
  author    = {Genki Shikada and Simon Armleder and Hiroshi Ito and Gordon Cheng and Tetsuya Ogata},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801317},
  month     = {10},
  pages     = {496-503},
  title     = {Real-time coordinated motion generation: A hierarchical deep predictive learning model for bimanual tasks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving out-of-distribution generalization of trajectory
prediction for autonomous driving via polynomial representations.
<em>IROS</em>, 488–495. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robustness against Out-of-Distribution (OoD) samples is a key performance indicator of a trajectory prediction model. However, the development and ranking of state-of-the-art (SotA) models are driven by their In-Distribution (ID) performance on individual competition datasets. We present an OoD testing protocol that homogenizes datasets and prediction tasks across two large-scale motion datasets. We introduce a novel prediction algorithm based on polynomial representations for agent trajectory and road geometry on both the input and output sides of the model. With a much smaller model size, training effort, and inference time, we reach near SotA performance for ID testing and significantly improve robustness in OoD testing. Within our OoD testing protocol, we further study two augmentation strategies of SotA models and their effects on model generalization. Highlighting the contrast between ID and OoD performance, we suggest adding OoD testing to the evaluation criteria of trajectory prediction models.},
  archive   = {C_IROS},
  author    = {Yue Yao and Shengchao Yan and Daniel Goehring and Wolfram Burgard and Joerg Reichardt},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801535},
  month     = {10},
  pages     = {488-495},
  title     = {Improving out-of-distribution generalization of trajectory prediction for autonomous driving via polynomial representations},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VoxelContrast: Voxel contrast-based unsupervised learning
for 3D point clouds. <em>IROS</em>, 480–487. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The annotation process for 3D point cloud data is more complex than for image data, and training with a small amount of annotated data significantly reduces the performance of deep learning models. Unsupervised learning can better utilize large amounts of unlabeled point cloud data for model pretraining, thereby achieving excellent performance on small-scale datasets. However, many existing 3D point cloud unsupervised learning methods are primarily focused on single-object CAD point clouds and may not be suitable for larger-scale autonomous driving LiDAR point clouds. To address this challenging problem, we propose a voxel contrast-based unsupervised learning method (VoxelContrast), which adapts well to different types of point cloud data through voxelization and can be seamlessly integrated with existing model frameworks. Specifically, we utilize voxelization methods to preprocess point cloud data. Then, we incorporate voxel information into contrastive learning, facilitating the creation of more meaningful positive and negative sample pairs. Finally, we conduct unsupervised training of the model using instance discrimination as the proxy task. Our method was validated in two downstream tasks: point cloud shape classification and 3D object detection. Experimental results demonstrated that models pretrained using a substantial amount of unlabeled data can further enhance the effectiveness of existing supervised learning methods.},
  archive   = {C_IROS},
  author    = {Yuxiang Qin and Hao Sun},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802435},
  month     = {10},
  pages     = {480-487},
  title     = {VoxelContrast: Voxel contrast-based unsupervised learning for 3D point clouds},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active propulsion noise shaping for multi-rotor aircraft
localization. <em>IROS</em>, 472–479. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-rotor aerial autonomous vehicles (MAVs) primarily rely on vision for navigation purposes. However, visual localization and odometry techniques suffer from poor performance in low or direct sunlight, a limited field of view, and vulnerability to occlusions. Acoustic sensing can serve as a complementary or even alternative modality for vision in many situations, and it also has the added benefits of lower system cost and energy footprint, which is especially important for micro aircraft. This paper proposes actively controlling and shaping the aircraft propulsion noise generated by the rotors to benefit localization tasks, rather than considering it a harmful nuisance. We present a neural network architecture for self-noise-based localization in a known environment. We show that training it simultaneously with learning time-varying rotor phase modulation achieves accurate and robust localization. The proposed methods are evaluated using a computationally affordable simulation of MAV rotor noise in 2D acoustic environments that is fitted to real recordings of rotor pressure fields. Code3 and data4 are accompanied.},
  archive   = {C_IROS},
  author    = {Gabriele Serussi and Tamir Shor and Tom Hirshberg and Chaim Baskin and Alex M. Bronstein},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802115},
  month     = {10},
  pages     = {472-479},
  title     = {Active propulsion noise shaping for multi-rotor aircraft localization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Binary amplitude-only hologram generation for acoustic
end-effector design by physics-based deep learning. <em>IROS</em>,
466–471. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Acoustic holography has emerged as a cutting-edge technique for constructing a micro-robot acoustic end-effector for non-contact manipulation. As one of typical implementations of acoustic holography, Binary Amplitude-Only Hologram (BAOH) featured with a simple structure provides an efficient alternative for modulating acoustic fields that support micro-robotic manipulation. In the present study, we propose a deep learning based BAOH generation method for constructing precise and high-resolution end-effector based on acoustic field. Specifically, we model the BAOH generation problem into an optimization framework. The framework combines an acoustic wave propagation model with the deep neural network, in favor of bypassing the laborious collection of labeled data and facilitating the model to learn the inverse mapping. Additionally, to address the issues of gradient invalidation and information loss caused by binarization, the framework uses an adaptive binarization layer consisting of differentiable binarization and adaptive threshold automatically learned during training, which facilitates to realize end-to-end optimization and increase the non-linear capacity of the model. The simulation experiments show that the proposed method is capable to predict BAOH that supports precise, robust, versatile and real-time construction of acoustic end-effector, enjoying broad prospects in various applications related to micro-robotic manipulation.},
  archive   = {C_IROS},
  author    = {Qing Liu and Hu Su and Jiaqi Li and Youfu Li and Zhiyuan Zhang and Song Liu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801787},
  month     = {10},
  pages     = {466-471},
  title     = {Binary amplitude-only hologram generation for acoustic end-effector design by physics-based deep learning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). X-neuron: Interpreting, locating and editing of neurons in
reinforcement learning policy. <em>IROS</em>, 458–465. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite the impressive performance of Reinforcement Learning (RL), the black-box neural network backbone hinders users from trusting and deploying trained agents in real-world applications where safety is crucial. In order to make agents more trustworthy and controllable, for a given RL-trained policy, we propose to enhance its interpretability and make it human-controllable without retraining. We accomplish this goal by following a 3-step pipeline: 1) We interpret neurons by analyzing the causal effect of neurons on the kinematic attributes; To help agents unlock novel skills and enable human to assist agents in accomplishing tasks, 2) we locate the X-neuron, the optimal neuron that is capable of evoking the desired behavior; 3) and edit its activation values to achieve the precise control. We evaluate our method on various RL tasks ranging from autonomous driving to robot locomotion, and the results display that our approach outperforms previous work regarding almost all evaluation metrics. Through enhancing interpretability and introducing human control, the agents can improve safety and performance, even in unseen environments and novel tasks. For locomotion robots simply trained to walk forward, our method unlocks diverse controllable behaviors ranging from jump to backflip.},
  archive   = {C_IROS},
  author    = {Yuhong Ge and Xun Zhao and Jiangmiao Pang and Mingguo Zhao and Dahua Lin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801727},
  month     = {10},
  pages     = {458-465},
  title     = {X-neuron: Interpreting, locating and editing of neurons in reinforcement learning policy},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uncertainty-aware semi-supervised semantic key point
detection via bundle adjustment. <em>IROS</em>, 450–457. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual relative localization is widely used in multi-robot systems. While semantic key points offer a promising solution for 6DoF pose estimation, manual data labeling for network training remains unavoidable. In this paper, we introduce a novel method that jointly estimates the semantic key point detection model and 6DoF camera pose. Our key idea is to leverage the 3D-2D projection to produce pseudo labels for detection model training while taking the key point predictions as landmarks for 6DoF camera pose estimation. Compared with state-of-the-art works, our method eliminates the need for calibration and time synchronization of multi-camera systems, requiring only a handful of manually labeled data, which significantly improves the training efficiency. The experiment validates the effectiveness and practicality of our method in public datasets and real-world robotic applications. Code and data are made available3.},
  archive   = {C_IROS},
  author    = {Kai Li and Yin Zhang and Shiyu Zhao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801395},
  month     = {10},
  pages     = {450-457},
  title     = {Uncertainty-aware semi-supervised semantic key point detection via bundle adjustment},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gaining the sparse rewards by exploring lottery tickets in
spiking neural networks. <em>IROS</em>, 442–449. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deploying energy-efficient deep learning algorithms on computational-limited devices, such as robots, is still a pressing issue for real-world applications. Spiking Neural Networks (SNNs), a novel brain-inspired algorithm, offer a promising solution due to their low-latency and low-energy properties over traditional Artificial Neural Networks (ANNs). Despite their advantages, the dense structure of deep SNNs can still result in extra energy consumption. The Lottery Ticket Hypothesis (LTH) posits that within dense neural networks, there exist winning Lottery Tickets (LTs), namely sub-networks, that can be obtained without compromising performance. Inspired by this, this paper delves into the spiking-based LTs (SLTs), examining their unique properties and potential for extreme efficiency. Then, two significant sparse Rewards are gained through comprehensive explorations and meticulous experiments on SLTs across various dense structures. Moreover, a sparse algorithm tailored for spiking transformer structure, which incorporates convolution operations into the Patch Embedding Projection (ConvPEP) module, has been proposed to achieve Multi-level Sparsity (MultiSp). MultiSp refers to (1) Patch number sparsity; (2) ConvPEP weights sparsity and binarization; and (3) ConvPEP activation layer binarization. Extensive experiments demonstrate that our method achieves extreme sparsity with only a slight performance decrease, paving the way for deploying energy-efficient neural networks in robotics and beyond.},
  archive   = {C_IROS},
  author    = {Hao Cheng and Jiahang Cao and Erjia Xiao and Mengshu Sun and Renjing Xu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802854},
  month     = {10},
  pages     = {442-449},
  title     = {Gaining the sparse rewards by exploring lottery tickets in spiking neural networks},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Reinforcement learning with generalizable gaussian
splatting. <em>IROS</em>, 435–441. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An excellent representation is crucial for reinforcement learning (RL) performance, especially in vision-based reinforcement learning tasks. The quality of the environment representation directly influences the achievement of the learning task. Previous vision-based RL typically uses explicit or implicit ways to represent environments, such as images, points, voxels, and neural radiance fields. However, these representations contain several drawbacks. They cannot either describe complex local geometries or generalize well to unseen scenes, or require precise foreground masks. Moreover, these implicit neural representations are akin to a &quot;black box&quot;, significantly hindering interpretability. 3D Gaussian Splatting (3DGS), with its explicit scene representation and differentiable rendering nature, is considered a revolutionary change for reconstruction and representation methods. In this paper, we propose a novel Generalizable Gaussian Splatting framework to be the representation of RL tasks, called GSRL. Through validation in the RoboMimic environment, our method achieves better results than other baselines in multiple tasks, improving the performance by 10%, 44%, and 15% compared with baselines on the hardest task. This work is the first attempt to leverage generalizable 3DGS as a representation for RL.},
  archive   = {C_IROS},
  author    = {Jiaxu Wang and Qiang Zhang and Jingkai Sun and Jiahang Cao and Gang Han and Wen Zhao and Weining Zhang and Yecheng Shao and Yijie Guo and Renjing Xu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801348},
  month     = {10},
  pages     = {435-441},
  title     = {Reinforcement learning with generalizable gaussian splatting},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FlowTrack: Point-level flow network for 3D single object
tracking. <em>IROS</em>, 427–434. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D single object tracking (SOT) is a crucial task in fields of mobile robotics and autonomous driving. Traditional motion-based approaches achieve target tracking by estimating the relative movement of target between two consecutive frames. However, they usually overlook local motion information of the target and fail to exploit historical frame information effectively. To overcome the above limitations, we propose a point-level flow method with multi-frame information for 3D SOT task, called FlowTrack. Specifically, by estimating the flow for each point in the target, our method could capture the local motion details of target, thereby improving the tracking performance. Meanwhile, to handle scenes with sparse points, we present a learnable target feature as the bridge to efficiently integrate target information from past frames. Moreover, we design a Instance Flow Head to transform dense point-level flow into instance-level motion, effectively aggregating local motion information to obtain global target motion. Finally, our method achieves competitive performance with improvements of 5.9% on the KITTI and 2.9% on the NuScenes, compared to the next best method.},
  archive   = {C_IROS},
  author    = {Shuo Li and Yubo Cui and Zhiheng Li and Zheng Fang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801484},
  month     = {10},
  pages     = {427-434},
  title     = {FlowTrack: Point-level flow network for 3D single object tracking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Contrastive mask denoising transformer for 3D instance
segmentation. <em>IROS</em>, 419–426. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In transformer-based methods for point cloud instance segmentation, bipartite matching is used to establish one-to-one correspondences between predictions and ground truths. However, in early training stages, matches can be unstable and inconsistent between epochs, requiring the model to frequently adjust its learning path, thus reducing the quality of model convergence. To address this challenge, we propose the contrastive mask denoising transformer for 3D instance segmentation, which utilizes a mask denoising module to guide the model towards a more stable optimization path in early training stages. Furthermore, we introduce a multi-pattern-aware query selection module to assist the model learn multiple patterns at one position such that clustered objects can be discerned. In addition, the proposed modules are &quot;plug and play&quot;, which can easily be integrated into transformer-based architectures. Experimental results on ScanNetv2 dataset show that the proposed modules improve the performance of multiple pipelines, notably achieving +1.0 mAP on the main pipeline.},
  archive   = {C_IROS},
  author    = {He Wang and Minshen Lin and Guofeng Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802860},
  month     = {10},
  pages     = {419-426},
  title     = {Contrastive mask denoising transformer for 3D instance segmentation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simultaneous super-resolution and depth estimation for
satellite images based on diffusion model. <em>IROS</em>, 411–418. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Satellite images provide an effective way to observe the earth surface on a large scale. 3D landscape models can provide critical structural information, such as forestry and crop growth. However, there has been very limited research to estimate the depth and the 3D models of the earth based on satellite images. LiDAR measurements on satellites are usually quite sparse. RGB images have higher resolution than LiDAR, but there has been little research on 3D surface measurements based on satellite RGB images. In comparison with in-situ sensing, satellite RGB images are usually low resolution. In this research, we explore the method that can enhance the satellite image resolution to generate super-resolution images and then conduct depth estimation and 3D reconstruction based on higher-resolution satellite images. Leveraging the strong generation capability of diffusion models, we developed a simultaneous diffusion model learning framework that can train diffusion models for both super-resolution images and depth estimation. With the super-resolution images and the corresponding depth maps, 3D surface reconstruction models with detailed landscape information can be generated. We evaluated the proposed methodology on multiple satellite datasets for both super-resolution and depth estimation tasks, which have demonstrated the effectiveness of our methodology.},
  archive   = {C_IROS},
  author    = {Yuwei Zhou and Yangming Lee},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802345},
  month     = {10},
  pages     = {411-418},
  title     = {Simultaneous super-resolution and depth estimation for satellite images based on diffusion model},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VIHE: Virtual in-hand eye transformer for 3D robotic
manipulation. <em>IROS</em>, 403–410. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we introduce the Virtual In-Hand Eye Transformer (VIHE), a novel method designed to enhance 3D manipulation capabilities through action-aware view rendering. VIHE autoregressively refines actions in multiple stages by conditioning on rendered views posed from action predictions in the earlier stages. These virtual in-hand views provide a strong inductive bias for effectively recognizing the correct pose for the hand, especially for challenging high-precision tasks such as peg insertion. On 18 manipulation tasks in RLBench simulated environments, VIHE achieves a new state-of-the-art, with a 12% absolute improvement, increasing from 65% to 77% over the existing state-of-the-art model using 100 demonstrations per task. In real-world scenarios, VIHE can learn manipulation tasks with just a handful of demonstrations, highlighting its practical utility. Videos and code implementation can be found at our project site: https://vihe-3d.github.io.},
  archive   = {C_IROS},
  author    = {Weiyao Wang and Yutian Lei and Shiyu Jin and Gregory D. Hager and Liangjun Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802366},
  month     = {10},
  pages     = {403-410},
  title     = {VIHE: Virtual in-hand eye transformer for 3D robotic manipulation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). STAIR: Semantic-targeted active implicit reconstruction.
<em>IROS</em>, 395–402. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many autonomous robotic applications require object-level understanding when deployed. Actively reconstructing objects of interest, i.e. objects with specific semantic meanings, is therefore relevant for a robot to perform downstream tasks in an initially unknown environment. In this work, we propose a novel framework for semantic-targeted active reconstruction using posed RGB-D measurements and 2D semantic labels as input. The key components of our framework are a semantic implicit neural representation and a compatible planning utility function based on semantic rendering and uncertainty estimation, enabling adaptive view planning to target objects of interest. Our planning approach achieves better reconstruction performance in terms of mesh and novel view rendering quality compared to implicit reconstruction baselines that do not consider semantics for view planning. Our framework further outperforms a state-of-the-art semantic-targeted active reconstruction pipeline based on explicit maps, justifying our choice of utilising implicit neural representations to tackle semantic-targeted active reconstruction problems.},
  archive   = {C_IROS},
  author    = {Liren Jin and Haofei Kuang and Yue Pan and Cyrill Stachniss and Marija Popović},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801401},
  month     = {10},
  pages     = {395-402},
  title     = {STAIR: Semantic-targeted active implicit reconstruction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sim-to-real domain shift in online action detection.
<em>IROS</em>, 388–394. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human reasoning comprises the ability to understand and reason about the current action solely based on past information. To provide effective assistance in an eldercare or household environment an assistive robot or intelligent assistive system has to assess human actions correctly. Based on this presumption, the task of online action detection determines the current action solely based on the past without access to future information. During inference, the performance of the model is largely impacted by the attributes of the underlying training dataset. However, as high costs and ethical concerns are associated with the real-world data collection process, synthetically created data provides a way to mitigate these problems while providing additional data for the training process of the underlying action detection model to improve performanceDue to the inherent domain shift between the synthetic and real data, we introduce a new egocentric dataset called Human Kitchen Interactions (HKI) to investigate the sim-to-real gap. Our dataset contains in total 100 synthetic and real videos in which 21 different actions are executed in a kitchen environment. The synthetic data is acquired in an egocentric virtual reality (VR) setup while capturing the virtual environment in a game engine. We evaluate state-of-the-art online action detection models on our dataset and provide insights into sim-to-real domain shift. Upon acceptance, we will release our dataset and the corresponding features at https://c-patsch.github.io/HKI/.},
  archive   = {C_IROS},
  author    = {Constantin Patsch and Wael Torjmene and Marsil Zakour and Yuankai Wu and Driton Salihu and Eckehard Steinbach},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802421},
  month     = {10},
  pages     = {388-394},
  title     = {Sim-to-real domain shift in online action detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning concept-based causal transition and symbolic
reasoning for visual planning. <em>IROS</em>, 380–387. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual planning simulates how humans make decisions to achieve desired goals in the form of searching for visual causal transitions between an initial visual state and a final visual goal state. It has become increasingly important in egocentric vision with its advantages in guiding agents to perform daily tasks in complex environments. In this paper, we propose an interpretable and generalizable visual planning framework consisting of i) a novel Substitution-based Concept Learner (SCL) that abstracts visual inputs into disentangled concept representations, ii) symbol abstraction and reasoning that performs task planning via the learned symbols, and iii) a Visual Causal Transition model (ViCT) that grounds visual causal transitions to semantically similar real-world actions. Given an initial state, we perform goal-conditioned visual planning with a symbolic reasoning method fueled by the learned representations and causal transitions to reach the goal state. To verify the effectiveness of the proposed model, we collect a large-scale visual planning dataset based on AI2-THOR, dubbed as CCTP. Extensive experiments on this challenging dataset demonstrate the superior performance of our method in visual planning. Empirically, we show that our framework can generalize to unseen task trajectories, unseen object categories, and real-world data. Further details of this work are provided at https://fqyqc.github.io/ConTranPlan/.},
  archive   = {C_IROS},
  author    = {Yilue Qian and Peiyu Yu and Ying Nian Wu and Yao Su and Wei Wang and Lifeng Fan},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802420},
  month     = {10},
  pages     = {380-387},
  title     = {Learning concept-based causal transition and symbolic reasoning for visual planning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TD-NeRF: Novel truncated depth prior for joint camera pose
and neural radiance field optimization. <em>IROS</em>, 372–379. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The reliance on accurate camera poses is a significant barrier to the widespread deployment of Neural Radiance Fields (NeRF) models for 3D reconstruction and SLAM tasks. The existing method introduces monocular depth priors to jointly optimize the camera poses and NeRF, which fails to fully exploit the depth priors and neglects the impact of their inherent noise. In this paper, we propose Truncated Depth NeRF (TD-NeRF), a novel approach that enables training NeRF from unknown camera poses - by jointly optimizing learnable parameters of the radiance field and camera poses. Our approach explicitly utilizes monocular depth priors through three key advancements: 1) we propose a novel depth-based ray sampling strategy based on the truncated normal distribution, which improves the convergence speed and accuracy of pose estimation; 2) to circumvent local minima and refine depth geometry, we introduce a coarse-to-fine training strategy that progressively improves the depth precision; 3) we propose a more robust inter-frame point constraint that enhances robustness against depth noise during training. The experimental results on three datasets demonstrate that TD-NeRF achieves superior performance in the joint optimization of camera pose and NeRF, surpassing prior works, and generates more accurate depth geometry. The implementation of our method has been released at https://github.com/nubot-nudt/TD-NeRF.},
  archive   = {C_IROS},
  author    = {Zhen Tan and Zongtan Zhou and Yangbing Ge and Zi Wang and Xieyuanli Chen and Dewen Hu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802634},
  month     = {10},
  pages     = {372-379},
  title     = {TD-NeRF: Novel truncated depth prior for joint camera pose and neural radiance field optimization},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual imitation learning of task-oriented object grasping
and rearrangement. <em>IROS</em>, 364–371. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Task-oriented object grasping and rearrangement are key skills for robots, which have to perform versatile real-world manipulation tasks. However, they remain challenging due to partial observations of the objects and shape variations in categorical objects. In this paper, we present the Multi-feature Implicit Model (MIMO), a novel object representation that encodes multiple spatial features between a point and an object in an implicit neural field. Training such a model on multiple features ensures that it embeds the object shapes consistently in different aspects, thus improving its performance in object shape reconstruction from partial observation, shape similarity measure, and modeling spatial relations between objects. Based on MIMO, we propose a framework to learn task-oriented object grasping and rearrangement from single or multiple human demonstration videos. The evaluations in simulation show that our approach outperforms the state-of-the-art methods for multi- and single-view observations. Real-world experiments demonstrate the efficacy of our approach in one- and few-shot imitation learning of manipulation tasks.},
  archive   = {C_IROS},
  author    = {Yichen Cai and Jianfeng Gao and Christoph Pohl and Tamim Asfour},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801466},
  month     = {10},
  pages     = {364-371},
  title     = {Visual imitation learning of task-oriented object grasping and rearrangement},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DailySTR: A daily human activity pattern recognition dataset
for spatio-temporal reasoning. <em>IROS</em>, 357–363. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recognizing daily human activities is essential for domestic robots to assist humans effectively in indoor environments. These activities typically involve sequences of interactions between humans and objects across different locations and times within a household. Identifying these events and understanding their temporal and spatial relationships is crucial for accurately modeling human behavior patterns. However, most current methods and datasets for human activity recognition focus on identifying singular events at specific moments and locations, neglecting the complexity of activities that span multiple times and places. To address this gap, we collected data on human activity patterns over a single day through crowdsourcing. Based on this, we introduce a novel synthetic video question-answering dataset. Our proposed dataset includes videos of daily activities accompanied by question-answer pairs that require models to reason about sequences of activities in both time and space. We evaluated state-of-the-art methods against our dataset, highlighting their limitations in handling the intricate spatio-temporal dynamics of human activity sequences. To improve upon these methods, we propose a two-stage model. The proposed model initially decodes the detailed content of individual videos using a transformer-based approach, then employs LLMs for advanced spatio-temporal reasoning across multiple videos. We hope our research provides valuable benchmarks and insights, paving the way for advancements in the recognition of daily human activity patterns.},
  archive   = {C_IROS},
  author    = {Yue Qiu and Shusaku Egami and Ken Fukuda and Natsuki Miyata and Takuma Yagi and Kensho Hara and Kenji Iwata and Ryusuke Sagawa},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802775},
  month     = {10},
  pages     = {357-363},
  title     = {DailySTR: A daily human activity pattern recognition dataset for spatio-temporal reasoning},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BEV-ODOM: Reducing scale drift in monocular visual odometry
with BEV representation. <em>IROS</em>, 349–356. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Monocular visual odometry (MVO) is vital in autonomous navigation and robotics, providing a cost-effective and flexible motion tracking solution, but the inherent scale ambiguity in monocular setups often leads to cumulative errors over time. In this paper, we present BEV-ODOM, a novel MVO framework leveraging the Bird’s Eye View (BEV) Representation to address scale drift. Unlike existing approaches, BEV-ODOM integrates a depth-based perspective-view (PV) to BEV encoder, a correlation feature extraction neck, and a CNN-MLP-based decoder, enabling it to estimate motion across three degrees of freedom without the need for depth supervision or complex optimization techniques. Our framework reduces scale drift in long-term sequences and achieves accurate motion estimation across various datasets, including NCLT, Oxford, and KITTI. The results indicate that BEV-ODOM outperforms current MVO methods, demonstrating reduced scale drift and higher accuracy.},
  archive   = {C_IROS},
  author    = {Yufei Wei and Sha Lu and Fuzhang Han and Rong Xiong and Yue Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801996},
  month     = {10},
  pages     = {349-356},
  title     = {BEV-ODOM: Reducing scale drift in monocular visual odometry with BEV representation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Masked mutual guidance transformer tracking. <em>IROS</em>,
341–348. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual mask learning has received increasing attention in the field of visual object tracking. However, most existing studies merely utilize visual mask learning works as pre-training models without fully exploiting their potential for visual representation. In this paper, we present a novel approach for learning tracking target features, leveraging an encoder-decoder architecture with a masked mutual guidance tracking(MMG). Initially, we perform joint visual feature extraction on both the template and search areas. Subsequently, these features undergo separate self-decoding processes, followed by mutual guidance decoding to reconstruct the original search and template images. This process fosters mutual understanding between the images, facilitating improved learning of object states and shapes across different frames. During the inference phase, we offload the decoder and implement a simple and effective tracker. Experimental results indicate that our proposed method is effective that the mutual guidance strategy can achieve state-of-the-art performance on five tracking datasets.},
  archive   = {C_IROS},
  author    = {Baojie Fan and Zhiquan Wang and Jiajun Ai and Caiyu Zhang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801735},
  month     = {10},
  pages     = {341-348},
  title     = {Masked mutual guidance transformer tracking},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DECADE: Towards designing efficient-yet-accurate distance
estimation modules for collision avoidance in mobile advanced driver
assistance systems. <em>IROS</em>, 334–340. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The proliferation of smartphones and other mobile devices provides a unique opportunity to make Advanced Driver Assistance Systems (ADAS) accessible to everyone in the form of an application empowered by low-cost Machine/Deep Learning (ML/DL) models to enhance road safety. For the critical feature of Collision Avoidance in Mobile ADAS, lightweight Deep Neural Networks (DNN) for object detection exist, but conventional pixel-wise depth/distance estimation DNNs are vastly more computationally expensive making them unsuitable for a real-time application on resource-constrained devices. In this paper, we present a distance estimation model, DECADE, that processes each detector output instead of constructing pixel-wise depth/disparity maps. In it, we propose a pose estimation DNN to estimate allocentric orientation of detections to supplement the distance estimation DNN in its prediction of distance using bounding box features. We demonstrate that these modules can be attached to any detector to extend object detection with fast distance estimation. Evaluation of the proposed modules with attachment to and fine-tuning on the outputs of the YOLO object detector on the KITTI 3D Object Detection dataset achieves state-of-the-art performance with 1.38 meters in Mean Absolute Error and 7.3% in Mean Relative Error in the distance range of 0-150 meters. Our extensive evaluation scheme not only evaluates class-wise performance, but also evaluates range-wise accuracy especially in the critical range of 0-70m.},
  archive   = {C_IROS},
  author    = {Muhammad Zaeem Shahzad and Muhammad Abdullah Hanif and Muhammad Shafique},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801667},
  month     = {10},
  pages     = {334-340},
  title     = {DECADE: Towards designing efficient-yet-accurate distance estimation modules for collision avoidance in mobile advanced driver assistance systems},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Construction of musculoskeletal simulation for shoulder
complex with ligaments and its validation via model predictive control.
<em>IROS</em>, 327–333. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The complex ways in which humans utilize their bodies in sports and martial arts are remarkable, and human motion analysis is one of the most effective tools for robot body design and control. On the other hand, motion analysis is not easy, and it is difficult to measure complex body motions in detail due to the influence of numerous muscles and soft tissues, mainly ligaments. In response, various musculoskeletal simulators have been developed and applied to motion analysis and robotics. However, none of them reproduce the ligaments but only the muscles, nor do they focus on the shoulder complex, including the clavicle and scapula, which is one of the most complex parts of the body. Therefore, in this study, a detailed simulation model of the shoulder complex including ligaments is constructed. The model will mimic not only the skeletal structure and muscle arrangement but also the ligament arrangement and maximum muscle strength. Through model predictive control based on the constructed simulation, we confirmed that the ligaments contribute to joint stabilization in the first movement and that the proper distribution of maximum muscle force contributes to the equalization of the load on each muscle, demonstrating the effectiveness of this simulation.},
  archive   = {C_IROS},
  author    = {Yuta Sahara and Akihiro Miki and Yoshimoto Ribayashi and Shunnosuke Yoshimura and Kento Kawaharazuka and Kei Okada and Masayuki Inaba},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802465},
  month     = {10},
  pages     = {327-333},
  title     = {Construction of musculoskeletal simulation for shoulder complex with ligaments and its validation via model predictive control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A robust visual SLAM system for small-scale quadruped
robots in dynamic environments. <em>IROS</em>, 321–326. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a robust visual SLAM system designed for small-scale quadruped robots (ViQu-SLAM) for accurate localization, especially to mitigate the issue of erroneous data association caused by moving objects in dynamic environments. The proposed approach leverages a selfadaptive framework that integrates semantic segmentation with alterations in the spatial location of categorized map points. Besides, combination of leg odometry derived from forward kinematics with IMU provides scale information for positional transformations between keyframes, thus optimizing the overall localization accuracy of quadruped robots. At last, we performed evaluation across various stages and the results demonstrate competitive performance, with 53.16% reduction in average absolute trajectory error compared to that of ORB-SLAM3 in dynamic benchmark datasets. As a result, ViQu-SLAM, including visual and IMU-fused leg odometry, exhibits promising results on a small quadruped robot, reducing positioning errors in dynamic scenes by an average of 29.36% compared to existing state-of-the-art methods.},
  archive   = {C_IROS},
  author    = {Chengyang Li and Yulai Zhang and Zhiqiang Yu and Xinming Liu and Qing Shi},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802163},
  month     = {10},
  pages     = {321-326},
  title     = {A robust visual SLAM system for small-scale quadruped robots in dynamic environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An ejecting system for autonomous takeoff of flapping-wing
robots. <em>IROS</em>, 315–320. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous takeoff of flapping-wing robots (FWRs) is crucial for accelerating response speed and reducing costs in executing tasks. Jumping-aided takeoff is an effective method adopted by birds. However, the limited power density of motors poses challenges in achieving this type of takeoff for FWRs. In this study, we introduce a ground-based FWR ejecting system that utilizes a symmetric slider-crank mechanism (S-SCM) to store energy in spring. This stored energy is then converted into the takeoff speed of the FWR. The dynamic model of each working stage is established, and the design parameters are optimized according to simulation results. The prototype of the FWR ejecting system is fabricated for experimental validations. The results indicate that the system can provide a takeoff speed of 4 m/s for the 270 g FWR. Notably, the system is deployable on rough terrains and only adds a 3.2 g payload to the robot. Our work advances the autonomous takeoff of FWRs, promoting the application of such robots.},
  archive   = {C_IROS},
  author    = {Xu Jiang and Jun Zhang and Aiguo Song},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802796},
  month     = {10},
  pages     = {315-320},
  title     = {An ejecting system for autonomous takeoff of flapping-wing robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Loco-manipulation with nonimpulsive contact-implicit
planning in a slithering robot. <em>IROS</em>, 309–314. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Object manipulation has been extensively studied in the context of fixed base and mobile manipulators. However, the overactuated locomotion modality employed by snake robots allows for a unique blend of object manipulation through locomotion, referred to as loco-manipulation. The following work presents an optimization approach to solving the loco-manipulation problem based on non-impulsive implicit contact path planning for our snake robot COBRA. We present the mathematical framework and show high-fidelity simulation results and experiments to demonstrate the effectiveness of our approach.},
  archive   = {C_IROS},
  author    = {Adarsh Salagame and Kruthika Gangaraju and Harin Kumar Nallaguntla and Eric Sihite and Gunar Schirner and Alireza Ramezani},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802450},
  month     = {10},
  pages     = {309-314},
  title     = {Loco-manipulation with nonimpulsive contact-implicit planning in a slithering robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An agile robotic penguin driven by submersible geared
servomotors: Various maneuvers by active feathering of the wings.
<em>IROS</em>, 303–308. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study introduces an agile robotic penguin featuring a pair of 2-degrees of freedom (DoF) wing mechanisms. Each wing can independently control flapping and feathering motions with two in-house submersible geared servomotors through a differential gear mechanism. Notably, our mechanism allows unrestricted feathering beyond 360°. Since feathering directly changes the wing’s angle of attack (AoA), the hydrodynamic forces can be significantly adjusted to achieve agile maneuvers. This paper demonstrates various maneuvers, including rapid acceleration, hard braking, rolling, pitching, and yawing, achieved solely by changing the feathering motion of both wings. Our robotic penguin reached a maximum forward speed of 1.8 m/s, comparable to the foraging speed of real penguins. The average roll, pitch, and yaw rates were 363°/s, 75°/s, and 92°/s, respectively. This robot serves as a model for the biological study of maneuverability in real penguins and the engineering exploration of bioinspired agile underwater robots.},
  archive   = {C_IROS},
  author    = {Taiki Shimooka and Atsushi Kakogawa and Hiroto Tanaka},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801509},
  month     = {10},
  pages     = {303-308},
  title     = {An agile robotic penguin driven by submersible geared servomotors: Various maneuvers by active feathering of the wings},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An active and dexterous bionic torso for a quadruped robot*.
<em>IROS</em>, 295–302. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The torso of quadruped mammals serves as a robust foundation for their bodies, enabling a diverse array of agile and intricate movements. However, current quadruped robots cannot match the extensive range of motion exhibited by biological torsos while also serving a load-bearing role. Therefore, this paper presents an active bionic torso that emulates the quadrupedal animal&#39;s spinal column and associated muscular structure. This innovative torso can mimic animal torsos movements, including flexing, extending, lateral bending, and axial rotation. A thorough analysis of the torso&#39;s kinetic, workspace, and structural dynamics has been conducted. This proposed torso boasts a considerable load-bearing capacity and can support a load that exceeds its weight tenfold. The passive spring incorporated into the bionic torso emulates the intervertebral discs&#39; shock-absorbing and load-bearing functions. Additionally, this paper documents the development of a quadrupedal robot fitted with the proposed bionic torso, demonstrating the torso&#39;s mobility in a real-world application.},
  archive   = {C_IROS},
  author    = {Ruyue Li and Yaguang Zhu and Yuntong Wang and Zhimin He and Mengnan Zhou},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801439},
  month     = {10},
  pages     = {295-302},
  title     = {An active and dexterous bionic torso for a quadruped robot*},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tension feedback control for musculoskeletal quadrupedal
locomotion over uneven terrain. <em>IROS</em>, 288–294. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Musculoskeletal quadruped robots driven by pneumatic artificial muscles (PAMs) have great softness. Due to the softness, the proprioceptive information of PAMs (e.g. tension) reflects the environmental information. However, how to utilize this information for stable quadrupedal gait has been rarely explored. In this work, we utilized PAM tension for stable locomotion control over uneven terrain. We newly developed a durable tension sensor and proposed tension feedback control for quadruped locomotion over uneven terrain. Our proposed controller stabilizes the trunk posture by modulating the phase of the leg. To verify the effectiveness of the proposed controller, we implement it in a simple quadrupedal model and a musculoskeletal quadruped robot driven by PAMs. Through experiments, with tension feedback, the trunk posture oscillated more stably than that without the feedback. Furthermore, over uneven terrain, the running velocity with tension feedback was higher than that without the feedback in the robot experiment. These successful results will lead to more robust musculoskeletal quadruped robots that can be employed in the real-world environment.},
  archive   = {C_IROS},
  author    = {Hiroaki Tanaka and Ojiro Matsumoto and Takumi Kawasetsu and Koh Hosoda},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801663},
  month     = {10},
  pages     = {288-294},
  title     = {Tension feedback control for musculoskeletal quadrupedal locomotion over uneven terrain},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A biomimetic robot crawling upstream using adhesive suckers
inspired by net-winged midge larvae. <em>IROS</em>, 281–287. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Net-winged midge larvae (genus Liponeura) can achieve robust attachment and crawl on the slippery surface in the fast stream with their powerful abdominal suckers. The rigid spine-like structures distributed in the sucker cavity called microtrichia, have been proven to be crucial in the adhesion process. In this work, we carry out a design of the biomimetic sucker with the spine-like structures and then implement various tests using biomimetic suckers to verify the adhesion capacity enhancement brought by spine-like structures. Finally, we assemble the suckers in a quadruped crawling robot capable of locomotion in both aerial and aquatic environments with a speed of 56.4 mm/s (0.225 BL/s) and can crawl upstream against a turbulent flow with a speed of 38.2 mm/s (0.152 BL/s). This study will inspire biomimetic design in future robotics, and pave the way for future robots to realize long-term observation and monitoring in complex environments.},
  archive   = {C_IROS},
  author    = {Haoyuan Xu and Shuyong Zhao and Jiale Zhi and Chongze Bi and Li Wen},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802850},
  month     = {10},
  pages     = {281-287},
  title     = {A biomimetic robot crawling upstream using adhesive suckers inspired by net-winged midge larvae},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Climbing gait for a snake robot by adapting to a flexible
net. <em>IROS</em>, 275–280. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces climbing gait for the snake robot by adapting to a flexible net. A net deforms in various ways due to external forces. Therefore, we tilt snake robot’s body to adapt to the deformation of the net. In addition, we can adjust the position of the head of a snake robot passing through the mesh. A snake robot can move not only vertically but also horizontally. We demonstrated the validity of the proposed method through experiments in vertical movement and movement in diagonal direction.},
  archive   = {C_IROS},
  author    = {Kodai Yoshida and Motoyasu Tanaka},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802564},
  month     = {10},
  pages     = {275-280},
  title     = {Climbing gait for a snake robot by adapting to a flexible net},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A perceptive pneumatic artificial muscle empowered by
double helix fiber reinforcement. <em>IROS</em>, 269–274. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the last decades, soft robotics has been growing rapidly as an emerging research topic, bringing new paradigms for robotic manipulation, locomotion, and human‒machine interactions. Pneumatic artificial muscle is a powerful, lightweight, rapid response with great design flexibility, making it promising for developing biological muscle-like robotic systems. The PPAM is made of a silicone tube body with double helix coil fiber reinforcement. The double helix coil fiber restricts the radial expansion of the cylinder tube to achieve extension in actuation, and monitors the muscle length change in real time by measuring its inductance. A finite element model was built to simulate the actuation characteristics of the PPAM. A theoretical formula was derived to analyze the inductive length sensing response of the double-helix coil on the PPAM. It is verified that the PPAM can sense its length change regardless of whether it is caused by active driving or external manipulation. Rigorous testing reveals that PPAM has an ultrahigh length sensing resolution of 5.9 μm in relaxed state, with a short response time of 50 ms. The self-length sensing of PPAM is hysteresis free, and highly repeatable, showing no degradation in 1000 operation cycles. In summary, the PPAM shows promising features for developing the next-generation perceptive and responsive soft robots, intelligent hybrid robots, or safer biomedical instruments.},
  archive   = {C_IROS},
  author    = {Yufeng Wang and Houping Wu and Chenchen Li and Yulian Peng and Hongbo Wang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801695},
  month     = {10},
  pages     = {269-274},
  title     = {A perceptive pneumatic artificial muscle empowered by double helix fiber reinforcement},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CompdVision: Combining near-field 3D visual and tactile
sensing using a compact compound-eye imaging system. <em>IROS</em>,
262–268. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As automation technologies advance, the need for compact and multi-modal sensors in robotic applications is growing. To address this demand, we introduce CompdVision, a novel sensor that employs a compound-eye imaging system to combine near-field 3D visual and tactile sensing within a compact form factor. CompdVision utilizes two types of vision units to address diverse sensing needs, eliminating the need for complex modality conversion. Stereo units with far-focus lenses can see through the transparent elastomer for depth estimation beyond the contact surface. Simultaneously, tactile units with near-focus lenses track the movement of markers embedded in the elastomer to obtain contact deformation. Experimental results validate the sensor’s superior performance in 3D visual and tactile sensing, proving its capability for reliable external object depth estimation and precise measurement of tangential and normal contact forces. The dual modalities and compact design make the sensor a versatile tool for robotic manipulation.},
  archive   = {C_IROS},
  author    = {Lifan Luo and Boyang Zhang and Zhijie Peng and Yik Kin Cheung and Guanlan Zhang and Zhigang Li and Michael Yu Wang and Hongyu Yu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801461},
  month     = {10},
  pages     = {262-268},
  title     = {CompdVision: Combining near-field 3D visual and tactile sensing using a compact compound-eye imaging system},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Single actuator undulation soft-bodied robots using a
precompressed variable thickness flexible beam. <em>IROS</em>, 255–261.
(<a href="https://doi.org/10.1109/IROS58592.2024.10801330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft robots - due to their intrinsic flexibility of the body - can adaptively navigate unstructured environments. One of the most popular locomotion gaits that has been implemented in soft robots is undulation. The undulation motion in soft robots resembles the locomotion gait of stringy creatures such as snakes, eels, and C. Elegans. Typically, the implementation of undulation locomotion on a soft robot requires many actuators to control each segment of the stringy body. The added weight of multiple actuators limits the navigating performance of soft-bodied robots. In this paper, we propose a simple tendon-driven flexible beam with only one actuator (a DC motor) that can generate a mechanical traveling wave along the beam to support the undulation locomotion of soft robots. The beam will be precompressed along its axis by shortening the length of the two tendons to form an S-shape, thus pretensioning the tendons. The motor will wind and unwind the tendons to deform the flexible beam and generate traveling waves along the body of the robot. We experiment with different pre-tension to characterize the relationship between tendon pre-tension forces and the DC-motor winding/unwinding. Our proposal enables a simple implementation of undulation motion to support the locomotion of soft-bodied robots.},
  archive   = {C_IROS},
  author    = {Tung D. Ta},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801330},
  month     = {10},
  pages     = {255-261},
  title     = {Single actuator undulation soft-bodied robots using a precompressed variable thickness flexible beam},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PINN-ray: A physics-informed neural network to model soft
robotic fin ray fingers. <em>IROS</em>, 247–254. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modelling complex deformation for soft robotics provides a guideline to understand their behaviour, leading to safe interaction with the environment. However, building a surrogate model with high accuracy and fast inference speed can be challenging for soft robotics due to the nonlinearity from complex geometry, large deformation, material nonlinearity etc. The reality gap from surrogate models also prevents their further deployment in the soft robotics domain.In this study, we proposed a physics-informed Neural Networks (PINNs) named PINN-Ray to model complex deformation for a Fin Ray soft robotic gripper, which embeds the minimum potential energy principle from elastic mechanics and additional high-fidelity experimental data into the loss function of neural network for training. This method is significant in terms of its generalisation to complex geometry and robust to data scarcity as compared to other data-driven neural networks. Furthermore, it has been extensively evaluated to model the deformation of the Fin Ray finger under external actuation. PINN-Ray demonstrates improved accuracy as compared with Finite element modelling (FEM) after applying the data assimilation scheme to treat the sim-to-real gap. Additionally, we introduced our automated framework to design, fabricate soft robotic fingers, and characterise their deformation by visual tracking, which provides a guideline for the fast prototype of soft robotics.},
  archive   = {C_IROS},
  author    = {Xing Wang and Joel Janek Dabrowski and Josh Pinskier and Lois Liow and Vinoth Viswanathan and Richard Scalzo and David Howard},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802217},
  month     = {10},
  pages     = {247-254},
  title     = {PINN-ray: A physics-informed neural network to model soft robotic fin ray fingers},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Harnessing symmetry breaking in soft robotics: A novel
approach for underactuated fingers. <em>IROS</em>, 241–246. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft robotics, an emerging domain in modern robotics, introduces innovative possibilities alongside challenges in controllability, particularly with multi-degree inflatable actuators. We present a novel manipulation method using underactuated soft fingers that addresses these challenges by harnessing symmetry breaking. Central to our approach is the mechanism of self-organization within a ring actuator equipped with five fingers. Typically considered a drawback, we exploit the actuator’s buckling behavior to facilitate in-hand manipulation. This strategic utilization enables object motion in both clockwise and counterclockwise directions via system perturbations and adjustments in frequency and duty cycle parameters. Employing the self-organizing properties of our actuator, our method is empirically validated through simulations and real actuator experiments, demonstrating the system’s ability in manipulating objects by leveraging the inherent flexibility and morphological advantages. The design enables two degrees of freedom with minimal input, allowing objects to rotate due to the actuator’s self-organizing actions. This simplification of control mechanisms is essential for soft robotics manipulation. Our findings indicate that control systems in soft robotics can be significantly simplified, harnessing the adaptable behavior inherent in its morphology.},
  archive   = {C_IROS},
  author    = {Ryman Hashem and Toby Howison and Agostino Stilli and Danail Stoyanov and Weiliang Xu and Fumiya Iida},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802658},
  month     = {10},
  pages     = {241-246},
  title     = {Harnessing symmetry breaking in soft robotics: A novel approach for underactuated fingers},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Manta ray-inspired soft robotic swimmer for high-speed and
multi-modal swimming. <em>IROS</em>, 235–240. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Manta rays exhibit complex motion behavior through their flexible fins. This study proposes a novel manta ray-inspired soft robotic swimmer with bistable flapping wings for high-speed and multi-modal swimming. The wings are created with prestressed bistable composite and actuated by small McKibben artificial muscles. Pressurizing and depressurizing the McKibben actuators integrated into the flapping wings generates alternating snap-throughs between two stable states, yielding swimming. Experiments are set up and conducted to analyze how the robot responses vary as a function of input pressures and actuation frequencies for both bistable and monostable modes. Experimental results show that the highest swimming velocity is 0.58 body lengths (BL) per second (equivalent to 12.23 cm/s), and the maximum turning angle speed is 22.5° per second with a smaller turning radius by holding the fins in asymmetric positions for bistable modes. Multimodal swimming motions are achieved including forward and backward translating, turning, and flip-turning.},
  archive   = {C_IROS},
  author    = {Zefeng Xu and Jiaqiao Liang and Yitong Zhou},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802678},
  month     = {10},
  pages     = {235-240},
  title     = {Manta ray-inspired soft robotic swimmer for high-speed and multi-modal swimming},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling of hydraulic soft hand with rubber sheet reservoir
and evaluation of its grasping flexibility and control. <em>IROS</em>,
229–234. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In situations where robots work alongside humans, they must be capable of responding flexibly to unexpected external forces. To address this challenge, researchers have conducted numerous studies on soft robotics. However, most of the soft hands studied so far are powered by pneumatic pressure and can only exert pressure up to a several hundred kPa, resulting in low output. To solve this problem, we have developed a hydraulic soft hand in our previous research. In this paper, we derive the relationship between driving pressure, bending angle, and grasping force of a soft hand with a reservoir to evaluate the effect of a rubber sheet reservoir. Additionally, we experimentally show that the soft hand provides grasping flexibility when angle control is applied using the model proposed in this paper.},
  archive   = {C_IROS},
  author    = {Kyosuke Ishibashi and Hiroki Ishikawa and Osamu Azami and Ko Yamamoto},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802355},
  month     = {10},
  pages     = {229-234},
  title     = {Modeling of hydraulic soft hand with rubber sheet reservoir and evaluation of its grasping flexibility and control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A large vision-language model based environment perception
system for visually impaired people. <em>IROS</em>, 221–228. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It is a challenging task for visually impaired people to perceive their surrounding environment due to the complexity of the natural scenes. Their personal and social activities are thus highly limited. This paper introduces a Large Vision-Language Model(LVLM) based environment perception system which helps them to better understand the surrounding environment, by capturing the current scene they face with a wearable device, and then letting them retrieve the analysis results through the device. The visually impaired people could acquire a global description of the scene by long pressing the screen to activate the LVLM output, retrieve the categories of the objects in the scene resulting from a segmentation model by tapping or swiping the screen, and get a detailed description of the objects they are interested in by double-tapping the screen. To help visually impaired people more accurately perceive the world, this paper proposes incorporating the segmentation result of the RGB image as external knowledge into the input of LVLM to reduce the LVLM’s hallucination. Technical experiments on POPE, MME and LLaVA-QA90 show that the system could provide a more accurate description of the scene compared to Qwen-VL-Chat, exploratory experiments show that the system helps visually impaired people to perceive the surrounding environment effectively.},
  archive   = {C_IROS},
  author    = {Zezhou Chen and Zhaoxiang Liu and Kai Wang and Kohou Wang and Shiguo Lian},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801813},
  month     = {10},
  pages     = {221-228},
  title     = {A large vision-language model based environment perception system for visually impaired people},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Using hip assisted running exoskeleton with impact isolation
mechanism to improve energy efficiency. <em>IROS</em>, 214–220. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Research has indicated that exoskeletons can assist human movement, but due to the influence of additional weight and challenges in control strategy design, only a few exoskeletons effectively reduce the wearers’ metabolic costs during running. This paper proposes an innovative and efficient hip-assisted running exoskeleton (HARE) designed to facilitate the flexion and extension movements of the joint along the sagittal plane. In the field of structural engineering, we propose implementing an active-passive combination constant force suspension system, hereinafter referred to as CFS, to effectively mitigate the impact of inertial forces during running. The decoupled transmission mechanism allows the CFS and assist mechanisms to operate independently, ensuring the tension of the cables. The flexible structural design can reduce the locomotion limitation on human bodies and reduce the additional energy burden on the body. In control strategy designing, the joint torque-generating strategy provides personalized assistance strategies for wearers to actively optimize the control parameters. Meanwhile, the safety control strategy based on abnormal gait recognition can ensure human safety. Experiments have shown that compared to not wearing exoskeletons, this device can reduce the energy consumption of the human body by 5.33 % at a speed of 9 km/h. This demonstrates its potential in human motion assistance processes.},
  archive   = {C_IROS},
  author    = {Ziqi Wang and Junchen Liu and Hongwu Li and Qinghua Zhang and Xianglong Li and Yi Huang and Haotian Ju and Tianjiao Zheng and Jie Zhao and Yanhe Zhu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802632},
  month     = {10},
  pages     = {214-220},
  title     = {Using hip assisted running exoskeleton with impact isolation mechanism to improve energy efficiency},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Enhancing prosthetic safety and environmental adaptability:
A visual-inertial prosthesis motion estimation approach on uneven
terrains. <em>IROS</em>, 206–213. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Environment awareness is crucial for enhancing walking safety and stability of amputee wearing powered prosthesis when crossing uneven terrains such as stairs and obstacles. However, existing environmental perception systems for prosthesis only provide terrain types and corresponding parameters, which fail to prevent potential collisions when crossing uneven terrains and may lead to falls and other severe consequences. In this paper, a visual-inertial motion estimation approach is proposed for prosthesis to perceive its movement and the changes of spatial relationship between the prosthesis and uneven terrain when traversing them. To achieve this, we estimate the knee motion by utilizing a depth camera to perceive the environment and align feature points extracted from uneven terrains. Subsequently, an error-state Kalman filter is incorporated to fuse the inertial data into visual estimations to obtain a more robust and accurate estimation, which is then utilized to derive the motion of the whole prosthesis for our prosthetic control scheme. Experiments conducted on our collected dataset and stair walking trials with powered prosthesis show that the proposed method can accurately track the motion of human leg and the prosthesis with the average root-mean-square error of toe trajectory less than 5 cm. The proposed method is expected to enable the environmental adaptive control for prosthesis, thereby enhancing amputee’s safety and mobility in uneven terrains.},
  archive   = {C_IROS},
  author    = {Chuheng Chen and Xinxing Chen and Shucong Yin and Yuxuan Wang and Binxin Huang and Yuquan Leng and Chenglong Fu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801585},
  month     = {10},
  pages     = {206-213},
  title     = {Enhancing prosthetic safety and environmental adaptability: A visual-inertial prosthesis motion estimation approach on uneven terrains},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal integration of hybrid FES-exoskeleton for precise
knee trajectory control. <em>IROS</em>, 199–205. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a novel hybrid torque allocation method for improving wearability and mobility in integrated functional electrical stimulation (FES) of the quadriceps muscles and powered exoskeleton systems. Our proposed approach leverages a hierarchical closed-loop controller for knee joint position tracking while addressing limitations of powered exoskeletons and FES systems by reducing power consumption and battery size and by mitigating FES-induced muscle fatigue, respectively. The core component is a model-free optimization algorithm that dynamically distributes torque between FES and the exoskeleton by considering tracking error, effort, and the prediction of muscle fatigue in the cost function, computing allocation gain in an online manner. The online optimization approach interactively changes the optimal allocation gain by taking into account the instantaneous value of error and effort and also penalizing FES-induced fatigue, a common challenge in long-duration experiments. The results demonstrate that this dynamic allocation significantly improves system wearability by reducing power consumption without increasing muscle fatigue during the extension phase of walking. This hybrid control approach contributes to improving exoskeleton wearability and rehabilitation outcomes for individuals with SCI and mobility impairments, enhancing assistive technology and quality of life.},
  archive   = {C_IROS},
  author    = {Masoud Jafaripour and Vivian Mushahwar and Mahdi Tavakoli},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801382},
  month     = {10},
  pages     = {199-205},
  title     = {Optimal integration of hybrid FES-exoskeleton for precise knee trajectory control},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Functional kinematic and kinetic requirements of the upper
limb during activities of daily living: A recommendation on necessary
joint capabilities for prosthetic arms. <em>IROS</em>, 191–198. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Prosthetic limb abandonment remains an unsolved challenge as amputees consistently reject their devices. Current prosthetic designs often fail to balance human-like performance with acceptable device weight, highlighting the need for optimised designs tailored to modern tasks. This study aims to provide a comprehensive dataset of joint kinematics and kinetics essential for performing activities of daily living (ADL), thereby informing the design of more functional and user-friendly prosthetic devices. Functionally required Ranges of Motion (ROM), velocities, and torques for the Glenohumeral (rotation), elbow, Radioulnar, and wrist joints were computed using motion capture data from 12 subjects performing 24 ADLs. Our approach included the computation of joint torques for varying mass and inertia properties of the upper limb, while torques induced by the manipulation of experimental objects were considered by their interaction wrench with the subject’s hand. Joint torques pertaining to individual ADL scaled linearly with limb and object mass and mass distribution, permitting their generalisation to not explicitly simulated limb and object dynamics with linear regressors (LRM), exhibiting coefficients of determination R = 0.99 ± 0.01. Exemplifying an application of data-driven prosthesis design, we optimise wrist axes orientations for two serial and two differential joint configurations. Optimised axes reduced peak power requirements, compared to anatomical configurations, by exploiting high torque correlations (r = −0.84, p &lt; 0.05) between Ulnar deviation and wrist flexion/extension joints. This study offers critical insights into the functional requirements of upper limb prostheses, providing a valuable foundation for data-driven prosthetic design that addresses key user concerns and enhances device adoption.},
  archive   = {C_IROS},
  author    = {Christopher Herneth and Amartya Ganguly and Sami Haddadin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802080},
  month     = {10},
  pages     = {191-198},
  title     = {Functional kinematic and kinetic requirements of the upper limb during activities of daily living: A recommendation on necessary joint capabilities for prosthetic arms},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design of upper-limb exoskeleton with distal branching link
mechanism for bilateral operation of humanoid robots. <em>IROS</em>,
184. (<a href="https://doi.org/10.1109/IROS58592.2024.10802346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Exoskeletons for robot operation necessitate shoulders with high range of motions and high degrees of freedom to fit the operator’s shoulder girdle. These shoulder joints need high torque for force feedback on the operator. Existing exoskeletons struggle to simultaneously meet these requirements of high DOFs, wide ROM, and high torque due to spatial constraints. This study introduces an exoskeleton with a distal branching link mechanism that addresses this issue by concentrating on each link’s absolute and relative degrees of freedom. In the proposed exoskeleton, the end-effector’s absolute DOF, the forearm’s absolute DOF, and the end-effector and forearm’s relative DOF are matched between the operator and the exoskeleton. This is achieved while reducing the overall DOF by sharing the root link system’s DOF. Furthermore, by avoiding direct attachment of the operator to the exoskeleton’s shoulder, the design can accommodate the human shoulder’s high torque and high ROM. The study demonstrates that the branching exoskeleton outperforms existing link-fixed exoskeletons in terms of tracking the operator’s arms and the torque required by the exoskeleton’s joints. Utilizing this exoskeleton, we successfully maneuvered an actual humanoid robot to perform daily activities where the forearm posture is crucial.},
  archive   = {C_IROS},
  author    = {Hiroki Yoshioka and Naoki Hiraoka and Kunio Kojima and Kei Okada and Masayuki Inaba},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802346},
  month     = {10},
  pages     = {184},
  title     = {Design of upper-limb exoskeleton with distal branching link mechanism for bilateral operation of humanoid robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A tactile lightweight exoskeleton for teleoperation: Design
and control performance. <em>IROS</em>, 178–183. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, an upgraded exoskeleton design is presented with enhanced trajectory tracking and mechanical transparency. Compared to the first version, the design features a 3-DoF actuated shoulder joint and a mechanism to regulate the pretension of Bowden cables. Force/torque sensors are installed to directly measure the interaction forces between the human arm and the exoskeleton at the connecting points. Three control strategies were evaluated to follow a desired trajectory; A PD controller, a PD controller with friction observer, and an adaptive controller based on Radial Basis Function (RBF). These strategies also form the basis for an admittance control, aimed at improving the exoskeleton’s mechanical transparency during interaction with the human arm. Simulations and experimental results demonstrate that the PD control, supported by friction estimation via a momentum observer, achieves superior tracking performance. Moreover, the system’s mechanical transparency is enhanced using the admittance RBF-based controller, showing marginally superior results.},
  archive   = {C_IROS},
  author    = {Moein Forouhar and Hamid Sadeghian and Daniel Perez Suay and Abdeldjallil Naceri and Sami Haddadin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802732},
  month     = {10},
  pages     = {178-183},
  title     = {A tactile lightweight exoskeleton for teleoperation: Design and control performance},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An adaptive robotic exoskeleton for comprehensive
force-controlled hand rehabilitation. <em>IROS</em>, 170–177. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study presents the development and validation of an innovative hand exoskeleton designed for the re-habilitation of patients with Complex Regional Pain Syndrome (CRPS), a condition frequently arising post-injury or surgeries. The prototype is tailored for the hand, a region commonly affected by CRPS, and is notable for its adaptability and a comprehensive sensor system for monitoring individual joint movements. Reliable sensor performance was defined through precise force measurements and stability over time, showing minimal drift. These features enable personalized rehabilitation and objective progress tracking, addressing limitations in traditional physiotherapy such as availability, cost, and time constraints. The contributions of this work lie in its innovative design and the potential for robotic systems to improve therapeutic outcomes in CRPS rehabilitation.},
  archive   = {C_IROS},
  author    = {Nikolas Wilhelm and Victor Schaack and Annick Leisching and Carina Micheler and Sami Haddadin and Rainer Burgkart},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802093},
  month     = {10},
  pages     = {170-177},
  title     = {An adaptive robotic exoskeleton for comprehensive force-controlled hand rehabilitation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data-driven predictive control for robust exoskeleton
locomotion. <em>IROS</em>, 162–169. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Exoskeleton locomotion must be robust while being adaptive to different users with and without payloads. To address these challenges, this work introduces a data-driven predictive control (DDPC) framework to synthesize walking gaits for lower-body exoskeletons, employing Hankel matrices and a state transition matrix for its data-driven model. The proposed approach leverages DDPC through a multi-layer architecture. At the top layer, DDPC serves as a planner employing Hankel matrices and a state transition matrix to generate a data-driven model that can learn and adapt to varying users and payloads. At the lower layer, our method incorporates inverse kinematics and passivity-based control to map the planned trajectory from DDPC into the full-order states of the lower-body exoskeleton. We validate the effectiveness of this approach through numerical simulations and hardware experiments conducted on the Atalante lower-body exoskeleton with different payloads. Moreover, we conducted a comparative analysis against the model predictive control (MPC) framework based on the reduced-order linear inverted pendulum (LIP) model. Through this comparison, the paper demonstrates that DDPC enables robust bipedal walking at various velocities while accounting for model uncertainties and unknown perturbations.},
  archive   = {C_IROS},
  author    = {Kejun Li and Jeeseop Kim and Xiaobin Xiong and Kaveh Akbari Hamed and Yisong Yue and Aaron D. Ames},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802759},
  month     = {10},
  pages     = {162-169},
  title     = {Data-driven predictive control for robust exoskeleton locomotion},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating the impact of a semi-autonomous interface on
configuration space accessibility for multi-DOF upper limb prostheses.
<em>IROS</em>, 154–161. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Powered upper limb prostheses offer a particularly interesting case of human-machine interaction, where the user and the robot are physically coupled as an open chain manipulator. The biological and mechanical degrees of freedom (DOF) must collaborate for the user to manipulate objects in the environment. Current state-of-the-art systems use machine learning models to classify electromyogram (EMG) signals into motion intent primitives, allowing users to move the prosthetic joints sequentially at a fixed velocity. This interface is intended to work for simple systems but does not extend well into higher DOF. Consequently, current commercially available systems are limited to 1 or 2 powered DOF. In this paper, we present a semi-autonomous (SA) hybrid gaze-EMG interface that allows users to command the device in task-space instead of joint-space. Target end-effector poses are selected by tracking the user’s gaze vector, and then EMG signals guide the prosthetic along a calculated trajectory towards that pose. To examine how prosthesis interface performance scales with available mechanical DOF, we had 4 subjects complete virtual pick and place tasks with SA and traditional controller interfaces, varying the available DOF in the prosthetic wrist. Our results show that with the SA interface, increased DOF leads to a significant (p≤0.05) reduction in compensatory motion of the upper arm, more effective (p≤0.01) utilization of the increased configuration space, and overall more efficient motion (p≤0.01) than traditional classification based interfaces. These findings indicate that when given SA interfaces, subjects can benefit from fully articulated prosthetic devices, which motivates more clinical research into SA systems and commercial development of higher-DOF devices.},
  archive   = {C_IROS},
  author    = {Rebecca J. Greene and Christopher Hunt and Brooklyn Acosta and Zihan Huang and Rahul Kaliki and Nitish Thakor},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802319},
  month     = {10},
  pages     = {154-161},
  title     = {Evaluating the impact of a semi-autonomous interface on configuration space accessibility for multi-DOF upper limb prostheses},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Development and functional evaluation of the PrHand v3
soft-robotics prosthetic hand. <em>IROS</em>, 147–153. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The affordability and functionality of hand prosthetics in developing countries are still very limited. This work aims to present and evaluate the new version of the PrHand affordable robotic prosthesis (PrHand V3), built with soft robotics and compliant mechanisms. PrHand V3 implements a new frictionless tendon unification system, the degree of freedom of thumb opposition was removed, and the finger flexion was improved to the previous version, PrHand V2. The study contributes by evaluating these mechanical changes and conducting the first functional assessment of PrHand V3 with an amputee user. The Anthropomorphic Hand Assessment Protocol (AHAP) dexterity test was the first evaluation in this work; it evaluated how the prosthesis performs eight different grips. PrHand V3 was compared with a PrHand V2 and a commercial robotic prosthesis A3D from Prótesis Avanzadas SAS. PrHand V3’s score on the AHAP test was 80%. This result is higher than the 69% obtained by the PrHand V2 and the 79% obtained by A3D. The Activities Measure for Upper Limb Amputees (AM-ULA) test was the second evaluation in this work; An A3D amputee user performed 23 Activities of Daily Living with PrHand V3 and an A3D. PrHand V3 obtained an average of 2.86/4 and A3D obtained an average of 2.96/4 without significant differences between the two tests. The soft actuation of PrHand V3 as an affordable prosthesis performs similarly to a commercial robotic prosthesis with the advantage of being more flexible to assist a trans-radial hand amputee.},
  archive   = {C_IROS},
  author    = {Orion Ramos and Laura De Arco and Marcela Múnera and Jorge Robledo and Mehran Moazen and Helge Wurdemann and Carlos A. Cifuentes},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801454},
  month     = {10},
  pages     = {147-153},
  title     = {Development and functional evaluation of the PrHand v3 soft-robotics prosthetic hand},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multimodal haptic interface for walker-assisted navigation.
<em>IROS</em>, 141–146. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study investigates the efficacy of a haptic interface, aiming to offer the walking frame users accurate, intuitive, and easily understandable directional cues. The research introduces a novel haptic feedback interface incorporated into a walking frame to enhance navigation assistance. The haptic handle encompasses three distinct haptic feedback modalities: vibration, skin stretch, and combined feedback. Ten participants, all in good health, engaged with the haptic handle for navigation. Across all three haptic feedback methods, 60% of participants found the combined feedback to be the most effective, while 40% favoured the vibration feedback; none selected the skin-stretch feedback. Comparative analysis revealed significant disparities between vibration and combined input regarding velocity (p-value: 0.04). These findings emphasize the haptic handle’s capacity to give users an instinctive perception of directional cues, thus offering a promising avenue for assistive navigation.},
  archive   = {C_IROS},
  author    = {Yikun Wang and Sergio D. Sierra M and Nigel Harris and Marcela Múnera and Carlos A. Cifuentes},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801733},
  month     = {10},
  pages     = {141-146},
  title     = {Multimodal haptic interface for walker-assisted navigation},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Force-triggered control design for user intent-driven
assistive upper-limb robots. <em>IROS</em>, 135–140. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Assistive devices are to be designed with the objective of use in daily-life as well as broad adoption by end users. In this context, it is necessary to tackle usability challenges by properly detecting and acting in accordance to user intents while minimizing the device installation complexity as well. In the case of physical assistive devices, using force/torque sensors is advantageous to detect user intent compared to EMG interfaces, but it remains difficult to correctly translate the detected intent into actuator motions. Focusing on upper-limb assistive robots, the user voluntary force is commonly used with a controller based on an admittance approach which leads to relatively poor reactivity and requires the user to develop force throughout the movement which can lead to fatigue, particularly for people with upper-limb impairments. This work proposes a Force-Triggered (FT) controller which can initiate and maintain movement only from short force impulses. The user voluntary forces are retrieved from total interaction forces by subtracting the passive component measured beforehand during a calibration phase. This paper presents the design of the proposed FT controller and its preliminary testing on pick-and-place tasks compared to an admittance strategy. This experiment was performed with one participant without impairment, equipped with an upper-limb exoskeleton prototype designed from recommendations of physical medicine therapists. This preliminary work highlights the potential of the proposed FT controller. Also, it provides directions for future work and clinical trials with end-users to assess the proposed FT approach usability while used alone or in the form of an hybrid controller between FT and admittance strategies.},
  archive   = {C_IROS},
  author    = {Maxime Manzano and Sylvain Guégan and Ronan Le Breton and Louise Devigne and Marie Babel},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801685},
  month     = {10},
  pages     = {135-140},
  title     = {Force-triggered control design for user intent-driven assistive upper-limb robots},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ChatMap: A wearable platform based on the multi-modal
foundation model to augment spatial cognition for people with blindness
and low vision. <em>IROS</em>, 129–134. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Spatial cognition refers to the ability to gain knowledge about their surroundings and utilize this information to identify their location, acquire resources, and navigate their way back to familiar places. People with blindness and low vision (pBLV) face significant challenges with spatial cognition due to the reliance on visual input. Without the full range of visual cues, pBLV individuals often find it difficult to grasp a comprehensive understanding of their environment, leading to obstacles in scene recognition and precise object localization, especially in unfamiliar environments. This limitation extends to their ability to independently detect and avoid potential tripping hazards, making navigation and interaction with their environment more challenging. In this paper, we present a pioneering wearable platform tailored to enhance the spatial cognition of pBLV through the integration of multi-modal foundation model. The proposed platform integrates a wearable camera with audio module and leverages the advanced capabilities of vision language foundation model (i.e., GPT-4 and GPT-4V), for the nuanced processing of visual and textual data. Specifically, we employ vision language models to bridge the gap between visual information and the proprioception of visually impaired users, offering more intelligible guidance by aligning visual data with the natural perception of space and movement. Then we apply prompt engineering to guide the large language model to act as an assistant tailored specifically for pBLV users to produce accurate answers. Another innovation in our model is the incorporation of a chain of thought reasoning process, which enhances the accuracy and interpretability of the model, facilitating the generation of more precise responses to complex user inquiries across diverse environmental contexts. To assess the practical impact of our proposed wearable platform, we carried out a series of real-world experiments across three tasks that are commonly challenging for people with blindness and low vision: risk assessment, object localization, and scene recognition. Additionally, through an ablation study conducted on the VizWiz dataset, we rigorously assess the contribution of each individual module, substantiating the integral role in the model’s overall performance.},
  archive   = {C_IROS},
  author    = {Yu Hao and Alexey Magay and Hao Huang and Shuaihang Yuan and Congcong Wen and Yi Fang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801606},
  month     = {10},
  pages     = {129-134},
  title     = {ChatMap: A wearable platform based on the multi-modal foundation model to augment spatial cognition for people with blindness and low vision},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kiri-spoon: A soft shape-changing utensil for robot-assisted
feeding. <em>IROS</em>, 121–128. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Assistive robot arms have the potential to help disabled or elderly adults eat everyday meals without relying on a caregiver. To provide meaningful assistance, these robots must reach for food items, pick them up, and then carry them to the human’s mouth. Current work equips robot arms with standard utensils (e.g., forks and spoons). But — although these utensils are intuitive for humans — they are not easy for robots to control. If the robot arm does not carefully and precisely orchestrate its motion, food items may fall out of a spoon or slide off of the fork. Accordingly, in this paper we design, model, and test Kiri-Spoon, a novel utensil specifically intended for robot-assisted feeding. Kiri-Spoon combines the familiar shape of traditional utensils with the capabilities of soft grippers. By actuating a kirigami structure the robot can rapidly adjust the curvature of Kiri-Spoon: at one extreme the utensil wraps around food items to make them easier for the robot to pick up and carry, and at the other extreme the utensil returns to a typical spoon shape so that human users can easily take a bite of food. Our studies with able-bodied human operators suggest that robot arms equipped with Kiri-Spoon carry foods more robustly than when leveraging traditional utensils. See videos here: https://youtu.be/nddAniZLFPk},
  archive   = {C_IROS},
  author    = {Maya N. Keely and Heramb Nemlekar and Dylan P. Losey},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801346},
  month     = {10},
  pages     = {121-128},
  title     = {Kiri-spoon: A soft shape-changing utensil for robot-assisted feeding},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BEV image-based lane tracking control system for autonomous
lane repainting robot. <em>IROS</em>, 113–120. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a novel study on a BEV (bird’s eye view) image-based lane tracking control system for an autonomous lane repainting robot. Our research introduces a cutting-edge lane detection method based on BEV images, leveraging row-anchor techniques to enhance precision and provide detailed error information for lane tracking algorithms. By utilizing real-time sensor data and advanced deep learning processes, we have successfully implemented a high-performance lane repainting system that minimizes errors and ensures accuracy. Our proposed position-based visual pure pursuit algorithm (PV-PP) plays a crucial role in guiding the lane repainting process with precision and efficiency, ultimately improving the functionality and feasibility of the linear actuator responsible for paint spraying in the real indusrial fields. Through our contributions, including innovative lane detection methods, real-time sensor utilization, and robot control algorithm design, we aim to advance the field of autonomous lane repainting robots and enhance the safety and effectiveness of road maintenance operations.},
  archive   = {C_IROS},
  author    = {Junghyun Seo and Hyeonjae Jeon and Joonyoung Choi and Kwangho Woo and Yongseob Lim and Yongsik Jin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802616},
  month     = {10},
  pages     = {113-120},
  title     = {BEV image-based lane tracking control system for autonomous lane repainting robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Extensive, long-term task and motion planning with signal
temporal logic specification for autonomous construction. <em>IROS</em>,
105–112. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a hierarchical task and motion planning (TAMP) for autonomous construction that manipulates deformable objects, such as terrain excavation. The TAMP is required to generate an efficient task plan to meet high-level construction goals at sites with environmental diversity while ensuring motion feasibility. The difficulty, however, is to manipulate deformable objects containing nonlinear dynamics with a target given by a continuous value as the task specification. Optimization-based TAMP with signal temporal logic specifications in robotics is promising because of its continuous task specification and formulation as a nonlinear programming problem. The key to its application to extensive, long-term planning at real construction sites is a computationally efficient and stable formulation. We introduce a new expression for deformable objects with a simple differentiable function and a system model that can represent mode transitions based on machine action on the objects. This allows TAMP to be formulated as simultaneously selecting an action for objects and planning the motion to execute it. Furthermore, a hierarchical method that gives appropriate initial values is combined to improve optimality for large-scale nonlinear problems. From the verification by numerical experiments, the proposed method can generate a plan that minimizes the time to meet the task goal, even when the area is expanded.},
  archive   = {C_IROS},
  author    = {Mineto Satoh and Rin Takano and Hiroyuki Oyama},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802424},
  month     = {10},
  pages     = {105-112},
  title     = {Extensive, long-term task and motion planning with signal temporal logic specification for autonomous construction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic throwing with robotic material handling machines.
<em>IROS</em>, 98–104. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automation of hydraulic material handling machinery is currently limited to semi-static pick-and-place cycles. Dynamic throwing motions which utilize the passive joints, can greatly improve time efficiency as well as increase the dumping workspace. In this work, we use Reinforcement Learning (RL) to design dynamic controllers for material handlers with under-actuated arms as commonly used in logistics. The controllers are tested both in simulation and in real-world experiments on a 12-ton test platform. The method is able to exploit the passive joints of the gripper to perform dynamic throwing motions. With the proposed controllers, the machine is able to throw individual objects to targets outside the static reachability zone with good accuracy for its practical applications. The work demonstrates the possibility of using RL to perform highly dynamic tasks with heavy machinery, suggesting a potential for improving the efficiency and precision of autonomous material handling tasks.},
  archive   = {C_IROS},
  author    = {Lennart Werner and Fang Nan and Pol Eyschen and Filippo A. Spinelli and Hongyi Yang and Marco Hutter},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802743},
  month     = {10},
  pages     = {98-104},
  title     = {Dynamic throwing with robotic material handling machines},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards human-centered construction robotics: A
reinforcement learning-driven companion robot for contextually assisting
carpentry workers. <em>IROS</em>, 90–97. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the dynamic construction industry, traditional robotic integration has primarily focused on automating specific tasks, often overlooking the complexity and variability of human aspects in construction workflows. This paper introduces a human-centered approach with a &quot;work companion rover&quot; designed to assist construction workers within their existing practices, aiming to enhance safety and workflow fluency while respecting construction labor’s skilled nature. We conduct an in-depth study on deploying a robotic system in carpentry formwork, showcasing a prototype that emphasizes mobility, safety, and comfortable worker-robot collaboration in dynamic environments through a contextual Reinforcement Learning (RL)-driven modular framework. Our research advances robotic applications in construction, advocating for collaborative models where adaptive robots support rather than replace humans and underscores the potential for an interactive and collaborative human-robot workforce.},
  archive   = {C_IROS},
  author    = {Yuning Wu and Jiaying Wei and Jean Oh and Daniel Cardoso Llach},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802698},
  month     = {10},
  pages     = {90-97},
  title     = {Towards human-centered construction robotics: A reinforcement learning-driven companion robot for contextually assisting carpentry workers},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward precise robotic weed flaming using a mobile
manipulator with a blowtorch. <em>IROS</em>, 82–89. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic weed flaming is a new and environmentally friendly approach to weed removal in the agricultural field. Using a mobile manipulator equipped with a blowtorch, we design a new system and algorithm to enable effective weed flaming, which requires robotic manipulation with a soft and deformable end effector, as the thermal coverage of the flame is affected by dynamic or unknown environmental factors such as gravity, wind, atmospheric pressure, fuel tank pressure, and pose of the nozzle. System development includes overall design, hardware integration, and software pipeline. To enable precise weed removal, the greatest challenge is to detect and predict dynamic flame coverage in real time before motion planning, which is quite different from a conventional rigid gripper in grasping or a spray gun in painting. Based on the images from two onboard infrared cameras and the pose information of the blowtorch nozzle on a mobile manipulator, we propose a new dynamic flame coverage model. The flame model uses a center-arc curve with a Gaussian cross-section model to describe the flame coverage in real time. The experiments have demonstrated the working system and shown that our model and algorithm can achieve a mean average precision (mAP) of more than 76% in the reprojected images during online prediction.},
  archive   = {C_IROS},
  author    = {Di Wang and Chengsong Hu and Shuangyu Xie and Joe Johnson and Hojun Ji and Yingtao Jiang and Muthukumar Bagavathiannan and Dezhen Song},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802119},
  month     = {10},
  pages     = {82-89},
  title     = {Toward precise robotic weed flaming using a mobile manipulator with a blowtorch},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Occlusion handling by pushing for enhanced fruit detection.
<em>IROS</em>, 76–81. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In agricultural robotics, effective observation and localization of fruits present challenges due to occlusions caused by other parts of the tree, such as branches and leaves. These occlusions can result in false fruit localization or impede the robot from picking the fruit. The objective of this work is to push away branches that block the fruit’s view to increase their visibility. Our setup consists of an RGB-D camera and a robot arm. First, we detect the occluded fruit in the RGB image and estimate its occluded part via a deep learning generative model in the depth space. The direction to push to clear the occlusions is determined using classic image processing techniques. We then introduce a 3D extension of the 2D Hough transform to detect straight line segments in the point cloud. This extension helps detect tree branches and identify the one mainly responsible for the occlusion. Finally, we clear the occlusion by pushing the branch with the robot arm. Our method uses a combination of deep learning for fruit appearance estimation, classic image processing for push direction determination, and 3D Hough transform for branch detection. We validate our perception methods through real data under different lighting conditions and various types of fruits (i.e. apple, lemon, orange), achieving improved visibility and successful occlusion clearance. We demonstrate the practical application of our approach through a real robot branch pushing demonstration.},
  archive   = {C_IROS},
  author    = {Ege Gursoy and Dana Kulić and Andrea Cherubini},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802174},
  month     = {10},
  pages     = {76-81},
  title     = {Occlusion handling by pushing for enhanced fruit detection},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design of stickbug: A six-armed precision pollination robot.
<em>IROS</em>, 69–75. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents the design of Stickbug, a six-armed, multi-agent, precision pollination robot that combines the accuracy of single-agent systems with swarm parallelization in greenhouses. Precision pollination robots have often been proposed to offset the effects of a decreasing population of natural pollinators, but they frequently lack the required parallelization and scalability. Stickbug achieves this by allowing each arm and drive base to act as an individual agent, significantly reducing planning complexity. Stickbug uses a compact holonomic Kiwi drive to navigate narrow greenhouse rows, a tall mast to support multiple manipulators and reach plant heights, a detection model and classifier to identify Bramble flowers, and a felt-tipped end-effector for contact-based pollination. Initial experimental validation demonstrates that Stickbug can attempt over 1.5 pollinations per minute with a 49% success rate. Additionally, a Bramble flower perception dataset was created and is publicly available alongside Stickbug’s software and design files.},
  archive   = {C_IROS},
  author    = {Trevor Smith and Madhav Rijal and Christopher Tatsch and R. Michael Butts and Jared Beard and R. Tyler Cook and Andy Chu and Jason Gross and Yu Gu},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801406},
  month     = {10},
  pages     = {69-75},
  title     = {Design of stickbug: A six-armed precision pollination robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Temporal- and viewpoint-invariant registration for
under-canopy footage using deep-learning-based bird’s-eye view
prediction. <em>IROS</em>, 61–68. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Conducting visual assessments under the canopy using mobile robots is an emerging task in smart farming and forestry. However, it is challenging to register images across different data-collection days, especially across seasons, due to the self-occluding geometry and temporal dynamics in forests and orchards. This paper proposes a new approach for registering under-canopy image sequences in general and in these situations. Our methodology leverages standard GPS data and deep-learning-based perspective to bird’s-eye view conversion to provide an initial estimation of the positions of the trees in images and their association across datasets. Furthermore, it introduces an innovative strategy for extracting tree trunks and clean ground surfaces from noisy and sparse 3D reconstructions created from the image sequences, utilizing these features to achieve precise alignment. Our robust alignment method effectively mitigates position and scale drift, which may arise from GPS inaccuracies and Sparse Structure from Motion (SfM) limitations. We evaluate our approach on three challenging real-world datasets, demonstrating that our method outperforms ICP-based methods on average by 50%, and surpasses FGR and TEASER++ by over 90% in alignment accuracy. These results highlight our method’s cost efficiency and robustness, even in the presence of severe outliers and sparsity. https://github.com/VIS4ROB-lab/bev_undercanopy_registration},
  archive   = {C_IROS},
  author    = {Jiawei Zhou and Ruben Mascaro and Cesar Cadena and Margarita Chli and Lucas Teixeira},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802707},
  month     = {10},
  pages     = {61-68},
  title     = {Temporal- and viewpoint-invariant registration for under-canopy footage using deep-learning-based bird’s-eye view prediction},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time semantic segmentation in natural environments with
SAM-assisted sim-to-real domain transfer. <em>IROS</em>, 53–60. (<a
href="https://doi.org/10.1109/IROS58592.2024.10801798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Semantic segmentation plays a pivotal role in many robotic applications requiring high-level scene understanding, such as smart farming, where the precise identification of trees or plants can aid navigation and crop monitoring tasks. While deep-learning-based semantic segmentation approaches have reached outstanding performance in recent years, they demand large amounts of labeled data for training. Inspired by modern Unsupervised Domain Adaptation (UDA) techniques, in this paper, we introduce a two-step training pipeline specifically tailored to challenging natural scenes, where the availability of annotated data is often quite limited. Our strategy involves the initial training of a powerful domain adaptive architecture, followed by a refinement stage, where segmentation masks predicted by the Segment Anything Model (SAM) are used to improve the accuracy of the predictions on the target dataset. These refined predictions serve as pseudo-labels to supervise the training of a final distilled architecture for real-time deployment. Extensive experiments conducted in two real-world scenes demonstrate the effectiveness of the proposed method. Specifically, we show that our pipeline enables the training of a MobileNetV3 that achieves significant mIoU gains of 3.60% and 11.40% on our two datasets compared to the DAFormer while only demanding 1/15 of the latter’s inference time. Code and datasets are available at https://github.com/VIS4ROB-lab/nature_uda_rt_segmentation.},
  archive   = {C_IROS},
  author    = {Han Wang and Ruben Mascaro and Margarita Chli and Lucas Teixeira},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10801798},
  month     = {10},
  pages     = {53-60},
  title     = {Real-time semantic segmentation in natural environments with SAM-assisted sim-to-real domain transfer},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal view point and kinematic control for grape stem
detection and cutting with an in-hand camera robot. <em>IROS</em>,
47–52. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, a methodology to find the best view of a grape stem and approach angle in order to crop it is proposed. The control scheme is based only on a classified point cloud obtained by the in-hand camera attached to the robot’s end effector without continuous stem tracking. It is shown that the proposed controller finds and reaches the optimal view point and subsequently the stem fast and efficiently, accelerating the overall harvesting procedure. The proposed control scheme is evaluated through experiments in the lab with a UR5e robot with an in-hand RealSense camera on a mock-up vine.},
  archive   = {C_IROS},
  author    = {Sotiris Stavridis and Zoe Doulgeri},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802572},
  month     = {10},
  pages     = {47-52},
  title     = {Optimal view point and kinematic control for grape stem detection and cutting with an in-hand camera robot},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Markerless aerial-terrestrial co-registration of forest
point clouds using a deformable pose graph. <em>IROS</em>, 39–46. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For biodiversity and forestry applications, end-users desire maps of forests that are fully detailed—from the forest floor to the canopy. Terrestrial laser scanning and aerial laser scanning are accurate and increasingly mature methods for scanning the forest. However, individually they are not able to estimate attributes such as tree height, trunk diameter and canopy density due to the inherent differences in their field-of-view and mapping processes. In this work, we present a pipeline that can automatically generate a single joint terrestrial and aerial forest reconstruction. The novelty of the approach is a marker-free registration pipeline, which estimates a set of relative transformation constraints between the aerial cloud and terrestrial sub-clouds without requiring any co-registration reflective markers to be physically placed in the scene. Our method then uses these constraints in a pose graph formulation, which enables us to finely align the respective clouds while respecting spatial constraints introduced by the terrestrial SLAM scanning process. We demonstrate that our approach can produce a fine-grained and complete reconstruction of large-scale natural environments, enabling multi-platform data capture for forestry applications without requiring external infrastructure.},
  archive   = {C_IROS},
  author    = {Benoît Casseau and Nived Chebrolu and Matias Mattamala and Leonard Freissmuth and Maurice Fallon},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802448},
  month     = {10},
  pages     = {39-46},
  title     = {Markerless aerial-terrestrial co-registration of forest point clouds using a deformable pose graph},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HortiBot: An adaptive multi-arm system for robotic
horticulture of sweet peppers. <em>IROS</em>, 31–38. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Horticultural tasks such as pruning and selective harvesting are labor intensive and horticultural staff are hard to find. Automating these tasks is challenging due to the semi-structured greenhouse workspaces, changing environmental conditions such as lighting, dense plant growth with many occlusions, and the need for gentle manipulation of non-rigid plant organs. In this work, we present the three-armed system HortiBot, with two arms for manipulation and a third arm as an articulated head for active perception using stereo cameras. Its perception system detects not only peppers, but also peduncles and stems in real time, and performs online data association to build a world model of pepper plants. Collision-aware online trajectory generation allows all three arms to safely track their respective targets for observation, grasping, and cutting. We integrated perception and manipulation to perform selective harvesting of peppers and evaluated the system in lab experiments. Using active perception coupled with end-effector force torque sensing for compliant manipulation, HortiBot achieves high success rates in our indoor pepper plant mock-up.},
  archive   = {C_IROS},
  author    = {Christian Lenz and Rohit Menon and Michael Schreiber and Melvin Paul Jacob and Sven Behnke and Maren Bennewitz},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802082},
  month     = {10},
  pages     = {31-38},
  title     = {HortiBot: An adaptive multi-arm system for robotic horticulture of sweet peppers},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). (Real2Sim)−1: 3D branch point cloud completion for robotic
pruning in apple orchards. <em>IROS</em>, 23–30. (<a
href="https://doi.org/10.1109/IROS58592.2024.10803058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic branch pruning, a rapidly growing field addressing labor shortages in agriculture, requires detailed perception of branch geometry and topology. However, point clouds obtained in agricultural settings often lack completeness, limiting pruning accuracy. This work addressed point cloud quality via a closed-loop approach, (Real2Sim)−1. Leveraging a Real-to-Simulation (Real2Sim) data generation pipeline, we generated simulated 3D apple trees based on realistically characterized apple tree information without manual parameterization. These 3D trees were used to train a simulation-based deep model that jointly performs point cloud completion and skeletonization on real-world partial branches, without extra real-world training. The Sim2Real qualitative results showed the model’s remarkable capability for geometry reconstruction and topology prediction. Additionally, we quantitatively evaluated the Sim2Real performance by comparing branch-level trait characterization errors using raw incomplete data and the best complete data. The Mean Absolute Error (MAE) reduced by 75% and 8% for branch diameter and branch angle estimation, respectively, which indicates the effectiveness of the Real2Sim data in a zero-shot generalization setting. The characterization improvements contributed to the precision and efficacy of robotic branch pruning.},
  archive   = {C_IROS},
  author    = {Tian Qiu and Alan Zoubi and Nikolai Spine and Lailiang Cheng and Yu Jiang},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10803058},
  month     = {10},
  pages     = {23-30},
  title     = {(Real2Sim)−1: 3D branch point cloud completion for robotic pruning in apple orchards},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TriLoc-NetVLAD: Enhancing long-term place recognition in
orchards with a novel LiDAR-based approach. <em>IROS</em>, 16–22. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate long-term place recognition is crucial for agricultural robots operating in unstructured environments. However, in the challenging scene of orchard with high-frequency repetitive features, traditional LiDAR-based localization methods relying on geometric features prove to be inadequate. To address this challenge, we propose TriLoc-NetVLAD, a novel LiDAR-based long-term place recognition approach designed to handle the repetitive and ambiguous features of orchards. This approach initially fuses the point cloud density, height and spatial information to encode unordered 3D point clouds into a spatial context descriptor. then channel selection strategy based on descriptor’s sublayer similarity between query and its corresponding positive and negative samples is proposed to amplify the differences in environmental features. Finally, we use a Triplet Network to extract local features, encompassing both high-dimensional and low-dimensional information. These local features are then cascaded through NetVLAD layer to form a global descriptor. Furthermore, we have built a cross-seasonal orchard dataset to evaluate the performance of our place recognition method. The experiment results demonstrate the advantageous localization performances of the proposed place recognition algorithm over the existing methods.},
  archive   = {C_IROS},
  author    = {Na Sun and Zhengqiang Fan and Quan Qiu and Tao Li and Qingchun Feng and Chao Ji and Chunjiang Zhao},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802261},
  month     = {10},
  pages     = {16-22},
  title     = {TriLoc-NetVLAD: Enhancing long-term place recognition in orchards with a novel LiDAR-based approach},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SPVSoAP3D: A second-order average pooling approach to
enhance 3D place recognition in horticultural environments.
<em>IROS</em>, 9–15. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D LiDAR-based place recognition has been extensively researched in urban environments, yet it remains underexplored in agricultural settings. Unlike urban contexts, horticultural environments, characterized by their permeability to laser beams, result in sparse and overlapping LiDAR scans with suboptimal geometries. This phenomenon leads to intra-and inter-row descriptor ambiguity. In this work, we address this challenge by introducing SPVSoAP3D, a novel modeling approach that combines a voxel-based feature extraction network with an aggregation technique based on a second-order average pooling operator, complemented by a descriptor enhancement stage. Furthermore, we augment the existing HORTO-3DLM dataset by introducing two new sequences derived from horticultural environments. We evaluate the performance of SPVSoAP3D against state-of-the-art (SOTA) models, including OverlapTransformer, PointNetVLAD, and LOGG3D-Net, utilizing a cross-validation protocol on both the newly introduced sequences and the existing HORTO-3DLM dataset. The findings indicate that the average operator is more suitable for horticultural environments compared to the max operator and other first-order pooling techniques. Additionally, the results highlight the improvements brought by the descriptor enhancement stage. The code is publicly available at https://github.com/Cybonic/SPVSoAP3D.git},
  archive   = {C_IROS},
  author    = {T. Barros and C. Premebida and S. Aravecchia and C. Pradalier and U. J. Nunes},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802603},
  month     = {10},
  pages     = {9-15},
  title     = {SPVSoAP3D: A second-order average pooling approach to enhance 3D place recognition in horticultural environments},
  year      = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FruitNeRF: A unified neural radiance field based fruit
counting framework. <em>IROS</em>, 1–8. (<a
href="https://doi.org/10.1109/IROS58592.2024.10802065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce FruitNeRF, a unified novel fruit counting framework that leverages state-of-the-art view synthesis methods to count any fruit type directly in 3D. Our framework takes an unordered set of posed images captured by a monocular camera and segments fruit in each image. To make our system independent of the fruit type, we employ a foundation model that generates binary segmentation masks for any fruit. Utilizing both modalities, RGB and semantic, we train a semantic neural radiance field. Through uniform volume sampling of the implicit Fruit Field, we obtain fruit-only point clouds. By applying cascaded clustering on the extracted point cloud, our approach achieves precise fruit count. The use of neural radiance fields provides significant advantages over conventional methods such as object tracking or optical flow, as the counting itself is lifted into 3D. Our method prevents double counting fruit and avoids counting irrelevant fruit. We evaluate our methodology using both real-world and synthetic datasets. The real-world dataset consists of three apple trees with manually counted ground truths, a benchmark apple dataset with one row and ground truth fruit location, while the synthetic dataset comprises various fruit types including apple, plum, lemon, pear, peach, and mango. Additionally, we assess the performance of fruit counting using the foundation model compared to a U-Net.},
  archive   = {C_IROS},
  author    = {Lukas Meyer and Andreas Gilson and Ute Schmid and Marc Stamminger},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS58592.2024.10802065},
  month     = {10},
  pages     = {1-8},
  title     = {FruitNeRF: A unified neural radiance field based fruit counting framework},
  year      = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
